<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/09/the-closed-api-specification/">The ClosedAPI Specification</a></h3>
        <span class="post-date">09 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/fence_ww2_dresden.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>You’ve heard of OpenAPI, right? It is the API specification for defining the surface area of your web API, and the schema you employ–making your public API more discoverable, and consumable in a variety of tools services. OpenAPI is the API definition for documenting your API when you are just getting started with your platform, and you are looking to maximize the availability and access of your platform API(s). After you’ve acquired all the users, content, investment, and other value, ClosedAPI is the format you will want to switch to, abandoning OpenAPI, for something a little more discreet.</p>

<h3 id="collect-as-much-data-as-you-possibly-can">Collect As Much Data As You Possibly Can</h3>
<p>Early on you wanted to be defining the schema for your platform using OpenAPI, and even offering up a GraphQL layer, allowing your data model to rapidly scale, adding as may data points as you possible can. You really want to just ingest any data you can get your hands on the browser, mobile phones, and any other devices you come into contact with. You can just dump it all into big data lake, and sort it out later. Adding to your platform schema when possible, and continuing to establish new data points that can be used in advertising and targeting of your platform users.</p>

<h3 id="turn-the-firehose-on-to-drive-activity">Turn The Firehose On To Drive Activity</h3>
<p>Early on you wanted your APIs to be 100% open. You’ve provided a firehose to partners. You’ve made your garden hose free to EVERYONE. OpenAPI was all about providing scalable access to as many users as you can through streaming APIs, as well as lower volume transactional APIs you offer. Don’t rate limit too heavily. Just keep the APIs operating at full capacity, generating data and value for the platform. ClosedAPI is for defining your API as you begin to turn off this firehose, and begin restricting access to your garden hose APIs. You’ve built up the capacity of the platform, you really don’t need your digital sharecroppers anymore. They were necessary early on in your business, but they are not longer needed when it comes to squeezing as much revenue as you can from your platform.</p>

<h3 id="the-closedapi-specification">The ClosedAPI Specification</h3>
<p>We’ve kept the specification as simple as possible. Allowing you to still say you have API(s), but also helping make sure you do not disclose too much about what you actually have going on. Providing you the following fields to describe your APIs:</p>

<ul>
  <li>Name</li>
  <li>Description</li>
  <li>Email</li>
</ul>

<p>That is it. You can still have hundreds of APIs. Issue press releases. Everyone will just have to email you to get access to your APIs. It is up to you to decide who actually gets access to your APIs, which emails you respond, or if the email account is ever even checked in the first place. The objective is just to appear that you have APIs, and will entertain requests to access them.</p>

<h3 id="maintain-control-over-your-platform">Maintain Control Over Your Platform</h3>
<p>You’ve worked hard to get your platform to where it is. Well, not really, but you’ve worked hard to ensure that others do the work for you. You’ve managed to convince a bunch of developers to work for free building out the applications and features of your platform. You’ve managed to get the users of those applications to populate your platform with a wealth of data, making your platform exponentially more valuable that you could have done on your own. Now that you’ve achieved your vision, and people are increasingly using your APIs to extract value that belongs to you, you need to turn off the fire hose, garden hose, and kill off applications that you do not directly control.</p>

<p>The ClosedAPI specification will allow you to still say that you have APIs, but no longer have to actually be responsible for your APIs being publicly available. Now all you have to do is worry about generating as much revenue as you possibly can from the data you have. You might lose some of your users because you do not have publicly available APIs anymore, as well as losing some of your applications, but that is ok. Most of your users are now trapped, locked-in, and dependent on your platform–continuing to generate data, content, and value for your platform. Stay in tune with the specification using the road map below.</p>

<h3 id="roadmap">Roadmap:</h3>

<ul>
  <li><strong>Remove Description</strong> – The description field seems extraneous.</li>
</ul>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/09/the-closed-api-specification/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/09/i-will-be-speaking-at-the-api-conference-in-london-this-week/">I Will Be Speaking At The API Conference In London This Week</a></h3>
        <span class="post-date">09 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-conference/api-conference-keynote-details.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I am hitting the road this week heading to London to speak at <a href="https://apiconference.net">the API Conference</a>. I will be giving a keynote on Thursday afternoon, and conducting an all day workshop on Friday. Both of my talks are a continuation of my API life cycle work, and pushing forward my use of a transit map to help me make sense of the API life cycle. My keynote will be covering the big picture of why I think the transit model works for making sense of complex infrastructure, and my workshop is going to get down in the weeds with it all.</p>

<p>My keynote is titled, “<a href="https://apiconference.net/api-management-microservices/looking-at-the-api-life-cycle-through-the-lens-of-a-municipal-transit-system/">Looking At The API Life Cycle Through The Lens Of A Municipal Transit System</a>”, with the following abstract, “As we move beyond a world of using just a handful of internal and external APIs, to a reality where we operate thousands of microservices, and depend on hundreds of 3rd party APIs, modern API infrastructure begins to look as complex as a municipal transit system. Realizing that API operations is anything but a linear life cycle, let’s begin to consider that all APIs are in transit, evolving from design to deprecation, while still also existing to move our value bits and bytes from one place to another. I would like to share with you a look at how API operations can be mapped using an API Transit map, and explored, managed, and understood through the lens of a modern, Internet enabled “transit system”.”</p>

<p>My workshop is titled, “<a href="https://apiconference.net/api-management-microservices/beyond-the-api-lifecycle-and-beginning-to-establish-an-api-transit-system/">Beyond The API Lifecycle And Beginning To Establish An API Transit System</a>”, with the following abstract, “Come explore 100 stops along the modern API life cycle, from definition to deprecation. Taking the learnings from eight years of API industry research, extracted from the leading API management pioneers, I would like to guide you through each stop that a modern API should pass across, mapped out using a familiar transit map format, providing an API experience I have dubbed API Transit. This new approach to mapping out, and understanding the API life cycle as seen through the lens of a modern municipal transit system allows us to explore and learn healthy patterns for API design, deployment, management, and other stops along an APIs journey. Which then becomes a framework for executing and certifying that each API is following the common patterns which each developer has already learned, leading us to a measurable and quantifiable API governance strategy that can be reported upon consistently. This workshop will go into detail on almost 100 stops, and provide a forkable blueprint that can be taken home and applied within any API operations, no matter how large, or small.”</p>

<p>I will also be spending some time meeting with a variety of API service providers, and looking to meet up with some friends while in London. If you are around, ping me. I’m not sure how much time I will have, but I’m always game to try and connect. I’m eager to talk with folks about banking activity in the UK, as well as talking about the event-driven architecture, and API life cycle / governance work I’m doing with Streamdata.io. I’ll be at the event most of the day Thursday, and all day Friday, but both evenings I should have some time available to connect. See you in London!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/09/i-will-be-speaking-at-the-api-conference-in-london-this-week/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/09/details-of-the-api-evangelist-partner-program/">Details Of The API Evangelist Partner Program</a></h3>
        <span class="post-date">09 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist-logos/api-evangelist-blue-seal.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve been retooling the partner program for API Evangelist. There are many reasons for this, and <a href="http://apievangelist.com/2018/04/09/creating-a-productive-api-industry-partner-program/">you can read the full backstory I have written a narrative for these changes to the way in which I partner</a>. I need to make a living, and my readers are expecting me to share relevant stories from across the sector on my blog. I’m also tired of meaningless partner arrangements that never go anywhere, and I’m looking to incentivize my partners to engage with me, and the API community in an impactful way. I’ve crafted a program that I think will do that.</p>

<p>While there will still be four logo slots available for partnership, the rest of the references to API services providers and the solutions they provide will be driven by my partner program. If you want to be involved, you need to partner with me. All that takes is to email me that you want to be involved. It doesn’t cost you a dime, all you have to do is reach out, let me know you are interested, and be willing to play the part of an active API service provider. If you are making an impact on the API space, then you’ll enjoy the following exposure across my work:</p>

<ul>
  <li><strong>API Lifecycle</strong> - Your services and tooling will be listed as part of my API lifecycle research, and displayed on the home page of my website, and across my network of sites.</li>
  <li><strong>Short Form Storytelling</strong> - Being referenced as part of my stories when I talk about the areas of the API sector in which your services and tooling provides solutions.</li>
  <li><strong>Long Form Storytelling</strong> - Similar to short form, when i’m writing white papers, and guides, I will use your products and services as examples to highlight the solutions I’m talking about.</li>
  <li><strong>Story Ideas</strong> - You will have access a list of working story ideas I’m working through, and able to add to the list, as well as extract ideas from the list for providing story idea seeds for your own content teams.</li>
  <li><strong>In-Person Talks</strong> - When I’m giving talks at conferences I will be including your products and services into my slides, and storytelling, using them as a cornerstone for my in-person talks.</li>
  <li><strong>Workshops</strong> - Similar to talks, I will be working my partners into the workshops I give, and when I walk through my API lifecycle and governance work, I will be referencing the tools and services you provide.</li>
  <li><strong>Consulting</strong> - When working with clients, helping them develop their API lifecycle and governance strategy I will be recommending specific services and tooling that are offered by my partners.</li>
</ul>

<p>I will be doing this for the partners who have the highest ranking. I won’t be doing this in exchange for money. Sure, some of these partners will be buying logos on the site, and paying me for referrals, and deals they land. However, that is just one aspect of how I fund what I do. It won’t change my approach to research and storytelling. I think I’ve done a good job of demonstrating my ability to stay neutral when it comes to my work, and something that my work will continue to demonstrate. If you question how partner relationships will affect my work, then you probably don’t know me very well.</p>

<p>I won’t be taking money to write stories or white papers anymore. It never works, and always gets in the way of me producing stories at my regular pace. My money will come from consulting, speaking, and the referrals I send to my partners. Anytime I have taken money from a partner, I will disclose that I have on my site, and acknowledge the potential influence on my work. However, in the end, nothing will change. I’ll still be writing as I always have, and maintain as critical as I always have been, even of my partners. The partners who make the biggest impact on the space will rise to the top, and the ones who do not, will stay at the bottom of the list, and rarely show up in my work. If you’d like to partner, just drop me an email, and we’ll see what we can make happen working together.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/09/details-of-the-api-evangelist-partner-program/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/09/creating-a-productive-api-industry-partner-program/">Crafting A Productive API Industry Partner Program</a></h3>
        <span class="post-date">09 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist-logos/api-evangelist-red-seal.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I struggle with partner relationships. I’ve had a lot of partners operating API Evangelist over the years. Some good. Some not so good. And some amazing! You know who you are. It’s tough to fund what I do as the API Evangelist. It’s even harder to fund who I am as Kin Lane. I’ve revamped my approach to partnering several times now trying to find the right formula for me, my partners, and for my readers. As the partner requests pile up, and I fall short for some of my existing partners, while delivering as expected for others, it is time for me to take another crack at shaping my partner program.</p>

<p><strong>A Strong Streamdata.io Partnership Base</strong><br />
A cornerstone of my new approach to partnering is based upon my relationship with Streamdata.io. They are my primary partner, and supporter of API Evangelist. They have not just helped provide me with the financial base I need to live and operate API Evangelist, they are investing in, and helping grow my existing API lifecycle and <a href="http://theapistack.com">API Stack</a> work. We are also working in concert to formalize my API lifecycle work into a growing consultancy, and pushing forward my API Stack research, syndicating it as the Streamdata.io API Gallery, and refining the my ranking system for both API providers, and API service providers. Without Streamdata.io this latest round of API Evangelist wouldn’t be happening.</p>

<p><strong>Difficulties With The Current Mode Of Partnering</strong><br />
One of the biggest challenges I have right now with partnering is that my partners want me to produce content about them. Writing stories for pay just isn’t a good idea for their brand, or for mine. I know it is what people want, but it just doesn’t work. The other challenge I have is people tend to want predictable stories on a schedule. I know it seems like I’m a regular machine, churning out content, but honestly when it flows, it all works. When it doesn’t flow, it doesn’t work. I schedule things out ahead enough that the bumps are smoothed out. I love writing. The anxiety I get from writing on a deadline, or for expected topics isn’t conducive to producing good content, and storytelling. This applies to both my short form (blog posts), and my long form (white papers). I’ll still be producing both of these things, but I can’t do it for money any longer.</p>

<p><strong>Diffciulties With Incentivizing Partner Behavior</strong><br />
I have no shortage of people who’d like to partner and get exposure from what I do, but incentivizing what I’d like to see from these partners is difficult. I’m not just looking for money from them, I’m looking to incentivize companies to build interesting products and services, tell stories that are valuable to the community, and engage with the API space in a meaningful way. I want to leverage my partners to behave as good citizens, give back to the space, and yes, get new users and generate revenue from their activity. I’ve seen too many partnerships exclusively be about the money involved, or just be about the press release, with no meaningful substance actually achieved in between. This type of hollow, meaningless, partnership masturbation does no good for anyone, and honestly is a waste of my time. I don’t expect ALL partnerships to bear fruit, but there should be a high bar for defining what partnership means, and we should be working to making it truly matter for everyone involved.</p>

<p><strong>There Are Three Dimensions To Creating Partner Value</strong><br />
As I see it, there are three main dimensions to establishing a productive API industry partnership program. There is partner A, and partner B, but then there is potential customer. If a partnership isn’t benefiting all three of these actors equally, then it just won’t work. I understand that as a company you are looking to maximize the value generation and extraction for your business, but there is enough to go around, and one of the core tenets of partnerships is that this value is shared, with the customer and community in mind. Not all companies get this. It is my role to make sure they are reminded of it, and push for balanced partnerships that are healthy, active, fruitful, but also benefit the community. We will all benefit from this, despite many shortsighted, self-centered approaches to doing business in API-land that I’ve encountered over my eight years operating as the API Evangelist. To help me balance my API partnership program, I’m going to be applying mechanism I’ve used for years to help me define and understand who my true partners are, and whoever rises to the top will see the most benefits from the program.</p>

<p><strong>Tuning Into API Service Provider Partner Signals</strong><br />
When it comes to quantifying my partners, who they are, and what they do, I’m looking at a handful of signals to make sure they are contributing not just to the partnership, but to the wider community. I’m looking to incentivize API service providers to deliver as much value to the community than they are extracting as part of the partnership. Here is what I’m looking for to help quantify the participation of any partner on the table:</p>

<ul>
  <li><strong>Blog</strong> - The presence of an active blog with and RSS / Atom feed. Allowing me to tune into what is happening, and help share the interesting things that are happening via their platform and community. If something good is being delivered, and the story isn’t told, it didn’t happen. An active blog is one of the more important signals I can tune into as the API Evangelist – also I need a feed!!!</li>
  <li><strong>Github</strong> - The presence of an active Github account. Possessing a robust number of repositories for a variety of API ecosystem projects from API definitions to SDKs. I’m tuning into all the Github signals including number of repos, stars, commits, forks, and other chatter to understand the activity and engagement levels occurring within a community. Github is the social network for API providers–make sure you are being active, observable, and engaging.</li>
  <li><strong>Twitter</strong> - The presence of an active, dedicated Twitter account. Because of it’s public nature, Twitter provides a wealth of signals regarding what is happening via any platform, and provides one of the most important ways in which a service provider can be engaging with their customers, and the wider APIs pace. I understand that not everyone excels at Twitter engagements, but using as an information and update sharing channel, and general support and feedback loop is within the realm of operation for EVERYONE.</li>
  <li><strong>Search</strong> - I’m always looking for a general, and hopefully wide search engine presence for API service providers. I have alerts setup for all the API service providers I monitor, which surfaces relevant stories, conversations, and information published by each platform. SEO isn’t hard, but takes a regular, concerted effort, and it is easy to understand how much a service provider is investing in their presence, or not.</li>
  <li><strong>Business Model</strong> - The presence of, or lack of a business model, as well as investment is an important signal I keep an eye on, trying to understand where each API service provider in their evolution. How new a company is, how far along their runway they are, and what the exit and long term strategy for an API service provider is. Keeping an eye on Crunchbase and other investment, pricing plans, and revenue signals coming out of a platform will play a significant role in understand the value an API service is bringing to the table.</li>
  <li><strong>Integrations</strong> - I’m also tracking on the integrations any service provider offers, ensuring that their API service providers are investing in, and encouraging interpretability with other platforms by default. API service providers that do not play well with others, often do not make good partners, insisting on all roads leading to their platform. I’m always on the hunt for a dedicated integrations and plugin page for any API service provider I’m partnering with.</li>
  <li><strong>Partnerships</strong> - Beyond integrations I want to see the all the other partnerships one of my partners is engaging with. The relationships they are engaging in tell a lot about how well they will partner with me, and define what signals they are looking to send to the community. Partnerships tell a lot of story about the motivations behind a companies own partner program, and how it will benefit and impact my own partner program.</li>
  <li><strong>API</strong> - I am always looking for whether or not an API service provider has an API. If a company is selling services, products, and tooling to the API sector, but doesn’t have a public facing API, I’m always skeptical of what they are up to. I get it, it can often not be a priority to operate your own program, but in reality, can anyone trust an API service provider to help deliver on their own program, if they don’t have the same experience operating their own API program? This is one of my biggest pet peeves with API service providers, and a very telling sign about what is happening behind the scenes.</li>
  <li><strong>API Definitions</strong> - If you have an API as an API service provider then they should have an OpenAPI and Postman Collections available for their API. The presence of API definitions, as well as robust portal, docs, and other API building blocks is essential for any API service provider.</li>
  <li><strong>Story Ideas</strong> - I’m very interested in the number of story ideas submitted by API service providers, pointing me to interesting stories I should cover, as well as the interesting things that are occurring via their platform. Ideally, these are referenced by public blog posts, Tweets, and other signals sent by the API service provider, but they also count when received via direct message, email, and carrier pigeon (bonus points for pigeon).</li>
</ul>

<p>There are other signals I’m looking for from my partners, but that is the core of what I’m looking for. I know it sounds like a lot, but it really isn’t. It is the bar that defines a quality API service provider, based upon eight years of tracking on them, and watching them come and go. The API service providers who are delivering in these areas will float up in my automated ranking system, and enjoy more prominence in my storytelling, and across the API Evangelist network.</p>

<p><strong>Tuning Into The API Community Signals</strong><br />
Beyond the API service providers themselves I’m always tuned into what the community is saying about companies, products, services, and tooling. While there is always a certain level of hype around what is happening in the API sector, you can keep your finger on the pulse of what is going on through a handful of channels. Sure, many of the signals present on these channels can be gamed by the API service providers, but others are much more difficult, and will take a lot of work to game. Which can be a signal on its own. Here is what I’m looking for to understand what the API community is tuning into:</p>

<ul>
  <li><strong>Blogs</strong> - I’m tuning into what people across the space are writing about on their personal, and company blogs. I’m regularly curating stories that reference tools, services, and the products offered by API service providers. If people are writing about specific companies, I want to know about it.</li>
  <li><strong>Twitter</strong> - Twitter provides a number of signals to understand what the community thinks about an API service provider, including follows, and retweets. The conversation volume initiated and amplified by the community tells a significant story about what an API service provider is up to.</li>
  <li><strong>Github</strong> - Github also provides one of the more meaningful platforms for harvesting signals about what API service providers are up to. The stars, forks, and discussion initiated and amplified by the community on the Github repos and organizations of API services providers tell an important story.</li>
  <li><strong>Search</strong> - Using the Google and Talkwalker alerts setup for each API service provider, I curate the Reddit, Hacker news, and other search indexed conversations occurring about providers, tracking on the overall scope of conversation around each API service provider.</li>
</ul>

<p>There are other signals I’m developing to understand the general tone of the API community, but these reflect the proven ones I’ve been using for years, and provide a reliable measure of the impact an API service provider is making, or not. One important aspect of this search and social media exhaust is in regards to sentiment, which can be good or bad, depending on the tone the API service provider is setting within the community.</p>

<p><strong>Generating My Own Signals About What Is Happening</strong><br />
This is the portion of the partner relationship where I’m held accountable. These are the things that I am doing that deliver value to my partners, as well as the overall API community. These items are the signals partners are looking for, but also the things I’m measuring to understand the role an API service provider plays in my storytelling, speaking, consulting, and research. Each of these areas reflect how relative an API service, their products, services, and tooling is to the overall API landscape as it pertains to my work. Here is what I’m tracking on:</p>

<ul>
  <li><strong>Short Form Storytelling</strong> - Tracking on how much I write about an API service provider in my blog posts on API Evangelist, Streamdata.io, and other places I’m syndicated like DZone. If I’m talking about a company, they are doing interesting an relevant things, and I want to be showcasing their work. I can’t just write about someone because I’m paid, it is because they are relevant to the story I’m telling.</li>
  <li><strong>Long Form Storytelling</strong> - Understanding how API service providers are referenced in my guides and white papers. Similar to the short form references, I’m only referencing companies when they are doing relevant things to the stories I’m telling. My guides used to be comprehensive when it came to mentioning ALL API service providers, but going forward they will only reference those that float to the top, and rank high in my overall API partner ranking.</li>
  <li><strong>Story Ideas</strong> - I’m regularly writing down story ideas, and aggregating them into lists. Not everything ever turns into a story, but still the idea demonstrates something that grabbed my attention. I tend to also share story ideas with other content producers, and publications, providing the seeds for interesting stories that I may not have the time to write about myself. Providing rich, and relevant materials for others to work from.</li>
  <li><strong>My In-Person Talks</strong> - Throughout the talks I give at conferences, Meetups, and in person within companies, organizations, institutions, and government I am referencing tools and service to accomplish a specific API related task–I need the best solutions to reference as part of these talks. I carefully think about which providers I will reference, and I keep track of which ones I reference.</li>
  <li><strong>API Lifecycle</strong> - My API lifecycle work is build upon my research across the API stack, what I learn from the public API providers I study, as well as what I learn as part of my consulting work. All of this knowledge and research goes back into my API lifecycle and governance strategy, and becomes part of my outreach and strategy which gets implemented on the ground as part of my consulting. Across the 68+ stops along the API there are always API service providers I need to reference, and refer to as part of my work.</li>
  <li><strong>API Stack</strong> - My API Stack work is all about profiling publicly available APIs out there and understanding the best practices at work when it comes to operating their APIs. When I notice that a service or tool is being put to use I take note of it. I use these references as part of my overall tracking and understanding of what is being put to use across the industry.</li>
  <li><strong>Website Logos</strong> - There are four logos for partners to sponsor across the network of API Evangelist sites. While I’m not weighting these in my ranking, when someone is present on the site like that they are part of my consciousness. I recognize this, and acknowledge that it does influence my overall view of the API sector.</li>
  <li><strong>Conversations</strong> - I’m regularly engaging in conversations with my partners, learning about their roadmaps, and staying in tune with what they are working on. These conversations have a significant impact on my view of the space, and help me understand the value that API service providers bring to the table.</li>
  <li><strong>Referrals</strong> - Now is where we start getting to the money part of this conversation. When I refer clients to some of my partners, there are some revenue opportunities available for me. Not every referral is done because I’m getting paid, but there are partners who do pay me when I send business their way. This influences the way that I see the landscape, not because I’m getting paid, but because someone I’m talking to chose to use a service, and this will influence how I see the space.</li>
  <li><strong>Deals Made</strong> - I do make deals, and bring in revenue based upon the business I send to my partners. This isn’t why I do what I do, but it does pay the bills. I’m not writing and telling stories about companies because I have relationships with them. I write and tell stories about them because they do interesting things, and provide solutions for my audience. I’m happy to be transparent about this side of my business, and always work to keep things out in the open.</li>
</ul>

<p>This is the core the signals I’m generating that tell me which API service providers are making an impact on the API sector. Everything I do reinforces who the most relevant API service providers are. If I’m telling stories about them, then I’m most likely telling more stories. If I’m referring API service providers to my readers and clients, then it just pushes me to read more about what they are doing, and tell more stories about them. The more an API service providers floats up in my world, the more likely they will stay there.</p>

<p><strong>Partnerships Balanced Across Three Dimensions</strong><br />
My goal with all of this is to continue applying my ranking of API service providers across three main dimensions. Things that are within their control. Things that are within the communities control. Then everything weighted and influenced by my opinion. While money does influence this, that isn’t what exclusively drives which API service providers show up across my work. If I take money to write content, then its hard to say that content is independent. The content is what drives the popularity and readership of API Evangelist, so I don’t want to negatively impact this work. It is easy to say that my storytelling and research is influenced by referral fees and deals I make with partners, but I find this irrelevant if my list of partners is ranked by a number of other elements, within the API service providers control, but also due to signals that are not.</p>

<p><strong>The Most Relevant Partners Rise To The Top</strong><br />
Everything on API Evangelist is YAML driven. The stories, the APIs, API service providers, and the partners that show across the stories I tell, and the talks I give. I used to drive the listings of APIs and API service providers using my ranking, floating the most relevant to the top. I’m going to start doing that again. I’ve been slowly turning back on my automated monitoring, and updating the ranking for APIs and API service providers. When an API service providers shows up on the list of services or tools I showcase, only the highest rank will float to the top. If API service providers are active, the community is talking about them, and I’m tuned into what they are doing, then they’ll show up in more work more frequently. Keeping the most relevant services and tooling available across the API space, and my research, being showcased in my storytelling, talks, and consulting.</p>

<p>My objective with this redefining of my partner program is to take what I’ve learned over the years, and retool my partnership to deliver more value, help keep my website up and running, but most importantly get out of the way of what I do best–research and storytelling around APIs. I can’t have my partnerships slow or redirect my storytelling. I can’t let my partnerships dilute or discredit my brand. However, I still need to make a living. I still need partners. They still need value to be generated by me, and I want to help ensure that they bring value to the table for me, my readers, as well as the wider API community. This is my attempt to do that. We’ll see how it goes. I will update each month and see what I can do to continue dialing it in. If you have any comments, questions, concerns, or would like to talk about partnering–let me know.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/09/creating-a-productive-api-industry-partner-program/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/05/nest-branding-and-marketing-guidelines/">Nest Branding And Marketing Guidelines</a></h3>
        <span class="post-date">05 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/nest/nestreview-workswithnest.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m always looking out for examples of API providers who have invested energy into formalizing process around the business and politics of API operations. I’m hoping to aggregate a variety of approaches that I can aggregate into a single blueprint that I can use in my API storytelling and consulting. The more I can help API providers standardize what they do, the better off the API sector will be, so I’m always investing in the work that API providers should be doing, but doesn’t always get prioritized.</p>

<p>The other day <a href="https://streamdata.io/blog/streaming-nest-api/">while profiling the way that Nest uses Server-Sent Events (SSE) to stream activity via thermostats, cameras, and the other devices they provide</a>, and <a href="https://developers.nest.com/documentation/cloud/marketing-guide">I stumbled across their branding policies</a>. It provides a pretty nice set of guidance for Nest developers in respect to the platform’s brand, and something you don’t see with many other API providers. I always say that branding is the biggest concern for new API providers, but also the one that is almost never addressed by API providers who are in operation–which doesn’t make a whole lot of sense to me. If I had a major corporate brand, I’d work to protect it, and help developers understand what is important.</p>

<p>The Nest marketing program is intended to qualify applications, and then allow them to use the “Works with Nest” branding in your marketing and social media. To get approved you have submit your product or service for review. As part of the review process verifies that you are in compliance with all of their branding policies, including:</p>

<ul>
  <li><a href="https://developers.nest.com/documentation/cloud/ui-guide">User interface guide</a></li>
  <li><a href="https://developers.nest.com/documentation/cloud/ux-guide">User experience guide</a></li>
  <li><a href="https://developers.nest.com/documentation/cloud/brandguide-tm">Trademark policy</a></li>
</ul>

<p>To apply for the program you have to email them with all the following details regarding the marketing efforts our your product or service where you will be using the “Works with Nest” branding:</p>

<ul>
  <li>Description of your marketing program</li>
  <li>Description of your intended audience</li>
  <li>Planned communication (list all that apply): Print, Radio, TV, Digital Advertising, OOH, Event, Email, Website, Blog Post, Email, Social Post, Online Video, PR, Sales, Packaging, Spec Sheet, Direct Mail, App Store Submission, or other (please specify)</li>
  <li>High resolution assets for the planned communication (please provide URL for file download); images should be at least 1000px wide</li>
  <li>Planned geography (list all that apply): Global, US, Canada, UK, or other (please specify)</li>
  <li>Estimated reach: 1k - 50k, 50k- 100k, 100k - 500k, or 500k+</li>
  <li>Contact Information: First name, last name, email, company, product name, and phone number</li>
</ul>

<p>Once submitted, Nest says they’ll provide feedback or approval of your request within 5 business days, and if all is well, they’ll approve your marketing plan for that Works with Nest product. If they find any submission is not in compliance with their branding policies, they’ll ask you to make corrections to your marketing, so you can submit again. I don’t have too many examples of marketing and branding submission process as part of <a href="http://branding.apievangelist.com/">my API branding research</a>. I have the user interface guide, and trademark policy as building blocks, but the user experience guide, and the details of the submission process are all new entries to my research.</p>

<p>I feel like API providers should be able to defend the use of their brand. I do not feel API providers can penalize and cut-off API consumers access unless there are clear guidelines and expectations presented regarding what is healthy, and unhealthy behavior. If you are concerned about how your API consumers reflect your brand, then take the time to put together a program like Nest has. You can look at <a href="http://branding.apievangelist.com/">my wider API branding research</a> if you need other examples, or I’m happy to dive into the subject more as part of a consulting arrangement, and help craft an API branding strategy for you. Just let me know.</p>

<p><em><strong>Disclosure:</strong> I do not “Work with Nest” – I am just showcasing the process. ;-)</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/05/nest-branding-and-marketing-guidelines/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/05/seamless-api-lifecycle-integration-with-github-gitlab-and-bitbucket/">Seamless API Lifecycle Integration With Github, Gitlab, And BitBucket</a></h3>
        <span class="post-date">05 Apr 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_161_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p><em>This is a story, in a series of stories that I’m doing as part of the version 3.0 release of the <a href="https://next.stoplight.io/">Stoplight.io platform</a>. Stoplight.io is one the few API service providers I’m excited about what when it comes to what they are delivering in the API space, so I jumped at the opportunity to do some paid work for them. As I do, I’m working to make these stories about the solutions the provide, and refrain from focusing on just their product, hopefully maintaining my independent stance as the API Evangelist, and keeping some of the credibility I’ve established over the years.</em></p>

<p>Github, Gitlab, and Bitbucket have taken up a central role in the delivery of the valuable API resources we are using across our web, mobile, and device-based applications. These platforms have become integral parts of our development, and software build processes, with Github being the most prominent player when it comes to defining how we deliver not just applications, but increasingly API-driven applications on the web, our mobile phones, and common Internet connected objects becoming more ubiquitous across our physical worlds.</p>

<p><strong>A Lifecycle Directory Of Groups and Users</strong>
These social coding platforms come with the ability to manage different groups, projects, as well as the users involved with moving project forward. While version control isn’t new, Github is credited with making this aspect of managing code a very social endeavor which can be done publicly, or privately within select groups. Providing a rich environment for defining who is involved with each microservice that is being moved along the API lifecycle, allowing teams to leverage the existing organization and user structure provided by these platforms as part of their API lifecycle organizational structure.</p>

<p><strong>Repositories As A Wrapper For Each Service</strong>
Five years ago you would most likely find just code within a Github repository. In 2018, you will find schema, documentation, API definitions, and numerous artifacts stored and evolved within individual repositories. The role of the code repository in the API lifecycle when it comes to helping to move forward an API from design, to mocking, to documentation, testing, and other stops along the lifecycle can’t be understated. The individual repository has become synonymous with a single unit of value in microservices, containerization, and continuous deployment and integration conversations, making it the logical vehicle for the API lifecycle.</p>

<p><strong>Knowing the Past And Seeing The Future Of The API Lifecycle</strong>
The API lifecycle has a beginning, an end, as well as many stops along the way. Increasingly the birth of an API begins in a repository, with a simple README file explaining its purpose. Then the lifecycle becomes defined through a series of user-driven submissions via issues, comments, and commits to the repository. Which are then curated, organized and molded into a road map, list of currently known issues, and a resulting change log of everything that has occurred. Making seamless integration for individual repositories, as well as across all of an organization’s API-centric repositories pretty critical to moving along the API lifecycle in an efficient manner.</p>

<p><strong>Providing A Single Source Voice And Source Of Truth</strong>
With the introduction of Jekyll, and other static page solutions to the social coding realm, the ability for a repository to become a central source of information, communications, notifications, and truth has been dramatically elevated. The API lifecycle is being coordinated, debated, and published using the underlying Git repository, becoming a platform for delivering the code, definitions, and other more technical elements. We are increasingly publishing documentation along side code and OpenAPI definition updates, and aggregating issues, milestones, and other streams generated at the repository level to provide a voice for the API lifecycle.</p>

<p><strong>Seamless Integration With Lifecycle Tooling Is Essential</strong>
Version control in its current form is the movement we experience across the API lifecycle. The other supporting elements for organizational, project, and user management provide the participation layer throughout the lifecycle. The communication and notification layers give the life cycle a voice, keeping everyone in sync and moving forward in concert. For any tool or service to contribute to the API lifecycle in a meaningful way, it has to seamless integrate with Git, as well as the underlying API for these software development, version control, devops, and social coding platforms. Successful API service providers will integrate with our existing software lifecycle(s), serving one or many stops, and helping us all move beyond a single, walled garden platform mentality.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/05/seamless-api-lifecycle-integration-with-github-gitlab-and-bitbucket/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/05/helping-stoplight-io-get-the-word-out-about-version-30/">Helping Stoplight.io Get The Word Out About Version 3.0</a></h3>
        <span class="post-date">05 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/stoplight/stoplight-logo.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve been telling stories about what the <a href="https://next.stoplight.io/">Stoplight.io</a> team has been building for a couple of years now. They are one of the few API service provider startups left that are doing things that interest me, and really delivering value to their API consumers. In the last couple of years, as things have consolidated, and funding cycles have shifted, there just hasn’t been the same amount of investment in interesting API solutions. So when Stoplight.io approached me to do some storytelling around their version 3.0 release, I was all in. Not just because I’m getting paid, but because they are doing interesting things, that I feel are worth talking about.</p>

<p>I’ve always categorized Stoplight.io as an API design solution, but as they’ve iterated upon the last couple of versions, I feel they’ve managed to find their footing, and are maturing to become one of the few truly API lifecycle solutions available out there. They don’t serve every stop along the API lifecycle, but they do focus on a handful of the most valuable stops, and most importantly, they have adopted OpenAPI as the core of what they do, allowing API providers to put Stoplight.io to work for them, as well as any other solutions that support OpenAPI at the core.</p>

<p>As far as the stops along the API lifecycle that they service, here is how I break them down:</p>

<ul>
  <li><strong>Definitions</strong> - An OpenAPI driven way of delivering APIs, that goes beyond just a single definition, and allows you to manage your API definitions at scale, across many teams, and services.</li>
  <li><strong>Design</strong> - One of the most advanced API design GUI solutions out there, helping you craft and evolve your APIs using the GUI, or working directly with the raw JSON or YAML.</li>
  <li><strong>Virtualization</strong> - Enabling the mocking and virtualization of your APIs, allowing you to share, consume, and iterate on your interfaces long before you have deliver more costly code.</li>
  <li><strong>Testing</strong> - Provides the ability to not just test your individual APIs, but define and automate using detailed tests, assertions, and deliver a variety of scenarios to ensure APIs are doing what they should be doing.</li>
  <li><strong>Documentation</strong> - Allows for the publishing of simple, clean, but interactive documentation that is OpenAPI driven, and share with your team, and your API community through a central portal.</li>
  <li><strong>Discovery</strong> - Tightly integrated with Github, and maximizing an OpenAPI definition in a way that makes the entire API lifecycle discoverable by default.</li>
  <li><strong>Governance</strong> - Allows for teams to get a handle on the API design and delivery lifecycle, while working to define and enforce API design standards, and enforce a certain quality of service across the lifecycle.</li>
</ul>

<p>They may describe themselves a little differently, but in terms of the way that I tag API service providers, these are the stops they service along the API lifecycle. They have a robust API definition and design core, with an attractive easy to use interface, which allows you to define, design, virtualize, document, test, and collaborate with your team, community, and other stakeholders. Which makes them a full API lifecycle service provider in my book, because they focus on serving multiple stops, and they are OpenAPI driven which allows every other stop to also be addressed using any other tools and service that supports OpenAPI–which is how you do business with APIs in 2018.</p>

<p>I’ve added API governance to what they do, because they are beginning to build in much of what API teams are going to need to begin delivering APIs at scale across large organizations. Not just design governance, but the model and schema management you’ll need, combined with mocking, testing, documentation, and the discovery that comes along with delivering APIs like Stoplight.io does. They reflect not just where the API space is headed with delivering APIs at scale, but what organizations need when it comes to bringing order to their API-driven, software development lifecycle in a microservices reality.</p>

<p>I have five separate posts that I will be publishing over the next couple weeks as Stoplight.io releases version 3.0 of their API development suite. Per my style I won’t always be directly about their product, but I’ll be talking about the solutions it deliver, but occasionally you’ll hear me mention them directly, because I can’t help it. Thanks to Stoplight.io for supporting what I do, and thanks to you my readers for <a href="https://next.stoplight.io/">checking out what Stoplight.io brings to the table</a>. I think you are going to dig what they are up to.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/05/helping-stoplight-io-get-the-word-out-about-version-30/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/03/openapi-is-the-contract-for-your-microservice/">OpenAPI Is The Contract For Your Microservice</a></h3>
        <span class="post-date">03 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/OpenAPI_Pantone.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve talked about how <a href="http://apievangelist.com/2018/02/14/code-generating-openapi-still-prevailing-approach/">generating an OpenAPI (fka Swagger) definition from code is still the dominate way that microservice</a> owners are producing this artifact. This is a by-product of developers seeing it as just another JSON artifact in the pipeline, and from it being primarily used to create API documentation, often times using Swagger UI–which is also why it is still called Swagger, and not <a href="https://www.openapis.org/">OpenAPI</a>. I’m continuing my campaign to help the projects I’m consulting on be more successful with their overall microservices strategy by helping them better understand how they can work in concert by focus in on OpenAPI, and realizing that it is the central contract for their service.</p>

<p><strong>Each Service Begins With An OpenAPI Contract</strong><br />
There is no reason that microservices should start with writing code. It is expensive, rigid, and time consuming. The contract that a service provides to clients can be hammered out using OpenAPI, and made available to consumers as a machine readable artifact (JSON or YAML), as well as visualized using documentation like Swagger UI, Redocs, and other open source tooling. This means that teams need to put down their IDE’s, and begin either handwriting their OpenAPI definitions, or being using an open source editor like <a href="https://editor.swagger.io/">Swagger Editor</a>, <a href="https://github.com/Apicurio">Apicurio</a>, <a href="https://github.com/Mermade/openapi-gui">API GUI</a>, or even within the Postman development environment. The entire surface area of a service can be defined using OpenAPI, and then provided using mocked version of the service, with documentation for usage by UI and other application developers. All before code has to be written, making microservices development much more agile, flexible, iterative, and cost effective.</p>

<p><strong>Mocking Of Each Microservice To Hammer Out Contract</strong><br />
Each OpenAPI can be used to generate a mock representation of the service using Postman, Stoplight.io, or other OpenAPI-driven mocking solution. There are a number of services, and tooling available that takes an OpenAPI, an generates a mock API, as well as the resulting data. Each service should have the ability to be deployed locally as a mock service by any stakeholder, published and shared with other team members as a mock service, and shared as a demonstration of what the service does, or will do. Mock representations of services will minimize builds, the writing of code, and refactoring to accommodate rapid changes during the API development process. Code shouldn’t be generated or crafted until the surface area of an API has been worked out, and reflects the contract that each service will provide.</p>

<p><strong>OpenAPI Documentation Always AVailable In Repository</strong><br />
Each microservice should be self-contained, and always documented. Swagger UI, Redoc, and other API documentation generated from OpenAPI has changed how we deliver API documentation. OpenAPI generated documentation should be available by default within each service’s repository, linked from the README, and readily available for running using static website solutions like Github Pages, or available running locally through the localhost. API documentation isn’t just for the microservices owner / steward to use, it is meant for other stakeholders, and potential consumers. API documentation for a service should be always on, always available, and not something that needs to be generated, built, or deployed. API documentation is a default tool that should be present for EVERY microservice, and treated as a first class citizen as part of its evolution.</p>

<p><strong>Bringing An API To Life Using It’s OpenAPI Contract</strong><br />
Once an OpenAPI contract has been been defined, designed, and iterated upon by service owner / steward, as well as a handful of potential consumers and clients, it is ready for development. A finished (enough) OpenAPI can be used to generate server side code using a popular language framework, build out as part of an API gateway solution, or common proxy services and tooling. In some cases the resulting build will be a finished API ready for use, but most of the time it will take some further connecting, refinement, and polishing before it is a production ready API. Regardless, there is no reason for an API to be developed, generated, or built, until the OpenAPI contract is ready, providing the required business value each microservice is being designed to deliver. Writing code, when an API will change is an inefficient use of time, in a virtualized API design lifecycle.</p>

<p><strong>OpenAPI-Driven Monitoring, Testing, and Performance</strong><br />
A read-to-go OpenAPI contract can be used to generate API tests, monitors, and deliver performance tests to ensure that services are meeting their business service level agreements. The details of the OpenAPI contract become the assertions of each test, which can be executed against an API on a regular basis to measure not just the overall availability of an API, but whether or not it is actually meeting specific, granular business use cases articulated within the OpenAPI contract. Every detail of the OpenAPI becomes the contract for ensuring each microservice is doing what has been promised, and something that can be articulated and shared with humans via documentation, as well as programmatically by other systems, services, and tooling employed to monitor and test accordingly to a wider strategy.</p>

<p><strong>Empowering Security To Be Directed By The OpenAPI Contract</strong><br />
An OpenAPI provides the entire details of the surface area of an API. In addition to being used to generate tests, monitors, and performance checks, it can be used to inform security scanning, fuzzing, and other vital security practices. There are a growing number of services and tooling emerging that allow for building models, policies, and executing security audits based upon OpenAPI contracts. Taking the paths, parameters, definitions, security, and authentication and using them as actionable details for ensuring security across not just an individual service, but potentially hundreds, or thousands of services being developed across many different teams. OpenAPI quickly is becoming not just the technical and business contract, but also the political contract for how you do business on web in a secure way.</p>

<p><strong>OpenAPI Provides API Discovery By Default</strong><br />
An OpenAPI describes the entire surface area for the request and response of each API, providing 100% coverage for all interfaces a services will possess. While this OpenAPI definition will be generated mocks, code, documentation, testing, monitoring, security, and serving other stops along the lifecycle, it provides much needed discovery across groups, and by consumers. Anytime a new application is being developed, teams can search across the team Github, Gitlab, Bitbucket, or Team Foundation Server (TFS), and see what services already exist before they begin planning any new services. Service catalogs, directories, search engines, and other discovery mechanisms can use OpenAPIs across services to index, and make them available to other systems, applications, and most importantly to other humans who are looking for services that will help them solve problems.</p>

<p><strong>OpenAPI Deliver The Integration Contract For Client</strong><br />
OpenAPI definitions can be imported in Postman, Stoplight, and other API design, development, and client tooling, allowing for quick setup of environment, and collaborating with integration across teams. OpenAPIs are also used to generate SDKs, and deploy them using existing continuous integration (CI) pipelines by companies like <a href="https://apimatic.io/">APIMATIC</a>. OpenAPIs deliver the client contract we need to just learn about an API, get to work developing a new web or mobile application, or manage updates and version changes as part of our existing CI pipelines. OpenAPIs deliver the integration contract needed for all levels of clients, helping teams go from discovery to integration with as little friction as possible. Without this contract in place, on-boarding with one service is time consuming, and doing it across tens, or hundreds of services becomes impossible.</p>

<p><strong>OpenAPI Delivers Governance At Scale Across Teams</strong><br />
Delivering consistent APIs within a single team takes discipline. Delivering consistent APIs across many teams takes governance. OpenAPI provides the building blocks to ensure APIs are defined, designed, mocked, deployed, documented, tested, monitored, perform, secured, discovered, and integrated with consistently. The OpenAPI contract is an artifact that governs every stop along the lifecycle, and then at scale becomes the measure for how well each service is delivering at scale across not just tens, but hundreds, or thousands of services, spread across many groups. Without the OpenAPI contract API governance is non-existent, and at best extremely cumbersome. The OpenAPI contract is not just top down governance telling what they should be doing, it is the bottom up contract for service owners / stewards who are delivering the quality services on the ground inform governance, and leading efforts across many teams.</p>

<p>I can’t articulate the importance of the OpenAPI contract to each microservice, as well as the overall organizational and project microservice strategy. I know that many folks will dismiss the role that OpenAPI plays, but look at <a href="https://www.openapis.org/membership/members">the list of members who govern the specification</a>. Consider that Amazon, Google, and Azure ALL have baked OpenAPI into their microservice delivery services and tooling. OpenAPI isn’t a WSDL. An OpenAPI contract is how you will articulate what your microservice will do from inception to deprecation. Make it a priority, don’t treat it as just an output from your legacy way of producing code. Roll up your sleeves, and spend time editing it by hand, and loading it into 3rd party services to see the contract for your microservice in different ways, through different lenses. Eventually you will begin to see it is much more than just another JSON artifact laying around in your repository.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/03/openapi-is-the-contract-for-your-microservice/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/03/github-is-you-asynchronous-microservice-showroom/">Github Is Your Asynchronous Microservice Showroom</a></h3>
        <span class="post-date">03 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-github.jpg" width="30%" align="right" style="padding: 15px;" /></p>
<p>When you spend time looking at a lot of microservices, across many different organizations, you really begin to get a feel for the ones who have owners / stewards that are thinking about the bigger picture. When people are just focused on what the service does, and not actually how the service will be used, the Github repos tend to be cryptic, out of sync, and don’t really tell a story about what is happening. Github is often just seen as a vehicle for the code to participate in a pipeline, and not about speaking to the rest of the humans and systems involved in the overall microservices concert that is occurring.</p>

<p><strong>Github Is Your Showroom</strong><br />
Each microservices is self-contained within a Github repository, making it the showcase for the service. Remember, the service isn’t just the code, and other artifacts buried away in the folders for nobody to understand, unless you understand how to operate the service or continuously deploy the code. It is a service. The service is part of a larger suite of services, and is meant to be understood and reusable by other human beings in the future, potentially long after you are gone, and aren’t present to give a 15 minute presentation in a meeting. Github is your asynchronous microservices showroom, where ANYONE should be able to land, and understand what is happening.</p>

<p><strong>README Is Your Menu</strong><br />
The README is the centerpiece of your showroom. ANYONE should be able to land on the README for your service, and immediately get up to speed on what a service does, and where it is in its lifecycle. README should not be written for other developers, it should be written for other humans. It should have a title, and short, concise, plain language description of what a service does, as well as any other relevant details about what a service delivers. The README for each service should be a snapshot of a service at any point in time, demonstrating what it delivers currently, what the road map contains, and the entire history of a service throughout its lifecycle. Every artifact, documentation, and relevant element of a service should be available, and linked to via the README for a service.</p>

<p><strong>An OpenAPI Contact</strong><br />
Your OpenAPI (fka Swagger) file is the central contract for your service, and the latest version(s) should be linked to prominently from the README for your service. This JSON or YAML definition isn’t just some output as exhaust from your code, it is the contract that defines the inputs and outputs of your service. This contract will be used to generate code, mocks, sample data, tests, documentation, and drive API governance and orchestration across not just your individual service, but potentially hundreds or thousands of services. An update to date, static representation of your services API contract should always be prominently featured off the README for a service, and ideally located in a consistent folder across services, as services architects, designers, coaches, and governance people will potentially be looking at many different OpenAPI definitions at any given moment.</p>

<p><strong>Issues Dirven Conversation</strong><br />
Github Issues aren’t ust for issues. It is a robust, taggable, conversational thread around each individual service. A github issues should be the self-contained conversation that occurs throughout the lifecycle of a service, providing details of what has happened, what the current issues are, and what the future road map will hold. Github Isuses are designed to help organize a robust amount of conversational threads, and allow for exploration and organization using tags, allowing any human to get up to speed quickly on what is going on. Github issues should be an active journal for the work being done on a service, through a monologue by the service owner / steward, and act as the feedback loop with ALL OTHER stakeholders around a service. Github issues for each service will be be how the antrhopolgists decipher the work you did long after you are gone, and should articulate the entire life of each individual service.</p>

<p><strong>Services Working In Concert</strong><br />
Each microservice is a self-contained unit of value in a larger orchestra. Each microservice should do one thing and do it well. The state of each services Github repository, README, OpenAPI contract, and the feedback loop around it will impact the overall production. While a service may be delivered to meet a specific application need in its early moments, the README, OpenAPI contract, and feedback loop should attract and speak to any potential future application. A service should be able to reused, and remixed by any application developer building internal, partner, or some day public applications. Not everyone landing on your README will have been in the meetings where you presented your service. Github is your service’s showroom, and where you will be making your first, and ongoing impression on other developers, as well as executives who are poking around.</p>

<p><strong>Leading Through API Governance</strong><br />
Your Github repo, README, and OpenAPI contract is being used by overall microservices governance operations to understand how you are designing your services, crafting your schema, and delivering your service. Without an OpenAPI, and README, your service does nothing in the context of API governance, and feeding into the bigger picture, and helping define overall governance. Governance isn’t scripture coming off the mountain and telling you how to operate, it is gathered, extracted, and organized from existing best practices, and leadership across teams. Sure, we bring in outside leadership to help round off the governance guidance, but without a README, OpenAPI, and active feedback loop around each service, your service isn’t participating in the governance lifecycle. It is just an island, doing one thing, and nobody will ever know if it is doing it well.</p>

<p><strong>Making Your Github Presence Count</strong><br />
Hopefully this post helps you see your own microservice Github repository through an external lens. Hopefully it will help you shift from Github being just about code, for coders, to something that is part of a larger conversation. If you care about doing microservices well, and you care about the role your service will play in the larger application production, you should be investing in your Github repository being the showroom for your service. Remember, this is a service. Not in a technical sense, but in a business sense. Think about what good service is to you. Think about the services you use as a human being each day. How you like being treated, and how informed you like to be about the service details, pricing, and benefits. Now, visit the Github repositories for your services, think about the people who who will be consuming them in their applications. Does it meet your expectations for a quality level of service? Will someone brand new land on your repo and understand what your service does? Does your microservice do one thing, and does it do it well?</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/03/github-is-you-asynchronous-microservice-showroom/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/02/openapi-dependencies-vendor-extension/">An OpenAPI Service Dependency Vendor Extension</a></h3>
        <span class="post-date">02 Apr 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/71_113_800_500_0_max_0_-1_-1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m working on a healthcare related microservice project, and I’m looking for a way to help developers express their service dependencies within the OpenAPI or some other artifact. At this point I’m feeling like the OpenAPI is the place to articulate this, adding a vendor extension to the specification that can allow for the referencing of one or more other services any particular service is dependent on. Helping make service discovery more machine readable at discovery and runtime.</p>

<p>To help not reinvent the wheel, I am looking at <a href="https://apievangelist.com/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/">using the Schema.org Web API type including the extensions put forth by Mike Ralphson and team.</a> I’d like the x-api-dependencies collection to adopt a standardized schema, that was flexible enough to reference different types of other services. I’d like to see the following elements be present for each dependency:</p>

<ul>
  <li>versions (OPTIONAL array of thing -&gt; Property -&gt; softwareVersion). It is RECOMMENDED that APIs be versioned using [semver]</li>
  <li>entryPoints (OPTIONAL array of Thing -&gt; Intangible -&gt; EntryPoint)</li>
  <li>license (OPTIONAL, CreativeWork or URL) - the license for the design/signature of the API</li>
  <li>transport (enumerated Text: HTTP, HTTPS, SMTP, MQTT, WS, WSS etc)&lt;/p&gt;</li>
  <li>apiProtocol (OPTIONAL, enumerated Text: SOAP, GraphQL, gRPC, Hydra, JSON API, XML-RPC, JSON-RPC etc)</li>
  <li>webApiDefinitions (OPTIONAL array of EntryPoints) containing links to machine-readable API definitions</li>
  <li>webApiActions (OPTIONAL array of potential Actions)</li>
</ul>

<p>Using the Schema.org Web type would allow for a pretty robust way to reference dependencies between services in a machine readable way, that can be indexed, and even visualized in services and tooling. When it comes to evolving and moving forward services, having dependency details baked in by default make a lot of sense, and ideally each dependency definition would have all the details of the dependency, as well as potential contact information, to make sure everyone is connected regarding the service road map. Anytime a service is being deprecated, versioned, or impacted in any way, we have all the dependencies needed to make an educated decision regarding how to progress with the least amount of friction as possible.</p>

<p>I’m going to go ahead and create a draft OpenAPI vendor extension specification for x-service-dependencies, and use <a href="https://pending.schema.org/WebAPI">the Schema.org WebAPI type</a>, complete with <a href="https://webapi-discovery.github.io/rfcs/rfc0001.html#content-types">the added extensions</a>. Once I start using it, and have successfully implemented it for a handful of services I will publish and share a working example. I’m also on the hunt for other examples of how teams are doing this. I’m not looking for code dependency management solutions, I am specifically looking for API dependency management solutions, and how teams are making these dependencies discoverable in a machine readable way. If you know of any interesting approaches, please let me know, I’d like to hear more about it.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/02/openapi-dependencies-vendor-extension/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/02/the-api-stack-checklist/">The API Stack Profiling Checklist</a></h3>
        <span class="post-date">02 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-stacks.png" width="45%" align="right" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2018/04/02/my-api-stack-profiling-process/">I just finished a narrative around my API Stack profiling</a>, telling the entire story around the profiling of APIs for inclusion in the stack. To help encourage folks to get involved, I wanted to help distill down the process into a single checklist that could be implemented by anyone.</p>

<p><strong>The Github Base</strong>
Everything begins as a Github repository, and it can existing in any user or organization. Once ready, I can fork and publish as part of the API stack, or sync with an existing repository project.</p>

<ul>
  <li><strong>Create Repo</strong> - Create a single repository with the name of the API provider in plain language.</li>
  <li><strong>Create README</strong> - Add a README for the project, articulating who the target is and the author.</li>
</ul>

<p><strong>OpenAPI Definition</strong>
Profiling the API surface area using OpenAPI, providing a definition of the request and response structure for all APIs. <a href="https://github.com/OAI/OpenAPI-Specification">Head over to their repository if you need to learn more about OpenAPI</a>. Ideally, there is an existing OpenAPI you can start with, or other machine readable definition you can use as base–look around within their developer portal, because sometimes you can find an existing definition to start with. Next look on Github, as you never know where there might be something existing that will save you time an energy. However you approach, I’m looking for complete details on the following:</p>

<ul>
  <li><strong>info</strong> - Provide as much information about the API.</li>
  <li><strong>host</strong> - Provide a host, or variables describing host.</li>
  <li><strong>basePath</strong> - Document the basePath for the API.</li>
  <li><strong>schemes</strong> - Provide any schemes that the API uses.</li>
  <li><strong>produces</strong> - Document which media types the API uses.</li>
  <li><strong>paths</strong> - Detail the paths including methods, parameters, enums, responses, and tags.</li>
  <li><strong>definitions</strong> - Provide schema definitions used in all requests and responses.</li>
</ul>

<p>To help accomplish this, I often will scrape, and use any existing artifacts I can possible find. Then you just have to roll up your sleeves and begin copying and pasting from the existing API documentation, until you have a complete definition. There is never any definitive way to make sure you’ve profiled the entire API, but do your best to profile what is available, including all the detail the provider as shared. There will always be more that we can do later, as the API gets used more, and integrated by more providers and consumers.</p>

<p><strong>Postman Collection</strong>
Once you have an OpenAPI definition available for the API, import it into Postman. Make sure you have a key, and the relevant authentication environment settings you need. Then begin making API calls to each individual API path, making sure your API definition is as complete as it possibly can. This can be the quickest, or the most time consuming of the profiling, depending on the complexity and design of the API. The goal is to certify each API path, and make sure it truly reflects what has been documented. Once you are done, export a Postman Collection for the API, complimenting the existing OpenAPI, but with a more run-time ready API definiton.</p>

<p><strong>Merging the Two Definitions</strong>
Depending on how many changes occurred within the Postman portion of the profiling you will have to sync things up with the OpenAPI. Sometimes it is a matter of making minor adjustments, sometimes you are better off generating an entirely new OpenAPI from the Postman Collection using <a href="https://apimatic.io/transformer">APIMATIC’s API Transformer</a>. The goal is to make sure the OpenAPI and Postman are in sync, and working the same way as expected. Once they are in sync they can uploaded to the Github repository for the project.</p>

<p><strong>Managing the Unkown Unknowns</strong>
There will be a lot of unknowns along the way. A lot of compromises, and shortcuts that can be taken. Not every definition will be perfect, and sometimes it will require making multiple definitions because of the way an API provider has designed their API and used multiple subdomains. Document it all as Github issues in the repository. Use the Github issues for each API as the journal for what happened, and where you document any open questions, or unfinished dwork. Making the repository the central truth for the API definition, as well as the conversation around the profiling process.</p>

<p><strong>Providing Central APIs.json Index</strong>
The final step of the process is to create an APIs.json index for the API. <a href="http://apisjson.org/format.html">You can find a sample one over at the specification website</a>. When I profile an API using APIs.json I am always looking for as much detail as I possibly can, but for the purposes of API Stack profiling, I’m looking for these essential building blocks:</p>

<ul>
  <li><strong>Website</strong> - The primary website for an entity owning the API.</li>
  <li><strong>Portal</strong> - The URL to the developer portal for an API.</li>
  <li><strong>Documentation</strong> - The direct link to the API documentation.</li>
  <li><strong>OpenAPI</strong> - The link to the OpenAPI I created on Github.</li>
  <li><strong>Postman</strong> - The link to the Postman Collection I created on Github.</li>
  <li><strong>Sign Up</strong> - Where do you sign up for an API.</li>
  <li><strong>Pricing</strong> - A link to the plans, tiers, and pricing for an API.</li>
  <li><strong>Terms of Service</strong> - A URL to the terms of service for an API.</li>
  <li><strong>Twitter</strong> - The Twitter account for the API provider – ideally, API specific.</li>
  <li><strong>Github</strong> - The Github account or organization for the API provider.</li>
</ul>

<p>If you create multiple OpenAPIs, and Postman Collections, you can add an entry for each API. If you break a larger API provider into several entity provider repositories, you can link them together using the include property of the APIs.json file. I know the name of the specification is JSON, but feel free to do them in YAML if you feel more comfortable–I do. ;-) The goal of the APIs.json is to provide a complete profile of the API operations, going beyond what OpenAPI and Postman Collections deliver.</p>

<p><strong>Including In The API Stack</strong>
You should keep all your work in your own Github organization or user account. Once you’ve created a repository you would like to include in the API Stack, and syndicate the work to the Streamdata.io API Gallery, APIs.io, APIs.guru, Postman Network, and others, then <a href="https://github.com/api-stack/api-stack/issues">just submit as a Github issue on the main repository</a>. I’m still working on the details of how to keep repositories in sync with contributors, then reward and recognize them for work their work, but for now I’m relying on Github to track all contributions, and we’ll figure this part out later. The API Stack is just the workbench for all of this, and I’m using it as a place to merge the work of many partners, from many sources, and then work to sensibly syndicate out validated API profiles to all the partner galleries, directories, and search engines.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/02/the-api-stack-checklist/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/04/02/my-api-stack-profiling-process/">My API Stack Profiling Process</a></h3>
        <span class="post-date">02 Apr 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-stack/logos/the-api-stack-entities-snapshot.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I am escalating conversations I’m having with folks regarding how I profile and understand APIs as part of my <a href="http://theapistack.com">API Stack</a> work. It is something I’ve been evolving for 2-3 years, as I struggle with different ways to attack and provide solutions within the area of API discovery. I want to understand what APIs exist out there as part of my industry research, but along the way I want to also help others find the APIs they are looking for, while also helping API providers get their APIs found in the first place. All of this has lead me to establish a pretty robust process for profiling API providers, and document what they are doing.</p>

<p>As I shift my <a href="http://theapistack.com">API Stack</a> work into 2nd gear, I need to further formalize this process, articulate to partners, and execute at scale. Here is my current snapshot of what is happening whenever I’m profiling an API and adding it to my API Stack organization, and hopefully moving it forward along this API discovery pipeline in a meaningful way. It is something I’m continuing to set into motion all by myself, but I am also looking to bring in other API service providers, API providers, and API consumers to help me realize this massive vision.</p>

<p><strong>Github as a Base</strong><br />
Every new “entity” I am profiling starts with a Github repository and a README, within <a href="https://github.com/api-stack-providers">a single API Stack organization for providers</a>. Right now this is all automated, as I’m transition to this way of doing things at scale. Once in motion, each provider I’m profiling will become pretty manual. What I consider an “entity” is fluid. Most smaller API providers have a single repository, while other larger ones are broken down by line of business, to help reduce API definitions and conversation around APIs down to the smallest possible unit we can. Every API service provider I’m profiling gets its own Github repository, subdomain, set of definitions, and conversation around what is happening. Here are the base elements of what is contained within each Github repo.</p>

<ul>
  <li><strong>OpenAPI</strong> - A JSON or YAML definition for the surface are of an API, including the request, responses, and underlying schema using as part of both.</li>
  <li><strong>Postman Collection</strong> - A JSON Postman Collection that has been exported from the Postman client after importing the complete OpenAPI definition, and full tested.</li>
  <li><strong>APIs.json</strong> - A YAML definition of the surface area of an API’s operation, including home page, documentation, code samples, sign up and login, terms of service, OpenAPI, Postman Collection, and other relevant details that will matter to API consumers.</li>
</ul>

<p>The objective is to establish a single repository that can be continuously updated, and integrated into other API discovery pipelines. The goal is to establish a complete (enough) index of the APIs using OpenAPI, make it available in a more runtime fashion using Postman, and then indexing the entire API operations, including the two machine readable definitions of the API(s) using APIs.json. Providing a single URL in which you can ingest and use to integrate with an API, and make sense of API resources at scale.</p>

<p><strong>Everything Is OpenAPI Driven</strong><br />
My definition still involves OpenAPI 2.0, but it is something I’m looking to shift towards 3.0 immediately. Each entity will contain one or many OpenAPI definitions location in the underscore openapi folder. I’m still thinking about the naming and versioning convention I’m using, but I’m hoping to work this out over time. Each OpenAPI should contain complete details for the following areas:</p>

<ul>
  <li><strong>info</strong> - Provide as much information about the API.</li>
  <li><strong>host</strong> - Provide a host, or variables describing host.</li>
  <li><strong>basePath</strong> - Document the basePath for the API.</li>
  <li><strong>schemes</strong> - Provide any schemes that the API uses.</li>
  <li><strong>produces</strong> - Document which media types the API uses.</li>
  <li><strong>paths</strong> - Detail the paths including methods, parameters, enums, responses, and tags.</li>
  <li><strong>definitions</strong> - Provide schema definitions used in all requests and responses.</li>
</ul>

<p>While it is tough to define what is complete when crafting an OpenAPI definition, every path should be represented, with as much detail about the surface area of requests and responses. It is the detail that really matters, ensuring all parameters and enums are present, and paths are properly tagged based upon the resources being made available. The more OpenAPI definitions you create, the more you get a feel for what is a “complete” OpenAPI definition. Something that becomes real once you create a Postman Collection for the API definition, and begin to actually try and use an API.</p>

<p><strong>Validating With A Postman Collection</strong><br />
Once you have a complete OpenAPI definition defined, the next step is to validate your work in Postman. You simply import the OpenAPI into the client, and get to work validating that each API path actually works. You make sure you have an API key, and any other authentication requirements, and work your way through each path, validating that the OpenAPI is indeed complete. If I’m missing sample responses, the Postman portion of this API discovery journey is where I get them. Grabbing validated responses, anonymizing them, and pasting them back into the OpenAPI definition. Once I’ve worked my way through every API path, and verified it works, I then export a final Postman Collection for the API I’m profiling.</p>

<p><strong>Indexing API Operations With APIs.json</strong><br />
I upload my OpenAPI, and Postman Collection to the Github repository for the API entity I am targeting. Then I get to work on an APIs.json for the API. This portion of the process is about documenting the OpenAPI and Postman I just created, but then also doing the hard work of identifying everything else an API provider offers to support their operations, and will be needed to get up and running with API integration. Here are a handful of the essential building blocks I’m documenting:</p>

<ul>
  <li><strong>Website</strong> - The primary website for an entity owning the API.</li>
  <li><strong>Portal</strong> - The URL to the developer portal for an API.</li>
  <li><strong>Documentation</strong> - The direct link to the API documentation.</li>
  <li><strong>OpenAPI</strong> - The link to the OpenAPI I created on Github.</li>
  <li><strong>Postman</strong> - The link to the Postman Collection I created on Github.</li>
  <li><strong>Sign Up</strong> - Where do you sign up for an API.</li>
  <li><strong>Pricing</strong> - A link to the plans, tiers, and pricing for an API.</li>
  <li><strong>Terms of Service</strong> - A URL to the terms of service for an API.</li>
  <li><strong>Twitter</strong> - The Twitter account for the API provider – ideally, API specific.</li>
  <li><strong>Github</strong> - The Github account or organization for the API provider.</li>
</ul>

<p>This list represents the essential things I’m looking for. I have almost 200 other building blocks I document as part of API operations ranging from SDKs to training videos. The presence of these building blocks tell me a lot about the motivations behind an API platform, and send off a wealth of signals that can be tuned into by humans, or programmatically to understand the viability and health of an API.</p>

<p><strong>Continuously Integratable API Definition</strong><br />
Now we should have the entire surface area of an API, and it’s operations defined in a machine readable format. The APIs.json, OpenAPI, and Postman can be used to on-board new developers, and used by API service providers looking to provides valuable services to the space. Ideally, each API provider is involved in this process, and once I have a repository setup, I will be reaching out to each provider as part of the API profiling process. Even when API providers are involved, it doesn’t always mean you get a better quality API definition. In my experience API providers often don’t care about the detail as much as some consumers and API service providers will. In the end, I think this is all going to be a community effort, relying on many actors to be involved along the way.</p>

<p><strong>Self-Contained Conversations About Each API</strong><br />
With each entity being profiled living in its own Github repository, it opens up the opportunity to keep API conversation localized to the repository. I’m working to establish any work in progress, and conversations around each APIs definitions within the Github issues for the repository. There will be ad-hoc conversations and issues submitted, but I’ll be also adding path specific threads that are meant to be an ongoing thread dedicated to profiling each path. I’ll be incorporating these threads into tooling and other services I’m working with, encouraging a centralized, but potentially also distributed conversation around each unique API path, and its evolution. Keeping everything discoverable as part of an API’s definition, and publicly available so anyone can join in.</p>

<p><strong>Making Profiling A Community Effort</strong><br />
I will repeat this process over and over for each entity I’m profiling. Each entity lives in its own Github repository, with the definition and conversation localized. Ideally API providers are joining in, as well as API service providers, and motivated API consumers. I have a lot of energy to be profiling APIs, and can cover some serious ground when I’m in the right mode, but ultimately there is more work here than any single person could ever accomplish. I’m looking to standardize the process, and distribute the crowdsourcing of this across many different Github repositories, and involve many different API service providers I’m working with to ensure the most meaningful APIs are being profiled, and maintained. Highlighting another important aspect of all of this, and the challenges to keep API definitions evolving, soliciting many contributions, while keeping them certified, rated, and available for production usage.</p>

<p><strong>Ranking APIs Using Their Definitions</strong><br />
Next, I’m ranking APIs based upon a number of criteria. <a href="http://apievangelist.com/2015/10/31/how-are-we-going-to-create-the-standard-and-poors-and-moodys-for-the-api-economy/">It is a ranking system I’ve slowly developed over the last five years</a>, and is something I’m looking to continue to refine as part of this API Stack profiling work. Using the API definitions, I’m looking to rate each API based upon a number of elements that exist within three main buckets:</p>

<ul>
  <li><strong>API Provider</strong> - Elements within the control of the API provider like the design of their API, sharing a road map, providing a status dashboard, being active on Twitter and Github, and other signals you see from healthy API providers.</li>
  <li><strong>API Community</strong> - Elements within the control of the API community, and not always controlled by the API provider themselves. Things like forum activity, Twitter responses, retweets, and sentiment, and number of stars, forks, and overall Github activity.</li>
  <li><strong>API Analyst</strong> - The last area is getting the involvement of API analysts, and pundits, and having them provide feedback on APIs, and get involved in the process. Tracking on which APIs are being talked about by analysts, and storytellers in the space.</li>
</ul>

<p>Most of this ranking I can automate through there being a complete APIs.json and OpenAPI definition, and machine readable sources of information present in those indexes. Signals I can tap into using blog RSS, Twitter API, Github API, Stack Exchange API, and others. However some it will be manual, as well as require the involvement of API service providers. Helping me add to my own ranking information, but also bringing their own ranking systems, and algorithms to the table. In the end, I’m hoping the activity around each APIs Github repository, also becomes its own signal, and consumers in the community can tune into, weight, or exclude based upon the signals that matter most to them.</p>

<p><strong>Streamdata.io Stream Rank</strong><br />
Adding to my ranking system, I am applying my partner <a href="https://streamdata.io/blog/benchmark-quantifying-api-performance/">Streamdata.io’s ranking system for APIs</a>, looking to determine that activity surrounding an API, and how much opportunity exists to turn an API event a real time stream, as well as identify the meaningful events that are occurring. Using the OpenAPI definitions generated for each API being profiled, I begin profiling for Stream Rank by defining the following:</p>

<ul>
  <li><strong>Download Size</strong> - What is the API response download size when I poll an API?</li>
  <li><strong>Download Time</strong> - What is the API response download time when I poll an API?</li>
  <li><strong>Did It Change</strong> - Did the response change from the previous response when I poll an API?</li>
</ul>

<p>I poll an API for at least 24-48 hours, and up to a week or two if possible. I’m looking to understand how large API responses are, and how often they change–taking notice of daily or weekly trends in activity along the way. The result is a set of scores that help me identify:</p>

<ul>
  <li><strong>Bandwidth Savings</strong> - How much bandwidth can be saved?</li>
  <li><strong>Processor Savings</strong> - How much processor time can be saved?</li>
  <li><strong>Real Time Savings</strong> - How real time an API resource is, which amplifies the savings?</li>
</ul>

<p>All of which go into what we are calling a Stream Rank, which ultimately helps us understand the real time opportunity around an API. When you have data on how often an API changes, and then you combine this with the OpenAPI definition, you have a pretty good map for how valuable a set of resources is based upon the ranking of the overall API provider, but also the value that lies within each endpoints definition. Providing me with another pretty compelling dimension to the overall ranking for each of the APIs I’m profiling.</p>

<p><strong>APIMetrics CASC Score</strong><br />
Next I’m partnering with <a href="https://apimetrics.io/2017/04/21/casc-score/">APIMetrics to bring in their CASC Scoring</a> when it makes sense. They have been monitoring many of the top API providers, and applying their CASC score to the overall API availability. I’d like to pay them to do this for ALL APIs in my API Stack work. I’d like to have every public API actively monitored from many different regions, and posses active, as well as historical data that can be used to help rank each API provider. Like the other work here, this type of monitoring isn’t easy, or cheap, so it will take some prioritization, and investment to properly bring in APIMetrics, as well as other API service providers to help me rank things.</p>

<p><strong>Event-Driven Opportunity With AsyncAPI</strong><br />
Something I’ve only just begun doing, but I wanted to include on this list, is adding a fourth definition to the stack when it makes sense. If any of these APIs possess a real time, event-driven, or message oriented API, I want to make sure it is profiled using <a href="https://www.asyncapi.com/">the AsyncAPI specification</a>. The specification is just getting going, but I’m eager to help push forward the conversation as part of my API Stack work. The format is a great sister specification for OpenAPI, and something I feel is relevant for any API that I profile with a high enough Stream Rank. Once an API possesses a certain level of activity, and has a certain level of richness regarding parameters and the request surface area, the makings for an event-driven API are there–the provider just hasn’t realized it. The map for the event-driven exists within the OpenAPI parameter enums, and tagging for each API path, all you need is the required webhooks or streaming layer to realize an existing APIs event-driven potential–this is where <a href="http://streamdata.io">Streamdata.io</a> comes in. ;-)</p>

<p><strong>One Stack, Many Discovery Opportunities</strong><br />
I am doing this work under my <a href="http://theapistack.com">API Stack</a> research. However it is all being funded by <a href="http://streamdata.io">Streamdata.io</a>. It is also being stimulated by my work with Postman, the OpenAPI Initiative, APIs.io, APIs.json, API Deck, APIs.guru, and beyond. The goal is to turn API Stack into a workbench, where anyone can contribute, as well as fork and integrate API definitions into their existing pipelines. All of this will take years, and a considerable amount of investment, but it is something I’ve already invested several years into. I’m also seeing the API discovery conversation reaching a critical mass thanks to the growth in the number of APIs, as well as adoption of OpenAPI and Postman. Making API discovery a more urgent problem. The challenge will be that most people want to extract value from these kind of effort, and not give back. While I feel strongly that API definitions should be open and freely accessible, the nature of the industry doesn’t alway reflect my vision, so I’ll have to get creative about how I incentivize folks, and reward or punish certain behaviors along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/04/02/my-api-stack-profiling-process/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/30/the-openapi-30.gui/">The OpenAPI 3.0 GUI</a></h3>
        <span class="post-date">30 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/openapi-gui/openapi-gui-dashboard.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I have been editing my OpenAPI definitions manually for some time now, as I just haven’t found a GUI editor that works well with my workflow. Swagger editor really isn’t a GUI solution, and while I enjoy services like Stoplight.io, APIMATIC, and others, I’m very picky about what I adopt within my world. One aspect of this is that I’ve been holding out for a solution that I can run 100% on Github using Github Pages. I’ve delusionally thought that someday I would craft a slick JavaScript solution that I could run using only Github Pages, but in reality I’ve never had the time to pull together. So, I just kept manually editing my OpenAPI definitions on the desktop using Atom, and publish to Github as I needed.</p>

<p>Well, now I can stop being so dumb, because my friend Mike Ralphson (<a href="https://twitter.com/permittedsoc">@permittedsoc</a>) has created <a href="https://github.com/Mermade/openapi-gui">my new favorite OpenAPI GUI</a>, that is OpenAPI 3.0 compliant by default. As Mike defines it, “OpenAPI-GUI is a GUI for creating and editing OpenAPI version 3.0.x JSON/YAML definitions. In its current form it is most useful as a tool for starting off and editing simple OpenAPI definitions. Imported OpenAPI 2.0 definitions are automatically converted to v3.0.”</p>

<p>He provides a pretty simple way to get up and running with OpenAPI GUI because, “OpenAPI-GUI runs entirely client-side using a number of Javascript frameworks including Vue.JS, jQuery and Bulma for CSS. To get the app up and running just browse to <a href="https://mermade.github.io/openapi-gui/">the live version on GitHub pages</a>, deploy a clone to GitHub pages, deploy to Heroku using the button below, or clone the repo and point a browser at index.html or host it yourself - couldn’t be simpler.” – “You only need to npm install the Node.js modules if you wish to use the openapi-gui embedded web server (i.e. not if you are running your own web-server), otherwise they are only there for PaaS deployments.”</p>

<p><strong>Portable, Forkable API Design GUI</strong>
The forkability of OpenAPI GUI is what attracts me to it. The client-side JavaScript approach makes it portable and forkable, allowing it to run wherever it is needed. The ability to quickly publish using Github Pages, or other static environment, makes it attractive. I’d like to work on a one click way for deploying locally, so I could make available to remote microservices teams to use locally to manage their OpenAPI definitions. I think it is important to have local, as well as cloud-based API design GUI locations, with more investment in the cloud-base solutions being more shared, possessing a team component, as well as a governance layer. However, I really like the thought of each OpenAPI having its own default editor, and you edit the definition within the repo it is located. I could see two editions of OpenAPI GUI, the isolated edition, and a more distributed team and definition approach. IDK, just thinking out loud.</p>

<p><strong>Setting the OpenAPI 3.0 Stage</strong>
The next thing I love about OpenAPI GUI is that it is all about OpenAPI 3.0, which I just haven’t had the time to fully migrate to. It has the automatic conversion of OpenAPI definitions, and gives you all the GUI elements you need to manage the robust features present in 3.0 of the OpenAPI specification. I’m itching to get ALL of my API definitions migrated from 2.0 to 3.0. The modular and reusable nature of OpenAPI 3.0, enabled by OpenAPI GUI is a formula for a rich API design experience. There are a lot of features built into the expand and collapse GUI, which I could see becoming pretty useful once you get the hang of working in there. Adding, duplicating, copying, and reusing your way to a powerful, API-first way of getting things done. I like.</p>

<p><strong>Bringing In The Magic With Wizards</strong>
Ok, now I’m gonna go all road map on you Mike. You know you don’t have to listen to everything I ask for, but cherry pick the things that make sense. I see what you are doing in the wizard section. I like it. I want a wizard marketplace. Go all Harry Potter and shit. Create a plugin framework and schema so we can all create OpenAPI driven wizards that do amazing things, and users can plugin and remove as they desire. Require each plug to be its own repo, which can be connected and disconnected easily via the wizards tab. I’d even consider opening up the ability to link to wizards from other locations in the editor, and via the navigation. Then we are talking about paid wizards. You know, of the API validation varietal. Free validations, and then more advanced paid validations. ($$)</p>

<p><strong>Branding And Navigation Customization</strong>
I would love to be able name, organize, and add my own buttons. ;-) Let’s add a page that allows for managing the navigation bars, and even the icons that show within the page. I know it would be a lot of work, but a great thing to have in the road map. You could easily turn on the ability to add logo, change color palette, and some other basics that allows companies to easily brand it, and make it their space. Don’t get me wrong. It looks great now. It is clean, simple, uncluttered. I’ve just seen enough re-skinned Swagger UI and editors to know that people are going to want to mod the hell out of their GUI API design tooling. Oh yeah, what about drag and drop–I’d like to reorder by paths. ;-) Ok, I’ll shut up now.</p>

<p><strong>OpenAPI GUI In A Microservices World</strong>
As I said before, I like the idea of one editor == one OpenAPI. Something that can be isolated within the repo of a service. Don’t make things too complicate and busy. In my API design editor I want to be able to focus on the task at hand. I don’t want to be distracted by other services. I’d like this GUI to be the self-container editor accessible via the Jekyll, Github Pages layer for each of my microservices. Imagine a slightly altered API design experience for each service. I could have the workspace configurable and personalized for exactly what I need to deliver a specific service. I would have a base API design editor for jumpstarting a new service, but I’d like to be able to make the OpenAPI GUI fit each service like a glove, even making the About tab a README for the service, rather than for the editor–making it a microservice dashboard landing page for each one of my services.</p>

<p><strong>Making API Design a Shareable Team Sport</strong>
Ok, contradicting my anti-social, microservices view of things. I want the OpenAPI GUI to be a shareable team environment. Where each team member possesses their own GUI, and the world comes to them. I want a shared catalog of OpenAPI definitions. I want to be able to discover widely, and checkout or subscribe to within my OpenAPI GUI. I want all the reusable components to have a sharing layer, allowing me to share my components, work with the shared components of other developers, and work from a central repository of components. I want a discovery layer across the users, environments, components, OpenAPI definitions, and wizards. I want to be able to load my API design editor with all of the things I need to deliver consistent API design at scale. I want to be able to reuse the healthiest patterns in use across teams, and possibly lead by helping define, and refine what reusable resources are available for development, staging, and production environments.</p>

<p><strong>Github, Gitlab, or BitBucket As A Backend For The OpenAPI GUI</strong>
I want to run on Github, Gitlab, or BitBucket environments, as a static site–no backend. Everything backend should be an API, and added as part of the wizard plugin (aka magic) environment. I want the OpenAPI, components, and any other artifacts to live as JSON or YAML within a Github, Gitlab, or BitBucket repository. If it is an isolated deployment, everything is self-contained within a single repository. If it is a distributed deployment, then you can subscribe to shared environments, catalogs, components, and wizards via many organizations and repositories. This is the first modification I’m going to do to my version, instead of the save button saving to the vue.js store, I’m going to have it write to Github using the Github API. I’m going to add a Github OAuth icon, which will give me a token, and then just read / write to the underlying Github repository. I’m going to begin with an isolated version, but then expand to search, discovery, read and write to many different Github repositories across the organizations I operate across.</p>

<p><strong>Annotation And Discussion Throughout</strong>
While OpenAPI GUI is a full featured, and potentially busy environment, I’d like to be able to annotate, share, and discuss within the API design environment. I’d like to be able to create Github issues at the path level, and establish conversations about the design of an API. I’d like to be able to discuss paths, schema, tags, and security elements as individual threads. Providing a conversational layer within the GUI, but one that spans many different Github repositories, organizations, and users. Adding to the coupling with the underlying repository. Baking a conversational layer into the GUI, but also into the API design process in a way that provides a history of the design lifecycle of any particular service. Continuing to make API design a team sport, and allowing teams to work together, rather than in isolation.</p>

<p><strong>API Governance Across Distributed OpenAPI GUIs</strong>
With everything stored on Github, Gitlab, Bitbucket, and shareable across teams using organizations and repositories, I can envision some pretty interesting API governance possibilities. I can envision a distributed API catalog coming into focus. Indexing and aggregating all services, their definitions, and conversations aggregated across teams. You can see the seeds for this under the wizard tab with validation, visualization, and other extensions. I can see there being an API design governance framework present, where all API definitions have to meet certain number of validations before they can move forward. I can envision a whole suite of free and premium governance services present in a distributed OpenAPI GUI environment. With an underlying Github, BitBucket, and Gitlab base, and the OpenAPI GUI environment, you have the makings for some pretty sweet API governance possibilities.</p>

<p><strong>Lot’s Of Potential With OpenAPI GUI</strong>
I’ll pause there. I have a lot of other ideas for the API design editor. <a href="http://apievangelist.com/2014/06/25/if-i-could-design-my-perfect-api-design-editor/">I’ve been thinking about this stuff for a long time</a>. I’m stoked to adopt what you’ve built Mike. It’s got a lot of potential, and a nice clean start. I’ll begin to put to use immediately across my API Evangelist, API Stack, and other API consulting work. With several different deployments, I will have a lot more feedback. It’s an interesting leap forward in the API design tooling conversation, building on what is already going on with API service providers, and some of the other API design tooling like Apicurio, and publishing the API design conversation forward. I’m looking forward to baking OpenAPI GUI into operations a little more–it is lightweight, open source, and extensible enough to get me pretty excited about the possibilities.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/30/the-openapi-30.gui/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/30/obtaining-a-scalable-api-definition-driven-api-design-workflow-on-github-complete-with-api-governance/">Obtaining A Scalable API Definition Driven API Design Workflow On Github Complete With API Governance</a></h3>
        <span class="post-date">30 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/G0520270_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>There is a lot of talk of an API design first way of doing things when it comes to delivering microservices. I’m seeing a lot of organizations make significant strides towards truly decoupling how they deliver the APIs they depend on, and continue to streamline their API lifecycle, as well as governance. However, if you are down in the weeds, doing this work at scale, you know how truly hard it is to actualy get everything working in concert across many different teams. While I feel like I have many of the answers, actually achieving this reality, and assembling the right tools and services for the job is much harder then it is in theory.</p>

<p>There was <a href="https://groups.google.com/forum/#!topic/api-craft/H3tpTPJnoZQ">a question posted on API Craft recently that I think gets at the challenges we all face</a>:</p>

<blockquote>
  <p><em>We plan to use Open API 3 specification for designing API’s that are required to build our enterprise web application. These API’s are being developed to integrate the backend with frontend. They are initially planned to be internal/private. To roll out an API First strategy across multiple teams (~ 30) in our organization we want to recommend and centrally deploy a standard set of tools that could be used by teams to design and document API’s.</em><br /><br />
<em>I am new to the swagger tool set. I understand that there is a swagger-editor tool that can help in API design while swagger-ui could help in API documentation. Trying them I realized a few problems</em><br /><br />
<em>1. How would teams save their API’s centrally on a server? Swagger editor does not provide a way to centrally store them.</em><br />
<em>2. How can we get a directory look up that displays all the designed API’s?</em><br />
<em>3. How can we integrate the API design and API documentation tool?</em><br />
<em>4. How can the API specifications be linked with the implementation (java) to keep them up-to-date?</em><br />
<em>5. How do we show API dependencies when one api uses the other one?</em><br /><br />
<em>We basically need to think about the end-to-end work-flow that helps teams in their SDLC to design API’s. For the start I am trying to see what can we achieve with free tools. Can someone share their thoughts based on their experience?</em>
This reflects what I’m hearing from the numerous projects I’m consulting with. Some of the OpenAPI (Swagger) related questions can be addressed using Github, but the code integration, dependency, and other aspects of this question are much harder to answer, let alone solve with a single tool. Each team has their existing workflows, pipelines, and software delivery processes in place, so helping articulate how you should do this at scale, gets pretty difficult very quickly.</p>
</blockquote>

<p><strong>Github, Gitlab, or BitBucket At Core</strong>
The first place you want to start is with Github, Gitlab, or Bitbucket at the core. If you aren’t managing your services, and supporting API definitions as individual repositories within a version control system, achieving this at scale will be difficult. These software development platforms are where your APIs should begin, live and evolve, as well as experience their end of life. Making not just the code accessible throughout the lifecycle, but also the API definitions and other artifacts that are essential throughout the API lifecycle.</p>

<p><strong>Full API Lifecycle Service Providers</strong>
There are a handful of API service providers out there who will help you get pretty far down this road. Evolving towards being what I consider to be a full lifecycle API service provider. Not just helping you with design, or any other individual stop along the API lifecycle, and emerging to help you with as much of what you’ll need to get the job done. Here are just a handful of the leading ones out there that I track on:</p>

<ul>
  <li><a href="https://www.getpostman.com/"><strong>Postman</strong></a> - Focusing on the client, but bringing in a full stack of lifecycle development tools to support.</li>
  <li><a href="https://stoplight.io/"><strong>Stoplight</strong></a> - Beginning with API design, but then expanding quickly to help you manage and govern the lifecycle.</li>
  <li><a href="https://apimatic.io/"><strong>APIMATIC</strong></a> - Emphasis on quality SDK generation, but approach life cycle deployment as a pipeline.</li>
  <li><a href="https://swagger.io/"><strong>Swagger</strong></a> - Born out of the API definition, but quickly introducing other lifecycle solutions as part of the package.</li>
</ul>

<p>I can’t believe I’m talking about Swagger, but I’ll vent about that in another post. Anyways, these are what I’d consider to be the leading API lifecycle development solutions out there. They aren’t going to get you all the way towards everything listed above, but they will get you pretty far down the road. Another thing to note is that they ALL support OpenAPI definitions, which means you can actually use several of the providers, as long as your central truth is an OpenAPI available on a Github repository.</p>

<p>No single one of these services will address the questions posted above, but collectively they could. It shows how fragmented services in the space are, and how many different interpretations of the API lifecycle there are. However, it also shows the importance of an API definition-driven approach to the lifecycle, and how using OpenAPI and Postman Collections can help bridge our usage of multiple API service providers to get the job done. Next, I’ll work on a post outlining some of the open source tooling I’d suggest to help solve some of these problems, and then work to connect the dots more on how you could actually do this in practice on the ground within your organization.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/30/obtaining-a-scalable-api-definition-driven-api-design-workflow-on-github-complete-with-api-governance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/30/using-postman-to-explore-the-triathalon-api/">Using Postman to Explore the Triathlon API</a></h3>
        <span class="post-date">30 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/triathalon/w2WBMG2bTD6TahNs2MG3_150919-chicago-elite-m-web-msj-43.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m taking time to showcase any API I come across who have published their OpenAPI definitions to Github like <a href="https://apievangelist.com/2017/03/01/new-york-times-manages-their-openapi-using-github/">New York Times</a>, <a href="https://apievangelist.com/2017/02/14/boxs-seamless-approach-to-api-documentation/">Box</a>, <a href="https://apievangelist.com/2017/06/02/the-github-repo-stripe-users-to-manage-their-openapi/">Stripe</a>, <a href="https://apievangelist.com/2018/03/20/sendgrid-managing-their-openapi-using-github/">SendGrid</a>, <a href="https://apievangelist.com/2018/03/26/nexmo-manages-their-openapi-30-definition-using-github/">Nexmo</a>, and others have. I’m also taking the time to publish stories showcasing any API provider who similarly publish Postman Collections as part of their API documentation. Next up on my list is <a href="https://developers.triathlon.org/docs/using-postman-to-explore-the-triathlon-api">the Triathlon API</a>, who provides a pretty sophisticated API stack for managing triathlons around the world, complete with a list of Postman Collections for exploring and getting up and running with their API.</p>

<p><a href="http://apievangelist.com/2018/03/20/breaking-down-your-postman-api-collections-into-meaningful-units-of-compute/">Much like Okta, which I wrote about last week</a>, the <a href="https://developers.triathlon.org/docs/using-postman-to-explore-the-triathlon-api">Triathlon API has broken their Postman Collections into individual service collections</a>, and provides a nice list of them for easy access. Making it quick and easy to get up and running making calls to the API. Something that ALL API providers should be doing. Sorry, but Postman Collections should be a default part of your API documentation, just like OpenAPI definition should be as the driver of your interactive API docs, and the rest of your API lifecycle.</p>

<p>Every provider should be maintaining their OpenAPI definitions, as well as Postman Collections on Github, and baking them into their API documentation. Your OpenAPI should be the central truth for your API operations, and then you can easily import it, and generate Postman Collections as you design, test, and evolve your API using the Postman development suite. I know there are many API providers who haven’t caught up to this approach to delivering API resources, but it is something they need to tune into, and make the necessary shift in how you are delivering your resources.</p>

<p>In addition to regular stories like this on the blog, you will find me reaching out to individual API providers asking if they have an OpenAPI and / or Postman Collections. I’m personally invested in getting API providers to adopt their API definition formats. I want to see their APIs present in <a href="http://theapistack.com">my API Stack work</a>, as well as other API discovery projects I’m contributing to like Streamdata.io, Postman Network, APIs.Guru, and others. Making sure your APIs are discovered, and making sure you are getting out of the way of your developers by baking API definitions into your API operations–it is just what you do in 2018.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/30/using-postman-to-explore-the-triathalon-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/29/the-impact-of-availability-zones-regions-and-api-deployment-around-the-globe/">The Impact Of Availability Zones, Regions, And API Deployment Around The Globe</a></h3>
        <span class="post-date">29 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/azure/azure-regions-map.png" width="45%" align="right" style="padding: 15px;" /></p>
<p><a href="https://www.allthingsdistributed.com/2018/03/ten-years-of-aws-compartimentalization.html">Werner Vogels shared a great story looking back at 10 years of compartmentalization at AWS</a>, where he talks about the impact Amazon has made on the landscape by allowing for the deployment of resources into different cloud regions, zones, and jurisdictions. I agree with him regarding the significant impact this has had on how we deliver infrastructure, and honestly isn’t something that gets as much recognition and discussion as it should. I think this is partly due to the fact that many companies, organizations, institutions, and governments are still making their way to the cloud, and aren’t far enough in their journeys to be able to sufficiently take advantage of the different availability zones.</p>

<p>In Werner’s piece he focuses on the availability, scalability, and redundancy benefits of operating in different zones. Which I think gets at the technical benefits of this benefit of operating infrastructure in the cloud, but there are also significant business, and even political considerations at play here. As the web matures, the business and political implications of being able to operate precisely within a specific region, zone, and jurisdiction is becoming increasingly important. Sure, you want your API infrastructure to be reliable, redundant, and failover when there has been an outage in a specific region, but increasingly clients are asking for APIs to be delivered close to where business occurs, and regulatory bodies are beginning to mandate that digital business gets done within specific borders as well.</p>

<p>Regions have become a top level priority for <a href="https://aws.amazon.com/about-aws/global-infrastructure/">Amazon</a>, <a href="https://azure.microsoft.com/en-us/global-infrastructure/regions/">Azure</a>, and <a href="https://cloud.google.com/about/locations/">Google</a>. Clearly, they are also becoming a top level priority for their customers who operate within their clouds. It is one of those things I notice evolving across the technology landscape and have felt the need to pay attention to more as I see more activity and chatter. I’ve begun documenting which regions each of the cloud providers are operating in, and have been increasing the number of stories I’m writing about the potential for API providers, as well as API service providers. So it was good to see Werner reflecting on the significant role regions have played in the evolution of the cloud, and backing up what I’m already feeling and seeing across the sector.</p>

<p>While Werner focused on the technical benefits, I think the political, legal, and regulatory benefits will soon dwarf the technical ones. While the web has enjoyed a borderless existence for the last 25 years, I think we are going to start seeing things change in the next decade. Making cloud regions more about maintaining control over your countries digital assets, how you generate tax revenue, and defend the critical digital infrastructure of your nation from your enemies. The cloud providers who are empowering companies, organizations, institutions, and government agencies to securely, but flexibly operate in multiple regions are going to be in a good position. Similarly, the API providers, and service providers who behave in a similar way, delivering API resources in a multi-cloud way, are going to emerge as the strongest players in the API economy.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/29/the-impact-of-availability-zones-regions-and-api-deployment-around-the-globe/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/28/sharing-the-apis-json-vision-again/">Sharing The APIs.json Vision Again</a></h3>
        <span class="post-date">28 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/apis-json/apis-json-home-page-2018.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I know many folks in the API sector don’t know about <a href="http://apisjson.org/format.html">APIs.json</a>, and if they do they often think it is yet another API definition format, competing wit OpenAPI, Postman Collections, and others. So, I want to take a moment to share the vision again, and maybe convert one or two more folks to the possibilities around having a machine readable format for the entire operations. This is where APIs.json elevates the conversation, is it isn’t just about defining an API, it is about defining API operations, looking to make things more discoverable, and executable by default.</p>

<p><a href="http://apisjson.org/format.html">APIs.json</a> is a JSON format, as the name implies, but admittedly a bad name, as I’ve already started created APIs.yaml editions, which defines the entire API operations, not just any single API. It starts with the basics of the API providers, with: name, description, image, tags, created, and modified. Then it has a collection for defining one or many actual APIs. Repeating the need for a name, description, image, and tags, but this time for each API–admittedly this might be redundant in many cases. Each API also has a humanURL and baseURL for each API, providing two key links that developers will need. After that, we start getting to the meat of APIs.json, with the properties collection.</p>

<p>The properties collection is an array of URLs and types, which can point to many different building blocks of API operations. Common elements that should be present with every API provider like:</p>

<ul>
  <li><strong>Documentation</strong> - URL to the API documentation.</li>
  <li><strong>Plans &amp; Pricing</strong> - Landing page for the API tiers.</li>
  <li><strong>Terms of Service</strong> - Where do you find the legal department.</li>
  <li><strong>Sign Up</strong> - How do you sign up for an API.</li>
</ul>

<p>These elements are often times meant for humans, but the goal of APIs.json is to also provide machine readable URLs of common building blocks, including:</p>

<ul>
  <li><strong><a href="https://www.openapis.org/">OpenAPI</a></strong> - Where do you find the OpenAPI definition?</li>
  <li><strong><a href="https://www.getpostman.com/docs/v6/postman/collections/creating_collections">Postman Collection</a></strong> - Is there a Postman Collection available?</li>
  <li><strong><a href="http://apicommons.org/">API Commons</a></strong> - What is the licensing for the API?</li>
</ul>

<p>APIs.json isn’t meant to compete with these existing API definition formats, it is designed to index them. It is meant to provide a complete index of the human, and machine readable components required to operate an API platform. The properties collection is meant to index all of these, but also work to move the human elements towards being more machine readable, as we’ve seen happen with documentation and OpenAPI, as well as what we achieved with licensing and <a href="http://apicommons.org/">API Commons</a>. Ultimately we’d like to see all the essential aspects of API integration become machine readable–elements like:</p>

<ul>
  <li><strong>Pricing</strong> - A machine readable breakdown of pricing, tiers, and limits.</li>
  <li><strong>Terms of Services</strong> - Breaking down TOS so they can be interpreted by a machine.</li>
  <li><strong>Signup</strong> - Making signup for APIs something that is automated.</li>
</ul>

<p>Yes, this will all take a significant amount of work, just like indexing APIs, and making them more discoverable, but it has to be done. Currently, <a href="http://theapistack.com/">I’m just looking to keep APIs.json, OpenAPI, and Postman Collection definitions up to date across thousands of Github repositories included in my API Stack work</a>. As this work stabilizes, and continues to come into focus, I’ll be pushing forward with better defining signup and pricing across API providers, and then get to work on terms of service. <a href="http://apievangelist.com/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">We need all of these elements of API operations to become machine readable and executable as part of the discovery process</a>, otherwise this shit will never scale.</p>

<p>That is a fresh look at the <a href="http://apisjson.org/format.html">APIs.json</a> vision. There is more to it than just indexing API operations and making them more discoverable. I am also using to build different topical, and industry level collections, which you can reference as part of the APIs.json includes property. But, we’ll save that for another story. In the mean time, I’m going to get back to work profiling APIs using APIsjson, and publishing them to my API Stack organization(s), and syndicating them out to other partners like APIs.io, Streamdata.io, and Postman Network. If you’d like to know more about APIs.json, feel free to reach out, and I’ll make sure and work on more stories around how I am using the API discovery format.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/28/sharing-the-apis-json-vision-again/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/28/a-decade-later-twitters-api-plan-slowly-bgins-to-take-shape/">A Decade Later Twitter's API Plan Slowly Begins To Take Shape</a></h3>
        <span class="post-date">28 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/twitter/twitter-bird-blue-on-white.png" width="30%" align="right" style="padding: 15px;" /></p>
<p>It’s been a long, long road for Twitter when it comes to realizing the importance of having a plan when it comes to their API management strategy. Aside from monetizing the firehose through former parter, and now acquired solution Gnip, Twitter has never had any sort of plan when it came to providing access tiers for their API. I try to revisit the Twitter developer portal a couple times a year, but I’m going to have to increase the number of visits as there seems to be more rapid shifts towards getting control over their API management layer in recent months.</p>

<p>The <a href="https://developer.twitter.com/en/pricing">API plan matrix that has emerged as part of Twitter’s pricing page</a> provides a view of the unfolding plan for the social media platform.</p>

<p align="center"><a href="https://developer.twitter.com/en/pricing"><img src="https://s3.amazonaws.com/kinlane-productions/twitter/twitter-pricing-plan.png" width="100%" /></a></p>

<p>The Twitter API plan doesn’t show the entire scope, as it doesn’t cover the streaming layer of the API, but it does provide an important first step towards bringing coherence to how developers can access the API, and pay for premium levels of access. This conversation isn’t just about Twitter making money or Twitter charging for access to our data, this is about Twitter taking control over who has access to our data, and the platform. It is up to Twitter whether or not they focus on revenue, or the needs of end-users, something that we will only realize as their API plan continues to unfold.</p>

<p>This is the layer where Twitter will begin to reign in bots, and other malicious activity, but it is something that won’t be easy, and undoubtedly cause a lot of pain and frustration for developers, and end-users. It is something they should have done a long time ago, but as we are seeing play out with Facebook, to incentivize the growth their investors and shareholders desired, these platform needed to look the other way at the API management layer. To achieve the eyeballs, clicks, and engagements they desired, it required them to ignore harassment, bots, and other forms of abuse that occurs. Twitter is different than Facebook, because it is primarily a public platform, but the lack of a plan at the API management layer, has had a similar impact.</p>

<p>I’ll keep an eye on where Twitter is headed with all of this. While I’m concerned for how they’ll prioritize the needs of end-users and developers, It makes me happy to see them putting a plan in place at the API management level. <a href="http://apievangelist.com/2012/06/29/twitter-continues-to-restrict-access-to-our-tweets/">Twitter has operated for too long without a plan</a>, and without a clear path for serious applications built on the platform to grow and evolve. I’ll keep an eye out for continued examples of Twitter restricting access to our own data, and call them out when they lean to heavily towards revenue generation. While it is a decade too late, I have to commend them for putting their API plan in place, and begin the messy work of reigning in API access to the platform.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/28/a-decade-later-twitters-api-plan-slowly-bgins-to-take-shape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/28/data-driven-apps-at-scale-using-github-and-jekyll/">Data Driven Apps At Scale Using Github And Jekyll</a></h3>
        <span class="post-date">28 Mar 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/43_113_800_500_0_max_0_-5_-5.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve pushed my Github driven, Jekyll fueled, application delivery process to a new levels the last couple of weeks. I’ve been investing more cycles into <a href="http://theapistack.com/">my API Stack work</a>, as I build out an <a href="http://api.gallery.streamdata.io/">API Gallery for Streamdata.io</a>. All of my work as the API Evangelist has long lived as independent Github repositories, with a JSON or YAML core that drives the static Jekyll UI for my websites, applications, and API developer portals. Designed to be independent, data and API driven micro-solutions to some related aspect of my API research. I’m just turning up the volume on what already exists.</p>

<p>I’m taking this to the next level with my approach to API discovery. API Stack used to be a single Github repository with huge number of APIs.json and OpenAPI definitions, but after hitting the ceiling with the size of the repository, I’ve begun to shard projects by entity, topic, and collection. So far I have <a href="https://github.com/api-stack-providers">285 entities</a>, with 9850 API paths, <a href="https://github.com/api-stack-topics">spanning 397 topics</a>. Each entity, and topic exists within its own Github repository, acting as an individual set of API definitions, including an OpenAPI, Postman Collection, and APIs.json index. Allowing me to move forward each API definition for an entity independently, and begin aggregating APIs into interesting collections by topic, industry, or other relevant area.</p>

<p>There is no backend database for these projects. Well, there is a central system I use to help me orchestrate things, but each project is standalone, with everything it needs stored in the Github repository. It is something that took some engineering, and time to setup and first, but now that it is setup, managing it is actually pretty easy. The approach has also allowing me to scale it without any additional performance issues. Each entity I add becomes its own repo, and unless I hit some magic number for the number of repositories I can have within an organization, I should be able to scale to several thousand repos pretty quickly. Doing the same thing for topics, and other types of collections I’m aggregating APIs into.</p>

<p>Each entity repository has the OpenAPI, and Postman Collection for its provider. I’m also breaking out each individual API path into it’s own OpenAPI definition, and considering doing the same for Postman Collections–distilling things down to its smallest possible unit of compute. The APIs.json for each repository indexes the OpenAPI, Postman collection, as well as the other elements of API operations like documentation, blog, Twitter, and Github accounts. In the future I will be publishing other machine readable artifacts here including details on plans and pricing, terms of service, monitoring, performance, and other aspects of the ranking and profiling of the API sector that I do.</p>

<p>I’m getting ready to softly launch Streamdata.io’s API Gallery, but my API Stack work will continue to be my workbench for this type of API discovery work. I’ll keep harvesting, scraping, and profiling APIs and their operations, and as they come into focus I’ll sync with Streamdata.io’s API Gallery, and look at how I can help Postman with their API Network, APIs.guru with their directory, as well as other API aggregation and integration platforms like AnyAPI, APIDeck, and others. I’m just getting to the point where I feel like things are working as I want them to, and are ready for me to shift into higher gear. There are still a lot of issues with individual entities, and topics, but the process is working well–I just need to spend more time refining the API definitions themselves, and work on syncing with other efforts in the space.</p>

<p>Right now I’m heavily using the Github API to make all this work, but I’m probably going to switch to relying on Git for the heaviest lifting. However, as a 30 year database person, I have admit that I am rather enjoying the scope of delivering JSON and YAML driven API-driven continuous deployment and integration projects like this that have a machine readable layer, but also a UI layer for humans to put to use. It provides me a with a very API way of driving how I profile APIs, quantify what it is they do, and begin to rank and make sense of the growing API landscape. Continuing with my way of doing things out in the open, while hopefully providing more value to the API discovery conversation, in an OpenAPI and Postman Collection driven API universe.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/28/data-driven-apps-at-scale-using-github-and-jekyll/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/27/oauth-has-many-flaws-but-it-is-the-best-we-have-at-the-moment/">OAuth Has Many Flaws But It is The Best We Have At The Moment</a></h3>
        <span class="post-date">27 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/capital-battle_internet_numbers.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I was enjoying the REST API Notes newsletter from my friend Matthew Reinbold (<a href="https://twitter.com/libel_vox">@libel_vox</a>) today, and wanted to share my thoughts on his mention of my work while it was top of my mind. I always enjoy what Matt has to say, and regularly encourage him writing on his blog, and keep publishing his extremely thoughtful newsletter. It is important for the API sector to have many thoughtful, intelligent voices breaking down what is going on. I recommend <a href="https://tinyletter.com/RESTAPINotes/">subscribing to REST API Notes</a> if you haven’t, you won’t regret it.</p>

<p>Anyways, Matt replied with the following about my Facebook response:</p>

<p><em>Kin Lane did a fine job <a href="https://apievangelist.com/2018/03/26/a-regulatory-framework-for-facebook-and-other-platforms-is-already-in-place/">identifying the mechanisms most companies already have in place to mitigate the kind of bad behavior displayed by Cambridge Analytica</a>. It is a good starting point. However, I do want to challenge one of his assertions. Kin implies that OAuth consent is a sufficient control for folks to manage their data. While better than nothing, I maintain that most consumers are incapable of making informed decisions. It’s not a question of their intelligence. It is a question of complexity and incentives for the business to be deliberately opaque.</em></p>

<p>I agree. To quote my other friend Mehdi Medjaoui (<a href="https://twitter.com/medjawii">@medjawii</a>), “OAuth is flawed, but it is the best we have”. I would say that “consumers are incapable of making informed decisions”, because we’ve crafted the world this way, and our profit margins depend on customers not being able to make informed decisions. It is how markets work things out, and “smart” people get ahead, and all that bullshit. You see this same thing playing out at the terms of service level as well. As a consumer of online services, I am regularly incapable of being able to make informed decisions around how my data is being used, in exchange for using a free web application. Is it because I’m not smart? No, it is because terms of service are purposefully confusing, verbose, legalize, and mind numbing. Why? So I don’t read them, understand them, and run the other way.</p>

<p>This is intentional. You see this in the Tweet responses from Facebook engineers as they call all of us ignorant fools, and say that we agreed to all of this. The problem isn’t OAuth. The problem is intentional deceit and manipulation by corporations, and for some reason we keep showcasing this type of behavior as being savvy, smart, and just doing business. There is no reason that terms of service or OAuth flows can’t be helpful, informative, and in plain language. Platforms just choose to not invest in these areas, because an uninformed user means more profit for the platform, its investors, and shareholders. The lack of available startups, services, and tooling at the terms of service and privacy layer of technology sector demonstrates what a scam all of this has been. If startups cared about their users, we would have seen investment in tools and services that help us all make sense of things. Period.</p>

<p><a href="http://apievangelist.com/2014/05/14/the-future-of-public-private-sector-partnerships-being-negotiated-at-the-api-oauth-scope-level/">I’ve sat in multi-day OAuth negotiation sessions on behalf of the White House</a>, negotiating OAuth scopes on behalf of power customers. I’ve seen how strong armed the corporations can be, and how little defense the average consumer has. I’ve seen technology platforms intentionally complexify things over and over. I’ve also seen plain language terms of service that actually make sense–<a href="https://slack.com/terms-of-service/api">read Slack’s for an example</a>. I’ve seen OAuth flows that protect a user’s interest, however I haven’t seen ANY investment in actually educating end-users about what is OAuth, and why it matters. You know, <a href="https://apievangelist.com/2013/10/17/api-and-oauth-literacy-is-as-important-as-financial-literacy-in-the-api-economy/">like the personal finance classes we all get in high school</a>? (I wrote that shit in 2013!!!) Why aren’t we educating the average consumer about managing their personal data and privacy? Why aren’t we setting standards for OAuth scopes, and flows for technology platforms? Good question!</p>

<p>I’ve spent the last eight years trying to encourage platforms to be better citizens using their APIs. I’ve been aggregating and showcasing the healthy building blocks they can use to better serve their customers, and <a href="https://apievangelist.com/2017/07/18/diagramming-the-components-of-api-observability/">provide more observability into how they operate</a>. At this point, I feel like the tech sector has had their chance. Y’all chose to willfully ignore the interest of end-users, and get rich on all of our data. I know you all will cry foul now that the regulatory winds are starting to blow. Too bad. You had you chance. I’m going to focus all my energy and resources into educating policy and lawmakers about how they can use APIs, OAuth, and other building blocks already in use to put y’all into check. There is no reason the average consumer can’t make an informed decision around the terms of service of the platforms they depend on, and intelligently participate in conversations around access to their data using OAuth. As Matt said, “it is a question of complexity and incentives for the business to be deliberately opaque.”–regulations and policy is how we shift that.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/27/oauth-has-many-flaws-but-it-is-the-best-we-have-at-the-moment/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/27/sharing-your-api-first-principles/">Sharing Your API First Principles</a></h3>
        <span class="post-date">27 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/zalando/zalando-screenshot.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve been publishing regular posts from the API design guides of API providers I’ve been studying. API providers who publish their API design guides tends to be further along in their API journey. These API providers tend to have more experience and insight, and are often worth studying further, and learning from. <a href="https://zalando.github.io/restful-api-guidelines/">I’ve bee getting a wealth of valuable information from the German fashion and technology company Zalando, who has shared some pretty valuable API first principles</a>.</p>

<p>In a nutshell Zalando’s API First principles focuses on two areas:</p>

<ul>
  <li>define APIs outside the code first using a standard specification language</li>
  <li>get early review feedback from peers and client developers</li>
</ul>

<p>By defining APIs outside the code, Zalando wants to facilitate early review feedback and also a development discipline that focus service interface design on:</p>

<ul>
  <li>profound understanding of the domain and required functionality</li>
  <li>generalized business entities / resources, i.e. avoidance of use case specific APIs</li>
  <li>clear separation of WHAT vs. HOW concerns, i.e. abstraction from implementation aspects — APIs should be stable even if we replace complete service implementation including its underlying technology stack</li>
</ul>

<p>Moreover, API definitions with standardized specification format also facilitate:</p>

<ul>
  <li>single source of truth for the API specification; it is a crucial part of a contract between service provider and client users</li>
  <li>infrastructure tooling for API discovery, API GUIs, API documents, automated quality checks</li>
</ul>

<p>Their guidance continues by stating:</p>

<p><em>An element of API rirst are also a review process to get early review feedback from peers and client developers. Peer review is important for us to get high quality APIs, to enable architectural and design alignment and to supported development of client applications decoupled from service provider engineering life cycle.</em></p>

<p><em>It is important to learn, that API First is not in conflict with the agile development principles that we love. Service applications should evolve incrementally — and so its APIs.</em></p>

<p><em>Of course, our API specification will and should evolve iteratively in different cycles; however, each starting with draft status and early team and peer review feedback. API may change and profit from implementation concerns and automated testing feedback. API evolution during development life cycle may include breaking changes for not yet productive features and as long as we have aligned the changes with the clients. Hence, API First does not mean that you must have 100% domain and requirement understanding and can never produce code before you have defined the complete API and get it confirmed by peer review.</em></p>

<p><em>On the other hand, API First obviously is in conflict with the bad practice of publishing API definition and asking for peer review after the service integration or even the service productive operation has started. It is crucial to request and get early feedback — as early as possible, but not before the API changes are comprehensive with focus to the next evolution step and have a certain quality (including API Guideline compliance), already confirmed via team internal reviews.</em></p>

<p>Zalando get’s at one of the reasons I’m a big advocate for a design, mock, and document API lifecycle, forcing developers to realize their API vision, get feedback, and iterate upon their designs, before they ever consider moving into a production environment. I feel like there are many views of what API first means, and many do not focus on the process, and obsess too much on just API design. I think Zalando’s API design guide reflects this, where the guide is more about API guidance, than it is just about the design of an API. It provides a wealth of wisdom and knowledge, but is still labeled as being about API design.</p>

<p>Similar to pushing companies, organizations, institutions, and government agencies to do in APIs in the first place, I’m encouraging more of them to begin publishing their API design guides as part of their journey. It seems like it is an important part of being able to articulate not just the API design guidelines, but also a place to articulate the overall reasons behind doing APIs in the first place, and the principle, ethics, and process surrounding how we are doing APIs across the disparate teams that make things go around in our worlds.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/27/sharing-your-api-first-principles/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/27/government-has-benefitted-from-lack-of-oversight-at-social-api-management-layers-as-well/">Government Has Benefitted From Lack Of Oversight At The Social API Management Layers As Well</a></h3>
        <span class="post-date">27 Mar 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/76_33_800_500_0_max_0_-1_-1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>There are many actors who have benefitted from <a href="https://apievangelist.com/2018/03/21/facebooks-business-model-is-out-of-alignment-with-their-api-management-layer/">Facebook not properly management their API platform</a>. Collecting so many data points, tracking so many users, and looking the other way as 3rd party developers put them to use in a variety of applications. Facebook did what is expected of them, and focused on generating advertising revenue from allowing their customers to target the many data points that are generated by the web and mobile applications developed on top of their platform.</p>

<p>I hear a lot of complaining about this being just about democrats being upset that it was the  republicans who were doing this, when in reality there are so many camps that are complicit in this game, if you are focusing in on any single camp you are probably looking to shift attention and blame from your own camp, in my opinion. I’m all for calling everyone out, <a href="https://apievangelist.com/2017/01/09/the-api-driven-marketplace-that-is-my-digital-self/">even ourselves for being so fucking complicit in this game where we allow ourselves to be turned into a surveillance marketplace</a>. Something that benefits the Mark Zuckerbergs, Cambridge Analyticas, and the surveillance capitalist platform, but also providing a pipeline to government, and law enforcement.</p>

<p>If government is going to jump on the bandwagon calling for change at Facebook, they need to acknowledge that they’ve been benefitting from the data free for all that is the Facebook API. We’ve heard stories about 3rd party developers building applications for law enforcement to target Black Lives Matters, all the way up to Peter Thiel’s (a Facebook Investor) Palantir who actively works with NSA, and other law enforcement. If we are going to shine a light on what is happening on Facebook, let’s make it a spotlight and call out ALL of the actors who have been sucking user’s data out of the API, and better understand what they have been doing. I’m all for a complete audit of how the RNC, DNC, and EVERY other election, policing, and government entity has been doing with social data they have access to.</p>

<p>Let’s bring all of the activity out of the shadows. Let’s take EVERY Facebook application and publish as part of a public database, with an API for making sense of who is behind each one. Let’s require every application share the name’s of individuals and business interests behind their applications, as well as their data retention, storage, and sharing practices. Let’s make the application database observable by the public, journalists, and researchers. If we are going to understand what is going on with our data, we need to turn the lights on in the application warehouse that is built on the Facebook API. Let’s not just point the finger at one group, let’s shine a spotlight on EVERYONE, and make sure we are actually being honest at this point in time, or we are going to see this same scenario play out every fiver years or so.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/27/government-has-benefitted-from-lack-of-oversight-at-social-api-management-layers-as-well/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/27/acknowledging-why-i-do-apis-is-very-different-than-why-others-do-apis/">Acknowledging That Why I Do APIs Is Very Different Than Why Others Are Doing APIs</a></h3>
        <span class="post-date">27 Mar 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/76_16_800_500_0_max_0_1_1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>When I started doing API Evangelist in 2010 I was still very, very, very naive about why people were doing APIs. While I do not always expect everyone doing APIs to be ethical and sensible with the reasons behind doing APIs, I have been regularly surprised at how many different views there are out there regarding why we should be doing APIs in the first place. The lesson for me is to never assume that someone is doing APIs for the same reasons I am doing APIs, because rarely that will ever be the case–hopefully minimizing the chances I’ll get continue to be surprised down the road.</p>

<p>After watching Salesforce, then Amazon, Flickr, Twitter, Facebook, and Google Maps doing APIs, I saw the potential for APIs to open up access to resources like never before. I also saw the potential for not just opening up these new opportunities to developers, but OAuth was beginning to give end-users a voice in how their data and content could be put to use. I saw the opportunity for a new type of partnership between platforms, developers, and end-users emerging that could change how we do business online, how government works, and much, much more. Sadly my white male privilege, and powers of denial prevented me from seeing the many other ways in which APIs were being seen as an opportunity.</p>

<p>I find that the reasons people publish regarding why they are doing APIs rarely size up with the real reasons why they are actually doing APIs. Sometimes you can read between the lines by evaluating their overall business model, or lack of one. Other times you can find some telling signs present in the design of their API, as the paths, parameters, and technical details tend to tell a more accidentally honest view of what is happening behind the curtain. However, when it comes to the theater of “open APIs”, everything quickly becomes a funhouse of mirrors when it comes to trying to understand why someone is doing APIs, with the only constant being that nothing last forever–all APIs will eventually change, and go away, no matter what the reasons are behind them.</p>

<p>The reasons I recommend doing APIs centers around opening up access to valuable data, content, and algorithms in a secure and observable way, that protects the privacy of everyone involved. I’m an advocate for API literacy amongst everyone involved in the API conversation, even the non-technical end-user who is most likely being impacted by their existence. I’m team API because they can bring observability into some very black box algorithms, and closed door platforms, that are increasingly governing our lives. My belief in APIs is not purely based on their technical merit, but in that some proven processes involving them have been established, which introduce some healthy balance into how we deliver web, mobile, and device-based applications. It isn’t the technology, it is how us humans are using the technology.</p>

<p>Like other web-based technology, APIs have been weaponized, and identified as a valuable tool for exploitation. Platforms like Facebook and Twitter have demonstrated how they can help a platform grow and evolve, but when this growth is in the service of an advertising-focused business, they can be used to influence, nudge, and harass people in some very damaging ways. With all the exploitation, bad behavior, and looking the other way that occurs around API operations, I find it very difficult to consider representing the sector, but I do find it important that I continue making sense of what is happening. I just need to make sure that I remember that the reasons I’m doing APIs will almost always be different than why others are doing API, and I should never assume folks have the healthiest, and most meaningful intent behind operating their API platforms, and why they are reaching out to me.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/27/acknowledging-why-i-do-apis-is-very-different-than-why-others-do-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/26/nexmo-manages-their-openapi-30-definition-using-github/">Nexmo Manages Their OpenAPI 3.0 Definition Using Github</a></h3>
        <span class="post-date">26 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/nexmo/nexmo-api-specifications.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m big on supporting API providers that publish their OpenAPI definitions to Github. It is important for the wider API community that ALL API definitions are machine readable, and available in a way that can be forked, and integrated into continuous integration pipelines. I’m not even talking about the benefits to the API providers when it comes to managing their own API lifecycle. I’m just focusing on the benefits to API consumers, and helping make on-boarding, integration, and keeping in sync with the road map as frictionless as possible.</p>

<p>To help incentivize API providers doing this I’m committed to writing up stories for each API provider that publishes their OpenAPI, APIs.json, or Postman Collections to Github. Bonus points if you are doing it in an interesting way that further benefits your operations, as well as your community. Today’s API provider to showcase is <a href="https://www.nexmo.com">the SMS, voice and phone verifications API provider Nexmo</a>, who tweeted <a href="https://github.com/Nexmo/api-specification">the Github repository at me, which contains their OpenAPI definition for their APIs</a>. As they say, it is a work in progress, but it provides a damn good start for a machine readable definition for their API(s), and I mean c’mon, aren’t all of our APIs a working progress?</p>

<p>Nexmo uses their API definition as “a single point of truth that can be used end-to-end can we used to:”</p>

<ul>
  <li>Planning Shared during product discussions for planning API functionality</li>
  <li>Implementation Inform engineering during development</li>
  <li>Testing As the basis for testing or mocking API endpoints</li>
  <li>Documentation For producing thorough and interactive documentation</li>
  <li>Tooling To generate server stubs and client SDKs.</li>
</ul>

<p>Nexmo has adopted version 3.0 of the OpenAPI definition, which is forward leaning for the API provider. They also provide a list of resources, tooling, and their API definition available as Ruby packages for easier integration. Another thing they do that I think is interesting, is they list the owner, and contributors for each API definition they have published, or are working on. Which is a concept I fully support, and would like to bake into my own API stack work. Working on API definitions is hard work, and we should be showcasing, and supporting anyone that steps up to do the hard work, as well as providing a point of contact for each available definition.</p>

<p>I am going to spend some time going through Nexmo’s API definition, and learning more about OpenAPI 3.0. I will also be spending time going through their API developer portal creating an APIs.json, and Postman Collection for their API. I’m looking to add their API to the Postman API Network, as well as my API Stack, Streamdata.io API Gallery, and other API aggregation, integration, and discovery projects I am working on. Thanks for bringing to my attention Nexmo, and keep up the good work. I’ll keep paying attention to what you are doing, with Github, and your OpenAPI definition being a conduit for guiding my attention.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/26/nexmo-manages-their-openapi-30-definition-using-github/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/26/a-regulatory-framework-for-facebook-and-other-platforms-is-already-in-place/">A Regulatory Framework For Facebook And Other Platforms Is Already In Place</a></h3>
        <span class="post-date">26 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/docks_copper_circuit.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>There is lots of talk this week about regulating Facebook after the Cambridge Analytica story broke. Individuals, businesses, lawmakers, and even Facebook are talking about how we begin to better regulate not just Facebook, but the entire data industry. From my perspective, as the API Evangelist, the mechanisms are already in place, they just aren’t being used to their fullest by providers, with no sufficient policy in place at the federal level to incentivize healthy behavior by API providers, data brokers, and 3rd party application developers.</p>

<p><strong>API Management Is Already In Place</strong><br />
Modern API platforms, Facebook included, leverage what is called an API management layer, to help manage the applications being developed on their platforms. Every developer who wants to access platform APIs has to sign up, submit the name and details of their application(s), and then receive an API key which they have to include with each call they make to a platform’s API when requesting ANY data, content, or usage of an algorithm. This means that all the platforms should already be in tune with every application that is using their platform, unless they allow internal groups, and specific partners to bypass the API management layer.</p>

<p><strong>An Application Review Process</strong><br />
Every application submitting for API keys has to go through some sort of review process. The bar for this review process might be as low as requiring you to have an email address, or as high as submitting drivers and business licenses, and verify how you are storing your data. Facebook has an application review process in place, and is something they are promising to tighten up a bit in response to Cambridge Analytica. How loose, or tight a platform is with their application review process is always in alignment with the platform’s business model, and how the value their user’s privacy and security.</p>

<p><strong>Govern Data Access OAuth Using Scopes</strong><br />
Most platforms use a standard called OAuth for enabling 3rd party developers access to a platform user’s data. Each quiz, game, and application has to ask a user to authorize access to their data, and OAuth scopes govern how much, or how little data can be accessed. OAuth scopes are defined by the platform, and negotiated by 3rd party developers, and end-users. Facebook has tightened their OAuth scopes in recent years, but has more work ahead of them to make sure they protect end user’s interest, and that 3rd party developers are properly following OAuth healthy practices, and end-users are aware and literate of what they are doing when participating in OAuth flows.</p>

<p><strong>Logging Of Every API Access</strong><br />
When it comes to API access, everything is logged, and you know exactly who has accessed what, and when. Everything an application does with Facebook, or any other platform’s data via their API is logged. This is standard practice across the industry. Since every application passes their API key with each call, you know the details of what each application is doing in real time. Saying you aren’t aware of what an application is doing with user’s data doesn’t exist in an API-driven world, it just means that you aren’t actively paying attention to, and responding to log activity in real time.</p>

<p><strong>Analysis And Reporting Of Usage</strong><br />
Detailed analysis and reporting is baked into API management, providing visual dashboards for API platforms, application developers, and end-users to understand how data is being accessed and put to use by applications. Understanding activity at the API management layer is one of the fundamental elements of striking a balance between access and control over platform data consumption. APIs aren’t secure if nobody is paying attention to what is happening, and actively responding to bad behavior, and undesirable access. Platforms, 3rd party developers, as well as end-users should all be equipped, and educated regarding the analysis and reporting that is available via all any platform in which they operate on.</p>

<p><strong>Application Directory And Showcase</strong><br />
Many API platforms actively showcase the applications that are built upon their APIs, sharing the interesting applications that are available, and allowing consumers to put them to use. When it comes to regulation, this existing practice just needs expanded upon, requiring ALL applications to be registered, whether for internal, partner, or external use. Then also expanding upon the data that is required and published as part of the application submission, review, and operation practice. Additionally, the application directory and showcase should be operated and audited by external entities, who aren’t beholden to platform interest, providing a more transparent, observable, and accountable approach to understanding what an application’s motivations are, and fairly reporting upon all platform usage.</p>

<p><strong>Recurring Application Auditing Practices</strong><br />
Building on the existing application review process, and in conjunction with the application directory, all active applications should be subject to a recurring review process triggered by time, and by different levels of consumption. You already see some of this from providers like Twitter who limit applications to 1M OAuth tokens, which then you are subject to further scrutiny. This is something that should happen regularly for all applications, pushing for annual review and audit of their operations, as well as at 10K, 100K, and 1M OAuth token levels, pushing to further understand the data gathering, storage, funding, and partner relationships in play behind the scenes with each application built on top of a platform’s API.</p>

<p><strong>Breach Notifications And Awareness</strong><br />
Every platform should be required to communicate with users, and the public regarding any breach that occurs within a 72 hour period. We see this practice taking hold in Europe around the GDPR regulation, and is something that needs to be adopted throughout the United States. This shouldn’t be limited to large scale, and malicious breaches, and also include any terms of service, privacy, or security violations incurred by individual applications. Publishing and sharing the details of the incident as part of the application directory and showcase. Highlighting not just the healthy behavior on the platform, but being open and transparent regarding the bad behavior that occurs. Bringing bad actors out of the shadows, and holding platforms, as well as 3rd party developers accountable for what goes on behind the scenes.</p>

<p><strong>API Access For API Management Layer</strong><br />
All aspects of a platform should have APIs. Every feature available in the UI for a web or mobile application, should have a corresponding API. This applies to the platform as well, requiring that the entire API management, logging, and application directory layers outlined above all have APIs. Providing access to all applications who are accessing a platform, as well as understanding how they are putting API resources to work, and consuming valuable data and content. Making the platform more observable and accountable, allowing it to be audited by 3rd party research, journalists, regulators, and other industry level actors. Of course, not everyone would gain access to this layer by default, requiring different tiers of access, but still holding ALL API consumers to the same levels of accountability, no matter which tier they are working at.</p>

<p><strong>Service Composition For Different Types Of Users</strong><br />
Another fundemental aspect of API management that exists across the tech sector, but isn’t always discussed or understood, is the different tiers of access available, depending on who you are. API service composition at the API management layer is what dictates that new API consumers only get access to a limited amount of resources, while more trusted partner, and internal groups get access to higher volumes, as well as additional number of API resources. Service composition is what limits the scope of API terms of service, privacy, and security breaches, and also allows for the trusted auditing, reporting, and analysis of how a platform operates by researchers, regulators, and other 3rd party actors. Every platform has different levels of API access as defined through their service composition at the API management layer. Sometimes this is reflected on their plans and pricing pages, but often times when there is no clear business models, these levels of access operate in the shadows.</p>

<p><strong>Looking At Existing Solutions When it Comes To Platform Regulations</strong><br />
The cat is out of the bag. APIs have been used to measure, understand, and police the access to data, content, and algorithms at the API level for over a decade. Nothing I’ve outline is groundbreaking, or suggests anything new be developed. The only changes required are at the government policy level, and a change in behavior at the platform level. The tooling and services to implement the regulations of platforms already exists, and in most cases are already in place–it just happens that they are serving the platform masters, and have not be enlisted to serve end-users, and most definitely not at the regulatory level. API management is how platforms like Facebook, Twitter, and others have gotten to where they are, by incentivizing development and user growth via API integrations, the problem is this often involves turning the API faucet completely on, and not limiting access based upon privacy or security, and only in service of the business model when it makes sense.</p>

<p>Facebook has the ability to understand what Cambridge Analytic was doing. They can identify healthy and not so healthy practices via API consumption long before they become a problem. The problem is that their business model hasn’t been in alignment with taking control of things at the API management level–it encourages bad behavior, not limits it. There also isn’t any API access to this API management layer, and no observability into the applications that are built on top of existing APIs. There is no understanding at the regulatory level that these things are possible, and there are no policies in place requiring platforms have APIs, or provide industry level regulatory access to their API management layers. You can see <a href="https://www.openbanking.org.uk/">the beginning of this emerge with banking regulatory efforts out of the UK</a>, and requiring the API management layer be operated by a 3rd part entity, with all actors be registered in the API management directory, and accountable to the community, and the industry. Setting a precedent for regulatory change at the API management layer, and providing a working blueprint for how this can work.</p>

<p>API management is over a decade old, and something that is baked into the Amazon, Azure, and Google Clouds. It is standard practice across all leading API providers like Facebook, Twitter, Google, Amazon, and others. The concept of regulating data, content, and algorithmic access and usage at the API layer isn’t anything new. The only thing missing is the policy, and regulatory mandate to make it so. Americans are acting like this is something new, but we see a precent already being set in Europe through GDPR, and PSD2 in banking. The CFPB in DC was already working on moving this conversation forward until the Trump White House began unraveling things at the agency. Regulating our personal data won’t be a technical, or business challenge to solve, it will ultimately be about the politics of how we operate our platforms in a more sustainable way, and truly begin protecting the privacy and security of users (or not).</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/26/a-regulatory-framework-for-facebook-and-other-platforms-is-already-in-place/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/21/investment-in-api-security-will-continue-to-fall-short-while-there-is-no-breach-accountability/">Investment In API Security Will Continue To Fall Short While There Is No Breach Accountability</a></h3>
        <span class="post-date">21 Mar 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/64_128_800_500_0_max_0_-1_-1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I have a lot of conversations with folks down in the trenches about API security, and what they are doing to be proactive when it comes to keeping their API infrastructure secure. The will and the desire amongst folks I talk to regarding API security is present. They want to do what it takes to truly understand what is needed to keep their APIs secure, but many have their hands tied because lack of resources to actually do what is needed. Every API team I know is short-handed, and doing the best they can with what they have available to them. A lack of investment in API security isn’t always intentional, it ends up just begin a reality of the priorities on the ground within the organizations they work in.</p>

<p>While I’m sure leaders within these companies are concerned about breaches within their API infrastructure, the urgency to invest in this area isn’t always a priority. Despite an increase in high profile, often API-induced breaches, IT and API groups are still not given the amount of resources they need to do something about potential security incidents. Other than the stress and bad press of a breach, there really are no consequences in the United States. We have seen this play out over and over, and when high profile breaches like Equifax go unpunished, other corporate leaders fully understand that there will no consequences, so why invest in preventative measures–we will just respond to it “if” it happens.</p>

<p>This is why GDPR, and other similar legislation will become important to the API security industry. Without real civil or criminal penalties involved with breaches, and even heavier penalties for poorly handled breaches, companies just aren’t going to care. Data is just a replaceable commodity, and a company can recover from the hit to their brand when a breach does occur. Making the investment in proactive API security training, staffing, services, and processes an unnecessary thing. Reflecting how health care is handled in this country, with 95% of the investment in treating things after they happen, and about 5% investment in preventative care. Hoping all along you don’t get sick, or have a breach.</p>

<p>I can talk until I blue in the face to business leaders about API security, and make them aware of healthy practices, but if there isn’t an incentive to invest in API security, it will never happen. At this point I feel that API security is more a reflection of a wider systemic illness around how we view data, and that country and industry level policy is where change needs to occur. I will keep showcasing specific building blocks of an API security strategy, as well as showcase services and tools to help you implement your strategy, but I feel like the most meaningful change will have to occur at the policy level. Otherwise business leaders will never prioritize API security, leaving all of OUR data vulnerable to exploitation–it is just a cost of doing business at this point.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/21/investment-in-api-security-will-continue-to-fall-short-while-there-is-no-breach-accountability/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/21/facebooks-business-model-is-out-of-alignment-with-their-api-management-layer/">Facebook's Business Model Is Out Of Alignment With Their API Management Layer</a></h3>
        <span class="post-date">21 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/adam-smith_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>All the controls for Facebook to have done something about the Cambridge Analytics situation are already in place. The Facebook API management layer has everything needed to have prevented the type of user data abuse we’ve experienced, and honestly the user data abuse that has happens in many other applications, and will continue to occur. The cause of this behavior is rooted in Facebook’s business model, and a wider culture that is fueled by venture capital investment, and backdoor data brokering. It is just how the big data / app economy is funded, and when your business model is all based upon advertising, user engagement to generate data / signals, and being about leveraging that behavior for targeting–you are never going to reign in your API management layer.</p>

<p>In alignment with my other story today around lack of investment in security, Facebook just doesn’t have the incentive to invest in policing their API management layer. There is no motivation to thoroughly vet new applications, let alone regularly review and audit what each application is doing as they approach 10K tokens, 100K, or 270K tokens. Sure you can’t get at all the friends data anymore, so they did work tighten things down at the API management schema layer, but most action we’ve seen is in response to bad situations, and not preventative. If you are properly managing the access to your APIs, and have the resources to monitor and respond to activity in real time, you are going to see the bad actors, and respond accordingly. Minimizing the damage that occurs to end-user, developing a robust set of patterns to keep an eye out for, and just get better at keeping your platform data consumption tight.</p>

<p>The problem is when your primary business model is centered around advertising, and providing a wealth of controls around targeting users based upon the data points they provide, you want as many apps, as many data points, and as many users as you possible can. You have no incentive to police what your applications are doing, as long as it drives the bottom line. You won’t invest in properly mapping out and restricting your schema, understanding what applications are doing, and sorting out the good from the bad. If it is fueling the delivery of advertising, allowing for more data points to target users based upon, driving clicking, sharing, and the eye balls on advertising, then it is by definition good. Investment in your API management becomes an extra cost that doesn’t need more investment, especially when it will ultimately hurt the bottom line.</p>

<p>We see this same behavior playing out via Twitter, Google, and other platforms. They will only manage their APIs if it directly competes with them, or makes for bad publicity. This is why Twitter hasn’t reigned in their bot networks until recently, and why Google doesn’t reign in the fake news, conspiracy, and propaganda networks. When advertising is your business model, you want the API faucet open wide, with very little filter on what flows. It isn’t a question of what mechanism we can put in place to bring some balance, these are already in place in the form of API management layers. The challenge is bringing business models in alignment with this layer, and incentivizing platforms to behave differently. Use Amazon as an example of this. Imagine if Amazon EC2 or S3 were advertising-driven, what type of bad behavior would we see? I’m not saying bad things don’t happen on these platforms, but they are managed much better, with different incentive models in play.</p>

<p>This conversation reflects one of the reasons I do API Evangelist. I don’t believe everyone should be doing APIs. However, the cat is out of the bag. The APIs are in place, and driving the web, mobile, and device applications across our Internet connected lives. The mechanisms are in place to monitor, limit, control, and understand what applications are doing with platform data. We just have to push for more observability at the API layer, and acknowledge that the business model for these platforms, and the wider technology sector is out of balance. This will prove to be the biggest challenge in changing all of this behavior, is that entrepreneurs and investors have gotten a taste of the value that can be generated at this layer, and it won’t be something they will give up easily. There is a lot of money to be via platforms when you can easily look the other way and say, “I didn’t know that was happening, so I shouldn’t be responsible.”</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/21/facebooks-business-model-is-out-of-alignment-with-their-api-management-layer/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/20/sendgrid-managing-their-openapi-using-github/">SendGrid Managing Their OpenAPI Using Github</a></h3>
        <span class="post-date">20 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/sendgrid/sendgrid-github-screenshot.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>This is a post that has been in my API notebook for quite a while. I feel it is important to keep showcasing the growing number of API providers who are not just using OpenAPI, but also managing them on Github, so I had to make the time to talk about <a href="https://github.com/sendgrid/sendgrid-oai">the email API provider SendGrid managing their OpenAPI using Github</a>. Adding to the stack of top tier API providers managing their API definitions in this way.</p>

<p>SendGrid is managing announcements around their OpenAPI definition using Github, allowing developers to signup <a href="https://dx.sendgrid.com/newsletter/oai">for email notifications around releases and breaking changes</a>. You can use the Github repository to stay in tune with the API roadmap, and <a href="https://github.com/sendgrid/sendgrid-oai/blob/master/CONTRIBUTING.md#feature-request">contribute feature requests</a>, <a href="https://github.com/sendgrid/sendgrid-oai/blob/master/CONTRIBUTING.md#submit-a-bug-report">submit bug reports</a>, and even <a href="https://github.com/sendgrid/sendgrid-oai/blob/master/CONTRIBUTING.md#cla">submit a pull request with the changes</a> you’d like to see. Going beyond just an API definition being about API documentation, and actually driving the API road map and feedback loop.</p>

<p>This approach to managing an API definition, while also making it a centerpiece of the feedback loop with your community is why I keep beating this drum, and showcasing API providers who are doing this. It is a way to manage the central API contract, where the platform provider and API consumers both have a voice, and producing a machine readable artifact that can be then used across the API lifecycle, from design to deprecation. Elevating the API definition beyond just a bi-product of creating API docs, and making it a central actor in everything that occurs as part of API operations.</p>

<p>I’m still struggling with convincing API providers that they should be adopting OpenAPI. That it is much more than an artifact driving interactive documentation. Showcasing companies like SendGrid using OpenAPI, as well as their usage of Github, is an important part of me convincing other API providers to do the same. If you want me to write about your API, and what you are working to accomplish with your API resources, then publish your OpenAPI definition to Github, and engage with your community there. It may take me a few months, but eventually I will get around to writing it up, and incorporating your story into my toolbox for changing behavior in our API community.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/20/sendgrid-managing-their-openapi-using-github/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/20/breaking-down-your-postman-api-collections-into-meaningful-units-of-compute/">Breaking Down Your Postman API Collections Into Meaningful Units Of Compute</a></h3>
        <span class="post-date">20 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/postman/postman-icon.png" width="30%" align="right" style="padding: 15px;" /></p>
<p>I’m fascinated with the unit of compute as defined by a microservice, OpenAPI definition, Postman Collection, or other way of quantifying an API-driven resource. Asking the question, “<a href="http://apievangelist.com/2018/02/15/how-big-or-small-is-an-api/">how big or how small is an API?</a>”, and <a href="http://apievangelist.com/2018/03/09/defining-the-smallest-unit-possible-for-use-at-api-runtime/">working to define the small unit of compute needed at runtime</a>. I do not feel there is a perfect answer to any of these questions, but it doesn’t mean we shouldn’t be asking the questions, and packaging up our API definitions in a more meaningful way.</p>

<p>As I was profiling APIs, and creating Postman Collections, the Okta team tweeted at me, their own approach to delivering their APIs. They tactically place Run in Postman buttons throughout their API documentation, as well as <a href="https://developer.okta.com/docs/api/getting_started/api_test_client.html#collections-quick-reference">provide a complete listing of all the Postman Collections they have</a>. Showcasing that they have broken up their Postman Collections along what I’d consider to be service lines. Providing small, meaningful collections for each of <a href="https://developer.okta.com/docs/api/getting_started/api_test_client.html">their user authentication and authorization APIs</a>:</p>

<table width="100%">
  <thead>
    <tr>
      <th style="text-align: left">Collections</th>
      <th style="text-align: left">Click to Run</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Authentication</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/f9684487e584101f25a3" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">API Access Management (OAuth 2.0)</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/e4d286b1af2294bb14a0" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">OpenID Connect</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/fd92d7c1ab0fbfdecab2" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Client Registration</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/291ba43cde74844dd4a7" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Sessions</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/b2e06a22c396bcc94530" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Apps</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/4857222012c11cf5e8cd" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Events</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/f990a71f061a7a16d0bf" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Factors</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/9fdda657d134039fcaba" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Groups</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/0bb414f9594ed93672a0" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Identity Providers (IdP)</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/00a7a643fc0ab3bb54c8" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Logs</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/9cfb0dd661a5432a77c6" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Admin Roles</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/04f5ec85685ac6f2827e" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Schemas</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/443242e60287fb4b8d6d" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Users</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/1755573c5cf5fbf7968b" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">Custom SMS Templates</td>
      <td style="text-align: left"><a href="https://app.getpostman.com/run-collection/d71f7946d8d56ccdaa06" target="_blank"><img src="https://run.pstmn.io/button.svg" alt="Run in Postman" /></a></td>
    </tr>
  </tbody>
</table>

<p>Okta’s approach delivers a pretty coherent, microservices approach to crafting their Postman Collections, providing separate API runtimes for each service they bring to the table. Which I think gets at what I’m looking to understand when it comes to defining and presenting our APIs. It can be a lot more work to create your Postman Collections like this, rather than just creating one single collection, with all API paths, but I think from a API consumer standpoint, I’d rather have them broken down like this. I may not care about all APIs, and I’m just looking at getting my hands on a couple of services–why make me wade through everything?</p>

<p>I have imported the Postman Collections for the Okta API, and added to my API Stack research. I’m going to convert them into OpenAPI definitions so I can use beyond just Postman. I will end up merging them all back into a single OpenAPI definition, and Postman Collection for all the API paths. However, I will also be exploding them into individual OpenAPIs and Postman Collections for each individual API path, going well beyond what Okta has done. Further distilling down each unit of compute, allowing it to be profiled, executed, streamed, or other meaningful action in isolation, without the constraints of the other services surrounding it.</p>

<script type="text/javascript">
  (function (p,o,s,t,m,a,n) {
    !p[s] && (p[s] = function () { (p[t] || (p[t] = [])).push(arguments); });
    !o.getElementById(s+t) && o.getElementsByTagName("head")[0].appendChild((
      (n = o.createElement("script")),
      (n.id = s+t), (n.async = 1), (n.src = m), n
    ));
  }(window, document, "_pm", "PostmanRunObject", "https://run.pstmn.io/button.js"));
</script>


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/20/breaking-down-your-postman-api-collections-into-meaningful-units-of-compute/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/19/gdpr-forcing-us-to-ask-questions-about-our-data/">General Data Protection Regulation (GDPR) Forcing Us To Ask Questions About Our Data</a></h3>
        <span class="post-date">19 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/face-2_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve been learning more about the EU General Data Protection Regulation (GDPR) recently, and have been having conversation about compliance with companies in the EU, as well as the US. In short, GDPR requires anyone working with personal data to be up front about the data they collect, making sure what they do with that data is observable to end-users, and takes a privacy and security by design approach when it comes to working with all personal data. While the regulations seems heavy handed and unrealistic to many, it really reflects a healthy view of what personal data is, and what a sustainable digital future will look like.</p>

<p>The biggest challenge with becoming GDPR compliant is the data mess most companies operate in. Most companies collect huge amounts of data, believing it is essential to the value they bring to the table, without no real understanding of everything that is being collected, and any logical reasons behind why it is gathered, stored, and kept around. A “gather it all”, big data mentality has dominated the last decade of doing business online. Database groups within organizations hold a lot of power and control because of the data they possess. There is a lot of money to be made when it comes to data access, aggregation, and brokering. It won’t be easy to unwind and change the data-driven culture that has emerged and flourished in the Internet age.</p>

<p>I regularly work with companies who do not have coherent maps of all the data they possess. If you asked them for details on what they track about any given customer, very few will be able to give you a consistent answer. Doing web APIs has forced many organizations to think more deeply about what data they posses, and how they can make it more discoverable, accessible, and usable across systems, web, mobile, and device applications. Even with this opportunity, most large organizations are still struggling with what data they have, where it is stored, and how to access it in a consistent, and meaningful way. Database culture within most organizations is just a mess, which contributes to why so many are freaking out about GDPR.</p>

<p>I’m guessing many companies are worried about complying with GDPR, as well as being able to even respond to any sort of regulatory policing event that may occur. This fear is going to force data stewards to begin thinking about the data the have on hand. I’ve already had conversations with some banks who are working on PSD2 compliant APIs, who are working in tandem on GDPR compliance efforts. Both are making them think deeply about what data they collect, where it is stored, and whether or not it has any value. Something I’m hoping will force some companies to stop collecting some of the data all together, because it just won’t be worth justifying its existence in the current cyber(in)secure, and increasingly accountable regulatory environment.</p>

<p>Doing APIs and becoming GDPR compliant go hand in hand. To do APIs you need to map out the data landscape across your organization, something that will contribute to GDPR. To respond to GDPR events, you will need APIs that provide access to end-users data, and leverage API authentication protocols like OAuth to ensure partnerships, and 3rd party access to end-users data are accountable. I’m optimistic that GDPR will continue to push forward healthy, transparent, and observable conversations around our personal data. One that focuses on, and includes the end-users who’s data we are collecting, storing, and often time selling. I’m hopeful that the stakes become higher, regarding the penalty for breaches, and shady brokering of personal data, and that GDPR becomes the normal mode of doing business online in the EU, and beyond.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/19/gdpr-forcing-us-to-ask-questions-about-our-data/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/19/facebook-cambridge-analytica-and-knowing-what-api-consumers-are-doing-with-our-data/">Facebook, Cambridge Analytica, And Knowing What API Consumers Are Doing With Our Data</a></h3>
        <span class="post-date">19 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/facebook/suspending-cambridge-analytica-facebook.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m processing <a href="https://newsroom.fb.com/news/2018/03/suspending-cambridge-analytica/">the recent announcement by Facebook to shut off the access of Cambridge Analytica</a> to it’s valuable social data. The story emphasizes the importance of having a real time awareness and response to API consumers at the API management level, as well as the difficulty in ensuring that API consumers are doing what they should be with the data and content being made available via APIs. Access to platforms using APIs is more art than science, but there are some proven ways to help mitigate serious abuses, and identify the bad actors early on, and prevent their operation within the community.</p>

<p>While I applaud Facebook’s response, I’m guessing they could have taken more action earlier on. Their response is more about damage control to their reputation, after the fact, than it is about preventing the problem from happening. Facebook most likely had plenty of warning signs regarding what Aleksandr Kogan, Strategic Communication Laboratories (SCL), including their political data analytics firm, Cambridge Analytica, were up to. If they weren’t than that is a problem in itself, and Facebook should be investing in more policing of their API consumers activity, as they claim they are doing in their release.</p>

<p>If Aleksandr Kogan has that many OAuth tokens for Facebook users, then Facebook should be up in his business, better understanding what he is doing, where his money comes from, and who is partners are. I’m guessing Facebook probably had more knowledge, but because it drove traffic, generated ad revenue, and was in alignment with their business model, it wasn’t a problem. They were willing to look the other way with the data sharing that was occurring, until it became a wider problem for the election, our democracy, and in the press. Facebook should have more awareness, oversight, and enforcement at the API management layer of their platform.</p>

<p>This situation I think highlights another problem of doing APIs, and ensuring API consumers are behaving appropriately with the data, content, and algorithms they are accessing. It can be tough to police what a developer does with data once they’ve pulled from an API. Where they store it, and who they share it with. You just can’t trust that all developers will have the platform, as well as the end user’s best interest in mind. Once the data has left the nest, you really don’t have much control over what happens with it. There are ways you can identify unhealthy patterns of consumption via the API management layer, but Aleksandr Kogan’s quizzes probably would appear as a normal application pattern, with no clear signs of the relationships, and data sharing going on behind the scenes.</p>

<p>While I sympathize with Facebook’s struggle to police what people do with their data, I also know they haven’t invested in API management as much as they should have, and they are more than willing to overlook bad behavior when it supports their bottom line. The culture of the tech space supports and incentivizes this type of bad behavior from platforms, as well as consumers like Cambridge Analytica. This is something that regulations like GDPR out of the EU is looking to correct, but the culture in the United States is all about exploitation at this level, that is until it becomes front page news, then of course you act concerned, and begin acting accordingly. The app, big data, and API economy runs on the generating, consuming, buying, and selling of people’s data, and this type of practice isn’t going to go away anytime soon.</p>

<p>As Facebook states, they are taking measures to reign in bad actors in their developer community by being more strict in <a href="https://www.facebook.com/help/792552774106866">their application review process</a>. I agree, <a href="http://apievangelist.com/2018/02/15/some-common-features-of-an-api-application-review-process/">a healthy application review process is an important aspect of API management</a>. However, this does not address the regular review of applications usage at the API management level, assessing their consumption as they accumulate access tokens, to more user’s data, and go viral. I’d like to have more visibility into how Facebook will be regularly reviewing, assessing, and auditing applications. I’d even go so far as <a href="http://apievangelist.com/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/">requiring more observability into ALL applications that are using the Facebook API, providing a community directory that will encourage transparency around what people are building</a>. I know that sounds crazy from a platform perspective, but it isn’t, and would actually force Facebook to know their customers.</p>

<p>If platforms truly want to address this problem they will embrace more observability around what is happening in their API communities. They would allow certified and verified researchers and auditors to get at application level consumption data available at the API management layer. I’m sorry y’all, self-regulation isn’t going to cut it here. We need independent 3rd party access at the platform API level to better understand what is happening, otherwise we’ll only see platform action after problems occur, and when major news stories are published. This is the beauty / ugliness of APIs. The cats out of the bag, and platforms need them to innovate, deliver resources to web, mobile, and device applications, as well as remain competitive. APIs also provide the opportunity to peek behind the curtain, and better understand what is happening, and profile the good and the bad actors within each ecosystem–let’s take advantage of the good here, to help regulate the bad.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/19/facebook-cambridge-analytica-and-knowing-what-api-consumers-are-doing-with-our-data/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/16/stops-in-a-comprehensive-api-strategy-transit-map/">68 Stops In A Comprehensive API Strategy Transit Map</a></h3>
        <span class="post-date">16 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/transit/calcagno-2017-01-01.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am refining my comprehensive list of stops that I highlight as part of the API lifecycle, or what I call <a href="https://apievangelist.com/2017/08/17/testing-out-the-concept-of-api-transit-instead-of-api-lifecycle/">API Transit</a>–the stops that each of the services we deliver will have “pass through” at some point. I’m sharing this list with other team members as part of existing consulting arrangements I’m engaged in with Streamdata.io, as well as baking into my API transit work for some workshops I’m doing in April. All of these are available on <a href="http://apievangelist.com/api-lifecycle/">the API lifecycle section of API Evangelist</a>, but I will be pushing them to become a first class citizen on the home page of API Evangelist once again.</p>

<p>Here are 68 potential stops I’d consider for any microservice to be exposed to, as part of a larger API transit strategy map:</p>

<ul>
  <li><strong>Definitions</strong> - From the simplest definition of what a service does, all the way to the complete OpenAPI definition for each service.</li>
  <li><strong>Design</strong> - The consistent design of each service leverage existing patterns, as well as the web to deliver each service.</li>
  <li><strong>Versioning</strong> - Plan for handling changes, and the inevitable evolution of each service, its definitions, and supporting code.</li>
  <li><strong>DNS</strong> - The domain addressing being used for routing, management and auditing of all service traffic.</li>
  <li><strong>Database</strong> - The backend database schema, infrastructure, and other relevant information regarding how data is managed.</li>
  <li><strong>Compute</strong> - How is the underlying compute delivered for each service using containers, serverless, cloud, and on-premise infrastructure.</li>
  <li><strong>Storage</strong> - What does storage of all heavy objects, media, and other assets involved with the delivery of services.</li>
  <li><strong>Deployment</strong> - Details services developed, and deployed, including frameworks and other libraries being used.</li>
  <li><strong>Orchestration</strong> - Defining what the build, syndication, and orchestration of service deployments and integrations look like, and are executed.</li>
  <li><strong>Dependencies</strong> - Identifying all backend service, code and infrastructure dependencies present for each service.</li>
  <li><strong>Search</strong> - Strategy for understanding what search will look like for each individual service, and play a role in larger, more comprehensive search across services.</li>
  <li><strong>Proxy</strong> - What proxies are in place to translate, cache, stream, and make services more efficient and secure?</li>
  <li><strong>Gateway</strong> - What gateways are in place to defend, protect, route, translate, and act as gatekeeper for service operations?</li>
  <li><strong>Virtualization</strong> - Details how how services and their underlying data are mocked, virtualized, sandboxed, and made available for non-production usage.</li>
  <li><strong>Authentication</strong> - What is involved with authentication across all services, including key-based approaches, basic authentication, JWT, and OAuth solutions.</li>
  <li><strong>Management</strong> - Information about how services are composed, limited, measured, reported upon, and managed consistently across all services.</li>
  <li><strong>Logging</strong> - What is being logged as part of service operations, and what is required to participate in overall logging strategy?</li>
  <li><strong>Portal</strong> - The strategy for how all services are required to be published to one or many public, private, and partner developer portals.</li>
  <li><strong>Documentation</strong> - The requirements for what documentation is expected as part of each service presence, defining what the services delivers.</li>
  <li><strong>Support</strong> - Relevant support channels, points of contact, and best practices for receiving support as an API consumer.</li>
  <li><strong>Communications</strong> - Information about the communication strategy around each service, and how blogs, social, and other channels should be leveraged.</li>
  <li><strong>Road Map</strong> - Details on a services road map, and what the future will hold, providing individual service, as well as larger organizational details on what is next.</li>
  <li><strong>Issues</strong> - Expectations, communications, and transparency around what the current bugs, issues, and other active problems exist on a platform.</li>
  <li><strong>Change Log</strong> - Translating the road map, and issues into a log of activity that has occurred for each service, providing a history of the service.</li>
  <li><strong>Monitoring</strong> - What is required when it comes to monitoring the availability and activity of each individual service.</li>
  <li><strong>Testing</strong> - The tactical, as well as strategic testing that is involved with each service, ensuring it is meeting service level agreements.</li>
  <li><strong>Performance</strong> - How is the performance of each service measured and report upon, providing a benchmark for the quality of service.</li>
  <li><strong>Observability</strong> - What does observability of the service look like, providing transparency and accountability using all of its existing outputs?</li>
  <li><strong>Caching</strong> - What caching exists at the server, CDN, and DNS layers for each service to provide a higher level of performance?</li>
  <li><strong>Encryption</strong> - Details regarding what is expected regarding encryption on disk, as well as in transport for each service.</li>
  <li><strong>Security</strong> - Detailed strategy and processes regarding how each service is secured on a regular basis as part of operations.</li>
  <li><strong>Terms of Service (TOS)</strong> - Considerations, and legal requirements applied to each service as part of the overall, or individual terms of services.</li>
  <li><strong>Privacy</strong> - Details regarding the privacy of platform owners, developers, and end users as it pertains to service usage.</li>
  <li><strong>Service Level Agreements (SLA)</strong> - Details regarding what service levels are required to be met when it comes to partner and public engagements.</li>
  <li><strong>Licensing</strong> - Information about backend server code, API surface area, data, and client licensing in play.</li>
  <li><strong>Branding</strong> - What branding requirements are in place for the platforms and its partners when it comes to the service.</li>
  <li><strong>Regulation</strong> - Information regarding regulations in place that effect service operations, and are required as part of its usage.</li>
  <li><strong>Discovery</strong> - How are services catalogued and made discoverable, making them accessible to other systems, developers, as well as to internal, partner, or public groups.</li>
  <li><strong>Client</strong> - Information about clients being used to interface with and work with the service, allowing it to be put to use without code.</li>
  <li><strong>Command Line Interface</strong> - The command line interface (CLI) tooling being used to developer or consume a service.</li>
  <li><strong>SDKs</strong> - What software development kits (SDK) are generated, or developed and maintained as part of a service’s operation?</li>
  <li><strong>Plugin</strong> - What platform plugins are developed and maintained as part of a services operations, allowing it to work with other platforms and services.</li>
  <li><strong>IDE</strong> - Are there integrated development environment (IDE) integrations, libraries, plugins, or availability considerations for each service?</li>
  <li><strong>Browsers</strong> - Are there any browser plugins, add-ons, bookmarklets and integrations used as part of each service’s operation?</li>
  <li><strong>Embeddable</strong> - Information about any embeddable badges, buttons, widgets, and other JavaScript enabled solutions built on top of a service?</li>
  <li><strong>Bots</strong> - What type of automation and bot implementations are in development or being supported as part of a service’s operation?</li>
  <li><strong>Visualization</strong> - Are there specific visualizations that exist to help present the resources available within any service?</li>
  <li><strong>Analysis</strong> - How are logs, APIs, and other aspects of a service being used as part of wider analysis, and analytics strategy?</li>
  <li><strong>Aggregation</strong> - Are there any aggregation solutions in place that involve the service, and depend on its availability?</li>
  <li><strong>Integration</strong> - What 3rd party integrations are available for working with a service in existing integration platforms like IFTTT, and Zapier.</li>
  <li><strong>Network</strong> - Information about networks that are setup, used, and allocated for each service governing how it can be accessed and consumed.</li>
  <li><strong>Regions</strong> - Which regions does a service operate within, making it available in specific regions, and jurisdictions–also which regions is it not allowed to operate within.</li>
  <li><strong>Webhooks</strong> - Are there webhooks employed to respond to events that occur via the service, pushing data, and notification externally to consumers?</li>
  <li><strong>Migration</strong> - What solutions are available for migrating an API between regions, cloud environments, and on-premise?</li>
  <li><strong>Backup</strong> - What types of backups are in place to bring redundancy to the database, server, storage, and other essential aspects of a service’s operations?</li>
  <li><strong>Real Time</strong> - Are there Server-Sent Events (SSE), Websocket, Kafka, and other real time aspects of a service’s delivery?</li>
  <li><strong>Voice</strong> - Are there any voice or speech enablement, integrating services with Alex, Siri, Google Home, or other conversational interface?</li>
  <li><strong>Spreadsheets</strong> - What types of spreadsheet integrations, connectors, and publishing solutions are available for a service?</li>
  <li><strong>Investment</strong> - Where do the funds for operating a service come from within a company, from external organizations, or VC funds?</li>
  <li><strong>Monetization</strong> - What does it cost to operate a service, breaking down the one time and recurring costs involved with delivering the service.</li>
  <li><strong>Plans</strong> - Outline of plans involved with defining, measuring, reporting, and quantifying value generated around a service’s operation.</li>
  <li><strong>Partners</strong> - An overview of partner program involved with a service’s operation, and how a wider partner strategy affects a service’s access and consumption.</li>
  <li><strong>Certification</strong> - Are there certification channels available for applications and developers, defining the knowledge required to competently operate and integrate a service.</li>
  <li><strong>Evangelism</strong> - What does internal, public, and partner evangelism efforts and requirements look like for a service, and its overall presence?</li>
  <li><strong>Showcase</strong> - How are developers and applications using a service showcased, and presented to the community, demonstrating the valuable around a service?</li>
  <li><strong>Deprecation</strong> - What are the plans for deprecating a service, involved the road map and communication around the individual and overall deprecation of service(s).</li>
  <li><strong>Training</strong> - What training materials need to be developed, or already exist to support the service.</li>
  <li><strong>Governance</strong> - How are all steps measured, quantified, aggregated, reported upon, and audited as part of a larger quality of service effort for each service.</li>
</ul>

<p>I struggle calling this a lifecycle, as most of these do not occur in any sort of linear fashion. This is why I’m resorting to using the API Transit approach, as it is something that reflects the ever-changing, but also hopefully eventually more consistent nature of delivering microservices. Ideally, every microservice passes through each of these stops in a consistent fashion, making sure that they operate in concert with a larger API platform strategy. However, I know what the reality on the ground is, and know that not all of these relevant to each organization I am talking with, requiring me to trim down and change the order in which I present these.</p>

<p>If you’d like to talk about how these stops apply to your API operations, feel free to reach out. I’m doing more API lifecycle and governance strategy consulting with my partners Streamdata.io, and I’m happy to help your company, organization, institution, or government agency develop your own API transit map, which can be used across your API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/16/stops-in-a-comprehensive-api-strategy-transit-map/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/14/a-visual-view-of-api-responses-within-our-documentation/">A Visual View Of API Responses Within Our Documentation</a></h3>
        <span class="post-date">14 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/webhose/webhose-io-visual-documentation.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>Interactive API documentation is nothing new. We’ve had Swagger UI, and other incarnations for over five years now. We also have API explorers, and full API lifecycle client solutions like Postman to help us engage with APIs, and be able to quickly see responses from the APIs we are consuming. In my effort to keep pushing forward the API documentation conversation I’ve  been <a href="http://apievangelist.com/2014/06/17/adding-data-visualization-layer-to-interactive-api-documentation/">beating the drum for more visual solutions to be baked into our interactive documentation for a while now</a>, encouraging providers to make the responses we receive much more meaningful, and intuitive for consumers.</p>

<p>To help drum up awareness to this aspect of API documentation I’m always on the lookout for any interesting examples of it in the wild. There was <a href="http://apievangelist.com/2014/06/04/another-strong-api-implementation-in-federal-government-with-openfda/">the interesting approach out of the Food and Drug Administration (FDA)</a>, and now I’ve seen one out of <a href="https://webhose.io/">the web data feeds API provider Webhose.io</a>. When you are making API calls in their interactive dashboard you get a JSON response for things like news articles on the left hand side, but you also get an interesting slider that will show a visual representation of the JSON response on the right side–making it much more readable to non-developers.</p>

<p>It provides a nice way to quickly make sense of API responses. Making them more accessible. Making it something that even non-developers can do. Essentially providing a reverse view source (view results?) for API responses. Taking the raw JSON, and providing an HTML lens for the humans trying to make decisions around the usage of a particular API. View source is how I learned HTML back in the mid 1990s, and I can see visualization tools for API responses helping average businesses users learn JSON, or at least make it a little less intimidating, and something they feel like they put to work for them.</p>

<p>I really feel like more visualizations baked into API documentation is the future of interactive API docs. Being able to see API responses rendered as HTML, or as graphs, charts, and other visualizations, makes a lot of sense. APIs are an abstract thing, and even as a developer, I have a hard time understand what is contained within each API response. I think having visual API responses will help us craft a more meaningful API request, making our API consumption much precise, and impactful. If you see any interesting visualization layers to your favorite API’s documentation, please drop me a line, I’d like to add it to my list of interesting approaches.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/14/a-visual-view-of-api-responses-within-our-documentation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/14/linkedin-account-validation-for-new-api-consumers/">LinkedIn Account Validation For New API Consumers</a></h3>
        <span class="post-date">14 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/dawex/requiring-linkedin-validation.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I was on-boarding with <a href="https://www.dawex.com/en/">the data marketplace Dawex</a> the other day, and I thought their on-boarding process was interesting. It is pretty rigid, requiring users to validate themselves in multiple ways, but it provides some interesting approaches to knowing more about who your API developers are. Dawex has the basic level email and phone number validation, but they have added a 3rd dimension, validating who you are using your LinkedIn profile.</p>

<p>After signing up, I was required to validate my email account–pretty standard stuff. Then I was asked to enter a code sent to my cell phone via SMS–not as common, but increasingly a way that platforms are using to verify you. Then I was asked to OAuth and connect my LinkedIn account to my Dawex profile. I’ve seen sign up and login using LinkedIn, but never using it as a 3rd type of validating who you are, and that you are truly a legitimate business user. I haven’t been verified yet, it says it will take up to 72 hour is what the notification at the top of my dashboard says, but it caught my attention.</p>

<p>Dawes also has a complete, <a href="https://en.wikipedia.org/wiki/Know_your_customer">Know Your Customer (KYC)</a> process, which takes the validation to another level, and something I’ll write about separately. I think the social profile validation provides an interesting look at how platforms validate who we are. In an environment where API developers will often sign up for multiple accounts, and do other shady things with anonymous accounts, I can get behind requiring consumers to prove who they are. I also think that providing robust, active, verifiable social media accounts using Github, Twitter, LinkedIn, and Facebook makes a lot of sense. I’d say that all four of these profiles represent who I am as a person, as well as a business.</p>

<p>I have talked about using Github to validate and rate API consumers before. I could see a world where I get validated upon signup using my social profile(s), and the amount of access to an API I am entitled to varies depending on how complete, robust, and verifiable my presence is. I’m not keen on giving up all my data to every API I sign up, but providing access to my profile so they can validate and rank who I am, and then get out of my way when it comes to using their resources, is something I can get behind. I could even envision where you throw Paypal into the mix, and my billing profile is further rounded off, allowing me to consume whatever I desire, and pay for what I used.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/14/linkedin-account-validation-for-new-api-consumers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/14/an-openapi-vendor-extension-for-defining-your-api-audience/">An OpenAPI Vendor Extension For Defining Your API Audience</a></h3>
        <span class="post-date">14 Mar 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/97_193_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>The clothing marketplace Zalando has an interesting approach to classifying their APIs based upon who is consuming them. It isn’t just about APIs being published publicly, or privately, they actually have standardized their definition, and have established an OpenAPI vendor extension, so that the definition is machine readable and available via their OpenAPI.</p>

<p><a href="http://zalando.github.io/restful-api-guidelines/index.html#219">According to the Zalando API design guide</a>, “<em>each API must be classified with respect to the intended target audience supposed to consume the API, to facilitate differentiated standards on APIs for discoverability, changeability, quality of design and documentation, as well as permission granting. We differentiate the following API audience groups with clear organisational and legal boundaries.</em>”</p>

<ul>
  <li><strong>component-internal</strong> - The API consumers with this audience are restricted to applications of the same functional component (internal link). All services of a functional component are owned by specific dedicated owner and engineering team. Typical examples are APIs being used by internal helper and worker services or that support service operation.</li>
  <li><strong>business-unit-internal</strong> - The API consumers with this audience are restricted to applications of a specific product portfolio owned by the same business unit.</li>
  <li><strong>company-internal</strong> - The API consumers with this audience are restricted to applications owned by the business units of the same the company (e.g. Zalando company with Zalando SE, Zalando Payments SE &amp; Co. KG. etc.)</li>
  <li><strong>external-partner</strong> - The API consumers with this audience are restricted to applications of business partners of the company owning the API and the company itself.</li>
  <li><strong>external-public</strong> - APIs with this audience can be accessed by anyone with Internet access.</li>
</ul>

<p><em><strong>Note:</strong> a smaller audience group is intentionally included in the wider group and thus does not need to be declared additionally. The API audience is provided as API meta information in the info-block of the Open API specification and must conform to the following specification</em>:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#/info/x-audience:
  type: string
  x-extensible-enum:
    - component-internal
    - business-unit-internal
    - company-internal
    - external-partner
    - external-public
  description: |
    Intended target audience of the API. Relevant for standards around
    quality of design and documentation, reviews, discoverability,
    changeability, and permission granting.
</code></pre></div></div>
<p><em><strong>Note:</strong> Exactly one audience per API specification is allowed. For this reason a smaller audience group is intentionally included in the wider group and thus does not need to be declared additionally. If parts of your API have a different target audience, we recommend to split API specifications along the target audience — even if this creates redundancies (rationale).</em></p>

<p>Here is an example of the OpenAPI vendor extension in action, as part of the info block:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>swagger: '2.0'
info:
  x-audience: company-internal
  title: Parcel Helper Service API
  description: API for &lt;...&gt;
  version: 1.2.4
</code></pre></div></div>
<p>Providing a pretty interesting way of establishing the scope and reach of each API in a way that makes each API owner think deeply about who they are / should be targeting with the service. Done in a way that makes the audience focus machine readable, and available as part of it’s OpenAPI definition which can be then used across discovery, documentation, and through API governance and security.</p>

<p>I like the multiple views of who the audience could be, going beyond just public and private APIs. I like that it is an OpenAPI vendor extension. I like that they even have a schema crafted for the vendor extension–another interesting concept I’d like to see more of. Overall, making for a pretty compelling approach to define the reach of our APIs, and quantifying the audience we are looking to reach with each API we publish.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/14/an-openapi-vendor-extension-for-defining-your-api-audience/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/14/rules-for-extending-your-api-with-each-version/">Rules for Extending Your API With Each Version</a></h3>
        <span class="post-date">14 Mar 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/23_19_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m spending time learning from the API design guides of other leading API providers, absorbing their healthy practices, and assimilating them into my own consulting and storytelling. One API design guide I am learning a lot from is out of <a href="https://adidas-group.gitbooks.io/api-guidelines/">the Adidas Group</a>, which contains a wealth of wisdom regarding not just the design of your API, but also the deployment and evolution of the API resources we are publishing.</p>

<p><a href="https://adidas-group.gitbooks.io/api-guidelines/content/core-principles/rules-for-extending.html">One particularly interesting piece of advice I found within Adidas API design guidance were their rules for extending an API</a>, which I think is pretty healthy advice for an API developer to think about.</p>

<p><em>Any modification to an existing API MUST avoid breaking changes and MUST maintain backward compatibility.</em></p>

<p><em>In particular, any change to an API MUST follow the following Rules for Extending:</em></p>

<ul>
  <li><em>You MUST NOT take anything away (related: Minimal Surface Principle , Robustness Principle)</em></li>
  <li><em>You MUST NOT change processing rules</em></li>
  <li><em>You MUST NOT make optional things required</em></li>
  <li><em>Anything you add MUST be optional (related <a href="https://en.wikipedia.org/wiki/Robustness_principle">Robustness Principle</a>)</em></li>
</ul>

<p><em>NOTE: These rules cover also renaming and change to identifiers (URIs). Names and identifiers should be stable over the time including their semantics.</em></p>

<p>First of all, I think many API developers aren’t even thinking about what constitutes a breaking change most of the time. So having any guidance that makes them pause and think about this topic is a good thing. Second, I think we should be sharing more stories about when things break, helping folks think more about these elements–the problem is that many folks are embarrassed they introduced a breaking changes, and would rather not talk about it, let alone make it publicly known.</p>

<p>I am working my way through <a href="http://apistylebook.com/">the API Stylebook</a>, learning from all the API design guides it has aggregated. There is a wealth of knowledge in there to learn from, and contains topics that make for great stories here on the blog. I wish more API providers would actively publish their API governance strategy, so that we can keep aggregating, and learning from each other. Making the wider API space more consistent, and hopefully more reliable along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/14/rules-for-extending-your-api-with-each-version/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/13/seeing-messy-api-design-practices-as-an-opportunity/">Seeing Messy API Design Practices As An Opportunity</a></h3>
        <span class="post-date">13 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/13439120_10154285627219813_3054276594176638940_n_copper_circuit_2.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m generating OpenAPI definitions for a wide variety of APIs currently, and I’m regularly stumbling on the messiness of the API design practices being deployed. When you are exposed to a large number of different APIs it is easy to get frustrated, begin ranting, and bitching about how ignorant people are of healthy, sensible API practices. This is the easy route. Making sense of it all, finding the interesting signals and patterns, and extracting where the opportunity are takes a significant amount of effort (so does biting tongue).</p>

<p>After profiling almost 500 API providers, I have almost 25K separate API paths indexed using OpenAPI, and APIs.json. I’ve tagged each API path using its OpenAPI definition. Pulling words from the path name, and any summary and description provided within the API documentation. Doing my best to describe the value contained within each API resource. Then I started grouping by these tags, to see what it produced. Sometimes the API paths it lists makes total sense, but most of the time it makes no sense at all. Then, I started noticing interesting patterns in how people describe their resources. Grouping things like “favorites” across all types of APIs, revealing some pretty interesting perspectives, and honest views of the resources being exposed.</p>

<p>Something as simple as “activities” can mean 30 or 40 different things when applied across CRM, storage, DNS, travel or sports APIs. At first, I’m like this shit is broken. The more time I spend with the mess, the more I’m starting to think there is more to this mess than meets the eye. I could be wrong. I often am. It is likely just my contrarian view of things, and my unique view of the API landscape in action. <a href="http://apievangelist.com/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/">Where many people see duplicative work, I see common patterns</a>. I just see the landscape differently than people who are just looking to get their work done, sell a product or service, and find an exit for their startup. I’m not looking for any solution, doorway, or exit. I just see all of this as a journey, and I’m fascinated by how people view their API resources, then describe, tag and bag them.</p>

<p>I’m not sure where all of this API indexing and tagging will lead me. I don’t feel compelled to sort out the mess, and covert any of these API providers to a more sensible API religion. I’m just looking to understand more about the motivations behind why they did what they did. Provide commentary on where they fit into the bigger picture, and if they are delivering interesting and valuable API resources, help make them more discoverable, and executable when it matters. As I have been doing for the las eight years, I’m just looking to learn from others, and stay in tune with where things are headed, no matter how messy it might be. I see APIs as a kind of controlled chaos, which are increasingly driving our already chaotic lives.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/13/seeing-messy-api-design-practices-as-an-opportunity/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/13/google-releases-a-protocol-buffer-implementation-of-the-fast-healthcare-interoperability-resources-fhir-standard/">Google Releases a Protocol Buffer Implementation of the Fast Healthcare Interoperability Resources (FHIR) Standard</a></h3>
        <span class="post-date">13 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/fhir/FHIR_logo-1080x675.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>Google is renewing its interest in the healthcare space by <a href="https://research.googleblog.com/2018/03/making-healthcare-data-work-better-with.html">releasing a protocol buffer implementation of the fast healthcare interoperability resources (FHIR) standard</a>. Protocol buffers are “Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler”. Its the core of the next generation of APIs at Google, often using HTTP/2 as a transport, while also living side by side with RESTful APIs, which use OpenAPI as the definition, in parallel to what protocol buffers deliver.</p>

<p>It’s a smart move by Google. Providing a gateway for healthcare data to find its way to their data platform products like Google Cloud BigQuery, and their machine learning solutions built on Tensorflow. They want to empower healthcare providers with powerful solutions that help onboard their data, and be able to connect the dots, and make sense of it at scale. However, I wouldn’t stop with protocol buffers. I would also make sure they also invest in API infrastructure on the RESTful side of the equation, developing OpenAPI specs alongside the protocol buffers, and providing translation between, and tooling for both realms.</p>

<p>While I am a big supporter of gRPC, and protocol buffers, I’m skeptical of the complexity it brings, in exchange for higher performance. Part of making sense of health care data will require not just technical folks being able to make sense of what is going on, but also business folks, and protocol buffers, and gRPC solutions will be out of reach of these users. Web APIs, combined with YAML OpenAPI has begun to make the API contracts involved in all of this much more accessible to business users, putting everything within their reach. In our technological push forward, let’s not forget the simplicity of web APIs, and exclude business healthcare users as IT has done historically.</p>

<p>I’m happy to see more FHIR-compliant APIs emerging on the landscape. PSD2 for banking, and FHIR for healthcare are the two best examples we have of industry specific API standards. So it is important that the definitions proliferate, and the services and tooling emerge and evolve. I’m hoping we see even more movement on this front in 2018, but I have to say I’m skeptical of Google’s role, as they’ve come and gone within this arena before, and are exceptional at making sure all roads lead to their solutions, without many roads leading back to enrich the data owners, and stewards. If we can keep API definitions, simple, accessible, and usable by everyone, not just developers and IT folks, we can help empower healthcare data practitioners, and not limit, or restrict them, when it is most important.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/13/google-releases-a-protocol-buffer-implementation-of-the-fast-healthcare-interoperability-resources-fhir-standard/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/13/multi-lingual-machine-learning-api-deployments/">Multi-Lingual Machine Learning API Deployments</a></h3>
        <span class="post-date">13 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/paralleldots/multi-lingual-website-676x507.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p><a href="https://hackernoon.com/launching-paralleldots-ai-apis-in-multiple-languages-2bed9eeff664">Machine learning API ParallelDots has a story on launching their APIs in multiple languages</a>. Allowing them to “serve a truly global customer base with following language options for our key APIs (Sentiment Analysis, Emotion Analysis, and Keyword generator)”. Something that I think more APIs providers are going to have to think about in coming years, as the need for API resources expands around the globe.</p>

<p>I’m tracking on the localization efforts of different API providers, and I’d say that having service availability in different regions, and multi-lingual support are the top two areas I’m seeing movement. I see two driving forces behind this, 1) the customers are demanding localization for their businesses, and 2) the governments in those countries are imposing regulations and other laws that dictate where resources can be stored and operate. I guess, something that can be seen as markets working things out, if you also believe in the value of regulations.</p>

<p>I also see a negative in all of this, specifically in this case, the imperialistic aspects of artificial intelligence and machine learning. Meaning, the models are trained here in western countries, but then being applied, injected, and imposed upon people in other countries. Selling the service as some sort of truth, watermark, and organic solution to sentiment, emotion, and intelligence without actually localizing the models. I’m not making any assumptions around ParralelDots motivations, just pointing out that this will be a problem, and should be ignored–if it makes you mad, then you are probably part of the problem.</p>

<p>As I conduct my API research, I will keep tracking on localization efforts like this. When I have the bandwidth I will dive in deeper and better understand how providers are defining these localization layers to their APIs in their API design, deployment, documentation, and other elements. I’ll be keeping an eye on which industries are moving fastest when it comes to API localization, and try to understand where it is deemed the most important by providers, consumers, and regulators. Lots to keep an eye on, and understand in 2018 when it comes to the expansion of the world of APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/13/multi-lingual-machine-learning-api-deployments/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/13/api-as-a-product-principles-at-zalando/">API As A Product Principles From Zalando</a></h3>
        <span class="post-date">13 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/zalando/zalando-api-guidelines.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>As I’m working through the API design guides from API leaders, looking for useful practices that I can include in my own API guidance, I’m finding <a href="zalando.github.io/restful-api-guidelines/">electronic commerce company Zalando’s API design guide</a> full of some pretty interesting advice. I wanted to showcase the section about their API as a product principles, which I think reflects what I hear many companies striving for when they do APIs.</p>

<p>From the Zalando API design guide principles:</p>

<p><em>Zalando is transforming from an online shop into an expansive fashion platform comprising a rich set of products following a Software as a Platform (SaaP) model for our business partners. As a company we want to deliver products to our (internal and external) customers which can be consumed like a service.</em></p>

<p><em>Platform products provide their functionality via (public) APIs; hence, the design of our APIs should be based on the API as a Product principle:</em></p>

<ul>
  <li><em>Treat your API as product and act like a product owner</em></li>
  <li><em>Put yourself into the place of your customers; be an advocate for their needs</em></li>
  <li><em>Emphasize simplicity, comprehensibility, and usability of APIs to make them irresistible for client engineers</em></li>
  <li><em>Actively improve and maintain API consistency over the long term</em></li>
  <li><em>Make use of customer feedback and provide service level support</em></li>
</ul>

<p><em>RESTful API as a Product makes the difference between enterprise integration business and agile, innovative product service business built on a platform of APIs.</em></p>

<p><em>Based on your concrete customer use cases, you should carefully check the trade-offs of API design variants and avoid short-term server side implementation optimizations at the expense of unnecessary client side obligations and have a high attention on API quality and client developer experience.</em></p>

<p><em>API as a Product is closely related to our API First principle which is more focused on how we engineer high quality APIs.</em></p>

<p>Zalando provides a pretty coherent vision for how we all should be doing APIs. I like this guidance because it helps quantify something we hear a lot–APIs as a product. However, it also focuses in on what is expected of the product owners. It also gets at why companies should be doing APIs in the first place, talking about the benefits they bring to the table.</p>

<p>I’m enjoying the principles section of Zalando’s API design guide. It goes well beyond just API design, and reflects what I consider to be principles for wider API governance. Many companies are still considering this API design guidance, but I find that companies who are publishing these documents publicly are often maturing and moving beyond just thinking deeply about design–providing a wealth of other wisdom when it comes to doing APIs right.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/13/api-as-a-product-principles-at-zalando/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/12/the-importance-of-tags-in-openapi-definitions-for-machine-learning-apis/">The Importance Of Tags In OpenAPI Definitions For Machine Learning APIs</a></h3>
        <span class="post-date">12 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/contrafabulists/machine+learning.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I am profiling APIs as part of my partnership with <a href="http://streamdata.io">Streamdata.io</a>, and my continued <a href="http://theapistack.com">API Stack</a> work. As part of my work, I am creating OpenAPI, Postman Collections, and APIs.json indexes for APIs in a variety of business sectors, and as I’m finishing up the profile for <a href="https://docs.paralleldots.com/">ParallelDots machine learning APIs</a>, I am struck (again) by the importance of tags within OpenAPI definitions when it comes to defining what any API does, and something that will have significant effects on the growing machine learning, and artificial intelligence space.</p>

<p>While profiling ParallelDots, I had to generate the OpenAPI definition from the Postman Collection they provide, which was void of any tags. I went through the handful of API paths, manually adding tags for each of the machine learning resources. I’m adding tags like sentiment, emotions, semantics, taxonomy, and classification, to each path. Trying to capture what resources were available, allowing for the discovery, filtering, and execution of each individual machine learning model being exposed using a simple web API. While the summary and description explain what each API does to developers, the tags are really the precise meaning in a machine readable context.</p>

<p>In the fast moving, abstract realm of machine learning, and artificial intelligence it can be difficult to truly understand what each API does, or doesn’t do. I struggle with it, and I’m pretty well versed in what is possible and not possible with machine learning. Precise tagging provide us with a single definition of what each machine learning API does, setting a baseline of understanding for putting ML APIs to work. Something that if I consistently apply across all of the machine learning APIs I’m profiling, I can can begin honing and dialing in my catalog of valuable API resources, and begin creating a coherent map of what is possible with machine learning APIs–helping ground us in this often hyped realm.</p>

<p>Once I have a map of the machine learning landscape established, I want to continue evolving my API ranking strategy to apply specifically to machine learning models being exposed via APIs. Not just understanding what they do, but also the quality of what they do. Are the machine algorithms delivering as advertised? Which APIs have a consistent track record in not just the reliability of the APIs, but also the reliability of the responses. Further bringing clarity to the fast moving, venture capital fueled, magical, mystical realm of artificial intelligence. Helping average business consumers better understand which APIs they can depend on, and which ones they might want to avoid.</p>

<p>I’m hoping my API Stack profiling will encourage more API providers to begin doing the heavy lifting of creating OpenAPI definitions, complete with tags themselves. We are always going to need the participation of the community to help make sure they are complete, and as meaningful as they can, but API providers will need to step up and invest in the process whenever possible. As the machine learning and artificial intelligence realms mature, we are going to need a meaningful vocabulary for describing what it is they do, and a ranking system for sorting out the good, the mediocre, and the bad. We’ll need all of this to be contained within the machine readable definitions we are using for discovery, and at runtime, allowing us to automate more efficiently using machine learning APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/12/the-importance-of-tags-in-openapi-definitions-for-machine-learning-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/12/some-common-api-data-types-to-put-to-use/">Some Common API Data Types To Put To Use</a></h3>
        <span class="post-date">12 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/carryload_dali_three.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m continuing my exploration of the API design guidance published by leading API providers. This time, I am taking a look at the API design guide publish by Adidas, <a href="https://adidas-group.gitbooks.io/api-guidelines/content/application/common-data-types.html">specifically the portion of it addressing what some of the common API types should be</a>. Providing some API design essentials, that all of us API providers should be baking into our APIs by default.</p>

<p>Here is the short list of <a href="https://adidas-group.gitbooks.io/api-guidelines/">default standards Adidas</a> is supporting across their API operations:</p>

<ul>
  <li><strong>Date and Time Format</strong> - Date and Time MUST always conform to the <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601 format</a> e.g.: 2017-06-21T14:07:17Z (date time) or 2017-06-21 (date), it MUST use the UTC (without time offsets).</li>
  <li><strong>Duration Format</strong> - Duration format MUST conform to the <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601 format</a> standard e.g.: P3Y6M4DT12H30M5S (three years, six months, four days, twelve hours, thirty minutes, and five seconds).</li>
  <li><strong>Time Interval Format</strong> - Time Interval format MUST conform to the <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601 format</a> standard e.g.: 2007-03-01T13:00:00Z/2008-05-11T15:30:00Z.</li>
  <li><strong>Standard Time Stamps</strong> - Where applicable, a resource representation SHOULD contain the standard timestamps: createdAt, updatedAt, and finishedAt, using the <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601 format</a>.</li>
  <li><strong>Language Code Format</strong> - Language codes MUST conform to <a href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes">the ISO 639</a> e.g.: en for English.</li>
  <li><strong>Country Code Format</strong> - Country codes MUST conform to <a href="https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2">the ISO 3166-1</a> alpha-2 e.g.: DE for Germany.</li>
  <li><strong>Currency Format</strong> - Currency codes MUST conform to <a href="https://en.wikipedia.org/wiki/ISO_4217">the ISO 4217</a> e.g.: EUR for Euro.</li>
</ul>

<p>These are the common data formats we are all using across our APIs. We should be working to support standard formats wherever possible, embracing the wider web, and investing in the longevity and usability of our API designs. Making our APIs sure that they speak to as wide of audience as they possibly can, and operate in a consistent way, no matter which API you are integrating with.</p>

<p>I want to also give a shout out to the <a href="http://apistylebook.com/">API Stylebook</a>, where I’m easily finding these API design guides that I’m using for these stories, and including in my API design guidance in my consulting work. Also, I’d like to encourage other API providers to reuse the patterns present in these guides, as well as publishing their own API design governance guidance whenever possible. It is critical that we all share these stories, so that we can learn from each other, and emulate the healthiest patterns already in use across the space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/12/some-common-api-data-types-to-put-to-use/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/12/thinking-about-delete-and-destroyting-entities/">Thinking About API Status Codes For Destroying Entities Using DELETE</a></h3>
        <span class="post-date">12 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/deliveroo/deliveroo-new-visual-branding-logo.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I am pulling together some API design guidance for some projects I’m consulting on, so I’m spending time reviewing the API design guides the leading API providers who have published them publicly. Learning from what they are doing across their own companies, organizations, institutions, and government agencies when it comes to sensible API governance.</p>

<p>Today, I am learning from the British food delivery company, Deliveroo, and documenting <a href="https://deliveroo.engineering/guidelines/api-design/#external-facing">their guidance for which HTTP status codes should be returned for any API methods that use DELETE</a>:</p>

<p>If it exists, it should return status:</p>

<ul>
  <li>204 No Content if the entity was successfully destroyed,</li>
  <li>404 Not Found if the entity does not exist</li>
  <li>410 Gone if the entity is known to have existed but no longer does.</li>
</ul>

<p>Additionally 4xx response codes may be used:</p>

<ul>
  <li>412 Precondition Failed</li>
  <li>415 Unsupported if using versioning and the server doesn’t support the specified version.</li>
</ul>

<p>I have to admit, I haven’t been properly publishing status codes any DELETE methods I provide. I’ve just been returning a 200 if successful, and 404 if it didn’t exist. I hadn’t put any further thought into the proper way of handling it. I just haven’t had the time, or the knowledge within reach to be able to handle properly.</p>

<p>I know that the RESTafarians enjoy debating these finer details of API design, and using HTTP, but for the rest of us, we just need some guidance from y’all. Which is why I spend time learning from existing API leaders who publish their API design guidance, and sharing as individual stories here on the blog, as well as including in my official project consulting guidance.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/12/thinking-about-delete-and-destroyting-entities/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/12/a-machine-readable-baseline-for-api-providers-in-my-api-stack-work/">A Machine Readable Baseline For API Providers In My API Stack Work</a></h3>
        <span class="post-date">12 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-definitions-api-stack.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m rebooting <a href="http://theapistack.com/">my API Stack</a> work as part of my partnership with <a href="http://streamdata.io">Streamdata.io</a>. I’m spending a significant portion of my day profiling API providers, documenting what it is they bring to the table. Historically I’ve published the resulting APIs.json and OpenAPI definition(s) to a single Github repository driving theapistack.com. The primary folder was already getting too big, and since I’m looking at adding at least a thousand more API providers to the stack, I am going to need to shard things out a bit.</p>

<p>To help accommodate the increased scale, I’m breaking up the API Stack into two separate Github organizations. One for individual API providers called <a href="https://github.com/api-stack-providers">api-stack-providers</a>, and a second for individual topics called <a href="https://github.com/api-stack-topics">api-stack-topics</a>. I’ve began publishing individual repositories for each API provider that I am monitoring, establishing a self-contained, continuously deployable, and integratabtle set of definitions for each API. I’m needing each API provider to be independent from each other, and I’m even publishing individual API definitions for each API path, distilling things down to the smallest possible unit of compute possible.</p>

<p>Each repository contains three main API definitions, using <a href="https://github.com/api-stack-providers/paralleldots">the ParallelDots machine learning API</a> as an example:</p>

<ul>
  <li><a href="https://github.com/api-stack-providers/paralleldots/blob/master/apis.yaml">APIs.json</a> - The entire index of the API provider’s operation.</li>
  <li><a href="https://github.com/api-stack-providers/paralleldots/blob/master/openapi/complete.yaml">OpenAPI</a> - The definition of the surface area of entire API.</li>
  <li><a href="https://github.com/api-stack-providers/paralleldots/blob/master/postmancollections/complete.json">Postman Collection</a> - An execute-time ready definition for the Postman client.</li>
</ul>

<p>I’m also publishing an individual OpenAPI for each API path within a listings folder, allowing me to provide the detailed execute-time definition I’m needing to work with each API. I’m also considering how I can do this with the Postman Collections. Next, I’m looking to understand how I can include the multiple source definitions within the repository, providing original copies of API definitions that I’m pulling from sources like <a href="http://apis.guru">APIs.guru</a>, <a href="https://any-api.com/">Any API</a>, and directly from API providers. Sharing the entire history behind how each set of API definitions has come together, and hopefully encouraging others to submit pull requests with fresh copies.</p>

<p>Ideally, each API provider is maintaining their own set of definitions in their own Github repository, like <a href="https://apievangelist.com/2017/05/22/box-goes-all-in-on-openapi/">Box</a>, <a href="https://apievangelist.com/2017/03/01/new-york-times-manages-their-openapi-using-github/">NY Times</a>, and <a href="http://apievangelist.com/2017/11/06/an-example-of-how-every-api-provider-should-be-using-openapi-out-of-the-slack-platform/">Slack</a> do, which I’ll just keep in sync with. However, for the rest, I’m looking to maintain an independent Github repository for each API providers, providing a single repo, URL, and set of issues for discussing the evolution of the OpenAPI, Postman, and APIs.json indexes. Each project will also have <a href="http://paralleldots.stack.network/">its own portal landing page</a>, with generated API docs. It’s a lot to maintain, but it is something I’m already doing behind the scenes in a database, and I might as well continue the work out in the open where I can solicit the help of the community, and build on the hard work of existing projects.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/12/a-machine-readable-baseline-for-api-providers-in-my-api-stack-work/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/09/defining-the-smallest-unit-possible-for-use-at-api-runtime/">Defining The Smallest Unit Possible For Use At API Runtime</a></h3>
        <span class="post-date">09 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/peachtree/peachtree-email-validation.png" width="45%" align="right" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">I’m thinking a lot about what is needed at API runtime lately</a>. How we document and provide machine readable definitions for APIs, and how we provide authentication, pricing, and even terms of service to help reduce friction. As Mike Amundsen (<a href="https://twitter.com/mamund">@mamund</a>) puts it, to enable “find and bind”. This goes beyond simple API on-boarding, and getting started pages, and looks to make APIs executable within a single click, allowing us to put them to use as soon as we find them.</p>

<p>The most real world example of this in action can be found with <a href="https://www.getpostman.com/docs/v6/postman_for_publishers/run_button/creating_run_button">the Run in Postman button</a>, which won’t always deal with the business and politics of APIs at runtime, but will deal with just about everything else. Allowing API providers to publish Run in Postman Buttons, defined using a Postman Collection, which include authentication environment details, that API consumers can use to quickly fire up an API in a single click. One characteristic I’ve come across that contributes to Postman Collections being truly executable is that they reflect the small unit possible for use at API runtime.</p>

<p><a href="https://developer.peachtreedata.com/Documentation#overview">You can see an example of this in action over at Peachtree Data</a>, who like many other API providers have crafted Run in Postman buttons, but instead of doing this for the entire surface area of their API, they have done it for a single API path. Making the Run in Postman button much more precise, and executable. Taking it beyond just documentation, to actually being more of a API runtime executable artifact. This is a simple shift in how Postman Collections can be used, but a pretty significant one. Now instead of wading through all of Peachtree’s APIs in my Postman, I can just do an address cleanse, zip code lookup, or email validation–getting down to business in a single click.</p>

<p>This is an important aspect of on-boarding developers. I may not care about wading through and learning about all your APIs right now. I’m just looking for the API solution I need to a particular problem. Why clutter up my journey with a whole bunch of other resources? Just give me what I need, and get out of my way. Most other API providers I have looked at in <a href="https://www.getpostman.com/api-network/">Postman’s API Network</a> have provided a single Run in Postman button for all of their APIs, where Peachtree has opted to provide many Run in Postman buttons for each of their APIs. Distinguishing themselves, and the value of each of their API resources in a pretty significant way.</p>

<p>I asked the questions the other week, regarding <a href="http://apievangelist.com/2018/02/15/how-big-or-small-is-an-api/">how big or how small is an API</a>? I’m struggling with this question in my API stack work, as part of an investment by Streamdata.io to develop an API gallery. Do people want to find Amazon Web Services APIs? Amazon EC2 APIs? Or the single path for firing up an instance of EC2? What is the small unit of compute we should be documenting, generating OpenAPI and Postman Collections for? I feel like this is an important API discovery conversation to be having. I think depending on the circumstances, the answer will be different. It is a question I’ll keep asking in different scenarios, to help me better understand how I can document, publish, and make APIs not just more discoverable, but usable at runtime.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/09/defining-the-smallest-unit-possible-for-use-at-api-runtime/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/09/the-postman-api-network/">The Postman API Network</a></h3>
        <span class="post-date">09 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/postman/postman-api-network.png" width="45%" align="right" style="padding: 15px;" /></p>
<p><a href="https://www.getpostman.com/api-network/">The Postman API Network</a> is one of the recent movements in the API discovery space I’ve been working to get around to covering. As Postman continues its expansion from being just an API client, to a full lifecycle API development solution, they’ve added a network for discovering existing APIs that you can begin using within Postman in a single click. Postman Collections make it ridiculously easy to get up and running with an API. So easy, I’m confounded why ALL APIs aren’t publishing Postman Collections with Run in Postman Buttons published in their API docs.</p>

<p><a href="https://www.getpostman.com/api-network/">The Postman API Network</a> provides a catalog of APIs in over ten categories, with links to each API’s documentation. All of the APIs in the network have a Run in Postman button available as part of their documentation, which includes them in the Postman API Network. It is a pretty sensible approach to building a network of valuable APIs, who all have invested in there being a runtime-ready, machine readable Postman Collection for their APIs. One of the more interesting approaches I’ve seen introduced to help solve the API discovery problem in the eight years I’ve been doing API Evangelist.</p>

<p>I’ve been talking to Abhinav Asthana (<a href="https://twitter.com/a85">@a85</a>) about the Postman API Network, and working to understand how I can contribute, and help grow the catalog as part of my work as the API Evangelist. I’m a fan of Postman, and an advocate of it as an API lifecycle development solution, but I’m also really keen on bringing comprehensive API discovery solutions to the table. With the Postman API Network, and other API discovery solutions I’m seeing emerge recently, I’m finding renewed energy for this area of my work. Something I’ll be brainstorming and writing about more frequently in coming months.</p>

<p><a href="http://apis.how/streamdata">Streamdata.io</a> has been investing in me moving forward the API discovery conversation, to build out their vision of a Streamdata.io API Gallery, but also to contribute to the overall API discovery conversation. I’m in the middle of understanding how this aligns with my existing API Stack work, APIs.json and APIs.io effort, as well as with APIs.guru, AnyAPI, and the wider OpenAPI Initiative. If you have thoughts you’d like to share, feel free to ping me, and I’m happy to talk more about the API discovery, network, and run-time work I’m contributing to, and better understand how your work fits into the picture.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/09/the-postman-api-network/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/09/keeping-track-of-federal-government-open-source-projects-using-the-code.gov-api/">Keeping Track Of Federal Government Open Source Projects Using The Code.gov API</a></h3>
        <span class="post-date">09 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/code-gov/code-gov-screenshot.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>Open source software is increasingly driving the federal government, <a href="https://fcw.com/articles/2017/10/04/oracle-rips-18f-usds.aspx">despite the wishes of companies like Oracle</a>. I’ve been watching an interesting project grow within the federal government to help quantify open source across the federal government called Code.gov, which “leverages the power of code sharing and collaboration to help the US Government cut down on duplicative software development and save millions of taxpayer dollars for the American people.” Something I think we can all get behind, when it comes to the collision of technology and government we are seeing play out.</p>

<p>Code.gov allows you to <a href="https://code.gov/#/explore-code">browse open source projects by government agency</a>, see the details for the project, visit the repository, and contact the project owners. The project even has <a href="https://code.gov/#/help-wanted">a help wanted section where you can roll up your sleeves and actually contribute to specific projects</a>, and help agencies push forward their project. You can follow up on what is happening using <a href="https://code.gov/#/roadmap">the Code.gov roadmap</a>, and stay in tune with whats next for the very important project. However, the aspect I’m most interested in, is the evolution of the Code.gov API.</p>

<p>There is <a href="https://github.com/GSA/code-gov-api">an open source API behind the Code.gov project</a>, providing programmatic access to government agencies repositories, the code, types of programming languages used, status, and other critical details for projects. There is <a href="https://api.code.gov/docs/">an OpenAPI-driven, Swagger UI documentation for the project</a>, providing a nice look at the programmatic surface for the project. You can use the API they have provided, or <a href="https://github.com/GSA/code-gov-api">you can fork the API and operate yourself</a>, providing an interesting look at delivering open source APIs that span multiple federal agencies–a blueprint that should be considered for other types of projects.</p>

<p>Code.gov reflects what I want to see come out of the federal government when it comes to technology. Open source. API-driven. Doing on thing, and doing it well. I can see spinoffs of the project emerge, focusing on just the environment, transportation, healthcare, and other verticals, helping shine a light on important projects going on across the federal government, attract and orchestrate talent and partnerships around those projects. Then build valuable dashboards, visualizations, and other applications on top of the APIs behind the project(s). I’ll be continuing to document Code.gov’s approach, and the API behind it, so that I can have the blueprint ready as part of my federal government API toolbox.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/09/keeping-track-of-federal-government-open-source-projects-using-the-code.gov-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/08/a-mock-only-api-development-reality/">Imagine A Mock Only API Development Reality</a></h3>
        <span class="post-date">08 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/gears_smoking_cigarette.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>Imagine if we didn’t actually write code when we developed our APIs initially. What if the bar for putting an API into production was so high, that many APIs never actually made it to that level. I spend a lot of time mocking and prototyping APIs, but I don’t have a strict maturity model determining when I actually bring an API to life using code. I’d love it if I never actually write code for many of my API prototypes and just mocked them, and iterated upon the mocks instead of thinking they needed actually have a database backend, and code or gateway front-end.</p>

<p>What if I only executed these stops along the API lifecycle, instead of actually writing any code:</p>

<ul>
  <li><strong>Define</strong> - Define what I want my API to do, and what types of resources will be involved.</li>
  <li><strong>Design</strong> - Actually design my API and underlying schema using an OpenAPI definition.</li>
  <li><strong>Mock</strong> - Publish a mock representation of my API, complete with virtualized data behind responses.</li>
  <li><strong>Document</strong> - Make sure and publish documentation to a simple developer portal.</li>
  <li><strong>Plan</strong> - A overview of what it will cost, and the model for managing consumption.</li>
  <li><strong>Testing</strong> - Set up tests to ensure each API is up and doing what it was intended to do.</li>
  <li><strong>Communicate</strong> - Actually write stories, and communicate around what an API does.</li>
  <li><strong>Support</strong> - Establish and maintain feedback loops around the API to listen to consumers.</li>
  <li><strong>Road Map</strong> - Gather feedback into a road map, and plan next version, complete with a change log.</li>
</ul>

<p>In this life cycle I would never actually write any code. I would never actually setup a database backend. Everything would be mocked and virtualized, but seem as real as I possibly can. Even maybe providing different types of virtualized datasets and responses, so people can experience different types of responses or scenarios. I’d work hard to make sure each API was as complete, and function like a real APIs, as I possibly could.</p>

<p>Now imagine within a company, if I had to do all of this, and before I would ever be allowed to begin developing my API for real, I had to sell my API. Show its value, and get some consumers to buy into my overall API design, and even develop some mock integrations showing the potential. Before my API would be considered for inclusion in the production road map, I’d have to prove that my approach was mature enough in each of the areas outlined above:</p>

<ul>
  <li><strong>Define</strong> - Robust OpenAPI and JSON schema present.</li>
  <li><strong>Design</strong> - A complete design meeting all governance guidelines.</li>
  <li><strong>Document</strong> - Complete, up to date API document present.</li>
  <li><strong>Plan</strong> - A clear definition of costs, and how consumption will be measured.</li>
  <li><strong>Testing</strong> - A full suite of tests available with 100% coverage.</li>
  <li><strong>Communicate</strong> - Demonstrated ability to communicate around its operations.</li>
  <li><strong>Support</strong> - Demonstrated ability to support the APIs operation.</li>
  <li><strong>Road Map</strong> - Clearly articulated road map, and change log for the API.</li>
</ul>

<p>As an API developer, once I’ve build the case for my microservice, presented it, and demonstrated its value–then I would be approved to actually begin developing it. Maybe in some cases I would be required to iterate upon it for another version, and engage with a larger audience before actually given the license and budget to do it for real. Going well beyond just an API design first reality, and enforcing a maturity model around how we define and communicate around our services, and raising the bar for what gets let into production.</p>

<p>Many of the companies, organizations, institutions, and government agencies I’m working with are having a hard time adopting a define and design first strategy, let alone a reality such as this. Imagine how different our production worlds would look if you had all this experience before you were ever given the license to write code. Think about how much worthless code gets written, when we should just be iterating on our ideas. How much of our budget gets wasted on writing code that should never have existed to begin with. It’s a nice thought. However, it is just a fantasy.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/08/a-mock-only-api-development-reality/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/08/your-obbsessive-focus-on-the-api-resource-is-hindering-meaningful-events-from-happening/">Your Obsessive Focus On The API Resource Is Hindering Meaningful Events From Happening</a></h3>
        <span class="post-date">08 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/berlin_wall_graham_sutherland.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve been profiling a number of market data APIs as part of my work with <a href="http://apis.how/streamdata">Streamdata.io</a> to identify valuable sources of data that could be streamed using their service. A significant portion of the APIs I come across are making it difficult for me to get at the data they have because of their views around the value of the data, intellectual property, and maintaining control over it in an API-driven world. These APIs don’t end up on the list of APIs I’m including in the profiling work, the gallery / directory, and don’t get included in any of the stories I’m telling, as a result of this tight control.</p>

<p>The side effect of this is I end up getting repeated sales emails and phone calls asking if I am still interested in their data. If there was just one or two of these, I’d jump on phones and explain, but because I’m dealing with 50+ of them, I just don’t have the bandwidth, and I have to move on. The thing is, I’m personally not interested in their data. I’m interested in other people being interested in their data, and being an enabler to helping them to get at it. However, since I can’t actually profile the APIs, create OpenAPI definitions for the request and response structure for inclusion in the API gallery / directory I’m building, I really don’t need their APIs in my work.</p>

<p>I know these platforms are protective of their data because it is valuable. They should be. However modern API management allows for them to open up the sampling of everything they have to offer, without giving away the farm. This allows enablers, analysts, and storytellers like me to test drive things, profile what they have to offer, and include within our applications. Then my users find what they want, head over to the source of the data, sign up for API keys, talk to their sales staff about what data sets they are interested in. I’m just a middle man, a broker, someone who is looking to enable engagements with their data. I’m only interested in the data, because I understand it is valuable to others, not because I am personally interested in doing anything with it.</p>

<p>This reality is common amongst data brokers who live in the pre-API era. They don’t understand API management, and they don’t understand how innovation using APIs work. They still rely on a pretty closed, tight-gripped approach to selling data. API enablers like me don’t have the time to mess around in these worlds. There are too many APIs out there to waste our time. I’m looking to profile the best quality market data source, that are frictionless to get up and running with. I’m not looking for free data, I understand it costs to get at. I just want to send leads their way, but I need to be able to profile what you have to offer in detailed, in a machine readable way. In the end, it doesn’t matter, because these providers won’t be around for long. Other, more API-savvy data providers will emerge, and run them out of business–it is the circle of API life.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/08/your-obbsessive-focus-on-the-api-resource-is-hindering-meaningful-events-from-happening/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/08/axway-asking-for-an-openapi-of-a-streamdata-io-api-so-they-can-screenshot-it/">Axway Asking for an OpenAPI of The Streamdata.io API So They Can Screenshot It</a></h3>
        <span class="post-date">08 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/beachclouds_clean_view.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>We are working closely with <a href="https://www.axway.com/en">Axway</a> on a number of projects over here at <a href="http://apis.how/streamdata">Streamdata.io</a>. After we got out of a meeting with their team the other day we received an email from them asking if we had an OpenAPI definition for a demo Streamdata.io market data API. They were wanting to include it in some marketing materials, and needed a screenshot of it. To be able to generate the visual they desired, they needed an OpenAPI to make it tangible enough for capturing in a screenshot and presenting as part of a larger story.</p>

<p>This may sound like a pretty banal thing, but when you step back and realize the importance of OPenAPI when it comes to communication, and making something very abstract a tangible, visual thing, it becomes more significant. You can tell someone there is a market data API, but taking a screenshot of documentation generated via an OpenAPI which displays the market data paths, a couple of parameters like stocker ticker symbol and maybe date range, and then plug in some actual values like the ticker symbol for AAPL, and show the JSON response takes things to a new level. This is OpenAPI empowered storytelling, marketing and communications in my book. Elevating what OpenAPI brings to the table to new stops along the API life cycle.</p>

<p>This isn’t just about documentation. This is about making an abstract API concept more visual, more meaningful, and able to be captured in an image. Axway is trying to demonstrate the value of their API solutions, coupled potentially with Streamdata.io services, in a single image–providing a lot more rich context, and visualizations that amplify their marketing materials. This isn’t just documenting what is going on so that developers know what to do with an API, this is telling stories so that business users understanding what is possible with an API–using a machine readable format like OpenAPI to help deliver the 1000 words the image will be worth.</p>

<p>Using OpenAPI like this reflects where I’d like to see API documentation go. Sure, we still need dynamic API documentation driven by OpenAPI definitions for developers to understand what is going on, but we need more snippets, visualization, and emotion driving solutions to exist. Things that marketers, bloggers, and other storytellers can use in their materials. We need OpenAPI-driven tools that help them plug in a relevant API definition, and generate a meaningful visual that they can use in a slide deck, blog post, or other material. We need our API documentation to speak beyond the developer community and become something that anyone can put to work in their API storytelling efforts–no coding required.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/08/axway-asking-for-an-openapi-of-a-streamdata-io-api-so-they-can-screenshot-it/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/07/where-do-we-start-the-api-conversation-at-our-large-organization/">Where Do We Start The API Conversation At Our Large Organization?</a></h3>
        <span class="post-date">07 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/cityscape_dali_three.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>It can be tough to know where to start with APIs at your large organization. Everyone is already in motion, in a variety of groups, geographic regions, serving different lines of business. The API conversation is already occurring across your organization, it is just happening in a distributed and fragmented way, without any coherent orchestration, and strategy moving it forward in unison. I’d like to sit down with your team(s), and understand what APIs look like across your organization, and help work through a first draft of your API strategy, and define how we can better orchestrate existing efforts, and begin establishing an API governance framework for bringing the API vision into focus.</p>

<p>Let’s put together a schedule for me to sit down with a variety of teams in a partial day, full day, or multi-day series of workshops. Where we can work to define a more comprehensive, and coherent strategy for APIs. One that is rooted in what is already happening, but works to brig together the disparate voices from each team into a single conversation about how APIs can make change across your organization in a more consistent, and meaningful way. Let’s sit down and begin to define what APIs mean at your organization and map of what is already occurring, and develop framework that will allow us to begin to move things forward.</p>

<p><strong>Define Objectives</strong><br />
I’d like to learn more about the objectives behind your investment in an organization-wide API strategy, and understand more around the motivations, vision behind what you are looking to achieve. I’d like to cut through the hype, and get to the real reasons you want to be doing APIs.</p>

<ul>
  <li><strong>Internal</strong> - What benefits should APIs bring to internal teams within the organization?</li>
  <li><strong>Partner</strong> - How will APIs be used to further partner engagements, and maximize their deliverables?</li>
  <li><strong>Public</strong> - What can be achieved by performing APIs in the public sphere, and engaging 3rd party developers, and media?</li>
</ul>

<p>I’d like everyone to bring their stories of how APIs are currently being done within their groups, and how each team sees the future of ther API programs. As well as learning from leadership about what they’d like to achieve by doing APIs in a more coordinated fashion across the organizations and lines of business.</p>

<p><strong>Define Organization</strong><br />
To help move forward the conversation, I’d like to learn more about your organizational structure, and understand the cultural makeup of all groups, individuals, the leadership structures in place–all of which will impact how APIs are realized (or not).</p>

<ul>
  <li><strong>Groups</strong> - What groups will be involved with defining, deploying, managing, and consuming APIs?</li>
  <li><strong>Actors</strong> - Who are the different types of actors involved in shaping what APIs look like within your organization.</li>
  <li><strong>Partners</strong> - What external partners will immediately impact how APIs are done, and the value they can bring to the table?</li>
</ul>

<p>Your organizational map will have everything to do with why APIs are working or not working. Your organizational DNA will help define the impact APIs can have internally, as well as externally with partners and the wider public.</p>

<p><strong>Define Architecture</strong><br />
Next, I’d like to get a handle on the architecture that is currently in place across your organization. Understand the current technological makeup of how your organization gets business done on a day to day basis. Learning about how technology is being leveraged to deliver services today, and will continue as part of a wider API strategy.</p>

<ul>
  <li><strong>Platforms</strong> - Which of the major cloud platforms are you operating on?</li>
  <li><strong>Systems</strong> - What major systems are in place that drive services today?</li>
  <li><strong>Services</strong> - What external services do you currently put to use across operations?</li>
  <li><strong>Tools</strong> - What are the open source tools in use across your organization?</li>
</ul>

<p>I’m looking to understand how web services, APIs, and applications are delivered (and consumed) today. I’d like to learn more about the technical makeup of your organization so that I can better understand the current state of things, and how existing technology can be leveraged to deliver the next generation of APIs.</p>

<p><strong>Define Services</strong><br />
Then, I’d like to paint a landscape picture of APIs across your organization, starting with the past, understanding how you have gotten to where you are today. Identify existing, successful APIs that are in place, while also getting to work on what the future will bring.</p>

<ul>
  <li><strong>Legacy</strong> - What web services are already in place to make things work across organizations?</li>
  <li><strong>Existing</strong> - What are the existing APIs already in place which we can use as a blueprint for how APIs should be done?</li>
  <li><strong>Future</strong> - I’d like to learn about what the future will hold when it comes to APIs across the organization.</li>
</ul>

<p>This process can be as high level, or as technical as needed, depending on the different actors we assemble at the table. I’d like to learn about the services in terms of business leadership, as well as the finer technical views–of course, being respectful of the time we have.</p>

<p><strong>Define Lifecycle</strong><br />
With an understanding of the organization, and the existing architecture and services in play, let’s start defining the lifecycle(s) that exists for moving each individual APIs forward, understanding all the stops they’ll pass through from design to deprecation.</p>

<ul>
  <li><strong>Defining</strong> - How are APIs currently defined, from interface to schema?</li>
  <li><strong>Designing</strong> - What does API design look like across the organization?</li>
  <li><strong>Mocking</strong> - Is virtualization of APIs and data a common practice?</li>
  <li><strong>Documenting</strong> - How are APIs documented and detailed for wider usage?</li>
  <li><strong>Clients</strong> - What interfaces, SDKs, IDEs, and client tooling are used for API development?</li>
  <li><strong>Deploying</strong> - What does the current API build and deploy process look like?</li>
  <li><strong>Managing</strong> - What management and gateway layers exist as part of API operations?</li>
  <li><strong>Planning</strong> - How are costs and value generation quantified and measured involving APIs?</li>
  <li><strong>Testing</strong> - What are the current API monitoring, testing, and performance practices look like?</li>
  <li><strong>Security</strong> - How is security defined, evaluated, and responded to across API operations?</li>
  <li><strong>Legal</strong> - How do terms of service, privacy policies, licensing, and other regulations get addressed?</li>
  <li><strong>Discovery</strong> - How are APIs and services catalogued and discovered throughout operations?</li>
  <li><strong>Support</strong> - What does support look like when it comes to the API lifecycle?</li>
  <li><strong>Communications</strong> - What is expected around communications across teams for each API?</li>
  <li><strong>Evangelism</strong> - How are APIs evangelized internally, and externally amongst partners?</li>
</ul>

<p>I’m looking to develop an understanding of how the API lifecycle is currently approached, while also discussing what the first draft of the future organizational wide API lifecycle will look like. Something we can put put into action immediately, and begin measuring as part of an overall governance strategy.</p>

<p><strong>Define Governance</strong><br />
We should now have enough to begin talking about how a comprehensive API strategy can be put into place. How individual APIs will be developed and managed, as well as the organizations and actors who will be responsible for their existence. I’d like to talk about how we’ll establish a wider understanding of API operations, measure what is happening, and begin orchestrating its movement in a organized way:</p>

<ul>
  <li><strong>Mapping</strong> - Let’s take the map of the organization, architecture, and lifecycle, and talk about how we’ll turn it into a living map of API operations.</li>
  <li><strong>Measuring</strong> - What metrics will we use to measure our progress, and understand how APIs are moving forward throughout the lifecycle in a consistent way.</li>
  <li><strong>Maturity</strong> - How do we define maturity throughout API operations, understanding not all APIs will be production ready, and always allowing for innovation.</li>
  <li><strong>Reporting</strong> - Let’s discuss how we’ll be reporting on API operations, and communicating with leadership, and across teams about the progress being made.</li>
  <li><strong>Incentives</strong> - What are the incentives we can use to help make sure everyone is a team player, and working towards a common goal across the organization?</li>
  <li><strong>Coaching</strong> - I’d like to better understand how we can implement an evangelism and coaching strategy as part of the wider API governance execution.</li>
  <li><strong>Auditing</strong> - How will we be auditing the API governance strategy, as well as the wider API operations, and understanding where the gaps are?</li>
  <li><strong>Refining</strong> - Let’s make sure everyone understands that API governance will require constance refining, and evolving to make it work as expected.</li>
</ul>

<p>I’d like to understand what API governance means to your organization. Is it more carrot, or more stick. I’d like to understand how a draft governance plan can be put into place immediately, but fully knowing that it will be a living, evolving, and maturing approach to ensuring quality and consistency of delivering APIs at scale across your organization.</p>

<p><strong>Initial, As Well As Ongoing Engagement</strong><br />
Let’s get to work setting up an initial engagement. I’ll leave to you to decide whether it should be a partial, full, or multi day affair. This work to define an API strategy can be invested in at multiple scopes (high level, or in detail), and should continue to live on, expand, and evolve with each engagement. Ideally, it is is something that is worked through with as many teams possible, then repeated as part of a recurring schedule, as time allows. As the map of the organization, the existing architecture and service landscape, lifecycle and governance comes into focus, these working sessions will become even more important, and productive.</p>

<p>I’d like to ask anyone joining the conversation to bring any answers, thoughts, concerns, and questions they have regarding this outline with them. They are welcome to be as high level, or down in the technical weeds as they like–I’ll absorb it all. Ideally, there is a mix of business and technical folks present, with varying levels of leadership in attendance reflecting how API decision making occurs, or will occur across your organization. I’m looking forward to learning more about APIs across your organization, and learning from everyone at the table, while also sharing my knowledge and experience from studying the API sector for the last eight years.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/07/where-do-we-start-the-api-conversation-at-our-large-organization/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/07/us-companies-getting-ahead-of-eu-regulations/">US Companies Getting Ahead Of EU Regulations</a></h3>
        <span class="post-date">07 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/gargoyle-nd-paris_atari_missle.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I find it to be a telling sign of the culture at US companies when it comes to their response, or lack of response to EU regulations. I’ve been curating stories from the API providers that I track on when it comes to GDPR or PSD2, keeping a notebook or research that is ready when I get around to diving in deeper. The companies who are openly talking about these regulations and being proactive about responding to them, are usually the API providers who have a strong stance in the US market, and are poised to, or already expanding this reach to Europe. Companies who haven’t made any noise are probably not concerned with the European market, or just hoping the regulations fizzle out I guess?</p>

<p>After diving into my curation notebook the first company to stand out is <a href="https://auth0.com/">Auth0</a>, with a variety of blog posts, and resources on navigating both PSD2 and GDPR. Auth0 is in a good position to provide critical authentication and user information management APIs to other companies who are working to comply with the regulations, so it makes sense that they would be getting ahead of all of this. I fully grasp that many companies are simply issuing their press releases stating they can help with GDPR or PSD2, but you can quickly cut through the fluff by looking at how much they’ve invested in their response, materials, and services. <a href="https://auth0.com/docs/compliance/gdpr">Auth0 has a pretty extensive knowledge-base on GDPR</a>, <a href="https://auth0.com/blog/open-auth-standards-psd2/">providing PSD2 guidance on their blog</a>, as well as <a href="https://auth0.com/blog/auth0-europe-launches/">investing in their EU region for a couple of years</a>–demonstrating it is more than just a press release.</p>

<p>I’m working my way through the list of US API providers, and service providers who are being active on the subject of EU regulations. I feel it is an important sign of the strength of the company, and demonstrates a healthy understanding of how regulations aren’t always bad, and that they can actually help industries thrive. I’m actively working on projects involving GDPR and PSD2 in Europe, and I’m eager to develop my understanding of how these regulations are changing the face of the technology industry in Europe, but also how this will impact US companies. I’m hoping that it will begin to shift and evolve the culture around data ownership, privacy, and APIs as industry standards in the US. I feel that as Internet technology matures, we are going to need to view data a little different, otherwise things won’t be sustainable. I’m hopeful that EU regulations can help set this into motion–we’ll see, maybe I’m naive.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/07/us-companies-getting-ahead-of-eu-regulations/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/06/explaining-api-security-to-organizational-leadership/">Explaining API Security To Organizational Leadership</a></h3>
        <span class="post-date">06 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/castle-on-hill-edinburgh_propaganda_leaflets.JPG" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been tasked with helping explain API security to senior leadership, and wanted to work through my ideas here on the blog. For this audience, I’m not going to get down into the weeds regarding the technical specification behind OAuth, and other approaches, and try to keep things high level, introducing folks to the art that is API security. The phrase API security represents a balance of concepts because APIs are by nature about providing access, while security is about controlling and sometimes limiting access, resulting in a new way of getting business done on the open web.</p>

<p><strong>First, What Are APIs?</strong><br />
APIs are not the latest trend, or vendor solution, they are the next evolution in the web. Web sites and applications return HTML via a URL, and meant to display information to humans in a browser, while APIs return JSON or XML of the same information, but meant to be used in other applications and systems. API security is designed to allow access to our digital resources using the web, while also securing it in a way to ensure only the intended audience is able to obtain  access. APIs are designed to securely provide access to data, content, media, and algorithms using the same web that us humans use to access information online via our browsers.</p>

<p><strong>Access Using Secure URLs</strong><br />
APIs use web URLs get read and write data, content, media, and to allow engagement with algorithms. If you want a list of press releases, you visit https://api.example.com/press/. If you want a list of contacts from the CRM, you visit https://api.example.com/contacts/. The URL for all API resources should be encrypted by default, protecting all requests and responses in transit. Providing the first layer of security for APIs, ensuring only approved consumers can view data, content, media, and valuable algorithms being transmitted online.</p>

<p><strong>Registration Always Required For APIs</strong><br />
A common misconception about web APIs is that they are all like Twitter, and are publicly available on the web. However, almost all web APIs actually require that you register before you get any access to any resources. This process is the beginning of what is called API management, where developers have to sign up for an account, and in some cases be approved before they get access to APIs. Most of the time there is self-service, automatic registration, but developers only get limited access, which once they’ve been approved, proven their identity, or put in a credit card–will be able to obtain higher levels of access to resources.</p>

<p><strong>Application Keys For Each API Call</strong><br />
Once developers have been approved for access, they can begin making API calls. However, each API will require that API keys, and required credentials are present with each call. Providing identification of every API consumer, and exactly what they are consuming. API keys are often seen as all that is needed to properly securing APIs, but in reality, they are much more about identifying and tracking what API consumers are doing. Going beyond just focusing on securing the digital resources, and really developing an awareness of who is accessing what, which will prove to be more valuable than just requiring registration to access APIs alone.</p>

<p><strong>Suite Of Authentication Options</strong><br />
There are a handful of approaches in use when it comes to requiring developers to authenticate and pass along their API keys with each response. Each approach has pros and cons, but the industry has widely settled in on four main ways to require API developers to authenticate themselves when using APIs:</p>

<ul>
  <li><strong>Basic Auth</strong> - Usage of the basic authentication format that is part of the standard HTTP operations, employing a username and password as credentials for accessing API resources.</li>
  <li><strong>Key Access</strong> - Providing simple tokens, often called API key as a common way to access to APIs, issuing one to each developer and per application they register.</li>
  <li><strong>JSON Web Token</strong> - JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.</li>
  <li><strong>oAuth</strong> - Providing an oAuth layer to API operations, securing high value APIs, while also opening up a conversation between an API platform, developers, and end-users regarding the access of their content and data.</li>
</ul>

<p>Depending on the security requirements of the resource, and whether or not users generated data and content is involved, you may select a different path for how you require API developers to authenticate. API keys via the URL or headers, as well as Basic Auth are the most common. with JSON Web Tokens, and OAuth being put to work in higher security environments, and where users data, and permission might be required.</p>

<p><strong>Logging Of All API Activity</strong><br />
A essential aspect of securing APIs involves the logging of ALL API calls, no matter who the consumer is, what application it is serving, and whether it is for internal, or external consumption. All API calls get logged equally, and when all API developers are required to authenticate and pass their keys with each API call, all the evidence needed to understand API activity and consumption is present. When you combine the API level logging with backend database logs, and front-end DNS logs, you can define a set of perimeters that will help ensure the security of your API resources.</p>

<p><strong>Modern API Management Solutions</strong><br />
API management combines the authentication and logging described above with API plans, rate limiting, and analytics, to achieve a heightened awareness regarding who is access what API resources, and how they are putting them to use. Modern API management allows for the monitoring of API registrations, authentication, and consumption in real time, with controls for limiting or shutting off access whenever terms of service and security violations are identified. Providing API providers with the tools they need to monitor and respond to any security concerns, while staying in tune with exactly how resources are being accessed via APIs.</p>

<p><strong>Defining Access To APIs Using Service Composition</strong><br />
API management also allows API providers to develop different levels of API access plans, which govern the APIs that developers will have access to, and how much they are entitled to consume. API service composition is all about organizing different APIs into different plans, and setting rate limits that govern how much a user can use per second, minute, days, or by the month. New users are often placed into plans with stricter limitations, while more trusted, partner, and internal consumers enjoy higher rate limits, and less restrictive plans. Service composition helps minimize the damage that occurs whenever there are bad actors present, or security incidents occur, keeping the breaches limited to a small subset of low value APIs, and limited amount of resources accessed.</p>

<p><strong>Monitoring All API Availability</strong><br />
Beyond API management, and the analysis of API consumption, it is common for API providers to setup external monitors that keep an eye on whether APIs are up or down, and what their overall availability are. Providing a status dashboard showing whether APIs are available, which also often shows historical availability over time. The health and availability of an API is usually a barometer of the security of an API. Insecure, and compromised APIs often times have an unreliable availability and track record. Making monitoring critical to the overall security of API operations.</p>

<p><strong>Granular Testing of All APIs</strong><br />
Augmenting API management and monitoring, the most mature and secure API providers out there also run recurring tests on APIs, going beyond just seeing if they are up and available, and actually making sure they respond, and deliver the data and content that is expected. These tests will sometimes go further and test for the ability to publish bad data to APIs, input incorrect or additional information, and push the boundaries of what an API will respond to. Mimicking some of the behaviors in which malicious users and applications will perform, and testing the quality of the surface area of API.</p>

<p><strong>Security Scans For All APIs</strong><br />
Beyond granular tests for all APIs, more general security scans are regularly performed, looking beyond the potentially known security problems, and finding the more unknown issues. Scanning additional URLS, parameters, headers, and checking for holes, gaps, and other areas of the surface area for APIs which may have been overlooked or forgotten. Security scanning reflects the scanning that already occurs on most web and mobile applications, but will also consider many of the API specific vulnerabilities that we’ve seen behind breaches of the past, and those that are unique to API integration scenarios.</p>

<p><strong>API Security Is More About Building An Awareness</strong><br />
While API security centers around establishing a defensive perimeter around API resources, policing and enforcing rules along this perimeter, and encrypting all traffic, most of API security is realized through an awareness around how APIs are being accessed. API management, logging, monitoring, testing, and analytics provide an approach to understanding how data, content, media, and algorithms are being used, or not being used. Providing an evolved level of awareness that goes well beyond legacy web services and database connectivity.</p>

<p>API security should center around encryption, and common approaches to authenticating APIs. However, if they are not being properly monitored, tested, analyzed, and audited, a strong security perimeter, encryption, and strong authentication will not mean much. It is important for all key stakeholders involved in API operations to understand that API security is a balance between allowing for access and consumption, while also locking down, encrypting, and defending the perimeter. The API providers who find the most success with their API operations tend to strike a balance between these two opposing personalities of API security, making sure everything is secure, while also making sure providers, developers, and end-users feel secure while also being able to get access the resource they need to do their job.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/06/explaining-api-security-to-organizational-leadership/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/">Capital One DevExchange Provides An Important Banking API Blueprint</a></h3>
        <span class="post-date">06 Mar 2018</span>
        <p><a href="https://developer.capitalone.com"><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-banking-home-page.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>When you take a look at the banking API landscape in the United States, there is one clear leader in the game–Capital One. Their <a href="https://developer.capitalone.com/">DevExchange</a> program is miles ahead of every one of their competitors, giving them a significant head start when it comes to the banking API economy. Their approach to delivering APIs meets all of my minimum requirements for any successful API platform, and even exceeds it, providing what I’d consider to be a leading example blueprint that all banking API providers should be following.</p>

<p>The Capital One DevExchange begins as any API operation should, with a dedicated portal located at developer.[domain]:</p>

<ul>
  <li><a href="https://developer.capitalone.com/">developer.capitalone.com</a></li>
</ul>

<p>After landing on the home page for the Capital One DevExchange you get everything you need to get up and running with the APIs they have:</p>

<ul>
  <li><a href="https://developer.capitalone.com/platform-documentation/getting-started/">Getting started</a></li>
  <li><a href="https://developer.capitalone.com/platform-documentation/authorization-with-oauth-20/">Authentication</a></li>
  <li><a href="https://developer.capitalone.com/platform-documentation/">Documentation</a></li>
  <li><a href="https://developer.capitalone.com/platform-documentation/errors/">Errors</a></li>
  <li><a href="https://developer.capitalone.com/sign-in/">Login</a></li>
  <li><a href="https://developer.capitalone.com/sign-up">Registration</a></li>
</ul>

<p>The Capital One DevExchange provides four main groups of public APIs currently, started with access to account information in the following areas:</p>

<ul>
  <li>Retrieve account products - /deposits/account-products (GET)</li>
  <li>Retrieve account product details - /deposits/account-products/{productId} (GET)</li>
  <li>Create new account application - /deposits/account-applications (POST)</li>
  <li>Retrieve out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (GET)</li>
  <li>Answer out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (PUT)</li>
  <li>Retrieve account application details - /deposits/account-applications/{applicationId} (GET)</li>
</ul>

<p>As well as some credit card offers, showcasing the products they have available:</p>

<ul>
  <li>Retrieve product listings - /credit-offers/products (GET)</li>
  <li>Retrieve card products - /credit-offers/products/cards (GET)</li>
  <li>Retrieve card products - /credit-offers/products/cards (GET)</li>
  <li>Retrieve card products by type - /credit-offers/products/cards/{cardType} (GET)</li>
  <li>Retrieve card product details - /credit-offers/products/cards/{cardType}/{productId} (GET)</li>
</ul>

<p>Which you can actually sign up for and do a pre-qualification via APIs:</p>

<ul>
  <li>Create prequalification check - /credit-offers/prequalifications (POST)</li>
  <li>Create prequalification acknowledgment - /credit-offers/prequalifications/{prequalificationId} (POST)</li>
  <li>Create applicant key - /credit-offers/applicant-details POST</li>
</ul>

<p>Get access to Capital One rewards via APIs:</p>

<ul>
  <li>Retrieve rewards accounts - /rewards/accounts (GET)</li>
  <li>Retrieve rewards account details - /rewards/accounts/{rewardsAccountReferenceId} (GET)</li>
</ul>

<p>And details about merchants involved in transactions:</p>

<ul>
  <li>Retrieve merchant data - /merchant-insights/merchants (GET)</li>
  <li>Refresh merchant details - /merchant-insights/merchants/{merchantId} (GET)</li>
</ul>

<p>This version of the API is available in a sandbox and production environments:</p>

<ul>
  <li><a href="https://developer.capitalone.com/platform-documentation/using-the-sandbox/">Sandbox</a></li>
</ul>
<p><a href="http://api.reimaginebanking.com/"><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-hackathon-api.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>You can tell Capital One is moving cautiously with their public APIs, but they are definitely further along than other banks. Beyond what is publicly available in their sandbox and production environment they have another exploratory set of APIs coming from a project they call Nessie, <a href="http://api.reimaginebanking.com/">Capital One’s Hackathon API</a> that gives you access to a multitude of real public-facing data, such as ATM and bank branch locations, complete with mock customer account data. This set of APIs was used as part of a hackathon put on by the bank, but are worth showcasing as an example of where the bank is headed with their API road map.</p>

<p>By providing a pretty robust stack of API paths for working with accounts:</p>

<ul>
  <li>Get all accounts - /accounts (GET)</li>
  <li>Delete a specific existing account - /accounts/{id} (DELETE)</li>
  <li>Get account by id - /accounts/{id} (GET)</li>
  <li>Update a specific existing account - /accounts/{id} (PUT)</li>
  <li>Get customer that owns the specified account - /accounts/{id}/customer (GET)</li>
  <li>Get accounts by customer id - /customers/{id}/accounts (GET)</li>
  <li>Create an account - /customers/{id}/accounts (POST)</li>
</ul>

<p>As well as bills that are associated with accounts:</p>

<ul>
  <li>Get all bills for a specific account - /accounts/{id}/bills (GET)</li>
  <li>Create a bill - /accounts/{id}/bills (POST)</li>
  <li>Delete a specific existing bill - /bills/{billId} (DELETE)</li>
  <li>Get bill by id - /bills/{billId} (GET)</li>
  <li>Update a specific existing bill - /bills/{billId} (PUT)</li>
  <li>Get bills by customer id - /customers/{id}/bills (GET)</li>
</ul>

<p>Then API paths for managing deposits:</p>

<ul>
  <li>Get all deposits - /accounts/{id}/deposits (GET)</li>
  <li>Create a deposit - /accounts/{id}/deposits (POST)</li>
  <li>Delete a specific existing deposit - /deposits/{id} (DELETE)</li>
  <li>Get deposit by id - /deposits/{id} (GET)</li>
  <li>Update a specific existing deposit - /deposits/{id} (PUT)</li>
</ul>

<p>And for any details on loans:</p>

<ul>
  <li>Get all loans - /accounts/{id}/loans (GET)</li>
  <li>Create a loan - /accounts/{id}/loans (POST)</li>
  <li>Delete a specific existing loan - /loans/{id} (DELETE)</li>
  <li>Get loan by id - /loans/{id} (GET)</li>
  <li>Update a specific existing loan - /loans/{id} (PUT)</li>
</ul>

<p>Then providing insight into all purchases:</p>

<ul>
  <li>Get all purchases - /accounts/{id}/purchases (GET)</li>
  <li>Create a purchase - /accounts/{id}/purchases (POST)</li>
  <li>Get all purchases by account and merchant - /merchants/{id}/accounts/{accountId}/purchases (GET)</li>
  <li>Get all purchases by merchant - /merchants/{id}/purchases (GET)</li>
  <li>Delete a specific existing purchase - /purchases/{id} (DELETE)</li>
  <li>Get purchase by id - /purchases/{id} (GET)</li>
  <li>Update a specific existing purchase - /purchases/{id} (PUT)</li>
</ul>

<p>As well as bank account transfers:</p>

<ul>
  <li>Get all transfers - /accounts/{id}/transfers (GET)</li>
  <li>Create a transfer - /accounts/{id}/transfers (POST)</li>
  <li>Delete a specific existing transfer - /transfers/{transferId} (DELETE)</li>
  <li>Get transfer by id - /transfers/{transferId} (GET)</li>
  <li>Update a specific existing transfer - /transfers/{transferId} (PUT)</li>
</ul>

<p>And withdrawals:</p>

<ul>
  <li>Get all withdrawals - /accounts/{id}/withdrawals (GET)</li>
  <li>Create a withdrawal - /accounts/{id}/withdrawals (POST)</li>
  <li>Delete a specific existing withdrawal - /withdrawals/{id} (DELETE)</li>
  <li>Get withdrawal by id - /withdrawals/{id} (GET)</li>
  <li>Update a specific existing withdrawal - /withdrawals/{id} (PUT)</li>
</ul>

<p>Details about banking customers:</p>

<ul>
  <li>Get all customers - /customers (GET)</li>
  <li>Create a customer - /customers (POST)</li>
  <li>Get customer by id - /customers/{id} (GET)</li>
  <li>Update a specific existing customer - /customers/{id} (PUT)</li>
</ul>

<p>As well as available merchants:</p>

<ul>
  <li>Get all merchants - /merchants (GET)</li>
  <li>Create a merchant - /merchants (POST)</li>
  <li>Get merchant by id - /merchants/{id} (GET)</li>
  <li>Update a specific existing merchant - /merchants/{id} (PUT)</li>
</ul>

<p>Available ATMS:</p>

<ul>
  <li>Get all ATMs - /atms (GET)</li>
  <li>Get ATM by id - /atms/{id} (GET)</li>
</ul>

<p>And Bank Branches:</p>

<ul>
  <li>Get all branches - /branches (GET)</li>
  <li>Get branch by id - /branches/{id} (GET)</li>
</ul>

<p>I am not sure what Capital One’s intentions are with introducing these APIs to the main stack of public APIs, but they are still available within a separate Hackathon sandbox environment:</p>

<ul>
  <li><a href="http://api.reimaginebanking.com/">Hackathon API</a></li>
</ul>

<p>Beyond the sandbox, hackathon, and production APIs available, Capital One provides all the expected support and communication channels that you expect out of an active API program:</p>

<ul>
  <li><a href="https://developer.capitalone.com/support/">Support</a></li>
  <li><a href="https://developer-support.capitalone.com/apex/DXSearchArticles">Knowledgebase</a></li>
  <li><a href="https://developer.capitalone.com/blogs/">Blog</a></li>
  <li><a href="https://twitter.com/CapitalOneDevEx">Twitter</a></li>
</ul>

<p>As well as not forgetting the required elements present from the legal department, helping set the tone for API engagements:</p>

<ul>
  <li><a href="https://www.capitalone.com/identity-protection/privacy/statement">Privacy policy</a></li>
  <li><a href="https://developer.capitalone.com/single/terms-and-conditions/">Terms of service</a></li>
</ul>

<p><a href="https://developer.capitalone.com/open-source/"><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-open-source.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>Where the Capital One API program really begins to impress is with their usage of Github, and providing of open source solutions:</p>

<ul>
  <li><a href="https://github.com/capitalone">Capital One Github</a></li>
  <li><a href="https://github.com/CapitalOne-DevExchange">Capital One Devexchange Github</a></li>
  <li><a href="https://developer.capitalone.com/open-source/">Open Source</a></li>
</ul>

<p>It isn’t easy doing Open Source in a highly regulated industry, and Capital One not only is doing it, <a href="https://developer.capitalone.com/blog-post/open-source-in-a-regulated-environment-lessons-learned-on-our-open-source-journey-at-capital-one/">they actually share some of the story behind their struggle</a>. The open source projects they have been releasing, and the stories they’ve told about them on the blog tell just as important of a story as each of their APIs do. Further externalizing how the bank delivers technology at the bank, as well as with partners and 3rd party developers.</p>

<p>The Capital One DevExchange is what ALL US banks should be emulating. This isn’t just about publicly available APIs. This is about being able to deliver APIs consistently, in ways that are in sync with your lines of business. The more you can deliver this publicly, tell the story of it, and release open source code that demonstrates the value it brings, the more you will be able to do this internally, across groups, and amongst your partners. The biggest mistake any bank can be making in 2018 is thinking doing APIs is purely about retail banking. It is really about how agile and effective your bank will be leverage web technology internally, and externally within your company.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and-constraints/">Serverless, Like Microservices Is About Understanding Our Dependencies And Constraints</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/death-valley-national-park_dali_three_just_road.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am not buying into all the hype around the recent serverless evolution in compute. However, like most trends I study, I am seeing some interesting aspects of how people are doing it, and some positive outcomes for teams who are doing it sensibly. I am not going all in on serverless when it comes to deploying or integrating with APIs, but I am using it for some projects, when it makes sense. I find AWS Lambda to be a great way to get in between the AWS API Gateway and backend resources, in order to conduct a little transformation. Keeping serverless as just one of many tools in my API deployment and integration toolbox, yet never going overboard with any single solution.</p>

<p>I put serverless into the same section of my toolbox as I do microservices. Serverless is all about decoupling how I deploy my APIs, helping me keep things doing small, meaningful things behind each of my API paths. Serverless forces me to think through how I am decoupling my backend, and pushes me to identify dependencies, acknowledge constraints, and get more creative in how I write code in this environment. To do one thing, and do it well, I need an intimate understanding of where my data and other backend resources are, and how I can distill a unit of compute down to the smallest unit as I possibly can. Identifying, reducing, and being honest about what my dependencies are is essential to serverless working or not working for me.</p>

<p>The challenge I’m having with serverless at the moment is that it can be easy to ignore many of the benefits I get from dependencies, while just assuming all dependencies are bad. Specifically around the frameworks I deploy as part of my API solutions. When coupled with AWS API Gateway this isn’t much of a problem, but all by itself, AWS Lambda doesn’t have all fo the other HTTP, transformation, header, request, and response goodness I enjoy from the API frameworks I’ve historically depend on. Showing me that understanding our dependencies is important, both good and bad. Not all dependencies are bad, as long as I am aware of them, have a knowledge of what they bring to the table, and it is a stable, healthy, and regularly evaluated relationships. This is a challenge I also come across regularly in microservices as well as serverless journeys, that we just throw things out for the sake of size, without thinking deeply about why we are doing something, and whether or not it is worth it.</p>

<p>I can see plenty of scenarios where I will be thinking that a serverless approach is good, but after careful evaluation of dependencies and constraints, I abandon this path as an option. I’m guessing we’ll see a goldilocks style of serverless emerge that helps us find this sweet spot. Similar to what we are finding with the microservices evolution, in that micro isn’t always about small–it is often just about finding the sweet spot between big and small, hot and cold, dependent and independent. Being sensible about how we use the tools in our API toolbox, and realizing most of these approaches are more about the journey, than they are about the actual tool.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and-constraints/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/an-openapi-rules-engine/">An OpenAPI-Driven, API Governance Rules Engine</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/rules/9968073905_95ce575233_z.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Phil Sturgeon (<a href="https://twitter.com/philsturgeon">@philsturgeon</a>) alerted me to a pretty cool project he is cooking up, called <a href="https://github.com/wework/speccy">Speccy</a>. Which provides a rules engine for validating your OpenAPI definitions. “Taking off from where <a href="https://twitter.com/PermittedSoc/">Mike Ralphson</a> started with linting in <a href="https://github.com/Mermade/swagger2openapi/">swagger2openapi</a>, Speccy aims to become the <a href="https://github.com/bbatsov/rubocop">rubocop</a> or <a href="https://eslint.org/">eslint</a> of OpenAPI”, and to “sniff your files for potentially bad things. “Bad” is objective, but you’ll see validation errors, along with special rules for making your APIs better.” Helping make sure your API definitions are as consistent as they possibly can be, and deliver on your API governance strategy (you have one right?)</p>

<p>With Speccy, there are a default set of rules, things like ensuring you have a summary or a description for each API path:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "operation-summary-or-description",
	"object": "operation",
	"enabled": true,
	"description": "operation should have summary or description",
	"or": ["summary", "description"]
}
</code></pre></div></div>

<p>Or making sure you add descriptions to your parameters:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "parameter-description",
	"object": "parameter",
	"enabled": true,
	"description": "parameter objects should have a description",
	"truthy": "description"
}
</code></pre></div></div>

<p>Or making sure you include tags for each aPI path:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "operation-tags",
	"object": "operation",
	"enabled": true,
	"description": "operation should have non-empty tags array",
	"truthy": "tags",
	"skip": "isCallback"
}
</code></pre></div></div>

<p>Then you can get more strict by requiring contact information:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "contact-properties",
	"object": "contact",
	"enabled": true,
	"description": "contact object should have name, url and email",
	"truthy": [ "name", "url", "email" ]&lt;br /&gt;
}
</code></pre></div></div>

<p>And make sure youi have a license applied to your API:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "license-url",
	"object": "license",
	"enabled": true,
	"description": "license object should include url",
	"truthy": "url"
}
</code></pre></div></div>

<p>Speccy is available <a href="https://www.npmjs.com/package/speccy">as a Node package</a>, which you can easily run at the command line. Speccy is definitely what is needed out there right now, helping us validate the growing number of OpenAPI definitions in our life. As many companies are thinking about how they can apply API governance across their operations, they should be looking at contributing to <a href="https://github.com/wework/speccy">Speccy</a>. It is something I’ve been talking with API service providers about for some time, but haven’t seen an open source answer emerge, that can help us develop rules for what we expect of our OpenAPI definitions.</p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/openapi-logo.png" align="right" width="25%" style="padding: 15px;" /></p>
<p>My only feedback right now, is that we need lots of people using it, and helping contribute rules. Oh, and wrap it in an API, and make it available as an easy to use, and deploy containerized microservice. Then lets get to work on the Github Gist-driven marketplace of rules, where I can publish the rules I develop across the projects I’m working on, and of the clients I consult with. Let’s get to work making sure there are a wealth of rules, broken down into different categories for API providers to choose from. Then let’s get API tooling and service providers to begin baking a Speccy rules engine into their solutions, and allow for the import and management of open source rules.</p>

<p>Speccy only works with OpenAPI 3.0, which makes sense if we are going to be moving forward with this conversation. Spreccy is how we will validate that banking APIs are PSD2 compliant. It is how we will ensure healthcare APIs support the FHIR specification. I have other suggestions for the CLI and API usage of Speccy, but I’d rather see investment in the available rules, before I make too many functional suggestions. I think the rules are where we will begin to define what we are looking for in an OpenAPI rules engine, and that should drive the Speccy features which end up on the road map.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/an-openapi-rules-engine/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks/">People Seem to Want Lego Kits and Not a Bucket of Lego Blocks When It Comes To Doing APIs</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/legos/lego-millenium-falcon-instructions.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>While I wish everyone saw the modular potential of APIs and microservices like I do, I’ve come to the realization that most people are just interested in ready to go kits that walk them through every detail of doing APIs, rather than actually playing, learning, evolving, and learning to be productive with a big bucket of APIs. I’m not just focusing on business users here, I am talking about a significant portion of the developers I come across, who really don’t seem that interested in learning to apply API concepts, and understanding when and where to use them, they just want a set of instructions that walk them through each step of deploying an API.</p>

<p>I actually am a proponent of there being more boxed, lego style API kits that teach you how to build the product API, task API, press release API, and other common implementations. Robust, detailed, ready-to-go API implementations that would walk people through each and every step of defining, designing, deploying, managing, monitoring, testing, and documenting their API. I feel like this would significantly help folks think through what are healthy API practices, and be introduced to different ways of thinking around APIs. However, I do not want people to become reliant on only being able to operate within this paradigm, and not actually be able to fix, evolve, and deliver their own custom API solutions, using the healthy practices they are being introduced to.</p>

<p>People seem to just want shortcuts, and things done for them. People want the solutions packaged and delivered to their doorstep. They seem unable to be able to find the solutions on their own, or even be able to absorb a lesson delivered via a packaged solution. I’m not sure what the cure for this condition is. It is hard to tell whether it is vendor induced, or (would be) API provider induced. Have people been conditioned by vendors to just be spoon fed solutions? Or are people just lazy, and not interested in truly learning, truly tackling their technical debt, and finding a new path forward? IDK. I’m feeling like it is probably a little of both, feeding off of each other.</p>

<p>I get why the pre-built, ready-to-go Lego API kit is appealing. Everyone wants to have a full-blown millennium falcon when they are done working on a project, and not some blocky mcsquare flying machine. However, it takes time before you are able to deliver at that scope. You need to practice building smaller implementations, and yes playing with other pre-built kits, doing some reverse engineering, in addition to some forward engineering. I’m guessing this all comes down to if you truly want to know and understand, or if you are just looking for solutions. If you are just looking for solutions, I’m guessing in 5 years you’ll be eagerly buying the next solution for the API mess you are putting into place currently. It is the natural evolution of how technology gets bought and sold.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud-providers/">The Need for Standardized API Plans and Pricing to Compete with Cloud Providers</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/docks_copper_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://cloudplatform.googleblog.com/2018/02/introducing-Cloud-Billing-Catalog-API-GCP-pricing-in-real-time.html">Google launched their Cloud Billing Catalog API, providing access to thee pricing for their cloud API catalog</a> the other day. <a href="https://docs.microsoft.com/en-us/azure/billing/">Azure has their billing API</a>, and <a href="https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/Welcome.html">AWS has their cost explorer API service</a>, showing that programmatic access to what API resources cost, as well as management of usage, billing, invoicing, and other aspects of doing business with APIs is becoming the normal mode of operating an API platform.</p>

<p>I’ve long used AWS, Google, and increasingly Azure as a blueprint for what us smaller API providers should be doing. They are full of positive and negative lessons for any API provider. However, I’m starting to see what they are doing as not just a blueprint, but potentially something that will force many of us API providers out of businesses if we cannot emulate what they are doing at scale. The tractor beam that is the cloud providers is strong. They bring a lot of benefits to the table. So much so, it is getting harder and harder for independent API providers to compete. Offering benefits to consumers that will become deal breakers with using other 3rd party API providers services, pushing API consumers to stay within their chosen cloud platform walled garden.</p>

<p>As a developer, if I can programmatically manage the plans, pricing, and billing for ALL the APIs I use via AWS, Google, and Azure, but I have to manually manage this across many different 3rd party providers, I am going to be hesitant when it comes to adopting any new services that aren’t within my cloud domain. As I depend on more APIs, the benefits of being able to programmatically manage the business of my API consumption is becoming increasingly critical. If the individual 3rd party API providers I use don’t begin to offer APIs for managing the business of my API consumption, and adopting a standardized interface across all the APIs I depend on, I’m going to favor my cloud native API solutions over the 3rd party, and custom ones–giving cloud providers a significant advantage.</p>

<p>It is something that smaller API providers are going to have to start thinking about, and stop being special little snowflakes, and consider how they can start standardizing and being interoperable. Otherwise the cloud API providers are going to continue to gain marketshare, and everyone will just use the APIs available to use in the cloud. Reducing the competition, diversity, and utility that the API sector has become known for. We will just use Amazon, Google, and Azure APIs, and if they don’t have it, it probably won’t be done. Innovation will only occur within the cloud marketplaces, and be way less vibrant than the API sector we’ve enjoyed over the last decade.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud-providers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/">Thoughts On The Schema.Org WebAPI Type Extension</a></h3>
        <span class="post-date">02 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/schema-org/schema-org.png" width="30%" align="right" style="padding: 15px;" /></p>
<p>I’m putting some thought into <a href="https://webapi-discovery.github.io/rfcs/rfc0001.html#content-types">the Schema.Org WebAPI Type Extension proposal</a> by Mike Ralphson (Mermade Software) and Ivan Goncharov (APIs.guru), to “facilitate better automatic discovery of WebAPIs and associated machine and human-readable documentation”. It’s an interesting evolution in how we define APIs, in terms of API discovery, but I would also add potentially at “execute time”.</p>

<p>Here is what a base WebAPI type schema could look like:</p>

<p>```{</p>

<p>“@context”: “http://schema.org/”,</p>

<p>“@type”: “WebAPI”,</p>

<p>“name”: “Google Knowledge Graph Search API”,</p>

<p>“description”: “The Knowledge Graph Search API lets you find entities in the Google Knowledge Graph. The API uses standard schema.org types and is compliant with the JSON-LD specification.”,</p>

<p>“documentation”: “https://developers.google.com/knowledge-graph/”,</p>

<p>“termsOfService”: “https://developers.google.com/knowledge-graph/terms”,</p>

<p>“provider”: {</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"@type": "Organization",

"name": "Google Inc."   } }```
</code></pre></div></div>

<p>Then the proposed extensions could include the following:</p>

<ul>
  <li><strong>versions</strong> (OPTIONAL array of <a href="https://schema.org/Thing">thing</a> -&gt; <a href="http://meta.schema.org/Property">Property</a> -&gt; <a href="http://schema.org/softwareVersion">softwareVersion</a>). It is RECOMMENDED that APIs be versioned using [semver]</li>
  <li><strong>entryPoints</strong> (OPTIONAL array of <a href="https://schema.org/Thing">Thing</a> -&gt; <a href="https://schema.org/Intangible">Intangible</a> -&gt; <a href="https://schema.org/EntryPoint">EntryPoint</a>)</li>
  <li><strong>license</strong> (OPTIONAL, <a href="http://schema.org/CreativeWork">CreativeWork</a> or <a href="http://schema.org/URL">URL</a>) - the license for the design/signature of the API</li>
  <li><strong>transport</strong> (enumerated <a href="http://schema.org/Text">Text</a>: HTTP, HTTPS, SMTP,  MQTT, WS, WSS etc)&lt;/p&gt;</li>
  <li><strong>apiProtocol</strong> (OPTIONAL, enumerated <a href="http://schema.org/Text">Text</a>:  SOAP, GraphQL, gRPC, Hydra, JSON API, XML-RPC, JSON-RPC etc)</li>
  <li><strong>webApiDefinitions</strong> (OPTIONAL array of <a href="http://schema.org/EntryPoint">EntryPoints</a>) containing links to <a data-link-type="dfn" href="#machine-readable-api-definition" id="ref-for-machine-readable-api-definition-1">machine-readable API definition</a>s</li>
  <li><strong>webApiActions</strong> (OPTIONAL array of potential <a href="http://schema.org/Action">Actions</a>)</li>
</ul>

<p>The webApiDefinitions (EntryPoint) contentType property contains a reference to one of the following conten types:</p>

<ul>
  <li><strong>OpenAPI / Swagger in JSON</strong> - application/openapi+json or application/x-openapi+json</li>
  <li><strong>OpenAPI / Swagger in YAML</strong> - application/openapi</li>
  <li><strong>RAML</strong> - application/raml+yaml</li>
  <li><strong>API Blueprint in markdown</strong> - text/vnd.apiblueprint</li>
  <li><strong>API Blueprint parsed in JSON</strong> - application/vnd.refract.parse-result+json</li>
  <li><strong>API Blueprint parsed in YAML</strong> - application/vnd.refract.parse-result+yaml</li>
</ul>

<p>Then the webApiActions property brings a handful of actions to the table, with the following being suggested:</p>

<ul>
  <li><strong>apiAuthentication</strong> - Links to a resource detailing authentication requirements. Note this is a human-readable resource, not an authentication endpoint</li>
  <li><strong>apiClientRegistration</strong> - Links to a resource where a client may register to use the API</li>
  <li><strong>apiConsole</strong> - Links to an interactive console where API calls may be tested</li>
  <li><strong>apiPayment</strong> - Links to a resource detailing pricing details of the API</li>
</ul>

<p>I fully support extending the Schema.org WebAPI vocabulary in this way. It adds all the bindings needed to make the WebAPI type executable at runtime, as well as it states at discovery time. I like the transport and protocol additions, helping ensure the WebAPI vocabulary is as robust as it possibly can. webApiDefinitions provides all the technical details regarding the surface area of the API we need to actually engage with it at runtime, and webApiActions begins to get at some of the business of APIs friction that exists at runtime. Making for an interesting vocabulary that can be used to describe web APIs, which also becomes more actionable by providing everything you need to get up and running.</p>

<p>The suggestions are well thought out and complete. If I was to add any elements, I’d say it also needs a support link. There will be contact information embedded within the API definitions, but having a direct link along with registration, documentation, terms of service, authentication, and payment would help out significantly. I would say that the content type to transport and protocol coverage is deficient a bit. Meaning you have SOAP, but not referencing WSDL. I know that there isn’t a direct definition covering every transport and protocol, but eventually it should be as comprehensive as it can. (ie. adding AsyncAPI, etc. in the future)</p>

<p>The WebAPI type extensions reflect what we have been trying to push forward with our <a href="http://apisjson.org">APIs.json</a> work, but comes at it from a different direction. I feel there are significant benefits to having all these details as part of the Schema.org vocabulary, expanding on what you can describe in a common way. Which can then also be used as part of each APIs requests, responses, and messages. I don’t see APIs.json as part of a formal vocabulary like this–I see it more as the agile format for indexing APIs that exist, and building versatile collections of APIs which could also contain a WebAPI reference.</p>

<p>I wish I had more constructive criticism or feedback, but I think it is a great first draft of suggestions for evolving the WebAPI type. There are other webApiActions properties I’d like to see based upon my APIs.json work, but I think this represents an excellent first step. There will be some fuzziness between documentation and apiConsole, as well as gaps in actionability between apiAuthentication, and apiClientRegistration–thinks like application creation (to get keys), and opportunities to have Github, Twitter, and other OpenID/OAuth authentication, but these things can be worked out down the road. Sadly there isn’t much standardization at this layer currently, and I see this extension as a first start towards making this happen. As I said, this is a good start, and we have lots of work ahead as we see more adoption.</p>

<p>Nice work Mike and Ivan! Let me know how I can continue to amplify and get the word out. We need to help make sure folks are describing their APIs using Schema.org. I’d love to be able to automate the discovery of APIs, using existing search engines and tooling–I know that you two would like to see this as well. API discovery is a huge problem, which there hasn’t been much movement on in the last decade, and having a common vocabulary that API providers can use to describe their APIs, which search engines can tune into would help move us further down the road when it comes to having more robust API discovery.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/">An Observable Industry Level Directory Of API Providers And Consumers</a></h3>
        <span class="post-date">01 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/vancouver_diego_rivera1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve been breaking down the work on banking APIs coming out of <a href="http://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/">Open Banking in the UK</a> lately. I recently took all their <a href="http://open.banking.blueprint.apievangelist.com/">OpenAPI definitions and published as a demo API developer portal</a>. Bringing the definitions out of the shadows a little bit, and showing was is possible with the specification. Pushing the project forward some more today I published <a href="http://open.banking.blueprint.apievangelist.com/#Open Banking - Directory APIs">the Open Banking API Directory specification to the project</a>, showing the surface area of the very interesting, and important component of open banking APIs in the UK.</p>

<p>The Open Banking Directory provides a pretty complete, albeit rough and technical approach to delivering observability for <a href="http://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/">the UK banking industry API ecosystem actor layer</a>. Everyone involved in the banking API ecosystem in UK has to be registered in the directory. It provides profiles of the banks, as well as any third party players. It really provides an unprecedented, industry level look at how you can make API ecosystems more transparent and observable. This thing doesn’t exist at the startup level because nobody wants to be open with the number of developers, or much else regarding the operation of their APIs. Making any single, or industry level API ecosystem, operate as black boxes–even if they claim to be an “open API”.</p>

<p>Could you imagine if API providers didn’t handle their own API management layer, and an industry level organization would handle the registration, certification, directory, and dispute resolution between API providers and API consumers? Could you imagine if we could see the entire directory of Facebook and Twitter developers, understand what businesses and individuals were behind the bots and other applications? Imagine if API providers couldn’t lie about the number of active developers, and we knew how many different APIs each application developers used? And it was all public data? An entirely different API landscape would exist, with entirely different incentive models around providing and consuming gAPIs.</p>

<p>The Open Banking Directory is an interesting precedent. It’s not just an observable API authentication and management layer. It also is an API. Making the whole thing something that can be baked into the industry level, as well as each individual application. I’m going to have to simmer on this concept some more. I’ve thought a lot about collective API developer and client solutions, but never anything like this. I’m curious to see how this plays out in a heavily regulated country and industry, but also eager to think about how something like this might work (or not) in government API circles, or even in the private sector, within smaller, less regulated industries.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/28/an-open-banking-blueprint-api-portal/">An Open Banking API Portal Blueprint</a></h3>
        <span class="post-date">28 Feb 2018</span>
        <p>I have been learning all about <a href="https://www.openbanking.org.uk/">the banking API efforts out of Open Banking in the UK lately</a>. They are evolving a set of read / write account and transaction API, as well as public data APIs for some of the common information 3rd party developers are looking to get their hands on. I'm intrigued with the traction the organization has gotten, and I want to be able to fully understand what they are developing, as well help contribute where I can.</p>
<p>To help me understand the API specification, as well as hopefully contribute to the conversation, I am publishing <a href="http://open.banking.blueprint.apievangelist.com/">an blueprint API portal for the API</a>. It is a demo portal, running on Github, which uses the API Evangelist graphical look, but I am also publishing documentation for v1.1.1 of the account and payments API, as well as v2.1 of the public data APIs. I'm looking to publish the API specification like any bank would, but it won't actually be a live API--yet. I'd like to turn it into a mock API, with some virtualized data to demonstrate what is possible.</p>
<p align="center"><a href="http://open.banking.blueprint.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/open-banking/open-banking-accounts-documentation-screenshot.png" width="90%" align="center" style="padding: 15px;" /></a></p>
<p>I've only had time to publish the overview of the project, and the documentation for each current version. I have a todo list of things I would like to invest in when I have more time. Eventually, I want it to be a complete, forkable Open Banking API portal that any bank in the UK could publish. Then I'm looking to create country specific versions to help push French, German, and other banks to push a portal. It doesn't have to be my solution that the banks use, but hopefully they'll at least use what I have provided as a blueprint. The goal isn't just to get them to use the portal, it is to get them implementing their bank's API developer portal in a standardized way--similar to the API specification from Open Banking, but this is the portal specification.</p>
<p>I will spend time on the portal over the next couple of weeks. If there is something you'd like to see accomplished, or something I'm missing entirely, feel free to <a href="https://github.com/european-banking-apis/open-banking-blueprint/issues">submit a Github issue for the project</a>. The project repository is a little messy right now as it is in full development, so if you fork, be careful--you might want to wait.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/28/an-open-banking-blueprint-api-portal/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">What We Need To Be Machine Readable At API Run Time</a></h3>
        <span class="post-date">28 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/4882162452_fa3126b38d_b_spagetti_accident.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I had breakfast with Mike Amundsen (<a href="https://twitter.com/mamund">@mamund</a>) and Matt McLarty (<a href="https://twitter.com/MattMcLartyBC">@MattMcLartyBC</a>) of the CA API Academy team this morning in midtown this morning. As we were sharing stories of what each other was working on, the topic of what is needed to execute an API call came up. Not the time consuming find an API, sign up for an account, figure out the terms of service and pricing version, but all of this condensed into something that can happen in a split second within applications and systems.</p>

<p>How do we distill down the essential ingredients of API consumption into a single, machine readable unit that can be automated into what Mike Amundsen calls, “find and bind”. This is something I’ve been thinking a lot about lately as I work on my API discovery research, and there are a handful of elements that need to be present:</p>

<ul>
  <li><strong>Authentication</strong> - Having keys to be able to authentication.</li>
  <li><strong>Surface Area</strong> - What is the host, base url, path, headers, and parameters for a request.</li>
  <li><strong>Terms of Service</strong> - What are the legal terms of service for consumption.</li>
  <li><strong>Pricing</strong> - How much does each API request cost me?</li>
</ul>

<p>We need these elements to be machine readable and easily accessible at discover and runtime. Currently the surface area of the API can be described using OpenAPI, that isn’t a problem. The authentication details can be included in this, but it means you already have to have an application setup, with keys. It doesn’t include new users into the equation, meaning, discovering, registering, and obtaining keys. I have a draft specification I call “API plans” for the pricing portion of it, but it is something that still needs a lot of work. So, in short, we are nowhere near having this layer ready for automation–which we will need to scale all of this API stuff.</p>

<p>This is all stuff I’ve been beating a drum about for years, and I anticipate it is a drum I’ll be beating for a number of more years before we see come into focus. I’m eager to see Mike’s prototype on “find and bind”, because it is the only automated, runtime, discovery, registration, and execute research I’ve come across that isn’t some proprietary magic. I’m going to be investing more cycles into my API plans research, as well as the terms of service stuff I started way back when alongside my API Commons project. Hopefully, moving all this forward another inch or two, and flesh out more of the machine readable components we’ll need at this layer.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/28/the-business-of-running-government-as-a-microservices-platform/">The Business of Running Government As A Microservices Platform</a></h3>
        <span class="post-date">28 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/capital-battle_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I recently <a href="http://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">wrote a response to a recent Department of Veterans Affairs RFI which contained a section about the business of operating government as a microservices platform</a>.I know that many folks wouldn’t make it that far in the 10K word response, so I wanted to break it out into its own post. I feel pretty strongly about the potential of decoupling how we deliver technology across government, but for this to be successful we are also going to have to decouple the business and politics of it all as well. This post reflects <a href="http://public.data.api.management.apievangelist.com/">my current research and thinking about the business of APIs in government, and is part of some ongoing work I am doing around API management, public data, and how we begin to think differently about how government engages with the public in a digital age</a>.</p>

<p>There are many interpretations of what is a microservice, but for the purposes of this post, it is a simple set of APIs that meet one precise set of government services. The API definition, database, back-end code, management layer, documentation, support and all other essential elements are self-contained, and usually stored in a single Github, or Bitbucket repository, when delivering microservices. Each microservice possesses its own technical, business, and political contract, outline how the service will be delivered, managed, supported, communicated, and versioned. These contracts can be realized individually, or grouped together as a larger, aggregate contract that can be submitted, while still allowing each individual service within that contract to operate independently.</p>

<p><strong>Decoupling The Business Of Delivering Government Digital Services</strong>
The microservices approach isn’t just about the technical components. It is about making the business of delivering vital government services more modular, portable, and scalable. Something that will also decouple and shift the politics of delivering critical services to veterans. Breaking things down into much more manageable chunks that can move forward independently at the contract level. Helping both simplify, and streamline the deliver of services for both the provider, vendor, as well as any other stakeholders involved in the software lifecycle. Microservices isn’t just about decoupling the technology, it is about decoupling the business of delivering digital services:</p>

<ul>
  <li><strong>Micro Procurement</strong> - One of the benefits of breaking down services into small chunks, is that the money needed to deliver the service can become much smaller, potentially allowing for a much smaller, more liquid and flowing procurement cycle. Each service has a micro definition of the monetization involved with the service, which can be aggregated by groups of services and projects.</li>
  <li><strong>Micro Payments</strong> - Payments for service deliver can be baked into the operations and life cycle of the service. API management excels at measuring how much a service is accessed, and testing, monitoring, logging, security, and other stops along the API life cycle can all be measured, and payments can be delivered depend on quality of service, as well as volume of service.</li>
</ul>

<p>Amazon Web Services already has the model for defining, measuring, and billing for API consumption in this way. This is the bread and butter of the Amazon Web Services platform, and the cornerstone of what we know as the cloud. This approach to delivering, scaling, and ultimately billing or payment for the operation and consumption of resources, just needs to be realized within each agency, and the rest of the federal government. We have seen a shift in how government views the delivery and operation of technical resources using the cloud over the last five years, we just need to see the same shift for the business of APIs over the next five years.</p>

<p><strong>Changing The Way Government Does Business</strong>
API management is where you begin changing the way government does business. API management has been used for a decade to measure, limit, and quantify the value being exchanged at the API level. Now that API management has been baked into the cloud, we are starting to see the approach being scaled to deliver at a marketplace level. With over ten years of experience with delivering, quantifying, metering and billing at the API level, Amazon is the best example of this monetization approach in action, with two distinct ways of quantifying the business of APIs.</p>

<ul>
  <li><strong>AWS Marketplace Metering Service</strong> - SaaS style billing model which provides a consumption monetization model in which customers are charged only for the number of resources they use–the best known cloud model.</li>
  <li><strong>AWS Contract Service</strong> - Billing customers in advance for the use of software, providing an entitlement monetization model in which customers pay in advance for a certain amount of usage, which could be used to deliver certain amount of storage per month for a year, or a certain amount of end-user licenses for some amount of time.</li>
</ul>

<p>This provides a framework for thinking about how the business of microservices can be delivered. Within these buckets, AWS provides a handful of common dimensions for thinking through the nuts and bolts of these approaches, quantifying how APIs can be monetized, in nine distinct areas:</p>

<ul>
  <li><strong>Users</strong> – One AWS customer can represent an organization with many internal users. Your SaaS application can meter for the number of users signed in or provisioned at a given hour. This category is appropriate for software in which a customer’s users connect to the software directly (for example, with customer-relationship management or business intelligence reporting).</li>
  <li><strong>Hosts</strong> – Any server, node, instance, endpoint, or other part of a computing system. This category is appropriate for software that monitors or scans many customer-owned instances (for example, with performance or security monitoring). Your application can meter for the number of hosts scanned or provisioned in a given hour.</li>
  <li><strong>Data</strong> – Storage or information, measured in MB, GB, or TB. This category is appropriate for software that manages stored data or processes data in batches. Your application can meter for the amount of data processed in a given hour or how much data is stored in a given hour.</li>
  <li><strong>Bandwidth</strong> – Your application can bill customers for an allocation of bandwidth that your application provides, measured in Mbps or Gbps. This category is appropriate for content distribution or network interfaces. Your application can meter for the amount of bandwidth provisioned for a given hour or the highest amount of bandwidth consumed in a given hour.</li>
  <li><strong>Request</strong> – Your application can bill customers for the number of requests they make. This category is appropriate for query-based or API-based solutions. Your application can meter for the number of requests made in a given hour.</li>
  <li><strong>Tiers</strong> – Your application can bill customers for a bundle of features or for providing a suite of dimensions below a certain threshold. This is sometimes referred to as a feature pack. For example, you can bundle multiple features into a single tier of service, such as up to 30 days of data retention, 100 GB of storage, and 50 users. Any usage below this threshold is assigned a lower price as the standard tier. Any usage above this threshold is charged a higher price as the professional tier. Tier is always represented as an amount of time within the tier. This category is appropriate for products with multiple dimensions or support components. Your application should meter for the current quantity of usage in the given tier. This could be a single metering record (1) for the currently selected tier or feature pack.</li>
  <li><strong>Units</strong> – Whereas each of the above is designed to be specific, the dimension of Unit is intended to be generic to permit greater flexibility in how you price your software. For example, an IoT product which integrates with device sensors can interpret dimension “Units” as “sensors”. Your application can also use units to make multiple dimensions available in a single product. For example, you could price by data and by hosts using Units as your dimension. With dimensions, any software product priced through the use of the Metering Service must specify either a single dimension or define up to eight dimensions, each with their own price.</li>
</ul>

<p>These dimensions reflect the majority of API services being sold out there today, we don’t find ourselves in a rut with measuring value, like just paying per API call. Allowing government API plans to possess one or more dimensions, beyond any single use case.</p>

<ul>
  <li><strong>Single Dimension</strong> - This is the simplest pricing option. Customers pay a single price per resource unit per hour, regardless of size or volume (for example, $0.014 per user per hour, or $0.070 per host per hour).</li>
  <li><strong>Multiple Dimensions</strong> – Use this pricing option for resources that vary by size or capacity. For example, for host monitoring, a different price could be set depending on the size of the host. Or, for user-based pricing, a different price could be set based on the type of user (admin, power user, and read-only user). Your service can be priced on up to eight dimensions. If you are using tier-based pricing, you should use one dimension for each tier.</li>
</ul>

<p>This provides a business framework that government can provide for vendors and 3rd party developers, allowing them to operate their services within a variety of business models. Derived from many of the hard costs they face, and providing additional volume based revenue, based upon how may API calls of any particular service receives.</p>

<p>Beyond this basic monetization framework, I’d also recommend adding in an incentive framework that would dovetail with the business models proposed, but then provide different pricing levels depending on how well the services perform, and deliver on the agreed upon API contract. There are a handful of bullets I’d consider here.</p>

<ul>
  <li><strong>Design</strong> - How well does a service meet API design guidelines set forth in governance guidance.</li>
  <li><strong>Monitoring</strong> - Has a service consistently met its monitoring goals, delivering against an agreed upon service level agreement (SLA).</li>
  <li><strong>Testing</strong> - Beyond monitoring, are APIs meeting granular interface testing, along a regular testing &amp; monitoring schedule.</li>
  <li><strong>Communication</strong> - Are service owners meeting expectations around communication around a service operations.</li>
  <li><strong>Support</strong> - Does a service meet required support metrics, making sure it is responsive and helpful.</li>
  <li><strong>Ratings</strong> - Provide a basic set of metrics, with accompanying ratings for each service.</li>
  <li><strong>Certification</strong> - Allowing service providers to get certified, receiving better access, revenue, and priority.</li>
</ul>

<p>All of the incentive framework is defined and enforced via the API governance strategy for the platform. Making sure all microservices, and their owners meet a base set of expectations. When you take the results and apply weekly, monthly, and quarterly against the business framework, you can quickly begin to see some different pricing levels, and revenue opportunities around all microservices emerge. You deliver consistent, reliable, highly ranked microservices, you get paid higher percentages, enjoy greater access to resources, and prioritization in different ways via the platform–if you don’t, you get paid less, and operate fewer services.</p>

<p>This model is already visible on the AWS platform. All the pieces are there to make it happen for any platform, operating on top of the AWS platform. The marketplace, billing, and AWS API Gateway connection to API plans exists. When you combine the authentication and service composition available at the AWS API Gateway layer, with the IAM policy solutions available via AWS, an enterprise grade solution for delivering this model securely at scale, comes into focus.</p>

<p><strong>Incentivizing Government API Vendors and Contractors</strong>
Keep everything small, and well defined. Measure, reported upon, and priced using the cloud model, connecting to a clear set of API governance guidance and expectations. The following areas can support paying and incentivizing contractors based upon not just usage, but also meeting the API contract.</p>

<ul>
  <li><strong>Management</strong> - API management puts all microservices into plans, then log, meter, and track on value exchanged at this level.</li>
  <li><strong>Marketplace</strong> - Turning the platform into a marketplace that can be occupied by a variety of internal, pattern, vendor, 3rd party, and public actors.</li>
  <li><strong>Monetization</strong> - Granular understanding of all the resources it takes to deliver each individual service, and understand the costs associated with operating at scale.</li>
  <li><strong>Plans</strong> - A wealth of API plans in place at the API gateway level, something that is tied to IAM policies, and in alignment with API governance expectations.</li>
  <li><strong>Governance</strong> - Providing a map, and supporting guidance around government platform API governance. Understanding, measuring, and enforcing consistency across the API lifecycle–platform wide.</li>
  <li><strong>Value Exchange</strong> - Using the cloud model, which is essentially the original API management, marketplace, and economy model. Not just measuring consumption, but used to maximize and generate revenue from the value exchanged across the platform.</li>
</ul>

<p>When you operate APIs on AWS and Azure, the platform as a service layer can utilize and benefit from the underlying infrastructure as a service monetization framework. Meaning, you can use AWS’s business model for managing the measuring, paying, and incentivizing of microservice operators. All the gears are there, they just need to be set in motion to support the management of a government API marketplace platform.</p>

<p>I have been studying Amazon full time for almost eight years. I’ve been watching Azure play catch up for the last three years. I run my infrastructure, and a handful of clients on AWS. I understand the API landscape of both providers, and how they can be woven into this vision for the business of government APIs. I see the AWS API stack, and the Azure API stack, as a starter set of services that can be built upon to deliver a government microservices platform. All the components are there. It just need the first set of services to be defined, delivering the essential building blocks any platform needs–things like compute, storage, dns, messaging, etc. The progress to other more outward, application, and system integration services.</p>

<p>My objective with this approach is to enable government services to be delivered as individual, self-contained units, that can be used as part of a larger orchestration of government services. Open up government and letting some sunlight in. Think about what Amazon has been able to achieve by delivering its own internal operations as services, and remaking not just retail, but also touching almost every other industry with Amazon Web Services. The Amazon Web Services myth story provides a powerful and compelling narrative for any company, organizations, institution, or government agency to emulate.</p>

<p>My proposal is not meant to be a utopian vision for how government works. However it is meant to shine a light on existing ways of delivering services via the cloud, with APIs at the center. Helping guide each service in its own individual journey, while also serving the overall mission of the platform–to help the veteran be successful in their own personal journey.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/28/the-business-of-running-government-as-a-microservices-platform/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/27/what-it-the-streamdata-io-api-gallery/">What Is The Streamdata.io API Gallery?</a></h3>
        <span class="post-date">27 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation.</p>

<p><strong>What is the Streamdata.io API Gallery?</strong><br />
It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas.</p>

<p><strong>What are we looking to accomplish with the Streamdata.io API Gallery?</strong><br />
Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible.</p>

<p><strong>What is the opportunity around the Streamdata.io API Gallery?</strong><br />
Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow.</p>

<p><strong>Why is this relevant to Streamdata.io, and their road map?</strong><br />
It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which potential customers might possibly find interesting, bringing them new API consumers, while also opening their eyes up to the event-driven opportunity that exists out there.</p>

<p><strong>Distilling Things Down A Bit More</strong><br />
Ok, that answers the general questions about what the Streamdata.io API Gallery is, and why we are building it. Now I want to distill down a little bit more to help me articulate the gallery as part of a series of presentations, existing as just a handful of bullet points. Helping get the point across in hopefully 60 seconds or less.</p>

<ul>
  <li>What is the Streamdata.io API Gallery?
    <ul>
      <li>API directory, for finding individual units of compute within specific topics.</li>
      <li>OpenAPI (fka Swagger) driven, making each unit of value usable at run-time.</li>
      <li>APIs.json indexed, making the collections of resources easy to search and use.</li>
      <li>Github hosted, making it forkable and continuously deployable and integrate(able).</li>
    </ul>
  </li>
  <li>Why is the Streamdata.io Gallery relevant?
    <ul>
      <li>It maps out the API universe with an emphasis on the value each individual API path possesses.</li>
      <li>Categories, tags, and indexes APIs into collections which are published to Github.</li>
      <li>Provides a human and machine friendly view of the existing publish and subscribe landscape.</li>
      <li>Begins to organize the API universe in context of a real time event-driven messaging world.</li>
    </ul>
  </li>
  <li>What is the opportunity around the Streamdata.io API Gallery?
    <ul>
      <li>Redefining the API landscape from an event-driven perspective.</li>
      <li>Quantify, qualify, and rank APIs to understand what is the most interesting and highest quality.</li>
      <li>Help API providers realize events occurring via their existing platforms.</li>
      <li>Begin moving beyond a request and response model to an event-driven reality.</li>
    </ul>
  </li>
</ul>

<p>There is definitely a lot more going on within the Streamdata.io API Gallery, but I think this captures the essence of what we are trying to achieve. A lot of what we’ve done is building upon my existing API Stack work, where I have worked to profile and index public APIs using OpenAPI and APIs.json, but this round of work is taking things to a new level. With API Stack I ended up with lists of companies and organizations, each possessing a list of APIs. The Streamdata.io API Gallery is a list of API resources, broken down by the unit of value they bring to the table, which is further defined by whether it is a GET, POST, or PUT–essentially a publish or subscribe opportunity.</p>

<p>Additionally, I am finally finding traction with the API rating system(s) I have been developing for the last five years. Profiling and measuring the companies behind the APIs I’m profiling, and making this knowledge available not just at discover time, but potentially at event and run time. Basically being able to understand the value of an event when it happens in real time, and be able to make programmatic decisions regarding whether we care about the particular event or not. Eventually, allowing us to subscribe only to the events that truly matter to us, and are of the highest value–then tuning out the rest. Delivering API ratings in an increasingly crowded and noisy event-driven API landscape.</p>

<p>We have the prototype for the Streamdata.io API Gallery ready to go. We are still adding APIs, and refining how they are tagged and organized. The rating system is very basic right now, but we will be lighting up different dimensions of the rating(s) algorithm, and hopefully delivering on different angles of how we quantify the value of events that occurring. I’m guessing we will be doing a soft launch in the next couple of weeks to little fanfare, and it will be something that builds, and evolves over time as the API index gets refined and used more heavily.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/27/what-it-the-streamdata-io-api-gallery/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting-session/">Three Areas I Would Like To Cover When We Sit Down For An API Consulting Session</a></h3>
        <span class="post-date">27 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_146_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m putting together some presentations for a handful of upcoming engagements, where I’m wanting to help my audience understand what an initial engagement will look like. While I am looking to have just a handful of bullets that can live on a single, or handful of slides, I also want a richer narrative to go along with it. To achieve this I rely on my blog, which helps me work my way through the details of what I do, and distill things down into something that I can deliver on the ground within the companies, organizations, institutions, and government agencies I am conducting business with.</p>

<p>When I am sitting down with a new audience, and working to help them understand how I can help them begin, jumpstart, revive, and move forward with their API journey, I’m usually breaking things into three main areas:</p>

<ul>
  <li><strong>Landscape Mapping</strong> - Establish a map of what currently is within an organization.
    <ul>
      <li><strong>Internal Resources</strong> - What existing web services, APIs, teams, and resources exist?</li>
      <li><strong>External Objectives</strong> - What are the external objectives of doing APIs?</li>
    </ul>
  </li>
  <li><strong>Strategy Development</strong> - Craft a coherent strategy for moving forward with APIs.
    <ul>
      <li><strong>API Lifecycle</strong> - Lay out a step by step list of stops along a modern API life cycle.</li>
      <li><strong>API Support</strong> - Identify how the strategy and operations will be supported within an organization.</li>
      <li><strong>API Evangelism</strong> - Consider how the message around API operations will spread internally, and externally.</li>
    </ul>
  </li>
  <li><strong>Execution</strong> - Identify a clear set of next steps regarding how APIs will evolve.
    <ul>
      <li><strong>Infrastructure</strong> - What services, tooling, and other API infrastructure is needed?</li>
      <li><strong>Resources</strong> - What resources have been identified for moving the API conversation forward?</li>
      <li><strong>Governance</strong> - What is the governance strategy for measuring, reporting upon, and enforcing the deliver of APIs across the API lifecycle presented.</li>
    </ul>
  </li>
</ul>

<p>When I present to a new group of people within an organization, this is the outline I am looking to flesh out. I have to understand what is already occurring (or not) on the ground, which is why I need the landscape map. Then, borrowing from my existing API research I can help develop a a detailed strategy, which includes the critical elements of how we will be supporting and evangelizing the effort–which without, API efforts will always struggle. After that, I want to quickly get to work on how we will be executing on this vision, even if it just involves more investment in the landscape map, and overall strategy.</p>

<p>I am working on more detailed materials to hand out prior to, and at the time I sit down with new clients, but I wanted to articulate in a single page, and using a simple set of bullets what I am looking to accomplish with any new consulting relationship. With a map in hand, and an strategy in mind, I’m confident that I can help folks I talk with move forward with their API journey in a more meaningful way. Something not everyone I talk with is confident in doing on their own, but with a little assistance, I’m pretty sure they will be able to get to work defining what the API journey will look like for their organization.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting-session/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/27/mapping-out-the-api-landscape/">Mapping Out The API Landscape</a></h3>
        <span class="post-date">27 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation.</p>

<p><strong>What is the Streamdata.io API Gallery?</strong>
It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas.</p>

<p><strong>What are we looking to accomplish with the Streamdata.io API Gallery?</strong>
Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible.</p>

<p><strong>What is the opportunity around the Streamdata.io API Gallery?</strong>
Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow.</p>

<p><strong>Why is this relevant to Streamdata.io, and their road map?</strong>
It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which potential customers might possibly find interesting, bringing them new API consumers, while also opening their eyes up to the event-driven opportunity that exists out there.</p>

<p><strong>Distilling Things Down A Bit More</strong>
Ok, that answers the general questions about what the Streamdata.io API Gallery is, and why we are building it. Now I want to distill down a little bit more to help me articulate the gallery as part of a series of presentations, existing as just a handful of bullet points. Helping get the point across in hopefully 60 seconds or less.</p>

<ul>
  <li>What is the Streamdata.io API Gallery?
    <ul>
      <li>API directory, for finding individual units of compute within specific topics.</li>
      <li>OpenAPI (fka Swagger) driven, making each unit of value usable at run-time.</li>
      <li>APIs.json indexed, making the collections of resources easy to search and use.</li>
      <li>Github hosted, making it forkable and continuously deployable and integrate(able).</li>
    </ul>
  </li>
  <li>Why is the Streamdata.io Gallery relevant?
    <ul>
      <li>It maps out the API universe with an emphasis on the value each individual API path possesses.</li>
      <li>Categories, tags, and indexes APIs into collections which are published to Github.</li>
      <li>Provides a human and machine friendly view of the existing publish and subscribe landscape.</li>
      <li>Begins to organize the API universe in context of a real time event-driven messaging world.</li>
    </ul>
  </li>
  <li>What is the opportunity around the Streamdata.io API Gallery?
    <ul>
      <li>Redefining the API landscape from an event-driven perspective.</li>
      <li>Quantify, qualify, and rank APIs to understand what is the most interesting and highest quality.</li>
      <li>Help API providers realize events occurring via their existing platforms.</li>
      <li>Begin moving beyond a request and response model to an event-driven reality.</li>
    </ul>
  </li>
</ul>

<p>There is definitely a lot more going on within the Streamdata.io API Gallery, but I think this captures the essence of what we are trying to achieve. A lot of what we’ve done is building upon my existing API Stack work, where I have worked to profile and index public APIs using OpenAPI and APIs.json, but this round of work is taking things to a new level. With API Stack I ended up with lists of companies and organizations, each possessing a list of APIs. The Streamdata.io API Gallery is a list of API resources, broken down by the unit of value they bring to the table, which is further defined by whether it is a GET, POST, or PUT–essentially a publish or subscribe opportunity.</p>

<p>Additionally, I am finally finding traction with the API rating system(s) I have been developing for the last five years. Profiling and measuring the companies behind the APIs I’m profiling, and making this knowledge available not just at discover time, but potentially at event and run time. Basically being able to understand the value of an event when it happens in real time, and be able to make programmatic decisions regarding whether we care about the particular event or not. Eventually, allowing us to subscribe only to the events that truly matter to us, and are of the highest value–then tuning out the rest. Delivering API ratings in an increasingly crowded and noisy event-driven API landscape.</p>

<p>We have the prototype for the Streamdata.io API Gallery ready to go. We are still adding APIs, and refining how they are tagged and organized. The rating system is very basic right now, but we will be lighting up different dimensions of the rating(s) algorithm, and hopefully delivering on different angles of how we quantify the value of events that occurring. I’m guessing we will be doing a soft launch in the next couple of weeks to little fanfare, and it will be something that builds, and evolves over time as the API index gets refined and used more heavily.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/27/mapping-out-the-api-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/">One Of The Best API Getting Started I Have Come Across</a></h3>
        <span class="post-date">26 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/starling/starling-home-page.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m working my way through banking and Fintech companies in the UK, and I stumbled across the Starling banking API. I began doing my usual clicking around as I do with any API, looking at the documentation, the getting started, and other primary links. After landing on <a href="https://developer.starlingbank.com/get-started">the Starling getting started page</a>, I have to say that it is the single best example of a getting started page I have ever come across in my time as API Evangelist. It is robust, informative, well laid out, and has everything you need to well, get started.</p>

<p>The Starling getting started page is broken up into six separate sections:</p>

<p>1) Register Your Application<br />
2) Setup Starter Kit<br />
3) Play in the Sandbox<br />
4) Personal Access<br />
5) Going Live<br />
6) Contact Us<br /></p>

<p>Each getting started section has a simple, concise description with relevant visuals and code samples, as well as possession simple action buttons, like sign, login, register application, and the other meaningful things you need to get started. The Starling getting started is going to become my go to example of how to create an API getting started page. You can really tell whoever put it together spent a lot of time refining it, and walking through it until it was 100% complete.</p>

<p>Starling even has a sandbox, marketplace, and a join Slack button. I can’t rave about their approach enough. I’m going to turn it into a case study regarding how to create a getting started page, and showcase on the home page of the site. I wish every API put as much energy into their getting started page as Starling has. It would take the friction out of on-boarding APis, and make it a much more pleasant experience.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/">The Banking API Actors In The UK</a></h3>
        <span class="post-date">26 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/shakespeare/shakespeare.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been profiling <a href="https://www.openbanking.org.uk/">the work of the Open Banking Implementation Entity when nit comes to banking API standards in the UK</a>. As part of my getting up to speed on the banking ecosystem in the UK, and Europe, I’ve been posting a series of small blog posts, outlining different aspects of how things work, and who the players are. While going through the Open Banking documentation, I came across a great list of the “actors” int he Open Banking API ecosystem, which taught me a lot about who is involved, and was worth reposting here as a list.</p>

<p>The Open Banking eco-system consists of a number of actors, which may be a natural person or an entity:</p>

<ul>
  <li><strong>Payment Service User (PSU)</strong> - Person - Payment Services User is a natural or legal person making use of a payment service as a payee, payer or both</li>
  <li><strong>Payment Service Provider (PSP)</strong> - Legal Entity - A legal entity (and some natural persons) that provide payment services as defined by PSD2 Article 4(11)</li>
  <li><strong>Account Servicing Payment Service Provider (ASPSP)</strong> - Legal Entity - Account Servicing Payment Service Providers provide and maintain a payment account for a payer as defined by the PSRs and, in the context of the Open Banking Ecosystem are entities that publish Read/Write APIs to permit, with customer consent, payments initiated by third party providers and/or make their customers’ account transaction data available to third party providers via their API end points.</li>
  <li><strong>Third Party Providers / Trusted Third Parties (TPP)</strong> - Legal Entity - Third Party Providers are organisations or natural persons that use APIs developed to Standards to access customer’s accounts, in order to provide account information services and/or to initiate payments. Third Party Providers are either/both Payment Initiation Service Providers (PISPs) and/or Account Information Service Providers (AISPs).</li>
  <li><strong>Payment Initiation Service Provider (PISP)</strong> - Legal Entity - A Payment Initiation Services Provider provides an online service to initiate a payment order at the request of the payment service user with respect to a payment account held at another payment service provider.</li>
  <li><strong>Account Information Service Provider (AISP)</strong> - Legal Entity - An Account Information Service provides account information services as an online service to provide consolidated information on one or more payment accounts held by a payment service user with one or more payment service provider(s).</li>
  <li><strong>TPP Primary Technical Contact (TPP-PTC)</strong> - Person - A Primary Technical Contact is an individual nominated by a TPP to have access to the Directory and will be able to nominate other Directory technical users. This should be a main point of contact on technical configuration and a senior member of staff with responsibility for the management of the Open Banking digital identity.</li>
  <li><strong>TPP Secondory Technical Contact (TPP-STC)</strong> - Person - A person that carries out technical operations on behalf of a TPP. A TPP-STC has the same permissions as a TPP-PTC except for the ability to nominate other Directory technical users.</li>
  <li><strong>ASPSP Primary Technical Contact (ASPSP-PTC)</strong> - Person - A Primary Technical Contact is an individual nominated by the ASPSP to have access to the Directory and will be able to nominate other Directory technical users. This should be a main point of contact on technical configuration and a senior member of staff with responsibility for the management of the Open Banking digital identity.</li>
  <li><strong>ASPSP Secondory Technical Contact (ASPSP-STC)</strong> - Person - A person that carries out technical operations on behalf of an ASPSP. An ASPSP-STC has the same permissions as a ASPSP-PTC except for the ability to nominate other Directory technical users.</li>
  <li><strong>Regulatory Bodies</strong> - Legal Entity - Government or industry bodies that have a regulatory role in the payments industry. This includes, but is not limited to, the UK Competition &amp; Markets Authority (CMA), HM Treasury (HMT), EBA, etc.	No</li>
  <li><strong>Member State Competent Authorities (MSCA)</strong> - Legal Entity - Regulatory Body	The regulatory body (or bodies) in each of the EU member states that is responsible for maintaining a register of payment institutions.</li>
  <li><strong>Financial Conduct Authority (FCA)</strong> - Legal Entity - The Financial Conduct Authority is the competent authority for the UK	No</li>
  <li><strong>Open Banking Limited (OB)</strong> - Legal Entity - The Open Banking Implementation Entity is the delivery organization working with the CMA9 and other stakeholders to define and develop the required APIs, security and messaging standards that underpin Open Banking.</li>
  <li><strong>OB Directory Administrator</strong> - Person - A person working for OB that is responsible for executing various technical processes related to the Directory on behalf of OB.</li>
</ul>

<p>I wish I could find breakdowns of the actors within every industry I work in like this. I’ve been studying up on how the banking industry works, but my knowledge moved forward quite a bit after studying this. It is the best explanation of the acronyms I’ve been coming across lately, and is the best breakdown of everyone involved in an API ecosystem I have ever come across in my eight years of studying the space.</p>

<p>Open Banking is doing some really interesting work on the banking API front. This is just one of many artifacts I’m coming across that demonstrate they really have their act together when it comes to developing banking API specifications in the UK.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">Round Two Of The Department of Veterans Affairs Lighthouse Platform RFI</a></h3>
        <span class="post-date">24 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/23_113_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m spending some more time thinking about APIs at the Department of Veterans Affairs (VA), in response to round two of their request for information (RFI). <a href="http://apievangelist.com/2017/10/26/my-response-on-the-department-of-veterans-affairs-rfi-for-the-lighthouse-api-management-platform/">A couple months back I had responded to an earlier RFI</a>, providing as much information as I could think of, for consideration as part of their API journey. As a former VA employee, and son of two Vietnam Vets (yes two), you can say I’m always willing to invest some in APIs over at the VA.</p>

<p>To provide a response, I have taken the main questions they asked, broken them out here, and provided answers to the best of my ability. In my style, the answers are just free form rants, based upon my knowledge of the VA, and the wider API space. It is up to the VA, to decide what is relevant to them, and should be included in their agency API strategy.</p>

<p><strong><em>2. Current Scope<br />
While the acquisition strategy for Lighthouse has not yet been formalized, VA envisions that the program will consist of multiple contracts.  For example, a contract for recommending policy and standards to form governance would likely be separate from an API build team.  The key high level activities below are anticipated to be included within these contracts, and VA is requesting feedback from industry on how these activities should be aligned between multiple contracts.  The list below is not inclusive of all tasks required to support this program.  Additionally, VA intends to provide the IAM solution and the provisioning of necessary cloud resources to host the proposed technology stack.  VA’s current enterprise cloud providers are Microsoft Azure and Amazon Web Services.</em></strong></p>

<p><strong>Microservice Focused Operational &amp; Implementation</strong><br />
Lighthouse should embrace a microservices way of doing things, so that the platform can avoid legacy trappings when it comes to delivering software at the VA, which have resulted in large, monolithic systems, possessing enormous budgets, and entrenched teams, that are able to develop a resistance to change and evolution. This microservices way of doing things should be adopted internally, as well as externally, then applied to the technology, business, and politics of delivering ALL Lighthouse infrastructure.</p>

<p>All contracts should be defined and executed in a modular way, with the only distinction between  projects being operational, or for specific project implementations. Everything should be delivered as microservices, no matter whether it is in support of operating the Lighthouse platform, or delivering services to Lighthouse-driven applications. The technology and business of each service should be self-contained, modular, and focusing on doing one thing, and doing it well. Ensuring all services executed as part of Lighthouse operations are decoupled, working independently, allowing for easily defining, delivering, managing, evolving, and deprecating of every operational and implementational service that makes Lighthouse work.</p>

<p>Operational services will be the first projects delivered via the platform, and will be used to establish and mature the Lighthouse project deliver workflow, but then going forward, every additional operational, as well as specific implementation focused service will utilize the same workflow and life cycle.</p>

<ul>
  <li><strong>Definitions</strong> - Everything begins as a set of definitions. Leveraging OpenAPI, JSON Schema, Dockerfiles, and other common definitions to provide a human, and machine readable definition of every project, which is ultimately delivered as a microservice.</li>
  <li><strong>Github</strong> - Each microservice begins as either a public or private Github repository, with a README index of the definition of what a service will deliver. Providing a self-contained, continuously deployed and integration blueprint of what a service does.</li>
  <li><strong>Architecture</strong> - Always providing a comprehensive outline all backend architecture used to support a specific service, including the technical, as well as the business, and security policy elements of what it takes to deliver the required service.</li>
  <li><strong>Tooling</strong> - Always providing a comprehensive outline of any tools used as part of delivering a service, to provide what is needed from a front-end delivery and execution vantage point.</li>
  <li><strong>Lifecycle</strong> - Establish a lifecycle, that each service will need to pass through, ensuring consistent delivery, and management of services that adhere to governance standards.
    <ul>
      <li><strong>define</strong> - What definitions are required for services?</li>
      <li><strong>design</strong> - What is the API design guidance involved?</li>
      <li><strong>mock</strong> - How are APIs and data virtualized as part of development?</li>
      <li><strong>portal</strong> - Which portals are service published to, or will possess?</li>
      <li><strong>document</strong> - What documentation is required and delivered?</li>
      <li><strong>test</strong> - Where are the code, as well as interface level tests?</li>
      <li><strong>clients</strong> - What client environment are in use for design, development, and testing?</li>
      <li><strong>**</strong>* - Pause there, and repeat until the desired service is realized…</li>
      <li><strong>deploy</strong> - How are services delivered as part of a containerized, continuous deployment pipeline?</li>
      <li><strong>dns</strong> - What DNS is needed to address and route traffic to services?</li>
      <li><strong>manage</strong> - What API management level services are in place to secure, log, limit, and report of API and service consumption?</li>
      <li><strong>logging</strong> - What is the logging stack, how is it shipped, analyzed, and reported upon?</li>
      <li><strong>monitor</strong> - What monitors are required and in place for each service?</li>
      <li><strong>performance</strong> - How is performance measured and reported upon?</li>
      <li><strong>sdk</strong> - What client libraries, SDKs, and samples in place for service integration?</li>
      <li><strong>depenencies</strong> - What internal service, and external API dependencies are in play?</li>
      <li><strong>licensing</strong> - What is the data, code, interface, and other licensing that apply?</li>
      <li><strong>privacy</strong> - Are privacy policies in place, and considered for the platform, partners, developers, and end-users.</li>
      <li><strong>terms</strong> - Are terms of service in place, and independently considered for each service?</li>
      <li><strong>monetization</strong> - What are the operating costs, and other monetization considerations?</li>
      <li><strong>plans</strong> - What API consumption plans, rate limits, and policies in place to govern service usage?</li>
      <li><strong>support</strong> - What support mechanisms are in place, with relevant point of contacts?</li>
      <li><strong>communication</strong> - What communication channels are in place, such as blogs, social, and messaging channels?</li>
      <li><strong>observability</strong> - What is the observability of each service, from open source to monitoring, and CI/CD workflows, ensuring it can be audited?</li>
      <li><strong>discovery</strong> - What is required to register, route, and discover an API as part of overall operations?</li>
      <li><strong>evangelism</strong> - What is the plan for making sure a service is known, used, and evangelized amongst target audience?</li>
    </ul>
  </li>
  <li><strong>Governance</strong> - How is each step along the life cycle measured, reported upon, and audited as part of governance, to understand how a service is meeting platform requirements, and evolving along a maturity path–allowing for innovation to occur, and newer ideas to flourish, but also allow more hardened, secure, and mature services to rise to the top.</li>
</ul>

<p>The OpenAPI, JSON Schema, and other definitions for each microservice will ultimately be the contract for each project. Of course, to deliver the first set of operational platform services (compute, storage, DNS, pipeline, logging, etc.) these independent contracts might need to be grouped into a single, initial contract. Something that will also occur around different groups of services being delivered at any point in the future, but each individual service should be self-contained, with its own contract definition existing in it’s Github repository core.</p>

<p><em><strong>Question: API Roadmap Development (Backlog, Future)</strong></em><br />
Each service being delivered via Lighthouse will possess its own self-contained road map as part of its definition. Providing a standardized, yet scalable way to address what is being planned, is being delivered, operated, and when anything will ultimately be deprecated.</p>

<ul>
  <li><strong>Github Issues</strong> - Each Github repository has it’s own issues for managing all conversations around the service road map. Tags and milestones can be used to designate the past, present, future, and other relevant segmentation of the road map.</li>
  <li><strong>Micro / Macro</strong> - Each services posses micro level detail about the road map, which is available via Github APIs, in a machine readable way for inclusion at the macro level, serving governance, reporting, and higher level road map considerations.</li>
  <li><strong>Communication</strong> - Each service owner is responsible for road map related communication, support, and management providing their piece of the overall road map puzzle.</li>
</ul>

<p>The Lighthouse platform road map should work like an orchestra, with each participant bringing their contribution, but platform operators and conductors defining the overall direction the platforms is headed. At scale, Lighthouse will be thousands of smaller units, organized by hundreds of service owners and stewards, serving millions of end-users, with feedback loops in service through the stack.</p>

<p><em><strong>Question: Outreach (Internal &amp; External Parties)</strong></em><br />
Outreach is essential to the viability of any platform, and represents the business and political challenges that lie ahead for the VA, or any government agency looking to work seamlessly with public and private sector partners, as well as the public at large. There will be many services involved with Lighthouse operations that will need to be private, but the default should always be public, allowing for as much transparency and observability as possible, which will feed platform outreach in a positive way.</p>

<ul>
  <li><strong>Github Project Pages</strong> - Each Github repository can have a public facing Github Pages static site portal and landing page. Allowing for individual service, or group portals to exist, providing a destination for all stakeholders to get involved.</li>
  <li><strong>Github Social Framework</strong> - Github provides a wealth of outreach and communication solutions from organization and repository search, to issues and wikis, and tagging services with individual topics. All of which can be used as part of outreach and engagement in a private or public setting.</li>
  <li><strong>Twitter</strong> - Microblogging provides a great way to publish regular updates, and provide communication around platform operations.</li>
  <li><strong>Linkedin</strong> - Enterprise development groups, especially those in service of the government tend to use Github for establishing their profile, and maintaining their presence, which can be incorporated into all outreach efforts.</li>
  <li><strong>Blogs</strong> - The platform should possess its own public and / or private blogs, as well as potentially more topically, service, or project based blogs that expand outreach to the long tail of platform operations.</li>
</ul>

<p>This type of outreach around platform operations is something that scares the hell out of government folks, and the majority of government APIs operation are critically deficient in the area of outreach. This has to change. If there is no feedback loop in place, and outreach doesn’t occur regularly and consistently, the platform will not succeed. This is how the API world operates.</p>

<p><em><strong>Question: Management of API Request Process (Internal (VA)/External (Non-VA))</strong></em><br />
New services will always be needed. Operational and implementation related requests should all be treated the same. Obviously there will be different prioritization mechanisms in place, but API requests should just be the birth of any new service, allowing it to begin its journey, and transit through the API lifecycle described above. Not all requests will reach deployment, and not all deployments will reach maturity, but all API requests should be treated equally.</p>

<ul>
  <li><strong>Definitions</strong> - Each API request begins with a definition. A simple description of what a service will do.</li>
  <li><strong>Github</strong> - Each API request begins its journey as a Github repository, with a README containing its basic definition, and conversation around its viability within Github issues.</li>
  <li><strong>JSON Schema</strong> - As part of each request, all data that will be leverage as part of service operations should be defined as JSON Schema, and included in the Github repository.</li>
  <li><strong>OpenAPI</strong> - Additionally, the access to the service, and its underlying data and resource should be defined using a machine readable OpenaPI definition, outlining the contract of the service.</li>
  <li><strong>Certification</strong> - Some stakeholders will have submitted API requests before, and better understand the process, and be certified owners of existing services, working as part of trusted organizations, expediting and prioritizing the request process.</li>
  <li><strong>Template(s)</strong> - The most common service patterns to emerge should be defined as template, providing seeds and starter projects to help expedite and streamline the API request process, ensuring all the moving parts are there to make a decision, in a forkable, replicable package.</li>
</ul>

<p>New API requests should be encourage. Anyone should be able to submit a new service, replicate, or augment an existing service, or respond to a platform API RFP. The life cycle described above should be open to everyone looking to submit an API request. Allowing them to define, design, mock, and iterate their submission. Even providing a nearly usable representation of a service, even before the idea or service is accepted. Forcing everyone to flesh out their service, deliver a viable proof of concept, that will streamline the API acceptance process.</p>

<p><em><strong>Question: Propose, Implement and Manage the PaaS (technology stack)</strong></em><br />
As mentioned before, this aspect of Lighthouse should be delivered as microservices, alongside every other service being delivered via the platform. It just so happens that this portion of the stack will be the first to be delivered, and be iterated upon, evolved, and deprecated just like any other service. To put this in perspective, I will outline the AWS, and Azure infrastructure need to support management of the platform later on in this post, while considering the fact that AWS and Azure have been on the same journey that the VA is on with Lighthouse, something that has been playing out for the last decade.</p>

<p>The VA wants to be the Amazon of serving veterans. They want internal groups, vendors, contractors, veteran health and service organizations, and independent developers to come build their solutions on the Lighthouse platform. The VA should uses its own services for internal service delivery, as well as supporting external projects. The operational side of Lighthouse platform should be all microservice projects, with the underlying infrastructure being Azure or AWS solutions, providing a common platform as a service stack that can be leveraged, no matter where the actual service is deployed.</p>

<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>

<p><em><strong>Question: DevOps Continuous Integration and Continuous Delivery (CI/CD) of APIs</strong></em><br />
Every service in support of operations or implementations via the Lighthouse platform will exist as a self-contained Github repository, with all the artifacts needed to be included in any application pipeline. The basic DNA blueprint for each service should be crafted to support any single CI/CD service, or ideally even multiple types of CI/CD and orchestration solutions like AWS and Azure both support.</p>

<ul>
  <li><strong>Microservices</strong> - Lighthouse CI/CD will be all about microservice orchestration, and using a variety of pipelines to deliver and evolve initially hundreds, and eventually thousands of services in concert.</li>
  <li><strong>Github</strong> - Github will the cellular component driving the Lighthouse CI/CD workflow, providing individual service “legos” that can be composed, assembled, disassembled, and delivered in any way.</li>
  <li><strong>Definitions</strong> - Each microservice will contain all the artifacts needed for supporting the entire life cycle listed above, driven by a variety of CI/CD pipelines. Leveraging dockerfiles, build packages, OpenAPI definitions, schema, and other definitions to continuously deliver and integrate across platform operations.</li>
</ul>

<p>Both AWS and Azure provide CI/CD workflows, which can be used to satisfy the portion of the RFI. I will list out all the AWS and Azure services I think should be considered below. Additionally, Jenkins, CircleCI, or other 3rd party CI/CD could easily be brought in to deliver on this aspect of platform delivery. The microservices core can be used as part of any pipeline delivery model.</p>

<p><em><strong>Question: Environment Operations &amp; Maintenance (O&amp;M)</strong></em><br />
Again, everything operates as microservices, and gets delivered independently as services that can be configured and maintained as part of overall platform operations and maintenance, or in service of individual services, and groups of services supporting specific implementations.</p>

<ul>
  <li><strong>Microservices</strong> - Everything is available as microservices, allowing the underlying environment operations and maintainenace to be orchestration, and optimized in real time.</li>
</ul>

<p>Each of the AWS and Azure services listed below are APIs. They allow for the configuration and management of each service via API or CLI, allowing the architecture to be seamlessly managed as part of the overall API stack, as well as the CI/CD pipeline. Making environment operations and maintenance, just part of the continuous delivery cycle(s).</p>

<p><em><strong>Question: Release Management</strong></em><br />
Release occurs at the granular service level. With Github and CI/CD as the vehicle for moving release forward daily, versioning, defining, and communicating all the way. With the proper code and API level testing in place, release management can happen safely at scale.</p>

<ul>
  <li><strong>Github</strong> - Github version control, branches, and release management should  be used as part of the overall release management strategy.</li>
  <li><strong>Versioning</strong> - Establishment of a service versioning strategy for minor and major code, and interface releases, allowing independent release management that can occur at the higher orchestration level</li>
  <li><strong>CI/CD Pipelines</strong> - Everything should be a pipeline, broken down by logical operational, organization, and project boundaries, operating on a continuous release cycle.</li>
  <li><strong>Microservices</strong> - Everything is operated independently, and released independently via containers, with approach dependency management as part of each release.</li>
  <li><strong>Definitions</strong> - OpenAPI and JSON Schema are versioned and use to act as the contract for each release.</li>
  <li><strong>Communications</strong> - Along with each release, comes a standard approach to notification, communication, and support.</li>
</ul>

<p>Release management will horizontally take a significant amount of time to wrap your head around. Moving forward hundreds, and thousands of services in concert won’t be easy. However it will be more resilient, and forgiving than moving forward a single monolith.</p>

<p><em><strong>Question: API Analytics</strong></em><br />
Awareness should be baked in by default to the Lighthouse platform, measuring everything, and reporting on it consistently, providing observability across all aspects of operations in alignment with security policies. Analysis should be its own set of operational services, that span the entire length of the Lighthouse platform.</p>

<ul>
  <li><strong>Log Shipping</strong> - The database, container, web server, management, and DNS logs for ALL services should be shipped, and centralized, for complete access and analysis.</li>
  <li><strong>APIs</strong> - Centralized logs should be its own service, with programmatic access to logs for all platform services.</li>
  <li><strong>Modular</strong> - Analytics should be modular, bit-size API-driven elements that can be mixed, composed, published, and visualized in reusable ways.</li>
  <li><strong>Embeddables</strong> - Modular, embeddable UI elements should be developed as applications on top of platform analytics APIs, allowing for portable dashboard that can be remixed, reused, and evolved.</li>
  <li><strong>Search</strong> - The logging and reporting layer of the platform should have a core search element, allowing all logs to searched, as well as the logs for how API consumers are analyzing logs (mind blown).</li>
  <li><strong>Continouous</strong> - As with all other services, analytics, reporting, and visualizations should be continuous, and ever evolving and deployed on a day to day, week to week basis.</li>
</ul>

<p>A standard logging strategy across all services is how we achieve a higher level of API analytics, going beyond just database or web server statics, and even API management analytics, providing end to end, comprehensive platform service measurement, analysis, reporting, and visualization. Allowing platform operators, consumers, and auditors to access and understand how all service are being used, or not being used.</p>

<p><em><strong>Question: Approval to Operate (ATO) Support for Environments</strong></em><br />
Every service introduced as part of the Lighthouse platform should have all the information required to support ATO, with it baked into the governance and maturity life cycle for any service. It actually lends itself well to the maturity elements of the lifecycle above, ensuring there is ATO before anything is deployed.</p>

<ul>
  <li><strong>Definitions</strong> - All definitions are present for satisfying ATO.</li>
  <li><strong>Github</strong> - Everything is self-contained within a single place for submission.</li>
  <li><strong>Governance</strong> - ATO is part of the governance process, while still allowing for innovation.</li>
  <li><strong>Micro / Macro</strong> - ATO for each individual service can be considered, as well as at the project, group levels, understanding where services fit in at macro level.</li>
</ul>

<p>ATO can be built into the templated API request and submission process discussed earlier, allowing for already approved architecture, tooling, and patterns to be used over and over, streamlining the ATO cycle. Helping service developers enjoy more certainty around the ATO process, while still allowing for innovation to occur, pushing the ATO definition and process when it makes sense.</p>

<p><em><strong>Question: Build APIs including system level APIs that connect into backend VA systems</strong></em><br />
Everything is a microservice, and there are plenty of approaches to ensure that legacy backend systems can enjoy continued use and evolution through evolved APIs. The API life cycle allows for the evolution of existing backend systems that operate in the cloud and on-premise in small, bit-size service implementations.</p>

<ul>
  <li><strong>Gateway</strong> - AWS API Gateway and Azure API management makes it easy to publish newer APIs on top of legacy backend systems.</li>
  <li><strong>Facades</strong> - Establishing facade patterns for modernizing, and evolving legacy systems, allowing them to take on a new interface, while still maintaining existing system.</li>
  <li><strong>OpenAPI</strong> - Map out newer APIs using OpenAPI, then importing into gateways and wiring up with backend systems.</li>
  <li><strong>Schema</strong>- Mapping out the schema transformations from backend systems to front-end API requests and responses using JSON Path, and JSON Schema.</li>
  <li><strong>Microservices</strong> - Delivering newer APIs on top of legacy systems in smaller, more evolvable services.</li>
</ul>

<p>From the frontend, you shouldn’t be able to tell whether a legacy VA system is in use, or newer cloud infrastructure. All applications should be using APIs, and all APIs should be delivered as individual or groups of microservices, that do one thing and does it well. As APIs evolve, the backend systems should be decoupled and evolved as well, but until that becomes possible, all consumption of data, content, and other resources will be routed through the Lighthouse API stack.</p>

<p><em><strong>Question: API key management or managing third party access (authorization, throttling, etc.)</strong></em><br />
Both theAWS API Gateway, and Azure API Management allow for the delivery of modern API management infrastructure that can be used to govern internal, partner, and 3rd party access to resources. All applications should be using APIs, and ALL APIs should be using a standardized API management approach, no matter whether the consumption is internal or external. Ensuring consistent authorization, throttling, logging, and other aspects of doing business with APIs.</p>

<ul>
  <li><strong>IAM</strong> - Leverage API keys, JWT, and OAuth in conjunction with IAM policies governing which backend resources are available to API consumers.</li>
  <li><strong>Gateway</strong> - All API traffic is routed through the AWS API Gateway and Azure API management layers, allowing for consistent and comprehensive management across all API consumption.</li>
  <li><strong>Management</strong> - Apply consistent logging, rate limiting, transformations, error and security at the API management level, ensuring all services behave in the same way.</li>
  <li><strong>Plans</strong> - Establishing of a variety of API plans that dictate API levels of access, which services are accessible to different API key levels, that are in sync with backend IAM policies.</li>
  <li><strong>Logging</strong> - Every API call is logged, and contains user and application keys, allow ALL API consumption to be audited and reported upon, and responded to in real time.</li>
  <li><strong>Security</strong> - Providing a single point of entry, and the ability to shut down access, striking the balance between access and security which is the hallmark of doing APIs.</li>
</ul>

<p>API management is baked into the cloud. It is a discipline that has been evolving for over a decade, and is available on both the AWS and Azure platforms. The tools are there, Lighthouse just needs to establish a coherent strategy for authentication, service composition, logging, reporting, and responding to API consumption at scale in real time. Staying out of the way of consumers, while also ensuring that they only have access to the data, content, and other resources they are allowed to, in alignment with overall governance.</p>

<p><em><strong>Management of API lifecycle in cloud, hybrid, and/or on premise environments</strong></em><br />
All operational aspects of the Lighthouse platform should be developed as independent microservices, with a common API–no matter what the underlying architecture is. The DNS service API should be the same, regardless of whether it is managing AWS or Azure DNS, or possibly any other on-premise or 3rd party service–allowing for platform orchestration using a common API stack.</p>

<ul>
  <li><strong>Microservices</strong> - Each operational service is a microservice, with possibly multiple versions, depending on the backend architecture in use.</li>
  <li><strong>Containers</strong> - Every operational service is operated as a container, allowing it to run in any cloud environment.</li>
  <li><strong>Github</strong> - All services live as a Github repository, allowing it to be checked out and forked via any cloud platform.</li>
</ul>

<p>The modular, containerized, microservice approach to delivering the Lighthouse platform will allow for the deployment, scaling, and redundant implementation of services in any cloud environment, as well as on-premise, or hybrid scenarios. All services operate using the same microservice footprint, using containers, and a consistent API surface area, allowing for the entire platform stack to be orchestrated against no matter where the actual service resides.</p>

<p>_**Question: 3. Use Case<br />
To better provide insight into aligning activities to contracts, VA has provided the use case below. Please walk through this use case discussing each activity and the contract it would be executed under.</p>

<p><strong>Veteran Verification Sample Use Case: VA has a need for a Veteran Verification API to verify a Veteran status from a number of VA backend systems to be shared internally and externally as an authoritative data source.  These backend systems potentially have conflicting data, various system owners, and varying degrees of system uptime._</strong></p>

<p>This is a common problem within large organizations, institutions, and government agencies. This is why we work to decouple, modularize, and scale not just the technology of building applications on backend systems, but also the business, and politics of it all. Introducing a competitive element when it comes to data management access, and building in redundancy, resilience, and a healthier incentive model into how we provide access to critical data, content, and other resources.</p>

<p>I have personal experience with this particular use case. One of the things I did while working at the VA, was conduct public data inventory, and move forward the conversation around a set of veteran benefit web services, which included asking the question–who had the authoritative record for a veteran? Many groups felt they were the authority, but in my experience, nobody actually did entirely. The incentives in this environment weren’t about actually delivering a meaningful record on a veteran, it was all about getting a significant portion of the budget. I recommend decoupling the technology, business, and politics of providing access to veterans data using a microservices approach.</p>

<ul>
  <li><strong>Microservices</strong> - Break the veterans record into separate, meaningful services.</li>
  <li><strong>Definitions</strong> - Ensure the definitions for the schema and API are open and accessible.</li>
  <li><strong>Discovery</strong> - Make sure that the Veteran Verification API is full discoverable.</li>
  <li><strong>Testing</strong> - Make sure the Verification API is fully tested on a regular basis.</li>
  <li><strong>Monitoring</strong> - Ensure that there are regular monitors for the Verification API.</li>
  <li><strong>Redundancy</strong> - Encourage multiple implementations of the same API to be delivered and owned by separate groups in separate regions, with circuit breaker behavior in APIs and applications.</li>
  <li><strong>Balancing</strong> - Load balance between services and regions, allowing for auto-scaled APIs.</li>
  <li><strong>Aggregation</strong> - Encourage the development of aggregate APIs that bridge multiple source, providing aggregate versions of the veteran’s record, encouraging new service owners to improve on existing services.</li>
  <li><strong>Reliability</strong> - Incentivize and reward reliability with Verification API owners, through revenue and priority access.</li>
</ul>

<p>There should be no single owner of any critical VA service. Each service should have redundant versions of the service, available in different regions, and managed by separate owners. Competition should be encouraged, with facade and aggregate introduced, putting pressure on core service providers to deliver quality, or their service(s) will be de-prioritized, and newer services will be given traffic and revenue priority. The same backend database can be exposed via many different APIs, with a variety of owners and incentives in place to encourage the quality of service.</p>

<p>APIs, coupled with the proper terms of service in place can eliminate an environment where defensive data positions are established. If other API owners can get access to the same data, and offer a better quality API, then evangelize and gain traction with application owners, entrenched API providers will no longer flourish. Aggregate and facade APIs allow for the evolution of existing APIs, even if the API owners are unwilling to move and evolve. Shifting the definition of what is authoritative, making it much more liquid, allowing it to shift and evolve, rather than just be diluted and meaningless, as it is often seen in the current environment.</p>

<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/69_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>

<p>_<strong>Question: 4. Response<br />
In addition to providing the requested content above, VA asks for vendors to respond to the following questions:</strong></p>

<p><strong>Describe how you would align the aforementioned activities between contracts, and the recommended price structure for contracts?_</strong></p>

<p>Each microservice would have its own technical, business, and political contract, outline how the service will be delivered, managed, supported, communicated, and versioned. These contracts can be realized individually, or grouped together as a larger, aggregate contract that can be submitted, while still allowing each individual service within that contract to operate independently.</p>

<p>As mentioned before, the microservices approach isn’t just about the technical components. It is about making the business of delivering vital VA services more modular, portable, and scalable. Something that will also decouple and shift the politics of delivering critical services to veterans. Breaking things down into much more manageable chunks that can move forward independently at the contract level.</p>

<ul>
  <li><strong>Micro Procurement</strong> - One of the benefits of breaking down services into small chunks, is that the money needed to deliver the service can become much smaller, potentially allowing for a much smaller, more liquid and flowing procurement cycle. Each service has a micro definition of the monetization involved with the service, which can be aggregated by groups of services and projects.</li>
  <li><strong>Micro Payments</strong> - Payments for service deliver can be baked into the operations and life cycle of the service. API management excels at measuring how much a service is accessed, and testing, monitoring, logging, security, and other stops along the API life cycle can all be measured, and payments can be delivered depend on quality of service, as well as volume of service.</li>
</ul>

<p>Amazon Web Services already has the model for defining, measuring, and billing for API consumption in this way. This is the bread and butter of the Amazon Web Services platform, and the cornerstone of what we know as the cloud. This approach to delivering, scaling, and ultimately billing or payment for the operation and consumption of resources, just needs to be realized by the VA, and the rest of the federal government. We have seen a shift in how government views the delivery and operation of technical resources using the cloud over the last five years, we just need to see the same shift for the business of APIs over the next five years.</p>

<p>_<strong>Question: The Government envisions a managed service (ie: vendor responsible for all aspects including licenses, scaling, provisioning users, etc.) model for the entire technology stack.  How could this be priced to allow for scaling as more APIs are used?  For example, would it be priced by users, API calls, etc.?_</strong></p>

<p>API management is where you start this conversation. It has been used for a decade to measure, limit, and quantify the value being exchanged at the API level. Now that API management has been baked into the cloud, we are starting to see the approach being scaled to deliver at a marketplace level. With over ten years of experience with delivering, quantifying, metering and billing at the API level, Amazon is the best example of this monetization approach in action, with two distinct ways of quantifying the business of APIs.</p>

<ul>
  <li><strong>AWS Marketplace Metering Service</strong> - SaaS style billing model which provides a consumption monetization model in which customers are charged only for the number of resources they use–the best known cloud model.</li>
  <li><strong>AWS Contract Service</strong> - Billing customers in advance for the use of software, providing an entitlement monetization model in which customers pay in advance for a certain amount of usage, which could be used to deliver certain amount of storage per month for a year, or a certain amount of end-user licenses for some amount of time.</li>
</ul>

<p>This provides a framework for thinking about how the business of microservices can be delivered. Within these buckets, AWS provides a handful of common dimensions for thinking through the nuts and bolts of these approaches, quantifying how APIs can be monetized, in nine distinct areas:</p>

<ul>
  <li><strong>Users</strong> – One AWS customer can represent an organization with many internal users. Your SaaS application can meter for the number of users signed in or provisioned at a given hour. This category is appropriate for software in which a customer’s users connect to the software directly (for example, with customer-relationship management or business intelligence reporting).</li>
  <li><strong>Hosts</strong> – Any server, node, instance, endpoint, or other part of a computing system. This category is appropriate for software that monitors or scans many customer-owned instances (for example, with performance or security monitoring). Your application can meter for the number of hosts scanned or provisioned in a given hour.</li>
  <li><strong>Data</strong> – Storage or information, measured in MB, GB, or TB. This category is appropriate for software that manages stored data or processes data in batches. Your application can meter for the amount of data processed in a given hour or how much data is stored in a given hour.</li>
  <li><strong>Bandwidth</strong> – Your application can bill customers for an allocation of bandwidth that your application provides, measured in Mbps or Gbps. This category is appropriate for content distribution or network interfaces. Your application can meter for the amount of bandwidth provisioned for a given hour or the highest amount of bandwidth consumed in a given hour.</li>
  <li><strong>Request</strong> – Your application can bill customers for the number of requests they make. This category is appropriate for query-based or API-based solutions. Your application can meter for the number of requests made in a given hour.</li>
  <li><strong>Tiers</strong> – Your application can bill customers for a bundle of features or for providing a suite of dimensions below a certain threshold. This is sometimes referred to as a feature pack. For example, you can bundle multiple features into a single tier of service, such as up to 30 days of data retention, 100 GB of storage, and 50 users. Any usage below this threshold is assigned a lower price as the standard tier. Any usage above this threshold is charged a higher price as the professional tier. Tier is always represented as an amount of time within the tier. This category is appropriate for products with multiple dimensions or support components. Your application should meter for the current quantity of usage in the given tier. This could be a single metering record (1) for the currently selected tier or feature pack.</li>
  <li><strong>Units</strong> – Whereas each of the above is designed to be specific, the dimension of Unit is intended to be generic to permit greater flexibility in how you price your software. For example, an IoT product which integrates with device sensors can interpret dimension “Units” as “sensors”. Your application can also use units to make multiple dimensions available in a single product. For example, you could price by data and by hosts using Units as your dimension. With dimensions, any software product priced through the use of the Metering Service must specify either a single dimension or define up to eight dimensions, each with their own price.</li>
</ul>

<p>These dimensions reflect the majority of API services being sold out there today, we don’t find ourselves in a rut with measuring value, like just paying per API call. Allowing Lighthouse API plans to possess one or more dimensions, beyond any single use case.</p>

<ul>
  <li><strong>Single Dimension</strong> - This is the simplest pricing option. Customers pay a single price per resource unit per hour, regardless of size or volume (for example, $0.014 per user per hour, or $0.070 per host per hour).</li>
  <li><strong>Multiple Dimensions</strong> – Use this pricing option for resources that vary by size or capacity. For example, for host monitoring, a different price could be set depending on the size of the host. Or, for user-based pricing, a different price could be set based on the type of user (admin, power user, and read-only user). Your service can be priced on up to eight dimensions. If you are using tier-based pricing, you should use one dimension for each tier.</li>
</ul>

<p>This provides a framework that Lighthouse can provide to 3rd party developers, allowing them to operate their services within a variety of business models. Derived from many of the hard costs they face, and providing additional volume based revenue, based upon how may API calls of any particular service receives.</p>

<p>Beyond this basic monetization framework, I’d add in an incentive framework that would dovetail with the business models proposed, but then provide different pricing levels depending on how well the services perform, and deliver on the agreed upon API contract. There are a handful of bullets I’d consider here.</p>

<ul>
  <li><strong>Design</strong> - How well does a service meet API design guidelines set forth in governance guidance.</li>
  <li><strong>Monitoring</strong> - Has a service consistently met its monitoring goals, delivering against an agreed upon service level agreement (SLA).</li>
  <li><strong>Testing</strong> - Beyond monitoring, are APIs meeting granular interface testing, along a regular testing &amp; monitoring schedule.</li>
  <li><strong>Communication</strong> - Are service owners meeting expectations around communication around a service operations.</li>
  <li><strong>Support</strong> - Does a service meet required support metrics, making sure it is responsive and helpful.</li>
  <li><strong>Ratings</strong> - Provide a basic set of metrics, with accompanying ratings for each service.</li>
  <li><strong>Certification</strong> - Allowing service providers to get certified, receiving better access, revenue, and priority.</li>
</ul>

<p>All of the incentive framework is defined and enforced via the API governance strategy for the platform. Making sure all microservices, and their owners meet a base set of expectations. When you take the results and apply weekly, monthly, and quarterly against the business framework, you can quickly begin to see some different pricing levels, and revenue opportunities around all microservices emerge. You deliver consistent, reliable, highly ranked microservices, you get paid higher percentages, enjoy greater access to resources, and prioritization in different ways via the platform–if you don’t, you get paid less, and operate fewer services.</p>

<p>This model is already visible on the AWS platform. All the pieces are there to make it happen for any platform, operating on top of the AWS platform. The marketplace, billing, and AWS API Gateway connection to API plans exists. When you combine the authentication and service composition available at the AWS API Gateway layer, with the IAM policy solutions available via AWS, an enterprise grade solution for delivering this model securely at scale, comes into focus.</p>

<p><em><strong>Question: Is there a method of paying or incentivizing the contractor based on API usage?</strong></em><br />
I think I hit on this with the above answer(s). Keep payments small, and well defined. Measured, reported upon, and priced using the cloud model, connecting to a clear set of API governance guidance and expectations. The following areas can support paying and incentivizing contractors based upon not just usage, but also meeting the API contract.</p>

<ul>
  <li><strong>Management</strong> - API management puts all microservices into plans, then log, meter, and track on value exchanged at this level.</li>
  <li><strong>Marketplace</strong> - Turning the platform into a marketplace that can be occupied by a variety of internal, pattern, vendor, 3rd party, and public actors.</li>
  <li><strong>Monetization</strong> - Granular understanding of all the resources it takes to deliver each individual service, and understand the costs associated with operating at scale.</li>
  <li><strong>Plans</strong> - A wealth of API plans in place at the API gateway level, something that is tied to IAM policies, and in alignment with API governance expectations.</li>
  <li><strong>Governance</strong> - Providing a map, and supporting guidance around the Lighthouse platform API governance. Understanding, measuring, and enforcing consistency across the API lifecycle–platform wide.</li>
  <li><strong>Value Exchange</strong> - Using the cloud model, which is essentially the original API management, marketplace, and economy model. Not just measuring consumption, but used to maximize and generate revenue from the value exchanged across the platform.</li>
</ul>

<p>When you operate APIs on AWS and Azure, the platform as a service layer can utilize and benefit from the underlying infrastructure as a service monetization framework. Meaning, you can use AWS’s business model for managing the measuring, paying, and incentivizing of microservice owners. All the gears are there, they just need to be set in motion to support the management of a government API marketplace platform.</p>

<p><em><strong>Based on the information provided, please discuss your possible technology stack and detail your experience supporting these technologies.</strong></em><br />
Both Amazon Web Services and Azure provide the building blocks of what you need to execute the above. Each cloud platform has its own approach to delivering infrastructure at scale. Providing an interesting mix of API driven resources you can jumpstart any project.</p>
<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/27_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><strong>AWS</strong>
First, let’s take a look at what is relevant to this vision from the Amazon Web Services side of things. These are all the core AWS solutions on the table, with dashboard, API, and command line access to get the job done.</p>

<p><strong>Compute</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/ec2/?hp=tile&amp;so-exp=below">Amazon EC2</a> - Virtual Servers in the Cloud</li>
  <li><a href="https://aws.amazon.com/ec2/autoscaling/?hp=tile&amp;so-exp=below">Amazon EC2 Auto Scaling</a> - Scale Compute Capacity to Meet Demand</li>
  <li><a href="https://aws.amazon.com/ecs/?hp=tile&amp;so-exp=below">Amazon Elastic Container Service</a> - Run and Manage Docker Containers</li>
  <li><a href="https://aws.amazon.com/eks/?hp=tile&amp;so-exp=below">Amazon Elastic Container Service for Kubernetes</a> - Run Managed Kubernetes on AWS</li>
  <li><a href="https://aws.amazon.com/ecr/?hp=tile&amp;so-exp=below">Amazon Elastic Container Registry</a> - Store and Retrieve Docker Images</li>
  <li><a href="https://aws.amazon.com/lightsail/?hp=tile&amp;so-exp=below">Amazon Lightsail</a> - Launch and Manage Virtual Private Servers</li>
  <li><a href="https://aws.amazon.com/fargate/?hp=tile&amp;so-exp=below">AWS Fargate</a> - Run Containers without Managing Servers or Clusters</li>
  <li><a href="https://aws.amazon.com/batch/?hp=tile&amp;so-exp=below">AWS Batch</a> - Run Batch Jobs at Any Scale</li>
  <li><a href="https://aws.amazon.com/lambda/?hp=tile&amp;so-exp=below">AWS Lambda</a> - Run your Code in Response to Events</li>
  <li><a href="https://aws.amazon.com/serverlessrepo/?hp=tile&amp;so-exp=below">AWS Serverless Application Repository</a> - Discover, Deploy, and Publish Serverless Applications Auto Scaling</li>
</ul>

<p><strong>Storage</strong>
<a href="https://aws.amazon.com/s3/?hp=tile&amp;so-exp=below">Amazon S3</a> - Scalable Storage in the Cloud
<a href="https://aws.amazon.com/ebs/?hp=tile&amp;so-exp=below">Amazon EBS</a> - Block Storage for EC2
<a href="https://aws.amazon.com/efs/?hp=tile&amp;so-exp=below">Amazon Elastic File System</a> - Managed File Storage for EC2
<a href="https://aws.amazon.com/glacier1/?hp=tile&amp;so-exp=below">Amazon Glacier</a> - Low-cost Archive Storage in the Cloud
<a href="https://aws.amazon.com/storagegateway/?hp=tile&amp;so-exp=below">AWS Storage Gateway</a> - Hybrid Storage Integration</p>

<p><strong>Database</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/rds/aurora/?hp=tile&amp;so-exp=below">Amazon Aurora</a> - High Performance Managed Relational Database</li>
  <li><a href="https://aws.amazon.com/rds/?hp=tile&amp;so-exp=below">Amazon RDS</a> - Managed Relational Database Service for MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB</li>
  <li><a href="https://aws.amazon.com/dynamodb/?hp=tile&amp;so-exp=below">Amazon DynamoDB</a> - Managed NoSQL Database</li>
  <li><a href="https://aws.amazon.com/elasticache/?hp=tile&amp;so-exp=below">Amazon ElastiCache</a> - In-memory Caching System</li>
  <li><a href="https://aws.amazon.com/redshift/?hp=tile&amp;so-exp=below">Amazon Redshift</a> - Fast, Simple, Cost-effective Data Warehousing</li>
</ul>

<p><strong>Authentication</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/iam/?hp=tile&amp;so-exp=below">AWS Identity &amp; Access Management</a> - Manage User Access and Encryption Keys</li>
  <li><a href="https://aws.amazon.com/cognito/?hp=tile&amp;so-exp=below">Amazon Cognito</a> - Identity Management for your Apps</li>
  <li><a href="https://aws.amazon.com/single-sign-on/?hp=tile&amp;so-exp=below">AWS Single Sign-On</a> - Cloud Single Sign-On (SSO) Service</li>
  <li>a href=”https://aws.amazon.com/cloudhsm/?hp=tile&amp;so-exp=below”&gt;AWS CloudHSM&lt;/a&gt; - Hardware-based Key Storage for Regulatory Compliance</li>
</ul>

<p><strong>Management</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/api-gateway/?hp=tile&amp;so-exp=below">Amazon API Gateway</a> - Build, Deploy, and Manage APIs</li>
  <li><a href="https://aws.amazon.com/autoscaling/?hp=tile&amp;so-exp=below">AWS Auto Scaling</a> - Scale Multiple Resources to Meet Demand</li>
  <li><a href="https://aws.amazon.com/cloudformation/?hp=tile&amp;so-exp=below">AWS CloudFormation</a> - Create and Manage Resources with Templates</li>
</ul>

<p><strong>Logging</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/cloudwatch/?hp=tile&amp;so-exp=below">Amazon CloudWatch</a> - Monitor Resources and Applications</li>
  <li><a href="https://aws.amazon.com/cloudtrail/?hp=tile&amp;so-exp=below">AWS CloudTrail</a> - Track User Activity and API Usage</li>
</ul>

<p><strong>Network</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/vpc/?hp=tile&amp;so-exp=below">Amazon VPC</a> - Isolated Cloud Resources</li>
  <li><a href="https://aws.amazon.com/cloudfront/?hp=tile&amp;so-exp=below">Amazon CloudFront</a> - Global Content Delivery Network</li>
  <li><a href="https://aws.amazon.com/route53/?hp=tile&amp;so-exp=below">Amazon Route 53</a> - Scalable Domain Name System</li>
</ul>

<p><strong>Discovery</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/application-discovery/?hp=tile&amp;so-exp=below">AWS Application Discovery Service</a> - Discover On-Premises Applications to Streamline Migration</li>
  <li><a href="https://aws.amazon.com/servicecatalog/?hp=tile&amp;so-exp=below">AWS Service Catalog</a> - Create and Use Standardized Products</li>
</ul>

<p>Migration</p>
<ul>
  <li><a href="https://aws.amazon.com/dms/?hp=tile&amp;so-exp=below">AWS Database Migration Service</a> - Migrate Databases with Minimal Downtime</li>
  <li><a href="https://aws.amazon.com/server-migration-service/?hp=tile&amp;so-exp=below">AWS Server Migration Service</a> - Migrate On-Premises Servers to AWS</li>
</ul>

<p><strong>Orchestration</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/codedeploy/?hp=tile&amp;so-exp=below">AWS CodeDeploy</a> - Automate Code Deployment</li>
  <li><a href="https://aws.amazon.com/codepipeline/?hp=tile&amp;so-exp=below">AWS CodePipeline</a> - Release Software using Continuous Delivery</li>
  <li><a href="https://aws.amazon.com/config/?hp=tile&amp;so-exp=below">AWS Config</a> - Track Resource Inventory and Changes</li>
  <li><a href="https://aws.amazon.com/systems-manager/?hp=tile&amp;so-exp=below">AWS Systems Manager</a> - Gain Operational Insights and Take Action</li>
</ul>

<p><strong>Monitoring</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/trustedadvisor/?hp=tile&amp;so-exp=below">AWS Trusted Advisor</a> - Optimize Performance and Security</li>
  <li><a href="https://aws.amazon.com/premiumsupport/phd/?hp=tile&amp;so-exp=below">AWS Personal Health Dashboard</a> - Personalized View of AWS Service Health</li>
</ul>

<p><strong>Security</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/guardduty/?hp=tile&amp;so-exp=below">Amazon GuardDuty</a> - Managed Threat Detection Service</li>
  <li><a href="https://aws.amazon.com/certificate-manager/?hp=tile&amp;so-exp=below">AWS Certificate Manager</a> - Provision, Manage, and Deploy SSL/TLS Certificates</li>
  <li><a href="https://aws.amazon.com/shield/?hp=tile&amp;so-exp=below">AWS Shield</a> - DDoS Protection</li>
  <li><a href="https://aws.amazon.com/waf/?hp=tile&amp;so-exp=below">AWS WAF</a> - Filter Malicious Web Traffic</li>
</ul>

<p><strong>Analytics</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/athena/?hp=tile&amp;so-exp=below">Amazon Athena</a> - Query Data in S3 using SQL</li>
  <li><a href="https://aws.amazon.com/cloudsearch/?hp=tile&amp;so-exp=below">Amazon CloudSearch</a> - Managed Search Service</li>
  <li><a href="https://aws.amazon.com/redshift/?hp=tile&amp;so-exp=below">Amazon Redshift</a> - Fast, Simple, Cost-effective Data Warehousing</li>
</ul>

<p><strong>Integration</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/step-functions/?hp=tile&amp;so-exp=below">AWS Step Functions</a> - Coordinate Distributed Applications</li>
  <li><a href="https://aws.amazon.com/sqs/?hp=tile&amp;so-exp=below">Amazon Simple Queue Service (SQS)</a> - Managed Message Queues</li>
  <li><a href="https://aws.amazon.com/sns/?hp=tile&amp;so-exp=below">Amazon Simple Notification Service (SNS)</a> - Pub/Sub, Mobile Push and SMS</li>
  <li><a href="https://aws.amazon.com/amazon-mq/?hp=tile&amp;so-exp=below">Amazon MQ</a> - Managed Message Broker for ActiveMQ</li>
</ul>

<p>I’m a big fan of the AWS approach. Their marketplace, and AWS API gateway provide unprecedented access to backend cloud, and on-premise resources, which can be secured using AWS IAM. Amazon Web Services products a robust infrastructure as a services, adequate enough to deliver any platform as a services solutions.</p>

<p><strong>Azure</strong></p>

<p>Next, let’s look at the Azure stack to see what they bring to the table. There is definitely some overlap with the AWS list of resources, but Microsoft has a different view of the landscape than Amazon does. However, similar to Amazon, most of the building blocks are here to deliver on the proposal above.</p>

<p><strong>Compute</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/virtual-machines/">Virtual Machines</a> - Provision Windows and Linux virtual machines in seconds</li>
  <li><a href="https://azure.microsoft.com/en-us/services/app-service/">App Service</a> - Quickly create powerful cloud apps for web and mobile</li>
  <li><a href="https://azure.microsoft.com/en-us/services/functions/">Functions</a> - Process events with serverless code</li>
  <li><a href="https://azure.microsoft.com/en-us/services/batch/">Batch</a> - Cloud-scale job scheduling and compute management</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-instances/">Container Instances</a> - Easily run containers with a single command</li>
  <li><a href="https://azure.microsoft.com/en-us/services/service-fabric/">Service Fabric</a> - Develop microservices and orchestrate containers on Windows or Linux</li>
  <li>a href=”https://azure.microsoft.com/en-us/services/virtual-machine-scale-sets/”&gt;Virtual Machine Scale Sets&lt;/a&gt; - Manage and scale up to thousands of Linux and Windows virtual machines</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-service/">Azure Container Service (AKS)</a> - Simplify the deployment, management, and operations of Kubernetes</li>
  <li><a href="https://azure.microsoft.com/en-us/services/cloud-services/">Cloud Services</a> - Create highly-available, infinitely-scalable cloud applications and APIs</li>
  <li><a href="https://azure.microsoft.com/en-us/services/virtual-machines/linux-and-open/">Linux Virtual Machines</a> - Provision virtual machines for Ubuntu, Red Hat, and more</li>
  <li><a href="https://azure.microsoft.com/en-us/services/">Windows Virtual Machines</a> - Provision virtual machines for SQL Server, SharePoint, and more</li>
</ul>

<p><strong>Storage</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/">Storage</a> - Durable, highly available, and massively scalable cloud storage</li>
  <li><a href="https://azure.microsoft.com/en-us/services/backup/">Backup</a> - Simple and reliable server backup to the cloud</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storsimple/">StorSimple</a> - Lower costs with an enterprise hybrid cloud storage solution</li>
  <li><a href="https://azure.microsoft.com/en-us/services/site-recovery/">Site Recovery</a> - Orchestrate protection and recovery of private clouds</li>
  <li><a href="https://azure.microsoft.com/en-us/services/data-lake-store/">Data Lake Store</a> - Hyperscale repository for big data analytics workloads</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/blobs/">Blob Storage</a> - REST-based object storage for unstructured data</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/unmanaged-disks/">Disk Storage</a> - Persistent, secured disk options supporting virtual machines</li>
  <li><a href="https://azure.microsoft.com/en-us/services/managed-disks/">Managed Disks</a> - Persistent, secured disk storage for Azure virtual machines</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/queues/">Queue Storage</a> - Effectively scale apps according to traffic</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/files/">File Storage</a> - File shares that use the standard SMB 3.0 protocol</li>
</ul>
<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/27_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><strong>Deployment</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/app-service/api/">API Apps</a> - Easily build and consume Cloud APIs</li>
</ul>

<p><strong>Containers</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/app-service/">App Service</a> - Quickly create powerful cloud apps for web and mobile</li>
  <li><a href="https://azure.microsoft.com/en-us/services/batch/">Batch</a> - Cloud-scale job scheduling and compute management</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-registry/">Container Registry</a> - Store and manage container images across all types of Azure deployments</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-instances/">Container Instances</a> - Easily run containers with a single command</li>
  <li><a href="https://azure.microsoft.com/en-us/services/service-fabric/">Service Fabric</a> - Develop microservices and orchestrate containers on Windows or Linux</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-service/">Azure Container Service (AKS)</a> - Simplify the deployment, management, and operations of Kubernetes</li>
</ul>

<p><strong>Databases</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-database/">SQL Database</a> - Managed relational SQL Database as a service</li>
  <li><a href="https://azure.microsoft.com/en-us/services/cosmos-db/">Azure Cosmos DB</a> - Globally distributed, multi-model database for any scale</li>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-data-warehouse/">SQL Data Warehouse</a> - Elastic data warehouse as a service with enterprise-class features</li>
  <li><a href="https://azure.microsoft.com/en-us/services/cache/">Redis Cache</a> - Power applications with high-throughput, low-latency data access</li>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-server-stretch-database/">SQL Server Stretch Database</a> - Dynamically stretch on-premises SQL Server databases to Azure</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/tables/">Table Storage</a> - NoSQL key-value store using semi-structured datasets</li>
  <li><a href="https://azure.microsoft.com/en-us/services/postgresql/">Azure Database for PostgreSQL</a> - Managed PostgreSQL database service for app developers</li>
  <li><a href="https://azure.microsoft.com/en-us/services/mysql/">Azure Database for MySQL</a> - Managed MySQL database service for app developers</li>
  <li><a href="https://azure.microsoft.com/en-us/services/database-migration/">Azure Database Migration Service</a> - Simplify on-premises database migration to the cloud</li>
</ul>

<p><strong>Authentication</strong>
<a href="https://azure.microsoft.com/en-us/services/active-directory/">Azure Active Directory</a> - Synchronize on-premises directories and enable single sign-on
<a href="https://azure.microsoft.com/en-us/services/multi-factor-authentication/">Multi-Factor Authentication</a> - Add security for your data and apps without adding hassles for users
<a href="https://azure.microsoft.com/en-us/services/key-vault/">Key Vault</a> - Safeguard and maintain control of keys and other secrets
<a href="https://azure.microsoft.com/en-us/services/active-directory-b2c/">Azure Active Directory B2C</a> - Consumer identity and access management in the cloud</p>

<p><strong>Management</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/api-management/">API Management</a> - Publish APIs to developers, partners, and employees securely and at scale</li>
</ul>

<p><strong>Logging</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/log-analytics/">Log Analytics</a> - Collect, search, and visualize machine data from on-premises and cloud</li>
  <li><a href="https://azure.microsoft.com/en-us/services/traffic-manager/">Traffic Manager</a> - Route incoming traffic for high performance and availability</li>
</ul>

<p><strong>Monitoring</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/monitor/">Azure Monitor</a> - Highly granular and real-time monitoring data for any Azure resource</li>
  <li><a href="https://azure.microsoft.com/en-us/features/azure-portal/">Microsoft Azure portal</a> - Build, manage, and monitor all Azure products in a single, unified console</li>
</ul>

<p><strong>Analytics</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/hdinsight/">HDInsight</a> - Provision cloud Hadoop, Spark, R Server, HBase, and Storm clusters</li>
  <li><a href="https://azure.microsoft.com/en-us/services/hdinsight/apache-spark/">Apache Spark for Azure HDInsight</a> - Apache Spark in the cloud for mission critical deployments</li>
  <li><a href="https://azure.microsoft.com/en-us/services/hdinsight/apache-storm/">Apache Storm for HDInsight</a> - Real-time stream processing made easy for big data</li>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-data-warehouse/">SQL Data Warehouse</a> - Elastic data warehouse as a service with enterprise-class features</li>
  <li><a href="https://azure.microsoft.com/en-us/services/log-analytics/">Log Analytics</a> - Collect, search, and visualize machine data from on-premises and cloud</li>
  <li><a href="https://azure.microsoft.com/en-us/services/data-lake-store/">Data Lake Store</a> - Hyperscale repository for big data analytics workloads</li>
  <li><a href="https://azure.microsoft.com/en-us/services/data-lake-analytics/">Data Lake Analytics</a> - Distributed analytics service that makes big data easy</li>
  <li><a href="https://azure.microsoft.com/en-us/services/analysis-services/">Azure Analysis Services</a> - Enterprise grade analytics engine as a service</li>
  <li><a href="https://azure.microsoft.com/en-us/services/databricks/">Azure Databricks</a> - Fast, easy, and collaborative Apache Spark-based analytics platform</li>
</ul>

<p><strong>Network</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/cdn/">Content Delivery Network</a> - Ensure secure, reliable content delivery with broad global reach</li>
  <li><a href="https://azure.microsoft.com/en-us/services/dns/">Azure DNS</a> - Host your DNS domain in Azure</li>
  <li><a href="https://azure.microsoft.com/en-us/services/virtual-network/">Virtual Network</a> - Provision private networks, optionally connect to on-premises datacenters</li>
  <li><a href="https://azure.microsoft.com/en-us/services/traffic-manager/">Traffic Manager</a> - Route incoming traffic for high performance and availability</li>
  <li><a href="https://azure.microsoft.com/en-us/services/load-balancer/">Load Balancer</a> - Deliver high availability and network performance to your applications</li>
  <li><a href="https://azure.microsoft.com/en-us/services/network-watcher/">Network Watcher</a> - Network performance monitoring and diagnostics solution</li>
</ul>

<p><strong>Orchestration</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/scheduler/">Scheduler</a> - Run your jobs on simple or complex recurring schedules</li>
  <li><a href="https://azure.microsoft.com/en-us/services/automation/">Automation</a> - Simplify cloud management with process automation</li>
  <li><a href="https://azure.microsoft.com/en-us/services/automation-control/">Automation &amp; Control</a> - Centrally manage all automation and configuration assets</li>
</ul>

<p><strong>Integration</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/data-factory/">Data Factory</a> - Orchestrate and manage data transformation and movement</li>
  <li><a href="https://azure.microsoft.com/en-us/services/logic-apps/">Logic Apps</a> - Automate the access and use of data across clouds without writing code</li>
  <li><a href="https://azure.microsoft.com/en-us/services/event-grid/">Event Grid</a> - Get reliable event delivery at massive scale</li>
</ul>

<p><strong>Search</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/search/">Azure Search</a> - Fully-managed search-as-a-service</li>
</ul>

<p><strong>Discovery</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/active-directory-ds/">Azure Active Directory Domain Services</a> - Join Azure virtual machines to a domain without domain controllers</li>
</ul>

<p><strong>Security</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/security-center/">Security Center</a> - Unify security management and enable advanced threat protection across hybrid cloud workloads</li>
  <li><a href="https://azure.microsoft.com/en-us/services/security-compliance/">Security &amp; Compliance</a> - Enable threat detection and prevention through advanced cloud security</li>
  <li><a href="https://azure.microsoft.com/en-us/services/ddos-protection/">Azure DDoS Protection</a> - Protect your applications from Distributed Denial of Service (DDoS) attacks</li>
</ul>

<p><strong>Governance</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/azure-policy/">Azure Policy</a> - Implement corporate governance and standards at scale for Azure resources</li>
</ul>

<p><strong>Monetization</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/cost-management/">Cost Management</a> - Optimize what you spend on the cloud, while maximizing cloud potential</li>
</ul>

<p><strong>Experience</strong><br />
I have been studying Amazon full time for almost eight years. I’ve been watching Azure play catch up for the last three years. I run my infrastructure, and a handful of clients on AWS. I understand the API landscape of both providers, and how they can be woven into vision proposed so far.</p>

<p>I see the AWS API stack, and the Azure API stack, as a starter set of services that can be built upon to deliver the base Lighthouse implementation. All the components are there. It just need the first set of Lighthouse services to be defined, delivering the essential building blocks  any platform needs, things like compute, storage, dns, messaging, etc. I recommend that the VA Lighthouse team take the AWS API stack, and mold it into v1 of the Lighthouse API stack. Take the momentum from AWS’s own API journey, build upon it, and set into motion the VA Lighthouse API journey.</p>

<p>Enable VA services to be delivered as individual, self-contained units, that can be used as part of a larger VA orchestration of veteran services. Open up the VA and let some sunlight in. Think about what Amazon has been able to achieve by delivering its own internal operations as services, and remaking not just retail, but also touching almost every other industry with Amazon Web Services. <a href="https://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/">The Amazon Web Services myth story</a> provides a powerful and compelling narrative for any company, organizations, institution, or government agency like the VA to emulate.</p>

<p>This proposal is not meant to be a utopian vision for the VA. However it is meant to, as the name of the project reflects, shine a light on existing ways of delivering services via the cloud. Helping guide each service in its own individual journey, while also serving the overall mission of the platform–to help the veteran be successful in their own personal journey.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/pano-lighthouse_copper_circuit.jpg" width="100%" align="center" /></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/22/aws-iam-like-policies-for-aws-api-gateway-and-marketplace-billing/">AWS IAM-Like Policies For AWS API Gateway And Marketplace Billing</a></h3>
        <span class="post-date">22 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/server-cloud1_internet_numbers.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>The primary reason I’ve been adopting more AWS solutions as part of my API stack, and using tools I have historically felt lock me into the AWS ecosystem, is the available of AWS identity and access management (IAM). I just cannot deliver secure at this level as a small business owner, and their robust solution lets me dial in exactly what I need when it comes to defining who has access to what across my API infrastructure. I can define different policies, and apply them at the API management layer using both AWS Lambda and AWS API Gateway. Keeping everything separated, yet with a single API stack as the point of entry, for all consumers and applications.</p>

<p>I want all of this security goodness, but for the business of my APIs. Similar to the engine that drives the relationship between me as an AWS Marketplace user and AWS, I want a framework for applying business policies at the plan level within AWS API Gateway. I want to determine who has access to which resources, as well as what they can use, but I want to be able to meter this usage, and charge different rates. Compute, storage, and bandwidth for my partners is different than for retail API consumers, with a mix of resource and API call based metrics.</p>

<p><a href="https://plans.apievangelist.com/2017/10/23/api-monetization-framework-introduced-by-aws-marketplace/">The AWS monetization policies would reflect the AWS Marketplace framework</a>, giving me a mix of metering and contract based billing, reflecting single or multi-dimensional usage across the eight areas of consumption they support currently. I want to be able to establish common monetization policies across all my microservices, and allow product managers to implement them consistently at scale using AWS API Gateway. Like security, these API product managers shouldn’t be experts in the economics of the services being offered, they should just be able to apply from a common pool of business policies, and provide feedback on how to evolve, when appropriate.</p>

<p>This concept is very much in the realm of traditional API management service composition, but would possess a machine readable policy format just like IAM policies. API monetization policies could be reported upon, providing breakdown of consumption of resources at the backend system, or front-end API path level, helping translate the monetization side of our API strategy, into actual API plans that can be executed at run-time. Providing a standardized, scalable, quantifiable way to measure the value exchange that occurs at the API level. Done in a way that could be applied internally, or external with partners, and 3rd party developers. Making the business of my APIs more consistent, modular, and reusable–just like the rest of my API infrastructure.</p>

<p>I think AWS has a significant advantage in this area. They have the advanced resource management infrastructure, as well as the business side of all of this from managing their own APIs, but also from slowly rolling it out as part of the AWS Marketplace. AWS API Gateway has the plan, and marketplace key, providing the beginning of the implementation. All we need is the standardized policies based upon their existing pricing framework, and the ability to measure and report upon at the AWS API Gateway plan level. The working parts are there, it just needs to be brought together. It might also be something someone could piece together from logging, and other existing outputs on the AWS platform, and create an external reporting and billing solution. IDK. Just brainstorming, what I’d like to see, and getting it here on the blog before the thought passes.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/22/aws-iam-like-policies-for-aws-api-gateway-and-marketplace-billing/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/">Provisioning A Default App And Keys For Your API Consumers On Signup</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/sabre/sabre-travel-signup-keys.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I sign up for a lot of APIs. I love anything that reduces friction when on-boarding, and allows me to begin making an API call in 1-3 clicks. I’m a big fan of API providers that allow me to signup using my Github OAuth, preventing me from having to sign up for yet another account. I’m also a big fan of providers who automatically provision an application for me as part of the signup, and have my API keys waiting for me as soon as I’ve registered.</p>

<p><a href="https://developer.sabre.com/member/register">While signing up for the Sabre travel API I saw that they provisioned my application as part of the API sign up process in a way that was worth showcasing</a>. Saving me the time and hassle of having to add a new application after I’ve signed up. Stuff like this might seem like a pretty small detail when developing an API on-boarding process, but when you are signing up for many different APIs, and trying to manage your time–these little details add up to be a significant time saver.</p>

<p>Ideally, API providers would auto-provision a default application along with the signup, but I like the idea of also giving me the option to name my application while registering. When crafting your API registration flow, make sure you spend time signing up multiple times, and try to put yourself in your API consumers shoes. I even recommend signing up for an account each week, repeatedly experiencing what your consumers will be exposed to. I also recommend spending time signing up for other APIs on a regular basis, to experience what they offer–you will always surprised by what I find.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/open-banking-in-the-uk-openapi-template/">An Open Banking in the UK OpenAPI Template</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p>After learning more about <a href="https://www.openbanking.org.uk/">what Open Banking is doing for APIs in the UK</a>, I realized that I needed an OpenAPI template for the industry specification. There are six distinct schema available as part of the project, and I wanted a complete OpenAPI to describe which paths were available, as well as the underlying response schema. I got work crafting one from the responses that were available within <a href="https://www.openbanking.org.uk/open-data-apis/">the Open Banking documentation</a>.</p>

<p>Open Banking had schema available for their API definitions, but OpenAPI is the leading API and data specification out there today, so it makes sense that there should be an OpenAPI available, helping all participating banking API providers take advantage of all the tooling available within the OpenAPI community. To help support, I have published my Open Banking OpenAPI definition as a Github Gist:</p>

<script src="https://gist.github.com/kinlane/57c720c18e4d0ad370ad92c0ab9613f7.js"></script>

<p>I’ve applied this OpenAPI definition to the 17 banks they have listed, and will be including them in the next publishing of <a href="http://theapistack.com">my API Stack project</a>. Open Banking provides a common definition that can be used across many banks, and an OpenAPI template allows me to quickly apply the common template to each individual bank. Generating bank specific documentation, SDK and code samples, monitoring, tests, and other client tooling. Helping me put the valuable data being made available via each API to work.</p>

<p>I’d like to see more organizations like Open Banking emerge. I’d also like to help ensure they all make OpenAPI templates available for any API and schema specifications they establish. The API lifecycle is increasingly OpenAPI defined, and when you make your guidance available in the OpenAPI format, you are enabling actors within any industry to quickly get up and running with designing, deploying, managing, testing, monitoring, and almost every other stop along a modern API lifecycle. Increasing the chances of adoption for any API standards you are putting out there.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/open-banking-in-the-uk-openapi-template/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/">An Opportunity Around Providing A Common OpenAPI Enum Catalog</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/enums/bitcoin-pools.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m down in the details of the OpenAPI specification lately, working my way through hundreds of OpenAPI definitions, trying to once again make sense of the API landscape at scale. I’m working to prepare as many API path definitions as I possibly can to be runnable within one or two clicks. OpenAPI definitions, and Postman Collections are essential to making this happen, both of which require complete details on the request surface area for an API. I need to know everything about the path, as well as any headers, path, or query parameters that need to included. A significant aspect of this definition being complete includes default, and enum values being present.</p>

<p>If I can’t quickly choose from a list of values, or run with a default value, when executing an API, the time to seeing a live response grows significantly. If I have to travel back to the HTML documentation, or worse, do some Googling before I can make an API call, I just went from seconds to potentially minutes or hours before I can see a real world API response. Additionally, if there are many potential values available for each API parameter, enums become critical building blocks to helping me understand all the dimensions of an API’s surface area. Something that should have been considered as part of the API’s design, but often just gets left as part of API documentation.</p>

<p>When playing with a Bitcoin API with the following path /blocks/{pool_name}, I need to the list of pools I can choose from. When looking to get a stock market quote from an API with the following path, /stock/{symbol}/quote, I need a list of all the ticker symbols. Having, or not having these enum values at documentation, and execute time, are essential. Many of these lists of values are so common, developers take them for granted. Assuming that API consumers just have them laying around, and really aren’t worth including in documentation. You’d think we all have lists of states, countries, stock tickers, Bitcoin pools, and other data just laying around, but even as the API Evangelist, I often find myself coming up short.</p>

<p>All of this demonstrates a pretty significant opportunity for someone to create a Github hosted, searchable, forkable list of common OpenAPI enum lists. Providing an easy place for API providers, and API consumers to discover simple, or complex lists of values that should be present in API documentation, and included as part of all OpenAPIs. I recommend just publishing each enum JSON or YAML list as a Github Gist, and then publishing as a catalog via a simple Github Pages website. If I don’t see something pop up in the next couple of months, I’ll probably begin publishing something myself. However, I need another API related project like I need a hole in the head, so I’m holding off in hopes another hero or champion steps up and owns the enum portion of the growing OpenAPI conversation.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/">What Is Open Banking In The UK?</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/open-banking/open-banking-screenshot.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am profiling banks in the UK as part of an effort move forward my <a href="http://theapistack.com">API Stack</a> work, and populate the <a href="https://streamdata.io/developers/api-gallery/">Streamdata.io API Gallery</a>. One significant advantage that banks in the UK have over other countries in the EU, and even in the US, is the help of <a href="https://www.openbanking.org.uk">Open Banking</a>. To help profile the organization, I’ll just borrow from their website to define who they are and what they do.</p>

<p><em>The Open Banking Implementation Entity was created by the UK’s Competition and Markets Authority to create software standards and industry guidelines that drive competition and innovation in UK retail banking.</em></p>

<p>In 2016, The Competition and Markets Authority (CMA) published a report on the UK’s retail banking market which stated that older, larger banks do not have to compete hard enough for customers’ business, and smaller and newer banks were finding it difficult to grow and access the UK banking market. To solve this problem, they proposed a number of remedies including Open Banking, which defines API standards that are intended to help level that playing field.</p>

<p>The role of Open Banking is to:</p>

<ul>
  <li>Design the specifications for the Application Programming Interfaces (APIs) that banks and building societies use to securely provide Open Banking</li>
  <li>Support regulated third party providers and banks and building societies to use the Open Banking standards</li>
  <li>Create security and messaging standards</li>
  <li>Manage the Open Banking Directory which allows regulated participants like banks, building societies and third party providers to enroll in Open Banking</li>
  <li>Produce guidelines for participants in the Open Banking ecosystem</li>
  <li>Set out the process for managing disputes and complaints</li>
</ul>

<p>This approach to standardizing API definitions is the type of leadership that is needed to move API conversation forward in ALL industries. I know in the US, many enjoy viewing regulations as always bad, but this type of organizational designation can go a long way towards moving an industry forward in a concerted fashion. Doing the hard work to establish a common API definition, and play a central role in helping ensure each actor within an industry is implementing the definition as expected.</p>

<p>I’d like to see more organizations emerge that reflect Open Banking’s mission, in a variety of industries. Many companies do not have the time, expertise, or desire to do the homework and understand what needs to occur on the API front. Speaking from experience, there is’t a lot of vendor-free funding to do this kind of work, and it is something that will require public sector investment. In my opinion, this doesn’t always have to be government led, but there should be industry neutral funding available to move forward the conversation in a way that benefits everyone involved, without a focus on any single product or service.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/20/relationship-between-openapi-path-summary-tags-and-asyncapi-topics/">Relationship Between OpenAPI Path, Summary, Tags and AysncAPI Topics</a></h3>
        <span class="post-date">20 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/23_160_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m working my way through several hundred OpenAPI definitions that I have forked from <a href="https://apis.guru/">APIs.guru</a>, <a href="https://any-api.com/">Any API</a>, and have automagically generated from API documentation scrape scripts I have developed over time. Anytime I evolve a new OpenAPI definition, I first make sure the summary, description, and tags are as meaningful as they possibly can. Sadly this work is also constrained by how much time I have to spend with each API, as well as how well designed their API is in the first place. I have a number of APIs that help me enrich this automatically, by mining the API path, applying regular expressions, but often times it takes a manual review to add tags, polish summaries, and make the OpenAPI details as meaningful as I possibly can, in regards to what an API does.</p>

<p>As I’m taking a break from this work, I’m studying up on <a href="https://www.asyncapi.com/">AsyncAPI</a>, trying to get my head around how I can be crafting API definitions for the message-based, event-driven, streaming APIs I’m profiling alongside my regular API research. One of the areas the AsyncAPI team is pushing forward is around the concept of a topic–_“to create a definition that suites most use cases and establish the foundation for community tooling and better interoperability between products when using AsyncAPI.”_ or to elaborate further, <em>“a topic is a string representing where an AsyncAPI can publish or subscribe. For the sake of comparison they are like URLs in a REST API.”</em> Now I’m thinking about the relationships between the API design elements I’m wrestling with in my API definitions, and how the path, summary, and tags reflect what Async is trying to articulate with their topics discussion.</p>

<p><strong>{organization}.{group}.{version}.{type}.{resources}.{event}</strong></p>

<ul>
  <li><strong>organization</strong> - the name of the organization or company.</li>
  <li><strong>group</strong> - the service, team or department in charge of managing the message..</li>
  <li><strong>version</strong> - the version of the message for the given service. This version number should remain the same unless changes in the messages are NOT backward compatible.</li>
  <li><strong>type</strong> - the type of the message, e.g., is it a command or an event?. This value should always be event unless you’re trying to explicitly execute a command in another service, i.e., when using RPC.</li>
  <li><strong>resources</strong> - resources and sub-resources, in a word (or words) describing the resource the message refers to. For instance, if you’re sending a message to notify a user has just signed up, the resource should be user. But, if you want to send a message to notify a user has just changed her full name, you could name it as user.full_name.</li>
  <li><strong>event</strong> - an event or command name, in case message type is event, this should be a verb in past tense describing what happened to the resource, and in case message type is command, this should be a verb in infinitive form describing what operation you want to perform.</li>
</ul>

<p><strong>Example(s):</strong></p>

<ul>
  <li>hitch.accounts.1.event.user.signedup</li>
  <li>hitch.email.1.command.user.welcome.send</li>
</ul>

<p>As I’m crafting OpenAPI definitions, and publishing them to Github, I’m using Jekyll to give me access to the large numbers of OpenAPI definitions I’ve published, and indexed using APIs.json, as Liquid objects. For each site. I can references APIs path using a dotted notation, such as site.twilio.send-sms-get. I haven’t polished my naming conventions, and simply taking the path, stripping out everything but the alpha, numeric characters for the file names, but it got me thinking about how I might want to get more structured in how I name the individual units of compute I’m publishing using OpenAPI, and often times as Postman Collections.</p>

<p>As I publish these API definitions to Github, as part of my API profiling for inclusion in the Streamdata.io API, I’m looking to establish a map of the surface area, that I can potentially turn into webhooks, streamings, and other approaches to real time message delivery. This is why I’m looking to understand AsyncAPI, to help quantify the result of this work. After I map out the surface area of the APIs, and quantify the topics at play, and obtain an API key, I need a way to then map out the real time streams of messages that will get passed around. To do this, I will need a way to turn each potential API response and its resulting request into a topic definition into a well defined, measurable input or output–AsyncAPI is going to help me do this.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/20/relationship-between-openapi-path-summary-tags-and-asyncapi-topics/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/">People Who Provide Enum For Their OpenAPI Definitions Are Good People</a></h3>
        <span class="post-date">20 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/64_116_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m processing a significant amount of OpenAPI definitions currently, as well as crafting a number of them from scraped API documentation. After you work with a lot of OpenAPI definitions, aiming to achieve a specific objective, you really get to know which aspects of the OpenAPI are the most meaningful, and helpful when they are complete. <a href="http://apievangelist.com/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi-definition/">I talked about the importance of summary, description, and tags last week</a>, and this week I’d like to highlight how helpful it is when the stewards of OpenAPI definitions include enum values for their parameters, and I think they are just good people. ;-)</p>

<p>Enums are simply just a list of potential values for each of the parameters you outline as part of your API definition. So if you have state as a parameter for use in the request of your API, you have a list of the 50 US states as the enum. If you the parameter is color, you have just the color black, because we all know it is the only color(all the colors). ;-) If you provide a parameter that will accept a standard set of inputs, you should consider providing an enum list to help your consumers understand the potential for that parameter. Outlining the dimensions of the parameter in a simple JSON or YAML array of every single possible value.</p>

<p>I can’t articulate how many times I have to go looking for a list of values. Sometimes it is present within the description for the OpenAPI, but often times I have to go back to the portal for the API, and follow a link to a page that lists out the values. That is, if an API provider decides to provide this information at all. The thoughtful ones do, the even more thoughtful ones put it in their OpenAPI definitions as enum values. Anytime I come across a list of enums that I can quickly build an array, select, and other common aspects of doing business with APIs, I’m a happy camper.</p>

<p>Which is why you find me writing up enums. Boring. Boring. Boring. However, it is something that makes me happy, potentially multiple times in a single day, and imagine that multiplied by the number of developers you have, or maybe “had”, depending on how frustrating it is to find the values that can be used in your API’s parameters. In my opinion, enums add rich dimensions to what your API does, and can be as important as the overall design of your API. Depending on how you’ve designed your API, you may have invested heavily in design, or may be leaning on your API parameters to do the heavy lifting of helping you–making them even more important when it comes to documenting them as part of your API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using-apis/">Insecurity Around Providing Algorithmic Transparency And Observability Using APIs</a></h3>
        <span class="post-date">16 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/68_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://streamdata.io/blog/benchmark-quantifying-api-performance/">I’m working on a ranking API for my partner Streamdata.io to help quantify the efficiencies they bring to the table when you proxy an existing JSON web API using their service</a>. I’m evolving an algorithm they have been using for a while, wrapping it in a new API, and applying it across the APIs I’m profiling as part of my <a href="http://theapistack.com">API Stack</a>, and the <a href="https://streamdata.io/developers/api-gallery/">Streamdata.io API Gallery</a> work. I can pass the ranking API any OpenAPI definition, and it will poll and stream the API for 24 hours, and return a set of scores regarding how real time the API is, and what the efficiency gains are when you use Streamdata.io as a proxy for the API.</p>

<p>As I do this work, I find myself thinking more deeply about the role that APIs can play in helping make algorithms more transparent, observable, and accountable. My API ranking algorithm is pretty crude, but honestly it isn’t much different than many other algorithms I’ve seen companies defend as intellectual property and their secret sauce. Streamdata.io is invested in the ranking algorithm and API being as transparent as possible, so that isn’t a problem here, but each step of the process allows me to think through how I can continue to evangelize other algorithm owners to use APIs, to make their algorithms more observable and accountable.</p>

<p>In my experience, most of the concerns around keeping algorithms secret stem from individual insecurities, and nothing actually technical, mathematical, or proprietary. The reasons for the insecurities are usually that the algorithm isn’t that mathematically sophisticated (I know mine isn’t), or maybe it is pretty flawed (I know mine is currently), and people just aren’t equipped to admit this (I know I am). I’ve worked for companies who venomously defend their algorithms and refuse to open them up, because in the end they know they aren’t defensible on many levels. The only value the algorithm possesses in these scenarios is secrecy, and the perception that there is magic going on behind the scenes. When in reality, it is a flawed, crude, simple algorithm that could actually be improved upon if it was opened up.</p>

<p>I’m not insecure about my lack of mathematical skills, or the limitations of my algorithm. I want people to point out its flaws, and improve upon my math. I want the limitations of the algorithm to be point out. I want API providers and consumers to use the algorithm via the API (when I publish) to validate, or challenge the algorithmic assumptions being put forth. I’m not in the business of selling smoke and mirrors, or voodoo algorithmics. I’m in the business of helping people understand how inefficient their API responses are, and how they can possibly improve upon them. I’m looking to develop my own understanding of how can make APIs more event-driven, real time, and responsive. I’m not insecure about providing transparency and observability around the algorithms I develop, using APIs–all algorithm developers should be as open and confident in their own work.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/16/using-jekyll-and-openapi-to-evolve-api-documentation-and-storytelling/">Using Jekyll And OpenAPI To Evolve My API Documentation And Storytelling</a></h3>
        <span class="post-date">16 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/68_158_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m reworking my API Stack work as independent sets of <a href="https://jekyllrb.com/docs/collections/">Jekyll collections</a>. Historically I just dumped all <a href="http://apisjson.org/">APIs.json</a>, and OpenAPIs into the central data folder, and grouped them into folders by company name. Now I am breaking them out into tag based collections, using a similar structure. Further evolving how I document and tell stories using each API. I have been published a single OpenAPI for each platform, but now I’m publishing a separate OpenAPI for each API path–we will see where this goes, it might ultimately end up biting me in the ass. I’m doing this because I want to be able to talk about a single API path, and provide a definition that can be viewed, interpreted, and executed against, independent of the other paths–Jekyll+OpenAPI is helping me accomplish this.</p>

<p>With each API provider possessing its own APIs.json index, and each API path having its own OpenAPI definition, I’m able to mix up how I document and tell stories around these APIs. I can list them by API provider, or by individual API path. I can filter based upon tags, and provide execute-time links that reference each individual unit of API. I have separate JavaScript functions that can be referenced if the API path is GET, POST, or PUT. I can even inherit other relevant links like API sign up or terms of service as part of its documentation. I can reference all of this as part of larger documentation, or within blog posts, and other pages throughout the website–which will be refreshed whenever I update the OpenAPI definition.</p>

<p>If you aren’t familiar with how Jekyll works. It is a static content solution, that allows you do develop collections. You can put CSV, JSON, or YAML into these collections (folders), and they become objects you can reference using Liquid syntax. So if I put Twitter’s APIs.json, and OpenAPI into a folder within my social collection, I can reference as site.social.twitter which is the APIs.json for Twitter’s entire API operations, and I can reference individual APIs as site.social.twitter.search for the individual OpenAPI defining the Twitter search API path. This decouples API documentation for me, and allows me to not just document APIs, but tell stories with  API definitions, making my API portals much more interactive, and hopefully engaging.</p>

<p>I just got my API stack approach refreshed using this new format. Now I just need to go through all my APIs and rebuild the underlying Github repository. I have thousands of APIs that I track on, and I’m curious how this approach holds up at scale. While <a href="http://theapistack.com">API Stack</a> is a single repository, I can essentially publish any collection of APIs I desire to any of the hundreds of repositories that make up the API Evangelist network. Allowing me to seamless tell stories using the technical details of API operations, and the individual API resources they serve up. Further evolving how I tell stories around the APIs I’m tracking on. While my API documentation has always been interactive, I think this newer, more modular approach, reflects the value each unit of value an API brings to the table, rather than just looking to document all the APIs a provider possesses.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/16/using-jekyll-and-openapi-to-evolve-api-documentation-and-storytelling/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi-definition/">The Importance of the API Path Summary, Description, and Tags in an OpenAPI Definition</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/15_190_800_500_0_max_0_1_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am creating a lot of OpenAPI definitions right now. <a href="http://apis.how/streamdata.io">Streamdata.io</a> is investing in me pushing forward my <a href="http://theapistack.com">API Stack</a> work, where I profile API using OpenAPI, and index their operations using APIs.json. From the resulting indexes, we are building out the <a href="https://streamdata.io/developers/api-gallery/">Streamdata.io API Gallery</a>, which shows the possibilities of providing streaming APIs on top of existing web APIs available across the landscape. The OpenAPI definitions I’m creating aren’t 100% complete, but they are “good enough” for what we are needing to do with them, and are allowing me to catalog a variety of interesting APIs, and automate the proxying of them using Streamdata.io.</p>

<p>I’m finding the most important part of doing this work is making sure there is a rich summary, description, and set of tags for each API. While the actual path, parameters, and security definitions are crucial to programmatically executing the API, the summary, description, and tags are essential so that I can understand what the API does, and make it discoverable. As I list out different areas of my API Stack research, like <a href="http://market.data.apievangelist.com/">the financial market data APIs</a>, it is critical that I have a title, and description for each provider, but the summary, description, and tags are what provides the heart of the index for what is possible with each API.</p>

<p>When designing an API, as a developer, I tend to just fly through writing summary, descriptions, and tags for my APIs. I’m focused on the technical details, not this “fluff”. However, this represents one of the biggest disconnects in the API lifecycle, where the developer is so absorbed with the technical details, we forget, neglect, or just don’t are to articulate what we are doing to other humans. The summary, description, and tags are the outlines in the API contract we are providing. These details are much more than just the fluff for the API documentation. They actually describe the value being delivered, and allow this value to be communicated, and discovered throughout the life of an API–they are extremely important.</p>

<p>As I’m doing this work, I realize just how important these descriptions and tags are to the future of these APIs. Whenever it makes sense I’m translating these APIs into streaming APIs, and I’m taking the tags I’ve created and using them to define the events, topics, and messages that are being transacted via the API I’m profiling. I’m quantifying how real time these APIs are, and mapping out the meaningful events that are occurring. This represents the event-driven shift we are seeing emerge across the API landscape in 2018. However, I’m doing this on top of API providers who may not be aware of this shift in how the business of APIs is getting done, and are just working hard on their current request / response API strategy. These summaries, descriptions, and tags, represent how we are going to begin mapping out the future that is happening around them, and begin to craft a road map that they can use to understand how they can keep evolving, and remain competitive.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi-definition/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/">A Really Nice API Application Showcase Over At The Intrinio Market Data API</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p>I am profiling financial market data APIs currently, and as I’m doing my work profiling APIs, I’m always on the hunt for interesting elements of their API operations that I can showcase for my readers. While looking at the financial market data API from Intrinio, I found that <a href="https://intrinio.com/marketplace/apps">I really, really like their application showcase</a>, which providers a pretty attractive blueprint for how we can showcase what is being develop on top of our APIs.</p>

<p>The Intrinio application showcase is just clean looking, and has the bells and whistles you’d expect like categories, search, detail or list view, and detail pages providing you all the information you need about the application, and where you can find tutorials, code, and other relevant resources.</p>

<p align="center"><a href="https://intrinio.com/marketplace/apps"><img src="https://s3.amazonaws.com/kinlane-productions/intrinio/intrinio-app-showcase.png" /></a></p>

<p>Another thing I really like is it isn’t just about web and mobile applications. They have spreadsheet integrations, and help walk you through how to “apply” each type of integration. This is what the application in API means to me. It isn’t always just about finished web, mobile, and device applications. It is about applying the resources available via the programmatic interfaces to some problem you have in your world.</p>

<p>Anyways, the Intrinio application showcase is totally worth profiling as part of my research. It is a great blueprint for other API providers to follow when crafting their own application showcases. This post give me a single URL that I can share with folks, and reference throughout my stories, white papers, guides, and talks. I’d love to see this become the standard for how API providers showcase their applications, keeping things simple, clean, and bringing value to their consumers.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/how-big-or-small-is-an-api/">How Big Or Small Is An API?</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/31_156_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working to build out <a href="https://streamdata.io/developers/api-gallery/">the API Gallery for Streamdata.io</a>, profiling a wide variety of APIs for inclusion in the directory, adding to the wealth of APIs that could be streamed using the service. As I work to build the index, I’m faced with the timeless question regarding, what is an API? Not technically what an API does, but what is an API in the context of helping people discover the API they are looking for. Is Twitter an API, or is the Twitter search/tweets path an API? My answer to this question always distills down to a specific API path, or as some call it an API endpoint. Targeting a specific implementation, use case, or value generated by a single API provider.</p>

<p>Like most things in the API sector, words are used interchangeably, and depending on how much experience you have in the business, you will have much finer grained definitions about what something is, or isn’t. When I’m talking to the average business user, the Twitter API is the largest possible scope–the entire thing. In the context of API discovery, and helping someone find an API to stream or to solve a specific problem in their world, I’m going to resort to a very precise definition–in this case, it is the specific Twitter API path that will be needed. Depending on my audience, I will zoom out, or zoom in on what constitutes a unit of API. The only consistency I’m looking to deliver is regarding helping people understand, and find what they looking for–I’m not worried about always using the same scope in my definition of what an API is.</p>

<p><a href="https://streamdata.io/blog/robust-market-data-apis-alphavantage/">You can see an example of this in action with the Alpha Vantage market data API I’m currently profiling</a>, and adding to the gallery. Is Alpha Vantage is a single API, or 24 separate APIs? In the context of the Streamdata.io API Gallery, it will be 24 separate APIs. In the context of telling the story on the blog, there is a single Alpha Vantage API, with many paths available. I don’t want someone searching specifically for a currency API to have to wade through all 24 Alpha Vantage paths, I want them to find specifically the path for their currency API. When it comes to API storytelling, I am fine with widening the scope of my definition, but when it comes to API discovery I prefer to narrow the scope down to a more granular unit of value.</p>

<p>For me, it all comes down the definition of what an API is. It is all about applying a programmatic interface. If I’m applying in a story that targets a business user, I can speak in general terms. If I’m applying to solve a specific business problem, I’m going to need to get more precise. This precision can spin out of control if you are dealing with developers who tend to get dogmatic about programming languages, frameworks, platforms, and the other things that make their worlds go round. I’m not in the business of being “right”. I’m in the business of helping people understand, and solve the problems they have. Which gives me a wider license when it comes to defining how big or small an API can be. It is a good place to be.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/how-big-or-small-is-an-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/some-common-features-of-an-api-application-review-process/">Some Common Features Of An API Application Review Process</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p><a href="https://twitter.com/ktinboulder/status/961601920887607296"><img src="https://s3.amazonaws.com/kinlane-productions/kelly-taylor-app-approval-tweet.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="https://twitter.com/ktinboulder/status/961601920887607296">I received a tweet from my friend Kelly Taylor with USDS</a>, asking for any information regarding establishing an “approve access to production data” for developers. <a href="https://bluebutton.cms.gov/developers/">He is working on an OAuth + FHIR implementation for the Centers for Medicare and Medicaid Services (CMS) Blue Button API</a>. Establishing a standard approach for on-boarding developers into a production environment always makes sense, as you don’t want to give access to sensitive information without making sure the company, developer, and application has been thoroughly vetted.</p>

<p>As I do with my work, I wanted to think through some of the approaches I’ve come across in my research, and share some tips and best practices. <a href="https://bluebutton.cms.gov/developers/#production-api-access">The Blue Button API team has a section published regarding how to get your application approved</a>, but I wanted to see if I can expand on, while also helping share this information with other readers. This is a relevant use case that I see come up regularly in healthcare, financial, education, and other mainstream industries.</p>

<p><strong>Virtualization &amp; Sandbox</strong><br />
The application approval conversation usually begins with ALL new developers being required to work with a sandboxed set of APIs, only providing production API access to approved developers. This requires having a complete set of virtualized APIs, mimicking exactly what would be used in production, but in a much safer, protected environment. One of the most important aspects of this virtualized environment is that there also needs to be robust sets of virtualized data, providing as much parity regarding what developers will experience when they enter the production environment. The sandbox environment needs to be as robust and reliable as the production, which is a mistake I see made over and over from providers, where the sandbox isn’t reliable, or as functional, and developers never are able to reach production status in a consistent and reliable way.</p>

<p><strong>Doing a Background Check</strong><br />
Next, as reflected in the Blue Button teams approach, you should be profiling the company and organization, as well as the individual behind each application. <a href="http://apievangelist.com/2016/03/30/best-buy-will-not-issue-api-keys-to-free-email-accounts-and-wants-to-get-to-know-your-company/">You see company’s like Best Buy refusing any API signup that doesn’t have an official company domain that can be verified</a>. In addition to requiring developers provide a thorough amount of information about who they are, and who they work for, many API providers are using background and profiling services like <a href="https://clearbit.com/">Clearbit</a> to obtain more details about a user based upon their email, IP address, and company domain. Enabling different types of access to API resources depending on the level of scrutiny a developer is put under. I’ve seen this level of scrutiny go all the way up to requiring the scanning of drivers license, and providing corporate documents before production access is approved.</p>

<p><strong>Purpose of Application</strong><br />
One of the most common filtering approaches I’ve seen centers around asking developer about the purpose of their application. The more detail the better. As we’ve seen from companies like Twitter, the API provider holds a lot of power when it comes to deciding what types of applications will get built, and it is up to the developer to pitch the platform, and convince them that their application will serve the mission of the organization, as well as any stakeholders, and end-users who will be leveraging the application. This process can really be a great filter for making sure developers think through what they are building, requiring them to put forth a coherent proposal, otherwise they will not be able to get full access to resources. This part of the process should be conducted early on in the application submission process, reducing frustrations for developers if their application is denied.</p>

<p><strong>Syncing The Legal Department</strong><br />
Also reflected in the Blue Button team’s approach is the syncing of the legal aspects of operating an API platform, and it’s applications. Making sure the application’s terms of service, privacy, security, cookie, branding, and other policies are in alignment with the platform. One good way of doing this is offering a white label edition of the platforms legal documents for use by the each application. Doing the heavy legal work for the application developers, while also making sure they are in sync when it comes to the legal details. Providing legal develop kits (LDK) will grow in prominence in the future, just like providing software development kits (SDK), helping streamline the legalities of operating a safe and secure API platform, with a wealth of applications in its service.</p>

<p><strong>Live or Virtual Presentation</strong><br />
Beyond the initial pitch selling an API provider on the concept of an application, I’ve seen many providers require an in-person, or virtual demo of the working application before it can be added to a production environment, and included in the application gallery. It can be tough for platform providers to test drive each application, so making the application owners do the hard work of demonstrating what an application does, and walking through all of its features is pretty common. I’ve participated on several judging panels that operate quarterly application reviews, as well as part of specific events, hackathons, and application challenges. Making demos a regular part of the application lifecycle is easier to do when you have dedicated resources in place, with a process to govern how it will all work in recurring batches, or on a set schedule.</p>

<p><strong>Getting Into The Code</strong><br />
As part of the application review process many API providers require that you actually submit your code for review via Github. Providing details on ALL dependencies, and performing code, dependency, and security scans before an application can be approved. I’ve also see this go as far as requiring the use of specific SDKs, frameworks, or include proxies within the client layer, and requiring all HTTP calls be logged as part of production applications. This process can be extended to include all cloud and SaaS solutions involved, limiting where compute, storage, and other resources can be operated. Requiring all 3rd party APIs in use be approved, or already on a white list of API providers before they can be put to use. This is obviously the most costly part of the application review process, but depending on how high the bar is being set, it is one that many providers will decide to invest in, ensuring the quality of all applications that run in a production environment.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/apple-app-review.png" align="right" width="40%" style="padding: 15px;" /></p>

<p><strong>Regular Review &amp; Reporting</strong><br />
One important thing about the application review process is that it isn’t a one time process. Even once an application is accepted an added into the production environment, this process will need to be repeated for each version release of the application, along with the changes to the API. Of course the renewal process might be shorter than the initial approval workflow, but auditing and regular check-in should be common, and not forgotten. This touches on the client level SDK, and API management logging needs of the platform, and that regular reporting upon application usage and activity should be available in real time, as well as part of each application renewal. API operations is always about taking advantage the real time awareness introduced at the API consumption layer, and staying in tune with the healthy, and not so healthy patterns that emerge from logging everything an application is doing.</p>

<p><strong>Business Model</strong><br />
It is common to ask application developers about their business model. The absence of a business model almost always reflects the underlying exploitation and sale of data being access or generated as part of application’s operation. Asking developers how they will make money and sustain their operations, along with regular checkins to make sure it is truly in effect, is an easy to ensure that applications are protecting the interests of the platform, its partners, and the applications end-users.</p>

<p>There are many other approaches I’ve seen API providers require before accepting an application into production. However, I think we should also be working hard to keep the process simple, and meaningful. Of course, we want a high bar for quality, but as with everything in the API world, there will always be compromises in how we deliver on the ground. Depending on the industry you are operating the bar will be made higher, or possibly lowered a little to allow for more innovation. I’ve included a list of some of the application review process I found across my research–showing a wide range of approaches across API providers we are all familiar with. Hopefully that helps you think through the application review process a little more. It is something I’ll write about again in the future as I push forward my research, and distill down more of the common building blocks I’m seeing across the API landscape.</p>

<p><strong>Some Leading Application Review Processes</strong><br /></p>

<ul>
  <li><a href="https://www.instagram.com/developer/review/">Instagram</a></li>
  <li><a href="https://developer.concur.com/manage-apps/app-certification.html">SAP Concur</a></li>
  <li><a href="https://developer.paypal.com/docs/classic/lifecycle/goingLive/">Paypal</a></li>
  <li><a href="https://developers.google.com/adsense/host/review_main">Adsense</a></li>
  <li><a href="https://help.shopify.com/api/listing-in-the-app-store/app-requirements-and-success-criteria/app-review-checklist">Shopify</a></li>
  <li><a href="https://developer.riotgames.com/application-process.html">Riot Games</a></li>
  <li><a href="https://api.slack.com/security-review">Slack</a></li>
  <li><a href="https://www.docusign.com/blog/dsdev-docusign-go-live-process-now-automated/">Docusign</a></li>
  <li><a href="http://dev.splunk.com/view/app-cert/SP-CAAAE8P">Splunk</a></li>
  <li><a href="https://go.developer.ebay.com/compatible-application-check-and-checklist-going-live">Ebay</a></li>
  <li><a href="https://developer.apple.com/app-store/review/">Apple</a></li>
</ul>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/some-common-features-of-an-api-application-review-process/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/code-generating-openapi-still-prevailing-approach/">Code Generation Of OpenAPI (fka Swagger) Still The Prevailing Approach</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/gears-numbers-blue.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>Over 50% of the projects I consult on still generate OpenAPI (fka Swagger) from code, rather then the other way around. When I first begin working with any API development group as an advisor, strategist, or governance architect I always ask, “are you using OpenAPI?” Luckily the answer is almost always yes. The challenge is that most of the time they don’t understand the full scope of how to use OpenAPI, and are still opting for the more costly approach–writing code, then generating OpenAPI from annotations. It has been over five years since Jakub Nesetril(@jakubnesetril) of Apiary first decoupled this way of doing API design first, but clearly we still have a significant amount of work when it comes to API definition and design literacy amongst development groups.</p>

<p>When you study where API services and tooling are headed it is clear that API deployment, and the actual writing of code is getting pushed further down in the life cycle. Services like Stoplight.io, and Postman are focusing on enabling a design, mock, document, test, and iterate approach, with API definitions (OpenAPI, Postman, etc) at the core. The actual deployment of API, either using open source frameworks, API gateways, or other method, is coming into the picture more downstream. Progressive API teams are hammering out exactly the API they need without ever writing any code, making sure the API design is dialed in before the more expensive, and often permanent code gets written and sent to production.</p>

<p>You will see me hammering on this line of API design first messaging on API Evangelist over the next year. Many developers still see OpenAPI (fka Swagger) about generating API documentation, not as the central contract that is used across every stop along the API lifecycle. Most do not understand that you can mock instead of deploying, and even provide mock data, errors, and other scenarios, allowing you to prototype applications on top of API designs. It will take a lot of education, and awareness building to get API developers up to speed that this is all possible, and begin the long process of changing behavior on the ground. Teams just are used to this way of thinking, but once they understand what is possible, they’ll realize what they have been missing.</p>

<p>I need to come up with some good analogies for generating API definitions from code. It really is an inefficient, and a very costly way to get the job done. Another problem is that this approach tends to be programming language focused, which always leaves its mark on the API design. I’m going to be working with both Stoplight.io and Postman to help amplify this aspect of delivering APIs, and how their services and tooling helps streamline how we develop our APIs. I’m going to be working with banks, insurance, health care, and other companies to improve how they deliver APIs, shifting things towards a design-first way of doing business. You’ll hear the continued drumbeat around all of this on API Evangelist in coming months, as I try to get the attention of folks down in the trenches, and slowly shift the behavior towards a better way of getting things done.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/code-generating-openapi-still-prevailing-approach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/the-growing-importance-of-github-topics-for-your-api-seo/">The Growing Importance of Github Topics For Your API SEO</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-topics-screenshot.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>When you are operating an API, you are always looking for new ways to be discovered. I study this aspect of operating APIs from the flip-side–how do I find new APIs, and stay in tune with what APIs are to? Historically we find APIs using ProgrammableWeb, Google, and Twitter, but increasingly Github is where I find the newest, coolest APIs. I do a lot of searching via Github for API related topics, but increasingly Github topics themselves are becoming more valuable within search engine indexes, making them an easy way to uncover interesting APIs.</p>

<p><a href="https://streamdata.io/blog/robust-market-data-apis-alphavantage/">I was profiling the market data API Alpha Vantage today</a>, and one of the things I always do when I am profiling an API, is I conduct a Google, and then secondarily, a Github search for the APIs name. Interestingly, <a href="https://github.com/topics/alpha-vantage">I found a list of Github Topics while Googling for Alpha Vantage API</a>, uncovering some interesting SDKs, CLI, and other open source solutions that have been built on top of the financial data API. Showing the importance of operating your API on Github, but also working to define a set of standard Github Topic tags across all your projects, and helping encourage your API community to use the same set of tags, so that their projects will surface as well.</p>

<p>I consider Github to be the most important tool in an API providers toolbox these days. I know as an API analyst, it is where I learn the most about what is really going on. It is where I find the most meaningful signals that allow me to cut through the noise that exists on Google, Twitter, and other channels. Github isn’t just for code. As I mention regularly, 100% of my work as API Evangelist lives within hundreds of separate Github repositories. Sadly, I don’t spend as much time as I should tagging, and organizing projects into meaningful topic areas, but it is something I’m going to be investing in more. Conveniently, I’m doing a lot of profiling of APIs for my partner Streamdata.io, which involves establishing meaningful tags for use in defining real time data stream topics that consumers can subscribe to–making me think a little more about the role Github topics can play.</p>

<p>One of these days I will do a fresh roundup of the many ways in which Github can be used as part of API operations. I’m trying to curate and write stories about everything I come across while doing my work. The problem is there isn’t a single place I can send my readers to when it comes to applying this wealth of knowledge to their operations. The first step is probably to publish Github as its own research area on Github (mind blown), as I do with my other projects. It has definitely risen up in importance, and can stand on its own feet alongside the other areas of my work. Github plays a central role in almost every stop along the API life cycle, and deserves its own landing page when it comes to my API research, and priority when it comes to helping API providers understanding what they should be doing on the platform to help make their API operations more successful.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/the-growing-importance-of-github-topics-for-your-api-seo/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/summary-of-aws-api-gateway-as-an-api-deployment-and-management-solution/">A Summary Of AWS API Gateway As An API Deployment and Management Solution</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/aws/aws-api-gateway-icon.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was providing an overview of Kong, AWS API Gateway, and other solutions for a team I’m advising a couple weeks back. I was just looking to distill down some of the key features, and provide an overview to a large, distributed team. This work lends itself well to publishing here on the blog, so <a href="http://apievangelist.com/2018/02/13/summary-of-kong-as-an-api-management-solution/">I published an overview of Kong yesterday</a>, and today I wanted to publish the summary of the AWS API Gateway. The API gateway solution from AWS has some overlap with what Kong delivers, but I consider it to be more of an API deployment, as well as an API management gateway.</p>

<p>The <a href="https://aws.amazon.com/api-gateway/">AWS API Gateway brings API</a> deployment front and center, allowing you to define and deploy APIs that are wired up to your backend (AWS) infrastructure:</p>

<ul>
  <li><strong>API Endpoint</strong> - a host name of the API. the API endpoint can be edge-optimized or regional, depending on where the majority of your API traffic originates from. You choose a specific endpoint type when creating an API.</li>
  <li><strong>Backend Endpoint</strong> - A backend endpoint is also referred to as an integration endpoint and can be a Lambda function, an HTTP webpage, or an AWS service action, or a mock interface.</li>
  <li><strong>Swagger / OpenAPI</strong> - Using Swagger to import and export API configuration and definitions.</li>
</ul>

<p>Then the gateway brings a wealth of API management features, providing a look at how it has been baked into the cloud now:</p>

<ul>
  <li><strong>Accounts</strong> - Creation and management of Accounts.</li>
  <li><strong>Keys</strong> - Creation and management of API Keys</li>
  <li><strong>Certificates</strong> - Adding and management of certifications</li>
  <li><strong>Documentation</strong> - Publishing of ApI documentation</li>
  <li><strong>Domains</strong> - Mapping of domains</li>
  <li><strong>Response</strong> - Custom Gateway responses.</li>
  <li><strong>Models</strong> - Management of schema models.</li>
  <li><strong>Validation</strong> - Validation of API requests</li>
  <li><strong>SDK Generation</strong> - Generating of client SDKs</li>
  <li><strong>Staging</strong> - Establishing of stages</li>
  <li><strong>Tags</strong> - Tagging of resources</li>
  <li><strong>Templates</strong> - Mapping template used to transform a payload.</li>
  <li><strong>Plans</strong> - Establishing of different plans for API usage.</li>
  <li><strong>VPC</strong> - Usage of VP under the caller’s account in a region.</li>
  <li><strong>Regions</strong> - Deployment of gateways in different AWS regions.</li>
  <li><strong>Serverless</strong> - Usage of Lambda for serves integration.</li>
  <li><strong>Logging</strong> - Logging using Cloudwatch.</li>
  <li><strong>IAM</strong> - You can use AWS administration and security tools, such as AWS Identity and Access Management (IAM) and Amazon Cognito, to authorize access to your APIs.</li>
</ul>

<p>Then of course, everything with AWS has two separate programmatic interfaces for you to work with everything:</p>

<ul>
  <li><strong>API</strong> - Programmatic access through hypermedia API.</li>
  <li><strong>Command Line</strong> - The AWS CLI is an open source tool built on top of the AWS SDK for Python that provides commands for interacting with AWS services.</li>
</ul>

<p>AWS API Gateway doesn’t have some of the bells and whistles associated with other leading API management solutions, however it makes up for this with its API deployment capabilities–answering the age old question, which of the API management solutions will help me deploy my APIs. If you are operating your infrastructure within AWS, then AWS API Gateway makes a lot of sense. The connectivity it brings to the table is hard to ignore. What really sold it for me, is the IAM part of the equation. Before using AWS, I never had fine grained policies for what backend systems my APIs can or cannot access.</p>

<p>I avoided AWS API Gateway for a while. I was waiting for it to mature, and looking for enough benefit to get me beyond my vendor lock-in fears with API infrastructure. The IAM and Serverless aspects of delivering APIs are the features that pushed me to the point where I’m now using it for about 50% of my API infrastructure. It isn’t as portable, and versatile as solutions like Kong or Tyk are, but it does provide a solid set of API deployment and management features for me to put to work on projects that are already running in the AWS cloud.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/summary-of-aws-api-gateway-as-an-api-deployment-and-management-solution/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/your-microservices-effort-will-fail-because-you-will-never-decouple-your-business/">Your Microservices Effort Will Fail Because You Will Never Decouple Your Business</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/containership_deep_connections.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m regularly surprised by companies who are doing microservices which are failing to see the need the change organizational culture, and that microservices will be some magic voodoo to fix all their legacy technical debt. That simply decoupling and breaking down the technology, without any re-evaluation of the business and politics behind, will fix everything, and set the company, organizations, institution, or government agency on a more positive trajectory. In coming years, we will continue to hear stories about why microservices do not work, from endless waves or groups who were unable to do the hard work to decouple, and reorganize the operations behind the services they provide.</p>

<p>The monolith legacy systems I’m seeing targeted are widely seen as purely technology, which is why it is often labeled as technical debt. What is missing from this targeting and labeling is any acknowledgement of the people and decisions behind the monolith. The years of business, political, and cultural investment into the monolith. How will we every unwind, or properly address the monolith, if we do not see the organizational, human, and business aspects of why it exists in the first place? Are we talking about the business decisions that went into creating and perpetuating the monolith? It is highly likely we will be making some of the same decisions with microservices, which could end up being worse than when we made them with a single system. Distributed mess, is often more painful than consolidated mess.</p>

<p>I’m seeing endless waves of large organizations mandating that their teams invest in microservices, with no mandating for microteams, microbudgets, microdecisionmaking, or any of the other decoupling needed to make microservices truly work independently. I attach micro as a joke. I really don’t feel micro is the constant that needs applying when it comes to services, or the business and organizational mechanism behind them. However, it is the word du jour, and one that gets at some of the illnesses our organizations are facing in 2018. In reality, it is more about decoupling and decomposing the technology, business, and politics of our operations, into meaningful units that can be deployed, operate, and deprecated independent of each other.</p>

<p>My point is that your microservices effort will fail if you aren’t addressing the business side of the equation. If your microservices team(s) still exist within your legacy organizational structure, you really haven’t decoupled or decomposed anything. The old way of making decisions, dealing with budget impacts, will still reflect what happened with the previous monolith. Your technology will be independently operating, but still beholden to the same ways of deciding and funding what actually happens on the ground. The result will resemble having a entirely new motor, where you are running without lubricant, or possibly old, thick, expired lubricant that prevents your new motor from ever delivering at full capacity, and eventually breaking down in ways you have never imagined while operating your existing monolith.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/your-microservices-effort-will-fail-because-you-will-never-decouple-your-business/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/13/summary-of-kong-as-an-api-management-solution/">A Summary Of Kong As An API Management Solution</a></h3>
        <span class="post-date">13 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/kong/get-kong-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was breaking down what the API management solution Kong delivers for a customer of mine, and I figured I’d take what I shared via the team portal, and publish here on the blog. It is an easy way for me to create content, and make my consulting work more transparent here on the blog. I am using Kong as part of several healthcare and financial projects currently, and I am actively employing it to ensure customers are properly managing their APIs. I wasn’t the decision maker on any of these projects when it came to choosing the API management layer, I am just the person who is helping standardize how they are using API services and tooling across the API life cycle for these projects.</p>

<p>First, <a href="https://konghq.com">Kong is an open source API management solution</a> with an easy to install <a href="https://konghq.com/install/">community edition</a>, and <a href="https://konghq.com/kong-enterprise-edition/">enterprise level support when needed</a>. They provide an <a href="https://konghq.com/api-admin-gui/">admin interface</a>, and <a href="https://konghq.com/api-dev-portal/">developer portal</a> for the API management proxy, but there is also a growing number of community editions like <a href="https://ajaysreedhar.github.io/kongdash/">KongDash</a>, and <a href="ttps://pantsel.github.io/konga/">Konga</a> emerging to make it a much more richer ecosystem. And of course, <a href="https://getkong.org/docs/0.12.x/admin-api/">Kong has an API for managing the API management layer</a>, as every API service and tooling provider should have.</p>

<p>Now, let’s talk about what Kong does for helping in the deploying of your APIs:</p>

<ul>
  <li><strong>API Routing</strong> - The API object describes an API that’s being exposed by Kong. Kong needs to know how to retrieve the API when a consumer is calling it from the Proxy port. Each API object must specify some combination of hosts, uris, and methods</li>
  <li><strong>Consumers</strong> - The Consumer object represents a consumer - or a user - of an API. You can either rely on Kong as the primary datastore, or you can map the consumer list with your database to keep consistency between Kong and your existing primary datastore.</li>
  <li><strong>Certificates</strong> - A certificate object represents a public certificate/private key pair for an SSL certificate.</li>
  <li><strong>Server Name Indication (SNI)</strong> - An SNI object represents a many-to-one mapping of hostnames to a certificate.</li>
</ul>

<p>Then it focuses on the core aspects of what is needed to help manage your APIs:</p>

<ul>
  <li><strong>Authentication</strong> - Protect your services with an authentication layer.</li>
  <li><strong>Traffic Control</strong> - Manage, throttle, and restrict inbound and outbound API traffic.</li>
  <li><strong>Analytics</strong> - Visualize, inspect, and monitor APIs and microservice traffic.</li>
  <li><strong>Transformations</strong> - Transform requests and responses on the fly.</li>
  <li><strong>Logging</strong> - Stream request and response data to logging solutions.</li>
</ul>

<p>After that, it has a bunch of added features to help make it a scalable, evolvable solution:</p>

<ul>
  <li><strong>DNS-based loadbalancing</strong> - When using DNS based load balancing the registration of the backend services is done outside of Kong, and Kong only receives updates from the DNS server.</li>
  <li><strong>Ring-balancer</strong> - When using the ring-balancer, the adding and removing of backend services will be handled by Kong, and no DNS updates will be necessary.</li>
  <li><strong>Clustering</strong> - A Kong cluster allows you to scale the system horizontally by adding more machines to handle more incoming requests. They will all share the same configuration since they point to the same database. Kong nodes pointing to the same datastore will be part of the same Kong cluster.</li>
  <li><strong>Plugins</strong> - lua-nginx-module enables Lua scripting capabilities in Nginx. Instead of compiling Nginx with this module, Kong is distributed along with OpenResty, which already includes lua-nginx-module. OpenResty is not a fork of Nginx, but a bundle of modules extending its capabilities.</li>
  <li><strong>API</strong> - Administrative API access for programmatic control.</li>
  <li><strong>CLI Reference</strong> - The provided CLI (Command Line Interface) allows you to start, stop, and manage your Kong instances. The CLI manages your local node (as in, on the current machine).</li>
  <li><strong>Serverless</strong> - Invoke serverless functions via APIs.</li>
</ul>

<p>There are a number of API management solutions available out there today. I will profile each one  I am actively using as part of my work on the ground. I’m agnostic towards which provider my clients should use, but I like having the details about what features they bring to the table readily available via a single URL, so that I can share when these conversations come up. I have many <a href="http://management.apievangelist.com">API management solutions profiled as part of my API management research</a>, but in 2018 there are just a handful of clear leaders in the game. I’ll be focusing on the ones who are still actively investing in the API community, and the ones I have an existing relationship with in a partnership capacity. Streamdata.io is a reseller of Kong in France, making it something I’m actively working with in the financial space, and also something I’m using within the federal government, also bringing it front and center for me in the United States.</p>

<p>If you have more questions about Kong, or any other API management solution, feel free to reach out, and I’ll do my best to answer any questions. We are also working to provide more API life cycle, strategy, and governance services along with <a href="http://skylight.digital">my government API partners at Skylight</a>, and through <a href="http://apis.how/streamdata">my mainstream API partners at Streamdata.io</a>. If you need help understanding the landscape and where API management solutions like Kong fits in, me and my partners are happy to help out–just let us know.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/13/summary-of-kong-as-an-api-management-solution/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/13/aggregating-multiple-github-account-rss-feeds-into-single-json-api-feed/">Aggregating Multiple Github Account RSS Feeds Into Single JSON API Feed</a></h3>
        <span class="post-date">13 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/octocat-aggregate.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>Github is the number one signal in my API world. The activity that occurs via Github is more important than anything I find across Twitter, Facebook, LinkedIn, and other social channels. Commits to repositories and the other social activity that occurs around coding projects is infinitely more valuable, and telling regarding what a company is up to, than the deliberate social media signals blasted out via other channels is. I’m always working to dial in my monitoring of Github using the Github API, but also via the RSS feeds that are present on the public side of the platform.</p>

<p>I feel RSS is often overlooked as an API data source, but I find that RSS is not only alive and well in 2018, it is something that is actively used on many platforms. The problem with RSS for me, is the XML isn’t always conducive to working with in many of my JavaScript enabled applications, and I also tend to want to aggregate, filter, and translate RSS feeds into more meaningful JSON. To help me accomplish this for Github, I crafted a simple PHP RSS aggregator and converter script which I can run in a variety of situations. I published the basic script to Github as a Gist, for easy reference.</p>

<script src="https://gist.github.com/kinlane/30461b54300f29da462db4f63fccd6f5.js"></script>

<p>The simple PHP script just takes an array of Github users, loops through them, pulls their RSS feeds, and then aggregates them into a single array, sorts by date, and then outputs as JSON. It is a pretty crude JSON API, but it provides me with what I need to be able to use these RSS feeds in a variety of other applications. I’m going to be mining the feeds for a variety of signals, including repo and user information, which I can then use within other applications. The best part is this type of data mining doesn’t require a Github API key, and is publicly available, allowing me to scale up much further than I could with the Github API alone.</p>

<p>Next, I have a couple of implementations in mind. I’m going to be creating a Github user leaderboard, where I stream the updates using Streamdata.io to a dashboard. Before I do that, I will have to aggregate users and repos, incrementing each commit made, and publishing as a separate JSON feed. I want to be able to see the raw updates, but also just the most changed repositories, and most active users across different segments of the API space. <a href="http://apis.how/streamdata">Streamdata.io allows me to take these JSON feeds and stream them to the dashboard using Server-Sent Events(SSE)</a>, and then applying each update using JSON Patch. Making for a pretty efficient way to put Github to work as part of my monitoring of activity across the API space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/13/aggregating-multiple-github-account-rss-feeds-into-single-json-api-feed/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/13/having-a-developer-yourdomain-is-clear-differentiator-in-the-api-game/">Having A Developer.[YourDomain] Is Clear Differentiator In The API Game</a></h3>
        <span class="post-date">13 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/developer-you-com.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am profiling US, UK, French, and German banks as part of some research I am doing for <a href="http://apis.how/streamdata">Streamdata.io</a>. I am profiling how far along in the API journey these banks are, and one clear differentiator for me is whether a bank has a developer.[bankdomain] subdomain setup for their APIs or not. The banks that have a dedicated subdomain for their API operations have a clear lead over those who do not. The domain doesn’t do much all by itself, but it is clear that when a bank can get this decision made, many of the other decisions that need to be made are also happening in tandem.</p>

<p>This isn’t unique just to banking. This is something I’ve written about several times over the years, and remains constant after looking at thousands of APIs over the last eight years. When a company’s API presence exists within the help section of their website, the API is almost always secondary to the core business. When a company relies on a 3rd party service for their API and developer presence, it almost always goes dormant after a couple months, showing that APIs are just not a priority within the company. Having a dedicated subdomain, landing page, and set of resources dedicated to doing APIs goes a long way towards ensuring an API program gains the momentum it needs to be successful within an organization, and industry.</p>

<p>I know that having a dedicated subdomain for API operations seems like a small thing to many folks. However, it is one of the top symptoms of a successful API in my experience. Making data, content, and algorithms available in a machine readable way for use in other applications by 3rd party via the web is something every company, organization, institution, and government agency should be doing in 2018. It is the next iteration of the web, and is not something that should be a side project. Having a dedicated subdomain demonstrates that you understand this, and an API won’t just be the latest trend at your organization. Even if your APIs are entirely private in the beginning, having a public portal for your employees, partners, and other stakeholders will go along way towards helping you get the traction you are looking for in the API game.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/13/having-a-developer-yourdomain-is-clear-differentiator-in-the-api-game/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/13/streaming-and-event-driven-architecture-represents-maturity-in-your-api-journey/">Streaming And Event-Driven Architecture Represents Maturity In The API Journey</a></h3>
        <span class="post-date">13 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/downtheline_dark_dali.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Working with Streamdata.io has forced a shift in how I see the API landscape. When I started working with their proxy I simply saw it about doing API in real time. I was hesitant because not every API had real time needs, so I viewed what they do as just a single tool in my API toolbox. While Server-Sent Events, and proxying JSON APIs is just one tool in my toolbox, like the rest of the tools in my toolbox it forces me to think through what an API does, and understand where it exists in the landscape, and where the API provider exists in their API journey. Something I’m hoping the API providers are also doing, but I enjoy doing from the outside-in as well.</p>

<p>Taking any data, content, media, or algorithm and exposing as an API, is a journey. It is about understanding what that resource is, what it does, and what it means to the provider and the consumer. What this looks like day one, will be different from what it looks like day 365 (hopefully). If done right, you are engaging with consumers, and evolving your definition of the resource, and what is possible when you apply it programmatically through the interfaces you provide. API providers who do this right, are leveraging feedback loops in place with consumers, iterating on their APIs, as well as the resources they provide access to, and improving upon them.</p>

<p>Just doing simple web APIs puts you on this journey. As you evolve along this road you will begin to also apply other tools. You might have the need for webhooks to start responding to meaningful events that are beginning to emerge across the API landscape, and start doing the work of defining your event-driven architecture, developing lists of most meaningful topics, and events that are occurring across your evolving API platform. Webhooks provide direct value by pushing data and content to your API consumers, but they have indirect value in helping you define the event structure across your very request and response driven resource landscape. Look at <a href="https://developer.github.com/v3/activity/events/types/">Github webhook events</a>, or <a href="https://api.slack.com/events/api">Slack webhook events</a> to understand what I mean.</p>

<p>API platforms that have had webhooks in operation for some time have matured significantly towards and event-driven architecture. Streaming APIs isn’t simply a boolean thing. That you have data that needs to be streamed, or you don’t. That is the easy, lazy way of thinking about things. Server-Sent Events (SSE) isn’t just something you need, or you don’t. It is something that you are ready for, or you aren’t. Like webhooks, I’m seeing Server-Sent Events (SSE) as having the direct benefits of delivering data and content as it is updated, to the browser or for other server uses. However, I’m beginning to see the other indirect benefits of SSE, and how it helps define the real time nature of a platform–what is real time? It also helps you think through the size, scope, and efficiency surrounding the use of APIs for making data, content, and algorithms available via the web. Helping us think through how and when we are delivering the bits and bytes we need to get business done.</p>

<p>I’m learning a lot by applying <a href="http://apis.how/streamdata">Streamdata.io</a> to simple JSON APIs. It is adding another dimension to the API design, deployment, and management process for me. There has always been an evolutionary aspect of doing APIs for me. This is why you hear me call it the API journey on a regular basis. However, now that I’m studying event-driven architecture, and thinking about how tools like webhooks and SSE assist us in this journey, I’m seeing an entirely new maturity layer for this API journey emerge. It goes beyond just getting to know our resources as part of the API design, and deployment process. It builds upon API management and monitoring and helps us think through how our APIs are being consumed, and what the most meaningful and valuable events are. Helping us think through how we deliver data and content over the web in a more precise manner. It is something that not every API provider will understand right away, and only those a little further along in their journey will be able to take advantage of. The question is, how do we help others see the benefits, and want to do the hard work to get further along in their own API journey.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/13/streaming-and-event-driven-architecture-represents-maturity-in-your-api-journey/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/12/more-outputs-are-better-when-it-comes-to-establishing-an-api-observability-ranking/">More Outputs Are Better When It Comes To Establishing An API Observability Ranking</a></h3>
        <span class="post-date">12 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/ellisisland_blue_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been evolving an observability ranking for the APIs I track on for a couple years now. I’ve bene using the phrase to describe my API profiling and measurement approach <a href="http://apievangelist.com/2016/10/25/thinking-about-an-api-observability-stack/">since I first learned about the concept from Stripe</a>. There are many perspectives floating around the space about what observability means in the context of technology, however mine is focused completely on APIs, and is more about communicating with external stakeholders, more than it is just about monitoring of systems. To recap, <a href="https://en.wikipedia.org/wiki/Observability">the Wikipedia definition for observability is</a>:</p>

<blockquote>
  <p><em>Formally, a system is said to be observable if, for any possible sequence of state and control vectors, the current state can be determined in finite time using only the outputs (this definition is slanted towards the state space representation). Less formally, this means that from the system’s outputs it is possible to determine the behavior of the entire system. If a system is not observable, this means the current values of some of its states cannot be determined through output sensors.</em></p>
</blockquote>

<p>Most of the conversations occurring in the tech sector are focused on monitoring operations, and while this is a component of my definition, I lean more heavily on the observing occurring beyond just internal groups, and observability being about helping keep partners, consumers, regulators, and other stakeholders be more aware regarding how complex systems work, or do not work. I feel that observability is critical to the future of algorithms, and making sense of how technology is impacting our world, and APIs will play a critical role in ensuring that the platforms have the external outputs required for delivering meaningful observability.</p>

<p>When it comes to quantifying the observability of platforms and algorithms, the more outputs available the better. Everything should have APIs for determining the inputs and outputs of any algorithm, or other system, but there should also be operational level APIs that give insight into the underlying compute, storage, logging, DNS, and other layers of delivering technological solutions. There should also be higher level business layer APIs surrounding communication via blog RSS, Twitter feeds, and support channels like email, ticketing, and other systems. The more outputs around platform operations there are, the more we can measure, quantify, and determine how observable a platform is using the outputs that already exist for ALL the systems in use across operations.</p>

<p>To truly measure the observability of a platform I need to be able to measure the technology, business, and politics surrounding its operation. If communication and support exist in the shadows, a platform is not observable, even if there are direct platform APIs. If you can’t get at the operational layer like logging, or possibly Github repositories used as part of continuous integration or deployment pipelines, observability is diminished. Of course, not all of these outputs should all be publicly available by default, but in many cases, there really is no reason they can’t. At a minimum there should be API access, with some sort of API management layer in place, allowing for 3rd party auditors, and analysts like me to get at some, or all of the existing outputs, allowing us to weigh in on an overall platform observability workflow.</p>

<p>As I continue to develop my API observability ranking algorithm, the first set of values I calculate are the number of existing outputs an API has. Taking into consideration the scope of core and operational APIs, but also whether I can get at operations via Twitter, Github, LinkedIn, and other 3rd party APIs. I find many of these channels more valuable for understanding platform operations, than the direct APIs themselves. Chatter by employees, and commits via Github can provide more telling signals about what is happening, than the intentional signals emitted directly by the platform itself. Overall, the more outputs available the better. API observability is all about leveraging existing outputs, and when companies, organizations, institutions, and government agencies are using solutions that have existing APIs, they are more observable by default, which can have a pretty significant impact in helping us understand the impact a technological solution is having on everyone involved.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/12/more-outputs-are-better-when-it-comes-to-establishing-an-api-observability-ranking/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/12/labeling-your-high-usage-apis-and-externalizing-api-metrics-within-your-api-documentation/">Labeling Your High Usage APIs and Externalizing API Metrics Within Your API Documentation</a></h3>
        <span class="post-date">12 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/alpha-advantage/alpha-vantage-high-usage-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am profiling a number of market data APIs as part of my research with <a href="http://apis.how/streamdata">Streamdata.io</a>. As I work my way through the process of profiling APIs I am always looking for other interesting ideas for stories on API Evangelist. <a href="https://www.alphavantage.co/documentation/">One of the things I noticed while profiling Alpha Vantage</a>, was that they highlighted their high usage APIs with prominent, very colorful labels. One of the things I’m working to determine in this round of profiling is how “real time” APIs are, or aren’t, and the high usage label adds another interesting dimension to this work.</p>

<p>While reviewing API documentation it is nice to have labels that distinguish APIs from each other. Alpha Vantage has a fairly large number of APIs so it is nice to be able to focus on the ones that are used the most, and are more popular. For example, as part of my profiling I focused on the high usage technical indicator APIs, rather than profiling all of them. I need to be able to prioritize my work, and these labels helped me do that. Providing one example of the benefit that these types of labels can bring to the table. I’m guessing that there are many other time saving aspects of labeling popular APIs, beyond just saving me time.</p>

<p>This type of labeling is an interesting way of externalizing API analytics in my opinion. Which is another interesting concept to think about across API operations. How can you take the most meaningful data points across your API management processes, and distill them down, externalize and share them so that your API consumers can benefit from valuable API metrics? In this context, I could see a whole range of labels that could be established, applied to interactive documentation using OpenAPI tags, and made available across API documentation, helping make APIs even more dynamic, and in sync with how they are actually being used, measured, and making an impact on operations.</p>

<p>I’m a big fan of making API documentation even more interactive, alive, and meaningful to API consumers. I’m thinking that tagging and labeling is how we are going to do this in the future. Generating a very visual, but also semantic layer of meaning that we can overlay in our API documentation, making them even more accessible by API consumers. I know that Alpha Advantages’s high usage labels have saved me significant amounts work, and I’m sure there are other approaches that could continue delivering in this way. It is something I’m keeping a close eye in this increasingly event-driven, API landscape, where API integration is becoming more dynamic and real time.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/12/labeling-your-high-usage-apis-and-externalizing-api-metrics-within-your-api-documentation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/12/be-clear-about-your-api-pricing/">Be Clear About Your API Pricing</a></h3>
        <span class="post-date">12 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/green-gears-matrix.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m profiling a large number of APIs right now, and I am ranking APIs based upon how easy or difficult they are to access. Whether or not an API provide has a business model is part of the ranking, and how clearly articulated the access and pricing is around that model is a critical part of my profiling algorithm. The APIs that end up included in the API gallery I’m developing for <a href="http://apis.how/streamdata">Streamdata.io</a>, and available as part of my wider <a href="http://theapistack.com">API Stack research</a> will all have to possess easy to articulate access levels. Not all of them will be free, but the ones that cost money will have straightforward pricing that can be articulate in a single sentence–something that seems to be elusive with many of the API providers I am profiling.</p>

<p>I am regularly confused regarding the myriad of ways in which API providers obfuscate the pricing for their APIs. I’ve long been weary of API providers who don’t have a clear business model, but when they have a pricing page, but bu do not consistently apply it to APIs, I’m just left confounded. I can’t always tell if it is done maliciously, or they just haven’t approached their API through an external lens. If I find a pricing page, and the plans seem reasonable, and I’ve plugged my credit card in, but then I still don’t have access to some APIs, and there is no clear labeling of which APIs I have access to as part of my plan, I just can’t spend the afternoon testing and seeing which APIs return a 403 to understand the landscape. The API service composition, and pricing tiers needs to be coherent and front and center, otherwise I just have to move on. If I can’t communicate what is going on to others, it won’t be included in my work.</p>

<p>I do not have a problem with different tiers of access, as long as they are communicated, and information about them is accessible. I won’t complain when some APIs are out of reach to me, and placed in premium tiers–I’ll just pass that information on to my readers. However, if I have to do some sort of secret handshake, or call some special sales hotline to understand what is going on, in my experience there are usually other illnesses occurring behind the scene, and I’m pretty well conditioned to just move on. If you have a publicly available API, be clear about your pricing. Even if I need approval for higher levels of usage, or it costs me to gain access to high level tiers. Don’t play games, there are too many APIs out there to mess around with hidden API pricing plans, unless of course you really aren’t interested in folks covering your API, and putting them to use, which I feel like some of these companies are actually hoping occurs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/12/be-clear-about-your-api-pricing/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/12/a-dedicated-guest-blogger-program-for-your-api/">A Dedicated Guest Blogger Program For Your API</a></h3>
        <span class="post-date">12 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/intrinio/intrinio-guest-blogger-program.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I get endless waves of people wanting to “guest post” on API Evangelist. It isn’t something I’m interested in because of the nature of API Evangelist, and that it really is just my own stream of consciousness, and not about selling any particular product or service. However, if you are an API provider, looking for quality content for your blog, having a formal approach to managing guest bloggers might make sense. Sure, you don’t want to accept all the spammy requests that you will get, but with the right process, you could increase the drumbeat around your API, and build relationships with your partners and API consumers.</p>

<p>There is an example of this in action at the financial data marketplace Intrinio, with <a href="https://intrinio.com/bloggers">their official blogger program</a>. The blogging program for the platform has a set of established benchmarks defined by the Intrinio team, to establish quality for any post that is accepted as part of the program. What I find really interesting, is that they also offer three months of free access to data feeds for API consumers who publish a post via the platform. “Exceptional” participants in the program may have their free access extended, and ALL participants will receive discounts on paid data access subscriptions via the platforms APIs.</p>

<p>This is the type of value exchange I like to see via API platforms. Too many APIs are simple one way streets, paying for GET access to data, content, media, and algorithms. API management shouldn’t be just about about metering one way access and charging for it. Sensible API management should measure value exchange around ALL platform resources, including blog and forum posts, and other activities API providers should be incentivizing via their platforms. This is one of the negative side effects of REST I feel–too much focus on resources, and not about the events that occur around these resources. Something we are beginning to move beyond in an event-driven API landscape.</p>

<p>Next, I will be profiling the concept of having dedicated data partner programs for your API platform. Showcasing how your API consumers can submit their own data APIs for resell alongside your own resources. In my opinion, every API platform should be opening up every resource for GET, POST, PUT, and DELETE, as well as allow for the augmenting, aggregation, enrichment, and introduction of other data, content, media, and algorithms, to add more value to what is already going on. Opening up a dedicated guest blogger program modeled after Intrinio’s is a good place to start. Learning about how to set up guidelines and benchmarks for submission, and evolving your API management to allow for incentivizing of participation. Once you get your feet wet with the blog, you may want to expand to other resources available via the platform, making your API operations a much more community thing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/12/a-dedicated-guest-blogger-program-for-your-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/you-have-to-know-where-all-your-apis-are-before-you-can-deliver-on-api-governance/">You Have to Know Where All Your APIs Are Before You Can Deliver On API Governance</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/64_185_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I wrote an earlier article that <a href="http://apievangelist.com/2017/12/20/basic-api-design-guidelines-are-you-first-step-towards-api-governance/">basic API design guidelines are your first step towards API governance</a>, but I wanted to introduce another first step you should be taking even before basic API design guides–cataloging all of your APIs. I’m regularly surprised by the number of companies I’m talking with who don’t even know where all of their APIs are. Sometimes, but not always, there is some sort of API directory or catalog in place, but often times it is out of date, and people just aren’t registering their APIs, or following any common approach to delivering APIs within an organization–hence the need for API governance.</p>

<p>My recommendation is that even before you start thinking about what your governance will look like, or even mention the word to anyone, you take inventory of what is already happening. Develop an org chart, and begin having conversations. Identify EVERYONE who is developing APIs, and start tracking on how they are doing what they do. Sure, you want to get an inventory of all the APIs each individual or team is developing or operating, but you should also be documenting all the tooling, services, and processes they employ as part of their workflow. Ideally, there is some sort of continuous deployment workflow in place, but this isn’t a reality in many of the organization I work with, so mapping out how things get done is often the first order of business.</p>

<p>One of the biggest failures of API governance I see is that the strategy has no plan for how we get from where we are to where we ant to be, it simply focuses on where we want to be. This type of approach contributes significantly to pissing people off right out of the gate, making API governance a lot more difficult. Stop focusing on where you want to be for a moment, and focus on where you are. Build a map of where people are, tools, services, skills, best and worst practices. Develop a comprehensive map of where organization is today, and then sit down with all stakeholders to evaluate what can be improved upon, and streamlined. Beginning the hard work of building a bridge between your existing teams and what might end up being a future API governance strategy.</p>

<p>API design is definitely the first logical step of your API governance strategy, standardizing how you design your APIs, but this shouldn’t be developed from the outside-in. It should be developed from what already exists within your organization, and then begin mapping to healthy API design practices from across the industry. Make sure you are involving everyone you’ve reached out to as part of inventory of APIs, tools, services, and people. Make sure they have a voice in crafting that first draft of API design guidelines you bring to the table. Without buy-in from everyone involved, you are going to have a much harder time ever reaching the point where you can call what you are doing governance, let alone seeing the results you desire across your API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/you-have-to-know-where-all-your-apis-are-before-you-can-deliver-on-api-governance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/riot-games-regional-endpoints/">Riot Games Regional API Endpoints</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/riot-games/riot-games-developer-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m slowly categorizing all the APIs I find who are offering up some sort regional availability as part of their operations. With the easy of deployment using leading cloud services, it is something I am beginning to see more frequently. However, there is still a wide variety of reasons why an API provider will invest in this aspect of their operations, and I’m looking to understand more about what these motivations are. Sometimes it is because they are serving a global audience, and latency kills the experience, but other times I’m seeing it is more about the maturity of the API provider, and they’ve have such a large user base that they are getting more requests to deliver resources closer to home.</p>

<p><a href="https://developer.riotgames.com/regional-endpoints.html">The most recent API provider I have come across who is offering regional API endpoints is from Riot Games</a>, the makers of League of Legends, who offers <a href="https://developer.riotgames.com/regional-endpoints.html">twelve separate regions for you to chose from</a>, broken down using a variety of regional subdomains. The Riot Games API provides a wealth of meta data around their games, and while they don’t state their reasons for providing regional APIs, I’m guessing it is to make sure the meta data is localized to whichever country their customers are playing in. Reducing an latency across networks, making the overall gaming and supporting application experience as smooth and seamless as possible. Pretty standard reasons for doing regional APIs, and providing a simple example of how you do this at the DNS level.</p>

<p><a href="https://developer.riotgames.com/api-status/">RIot Games also provides a regional breakdown of the availability of their regional endpoints on their API status page</a>, adding another dimension to the regional API delivery conversation. If you are providing regional APIs, you should be monitoring them, and communicating this to your consumers. This is all pretty standard stuff, but I’m working to document every example of regional APIs I come across as part of my research. I’m considering adding a separate research area to track on the different approaches so I can publish a guide, and supporting white papers when I have enough information organized. All part of my work to understand how the API business operates, and is expanding. Showcasing how the leaders are delivering resources via APIs in a scalable way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/riot-games-regional-endpoints/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/consistentency-and-branding-across-api-portals/">Consistency in Branding Across API Portals</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/subway/london-underground.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://www.bbc.co.uk/programmes/b0903ppd">I recently watched a BBC documentary about the history of the branding used as part of the London Underground</a>. I’m pretty absorbed lately with using public transit as an analogy for complex API implementations, and moving beyond just using subway maps, I thought the branding strategy for the London Underground provided other important lessons for API providers. The BBC documentary went into great detail regarding how much work was put into standardizing the font, branding, and presentation of information for each London Underground, to help reduce confusion, and help riders get where they needed, and making the city operate more efficiently.</p>

<p>As I continue to study the world of <a href="http://documentation.apievangelist.com">API documentation</a>, I think we have so much work ahead of us when it comes to standardizing how we present our API portals. Right now every API portal is different, even often times with multiple portals from the same company–see Amazon Web Services for example. I think we underestimate the damage this has to the overall API experience for consumers, and why we see API documentation like Swagger UI, Slate, and Read the Docs have such an impact. However this is just documentation, and we need this to occur as part of the wider API portal user experience. I’ve seen some standardized open source API portal solutions, and there are a handful of API portal services out there, but there really is no standard for how we deliver, brand, and operate the wider API experience.</p>

<p>I have <a href="https://apievangelist.com/2015/04/10/my-minimum-viable-api-footprint-definition/">my minimum viable API portal definition</a>, and have been tracking on the common building blocks of API operations for eight years now, but there are no “plug and play” solutions that users can implement, following any single approach. I have the data, and <a href="http://portal.minimum.apievangelist.com/">I even have a simple Twitter Bootstrap version of my definition (something I’m upgrading ASAP)</a>, but in my experience people get very, very, very hung up on the visual aspects of this conversation, want different visual elements, and quickly get lost on the functional details. I’m working with my partners APIMATIC to help standardize their portal offering, but honestly it is something that needs to be wider than just me, and any single provider. It is something that needs to emerge as a common API portal standard. If we can bring this into focus, I think we will see API adoption significantly increase, reducing much of the confusion we all face getting up and running with any new API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/consistentency-and-branding-across-api-portals/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/keeping-api-schema-simple-for-wider-adoption/">Keeping API Schema Simple For Wider Adoption</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-csv.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>One aspect of <a href="http://apievangelist.com/2018/02/03/api-is-not-just-rest/">my talk at APIDays Paris this last week</a>, included a slide about considering to allow API consumers to negotiate CSV responses from our API. Something that would probably NEVER occur to most API providers, and probably would make many even laugh at me. I’m used to it, and don’t care. While not something that every API provider should be considering, depending on the data you are serving up, and who your target API consumer ares, it is something that might make sense. Allowing for the negotiation of CSV responses represents lowering the bar for API consumption, and widening the audience who can put our APIs to work.</p>

<p>I was doing more work around public data recently, and was introduced to an interesting look at some lessons from developing open data standards. I’m doing a deep dive into municipal data lately as part of my partnership with <a href="http://apis.how/streamdata">Streamdata.io</a>, and I found <a href="http://www.opennorth.ca/2017/12/21/from-development-to-adoption-lessons-from-three-open-standards.html">the lessons they published interesting</a>, and something that reflects my stance on API content negotiation.</p>

<blockquote>
  <p><em>From the development and maintenance of the API, it quickly became clear that adjusting scripts after every election (and by-election) and website modification, was quickly becoming unsustainable. To address this issue, a simple CSV schema was developed to encourage standardisation of this data from the outset. The schema was designed to be as simple and easy to understand and implement as possible. Comprised of just 21 fields, 7 of which are recommended fields, the schema does not have hierarchical relationships between terms and can be implemented in a single CSV file. By making the standard this simple, we were able to get a number of adopters onboard and outputting their lists of elected representatives on their own open data portals.</em></p>
</blockquote>

<p>When it comes to APIs, simplicity rules. The simpler you can make your API, the more impact you will make. Allowing for the negotiation of CSV responses from your API when possible allows API consumers to go from API to a spreadsheet in just one or two clicks. This is huge when it comes to on boarding business users with the concepts of APIs, and what they do, and allows them to easily put valuable data resources to work in their native environment–the spreadsheet. This is something many API consumers won’t understand, but when it comes to seeking meaningful API adoption, it is something that expand the reach of any API beyond the developer class, putting it within reach of business users.</p>

<p>I am a big fan of pushing our APIs to allow for the negotiation of CSV. XML, and JSON by default, whenever possible. I’m also a fan of delivering richer experiences by allowing for the negotiation of hypermedia media types. While delivering hypermedia takes a significant amount of thought and investment, allowing for the negotiation of CSV, XML, and JSON doesn’t take a lot of work. When delivering your APIs, I highly recommend thinking about who your API consumers are, and whether offering CSV responses might shift the landscape even a little bit, making your valuable data resources a little more usable by business users who won’t necessarily be delivering web or mobile applications.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/keeping-api-schema-simple-for-wider-adoption/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/api-quota-api-webhooks-and-server-sent-events-sse/">API Quota API, Webhooks, and Server-Sent Events (SSE)</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/goldsilverfalls/creativity/file-00_00_40_84.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am profiling market data APIs as part of my partnership with <a href="http://apis.how/streamdata">Streamdata.io</a>. It is a process I enjoy, because it provides me with a number of interesting stories I can tell here on API Evangelist. Many of the APIs I profile just frustrate me, but there are always the gems who are doing interesting things with their APIs, and understand providing APIs, as well as consuming APIs. One API that I’ve been profiling, and I am able to put to use in my work to build a gallery of real time data APIs, was <a href="https://1forge.com">1Forge</a>.</p>

<p><a href="https://1forge.com/forex-data-api/api-documentation">1Forge provides dead simple APIs for accessing market data</a>, and surprise!! – you can sign up for a key, and begin making API calls within minutes. It might not sound like that big of a deal, but after going through 25+ APIs, I only have about 5 API keys. I’m working on an OpenAPI definition for 1Forge, so I can begin to poll, and stream the data they make available, including it in the Streamdata.io API gallery I’m building. However, as I was getting up and running with the API, I noticed <a href="https://1forge.com/forex-data-api/api-documentation">their quota endpoint</a>, which allows me to check my usage quote with the 1Forge API–something that I thought was story worthy.</p>

<p>The idea of an endpoint to check my applications usage quota for an API seems like a pretty fundamental concept, but sadly it is something I do not see very often. It is something that should be default for ALL APIs, but additionally I’d like to see a webhook for, letting me know when my API consumption reaches different levels. Since I’m talking about Streamdata.io, it would also make sense to offer a Server-Sent Event (SSE) for the API quote endpoint, allowing me to bake the usage quota for all the APIs I depend on into a single API dashboard–streaming real time usage information across the APIs depend on, and maybe displaying things in RED when I reach certain levels.</p>

<p>An API quota API is useful for when you depend on a single API, but is something that becomes almost critical when it comes to depending on many APIs. These are one of those APIs that API providers are going to need to realize has to be present by default for their API platforms. It is something that can keep us humans in tune with our consumption, but more importantly can help us programmatically manage our API consumption, and adjust our polling frequency automatically as reach the limits of our API access tier, or even upgrade as we realize our rate limit constraints are too tight for a specific application. I’m going to add an API quota API to my list of default administrative APIs that API providers should be offering. Updating the default set of resources we should have available for ALL APIs we are operating.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/api-quota-api-webhooks-and-server-sent-events-sse/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/the-more-we-know-about-you-the-more-api-acces-you-get/">The More We Know About You The More API Access You Get</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/65_144_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2018/01/24/where-am-i-in-the-sales-funnel-for-your-api/">I’ve been trash talking APIs that identify me as part of some sort of sales funnel</a>, and automate the decision around whether or not I get access to their API. My beef isn’t with API providers profiling me and making decisions about how much access I get, it is about them limiting profiles making it so I do not get access to their APIs at all. Their narrow definitions of the type of API consumers they are seeking does not include me, even though I have thousands of regular readers of my blog who do fit their profile. In the end, it is their loss, not mine, that they do not let me in, but the topic is still something I feel should be discussed out in the open, hopefully expanding the profile definitions for some API providers who may not have considered the bigger picture.</p>

<p>I’ve highlighted the limiting profiling of API consumers that prevent access to APIs, but now I want to talk about how profiling can be sensibly used to limit access to API resources. Healthy API management always has an entry level tier, but what tiers are available after that often depend on a variety of other data points. One thing I see API providers regularly doing is requiring API consumers to provide more detail about who they are and what they are doing with an API. I don’t have any problem with API providers doing this, making educated and informed decisions regarding who an API consumer is or isn’t. As the API Evangelist I am happy to share more data points about me to get more access. I don’t necessarily want to do this to sign up for your entry level access tier, just so I can kick the tires, but if I’m needing deeper access, I am happy to fill our a fuller profile of myself, and what I am working on.</p>

<p>Stay out of my way when it comes to getting started and test driving your APIs. However, it is perfectly acceptable to require me to disclose more information, require me to reach out an connect with your team, and other things that you feel are necessary before giving me wider access to your APIs, and provide me with looser rate limits. I encourage API providers to push on API consumers before you give away the keys to the farm. Developing tiered levels of access is how you do this. Make me round off the CRM entry for my personal profile, as well as my company. Push me to validate who I am, and that my intentions are truly honest. I encourage you to reach out to each one of your API consumers with an honest “hello” email after I sign up. Don’t require me to jump on the phone, or get pushy with sales. However, making sure I provide you with more information about myself, my project and company in exchange for higher levels of API access is a perfectly acceptable way of doing business with APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/the-more-we-know-about-you-the-more-api-acces-you-get/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

<p align="center"><a href="http://apievangelist.com/archive/"><strong>View Previous Posts Via Archives</strong></a></p>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/" target="_blank"><img src="https://apievangelist.com/images/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/" class="image"><img src="https://apievangelist.com/images/postman-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
