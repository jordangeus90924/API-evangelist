<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }
    
    .container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
    }
    .video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }    

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/08/i-created-an-openapi-for-the-hashicorp-consul-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/hashicorp/hashicorp-consul-api-openapi-githbu.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/08/i-created-an-openapi-for-the-hashicorp-consul-api/">I Created An OpenAPI For The Hashicorp Consul API</a></h3>
			<p><em>08 Jan 2018</em></p>
			<p>I was needing an OpenAPI (fka Swagger) definition for the Hashicorp Consul API, so that I could use in a federal government project I’m advising on. We are using the solution for the microservices discovery layer, and I wanted to be able to automate using the Consul API, publish documentation within our project Github, import into Postman across the team, as well as several other aspects of API operations. I’m working to assemble at least a first draft OpenAPI for the entire technology stack we’ve opted to use for this project. First thing I did was Google, “Consul API OpenAPI”, then “Consul API Swagger”, which didn’t yield any results. Then I Githubbed “Consul API Swagger”, and came across a Github Issue where a user had asked for “improved API documentation”. The resulting response from Hashicorp was, “we just finished a revamp of the API docs and we don’t have plans to support Swagger at this time.” Demonstrating they really don’t understand what OpenAPI (fka Swagger) is, something I’ll write about in future stories this week. One of the users on the thread had created an API Blueprint for the Consul API, and published the resulting documentation to Apiary. Since I wanted an OpenAPI, instead of an API Blueprint, I headed over to APIMATIC API Transformer to see if I could get the job done. After trying to transform the API Blueprint to OpenAPI 2.0 I got some errors, which forced to me to spend some time this weekend trying to hand-craft / scrape the static API docs and publish my own OpenAPI. The process was so frustrating I ended up pausing the work, and writing two blog posts about my experiences, and then this morning I received an email from the APIMATIC team that they caught the errors, updated the API Blueprint, allowing me to continue transforming it into an OpenAPI definition. Benefits of being the API Evangelist? No, benefits of using APIMATIC! Anyways, you...[<a href="/2018/01/08/i-created-an-openapi-for-the-hashicorp-consul-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/08/api-life-cycle-basics-gateway/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-gateway.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/08/api-life-cycle-basics-gateway/">API Life Cycle Basics: Gateway</a></h3>
			<p><em>08 Jan 2018</em></p>
			<p>API gateways have long played a role in providing access to backend resources via web services and APIs. This is how web services have historically been deployed, but it is also how modern web APIs are being managed. Providing a gateway that you can stand up in front of existing web APIs, and proxy them through a single gateway that authenticates, logs, and manages the traffic that comes in and out. There are many management characteristics of API gateways, but I want to provide a stop along the API lifecycle that allows us to think about the API deployment, as well as the API management aspects of delivering APIs. I wanted to separate out the API gateway discussion from deploy and manage, focusing specifically on the opportunities to deploy one or many gateways, while also looking at it separately as a pattern in service of microservices. While code generation for API deployment is common, gateways are making a resurgence across the sector when it comes to working with a variety of backend systems, on-premise and in the cloud. There are many API gateway solutions available on the market, but I wanted to focus in on a handful that help span deployment and management, as well as allowing for new types of routing, and transformation patterns to emerge. Here are a couple of the gateway solutions I’m studying more these days: AWS API Gateway - The Amazon API Gateway allows for the ingestion of OpenAPIs (Swagger) and the deployment of APIs that connect to a variety of backend services define as part of the AWS infrastructure. Kong - Quickly build API-centric applications. Leverage the latest microservice and container design patterns. And tie it all together with the Kong microservice API gateway. Zuul - I’m putting Zuul here, because it has some routing characteristics with makes it a deployment, as well as management solution. One you begin routing, you start to do some of the heavy lifting...[<a href="/2018/01/08/api-life-cycle-basics-gateway/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/08/api-life-cycle-basics-deployment/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-api-deployment.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/08/api-life-cycle-basics-deployment/">API Life Cycle Basics: Deployment</a></h3>
			<p><em>08 Jan 2018</em></p>
			<p>There are many ways to deploy an API, making this another confusing stop along the API life cycle for some of my readers. My goal in having this be a separate stop from design, or possibly management, is to help API providers think about where and how they deploy APIs. From my perspective, API deployment might be about which framework and language you choose to deploy in, spanning all the way to where you might deploy it, either on-premise, on-device, or in the cloud. The how and why of deploying your API will play a significant role in determining how stable and consistent you are able to deploy API resources, impacting almost every other stop along the API life cycle. Many API providers still think of API deployment in the context of their internal operations, as opposed to thinking about how they will be put to use. The providers I’m seeing enjoy more flexibility and agility when it comes to API consumption are able to deploy APIs in a variety of languages, supporting a variety of existing platforms, and in any environment where they are needed. There are several concepts that are beginning to define API deployment in this new generation of compute in the cloud, here are just a handful of them. Polyglot Deployment - The ability to deploy APIs in a variety of programming languages. Multi-Platform - The ability to deploy APIs in a variety of platforms, and using existing system. Multi-Cloud - The ability to deploy APIs within Amazon, Azure, Heroku, and Google environments. Frameworks - Leverage a variety of open source API frameworks for deploying APIs. I normally would put API gateways here as well, but because of renewed energy around gateway solutions actually deploying APIs instead of just managing and securing them, I’m breaking out gateway into its own stop along the API lifecycle. Gateway spans API deployment and management in my opinion, and while it should be considered alongside...[<a href="/2018/01/08/api-life-cycle-basics-deployment/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/04/keeping-things-in-the-club-by-drowning-everyone-in-api-complexity/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/04/keeping-things-in-the-club-by-drowning-everyone-in-api-complexity/">Keeping Things In The Club By Drowning Everyone In API Complexity</a></h3>
			<p><em>04 Jan 2018</em></p>
			<p>After seven years of doing API Evangelist I have learned a lot about the realities of the technology sector, versus my own beliefs. One of the things that attracted me to web APIs in the first place was the ability to simplify the access to data, content, algorithms, and other resources using a web url. I could get a list of news articles, post a picture, launch a server in the cloud, and many other common business tasks using a simple URL. To me good API design is more about simplicity, than it was ever about REST, or any other dogmatic approach to doing APIs. However, after seven years of doing this, I’m pretty convinced that most folks have very little interest in truly making things simple for anyone. As API space continues to move forward with efforts to address technical debt, and the cultural issues involved with the technology we are using within large enterprises as a part of the microservices movement–we are simultaneously see other fronts where leading edge practitioners are embracing technical complexity in service of scope, volume, and satisfying the requests of developers down in the weeds, and not taking time to consider the big picture. You see this with trends like Kafka, GraphQL, ad other areas, where we are moving forward with technology that isn’t entirely embracing the web, and introducing some pretty complex approaches to getting the job done. I get it. The problems being solved are big. There is a lot of data. Complex delivery models. Robust, and highly functional applications. Simple web APIs can’t always deliver at the scope, scale, and satisfaction of the very technical folks involved. I’m not knocking things moving forward, but I am asking if everyone involved is thinking seriously about the big picture, and assessing the costs down the road–as well as those who get left behind. Not everyone will have the resources, knowledge, and ability to keep up, and I actually...[<a href="/2018/01/04/keeping-things-in-the-club-by-drowning-everyone-in-api-complexity/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/04/api-transit-basics-mocking/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-mock-interface.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/04/api-transit-basics-mocking/">API Transit Basics: Mocking</a></h3>
			<p><em>04 Jan 2018</em></p>
			<p>This is a series of stories I’m doing as part of my API Transit work, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling API transit–leveraging the verb form of transit, to describe what every API should go through. One key deficiency I see in organizations that I work with on a regular basis, is the absence of the ability to quickly deploy a mock version of an API. Meaning, the ability to deliver a virtualized instance of the surface area of an API, that will accept requests, and return responses, without writing or generating any existing backend code. Mocking APIs require an API definition, and with many groups still producing these definitions from code, the ability to mock an API is lost in the shuffle. Leaving out the ability to play with an API before it ever gets built–which if you think about it, goes against much of why we design APIs in the first place. Mocking of an API goes hand in hand with a design first approach. Being able to define, design, mock, and then receive feedback from potential consumers, then repeat until the desired API is delivered is significantly more efficient than writing code, deploying an API, and iterating on it over a longer time frame. Over the last couple of years, a growing number of services and tooling have emerged to help us mock our APIs, as well as the schema that are used as part of their requests and responses, giving birth to this entirely new stop along the API life cycle. Mockable - A simple service for mocking web and SOAP APIs Sandbox - A simple service for generating sandboxes using a variety of formats. Stoplight Prism - An open source tool for mocking and transforming...[<a href="/2018/01/04/api-transit-basics-mocking/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/04/api-discovery-is-mostly-about-you-sharing-stories-about-the-apis-you-use/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/man-on-moon-flag.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/04/api-discovery-is-mostly-about-you-sharing-stories-about-the-apis-you-use/">API Discovery Is Mostly About You Sharing Stories About The APIs You Use</a></h3>
			<p><em>04 Jan 2018</em></p>
			<p>I do a lot of thinking about API discovery, and how I can help people find the APIs they need. As part of this thinking I’m always curious why API discovery hasn’t evolved much in the last decade. You know, no Google for APIs. No magical AI, ML, AR, VR, or Blockchain for distributed API mining. As I’m thinking, I ask myself, “how is it that the API Evangelist finds most of his APIs?” Well, word of mouth. Storytelling. People talking about the APIs they are using to solve a real world business problem. That is it! API storytelling is API discovery. If people aren’t talking about your API, it is unlikely it will be found. Sure people still need to be able to Google for solutions, but really that is just Googling, not API discovery. It is likely they are just looking for a company that does what they need, and the API is a given. We really aren’t going to discover new APIs. I don’t know many people who spend time looking for new APIs (except me, and I have a problem). People are going to discover new APIs by hearing about what other people are using, through storytelling on the web and in person. In my experience as the API Evangelist I see three forms of this in action: 1) APIs talking about their API use cases on their blog 2) Companies telling stories about their infrastructure on their blog 3) Individuals telling stories about the APIs they use in job, side projects, and elsewhere. This represent the majority of ways in which I discover new APIs. Sure, as the API Evangelist I will discover new APIs occasionally by scouring Github, Googling, and harvesting social media, but I am an analyst. These three ways will be how the average person discovers new APIs. Which means, if you want your API to be discovered, you need to be telling stories about it. If...[<a href="/2018/01/04/api-discovery-is-mostly-about-you-sharing-stories-about-the-apis-you-use/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/03/the-transit-feed-api-is-a-nice-blueprint-for-your-home-grown-api-project/"><img src="https://s3.amazonaws.com/kinlane-productions2/transit-feeds-api/transit-feeds-api-home-page.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/03/the-transit-feed-api-is-a-nice-blueprint-for-your-home-grown-api-project/">The Transit Feed API Is A Nice Blueprint For Your Home Grown API Project</a></h3>
			<p><em>03 Jan 2018</em></p>
			<p>I look at a lot of APIs. When I land on the home page of an API portal, more often than not I am lost, confused, and unsure of what I need to do to get started. Us developers are very good at complexifying things, and making our APIs implementations as messy as our backends, and the API ideas in our heads. I suffer from this still, and I know what it takes to deliver a simple, useful API experience. It just takes time, resources, as well as knowledge to it properly, and simply. Oh, and caring. You have to care. I am always on the hunt for good examples of simple API implementations that people can emulate, that aren’t the API rockstars like Twilio and Stripe who have crazy amounts of resources at their disposal. One good example of a simple, useful, well presented API can be found with the Transit Feeds API, which aggregates the feeds of many different transit providers around the world. When I land on the home page of Transit Feeds, I immediately know what is going on, and I go from home page to making my first API call in under 60 seconds–pretty impressive stuff, for a home grown API project. While there are still some rough edges, Transit Feeds has all the hallmarks of a quality API implementation. Simple UI, with a clear message about what it does on the home, but most importantly an API that does one thing, and does it well–providing access to transit feeds. The site uses Github OAuth to allow me to instantly sign up and get my API key–which is how ALL APIs should work. You land on the portal, you immediately know what they do, and you have your keys in hand, making an API call, all without having to create yet another API developer account. The Transit Feed API provides an OpenAPI for their API, and uses it to drive...[<a href="/2018/01/03/the-transit-feed-api-is-a-nice-blueprint-for-your-home-grown-api-project/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/03/api-transit-basics-api-design/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-design.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/03/api-transit-basics-api-design/">API Transit Basics: API Design</a></h3>
			<p><em>03 Jan 2018</em></p>
			<p>This is a series of stories I’m doing as part of my API Transit work, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling API transit–leveraging the verb form of transit, to describe what every API should go through. API design is not just about REST. Sure, a great deal of the focus within this stop along the API journey will be focused on REST, but this is because it is the dominant methodology at this moment in time. API design is about establishing a framework for how you will consistently craft your APIs across teams, whether they are REST, GraphQL, Microservices, or even gRPC. Your API design strategy might be dominated by RESTful practices, especially early on in your journey, but API design should not be considered to be only REST methodologies. In the last five years API design has matured into its own discipline, focusing on a define and design first approach to developing APIs, shifting away from a code then document approach we’ve seen dominate for the last decade, and is still common place at many organizations. There are a handful of tooling, and websites that have emerged to help API providers, architects, developers, and designers get a handle on this stop along the API journey–here are just a few. Swagger Editor - Leverage the Swagger editor for manually working with OpenAPI definitions. Apicurio - A robust, and beautiful open source tool for designing APIs. API Design Guide - Continue to establish, evolve, and disseminate the organizational API design guide, providing guidance for all teams–make sure there is a feedback loop involved with its development. API Stylebook - Learning and extracting from other companies API design guides. Designing and Implementing Hypermedia APIs - A thoughtful post on how to design...[<a href="/2018/01/03/api-transit-basics-api-design/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/03/api-transit-basics-api-definitions/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-definition.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/03/api-transit-basics-api-definitions/">API Transit Basics: API Definitions</a></h3>
			<p><em>03 Jan 2018</em></p>
			<p>This is a series of stories I’m doing as part of my API Transit work, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling API transit–leveraging the verb form of transit, to describe what every API should go through. Defining an API is the first stop along any API journey. When I say definitions, I’m not just talking about OpenAPI (fka Swagger), and specifically definitions for the surface area of your API. I’m talking about defining your idea, your goals, and the standard aspects of doing business with APIs. By API definitions, I mean having a robust toolbox of definitions for everything that is going into your API operations, from standardized dates and currencies, to common data schema, and yes to making sure there is an active OpenAPI definition for every single one of your APIs. I’d say that 75% of the companies, organizations, institutions, and government agencies I’m talking with about APIs begin API development by coding. A very costly, and rigid approach to defining a solution to a problem. Many of the groups I know who are using OpenAPI in their operations still rely on it being generated from systems and code, and do not actually hand-define, or hand-craft the definitions for their APIs, which should be being applied across API operations, not just for delivering documentation. When it comes to establishing a robust API definition strategy for operations, I recommend starting with a handful of tools and concepts. OpenAPI - Ensuring there are OpenAPI definitions for ALL APIs / microservices. JSON Schema - Ensuring there are robust JSON schema for all data in use. Postman Collections - Postman’s proprietary format for defining APIs, which can be translated to and from OpenAPI. API Transformer - Opening up the ability to transform...[<a href="/2018/01/03/api-transit-basics-api-definitions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/03/alexa-voice-skills-are-the-poster-child-for-your-enterprise-api-efforts/"><img src="https://s3.amazonaws.com/kinlane-productions2/alexa/alexa-skills.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/03/alexa-voice-skills-are-the-poster-child-for-your-enterprise-api-efforts/">Alexa Voice Skills Are The Poster Child For Your Enterprise API Efforts</a></h3>
			<p><em>03 Jan 2018</em></p>
			<p>I was sitting in an IT architectural planning meeting for a large enterprise organization the other day, and one of the presentation from one of the executives contained a complex diagram of their IT infrastructure, with a column to the right showing a simple five step Alexa conversation, asking a specific question from customer. Each question posed as part of the Alexa conversation theoretically accessed a different system, weaving a pretty complex web of IT connections, to enable this simple conversation. This presentation reflects why I feel that Alexa Skills development poses some interesting questions in the API world, and why the platform becomes interesting to so many business users. It reflects the end goal of why we are doing all of this (in theory), but then quickly illustrates how complicated we’ve actually made all of this, demonstrating how challenging delivering conversational interfaces will be in reality. There are many conversational challenges in enabling our system to be able to talk with humans, but I think many of the most daunting challenges companies will face in coming years will be to actually get at the right data to provide a relevant answer to questions poised in voice, bot, and other conversationally-enabled solutions. Being able to quickly respond to information requests is why many companies, organizations, institutions, and government agencies are doing APIs. Being able to respond to them in real time conversations is definitely a question of doing APIs, but I’m finding in most organizations it is more about solving human and political questions, than it is just a technical one. Sure, you can envision the most beautiful stack of microservices reaching into every aspect of your organization(s), and even develop a robust conversational layer for answering questions posed across that stack, but delivering it all consistently, at scale, across multiple teams of human beings will never be easy, or quick. I think that conversational interfaces provide an excellent exercise for companies, to help them...[<a href="/2018/01/03/alexa-voice-skills-are-the-poster-child-for-your-enterprise-api-efforts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/02/treating-all-apis-like-they-are-public/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/45_78_800_500_0_max_0_1_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/02/treating-all-apis-like-they-are-public/">Treating All APIs Like They Are Public</a></h3>
			<p><em>02 Jan 2018</em></p>
			<p>I was talking with the Internal Revenue Service (IRS) about their internal API strategy the week before Christmas, sharing my thoughts on the strategy that they were pitching internally when it comes to the next phase of their API journey. One topic that kept coming up is the firm line of separate between public and private APIs, which you kind of get at an organization like the IRS. It isn’t really the type of organization you want to be vague about this line, making sure everyone understands where an API should be consumed, and where it should not be consumed. Even with that reality, I still made the suggestion that they should be treating ALL APIs like they are public. I clarified by saying you shouldn’t be getting rid of the hard line dictating whether or not an API is internal or external, but if you treat them all like they are public, and act like they are all under threat, you will be better off for it. This peaked their interest, was something they did not expect to hear from me, and was something they would be adding to their recommendations for the next version of their API strategy. The first benefit of treating your internal APIs like they are public is when it come to security, logging, and overall API management. You have the tools in place to catch any threats, and develop awareness regarding how an API is being used, both good and bad. While the threats might be minimized internally, developing the same awareness, and having the tools to identify who is using what, and respond accordingly will benefit operations. API security isn’t just about firewalls, it is about an awareness of who is using what. The next benefit is about the future of your APIs. If you treat APIs like they are public, and you ever want to make it public, you will be in much better shape. You will...[<a href="/2018/01/02/treating-all-apis-like-they-are-public/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/02/the-national-transit-database-ntd-needs-to-be-an-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/government/federal-transit-agency/the-national-transit-database.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/02/the-national-transit-database-ntd-needs-to-be-an-api/">The National Transit Database (NTD) Needs To Be An API</a></h3>
			<p><em>02 Jan 2018</em></p>
			<p>I’ve been looking for sources of transit data as part of some research I’m doing with Streamdata.io. Like most industries I study as part of my API research, it is a mess. There is no single source of truth, lack of robust open source solutions, government PDFs acting as databases, and tech companies extracting as much value as they can, and giving as little in return as they possibly can. Todays frustration centers around the unfortunately common federal government PDF database, or more specifically, the National Transit Database (NTD). In 2017, when you publish something to the web as a “database”, it should be machine readable. There is some valuable data in the agency profile reports for the 800+ transit agencies available in the database, but this information is locked up in PDFs. You can find machine readable, historic versions of this data up to 2015 in data.gov, but for 2016, and 2017, the data is only available in individual PDFs for each agency profile. To make things more difficult, the listing of transit agencies uses some Ajax voodoo for its pagination and detail pages, making it even harder to scrape, on top of rendering each agencies detail useless by storing it as a PDF. I understand why government is stuck in this mode. The systems they use only provide them with PDF as a their primary output. Staff hasn’t been trained on the importance of making data available in machine readable formats. People just don’t understand the negative impact they are making on the life of their data, and how it restricts people putting it to work. In some cases, people are fully aware of this, and want to limit how the data gets used, interpreted, keeping them as the definitive source of truth. I’m not saying this is what the Federal Transit Agency (FTA) is up to, but I’m saying it is the effect of their actions, which is having a chilling effect...[<a href="/2018/01/02/the-national-transit-database-ntd-needs-to-be-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/02/my-evolving-definition-of-a-robust-and-diverse-api-toolbox/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-toolbox/API+Toolbox.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/02/my-evolving-definition-of-a-robust-and-diverse-api-toolbox/">My Evolving Definition Of A Robust And Diverse API Toolbox</a></h3>
			<p><em>02 Jan 2018</em></p>
			<p>It is always telling when folks assume I mean REST when I say API. While the web dominates my definition of API, and REST is definitely a leading architectural style, these assumptions always define the people who bring them to the table, more than they ever do me. I’m in the business of studying how people are applying programmatic interfaces using the web. To reflect my research I’ve been evolving a diagram of my toolbox that I’ve been publishing as part of workshops, presentations, and some talks I’m preparing for 2018. It reflects what I’m seeing as the evolving API toolbox that I’m seeing companies working with, and a diversity in which I’m encouraging others to think about more, as we choose to ignore the polarizing forces in the API sector. To set the tone for any API conversation I am participating in, I prefer to introduce the concept of the API toolbox including more tools than just REST, acknowledging that there are a growing number of tools in our API infrastructure toolbox which can be applied to different APIs, to solve a variety of problems and challenges we face. Also we need to be more honest about the fact that there are many legacy solutions still in use across large organizations, even as we consider adopting the latest in leading edge approaches to API deployment in newer projects. HTTP - Leverage the web, and the HTTP standard across ALL API efforts. SOAP - Acknowledging there are still a number of SOAP services in use. RPC - Understand how and why RPC APIs still might be viable in production. REST - Making REST, and a resource-centered approach the focus of the operations. Microservices - Emphasis on independently deployable and module API services. Verbs - Knowing, and putting to use HTTP verbs across API implementations. Content-Type - Understanding the negotiation between XML, JSON, and other types. Hypermedia - Considering how hypermedia design, and content types play...[<a href="/2018/01/02/my-evolving-definition-of-a-robust-and-diverse-api-toolbox/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/02/api-transit-the-basics/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-transit/api-transt-subway.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/02/api-transit-the-basics/">API Transit - The Basics</a></h3>
			<p><em>02 Jan 2018</em></p>
			<p>I have been evolving my approach to mapping out all the stops along my API research, using a subway map approach lately. It has been something I’ve been working on since 2014, and had developed as a keynote talk in 2015. My goal is to be able to lay out simple, as well as increasingly complex aspects of consistently operating an API. Something I’ve historically called the API life cycle, but will work to call API transit in the future. Right now, I have two main approaches to delivering the API Transit maps. 1) API Life Cycle, and 2) API Documentation. The first is about applying consistent practices to API operations, and the second is about understanding API operations as they happen. In my mind, both these types of API Transit maps will eventually work in sync, but I have to work my way up to that. Right now, I’m focusing on the API Life Cycle version, which is becoming more about API governance, but I’m going to try and rebrand as API Transit. I’m using transit as a verb, “pass across or through” a standard, and consistent way of doing APIs. What some might consider API design, or governance, but I’m considering more holistically. To support a couple of my consulting projects I am working on at the moment, I have published a simple API Transit project to help navigate some API teams through what I’d consider to be the basics they should be considering as they look to standardize how they deliver APIs across teams. It’s a basic single line, 19 stop API Transit map. It is something I will keep adding stops to, and expand many into their own lines, serving up much more detail, but for this first project I wanted to keep simple, and speaking to a specific enterprise audience. I don’t want to overwhelm them with information as they are just getting started on their API journey. They still...[<a href="/2018/01/02/api-transit-the-basics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/20/understanding-serversent-events-sse-as-part-of-the-api-landscape/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/server-racks-clouds_clean_view.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/20/understanding-serversent-events-sse-as-part-of-the-api-landscape/">Understanding Server-Sent Events (SSE) As Part Of The API Landscape</a></h3>
			<p><em>20 Dec 2017</em></p>
			<p>I’m continuing to break down the technology stack as I get to know my new partner Streamdata.io. Yesterday I wrote about their use of JSON Patch for returning partial responses of changes made to an API that has been proxied through the service, and today I want to focus on understanding Server-Sent Events (SSE), which Streamdata.io uses to stream those events in real time to any consumer. In my experience, SSE is a lesser known of the real time technologies out there, but is one that holds a lot of potential, so I wanted to spend some time covering it here on the blog. As opposed to technology that delivers a two-way stream, Server-sent events (SSE) is all about a client receiving automatic updates from a server via HTTP connection. The technology is a standard, with the Server-sent events (SSE) EventSource API being standardized as part of the HTML5 specification out of the W3C. Similar to Streamdata.io’s usage of JSON Patch, SSE is all about efficiency. Making web APIs real time isn’t always about having a two-way connection, and SEE is a great way to make things streaming in a one-way direction, only sending you what has changed in real-time using JSON Patch. Efficiency in direction, delivery, and in message. Server-sent events (SSE) definitely shines when you look at how it can be used to constantly push and refresh data in any web UI using JavaScript. It’s HTML5 roots makes it a first-class citizen in the browser, but I also think there are a huge number of scenarios to play with when it comes to system integration, and reducing polling on APIs. I think the news, currency, stock, and other financial data scenarios are the low hanging fruit, but I feel like Streamdata.io as a rapid deploy proxy that developers can throw in between any API and a system integration is where the killer use cases of Server-sent events (SSE) could be. To help me...[<a href="/2017/12/20/understanding-serversent-events-sse-as-part-of-the-api-landscape/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/20/i-am-now-realizing-that-streamdata-io-is-not-just-for-api-providers/"><img src="https://s3.amazonaws.com/kinlane-productions2/streamdata/streamdata-push.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/20/i-am-now-realizing-that-streamdata-io-is-not-just-for-api-providers/">I Am Now Realizing That Streamdata.io Is Not Just For API Providers</a></h3>
			<p><em>20 Dec 2017</em></p>
			<p>When I first started diving into what Streamdata.io does, and thinking of their role in the wider API landscape, I was pretty exclusively focused API providers. Meaning, if you are an API provider, depending on the resources you are serving up, you should consider augmenting it with a real time stream using Streamdata.io. This still holds true, but after using Streamdata.io more as a developer, it is becoming clear of Streamdata.io’s value in my toolbox as an API consumer, and thinking about how I can make my applications more efficient, real time, and event-driven. Right now, I’m just taking a wide variety of existing web APIs and running through the Streamdata.io proxy, and seeing what comes out the other end. I’m in the phase where I’m just understanding what Server-Sent Events (SSE) combined with JSON Patch does to existing web APIs, and their resources. This process is helping me understand the possibilities with streaming existing web APIs, but as I fire up each API I’m seeing it also reveal a new layer of events that exist in between providing APIs, and consuming APIs. I feel like this layer isn’t always evident to API providers, who haven’t made it very far in their API journey. While I study how the bleeding, and leading edge developers are deploying event-driven architecture, mining for the event value that exists within big data, I’m thinking there is also a pretty interesting opportunity in mining the event layer for existing web APIs. Once I turn on streaming for a web API, the immediate value you see is when a new resource is added. However, this really isn’t that amazing beyond just subscribing to a webhook, or polling an API. I feel like the valuable events we don’t fully see without Server-Sent Events (SSE) is the changes. When a price changes. When a link is modified. When content is refreshed. The subtle events that occur that might not be noticed in regular...[<a href="/2017/12/20/i-am-now-realizing-that-streamdata-io-is-not-just-for-api-providers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/20/basic-api-design-guidelines-are-your-first-step-towards-api-governance/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/54_32_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/20/basic-api-design-guidelines-are-your-first-step-towards-api-governance/">Basic API Design Guidelines Are Your First Step Towards API Governance</a></h3>
			<p><em>20 Dec 2017</em></p>
			<p>I am working with a group that has begun defining their API governance strategy. We’ve discussed a full spectrum of API lifecycle capabilities that need to be integrated into their development practices, and CI/CD workflow, as well as eventually their API governance documentation. However, they are just getting going with the concept of API governance, and I want to make sure they don’t get ahead of themselves and start piling in too much into their API governance documentation, before they can get buy in, and participation from other groups. We are approaching the first draft of an API governance document for the organization, and while it has lofty aspirations, the first draft is really nothing more than some basic API design guidelines. It is basically a two-page document that explains why REST is good, provides guidance on naming paths, using your verbs, and a handful of other API design practices. While I have a much longer list of items I want to see added to the document, I feel it is much more important to get the basic first draft up, circulated amongst groups, and establishing feedback loops, than making sure the API governance document is comprehensive. Without buy-in from all groups, any API governance strategy will be ignored, and ultimately suffocated by teams who feel like they don’t have any ownership in the process. I am lobbying that the API governance strategy be versioned and evolved much like any other artifact, code, or documentation applied across API operations. This is v1 of the API governance, and before we can iterate towards v2, we need to get feedback, accept issues, comments, and allow for pull requests on the strategy before it moves forward. It is critical that ALL teams feel like they have been part of the conversation from day one, otherwise it can be weakened as a strategy, and any team looking to implement, coach, advise, report on, and enforce will be hobbled. API...[<a href="/2017/12/20/basic-api-design-guidelines-are-your-first-step-towards-api-governance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/20/api-discovery-will-be-about-finding-companies-who-do-what-you-need-and-api-is/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/27_127_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/20/api-discovery-will-be-about-finding-companies-who-do-what-you-need-and-api-is/">API Discovery Will Be About Finding Companies Who Do What You Need And API Is</a></h3>
			<p><em>20 Dec 2017</em></p>
			<p>While I’m still investing in defining the API discovery space, and I’m seeing some improvements from other API service and tooling providers when it comes to finding, sharing, indexing, and publishing API definitions, I honestly don’t think in the end API discovery will ever be a top-level concern. While API design, deployment, management, and even testing and monitoring have floated to the top as primary discussion areas for API providers, and consumers, the area of API discovery never has quite become a priority. There is always lots of talk about API discovery, mostly about what is broken, rarely about what is needed to fix, with regular waves of directories, marketplaces, and search solutions emerging to attempting to fix the problem, but always falling short. As I watch more mainstream businesses on-board with the world of APIs, and banks, healthcare, insurance, automobile, and other staple industries work to find their way forward, I’m thinking that the mainstreamification of APIs will surpass API discovery. Meaning that people will be looking for companies who do the thing that they want, and that API is just assumed. Every business will need to have an API, just like every business is assumed to have an website. Sure there will be search engines, directories, and marketplaces to help us find what we are looking for, but when we just won’t always be looking for APIs, we will be looking for solutions. The presence of an API be will be assumed, and if it doesn’t exist we will move on looking for other companies, organizations, institutions, and agencies who do what we need. I feel like this is one of the reasons API discovery really became a thing. It doesn’t need to be. If you are selling products and services online you need a website, and as the web has matured, you need the same data, content, media, and algorithms available in a machine readable format so they can be distributed to...[<a href="/2017/12/20/api-discovery-will-be-about-finding-companies-who-do-what-you-need-and-api-is/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/19/robust-public-storytelling-around-your-api-process-is-sign-of-maturity/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/68_113_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/19/robust-public-storytelling-around-your-api-process-is-sign-of-maturity/">Robust Public Storytelling Around Your API Process Is Sign Of Maturity</a></h3>
			<p><em>19 Dec 2017</em></p>
			<p>Sharing stories around your API is something you hear me talk about a lot. Many of my readers like to let me know how they are serious API people, and my storytelling emphasis is silly. Just do APIs. Storytelling is unnecessary fluff. When in reality, storytelling has real, direct benefits on your business bottom line, but also have many other indirect aspects, and its presence is a sign of the overall health of an organization from my vantage point. When you are actively telling stories about your operations, in my experience, it is a sign of the overall maturity of your API process. I’m working through my storytelling around what Capital One is up to with their DevExchange, studying their approach to API governance, as well as the wider role they are playing in the banking, and even API regulation game here in the United States. I can find stories about each of the topics I’m looking for on their public blog(s), and out in the open. This type of storytelling isn’t accidental, it is an intentional part of a maturing internal and external API strategy. Sure, they have a lot of work ahead of them, but based upon my internal conversations with them, and their external storytelling, I’m aware of how far along they are in their API journey–compared to other banks I’m talking to. Take a look at Capital One’s storytelling on Medium. Tune into the storytelling within the DevExchange community. This isn’t Capital One just being confident in what they do, and are able to tell their story publicly. This is part of what you do to work through your API processes and break down the monolith. Check out the Breaking Down the Monolith guide from my friend Irakli Nadareishvili over at DZone. When you know your stuff, you are able tell the story of how you are unwinding the enterprise mess publicly like this. You aren’t embarrassed to tell these stories...[<a href="/2017/12/19/robust-public-storytelling-around-your-api-process-is-sign-of-maturity/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/19/no-more-scraping-of-banking-data-in-europe-according-to-psd2-only-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/psd2/european-commission-press-release-psd2-scraping.png"" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/19/no-more-scraping-of-banking-data-in-europe-according-to-psd2-only-apis/">No More Scraping Of Banking Data In Europe According to PSD2, Only APIs</a></h3>
			<p><em>19 Dec 2017</em></p>
			<p>Part of my partnership with http://streamdata.io centers around me investing more time into studying the banking industry, starting with the rollout of PSD2 in Europe next month. I’ll be working through each aspect of the regulations for the banking industry when it comes to APIs, but I wanted to highlight a recent change regarding scraping that is pretty monumental. In a recent press release from the European Commission they further clarified guidance for third party payment services providers (TPPs), and whether or not they can be scraping data from bank still, instead of using the APIs being mandated by the commission. Here is the section from the press release specifically addressing “what data can TPPs access and use via screen scraping”: PSD2 prohibits TPPs from accessing any other data from the customer payment account beyond those explicitly authorised by the customer. Customers will have to agree on the access, use and processing of these data. With these new rules, it will no longer be allowed to access the customer’s data through the use of the techniques of “screen scraping”. Screen scraping means accessing the data through the customer interface with the use of the customer’s security credentials. Through screen scraping, TPPs can access customer data without any further identification vis-à-vis the banks. Banks will have to put in place a communication channel that allows TPPs to access the data that they need in accordance with PSD2. The channel will also be used to enable banks and TPPs to identify each other when accessing these data. It will also allow them to communicate through secure messaging at all times. Banks may establish this communication channel by adapting their customer online banking interface. They may also create a new dedicated interface that will include all necessary information for the relevant payment service providers. The RTS specifies the contingency safeguards that banks shall put in place if they decide to develop a dedicated interface. This will ensure fair...[<a href="/2017/12/19/no-more-scraping-of-banking-data-in-europe-according-to-psd2-only-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/19/javascript-object-notation-json-patch/"><img src="https://s3.amazonaws.com/kinlane-productions2/rfc/6902/javascript-object-notation-json-patch.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/19/javascript-object-notation-json-patch/">JavaScript Object Notation (JSON) Patch</a></h3>
			<p><em>19 Dec 2017</em></p>
			<p>I’m continuing my studying into what my new partner in crime Streamdata.io does, and part of this research is understanding the details of their technology stack. Today’s work involves understanding their usage of JavaScript Object Notation (JSON) Patch. When you proxy any existing web API using Streamdata.io, the first thing you get back is a complete JSON representation of the response, but then with each change you just get back a JSON Patch response with only the details of what has changed. JSON Patch is used for expressing a sequence of operations to apply to a any JSON object or document and you’ll find used with the HTTP PATCH method. The introduction for JSON Patch from RFC [RFC4627] describes it this way: JavaScript Object Notation (JSON) [RFC4627] is a common format for the exchange and storage of structured data. HTTP PATCH [RFC5789] extends the Hypertext Transfer Protocol (HTTP) [RFC2616] with a method to perform partial modifications to resources. JSON Patch is a format (identified by the media type “application/json-patch+json”) for expressing a sequence of operations to apply to a target JSON document; it is suitable for use with the HTTP PATCH method. This format is also potentially useful in other cases in which it is necessary to make partial updates to a JSON document or to a data structure that has similar constraints (i.e., they can be serialized as an object or an array using the JSON grammar). JSON Patch is an efficient way to only get the details from an API regarding only what has changed, instead of sending everything over the pipes each time. It makes sense that Streamdata.io has used it in conjunction with Server-Sent Events (SSE) to efficiently cache and stream data from existing web APIs. I have to admit I never put this together with the PATCH method for API responses. Most of the APIs I’ve seen that use PATCH, do not actually implement JSON PATCH, so this was a...[<a href="/2017/12/19/javascript-object-notation-json-patch/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/19/from-cicd-to-a-continuous-everything-ce-workflow/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/27_93_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/19/from-cicd-to-a-continuous-everything-ce-workflow/">From CI/CD To A Continuous Everything (CE) Workflow</a></h3>
			<p><em>19 Dec 2017</em></p>
			<p>I am evaluating an existing continuous integration and deployment workflow to make recommendations regarding how they can evolve to service their growing API lifecycle. This is an area of my research that spans multiple areas of my work, but I tend to house under what I call API orchestration. I try to always step back and look at an evolving area of the tech space as part of the big picture, and attempt to look beyond any individual company, or even the wider industry hype in place that is moving something forward. I see the clear technical benefits of CI/CD, and I see the business benefits of it as well, but I haven’t always been convinced of it as a standalone thing, and have spent the last couple of years trying understand how it fits into the bigger picture. As I’ve been consulting with several enterprise groups working to adopt a CI/CD mindset, and having similar conversations with government agencies, I’m beginning to see the bigger picture of “continuous”, and starting to decouple it from just deployment and even integration. The first thing that is assumed, not always evident for newbies, but is always a default–is testing. You alway test before you integrate or deploy, right? As I watch groups adopt I’m seeing them struggle with making sure there are other things I feel are an obvious part of the API lifecycle, but aren’t default in a CI/CD mindset, but quickly are being plugged in–things like security, licensing, documentation, discovery, support, communications, etc. In the end, I think us technologists are good at focusing on the tech innovations, but often move right past many of the other things that are essential for the business world. I see this happening with containers, microservices, Kubernetes, Kafka, and other fast moving trends. I guess the point I want to make is that there is more to a pipeline than just deployment, integration, and testing. We need to make...[<a href="/2017/12/19/from-cicd-to-a-continuous-everything-ce-workflow/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/18/what-you-can-expect-as-a-client-from-soap-to-grpc/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/57_64_800_500_0_max_0_-5_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/18/what-you-can-expect-as-a-client-from-soap-to-grpc/">What You Can Expect As A Client From SOAP To gRPC</a></h3>
			<p><em>18 Dec 2017</em></p>
			<p>
I’m working hard on what I consider to be my definition of a robust API deployment toolbox, and was enjoying the 100K perspective. As I explore, I wanted to share some of my thoughts about by you might expect to receive as a client in each of these scenarios.


  SOAP: You get what the vendor says we can send to you in very structured way.
  REST: Is this what you want? Let us know if it wasn’t via StackOverflow.
  Hypermedia: We are prepared to send you whatever we want at any point in the future.
  Microservices: You are just going get a little bit of this one thing.
  GraphQL: You get exactly what you want, you better know what to ask for!
  Websockets: Here you get that, and this, and that, and that…
  PubSub: You get only the topic you wish to subscribe to.
  Webhooks: Here you asked us to send this to you–here you go.
  Event Architecture: You get something whenever that something happens.
  gRPC: You get what we want really fast, and can accept what we want really fast!!


It is fun to step back and think about the motivations, ideology, and pros/cons of each of these API deployment scenarios. I’d love to hear your additions or perspective on what you think the client view of the conversation might be. As I see the API universe continue to expand, I’m curious to see how others are seeing it.

In coming months you’ll hear me write more about event driven architecture, gRPC, and how the pace of things are picking up when it comes to API consumption. I’m working with Streamdata.io to help try and map out this landscape, as well as some of the usual areas I focus on as the API Evangelist.

[<a href="/2017/12/18/what-you-can-expect-as-a-client-from-soap-to-grpc/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/18/streaming-data-from-the-google-sheet-json-api-and-streamdata-io/"><img src="https://s3.amazonaws.com/kinlane-productions2/streamdata/streamdata-google-sheet.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/18/streaming-data-from-the-google-sheet-json-api-and-streamdata-io/">Streaming Data From The Google Sheet JSON API And Streamdata.io</a></h3>
			<p><em>18 Dec 2017</em></p>
			<p>I am playing with Streamdata.io as I learn how to use my new partner’s service. Streamdata.io proxies any API, and uses Server-Sent Event (SSE) to push updates using JSON Patch. I am playing with making a variety of APIs real time using their service, and in my style, I wanted to share the story of what I’m working on, here on the blog. I was making updates to some data in a Google Sheet that I use to drive some data across a couple of my websites, and thought…can I make this spreadsheet streaming using Streamdata.io? Yes. Yes, I can. To test out my theory I went and created a basic Google Sheet with two columns, one for product name, and one for price. Simulating a potential product pricing list that maybe I’d want to stream across multiple website, or possibly within client and partner portals. Then I published the Google Sheet to the web, making the data publicly available, so I didn’t have to deal with any sort of authentication–something you will only want to do with publicly available data. I’ll play around with an authenticated edition at some point in the future, showing more secure examples. Once I made the sheet public I grabbed the unique key for the sheet, which you can find in the URL, and placed into this URL: https://spreadsheets.google.com/feeds/list/[sheet key]/od6/public/basic?alt=json. The Google Sheet key takes a little bit to identify in the URL, but it is the long GUID in the URL, which is the longest part of the URL when editing the sheet. Once you put the key in the URL, you can take the URL and paste in the browser–giving you a JSON representation of your sheet, instead of HTML, basically giving you a public API for your Google Sheet. The JSON for Google Sheets is a little verbose and complicated, but once you study a bit it doesn’t take long for it to come into focus,...[<a href="/2017/12/18/streaming-data-from-the-google-sheet-json-api-and-streamdata-io/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/18/reducing-polling-of-your-existing-api-using-streamdata-io/"><img src="https://s3.amazonaws.com/kinlane-productions2/streamdata/cloud-architecture-1024x519.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/18/reducing-polling-of-your-existing-api-using-streamdata-io/">Reducing Polling Of Your Existing API Using Streamdata.io</a></h3>
			<p><em>18 Dec 2017</em></p>
			<p>I’ve partnered with Streamdata.io, resulting in me getting more acquainted with their API solutions, and telling the story of that process here on API Evangelist. I figured I would dive right in and start with the basics of what Streamdata.io does–turning your existing web API into a real-time stream. Streamdata.io acts as a reverse proxy that translates REST API polling into a stream of data. Instead of constantly polling your API for changes, your API clients will poll Streamdata.io and get a JSON Patch update if anything has changed, and reducing the impact of the requests your clients will make to your API. When thinking about what Streamdata.io does it is easy to get caught up on the real time and streaming nature of what they do, but the most immediate value they bring to the table is about making your relationship with your API clients more efficient. Streamdata.io reduces the costs associated with operating your API, stepping in between you and your demanding clients, and act as a buffer that will reduce the load on your servers. Eliminating one of the biggest headaches for API providers, and reigning in the behavior by our most demanding, and demanding clients. I’m always surprised by the answers I get from API providers when I ask them why they rate limit their APIs. I’d say that 80% of the time it is based upon reducing the overhead and impact on backend systems, and dealing with the bad behavior of API consumers. Streamdata.io provides a pretty compelling solution to help alleviate this reality of operating APIs for most API providers. It isn’t just about making things real-time, it is more about cost savings, and minimizing the impact of API consumption on our back-end solutions. Making rate limiting irrelevant, unless you have some other specific business needs behind your decision. There are numerous other benefits Streamdata.io brings to the table, but reducing the load on your APIs probably the most...[<a href="/2017/12/18/reducing-polling-of-your-existing-api-using-streamdata-io/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/18/definitiondriven-api-lifecycle-instead-of-codedriven-api-deployment/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/beach-rocks-currents_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/18/definitiondriven-api-lifecycle-instead-of-codedriven-api-deployment/">Definition-Driven API Lifecycle Instead Of Code-Driven API Deployment</a></h3>
			<p><em>18 Dec 2017</em></p>
			<p>You hear a lot about being API design first out of the API echo chamber these days. I’m finding that concept to be challenging for many groups I’m working with due to some of uninformed perceptions around REST, leaving many unable to move towards a design first approach because they are worried if they are doing it correctly. I shifted my own thinking a while back to be more about define-first, requiring that I thoroughly define each API project before I begin moving it along whatever API lifecycle I’ve quantified for a project. One thing I’m finding pretty common across the enterprise groups who have adopted OpenAPI (fka Swagger) as part of their operations is that many aren’t truly using the API specification format to its full potential as an API definition, let alone applying across multiple stops along the API lifecycle. Over and over I see groups “using Swagger”, but when you dig deeper you see the documentation being autogenerated as part of existing development, out of .JAR files, and (thankfully) evolving continuous deployment workflows. While this is progress, it’s not the definition-driven API lifecycle that organizations should be investing in, is is just code-driven APIs—not actually using OpenAPI to its full potential as a driver across all stops along the API lifecycle. Getting your hands dirty in the defining, designing, and crafting of OpenAPI definitions is where rubber really begins to meet the road in implementing the API specification. Sure, you can still be autogenerating the specifications from your services and tooling, but you should be actively polishing, and rounding off the rough edges in an API design tool, and putting your definitions to work in an API client like Restlet or Postman to truly define what your API does, or what it doesn’t do. Then taking your definitions and generating server side code, importing into your API gateway, building SDKs, and publishing your documentation and tests. If you skip over getting your...[<a href="/2017/12/18/definitiondriven-api-lifecycle-instead-of-codedriven-api-deployment/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/11/warming-up-api-providers-we-are-targeting-for-using-streamdata-io-with/"><img src="https://s3.amazonaws.com/kinlane-productions2/streamdata/damian-stream-data.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/11/warming-up-api-providers-we-are-targeting-for-using-streamdata-io-with/">Warming Up API Providers We Are Targeting For Using Streamdata.io With</a></h3>
			<p><em>11 Dec 2017</em></p>
			<p>My new partner in crime Damian Odoemena the technical account manager for Streamdata.io has said he is ready to work with me to deliver on the road map for the real-time streaming API. I explained to him that I will help fill his head with my knowledge of the API space, as well as be completely transparent around our strategy through storytelling here on API Evangelist. This is one of the reasons I jumped at the opportunity to partner with the team, because of their willingness to let me share what I do with the team, but also tell the story in real-time, streaming (pun intended) our day to day activities here on API Evangelist. I’ll be working 50% of my time on website copy, white papers, as well as evangelism and platform strategy for them, but the other 50% of the time I will be telling the story of what I did for them, here on the blog for me readers to learn from. This is essentially what I’ve been doing for API Evangelist for the last seven years, except I’ve crafted my storytelling based upon rolling waves of many partners, as well as ongoing private conversations I’ve had with API providers, and service providers across the space. I’m going to keep this up, but now it will be centered around the work I’m doing for Streamdata.io, as well as talking to their customers to better understand what they need when it comes to delivering regular web APIs, as well as streaming, real-time APIs. I know that Damian nodded his head when I explained this, but I’m guessing he doesn’t fully get how he is going to be thrust into the center of my storytelling, as we work together to develop Streamdata.io’s evangelism strategy, build prototypes, and the other fun things we have planned–he will soon enough! ;-) I’m working right now with Damian to create a list of API providers who would immediately...[<a href="/2017/12/11/warming-up-api-providers-we-are-targeting-for-using-streamdata-io-with/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/11/cost-saving-analysis-for-washington-metropolitan-area-transit-authority/"><img src="https://s3.amazonaws.com/kinlane-productions2/transit/washington-metropolitan-area-transit-authority-api.pn" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/11/cost-saving-analysis-for-washington-metropolitan-area-transit-authority/">Cost Saving Analysis For Washington Metropolitan Area Transit Authority</a></h3>
			<p><em>11 Dec 2017</em></p>
			<p>
Even before I engaged with Streamdata.io on our current partnership, I was working with them to quantity the value they bring to the table with their service. As I was working on my story regarding the roubling terms of service changes From Washington Metropolitan Area Transit Authority (WMATA) data APIs, the Streamdata.io team was running a cost savings analysis on the WMATA APIs. This is where they take their web API, and see what they could save if they used Streamdata.io, and turned it into a streaming API.

The Streamdata.io team took the WMATA Real-Time Bus PredictionsAPI, and assessed the efficiency gains for WMATA when it comes to their most demanding API consumers. Here are the bandwidth and CPU savings:


  Client Bandwidth (BW) Savings - 88%
  Server Bandwidth (BW) Savings - 99%
  Server CPU Savings - 87%


Streamdata.io does this by being stood up in front of the WMATA web API and caching the results, then only showing changes to clients–in real-time. This isn’t just about making something real-time, it is about reducing the number of times API consumers need to be polling an API. When it comes to transit data you can imagine that the client is probably polling every second to see what has changed.

I’m learning about Streamdata.io’s process for calculating these savings, which is why I’m writing this story. I’m going to work to help apply this to many other APIs, as well as look at productizing the tool so that maybe it can become a self-service tool for other API providers to evaluate their own cost savings, if they went to a real-time way of doing things. To help me understand the savings beyond WMATA, I’m going to be doing benchmarks across all the other US transit provides, and see what kind of numbers I can generate.

Disclosure: Streamdata.io is a paid API Evangelists partner.

[<a href="/2017/12/11/cost-saving-analysis-for-washington-metropolitan-area-transit-authority/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/11/api-evangelist-and-streamdata-io/"><img src="https://s3.amazonaws.com/kinlane-productions2/streamdata/083a1a91-3495-4fc8-ae2e-b5b6819548c6-original.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/11/api-evangelist-and-streamdata-io/">API Evangelist And Streamdata.io</a></h3>
			<p><em>11 Dec 2017</em></p>
			<p>Some of you in my backchannels know that I’ve been shopping around for a job lately. I’m looking to make a shift in API Evangelist, as I’ve written about some (and will write about more), and I’m also looking for a shift in how I fund my world. During a multi-week search I opened up conversations about a couple of different roles, and one particularly interesting partnership came my way from a company I’ve been working with for a while now. I’ve entered into a partnership with the Streamdata.io team, to help them chart the course for their real-time, streaming, event-sourced future, and they’ll continue to invest in me being the API Evangelist. In the past I’ve had several partners at any one time, but moving forward I’m going to limit it to being with a single partner–changing the formula a little bit. I’ll talk about why the older model wasn’t working in other posts on my personal blog, but moving forward API Evangelist will continue to be about my research into the wider API space, but it will increasingly have a focus about what I’m also doing with Streamdata.io. I’m interested in helping Streamdata.io understand that API sector, while I am helping my readers understand what Streamdata.io does. I will be actively telling stories about what I’m up to on a daily basis via API Evangelist, and I will continue to research the wider API space–this is valuable to Streamdata.io, and hopefully it is also valuable to you. I will also be applying this knowledge to helping the Streamdata.io team help define their place in the API sector, and telling the stories in real-time on API Evangelist–this is valuable to Streamdata.io, and hopefully it is also valuable to you. Streamdata.io is all about helping define the real-time, streaming layer of the API space–something I have been focused on for a number of years. However, they also invested into helping define the data streaming, event-sourced...[<a href="/2017/12/11/api-evangelist-and-streamdata-io/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/06/will-apis-still-be-relevant/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/32_119_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/06/will-apis-still-be-relevant/">Will APIs Still Be Relevant?</a></h3>
			<p><em>06 Dec 2017</em></p>
			<p>I named my blog, company, as well assumed my own title as “API Evangelist” in 2010. Every year since making that decision I’ve questioned it, and wonder if the concept and acronym will fade away. First of all, I have to admit its a bullshit concept in the first place. Its an acronym. It’s a pretty wide umbrella that allows us (me) to assemble a wide variety of technological concepts underneath. However, I made an investment in it, I was going to continue. I found some meaning that I was able to articulate to others, that would make an impact on businesses, organizations, institutions, and government agencies. It works. I am going to run with it, and in 2017, I’m renewing that perspective, and keeping it as my brand, title, and the central message I’m peddling in the tech sector. This all contributes to a significant under tow on my reality, pushing me to question reality on a regular basis. However, I’d say the strongest current I struggle with in this area is when it comes to endless waves of trends that crash on the shore. Maybe API is irrelevant because of microservices? Wait, maybe it is because of GraphQL? Sure, it will become irrelevant because of Kafka? Web APIs can’t do everything, and it something that will surely render them the wrong choice, just around the corner. There is truth in all of these statements. However, these statements are also just the nature of the game. Web APIs played these same cards when it came onto the scene. REST replaced SOAP, and JSON replaced XML. It is how the technology game is played. The frontline of this sector will always be developing and evangelizing new tools. It is how it disrupts, and builds new markets. I’m complicit in this. However, the mainstream world won’t ever move as fast as the frontline. No matter how much we want it to. Change just doesn’t happen that...[<a href="/2017/12/06/will-apis-still-be-relevant/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/06/the-shifting-api-landscape/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/canyon/yellow_collage/file-00_02_34_62.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/06/the-shifting-api-landscape/">The Shifting API Landscape</a></h3>
			<p><em>06 Dec 2017</em></p>
			<p>I’ve been watching, and trying to move forward the API conversation across all business sectors for seven years now. I’m not a startup. I’m not an API service provider. I’m not steering an enterprise group. I’m not an investor. I’m a software architect and storyteller who saw the potential for leveraging web infrastructure to deliver data, content, media, and algorithms across the web, to our mobile phones, as well as the seemingly endless number of devices we are connecting to the Internet in our personal, professional, and industrial worlds. I’m not studying the landscape so I sell to it. I am studying the landscape so I can understand it. While most of my readers will not grasp that difference, it gives me a fundamentally different view of what is going on across the space. In the last seven years I’ve had a focus on helping individuals at SMB, SME, enterprise, organizations, institutions, and government agencies understand what APIs are, and why they should be doing them. In 2017, I feel that mission becoming irrelevant based upon the shifting API landscape. As I work on my third API-first strategy for a top level federal agency in response to an RFI in recent months, prepare for an all week API workshop at Mutual of Omaha in Nebraska, and bookmark the job postings for API architect at almost every major bank in the US and UK, my cute little mission to help understand people understand what APIs are clearly needs to be retired. While there are plenty of people who still need to be educated what APIs are, and that they should be doing them, I’m going to leave it to the waves of other pundits, advocates, evangelists, and analysts to help onboard them. I’ve done my time. There are many changes on the horizon for API Evangelist which I’ll cover in future posts, but one significant one for me will be to lose “the mission”. As much...[<a href="/2017/12/06/the-shifting-api-landscape/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/05/what-is-more-important-having-an-api-or-having-a-welldesigned-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/16_38_600_500_0_avg_1_1_1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/05/what-is-more-important-having-an-api-or-having-a-welldesigned-api/">What Is More Important? Having An API? Or Having A Well-Designed API?</a></h3>
			<p><em>05 Dec 2017</em></p>
			<p>I got some expected flack this week for some stories on database to API deployments, and allowing folks to just auto-generate APIs from database structures. This approach is notorious for producing very badly designed APIs, which is something that just reflects whatever legacy infrastructure you have as a backend. It is something that drives many of API design, architects, and pundits crazy. Just do things properly!!! Follow good design practices! Put some thought into your API, and have some pride in this interface you are putting out there. All of this is easy for us to declare from our vantage point, but when your entrenched within an existing organization, battling for every movement forward, and often times just to not go backwards, this isn’t always the reality. As technologists we are always looking forward, and have a really hard time empathizing with folks who are stuck in positions that aren’t as forward leaning as ours. I know we have a well of experience we want everyone to see eye to eye with, but that isn’t always the reality. You can’t convince someone who is just trying to stay afloat within an organization that they should be investing in all of these possibilities in a future they aren’t tuned into. Not everyone holds the privileged position that many of us enjoy in the technology space, and I feel we can do a better job empathizing with some of them. I’m not saying we should give up on leading, and telling stories of a better future, but we need to work to build bridges to many who are less fortunate than we are. You know what is worse than being in an organization where you are battling for every bit of budget, resources, skills, and other things that help you stay afloat? Having people in more privileged positions making you feel stupid for what you do not understand, or have the time to learn. I wish folks...[<a href="/2017/12/05/what-is-more-important-having-an-api-or-having-a-welldesigned-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/05/the-picture-we-paint-with-the-stories-we-tell-around-each-api-version-release/"><img src="https://s3.amazonaws.com/kinlane-productions2/facebook/facebook-version-211-release.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/05/the-picture-we-paint-with-the-stories-we-tell-around-each-api-version-release/">The Picture We Paint With The Stories We Tell Around Each API Version Release</a></h3>
			<p><em>05 Dec 2017</em></p>
			<p>I fell down the rabbit hole of the latest Facebook version release, trying to understand the deprecation of their User Insights API. The story of the deprecation of the API isn’t told accurately as part of the the regular release process, so I found myself thinking more deeply about how we tell stories (or don’t) around each step forward of our APIs. I have dedicated areas of my API research for the road map, issues, and change log for API operations, because their presence tell a lot about the character of an API, and their usage I feel paints and accurate painting of each moment in time for an API. Facebook has a dedicated change log for their API platform, as well as an active status and issues pages, but they do not share much about what their road map looks like. They provide a handful of elements with each releases change log: New Features — New products or services, including new nodes, edges, and fields. Changes — Changes to existing products or services (not including Deprecations). Deprecations — Existing products or services that are being removed. 90-Day Breaking Changes — Changes and deprecations that will take effect 90 days after the version release date. The presence, or lack of presence, of a road map, change log, status and issue pages for an API paints a particular picture of a platform in my mind. Also, the stories they tell, or do not tell with each release paint an evolving picture of where a platform is headed, and whether or not we want to participating in the journey. Facebook does better than most platforms I track on when it comes to storytelling, by also releasing a blog post telling the story of each release, providing separate posts for the Graph API, as well as the Marketing API. It is too bad that they omitted the deprecation of the Audience Insight API, which occurred at the time...[<a href="/2017/12/05/the-picture-we-paint-with-the-stories-we-tell-around-each-api-version-release/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/05/api-deployment-templates-as-part-of-a-wider-api-governance-strategy/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/server-cloud1_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/05/api-deployment-templates-as-part-of-a-wider-api-governance-strategy/">API Deployment Templates As Part Of A Wider API Governance Strategy</a></h3>
			<p><em>05 Dec 2017</em></p>
			<p>People have been asking me for more stories on API governance. Examples of how it is working, or not working at the companies, organizations, institutions, and government agencies I’m talking with. Some folks are looking for top down ways of controlling large teams of developers when it comes to delivering APIs consistently across large disparate organizations, while others are looking for bottom ways to educate and incentivize developers to operate APIs in sync, working together as a large, distributed engine. I’m approach my research into API governance as I would any other area, not from the bottom up, or top down. I’m just assembling all the building blocks I come across, then began to assemble them into a coherent picture of what is working, and what is not. One example I’ve found of an approach to helping API providers across the federal government better implement consistent API patterns is out of the General Services Administration (GSA), with the Prototype City Pairs API. The Github repository is a working API prototype, documentation and developer portal that is in alignment with the GSA API design guidelines, providing a working example that other API developers can reverse engineer. The Prototype City Pairs API is a forkable example of what you want developers to emulate in their work. It is a tool in the GSA’s API governance toolbox. It demonstrates what developers should be working towards in not just their API design, but also the supporting portal and documentation. The GSA leads by example. Providing a pretty compelling approach to model, and a building block any API provider could add to their toolbox. I would consider a working prototype to be both a bottom up approach because it is forkable, and usable, but also top down because it can reflect wider organizational API governance objectives. I could see mature API governance operations having multiple API design and deployment templates like the GSA has done, providing a suite of forkable,...[<a href="/2017/12/05/api-deployment-templates-as-part-of-a-wider-api-governance-strategy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/04/narrowing-in-on-my-api-governance-strategy-using-api-transit-to-map-out-psd2/"><img src="https://s3.amazonaws.com/kinlane-productions2/talks/november-2015/subway-map-15.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/04/narrowing-in-on-my-api-governance-strategy-using-api-transit-to-map-out-psd2/">Narrowing In On My API Governance Strategy Using API Transit To Map Out PSD2</a></h3>
			<p><em>04 Dec 2017</em></p>
			<p>I’m still kicking around my API Transit strategy in my head, trying to find a path forward with applying to API governance. I started moving it forward a couple years ago as a way to map out the API lifecycle, but in my experience, managing APIs are rarely a linear lifecycle. I have been captivated by the potential of the subway map to help us map out, understand, and navigate complex infrastructure since I learned about Harry Beck’s approach to the London Tube map which has become the standard for quantifying transit around the globe. I am borrowing from Beck’s work, but augmenting for a digital world to try and map out the API practices I study in my research of the space in a way that allow them to be explored, but also implemented, measured, and reported upon by all stakeholders involved with API operations. While I’m still pushing forward this concept in the safe space of my own API projects, I’m beginning to dabble with applying at the industry level, by applying to PSD2 banking, and seeing if I can’t provide an interactive map that helps folks see, understand, and navigate what is going on when it comes to banking APIs. An API Transit map for PSD2 would build upon the framework I have derived from my API research, applied specifically for quantifying the PSD2 world. Each of the areas of my research broken down into separate subway lines, that can be plotted along the map with relative stops along they way: Definition - Which definitions are used? Where are the OpenAPI, schema, and other relevant patterns. Design - What design patterns are in play across the API definitions, and what is the meaning behind the design of all APIs. Deployment - What does deployment look like on-premise, in the cloud, and from region to region. Portals - What is the minimum viable standard for an API portal presence with any building blocks....[<a href="/2017/12/04/narrowing-in-on-my-api-governance-strategy-using-api-transit-to-map-out-psd2/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/04/facebook-quietly-deprecates-the-audience-insight-api-used-to-automate/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/04/facebook-quietly-deprecates-the-audience-insight-api-used-to-automate/">Facebook Quietly Deprecates The Audience Insight API Used To Automate</a></h3>
			<p><em>04 Dec 2017</em></p>
			<p>According to AdWeek, Facebook is quietly shutting down its Audience Insights API by the end of the year. They have a statement from Facebook stating, “We have decided to focus marketers on our more broadly available Audience Insights tool, so we are winding down the Audience Insights API by end of year. We’ll continue testing different ways to provide valuable insights to advertisers and agencies through the tool and across other destinations on Facebook.” which I assume they got directly from Facebook, because I can find no other communication regarding the deprecation of the API through normal newsroom, or API change log channels. It could be that I’m missing it, but it is clear they are trying to minimize chatter around this. According to the Facebook help page, Audience Insights, “shows you data about your target audiences so that you can create more relevant advertisements for them”. The platform uses native Facebook data to show you audience features such as: Age and gender, Relationship status, Education level, Job role, Top categories, Page likes, Top cities, Top countries, Top languages, Frequency of activities, and Device users. Then using third-party data (data come from sources like Acxiom, Datalogix and Epsilon) they show you audience features such as: Lifestyle, Household income, Home ownership, Household size, Home market value, Spending methods, Retail spending, Online purchases, Purchase behavior, and whether they are in market for a vehicle. You can still get at this via the Facebook Audience Insights web interface, but the APIs for automating this aspect of Facebook has mostly disappeared, or is in the process of disappearing. There are three layers to the Faceook Audience Insights API deprecation. You can still access some insights for ads, pages, and other objects, as well as one audience insight still available: /{object-id}/insights - Facebook Insights is a product available to all Pages and Apps on Facebook using the Insights dashboard. Audience Insights Rule - Definition of an audience insight rule. Then...[<a href="/2017/12/04/facebook-quietly-deprecates-the-audience-insight-api-used-to-automate/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/04/being-able-to-see-your-database-in-xml-json-and-csv/"><img src="https://s3.amazonaws.com/kinlane-productions2/slashdb/slashdb-content-negotiation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/04/being-able-to-see-your-database-in-xml-json-and-csv/">Being Able To See Your Database In XML, JSON, and CSV</a></h3>
			<p><em>04 Dec 2017</em></p>
			<p>This is a sponsored post by my friends over at SlashDB. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you SlashDB for your support, and helping me educate my readers about what is going on in the API space. I remember making the migration from XML to JSON. It was hard for me to understand that difference between the formats, and that you accomplish pretty much the same things in JSON that you could in XML. I’ve been seeing similarities in my migration to YAML from JSON. The parallels in each of these formats isn’t 100%, but this story is more about our perception of data formats, than it is about the technical details. CSV has long been a tool in my toolbox, but it was until this recent migration from JSON to YAML that I really started seeing the importance of CSV when it comes to helping onboard business users with the API possibilities. In my experience API design plays a significant role in helping us understand our data. Half of this equation is understanding our schema, and what the dimensions, field names, and data types of the data we are moving around using APIs. As I was working through some stories on how my friends over at SlashDB are turning databases into APIs, I saw that they were translating database, tables, and field names into API design, and that they also help you handle content negotiation between JSON, XML, CSV. Which I interpret as an excellent opportunity for learning more about the data we have in our databases, and getting to know the design aspects of the data schema. In an earlier post about what SlashDB does I mentioned that many API designers cringe at translating database directly into a web API. While I agree that people should be investing into API design to get...[<a href="/2017/12/04/being-able-to-see-your-database-in-xml-json-and-csv/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/01/the-conversational-interface-appetite-for-data-via-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/16_77_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/01/the-conversational-interface-appetite-for-data-via-apis/">The Conversational Interface Appetite For Data Via APIs</a></h3>
			<p><em>01 Dec 2017</em></p>
			<p>This is a sponsored post by my friends over at SlashDB. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you SlashDB for your support, and helping me educate my readers about what is going on in the API space. I spend a lot of time studying what is going on around bots on Twitter, Facebook, and Slack, as well as voice enablement like we see with Alexa, Google, and Siri. I lump these all under a research category called conversational interfaces. Conversational interfaces represent the next generation of API clients, with AWS Alexa being the most sophisticated example at how it will all work(eventually). While there are some interesting examples of conversational interfaces in action, for the most part they are still pretty simple, silly, and not providing much value. I’d say that any of the bots or voice implementations I’ve come across which are useful, are also pretty corporate, demonstrating the amount of resources you need to invest when crafting conversational interfaces. From my vantage point I’m seeing three main areas slowing the growth of true usability of conversational interfaces, 1) desire, and people not wanting or caring to engage, 2) availability of data via APIs in format that is usable, and 3) the performance of APIs that do have relevant data, and their ability to deliver it as an answer to a question in reasonable amount of time. You can put me squarely into the first category of not really wanting to use conversational interfaces, but I do understand that there are people who are into doing it, which gets me somewhat involved when it comes to thinking about the 2nd, and 3rd challenge. APIs are what delivers answers in conversational interfaces, and since APIs are my jam, I’m tuning in. One of the biggest challenges the conversational interface space will face in coming years...[<a href="/2017/12/01/the-conversational-interface-appetite-for-data-via-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/01/how-to-say-you-might-charge-for-api-access-in-the-future-without-being-a-jerk/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/statue-face-open-mouth_copper_circuit.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/01/how-to-say-you-might-charge-for-api-access-in-the-future-without-being-a-jerk/">How To Say You Might Charge For API Access In The Future Without Being A Jerk</a></h3>
			<p><em>01 Dec 2017</em></p>
			<p>I get it. It takes money to operate APIs. I’m a big advocate for making sure API providers, even public data API providers can sensibly charge for access to their valuable resources. I’m also painfully aware at how unrealistic a libertarian driven view of the web being open and free makes it very difficult to begin charging for data that has been historically free. However, I’m also a fan of helping API providers understand how they can communicate that they might / will be charging for access to data at some point in the future without being complete jerks about it. I see API providers regularly make the statement that they will begin charging for API access at some point in the future, but this particular story is driven from hearing it out of the Washington Metropolitan Area Transit Authority (WMATA) making changes to their terms of service, where one of the bullet points was that they would begin charging for access at some point. Making the announcement that you intend to begin charging for something that has been free is challenging in any API ecosystem, but especially so within public data API ecosystems like WMATA. In any of these environments you can’t just shoot across your community’s bow with a statement like this, and expect a positive response. Doing so, just shows how out of touch with your community you are. First rule of communicating around the business side of your road map is don’t just say you’ll be charging at some point and leave things there. Give details of what this means. Demonstrate your knowledge around how API management and service composition works. Will ALL developers be charged? Will it just be commercial developers? Will it be developers over a certain level of consumption? Do not leave it to the communities imagination regarding what will happen, because this is where the powers of Internet speculation will take hold, and begin working against your...[<a href="/2017/12/01/how-to-say-you-might-charge-for-api-access-in-the-future-without-being-a-jerk/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/12/01/how-do-you-ask-questions-of-data-using-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/27_93_800_500_0_max_0_-5_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/12/01/how-do-you-ask-questions-of-data-using-apis/">How Do You Ask Questions Of Data Using APIs?</a></h3>
			<p><em>01 Dec 2017</em></p>
			<p>I’m preparing to publish a bunch of transit related data as APIs, for us across a number of applications from visualizations to conversation interfaces like bots and voice-enablement. As I’m learning about the data, publishing it as unsophisticated CRUD APIs, I’m thinking deeply about how I would enable others to ask questions of this data using web APIs. I’m thinking about the hard work of deriving visual meaning from specific questions, all the way to how would you respond to an Alexa query regarding transit data in less than a second. Going well beyond what CRUD gives us when we publish our APIs and taking things to the next level. Knowing the technology sector, the first response I’ll get is machine learning! You take all your data, and you train up some machine learning models, put some natural language process to work, and voila, you have your answer to how you provide answers. I think this is a sensible approach to many data sets, and for organizations who have the machine learning skills and resources at their disposal. There are also a growing number of SaaS solutions for helping put machine learning work to answer complex questions that might be asked of large databases. Machine learning is definitely part of the equation for me, but I’m not convinced it is the answer in all situations, and it might not always yield the correct answers we are always looking for. After machine learning, and first on my list of solutions to this challenge is API design. How can I enable a domain expert to pull out the meaningful questions that will be asked of data, and expose as simple API paths, allowing consumers to easily get at the answers to questions. I’m a big fan of this approach because I feel like the chance we will get right answers to questions will be greater, and the APIs will help consumers understand what questions they might want...[<a href="/2017/12/01/how-do-you-ask-questions-of-data-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/30/troubling-terms-of-service-changes-from-washington-metropolitan-area-transit/"><img src="https://s3.amazonaws.com/kinlane-productions2/transit/wmata-transit-terms-of-service.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/30/troubling-terms-of-service-changes-from-washington-metropolitan-area-transit/">Troubling Terms of Service Changes From Washington Metropolitan Area Transit</a></h3>
			<p><em>30 Nov 2017</em></p>
			<p>I was turned onto a developing problem within the Washington Metropolitan Area Transit Authority (WMATA) around a recent terms of service change made around the transit data API by Technically DC. While the transit authority is saying the changes are business as usual and make sense for the platform, some of the developers, specifically one of the biggest API users MetroHero says the changes are targeting them specifically. MetroHero presented what they feel are the unreasonable changes to the WMATA API terms of service in a WMATA Board Meeting recently, focusing on four main areas: That no user or developer can mention “WMATA” in press releases without letting WMATA first review it. That WMATA can gain access to any user’s applications that use the data, can audit personnel information for anyone working on those applications, and WMATA can also create their own version at any time. That WMATA forbids users from claiming their data is accurate, complete or timely, or claiming it is more so than WMATA’s data. That the transit agency may now charge users in the future for using their data. These are all common changes I’ve seen made to API terms of service before, and are usually signs that a platform operator that is pretty out of touch with what it is like to be an API consumer, and with their own API community. It is a sign of a broken or pernicious feedback loop which leads to API providers making decisions that do lasting damage to their communities like this. These types of changes reflects the “rules of road” terms of service changes Twitter made to back in 2012. Which didn’t fully kill off the Twitter API, but set such a bad tone in the community, the company is still working to dig out of it five years later. I know platform operators feel they need to assert this level of control, but in an API community you need to learn...[<a href="/2017/11/30/troubling-terms-of-service-changes-from-washington-metropolitan-area-transit/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/30/the-average-person-will-never-care-about-apis-until-it-does-something/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/17_88_800_500_0_max_0_-5_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/30/the-average-person-will-never-care-about-apis-until-it-does-something/">The Average Person Will Never Care About APIs Until It Does Something</a></h3>
			<p><em>30 Nov 2017</em></p>
			<p>I am always looking for ways to introduce people to the concept of APIs, and that they are right below everything digital you do in your daily life. Even with my prolific writing, and sharing on social media, the number of new converts to API awareness are relatively low. I’m alright with what I do not scaling. I’m in this for the long haul, not to sell products or services. I’m looking to help turn on the API light for people not because I want them building the next API, I want to help enlighten folks so that they can take more control over their digital presence, and push back on the platforms and algorithms that are increasingly dominating our lives. One thing I’ve learned about normal folks in my journey as the API Evangelist is that nobody will ever care about APIs until they do something meaningful in their lives. Technologists learn about APIs for other reasons, but normal people aren’t motivated in the same ways, and need to have some meaning before they’ll wade into this more technical world of unknown, unknowns. When talking to technologists about APIs I focus on the API lifecycle, and the agility that APIs bring. With normal folks I tend to focus on platforms they already use, and algorithms that directly impact their lives, or impact people they know. As an API storyteller it is important for me to develop meaningful stories, that make APIs accessible in everyday scenarios to average people I encounter. If someone is a photographer I will tell stories of the Flickr or Instagram API. If someone is an accountant, I will work through how the Intuit API is rapidly being used by small businesses. If someone is a genealogist I will talk about how the Family Search API drives Ancestry.com. If someone is a music professional I will focus on Spotify, or maybe the Bandcamp API. This is why I play with as...[<a href="/2017/11/30/the-average-person-will-never-care-about-apis-until-it-does-something/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/30/sql-statement-passthrough-using-web-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/slashdb/slashdb-sql-pass-through-mode.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/30/sql-statement-passthrough-using-web-apis/">SQL Statement Pass-Through Using Web APIs</a></h3>
			<p><em>30 Nov 2017</em></p>
			<p>This is a sponsored post by my friends over at SlashDB. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you SlashDB for your support, and helping me educate my readers about what is going on in the API space. I’m closely following the approach of GraphQL when it comes to making data resources more accessible by API consumers when developing applications. I think there is some serious value introduced when it comes empowering front-end developers with the ability to get exactly the data they need using a variety of querying structures. I enjoy studying up on different approaches to making different dimensions of a database to consumers and end-users, and found a pretty scrappy one from my friends over at SlashDB, with their SQL statement pass through. It’s not the most formal approach to query a database, but I think it’s scrappy and simple enough, that it might work for a wide variety of technical, as well as non-technical users. Using the SlashDB mode, an administrator, or an application backend developer can define arbitrary SQL queries which once defined, can be executed as a smple URL. The example query they provide returns customers from London: http://demo.slashdb.com/query/customers-in-city/city/London.html. It is something that will make RESTafarians pull their hair (dreads?) out, but for business users looking to get their hands on some data to populate a spreadsheet, or share with a partner when developing an application–it will be a lifesaver. As the GraphQL folks like trumpet, REST isn’t the only way to get things done, and while I think we should be thinking critical about the long term impact of our API design choices, getting business done efficiently is an important aspect of doing APIs as well. What I like about the SlashDB approach is it makes for an intuitive URL. Something business users can understand. I could see crafting...[<a href="/2017/11/30/sql-statement-passthrough-using-web-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/29/sorry-your-api-effort-falls-a-little-short-for-the-apis-i-cover/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/supreme-court-judgement.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/29/sorry-your-api-effort-falls-a-little-short-for-the-apis-i-cover/">Sorry Your API Effort Falls A Little Short For The APIs I Cover</a></h3>
			<p><em>29 Nov 2017</em></p>
			<p>I get a lot of emails from companies asking me to look at their APIs. Too many for a one person operation like me to consider. I have to be picky about the APIs I’m taking a look at, and over time I’ve developed a set of criteria for determining how much energy I will invest in an API. Usually within about 2-3 minutes I can tell if it is an API I will be diving in deeper, or I will just be walking away and moving on with my work. The first thing that turns me off of an API is that it just isn’t interesting. I’ll land on the page and I can tell what it does, but it just doesn’t interest me. It doesn’t offer any value, or it is in a category that I’m just not eager to be thinking about and showcasing in my work. If an API doesn’t deliver value, and stand out as being interesting beyond the hundreds of other APIs I see each week, I’m just not going to stop and take notice. Sorry, it might be to others–don’t just take my opinion. The next thing that keeps me from going deeper is I can’t tell what an API does. I’m always amazed at how much head scratching, clicking and reading I will do before I ever figure out what an API does. I’m pretty hard headed, so sometimes its me, but other times I’m just stuck at figuring out what is going on under the hood. Usually after about 3-5 minutes of struggling to understand what is happening, I will just walk away. It is unlikely that other folks will be investing more time than that, and the API will not last long in my experience. After that, the biggest crime I see companies and organizations make is that they just do not invest enough into a dedicated portal, and the other supporting resources for their...[<a href="/2017/11/29/sorry-your-api-effort-falls-a-little-short-for-the-apis-i-cover/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/29/asyncapi-is-a-specification-format-for-messagedriven-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/asyncapi/asyncapi-editor-sample.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/29/asyncapi-is-a-specification-format-for-messagedriven-apis/">AsyncAPI Is A Specification Format For Message-Driven APIs</a></h3>
			<p><em>29 Nov 2017</em></p>
			<p>I’ve been learning about a new API definition format called AsyncAPI that allows you to define message-driven APIs in a machine-readable format. It is protocol-agnostic, which means you can use it for APIs that work over MQTT, AMQP, WebSockets, STOMP, and other real-time, and Internet of Things focused APIs. The specification format mirrors OpenAPI, making it pretty easy to get up to speed understanding what is going on. There are two primary concepts at play with the AsyncAPI: Messages - Consumer(s) communicate with your API via messages. A message is a piece of information two or more programs exchange. Most of the times to notify the other end(s) that, either an event has occurred or you want to trigger a command. Technically speaking the events and actions will always be sent in the same way. These are just messages, and their content can be anything. So when we talk about the difference between events and actions, this is only a semantic differentiation of message’s content. We do not enforce you to make any difference between them, although we encourage you to do it. A message can contain headers and a payload. However, both are optional. The specification allows you to define any header, to remain as much protocol-agnostic as possible. Topics - Message-driven protocols usually contain something called topic (MQTT), routing key (AMQP), destination (STOMP), etc. To some extent, they can compare to URLs in HTTP APIs. So, when you send a message to your API, it will be routed depending on the topic you published on. This feature allows you to create APIs that subscribe to specific topics and publish to other ones. There’s no standard way of naming topics, so we recommend you to have a look at our proposal here. I don’t have any APIs I can apply AsyncAPI to, so I have to just learn from the examples and any other work I come across. It makes me happy to see...[<a href="/2017/11/29/asyncapi-is-a-specification-format-for-messagedriven-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/29/api-deployment-is-about-publishing-them-wherever-they-are-needed/"><img src="https://s3.amazonaws.com/kinlane-productions2/slashdb/slashdb-database-deployment.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/29/api-deployment-is-about-publishing-them-wherever-they-are-needed/">API Deployment Is About Publishing Them Wherever They Are Needed</a></h3>
			<p><em>29 Nov 2017</em></p>
			<p>This is a sponsored post by my friends over at SlashDB. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you SlashDB for your support, and helping me educate my readers about what is going on in the API space. I spun out a separate research area for API deployment, from my core API management research back in 2012 when companies were regularly asking me which of the API management providers they should be using to publish new APIs. At the time, none of them would help you actually publish your APIs, and there just wasn’t enough conversations going on around the subject. When I give talks which include my section on API deployment, some people still scratch their heads thinking there really isn’t that many options on the table–they deploy APIs wherever they’ve been deploying their APIs. However, in a cloud-driven world, the opportunities for how and where we can deploy our APIs are increasing, and the savvy teams are getting more versatile in how they get things done. Supporting multi-cloud is something all API service providers should be supporting. I was reviewing the approach to pricing from my friends and partners over at SlashDB, and I noticed as part of their pricing tier that they have “deployment” as one of the options. Allowing for deployment of their database to API solution on Debian, RedHat, VMWare, VirtualBox, Docker, Vagrant, Amazon, Azure, as well as custom solutions at the enterprise tiers. Focusing on the needs of a diverse range of enterprise customers, while also paying attention to where the API deployment conversation has been shifting for some time with Amazon, Docker, and the other platforms that are dominating the IT landscape. API service providers should be supporting multiple cloud platforms like SlashDB does, but API providers should also be looking at their own API deployment in the context...[<a href="/2017/11/29/api-deployment-is-about-publishing-them-wherever-they-are-needed/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/28/the-openapipowered-mock-api-server-from-stripe/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripe-mock-api-server.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/28/the-openapipowered-mock-api-server-from-stripe/">The OpenAPI-Powered Mock API Server From Stripe</a></h3>
			<p><em>28 Nov 2017</em></p>
			<p>
I showcased Stripe’s OpenAPI definition the other week, so I wanted to also highlight a side effect of Stripe deciding to be OpenAPI-Driven. Stripe recently published an OpenAPI-powered mock server, allowing Stripe API consumers to test drive, and play with the Stripe API in a simulated environment. “It operates statelessly (i.e. it won’t remember new resources that are created with it) and responds with sample data that’s generated using a similar scheme to the one found in the API reference.”

The Stripe Mock Server is written in Go, and is available on Github. You can rebuild the Stripe API mock server from an updated OpenAPI anytime. It is a pretty dead simple mock server that seems like should be standard practice for any API. Providing a simple, safe, and portable way to play with an API. I’m going to fork the Stripe Mock API and play with it, see what is possible with the tool.

I will be keeping an eye out for any other OpenAPI-powered tools out of Stripe, now they are actively working with it. Adoption of OpenAPI at this level of API provider is helpful to the rest of the community, by providing an example of how you can bake OpenAPI into your operations, but also the open source tooling these companies produce. It’s an important community effect that makes this whole API thing work so well.

Ideally, the leading API providers, with the most resources, could coordinate their efforts and deliver a suite of open source tooling. However, I’m patient, I’m just happy that big companies like Stripe, Slack, Box, New York Times are doing OpenAPI at all. I can wait for all the cool tooling to happen next. I’ll keep an eye on Stripe’s Github organization to see what pops up.

[<a href="/2017/11/28/the-openapipowered-mock-api-server-from-stripe/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/28/making-your-api-pricing-page-accessible-to-everyone/"><img src="https://s3.amazonaws.com/kinlane-productions2/bitscoop/bitscoop-pricing-free.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/28/making-your-api-pricing-page-accessible-to-everyone/">Making Your API Pricing Page Accessible To Everyone</a></h3>
			<p><em>28 Nov 2017</em></p>
			<p>I’ve been talking with the folks over at Bitscoop about their integration platform as a service (iPaaS) offering. I would API mapping as a service, but that is another story. After talking with them, and going through their website, I wanted to focus on Bitscoop’s pricing page, which I feel reflects where API service pricing and plans are headed. There are three main areas of their pricing that I think are worth highlighting for accessing APIs at scale.

Bitscoop is really priced for EVERYONE, with a simple free tier to get started using the platform.



Next there are three tiers of access: developer, organization, and enterprise. It’s not as “ascendable” as I’d like it (smoother hop from tier to tier), but because Bitscoop clearly articulates how much additional calls are for each tier, the jump from tier to tier isn’t as painful.



Closing out the Bitscoop pricing page they have a custom solutions section letting you know they’ll deploy your API service to Google, Amazon, or Azure. Reflecting where API deployment, and API service deployment is headed.



Thats it. That is the story. Make your services accessible. Don’t price people out. Make your solutions available to everyone, with the opportunity to grow. I’m always fascinated by how many differing opinions there are out there regarding how you craft your SaaS and API plans. I think Bitscoop pricing reflects the reality of when you are integrating with hundreds or thousands of APIs. To be able to compete at this scale you are going to have to be plug and play with your tech, as well as the business of your APIs.

[<a href="/2017/11/28/making-your-api-pricing-page-accessible-to-everyone/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/28/getting-a-handle-on-our-database-schema-using-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/server-cloud1_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/28/getting-a-handle-on-our-database-schema-using-apis/">Getting A Handle On Our Database Schema Using APIs</a></h3>
			<p><em>28 Nov 2017</em></p>
			<p>This is a sponsored post by my friends over at SlashDB. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you SlashDB for your support, and helping me educate my readers about what is going on in the API space. When I take money from my partners, I am always looking for characteristics in their products and services that allow me to write honest stories about the solutions they provide. I can’t do this for all API companies that approach me, but the ones that are doing useful things, make it pretty easy for me. SlashDB helps me out on this front because they aren’t the shiny new startup doing APIs–they are the real world business helping other companies, organizations, institutions, and government agencies get a handle on their databases using APIs. One huge benefit of this process in my opinion is how it helps us get a handle on the schema we use, by letting a little light in on the process. One of the main reasons our databases are such a mess is because they are hidden away behind a dark technical or organizational curtain, and there really isn’t much accountability regarding how we define, name, organize, and store our data. Of course there are exceptions to this, but a messy, bloated, unwieldy database is a hallmark of about 75% of the organizations I’ve worked with over my 30 year career. Central databases are often a mashup of years, even decades of creating databases, tables, and adding columns, often times occurring over generations of database teams. The result is often an incoherent mess regarding how things are named, with layers of cryptic field names, and irrelevant table names, which might seem normal until you go and try to expose these data resources to 3rd party and partner developers. Many of the data APIs I come across...[<a href="/2017/11/28/getting-a-handle-on-our-database-schema-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/27/multiregion-apis-using-aws-api-gateway/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/amazon-api-gateway-regions.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/27/multiregion-apis-using-aws-api-gateway/">Multi-Region APIs Using AWS API Gateway</a></h3>
			<p><em>27 Nov 2017</em></p>
			<p>I’ve been deploying two project using AWS API Gateway, Lambda, and Amazon RDS lately. I’ve become so sold on this approach to deploying APIs as part of this work, that I am evolving my own internal API process to use the same approach. The technical aspect of serverless plus the gateway definitely convinced me of the potential, but it was also the usage of AWS IAM which sealed the deal for me. I’m all too aware of how much my API security lacks as a one person shop, something that I also see reflected in my client operations, and I’d rather be offloading security to AWS than ending up taking the hit on it down the road. While deploying my project using AWS API Gateway, and Lambda, I was faced with the question regarding which zone I should be deploying the APIs in. It is the first time I’ve been faced with the opportunity to deploy my APIs into multiple zones. Sure, I could have deployed my servers into any AWS zone before, but for some reason now that I’m doing with AWS API Gateway, and Lambda, the opportunity seemed more of a possibility. I’ve pitched it to my client to consider an east as well as a west coast API deployment, so that we can give developers the choice in the documentation to choose which availability zone they’d like to use in their application. Before I make the proposal I’m going to deploy some prototypes, and do some benchmark testing, and see what the benefits are. Even if I end up publishing APIs into separate regions, I still have the backend database to content with. Where do I put the database, and how to I replicate between zones. Amazon RDS gives me the tools to tackle this, but historically I would only do this just for backup, not for actual redundancy, as well as performance gains. Amazon zones have been a staple of the...[<a href="/2017/11/27/multiregion-apis-using-aws-api-gateway/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/27/making-sure-you-operate-in-the-cloud-marketplaces-as-an-api-service-provider/"><img src="https://s3.amazonaws.com/kinlane-productions2/slashdb/slashdb-automatic-rest-api-for-databases-in-aws-marketplaces.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/27/making-sure-you-operate-in-the-cloud-marketplaces-as-an-api-service-provider/">Making Sure You Operate In The Cloud Marketplaces As An API Service Provider</a></h3>
			<p><em>27 Nov 2017</em></p>
			<p>This is a sponsored post by my friends over at SlashDB. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you SlashDB for your support, and helping me educate my readers about what is going on in the API space. As the cloud giants like AWS, Microsoft, and Google continue to assert their dominance of the digital world, one aspect of their operations I’m watching closely has to do with their marketplaces. Google’s marketplaces are still very Android focused, but Amazon and Microsoft have shifted their recent editions of their marketplaces to be more cloud oriented, and accommodating a wide variety of applications, machine learning models, as well as APIs and API-focused services. While these marketplaces are still growing, and asserting their role in the digital economy, they are something I advise API providers, and service providers to be keeping a close eye on, and begin considering how they will want to operate within these environments. If you are an API service provider, and you are selling services to API providers anywhere along the API lifecycle, I recommend you follow the example of friends over at SlashDB, who have their database to API offerings in two of the leading marketplaces: AWS - Automatically constructing a REST API to databases for reading and writing on the AWS platform. Azure Marketplace - SlashDB enables you to do more with traditional databases and Microsoft Azure. As more companies, organizations, institutions, and government agencies move their databases into the cloud, SlashDB sees the opportunity to help them quickly turn databases and tables into web interfaces for querying data. Having your API service ready to go, in the environments where your potential customers are already operating is how much of this API stuff will go down in the future. Amazon has set the stage for how we’ll be delivering IT infrastructure over the...[<a href="/2017/11/27/making-sure-you-operate-in-the-cloud-marketplaces-as-an-api-service-provider/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/27/hints-of-banking-api-regulations-from-cfpb-with-consumer-authorized-financial-data-sharing-and-aggregation-rules/"><img src="https://s3.amazonaws.com/kinlane-productions2/cfpb/cfpb-outlines-principles-for-consumer-authorized-financial-data-sharing-and-aggregation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/27/hints-of-banking-api-regulations-from-cfpb-with-consumer-authorized-financial-data-sharing-and-aggregation-rules/">Hints of Banking API Regulations From CFPB With Consumer Authorized Financial Data Sharing And Aggregation Rules</a></h3>
			<p><em>27 Nov 2017</em></p>
			<p>The Consumer Finance Protection Bureau (CFPB) has started laying out some consumer-authorized data sharing and aggregation rules to begin moving forward the banking data scraping conversation in (hopefully) a more production way. It is common knowledge that many financial focused (Fintech) companies regularly access consumers account data using their credentials, so that they scrape relevant account information from their bank, for use in a wide variety of 3rd party tools. This is a common practice that everyone in the industry knows about, understands is a potential security and privacy risk, but everyone looks the other way because it adds value to the consumer ecosystem. In a perfect world each bank would have a public API portal where Fintech aggregators could come and sign up for application keys, and get the authorization of users via OAuth, and obtain access to their banking data in a secure, and accountable way. However, as we are well aware, we do not live in a perfect world, and banks are pretty resistant to change, so the scraping continues. At some point we are going to see the landscape begin to shift, and I’m guessing it will be at the regulatory level where we finally begin to see this behavior changed–making the CFPB’s rules announcement a reflection of what is coming down the pipes when it comes to banking API regulation. The consumer protection principles for consumer-authorized financial data sharing and aggregation announcement focuses on: Access - Consumers are able, upon request, to obtain information about their ownership or use of a financial product or service from their product or service provider. Such information is made available in a timely manner. Consumers are generally able to authorize trusted third parties to obtain such information from account providers to use on behalf of consumers, for consumer benefit, and in a safe manner. Financial account agreements and terms support safe, consumer-authorized access, promote consumer interests, and do not seek to deter consumers from...[<a href="/2017/11/27/hints-of-banking-api-regulations-from-cfpb-with-consumer-authorized-financial-data-sharing-and-aggregation-rules/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/21/when-you-believe-everything-in-tech-is-new-and-nothing-repeats-itself/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/beach-rocks-currents_blue_circuit_4.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/21/when-you-believe-everything-in-tech-is-new-and-nothing-repeats-itself/">When You Believe Everything In Tech Is New And Nothing Repeats Itself</a></h3>
			<p><em>21 Nov 2017</em></p>
			<p>I get regular waves of commenters and tweeters who like to point out the API patterns I’m covering in the API space, have all been done before. We tried discovery docs before they are called WSDL! That API discovery thing is called UDDI! RPC is nothing new! That isn’t new. We tried that before, and it didn’t work. I rarely ever engage with these folks, as this behavior is one pattern in behavior I actually do believe we SHOULDN’t be repeating and showcasing. I’m fascinated by the reasons someone would feel so strongly they need to respond. That something happened in the past, and because it didn’t work we shouldn’t try again today. That somehow the world of compute isn’t built upon, and remixed upon previous ideas that worked, and many that didn’t work until just the right conditions existed. This kind of behavior is really fascinating for me in the world of APIs where reuse, aggregation, facades, and so many patterns of reworking what already exists is core to the entire concept. Where do folks get such strange believes in the past, and what can and cannot be re-interpreted in the future? Hey you, electric car manufacturers, the electric car was done in early 20th century and it didn’t work! Hey musician, that baseline was originally present in the big band era and didn’t go over well, it won’t work now! Those pants were first tried in the 1950s and were a flop. Someone already wrote a book on Abraham Lincoln, why would you want to write another? Where do people get the idea that something that existed in the past shouldn’t be tried again, when it comes to the world of technology? Not only have the thought, but so many feel so strongly that they have to reach out and tell me what I’m writing about is dumb because it’s already been done? What is it about web technology that makes people think...[<a href="/2017/11/21/when-you-believe-everything-in-tech-is-new-and-nothing-repeats-itself/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/21/the-defensive-database-administrator-and-the-eager-blockchain-believer/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/21/the-defensive-database-administrator-and-the-eager-blockchain-believer/">The Defensive Database Administrator And The Eager Blockchain Believer</a></h3>
			<p><em>21 Nov 2017</em></p>
			<p>Think about the power that database administrators have in your organizations world? I’ve been working with databases since my first job in 1987. I’ve seen the power bestowed upon database administrators in organization after organization. They are fully aware of the power they control, and most other people in an organization are regularly reminded of this power. The defensive database administrator is always the biggest obstacle in the way of API teams who are often seen as a threat to the power and budgets that database groups command. This power is why databases are often centralized, scaled vertically, and are the backends to so many web, mobile, desktop, and server applications. I spend a significant amount time thinking about the power that database administrators wield, and how we can work to find more constructive, secure, and sensible approaches to shifting legacy database behaviors. Lately, I also find myself thinking a lot more about Blockchain. Not because I’m a believer, but because so many believers are pushing it onto my radar. Blockchain will continue to be a thing, not because it is a thing, but because so many people believe it is a thing. Most blockchains will not withstand the test of time, they are vapor, but the blockchains that remain will because people have convinced other people to put something meaningful into their blockchain. Much like we have convinced so many companies, organizations, institutions, and government agencies to put data into databases. Yes we. I’m complicit. A definition of the blockchain is, “a continuously growing list of records, called blocks, which are linked and secured using cryptography”. It’s a database, linked and secured using cryptography. The reason you hear about the blockchain so much, and how it can revolutionize almost every business sector, is the blockchain believers want to convince you to put your digital assets into their blockchain, which will eventually make it something real. I can setup a blockchain today, call it anything...[<a href="/2017/11/21/the-defensive-database-administrator-and-the-eager-blockchain-believer/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/21/day-2638-apis-are-dumb/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/christianity-under-construction_atari_asteroids.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/21/day-2638-apis-are-dumb/">Day 2,638: APIs Are Dumb</a></h3>
			<p><em>21 Nov 2017</em></p>
			<p>It is one of those weeks where writing API stories, and doing my API work is completely uninteresting, and my three year old self is throwing a temper tantrum when it comes to doing anything. APIs are dumb. Why the hell would I care about this aspect of technology? Most people don’t understand what the fuck I’m talking about, and people keep doing really dumb shit with them, instead of working on the problems that really matter. Why do I keep doing what I’m doing? Why don’t I just go get a real job, make some real money, and give a shit less? Great question! Most weeks I can just turn the API Evangelist persona on, and with a notebook full of ideas, and inbox full of questions, I begin writing the API blah blah blah. It just flows. This week it all seems dumb, and I have to fabricate any ounce of caring about APIs. Beyond APIs and Internet technology in general feeling like a pretty bad idea, I feel complicit in helping bring about this technological beast that is wreaking havoc on our world right now. Why the hell should I continue doing API Evangelist, when so many of my ideas can be used for exploitation, and just keep making rich white people richer? It just seems like a bad idea, so why shouldn’t I just shut things down and go find a meaningful job (does that exist)? First, I always start with the basic API Evangelist mission: helping non-techies understand what APIs are, and how they are right under the hood of everything we are using that is digital. What I do will never receive venture capital, be profitable, and return measurable ROI. Few other companies, let alone individual care about a digitally literate world, they just want consumers, and refuse to see the correlation. I’m the one showcasing API stories consistently regardless of the latest trends, and focus on understanding what...[<a href="/2017/11/21/day-2638-apis-are-dumb/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/20/generating-operational-revenue-from-public-data-access-using-api-management/"><img src="https://s3.amazonaws.com/kinlane-productions2/public-data-api-management/parks-prohibit-commercial-use.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/20/generating-operational-revenue-from-public-data-access-using-api-management/">Generating Operational Revenue From Public Data Access Using API Management</a></h3>
			<p><em>20 Nov 2017</em></p>
			<p>This is part of some research I'm doing with Streamdata.io. We share a common interest around the accessibility of public data, so we thought it would be a good way for us to partner, and Streamdata.io to underwrite some of my work, while also getting the occasional lead from you, my reader. Thanks for supporting my work Streamdata.io, and thanks for support them readers! A concept I have been championing over the years involves helping government agencies and other non-profit organizations generate revenue from public data. It is a quickly charged topic whenever brought up, as many open data and internet activists feel public data should remain freely accessible. Something I don’t entirely disagree with, but this is a conversation, that when approached right can actually help achieve the vision of open data, while also generating much needed revenue to ensure the data remains available, and even has the opportunity to improve in quality and impact over time. Leveraging API Management I’d like to argue that APIs, and specifically API management has been well established in the private sector, and increasingly in the public sector, for making valuable data and content available online in a secure and measurable way. Companies like Amazon, Google, and even Twitter are using APIs to make data freely available, but through API management are limiting how much any single consumer can access, and even charging per API call to generate revenue from 3rd party developers and partners. This proven technique for making data and content accessible online using low-cost web technology, requiring all consumers to sign up for a unique set of keys, then rate limiting access, and establishing different levels of access tiers to identify and organize different types of consumers, can and should be applied in government agencies and non-profit organizations to make data accessible, while also asserting more control over how it is used. Commercial Use of Public Data While this concept can apply to almost any...[<a href="/2017/11/20/generating-operational-revenue-from-public-data-access-using-api-management/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/17/we-love-what-you-do-in-the-api-space-but-could-you-do-it-our-way/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-butterfly-vertical.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/17/we-love-what-you-do-in-the-api-space-but-could-you-do-it-our-way/">We Love What You Do In The API Space But Could You Do It Our Way</a></h3>
			<p><em>17 Nov 2017</em></p>
			<p>I hear it daily in my inbox, on Twitter, and via LinkedIn. We love what you do! We’ve followed your work for a while, and love your unique voice, and the way you tell stories on your blog. I’m not very good at accepting praise on my work, especially when I know that much of it isn’t sincere and genuine. Saying it casually to me is weird, and I am not sure why people feel like they should be saying it, but it is the folk who go the distance to say it, but then also try to change the way I am, after acknowledging over and over, that they like what I do. From running a major conference, to my everyday storytelling, I get waves of people who like what I’ve done historically, want to support and be part of it, but once engaged actively try to change the conversation, and change the tone of what I do. The community has really seemed to rally around your conference, and clearly you’ve built a loyal group by making your event about ideas–we’d like to sponsor, but we really need a main stage talk where we can talk about our products. We love the tone of your storytelling on the blog and how you educated people people about the real world aspects of doing APIs–we’d love to sponsor, but we need you to talk about our products, and shift the focus to what we are doing. There are so many ways people acknowledge the value of what I do, but then want me to do the same old tired thing they’ve been doing. I get why you do it. It is easy. It is going from zero to what you want in as little time as possible. However, you seem to be all too willing to completely ignore why my thing is working and why your old tired thing isn’t, and why you are even attracted...[<a href="/2017/11/17/we-love-what-you-do-in-the-api-space-but-could-you-do-it-our-way/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/17/the-many-meanings-of-do-not-make-the-same-mistake-as-twitter-did-with-their/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/kinlane-white-board-twitter_copper_circuit.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/17/the-many-meanings-of-do-not-make-the-same-mistake-as-twitter-did-with-their/">The Many Meanings Of "Do Not Make The Same Mistake As Twitter Did With Their</a></h3>
			<p><em>17 Nov 2017</em></p>
			<p>I remember the first time I heard someone say that they didn’t want to make the same mistake as Twitter did with their API. It was from Pinterest. After that I heard the phrase uttered by many companies, with almost an entirely different meaning behind what the mistake was. Twitter is a darling of the API community when it comes to being the poster child for what not to do in the API space. I consider Twitter to be in the top 10 most important APIs out there, as well as being in the top ten APIs I wouldn’t want to be responsible for, and is a platform full of endless examples of how to do APIs right, and how to do them wrong. When some companies say this phrase, they mean they don’t want to make the mistake Twitter did by having an API at all–usually heard from executives. Other times, it is said in response to anti competitive behavior in their API ecosystem, and treating startups badly. When you hear from developers, it is usually about their rate limits, and their rules of the road they published a few years back. It coming years I predict we’ll be saying it about automation, and using Twitter as case study for how not to assert control of bots on your API platform. You’ll find me leveraging this statement regularly to talk about making sure you have a real API monetization strategy, and don’t wait a decade to start offering premium access to your APIs that are accessible to EVERYONE. I’ve been complaining about access to the Twitter API for over five years now. API plans are the heart of every API I keep an eye on. They set the tone for ALL conversations that go on around an API. The lack of a coherent, equitable, API access plan at Twitter has set into motion almost every other illness on the platform from harassment to bots....[<a href="/2017/11/17/the-many-meanings-of-do-not-make-the-same-mistake-as-twitter-did-with-their/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/17/my-basic-yaml-for-starter-api-plans/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_plans_pricing_tiers.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/17/my-basic-yaml-for-starter-api-plans/">My Basic YAML For Starter API Plans</a></h3>
			<p><em>17 Nov 2017</em></p>
			<p>I started developing a machine readable format for describing the API plans and pricing for leading API providers a few years back. Eventually I’d like to see the format live alongside OpenAPI, Postman, and other machine readable API specifications within a single APIs.json index. I am looking to adequately describe the plans and pricing for APIs, which are often just as important as the technical details, in the same way we’ve describe the technical surface area of an API using OpenAPI for some years now. People love to tell me that I will never be able to do it, which only makes me want to do it more. I’m revisiting my work as part of work I’m doing on a clients project, which I’m also using to push forward my API portal and management toolbox. The project I’m working on has two API plans: 1) Starter - The free level of access everyone gets when signing up for access to an API. 2) Verified - A verified level of pay as you go usage once you have credit card on file. I’ve taken the common elements across these plans and described them in a YAML format which allows me to remix the elements into the two plans I currently have, while also allowing me to reuse them for possible future plans, helping keep my approach consistent. I’m using the plan elements in this YAML file to generate the plans and pricing page for each API. Generating two separate plan boxes, with the details, and elements of each plan. I keep all the moving parts of each plan defined as separate fields and collections so that I can reuse in any new plans. I also make use of the individual elements in comparison charts, and other pricing and plan related resources through an APIs portal. The specification isn’t perfect, but it provides me a starting point for considering how I make my API plans and pricing...[<a href="/2017/11/17/my-basic-yaml-for-starter-api-plans/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/17/api-management-is-about-awareness-and-control-over-our-digital-resources/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/17/api-management-is-about-awareness-and-control-over-our-digital-resources/">API Management Is About Awareness And Control Over Our Digital Resources</a></h3>
			<p><em>17 Nov 2017</em></p>
			<p>I’ve been diving into the fundamentals of API management as part of several projects I am working on. I am setting up API management for a single API project, as well as thinking through API management practices across many API implementations in a single industry. I also just had lunch with a friend at an API startup I work with who is looking to invest in me doing some further research and storytelling when it comes to API management. All of this is providing me with a great opportunity to step back and think about API management from the small detailed moving parts, all the way up to the industry, regulatory, and macro levels of managing digital resources online. API management is the oldest aspect of my research, and one I still think is one of the most critical aspects of doing APIs in my opinion. While there are many features modern API management brings to the table, the core of it is all about allowing consumers to sign up to access some data, content, media, or algorithm. Each consumer receives a set of keys that will identify and allow for their access to be measured, which then allows the owners or stewards of digital resources to develop awareness around who is accessing a resource, and what they are doing with it. Some call it security, others analytics, but I see it about developing an awareness and asserting control over our digital resources. If you are focused on monetization, API management is about generating revenue. If you are worried about who has access to your digital assets, API management is about security. If you are doing API management right you realize it is about being aware of the digital resources you have, working to make sure they are well defined, and are tuned into your API management dashboard to understand how they are being used (or not used). I feel like this has been one...[<a href="/2017/11/17/api-management-is-about-awareness-and-control-over-our-digital-resources/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/16/you-thinking-i-mean-rest-when-i-say-api-is-about-your-limited-views-not-mine/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/kin-chesapeake-sun_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/16/you-thinking-i-mean-rest-when-i-say-api-is-about-your-limited-views-not-mine/">You Thinking I Mean REST When I Say API Is About Your Limited Views, Not Mine</a></h3>
			<p><em>16 Nov 2017</em></p>
			<p>I’m fascinated by the baggage people bring to the table when engaging in discussions around technology with me. A common opener for many conversations with season technologists centers around REST not penciling out as everyone thought, failing to be the catch-all solution, and will quickly move to how I feel about some new technology (GraphQL, gRPC, Kafka, other) making my work irrelevant. I wish I had some quick phrase to help folks understand how this line of questioning demonstrates their extremely limiting views of the tech sector, as well as my work with APIs, but alas I find silence usually does the job in these situations–allowing everyone to quickly move. For me, application programming interface, or API, is all about finding the right interface for programming against for a specific application. I’d say the closest things that anchors my belief system to REST, is that I tend to focus on leveraging the web when it comes to defining the web because it is low coast, usually well known, and avoids reinventing the wheel. I’m not a RESTafarian, and you will not find me online arguing the finer details of REST over other approaches. It just isn’t my style, and I leave it to ya’ll to work out these finer details, and share the stories about what is working, and what is not working in your operations. Your assumptions around what APIs means to me demonstrates your limited views, only partially because of the technological underpinnings. The technical details of API is only 1/3 of the equation for me, and the majority of my research and storytelling focuses on the business and politics of doing APIs, but I’m guessing you aren’t aware of this. I find that the technology definitely sets the tone for API implementations and conversations, but the ones that actually make a significant impact always transcend the technology, and help acknowledge, and understand the other aspects of operating online which is making doing...[<a href="/2017/11/16/you-thinking-i-mean-rest-when-i-say-api-is-about-your-limited-views-not-mine/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/16/three-stripe-openapi-vendor-extensions/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripes-openapi-vendor-extension.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/16/three-stripe-openapi-vendor-extensions/">Three Stripe OpenAPI Vendor Extensions</a></h3>
			<p><em>16 Nov 2017</em></p>
			<p>As part of my work on my OpenAPI toolbox I am keeping an eye out for how leading API providers are using OpenAPI. One layer of this part of my research is understanding how teams are extending the OpenAPI specification, while also encouraging other companies to understand that they can extend the specification in the first place. I’m always surprised how many people I come across that say they do not use the specification because it doesn’t do everything they need. I alternatively feel like it is my responsibility to understand what the spec can do, and then bend it to do what I need it to using vendor extensions. I have been studying how payment provider Stripe has been crafting their OpenAPI throughout the week, while also understanding how they are applying it across their platform operations. As part of their Github repository for managing the Stripe OpenAPI they share three vendor extensions they are using to evolve what is possible with OpenAPI: x-expandableFields - Resources include an x-expandableFields that contains a list of fields that are expandable by making an API request with an expand parameter. See expanding objects. x-polymorphicResources - Some API responses are “polymorphic” in that they might return multiple types of resources which is a case that OpenAPI can’t handle. In these cases the spec will reference a “synthetic” resource which is an aggregate of the properties common to all the possible resources. It will also include the field x-polymorphicResources which references those resources more precisely. x-resourceId - Resources include x-resourceId which is a canonical name for each resource. It can be used in conjunction with openapi/fixtures{2,3}.{json,yaml} to look up a sample representation (otherwise known as a “fixture”) of the resource. Some interesting extensions. The expandable fields, and resource id is pretty straight forward, but the polymorphic resources opens up some interesting questions when it comes to API design. It makes me want to learn more about the how and...[<a href="/2017/11/16/three-stripe-openapi-vendor-extensions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/16/the-information-you-get-when-allowing-developers-to-sign-up-for-an-api-using/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-authentication-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/16/the-information-you-get-when-allowing-developers-to-sign-up-for-an-api-using/">The Information You Get When Allowing Developers To Sign Up For An API Using</a></h3>
			<p><em>16 Nov 2017</em></p>
			<p>I’m a big Github user. I depend on Github for managing all my projects, and Github Pages for the presentation layer around all my research. When anything requires authentication, whether for accessing an API, or gaining access to any of my micro apps, I depend on Github authentication. I have a basic script that I deploy regularly after setting up a Github OAuth application, which I use to enable authentication for my API portals and applications, handling the OAuth dance, and returning me the information I need for my system. After a user authenticates I am left with access to the following fields: id, avatar_url, gravatar_id, url, html_url, followers_url, following_url, gists_url, starred_url, subscriptions_url, organizations_url, repos_url, events_url, received_events_url, type, site_admin, name, company, blog, location, email, hireable, bio, public_repos, public_gists, followers, following, created_at, updated_at, private_gists, total_private_repos, owned_private_repos, disk_usage, collaborators, two_factor_authentication, and plan. Not all these fields are filled out, and honestly I don’t care about most of them for my purposes, but it does provide an interesting look at what you get from Github, over a basic email and password approach to authentication. I’m just looking for any baseline information to validate someone is a human being when signing up. Usually a valid email is this baseline. However, I prefer some sort of active profile for a human being, and have chosen Github as the baseline. When anyone signs up I also quickly calculate some other considerations regarding how long they’ve had a Github account, how active it is, and some numbers regarding this history and activity. I don’t expect everyone to have a full blown public Github profile like I do, but if you are looking to use on of my APIs, or API-driven micro tools I’m looking for something more than just a valid email–I want some sign of life. I will be evolving this algorithm, and enforcing it in different ways at different times. I always hesitate using Github as the default login for...[<a href="/2017/11/16/the-information-you-get-when-allowing-developers-to-sign-up-for-an-api-using/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/16/stripe-elements-and-how-we-organize-our-api-embeddables/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripe-elements-grey-embeddable.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/16/stripe-elements-and-how-we-organize-our-api-embeddables/">Stripe Elements And How We Organize Our API Embeddables</a></h3>
			<p><em>16 Nov 2017</em></p>
			<p>I am setting up Stripe for a client, and I found myself browsing through Stripe Elements, and the examples they have published to Github. If you aren’t familiar, “Stripe Elements are pre-built rich UI components that help you build your own pixel-perfect checkout flows across desktop and mobile.” I put Stripe Elements into my bucket of API embeddables, which overlaps with my API SDK research, but because they are JavaScript open up a whole new world of possibilities for developers and non-developers, I keep separate. Stripe.js and supporting elements provides a robust set of solutions for integrating the Stripe API into your website, web or mobile application. You can choose the pre-made element, customize as you see fit, or custom build your own using the Stripe.js SDK. It provides a great place to start when learning about Stripe, reverse engineering some existing solutions, and figuring out what integration will ultimately look like. In my scenario, the default Stripe element in their documentation works just fine for me, but I couldn’t help but playing with some of the others just to see what is possible. You can tell Stripe has invest A LOT into their Sripe.js SDK, and the overall user experience around it. It provides a great example of how far you can go with embeddable API solutions. I like that they have the Stripe Elements published to Github, and available in six different languages. As I was learning and Googling, I came across other examples of Stripe.js deployment on other 3rd party sites, making me think it would be nice if Stripe had a user generated elements gallery as part of their offering, accepting pull requests from developers in the Stripe community. It wouldn’t be that hard to come up with a template markdown page that developers could fill out and submit, sharing their unique approach to publishing Stripe Elements. Having a Github repository to display example embeddable API tooling makes sense, and is...[<a href="/2017/11/16/stripe-elements-and-how-we-organize-our-api-embeddables/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/15/twitter-finally-begins-to-monetize-their-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/twitter/TwitterPremiumAPIs.gif" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/15/twitter-finally-begins-to-monetize-their-apis/">Twitter Finally Begins To Monetize Their APIs</a></h3>
			<p><em>15 Nov 2017</em></p>
			<p>It has been a long time coming, but Twitter has finally started charging for premium access to their APIs. Until now, you could only access data via the free Twitter API with limitations, or pay to use Gnip at the enterprise level–nothing in between. I’ve long complained about how Twitter restricts access to our tweets, as well as the lack of transparency and honesty around their business model. I’ve complained so much about it, I eventually stopped writing about it, and I never thought I’d see the day where Twitter starts charging for access to their platform. While I have concerns about Twitter further limiting access to our data by charging for API access, their initial release has some positive signs that give me hope that they are monetizing things in a sensible way. They seem to be focused on monetizing some of the key paint points around Twitter API consumption, like being able to get more access to historical data, offer more Tweets per request, higher rate limits, a counts endpoint that returns time-series counts of Tweets, more complex queries and metadata enrichments, such as expanded URLs and improved profile geo information. Twitter seems to be thinking about the primary pain we all experience at the lower rungs of Twitter access, and focusing on making the platform more scalable for companies of all shapes and sizes–which has been core to my complaints. Twitter even provides a quote from a client which highlights what I’ve been complaining about for some time about the inequity in Twitter API access: I wish these premium APIs were available during our first few years. As we grew, we quickly ran into data limitations that prevented expansion. Ultimately, we raised a round of funding in order to accelerate our growth with the enterprise APIs. With the premium APIs, we could have bootstrapped our business longer and scaled more efficiently. - Madeline Parra, CEO and Co-Founder, Twizoo (now part of Skyscanner)...[<a href="/2017/11/15/twitter-finally-begins-to-monetize-their-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/15/headless-cms-and-the-api-evolution-beyond-wordpress/"><img src="https://s3.amazonaws.com/kinlane-productions2/headless/headless-cms-brackets.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/15/headless-cms-and-the-api-evolution-beyond-wordpress/">Headless CMS And The API Evolution Beyond WordPress</a></h3>
			<p><em>15 Nov 2017</em></p>
			<p>I am a fan of what WordPress has done for the online world. I feel like it has enabled a lot of folks to take some control over their web presence, and in some situations even made programmers out of business people who never thought that is what they’d end up doing. Even with all the positive benefits of WordPress, it has had some significant negative side effects which I think warrant us to begin looking beyond the existing ecosystem–something I’m hoping the headless CMS, and static website movement can help fuel. I’m not anti-WordPress, but I think the movement has run its course, and we can do better when it comes to helping folks take control over their web presence, as well as avoid much of the security challenges we experience as a result of WordPress. If you aren’t familiar with the concept of headless, it is just about doing APIs, but centered around the end deliverable–the application. Headless focuses on decoupling content for use in apps, websites, or any other data-driven projects, allowing content to be created and managed independently from where it’s used. To us API-aware folks this is how All applications should behave, but I feel like the headless CMS concept is an important API gateway for business users who have drank the WordPress kool-aid, and are looking to do more with their CMS, and break free of some of the challenges of operating exclusively in a WordPress state of mind. The most damaging aspect of WordPress I have felt is it’s emphasis on the blog. Everything is centered around the blog with WordPress installs, which isn’t the reason many folks should be doing a website in the first place, but because of their platform they feel compelled to. Headless CMS can be about managing ANY content, and crafting an API backend, that can deliver exactly the content you need in any website, web or mobile backend, bypassing the concept of...[<a href="/2017/11/15/headless-cms-and-the-api-evolution-beyond-wordpress/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/15/form-posts-as-gateway-for-showing-people-they-can-program-the-web-using-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/forms/contact-form.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/15/form-posts-as-gateway-for-showing-people-they-can-program-the-web-using-apis/">Form Posts As Gateway For Showing People They Can Program The Web Using APIs</a></h3>
			<p><em>15 Nov 2017</em></p>
			<p>I am always looking for new avenues to help on-board folks with APIs. I’m concerned that folks aren’t quite ready for the responsibility that comes with a programmable web, and I’m looking for ways to help show them how the web is already programmable, and that APIs can help them take more control over their data and content online. A significant portion of my low hanging fruit API work centers around the forms already in use across websites, and how these forms are a doorway for data, and content that should also be available via an API. If information is already available on your website, and being gathered or displayed in response to a form on your website, it is a great place to start a conversation around providing an API that delivers the same functionality. Sometimes forms drive website searches, and act as a way to gather some data before presenting results–providing a good example of a GET API. In other situations forms act as an input for data, such as a contact form, or survey response. In these scenarios, forms provide a good example of a writable, or POST API path–allowing data and content to be added into a system. I’m always pointing out that if data is displayed in tables, or accessible through a form submission on a website, this is where you should start with readable APIs. The same holds true for form submissions that gather data, being where folks should bet starting with writable APIs. I feel like contact, messaging, and survey forms are all good examples of how companies, organizations, institutions, and government agencies can begin with write APIs. Business users get the concept of a form, and even its submission via a POST on the web. It is a great place to start the conversation with folks about having APIs that don’t just deliver information, but also accept new information using the web. I’m thinking about how I...[<a href="/2017/11/15/form-posts-as-gateway-for-showing-people-they-can-program-the-web-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/15/deploy-low-hanging-fruit-rogue-api-portals-for-those-who-are-behind-the-curve/"><img src="https://s3.amazonaws.com/kinlane-productions2/low-hanging-fruit/api-evangelist-low-hanging-fruit-story-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/15/deploy-low-hanging-fruit-rogue-api-portals-for-those-who-are-behind-the-curve/">Deploy Low Hanging Fruit Rogue API Portals For Those Who Are Behind The Curve</a></h3>
			<p><em>15 Nov 2017</em></p>
			<p>&lt;p&lt;/p&gt;The concept of rogue APIs isn’t anything new. Instagram started out as a rogue API, and many leading platforms who are less than open with their platforms have rogue APIs. They are usually APIs that have been reverse engineered from mobile applications, and published to Github for other developers to use. I’m looking to marry this concept with my low hanging fruit API work, where I help organizes start their API journey using data and content that is already on their website. Meaning, if it is already available on the web as table, form, or as CSV, spreadsheet, or other machine readable fie, it should be available via an API. As APIs are just the next step in the evolution, this is the logical place for the API journey to begin for many companies, organizations, institutions, and government agencies. I’ve spidered the entire web site of organizations to extract lists of data sources they should be turning into APIs. I’ve done this at the request of the website owner, as well as without the permission. Honestly, it provides a pretty compelling look at the digital presence for an organization when you harvest raw data like this and publish to a Github repository. It isn’t a view that every organization is ready for, or has thought about. Making it an even more important place for organizations to start their API journey. APIs aren’t just about providing access to your data and content for your partners and 3rd party developers, it is about getting a handle on your digital assets, and how you present and provide access to this digital representation of your organization–something many suck at profoundly. I’d like to invest more cycles into my low hanging fruit API research. I’d love to take some government agencies and not just identify the low hanging fruit, but actually deploy a rogue API portal, and hang some of the APIs there. I’d like to do this to a...[<a href="/2017/11/15/deploy-low-hanging-fruit-rogue-api-portals-for-those-who-are-behind-the-curve/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/14/the-seo-benefits-of-publishing-your-api-operations-to-github/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-api-evangelist.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/14/the-seo-benefits-of-publishing-your-api-operations-to-github/">The SEO Benefits Of Publishing Your API Operations To Github</a></h3>
			<p><em>14 Nov 2017</em></p>
			<p>I’ve been operating 100% of my public presence for API Evangelist on Github for almost five years now. I really like the public layer of my world being static, but I also like the modularity that using Github repos for my projects have injected into my workflow. API Evangelist runs as almost 100 separate Github repositories, all using a common Jekyll template for the UI, making it look like you are always on the same API Evangelist website. Any website, application, data, or API begins as a Github repository in my world, and grows from there depending on how much energy I give a project during my daily work. When I first started doing all of this, I worried a little bit about the search engine optimization of my public websites. From what I could tell in 2013, my sites ranked lower after the switch, but since I’m not in this for the numbers game, I shrugged it off. However, in 2017 the numbers look different, and some of the projects I’ve been cultivating on Github actually rank pretty high, even with minimal optimization on my part. This isn’t just the web front-end for my projects–I am also seeing the Github repositories themselves showing up pretty prominently in search engine results. All of this is anecdotal. I haven’t done any official measurements or testing on the topic, I just don’t care enough to invest that amount of work in it all. I just have to note that in the last year I am seeing significant benefit for my SEO by running things on Github. When you bundle this with the search and discovery opportunities via Github, the benefits to running an API project on Github as much as possible makes sense. It is something I encourage all of my clients who are operating public APIs consider as part of their overall marketing, communications, and evangelism strategy. I’ve been profiling all the possible ways that an...[<a href="/2017/11/14/the-seo-benefits-of-publishing-your-api-operations-to-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/14/i-added-a-simple-bulk-api-for-my-human-services-data-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-referral/human-services-data-bulk-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/14/i-added-a-simple-bulk-api-for-my-human-services-data-api/">I Added A Simple Bulk API For My Human Services Data API</a></h3>
			<p><em>14 Nov 2017</em></p>
			<p>The core Human Services Data API allows for adding of organizations, locations, services, and contacts one by one using a single POST on the core API paths for each available resource. However, if you want to add thousands, or even hundreds of records, it can quickly become cumbersome to submit each of the calls, so I wanted to introduce a simple Human Services Bulk API for helping handle the adding of large quantity of data, on a one-time, or recurring basis. I know there job queuing solutions available out there, but my goal with this project is to focus on the API definition, as well as the backend system(s). For this round, I just want to get a simple baseline definition in place, with a simple API backend for orchestrating. I’ll update to support AWS, and other queuing solutions as part of the road-map–further hammering out a consistent HSDA Bulk API. The first dimension of this new HSDA Bulk API focuses on providing paths for POSTing large quantities of data across the core human service resources: organizations/ - POST complete organizations JSON records as array. locations/ - POST complete locations JSON records as array. services/ - POST complete services JSON records as array. contacts/ - POST complete contacts JSON records as array. You can submit as many records to each of these paths (well, within reason), including the sub-resources for each object like physical address, phones, etc. When POSTed each record doesn’t immediately go into the main HSDA database. Each entry is entered into a jobs system, which can be run on a schedule, based upon events, or maybe just wait until the middle of the night. The goal is to offload the bulk insert to a job system, which can spread things out over time, and minimize negative impact on resources strapped human services database. HSDA Bulk API runs as a separate microservice which can be run side by side with the core HSDA...[<a href="/2017/11/14/i-added-a-simple-bulk-api-for-my-human-services-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/14/glitch-is-where-you-will-learn-the-essential-human-side-of-operating-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/glitch/1_-GNpo5PEhPm-1Ns4F76t9w.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/14/glitch-is-where-you-will-learn-the-essential-human-side-of-operating-your-api/">Glitch Is Where You Will Learn The Essential Human Side Of Operating Your API</a></h3>
			<p><em>14 Nov 2017</em></p>
			<p>The biggest deficiency I see in the world of APIs is an ability to understand the human side of what we are all doing. The space is dominated by men, and people who have an understanding of, and deep belief in technology, over that of humans. The biggest problems APIs face across their life cycle is humans, and increasingly one of the biggest threats to humans is an API (ie. Twitter API automation &amp; harassment, IoT device exploitation, Facebook advertising, etc.) APIs encounter human friction because their creators didn’t anticipate the human portion of the equation, and APIs often get used against humans because their creators again didn’t anticipate human nature, and how people might use their technology for doing harmful things. I rarely see folks in the API sector focusing on the human side of the equation, but I am pleasantly surprised to see a constant drumbeat coming out of Glitch, “the friendly community where you’ll build the app of your dreams.” Glitch is a platform where API consumers can remix apps that use APIs, and API providers can engage with API consumers who are building and remixing interesting things. Glitch has been on my list to write about more, and is something I’ll be using, and focusing more time on in future posts, but I wanted to just highlight how much focus is spent on the human side of the API world over at Glitch. Take a look at the articles coming out of the Glitch blog, Dev Rel success requires an ongoing connection to a community of peers, and Dev Rel must be supported with ongoing investment in professional development–all part of the ongoing stories around a Developer Bill of Rights, which Glitch has been very vocal about, emphasizing the importance of the human aspects of doing APIs and building applications. Which is the first startup I’ve seen come along that is investing so much energy into discussing what really makes all...[<a href="/2017/11/14/glitch-is-where-you-will-learn-the-essential-human-side-of-operating-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/14/could-i-please-get-an-api-discovery-tool-that-evaluates-an-openapi-diff/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/14/could-i-please-get-an-api-discovery-tool-that-evaluates-an-openapi-diff/">Could I Please Get An API Discovery Tool That Evaluates An OpenAPI Diff</a></h3>
			<p><em>14 Nov 2017</em></p>
			<p>I am increasingly tracking on OpenAPI definitions published to Github by leading API providers I track on. Platforms like Stripe, Box, New York Times are actively managing their OpenAPI definitions using Github, making them well suited for integration into their platform operations, API consumer scenarios, and even within analyst systems like what I have going on as the API Evangelist. Once I have an authoritative source of an OpenAPI, meaning a public URI for an OpenAPI that is actively being maintained by the API provider, I have a pretty valuable feed into the roadmap, as well as change log for an API. I feel like we are getting to the point where there are enough authoritative OpenAPIs that we can start using as a machine readable notification and narrative tool for helping us stay in tune with one or many APIs across the landscape. Helping us stay in tune with APIs in real-time, and giving APIs an effective tool for communicating out changes to the platform–we just need more OpenAPIs, and some new tooling to emerge. I’m envisioning an OpenAPI client that regularly polls OpenAPIs and caches them. Anytime there is a change it does a diff, and isolated anything new. Think of an RSS reader, but for OpenAPIs, and going well beyond new entries, and actually creates a narrative based upon the additions and changes. Tell me about the new paths added, and any new headers, parameters, or maybe how the schema has grown. Provide me insights on what has changed, and possibly what has been removed, or will be removed in future editions. As an API analyst, I’d like to be able to have an OpenAPI-enabled approach to receiving push notifications when an API changes, with a short, concise summary about what has change in my inbox, via Twitter, or Github notification. OpenAPI already provides API discovery features through the documentation it generates, and I’m increasingly using Github to find new APIs after...[<a href="/2017/11/14/could-i-please-get-an-api-discovery-tool-that-evaluates-an-openapi-diff/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/12/stripes-openapi-is-available-on-github-in-version-3-0/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripes-openapi-specification-on-github.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/12/stripes-openapi-is-available-on-github-in-version-3-0/">Stripes OpenAPI Is Available On Github In Version 3.0</a></h3>
			<p><em>12 Nov 2017</em></p>
			<p>I can’t write about every API provider who publishes their OpenAPI to Github, there are just too many. But, I can write about the rockstar API providers who do though, and showcase what they are doing, so I can help influence the API providers who have not started publishing their OpenAPIs in this way. If you are looking for a solid example of a leading API provider publishing their OpenAPI to Github, I recommend taking a look at the payment provider Stripe. Their repository contains OpenAPI specifications for Stripe’s API, with multiple files available in the in the openapi/ directory: spec3.{json,yaml} - OpenAPI 3.0 spec. spec2.{json,yaml} - OpenAPI 2.0 spec. We’re continuing to generate this for now, but it will be deprecated in favor of spec3. fixtures3.{json,yaml} - Test fixtures for resources in spec3. See below for more information. fixtures2.{json,yaml} - Test fixtures for resources in spec2. It is pretty exciting to see them already embracing version 3.0. They even provide a listing of the OpenAPI vendor extensions they are using, which are specific to their API. I’ll be adding these to my OpenAPI toolbox when I have the time, adding to the number of vendor extensions I have indexed. Stripe provides another pretty solid example of an API provider taking ownership of their OpenAPI spec, publishing to Github for their consumers to put tow rok, but clearly they are also using as part of their own internal workflows as well. Every API provider should have a Github repository with an up to date OpenAPI like Stripe does. I know many API architects envision a hypermedia API discovery landscape, where APIs are defined and discoverable by default, but I think an OpenAPI on Github is the best we can hope for at this stage in the evolution of the space. With the momentum I’m seeing in the number API providers publishing their OpenAPIs to Github, I’m feeling like Github is going to become the continuous...[<a href="/2017/11/12/stripes-openapi-is-available-on-github-in-version-3-0/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/12/locking-up-any-open-data-taxonomy-is-short-sighted-in-todays-online/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/open-referral/211tax.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/12/locking-up-any-open-data-taxonomy-is-short-sighted-in-todays-online/">Locking Up Any Open Data Taxonomy Is Short Sighted In Todays Online</a></h3>
			<p><em>12 Nov 2017</em></p>
			<p>I published a taxonomy API as part of my Human Services Data API (HSDA) work recently, and as part of the work I wanted it to support a handful of the human services taxonomies available currently. The most supported taxonomy available out there is the AIRS/211 LA County Taxonomy. It is a taxonomy in use by 211 of LA County, as well as owned and licensed by them. From what I gather, it is the most common format in use, and you can find licensing pages for it from other municipal 211 providers. Before you can download a copy of the taxonomy you have to agree to the license I’ve posted at the bottom of this post, something I was unwilling to do. Taxonomies shouldn’t be locked up this way. Let alone taxonomies for use in open data, helping citizens at the municipal level. I understand that 211 LA will argue that they’ve put a bunch of work into the schema, and therefore they want to protect what they view their intellectual property, but in 2017 this is wrong. This isn’t the way things should be done, sorry. The AIRS taxonomy should be openly available, and reusable in a machine readable format, and evolved by an open governance process. There is no reason for this valuable taxonomy, that has the potential to make our cities better, should be locked up like this–it needs to be widely used, and adopted without any legal friction along the way. I understand that it takes work, and resources to keep a taxonomy meaningful, and usable, but we should not stand in the way of people finding human services, and restricting 211 providers from using the same vocabulary. There are other was to generate revenue, and evolve forward a taxonomy in an online, collaborative environment, much like we are currently doing with open source software. This kind of stuff drives me nuts, and the licensing around this important technology is...[<a href="/2017/11/12/locking-up-any-open-data-taxonomy-is-short-sighted-in-todays-online/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/10/you-can-lead-a-horse-to-water-but-you-cannot-make-them-drinkthe-api-edition/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/bluelake/clean_view/file-00_00_38_67.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/10/you-can-lead-a-horse-to-water-but-you-cannot-make-them-drinkthe-api-edition/">You Can Lead A Horse To Water But You Cannot Make Them Drink--The API Edition</a></h3>
			<p><em>10 Nov 2017</em></p>
			<p>I have seven years of API research available at apievangelist.com. I regularly publish short form, and long form versions of this information on my blogs on a weekly basis. I publish prototypes, demo websites and portals, and develop API training curriculum for use across a wide variety of industries. I regularly take versions of my API research, and rework, rebrand, and dial in to speak to a specific company, organizations, institution, agency, or industry. In many cases I make this information freely available, helping make sure it is available to those who need it. Despite all this work, many folks who are already doing APIs refuse to read, listen, and learn from what is already going on in the API space, and doomed to repeat the mistakes many of us have already made and learned from in our API journeys. Many folks don’t really understand my motivations and think I have some sort of agenda to sell them something, disrupt their current reality, or other uninformed perspective. Ultimately, not trusting what I’m putting out there. I guess viewing that the water is poised in some way. Others don’t feel they need it, either because they feel like they have all the answers, or the problems haven’t become a reality in their worlds yet, so my solutions seem irrelevant. I find it tough to argue with someone about preventative care when it comes to their API operations, when they spend their days triaging bugs, problems, and legacy technology challenges. They are fire fighters, water isn’t for drinking! A long standing example of this can be found in the hypermedia realm. No matter how much some very smart people, with a wealth of experience deploying and managing APIs warn about challenges with maintaining API SDKs and clients, some folks will never see it as a problem until they actually face it themselves. I can showcase endless numbers of healthy practices extract from companies like AWS, Twitter, and...[<a href="/2017/11/10/you-can-lead-a-horse-to-water-but-you-cannot-make-them-drinkthe-api-edition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/10/i-can-keep-evangelizing-the-same-api-stories-for-the-next-decade-in-government/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/34_33_700_500_0_max_0_1_1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/10/i-can-keep-evangelizing-the-same-api-stories-for-the-next-decade-in-government/">I Can Keep Evangelizing The Same API Stories For The Next Decade In Government</a></h3>
			<p><em>10 Nov 2017</em></p>
			<p>I spoke on a panel at the Red Hat, Fed Scoop Government Symposium in Washington D.C. yesterday. I had some great conversations with technology vendors, as well as government agencies about everything API. I enjoy being outside the Silicon Valley echo chamber when it comes to technology because I enjoy helping folks understand the basics of what is going on with the basics of APIs, over getting too excited over the latest wave of new technology, and a constant need to be moving forward before ever getting a handle on the problems on the table. It can be hard to to repeat some of the same stories I’ve been telling for the last seven years while in these circles, but honestly the process helps me refine what I’m saying, and continue to actively think through the sustained relevancy of the stories I’ve been telling. After this round of discussions in D.C. I feel there are a some themes in my work, I can keep refining, and crafting stories for sharing in the government space. Open - I know its a tired term, but learning to be more open with other agencies, partners, and the public is an essential component of doing APIs in the federal government. Documentation - Do not reinvent the wheel with documentation, and leverage OpenAPI to help you keep documentation usable, up to date, and valuable to developers using existing open source API documentation solution. Support - Provide email, office hours, Twitter, ticketing, Github issues, and other common support building blocks for API consumers, making sure people know they can get help when they need. Communication - Talk to your consumers. Have a blog, Twitter account, and other social channels for communicating internally, with partners, and publicly with API consumers. Experiment - See your APIs as an R&amp;D extension of an agency, and allow for experimentation with APIs, as well as the consumption of the APIs. Think about sandboxes, data virtualization, and...[<a href="/2017/11/10/i-can-keep-evangelizing-the-same-api-stories-for-the-next-decade-in-government/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/10/are-people-ready-for-an-online-apidriven-world-that-is-programmable/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/van-gogh-starry-night-container-bridge-2.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/10/are-people-ready-for-an-online-apidriven-world-that-is-programmable/">Are People Ready For An Online API-Driven World That Is Programmable?</a></h3>
			<p><em>10 Nov 2017</em></p>
			<p>I am struggling with helping some folks get beyond their API being just readable, and helping them understand the potential of having POST, PUT, and other writable aspects to their resources, making things much more programmable. My client has a firm grasp on the ability to GET data from their API and publish on websites. They also have the concept that they can GET other data from other 3rd party APIs, and display on their website alongside their data. Where they are struggling is that they can also add new data to their API, and update existing data they are making available via their API, and ultimately their website as well. This hurdle isn’t limited to any single project I’m working on. I find a number of people who seem to have a decent grasp on APIs in general, struggling with or completely avoiding conversations around making the data writeable. They are able to make the transition from web to API when it comes to retrieving data, but making the same jump when it comes to adding and updating data is proving to be more difficult. I think there will always be a cognitive load with jumping from read to write, as you have to think more about security, data quality, and other common concerns. However, beyond that, I’m trying to explore what might be the challenges people are facing. Many of the folks I’m working with are a bit shaky on their grasp of APIs, and aren’t too confident in sharing what they don’t understand. As I do, I’ll put it out to the universe and ask my audience what they’ve seen. On the surface, I’d say that adding or updating data into a database online is tougher to wrap your head around without some context, and some of the affordances we enjoy in the browser. Adding a Tweet through mobile application or website? No problem. POST a tweet through API, is a little...[<a href="/2017/11/10/are-people-ready-for-an-online-apidriven-world-that-is-programmable/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/10/admitting-there-is-so-much-i-do-not-understand-makes-be-better-at-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/server-racks-clouds_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/10/admitting-there-is-so-much-i-do-not-understand-makes-be-better-at-apis/">Admitting There Is So Much I Do Not Understand Makes Be Better At APIs</a></h3>
			<p><em>10 Nov 2017</em></p>
			<p>One of the reasons I’m so good at APIs is because I embrace how little I know. This rolling realization keeps my appetite wet when it comes to learning to things, and working hard to discover, and realize sensible API practices. I am comfortable with the fact that I do not know something. I enjoy coming up against things I do not understand, eager to learn more. However, I think there is one big difference in the way I approach technology from other developers, is that I’m not confident that I will ever be able to fully understand a particular domain, let alone think that technology, or specifically APIs are a solution to a specific set of problems within every domain. Many developers are overly confident in what they know. They are also overly confident in their ability to learn new things. They are also overly confident that they can hammer out a technological solution that will solve all problems within a domain. I feel like many technologists aren’t in the game to learn, they are in the game to prove they have the chops to solve problems, and when they can’t they just walk away. When you approach APIs in this way you are leaving a lot of opportunity for learning and growth on the table. APIs shouldn’t be seen as simply a solution. APIs are just a tool (like the web) in a business toolbox, that should be applied when appropriate, and not applied when it doesn’t make sense. Beyond developers, I feel like many business users are scared off by the uncertainty in the world of APIs. They don’t thrive in an environment where there are so many possibilities, configurations, and ways to do things right and wrong. APIs give you more control over your data, content, and algorithms, allowing you to provide access to them in many ways, and reach across many client channels like the web, mobile, and other device...[<a href="/2017/11/10/admitting-there-is-so-much-i-do-not-understand-makes-be-better-at-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/09/using-apis-to-manage-my-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-on-sea_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/09/using-apis-to-manage-my-apis/">Using APIs To Manage My APIs</a></h3>
			<p><em>09 Nov 2017</em></p>
			<p>I’m going further down the AWS rabbit hole lately with my APIs. Historically my APIs ran on an AWS EC2 instance with leveraged Linux for the OS, Apache for the web server, and Slim for the RESTful framework of my APIs–all with an RDS MySQL backend. I’ve now evolved the EC2 instance to be spread across numerous AWS Lambda scripts, tied together into various stacks of APIs using AWS API Gateway. At first, I was hesitant to go further down the AWS rabbit hole, but the security benefits of AWS-driven solutions, as well as the API-driven aspects of operating my APIs, is slowly shifting my view of how I need to be managing my APIs. AWS RDS, Lambda, and API Gateway all have APIs. I’ve been spending the week developing Lambda scripts that help me manage my APIs, using the AWS APIs behind these three services, leveraging them to setup, configure, deploy, manage, and test my APIs. I enjoy how APis push me to think about my digital resources, and when my digital resources are APIs, the benefts begin to feel like API inception. I’m increasingly having APIs that do one thing and do it well, when it comes to API operations, allowing me to distill down the building blocks of my API operations, into a very workable world of API functionality. I am now deploying, backing up, migrating, and working with the database behind my APIs using APIs. I primarily use AWS RDS MySQL instances behind my APIs, but when I’m using AWS DynamoDB, I leverage AWS APIs even more, as DynamoDB lets you do adds, updates, queries, and deletes using the API, elevating beyond SQL to manage the contents of each data store. Whether it is RDS or DynamoDB, I’m using APIs manage the operational side of each database behind the API, and I’m beginning to explore how to make my database more ephemeral, and scalable using AWS APIs, giving me more control...[<a href="/2017/11/09/using-apis-to-manage-my-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/09/the-open-web-application-security-project-owasp-and-api-security/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/security/open-web-application-security-project-owasp.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/09/the-open-web-application-security-project-owasp-and-api-security/">The Open Web Application Security Project (OWASP) And API Security</a></h3>
			<p><em>09 Nov 2017</em></p>
			<p>This is a story from my latest API Evangelist API security industry guide. My partner ElasticBeam has underwritten my API security research, allowing me to publish a formal PDF of my guide, providing business and technical users with a walk-through of the moving parts, tools, and companies doing interesting things with API security. When I publish each guide, I publish each story here on the blog, helping build awareness around my research–this is a short one on OWASP. The Open Web Application Security Project (OWASP) is a 501(c)(3) worldwide not-for-profit charitable organization focused on improving the security of software, with a mission to make software security visible, so that individuals and organizations are able to make informed decisions. OWASP is looking to provide impartial, practical information about application security (AppSec) to individuals, corporations, universities, government agencies and other organizations worldwide. Operating as a community of like-minded professionals, OWASP issues software tools and knowledge-based documentation on application security. As the web API space has expanded OWASP has expanded its focus to include the most common threats to APIs. OWASP has acknowledged the overlap between web applications, and web APIs, and quickly becoming a valuable source for API specific security knowledge, expanding beyond its web application roots. Providing one of the best resources to find security related information, and tooling you can apply throughout your API operations. OWASP doesn’t endorse commercial services, and is a member driven organization, so you will find all the information they provide to be vendor neutral, and focused on the task at hand. You will find me regularly anchoring my API security work in what the OWASP community is doing, as security should always be a team effort. API security isn’t my primary focus as API Evangelist, but helping guide you to where you can find the latest information is what this guide is about. OWASP is your source for unbiased API security information! You can download or purchase my API Evangelist...[<a href="/2017/11/09/the-open-web-application-security-project-owasp-and-api-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/09/the-impact-of-api-management-on-api-security/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/security/api-security-guide-api-management.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/09/the-impact-of-api-management-on-api-security/">The Impact Of API Management On API Security</a></h3>
			<p><em>09 Nov 2017</em></p>
			<p>This is a story from my latest API Evangelist API security industry guide. My partner ElasticBeam has underwritten my API security research, allowing me to publish a formal PDF of my guide, providing business and technical users with a walk-through of the moving parts, tools, and companies doing interesting things with API security. When I publish each guide, I publish each story here on the blog, helping build awareness around my research–this is a short one on API management. API management has done an amazing job in helping companies, organizations, institutions, and government agencies make their digital resources more available on-line in a secure way. Allowing API providers to require developers to sign up, obtain keys, and tokens which need to accompany all API calls. This, along with encryption by default has gone a long way towards making data, content, and algorithms more accessible, while also being secure. However, many API providers have stopped here, and think their resources are secure, when in reality there is so much more work to be done. Requiring all developers obtain keys to access resources, and encryption data in transit is an important part of API security, but it is just one tool in the API security toolbox. Out of API management you also receive an enhanced set of logging, analysis, and reporting tools for how developers are putting API resources to work. When done well, this pushes the API security conversation forward, allowing API providers to balance access with security, and be proactive when it comes to limiting access, or even shutting off access when their is abuse. The problem is not all API providers are investing here, let alone going beyond what API management providers offer. The awareness brought to the table my API management is valuable, but there are so many aspects of API operations at the web server, DNS, and other levels that are often left out of the API management conversation. I’ll be pushing...[<a href="/2017/11/09/the-impact-of-api-management-on-api-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/09/learning-to-play-nicely-with-others-using-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/dark-clouds-la-buildings.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/09/learning-to-play-nicely-with-others-using-apis/">Learning To Play Nicely With Others Using APIs</a></h3>
			<p><em>09 Nov 2017</em></p>
			<p>This is a topic I talk about often, write about rarely, but experience on a regular basis doing APIs. It has to do with encounters I have with people in companies who do not know how to share and play nicely with other companies and people, and want to do APIs. For a variety of reasons these folks approach me to learn more about APIs, but are completely unaware of what it takes, and how it involves working with external actors. Not all of these APIs are public, but many of them involve engaging with individuals outside the corporate firewall, possess a heavy focus on the technical, and business of doing APIs, but rarely ever consider the human and more political aspects of engagements with APIs. I find that people tend to have wildly unrealistic expectations of technology, and believe that APIs will magically connect them to some unlimited pool of developers, or seamlessly connect disparate organizations across vast distances. People come to me hoping I will be able to explain it all to them in a single conversation, or provide in a short white paper, which is a state of being that also makes individual very susceptible to vendor promises. People want to believe that APIs will fix the problems they have introduced into their operations via technology, and effortlessly connect them with outside revenue streams and partnerships, unencumbered by the same human challenges they face within their firewalls. Shortly after getting to know people it often becomes apparent that they possess a lot of baggage, legacy processes and beliefs, that they are often unaware of. Or, I guess more appropriately they are unaware that these legacy beliefs are not normal, and are something everybody faces. Where they are usually pretty uniquely dysfunctional to a specific organization. People usually have hints that these problems aren’t normal, but just aren’t equipped to operate outside their regular sphere of influence, and within days or weeks of...[<a href="/2017/11/09/learning-to-play-nicely-with-others-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/08/the-api-evangelist-api-security-industry-guide/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/security/api-evangelist-api-security-industry-guide-screenshot-2017-11-08.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/08/the-api-evangelist-api-security-industry-guide/">The API Evangelist API Security Industry Guide</a></h3>
			<p><em>08 Nov 2017</em></p>
			<p>This edition of my API security industry guide has been underwritten by ElasticBeam, who provides next generation API security, leveraging machine learning, and behavorial analysis that works with the existing web and API management solutions you already have in place across your API operations. I have been working on this resulting guide from my API security research for over a year now. Thanks to ElasticBeam I’ve finally gotten it out the door. As with all my industry guides, it is a work in progress, and something that will never be finished. I’ll keep taking what I’ve learned, and publishing in as a PDF every couple months, and receive the edits, and feedback from my readers and the wider community, then publish again. I’m feeling like I’m finally finding my groove again with these guides, and there is no better time to be back on game, especially when it comes to API security. Security is the number one concern of companies, organizations, institutions, and government agencies considering investing more resources into their API infrastructure, as well as companies who are ramping up their existing efforts. At the same time it is also the most deficient area when it comes to investment in API infrastructure by existing API providers. Many groups are rushing along their API journey, and deploying web, mobile, device, and other applications, but rarely stopping to properly secure things with each step along the way. In 2016 I began investing more into the topic of API security. I have been ramping up my research into how APIs were being secured, and how they weren’t being secured. I’ve been tracking on breaches, vulnerabilities, as well as the companies who are offering products and services that help API providers secure their APIs, as well as some of the open source tooling that is available. As I do with my approach to researching everything APIs, along the way I’m keeping notes on the common building blocks, and...[<a href="/2017/11/08/the-api-evangelist-api-security-industry-guide/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/08/i-appreciate-this-api-walk-through-from-fannie-mae-but-just-give-me-the-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/fannie-mae/fannie-mae-d-messages-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/08/i-appreciate-this-api-walk-through-from-fannie-mae-but-just-give-me-the-api/">I Appreciate This API Walk Through From Fannie Mae But Just Give Me The API!</a></h3>
			<p><em>08 Nov 2017</em></p>
			<p>I came across the new Desktop Underwriter (DU) API from Fannie Mae which provides lenders a comprehensive credit risk assessment data that determines whether a loan meets Fannie Mae’s eligibility requirements. They have a slick new website for the project, with the tag line “building on certainty”, and a smooth HTML story to walk you through what the new DU API can do. While the API seems very exciting, and valuable, the whole production is missing one thing–the API! I am sure you have to be a partner to get access to the API, but you can tell the whole things is being led by people who have never actually used an API. Otherwise you would give us an API to actually use, and allow us to kick the tires. A hallmark of modern APIs is that you get to play with it. Marketing materials, and a sharp single page application website isn’t enough. We need the documentation, and be able to actually see what the request and response structure is, so that we can better understand the value being generated, and how we will be integrating with it. Without this, there isn’t any value. Of course, you don’t have to make the real API 100% public, you can always create API access tiers, and even deploy a sandboxed or virtualized version of the API and data for new users, protecting your valuable resources–just do not hide the API away from us, and make us consumers beg for access. When you hide your APIs, you leave first impressions like you did with me. Wouldn’t it be better if my first impression was all about writing a story on how cool your API was, and how all my readers should be using it? Instead, I’m using you as a case of how to not do APIs. There is no reason the Fannie Mae Desktop Underwriter (DU) API can’t be publicly available, allowing us analysts and developers...[<a href="/2017/11/08/i-appreciate-this-api-walk-through-from-fannie-mae-but-just-give-me-the-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/08/apis-and-other-ways-of-serving-up-machine-learning-models/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/crypto-machine-bletchley_copper_circuit.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/08/apis-and-other-ways-of-serving-up-machine-learning-models/">APIs And Other Ways Of Serving Up Machine Learning Models</a></h3>
			<p><em>08 Nov 2017</em></p>
			<p>As with most areas of the tech sector, behind the hype there are real world things going on, and machine learning is one area I’ve been studying, learning, and playing withd what is actually possible when it comes to APIs. I’ve been studying the approach across each of the major cloud platforms, including AWS, Azure, and Google t push forward my understanding of the ML landscape. Recently the Google Tensorflow team released an interesting overview of how they are serving up Tensorflow models, making machine learning accessible across a wide variety of use cases. Not all of these are API specific, but I do think they are should be considered equally as part of the wider machine learning (ML) application programming interface (API) delivery approach. Over the past year and half, with the help of our users and partners inside and outside of Google, TensorFlow Serving has advanced performance, best practices, and standards for ML delivery: Out-of-the-box Optimized Serving and Customizability - We now offer a pre-built canonical serving binary, optimized for modern CPUs with AVX, so developers don’t need to assemble their own binary from our libraries unless they have exotic needs. At the same time, we added a registry-based framework, allowing our libraries to be used for custom (or even non-TensorFlow) serving scenarios. Multi-model Serving - Going from one model to multiple concurrently-served models presents several performance obstacles. We serve multiple models smoothly by (1) loading in isolated thread pools to avoid incurring latency spikes on other models taking traffic; (2) accelerating initial loading of all models in parallel upon server start-up; (3) multi-model batch interleaving to multiplex hardware accelerators (GPUs/TPUs). Standardized Model Format - We added SavedModel to TensorFlow 1.0, giving the community a single standard model format that works across training and serving. Easy-to-use Inference APIs - We released easy-to-use APIs for common inference tasks (classification, regression) that we know work for a wide swathe of our applications. To support more...[<a href="/2017/11/08/apis-and-other-ways-of-serving-up-machine-learning-models/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/08/additional-call-pricing-info-are-the-pressure-relief-valves-for-api-plans/"><img src="https://s3.amazonaws.com/kinlane-productions2/bitscoop/bitscoop-pricing-plans-additional-calls.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/08/additional-call-pricing-info-are-the-pressure-relief-valves-for-api-plans/">Additional Call Pricing Info Are The Pressure Relief Valves For API Plans</a></h3>
			<p><em>08 Nov 2017</em></p>
			<p>I’ve complained about unfair API pricing tiers several times over the last couple years, even declaring API access tiers irrelevant in a mult-API consumer world. Every time I write about this subject I get friends who push back on me that this is a requirement for them to generate revenue as a struggling startup. With no acknowledgement that their API consumers might also be struggling startups trying to scale consumption within these plans, only to reach a rung in the ladder they might not be able to actually reach. My goal in this storytelling isn’t to condemn API providers, but make them aware of what things look like from the other side, and that their argument essentially pulls up the ladder after they’ve gotten theirs–leaving the rest of us at the bottom. My complaint isn’t with startups crafting pricing tiers, and trying to make their revenue projects more predictable. My complaint is when the plans are priced too far apart and I can’t afford to move from one plan to the next. More importantly, my complaint is when the tier I can’t moved from is rate limited with a cap on usage, and I can’t burst beyond my plans limits without scaling to the next access tier which I cannot afford to reach. I understand putting hard caps on public or free tier plans, but when you are squarely in a paid access tier, you shouldn’t be shut down when you reach the ceiling. Sure, I might pay a premium for each additional call, but I shouldn’t be shut down and forced to move to the next higher access tier–which might be out of my monthly price range. I just can’t go from $49.95 to $499.95 in monthly payments as a small business, sorry. The key element that needs to be present for me, even in situations where I cannot afford to jump to the next tier, is the ability to go beyond my plans...[<a href="/2017/11/08/additional-call-pricing-info-are-the-pressure-relief-valves-for-api-plans/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/07/when-we-are-told-that-api-security-investments-will-affect-profitability/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/castle_walls_cannon_satan_red.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/07/when-we-are-told-that-api-security-investments-will-affect-profitability/">When We Are Told That API Security Investments Will Affect Profitability</a></h3>
			<p><em>07 Nov 2017</em></p>
			<p>I was listening to Mark Zuckerberg talk about how security investments will affect the platforms profitability on the Facebook earnings call this last week. This line of thinking sounds pretty consistent with what I’m hearing from other folks when it comes to why they haven’t been investing more into their API security. My challenge for this line of thought is about shutting down proactive security investments, and does not speak of responsive security investments–meaning after you’ve had a breach, or when there is other security investment. From a leadership perspective this view of security just doesn’t do it for me, and I’d push back, and require it consider what profitability will look like if we do not invest properly in security. Viewing security in this way is common. It is also a short-sighted view of security, in the name of profits today, over health of a platform down the road. It demonstrates that leadership is more focused on profits, than whatever the platform focus actually doing. I would add that I think this line of thinking reflects a perspective of leadership that is out of sync with the technical details of operating a platform, and the current threat landscape. I get that a company has to be profitable, and that it is the job of the CEO is to represent the investors, but after Equifax, and the many other breaches, as well as what I’m seeing on the ground at companies I’m talking to, it is pretty clear that things are out of whack when it comes to overall security investment. I work with a lot of folks who want to invest in API security more, but they just don’t have the resources. I’ve been in leadership roles where I’ve had my hands tied when it came to decisions around infrastructure to deliver on PCI, and other compliance, as well as being able to hire security focused talent. This type of thought regarding security practices...[<a href="/2017/11/07/when-we-are-told-that-api-security-investments-will-affect-profitability/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/07/postman-as-a-live-coding-environment-in-presentations-at-apistrat/"><img src="https://s3.amazonaws.com/kinlane-productions2/postman/postman-working-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/07/postman-as-a-live-coding-environment-in-presentations-at-apistrat/">Postman As A Live Coding Environment In Presentations At APIStrat</a></h3>
			<p><em>07 Nov 2017</em></p>
			<p>We just wrapped up the 8th edition of APIStrat in Portland, Oregon this last week. I’ll be working through my notes, and memory of the event in future posts, but thing that stood out for me was the presence of Postman at the event. No, I’m not talking about their booth, and army of evangelists and company reps on site–although this was the first time I’ve seen them out in such force. I’m talking about the usage of the API development environment by presenters, as a live coding environment in their talks, replacing the command line and browser for how you demonstrate the magic of APIs to your audience. On the first day of the conference I attended two separate workshops where Postman was the anchor for the talk. As they worked their way through their slides they kept switching back to the Postman application to show some sort of real results, from an actual, or mocked API. It is the new live coding environment for API evangelist, architects, designers, developers, and security folks. It is the quickest way to go from API concept, to demonstrating API solutions in any presentation. What I also really like is that it transcends any single programming language. In the past, I’ve always hated when someone would bust out some .NET code to show an API call, or something very language or platform specific. Postman reflects a more API way of doing things, that is elevated above the dogma of any single programming language community. I am beginning to use Postman and Restlet client in my API training and curriculum more. Directing my users to actually try something out in the API client before moving on to the next step. It is kind of becoming the new interactive API documentation, but something that is linkable from any story, training materials, or incorporated directly into a live talk. As an evangelist it is yet another reason to maintain OpenAPI definitions...[<a href="/2017/11/07/postman-as-a-live-coding-environment-in-presentations-at-apistrat/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/07/hiding-apis-in-plain-sight/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/methuselah/clean_view/file-00_00_20_99.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/07/hiding-apis-in-plain-sight/">Hiding APIs In Plain Sight</a></h3>
			<p><em>07 Nov 2017</em></p>
			<p>I’m always surprised by how secretive folks are. I know that it is hard for many folks to be as transparent as I am with my work, but if you are doing public APIs, I have a basic level of expectation that you are going to be willing to talk and share stories publicly. I regularly have conversations with enterprise folks who are unwilling to talk about what they are doing on the record, or allow me to share stories about their PUBLIC API EFFORTS!!! I get the super secret internal stuff. I’ve worked for the government. I don’t have a problem keeping things private when they should be, but the secretive nature of companies around public API efforts continues to keep me shaking my head. People within enterprise groups almost seem paranoid when it comes to people keeping an eye on what they are up to. I don’t doubt their competitors keep an eye on what they are doing, but thinking that people are watching every move, everything that is published, and will be able to understand what is going on, and be able to connect the dots is borderline schizophrenic. I publish almost everything I do public by default on Github repositories, and my readers, clients, and other folks still have trouble finding what I am doing. You can Google API Evangelist + any API topic and find what I’m working on each day, or you can use the Github search to look across my repositories, and I still have an inbox and social messaging full of requests for information. My public by default stance has done amazing things for my search engine and social media presence. I don’t even have to optimize things, and I come up for almost every search. I get regular waves of connections from folks on topics ranging from student art to the government, because of my work. The schema, API definitions, documentation, tests, and stories for any...[<a href="/2017/11/07/hiding-apis-in-plain-sight/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/07/developing-a-talent-pool-within-your-api-community/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_blue_circuit_3.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/07/developing-a-talent-pool-within-your-api-community/">Developing A Talent Pool Within Your API Community</a></h3>
			<p><em>07 Nov 2017</em></p>
			<p>There are many reasons for having an API. The direct reason is to provide your partners and 3rd party developers access to your data, content, and algorithmic resources using the web. However, there are many indirect, and less obvious reasons for having an active API program at your company, organization, institution, or government agency. Things that you probably haven’t thought of, but the groups who are already doing APIs have known about these benefits for a while. One of these benefits is in the area of talent acquisition, and building relationships with, and identifying folks with the skills you are looking for. I remember the first time I heard the executives at Paypal say that they often hire out of their development community. After hearing that I began asking more API providers about the talent acquisition opportunities within their API developer ecosystem, and about 50% of the people I talked said they had hired someone building an application on their API, or had shown up to a hackathon to participate and build interesting things. It is easy to think of our API platforms as simply a place for developing applications, but along with each integration and application, there is one or many humans behind it, who have an understanding of your API, and possibly the industry you are targeting. Once you are introduced to the concept, it makes complete sense to be using an API as a talent acquisition vehicle, allowing developers to flex their skills in a place you are already paying attention in. This is my new response to people who are either looking for talent. Have you tried your API community? Many of the people I’ve tested this out on do not have a public API program. They aren’t at this place in their journey where they have a public presence in this way. Either because they don’t have any public resources to make available, or they just haven’t evolved to the...[<a href="/2017/11/07/developing-a-talent-pool-within-your-api-community/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/06/api-security-beginning-to-outweigh-my-vendor-lockin-concerns/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/iam/aws-iam-api-gateway.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/06/api-security-beginning-to-outweigh-my-vendor-lockin-concerns/">API Security Beginning To Outweigh My Vendor Lock-In Concerns</a></h3>
			<p><em>06 Nov 2017</em></p>
			<p>I’ve been on the AWS train since day one. I’ve been integrating Amazon S3 and EC2 into my business(es) since they first launched a decade ago. While the platform has faithfully provided my storage and compute for over a decade I’ve always been wary of vendor lock-in. After a decade long ride on Microsoft (1998-2008), I felt pretty burned. Then recently after a similar decade long ride on Google (2005-2015), I felt burned, but in a different way. After a decade on AWS I’m nervous, but I don’t feel as burned, however I’d say there is one aspect of doing business online that is making me put aside some of my concerns regarding vendor lock-in on AWS, and even on Google, and Azure–SECURITY. I’m just not convinced I can do this alone, and I need the help of the platforms I operate on to help make sure my operations are secure. I spent the weekend setting up a set of APIs using AWS RDS as the backend database, AWS Lambda as the code layer, and AWS API Gateway as the API front-end. As part of this work I established an identity and access management (IAM) role with policies tailored for brokering the exchanges between RDS, Lambda, and the API Gateway. I’m leaning on AWS for two key API security pieces here: 1) API key management with AWS API Gateway, and 2) backend security using AWS IAM. The key management stuff is pretty straightforward and something I can easily replicate, but the AWS IAM stuff is a little more involved, and I’m grateful for how easy AWS IAM makes it for me to setup roles, cherry pick from common policies, and lock down the infrastructure I a using to drive the backend of my APIs and other applications. As I moved another API project into the home stretch this weekend, I migrated an RDS, and EC2 instance into a separate account for a client. I had...[<a href="/2017/11/06/api-security-beginning-to-outweigh-my-vendor-lockin-concerns/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/06/an-example-of-how-every-api-provider-should-be-using-openapi-out-of-the-slack/"><img src="https://s3.amazonaws.com/kinlane-productions2/slack/slack-standard-practice.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/06/an-example-of-how-every-api-provider-should-be-using-openapi-out-of-the-slack/">An Example Of How Every API Provider Should Be Using OpenAPI Out Of The Slack</a></h3>
			<p><em>06 Nov 2017</em></p>
			<p>The Slack team has published the most robust and honest story about using OpenAPI, providing a blueprint that other API providers should be following. What I like most about approach by Slack to develop, publish, and share their OpenAPI, is the honesty behind why their are doing it to help standardize around a single definition. They publish and share the OpenAPI to Github, which other API providers are doing, and I think should be standard operating procedure for all API providers, but they also go into the realities regarding the messy history of their API documentation–an honesty that I feel ALL API providers should be embracing. My favorite part of the story from Slack is the opening paragraph that honestly portrays how they’ve got here: “The Slack Web API’s catalog of methods and operations now numbers nearly 150 reads, writes, rights, and wrongs. Its earliest documentation, much still preserved on api.slack.com today, often originated as hastily written notes left from one Slack engineer to another, in a kind of institutional shorthand. Still, it was enough to get by for a small team and a growing number of engaged developers.” Even though we all wish we could do APIs correctly, and supporting API document perfectly from day one, this is never the reality of API operations, and something OpenAPI will not be a silver bullet for fixing all of this, but can go a long way in helping standardize what is going on across teams, and within an API community. Slack focuses on SDK development, Postman client usage, alternative forms of documentation, and mock servers as the primary reasons for publishing the OpenAPI for their API. They also share some of the back story regarding how they crafted the spec, and their decision making process behind why they chose OpenAPI over other specifications. They also share a bit of their road map regarding the API definition, and that they will be adopting v3.0 of OpenAPI v3.0,...[<a href="/2017/11/06/an-example-of-how-every-api-provider-should-be-using-openapi-out-of-the-slack/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/11/06/a-simple-api-using-aws-rds-lambda-and-api-gateway/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/api-gateway/aws-rds-lambda-api-gateway.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/11/06/a-simple-api-using-aws-rds-lambda-and-api-gateway/">A Simple API Using AWS RDS, Lambda, and API Gateway</a></h3>
			<p><em>06 Nov 2017</em></p>
			<p>I wrote about a simple API with AWS DynamoDB, Lambda, and API Gateway last week. I like this approach because of the simple nature of AWS DynamoDB. One benefit of going this route is that you can even bypass Lambda, as the AWS API Gateway can work directly with AWS DynamoDB API. I’m just playing around with different configurations and pushing forward my understanding of what is possible, and this week I switched out the database in this with AWS RDS, which opens up the ability to use MySQL or Postgres as the backend for any API. For this example, I’m using a simple items database, which you can build with this SQL script after you fire up an RDS instance (I’m using MySQL): Next I wanted to have the basic CRUD operations for my API. I opted to use Node.js running in Lambda for the code layer of this API, starting with the ability to get all records from the database: After that I want to be able to insert new records: Then of course be able to get a single record: Then be able to update a single record: And of course I want to be able to delete records: Now that I have the business logic setup in AWS Lambda for reading, and writing data to my relational database I want an API front-end for this backend setup. I am using AWS API Gateway as the API layer, and to setup I’m just importing an OpenAPI definition to jumpstart things: This gives me the skeleton framework for my API, with the paths and methods I need to accomplish the basics of reading and writing data. Now, I just need to wire up each API method to its accompanying Lambda function, something API Gateway makes easy. Now I have an API for my basic backend. There is one thing you have to do to make each method work properly with the Lambda function....[<a href="/2017/11/06/a-simple-api-using-aws-rds-lambda-and-api-gateway/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/26/my-response-on-the-department-of-veterans-affairs-va-rfi-for-the-lighthouse/"><img src="https://s3.amazonaws.com/kinlane-productions2/veterans-affairs/va-logo.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/26/my-response-on-the-department-of-veterans-affairs-va-rfi-for-the-lighthouse/">My Response On The Department Of Veterans Affairs (VA) RFI For The Lighthouse</a></h3>
			<p><em>26 Oct 2017</em></p>
			<p>I am working with my partners in the government API space (Skylight, 540, Agile Six) to respond to a request for information (RFI) out of the Department of Veterans Affairs (VA), for what they call the Lighthouse API Management platform. The RFI provides a pretty interesting look into the way the government agency which supports our vets is thinking about how they should be delivering government resource using APIs, but also how they play a role in the wider healthcare ecosystem. My team is meeting today to finalize our response to the RFI, and in preparation I wanted to prepare my thoughts, and in my style of doing things, involves publishing them here on API Evangelist. You can read the whole RFI, but I’ll provide the heart of it, to help set the table for my response. Introduction: To accelerate better and more responsive service to the Veteran, the Department of Veterans Affairs (VA) is making a deliberate shift towards becoming an Application Programming Interface (API) driven digital enterprise. A cornerstone of this effort is the setup of a strategic Open API Program that is adopting an outside-in, value-to-business driven approach to create APIs that are managed as products to be consumed by developers within and outside of VA. Objectives: VA has started the process of establishing an API Management Platform, named Lighthouse. The purpose of Lighthouse is to establish the Next Generation Open Digital Platform for Veterans, accelerating the transformation in core domains of VA, such as Health, Benefits, Burial and Memorials. This platform will be a system for designing, developing, publishing, operating, monitoring, analyzing, iterating and optimizing VA’s API ecosystem. These APIs will allow VA to leverage its investment in various digital assets, support application rationalization, and allow it to decouple outdated systems and replace them with new, commercial, off the shelf, Software as a Service (SaaS) solutions. It will enable creation of new, high value experiences for our Veterans, VA’s provider partners,...[<a href="/2017/10/26/my-response-on-the-department-of-veterans-affairs-va-rfi-for-the-lighthouse/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/25/we-are-all-using-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/apis-are-all-around-us.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/25/we-are-all-using-apis/">We Are All Using APIs</a></h3>
			<p><em>25 Oct 2017</em></p>
			<p>When I talk to ordinary people about what I do as the API Evangelist, they tend to think APIs don’t have much of anything to do with their world. APIs exist in a realm of startups, technology, and make believe that doesn’t have much to do with their ordinary lives. When trying to make the connection with folks on airplanes, in the hotel lobby, and at the coffee shop, I always resort to the most common API-driven thing in all of our lives–the smart phone. Pulling out my iPhone is the quickest way I can go from zero to API understanding, with almost anyone. When people ask what an API is, or how it has anything to do with them, I always pull out my iPhone, and say that all of the applications on the home page of your mobile phone use APIs to communicate. When you post something to your Facebook wall, you are using the Facebook API. When you publish an image to Instagram, you are using the Instagram API. When you check the balance on your bank account, you are using your banks API. APIs are everywhere. We are all using APIs. We are all impacted by good APIs, and bad APIs. Most of the time we just don’t know it, and are completely unaware of what is going on behind the curtain that is our smart phones. I started paying attention to APIs in 2010 when I saw the impact mobile phones were beginning to have in our lives, and the role APIs were playing behind this new technological curtain. In 2017, I’m watching APIs expand to our homes via our thermostats, and other appliances. I’m seeing APIs in our cars. Applied to security cameras, sensors, signage, and other common objects throughout public spaces. APIs aren’t something everyone should be doing, however I feel they are something that everyone should be aware of. I usually compare it to the financial and...[<a href="/2017/10/25/we-are-all-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/25/i-am-talking-about-jekyll-as-a-hypermedia-client-at-apistrat-in-portland-or/"><img src="https://s3.amazonaws.com/kinlane-productions2/jekyll/jekyll-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/25/i-am-talking-about-jekyll-as-a-hypermedia-client-at-apistrat-in-portland-or/">I Am Talking About Jekyll As A Hypermedia Client At APIStrat in Portland OR</a></h3>
			<p><em>25 Oct 2017</em></p>
			<p>Static website, and headless CMS approaches to providing API driven solutions have grown in popularity in recent years. Jekyll has been leading the charge when it comes to static website deployment, partly due to Github being behind the project, and their adoption for Github Pages. I’ve been pushing forward a new approach to using Jekyll as a hypermedia client to help deliver some of my API training and curriculum, and as part of this work I’m giving a talk at APISTrat next week on the concept. APIStrat is a great forum for this kind of project, helping me think through things in the form of a talk, the opportunity to share with an audience, and get immediate feedback on its viability, which I can then use to push forward my thinking on this aspect of my API work. If you aren’t familiar with Jekyll, I recommend spending some time reading about it, and even implementing a simple site using Github Pages. I have multiple non-developers in my life who I’ve introduced to Jekyll, and it has made a significant impact on the way they do their work. Even if you do know about Jekyll, additionally I recommend spending time learning more about the underscore data folder, and the concept of Jekyll collections. Every Jekyll site has its default data collection, and the opportunity to create any additional collection, using any name you desire. Within these folders you can publish static CSV, JSON, and YAML data files, using any schema you desire. All of these data collections then become available for referencing through the static Jekyll website or application. All of this functionality is native Jekyll. Nothing revelatory from me. What I’m looking to push things forward around is what happens when the data collections are using a hypermedia media type. I’m using Siren, which allows me to publish structured data collections, complete with links that define a specific experience across the data and content stored...[<a href="/2017/10/25/i-am-talking-about-jekyll-as-a-hypermedia-client-at-apistrat-in-portland-or/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/25/helping-business-users-get-over-perceived-technical-gaps-when-it-comes-to-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/cactus-flower.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/25/helping-business-users-get-over-perceived-technical-gaps-when-it-comes-to-api/">Helping Business Users Get Over Perceived Technical Gaps When It Comes To API</a></h3>
			<p><em>25 Oct 2017</em></p>
			<p>Every single API project I’m working on currently has one or more business users involved, or specifically leading the work. With every business user, no matter how fearless they are, there is always a pretty heavy perception that some things are over their head. I see this over and over when it comes to API design, and the usage of OpenAPI to define an API. I’ve known a handful of folks who aren’t programmers, and have learned OpenAPI fluently, but for the most part, all business users tend to put up a barrier when it comes to learning OpenAPI–it lives in the realm of code, and exists beyond what they are capable of. I get that folks are turned off by being exposed to code. Learning to read code takes a significant amount of time, and with the more framework, libraries, and other layers, you can find yourself pretty lost, pretty quickly. However, with OpenAPI, everything I do tends to be YAML, making it much more readable to humans. While there are rules and structure to things, I don’t feel it is out of the realm of the average user to study, learn, and eventually bring into focus. Along with the OpenAPI rules, there are a good deal of HTTP literacy required to fully understand what is going on, but I feel like the API design process is a much more forgiving environment to learn these things for both developers and business users. I put the ownership of everything technical being complicated squarely on the shoulders of IT and developer folks. We’ve gone out of our way to make this stuff difficult to access, and out of reach of the average person. We’ve spent decades keeping people we didn’t see fit from having a seat at the table. However, increasingly I also feel like there is a significant amount of ownership to be given to business users who are willing to put up walls when...[<a href="/2017/10/25/helping-business-users-get-over-perceived-technical-gaps-when-it-comes-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/25/a-suite-of-human-services-api-definitions-using-openapi/"><img src="http://org.open.referral.adopta.agency/images/openreferral-expanded.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/25/a-suite-of-human-services-api-definitions-using-openapi/">A Suite Of Human Services API Definitions Using OpenAPI</a></h3>
			<p><em>25 Oct 2017</em></p>
			<p>I’m needing to quantify some of the work that has occurred around my Human Services Data Specification work as part of a grant we received from Stanford. The grant shas helped us push forward almost three separate revisions of the API definition for working with human services data, and one of the things I’m needing to do is quantify the work that has occurred specifically around the OpenAPI definitions. At this point the specification is pretty verbose, and is now spanning multiple documents, making it difficult to quantify and share within an executive summary. To help support I wanted to craft some language that could help introduce the value that has been created to a non-technical audience. The Human Services Data Specification (HSDS) provides the common schema for accessing, storing, and sharing of human data, providing a machine readable definition that human service practitioners can follow in their implementations. The Human Services Data API (HSDA) takes this schema, and provides an API definition for accessing, as well as adding, updating, and deleting data using a web API. While there are a growing number of code projects emerging that support HSDS/A, the center of the project is a set of OpenAPI definitions that outline the finer details of working with human services data via a web API. With the assistance of the grant from Stanford, Open Referral was able to move forward the HSDA specification from version 1.0, to 1.1, 1.2, and now we are looking at delivering version 1.3 of the specification before end of 2017. The core OpenAPI definition for HSDA provides guidance for the design of human services APIs, with a focus on the core set of resources needed to operate a human services project. There are the handle of core resources defined as part of what is called HSDA core: Organizations (OpenAPI Definition) - Providing a framework for adding, updating, reading, and deleting organizational data. Describing the paths, parameters, and HSDS schema...[<a href="/2017/10/25/a-suite-of-human-services-api-definitions-using-openapi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/24/your-api-road-map-helps-others-tell-stories-about-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/los-angeles-downtow-freeway_blue_circuit_5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/24/your-api-road-map-helps-others-tell-stories-about-your-api/">Your API Road Map Helps Others Tell Stories About Your API</a></h3>
			<p><em>24 Oct 2017</em></p>
			<p>There are many reasons you want to have a road map for your API. It helps you communicate with your API community where you are going with your API. It also helps you have a plan in place for the future, which increases the chances you will be moving things forward in a predictable and stable way. When I’m reviewing and API I don’t see a public API road map available, I tend to give them a ding on the reliability and communication for their operations. One of the reasons we do APIs is to help us focus externally with our digital resources, which communication plays an important role, and when API providers aren’t communicating effectively with their community, there are almost always other issues right behind the scenes. A road map for your API helps you plan, and think through how and what you will be releasing for the foreseeable future. Communicating this plan externally helps force you think about your road map in context of your consumers. Having a road map, and successfully communicating about it via a blog, on Twitter, and other channels helps keep your API consumers in tune with what you doing. In my opinion, an API road map is an essential building block for all API providers, because it has direct value on the health of API operations, but because it also provides an external sign of the overall health of a platform. Beyond the direct value of having an API road map, there are other reasons for having one that will go beyond just your developer community. In a story in Search Engine Land about Google Posts, the author directly references the road map as part of their storytelling. “In version 4.0 of the API, Google noted that “you can now create Posts on Google directly through the API.” The changelog include a bunch of other features, but the Google Posts is the most notable.” Adding another significant...[<a href="/2017/10/24/your-api-road-map-helps-others-tell-stories-about-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/24/the-api-portal-outline-for-a-project-i-am-working-on/"><img src="https://s3.amazonaws.com/kinlane-productions2/portal/api-portal-forkable.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/24/the-api-portal-outline-for-a-project-i-am-working-on/">The API Portal Outline For A Project I Am Working On</a></h3>
			<p><em>24 Oct 2017</em></p>
			<p>I am working through a project for a client, helping them deliver a portal for their API. As I do with any of my recommendations with my clients, I take my existing API research, and refine it to help craft a strategy to meets their specific needs. Each time I do this it gives me a chance to rethink some of my recommendations I’ve already gathered, as well as learn from new types of projects. I’ve taken the building blocks from my API portal, as well as my API management research, and have taken a crack at organizing them into an outline that I can use to guide my current project. Here is a walk through of the outline I’m recommending as part of a basic API portal implementation, to support a simple public API: Overview - / - Everything starts with a landing page, with a simple overview of what an API does. Then you need some basics to help make on-boarding as frictionless as possible, providing everything an API consumer needs to get going: Getting Started - /getting-started/ - Handful of steps with exactly what is needed to get started. Authentication - /authentication/ - An overview of what type of authentication is used. Documentation - /documentation/ - Documentation for the APIs that are available. FAQ - /faq/ - Answer the most common questions. Code - /code/ - Provide code samples, libraries, and SDKs to get going. Then get API consumers signed up, or able to login and get at their API keys as quickly as you possibly can: Sign Up - /developer/ - Provide a simple sign up for a developer account. Login - /developer/ - Allow users to quickly log back in after they have account. Next, provide a wealth of communication and support mechanisms, helping make sure developers are aware of what is going on: Blog - /blog/ - A simple blog dedicated to sharing stories about the API. Support -...[<a href="/2017/10/24/the-api-portal-outline-for-a-project-i-am-working-on/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page7" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page9" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home Page</a></li>
    <li><a href="/blog/">The Blog</a></li>
    <li><a href="https://101.apievangelist.com/">API 101</a></li>
    <li><a href="http://history.apievangelist.com">History of APIs</a></li>
    <li><a href="https://women-in-tech.apievangelist.com/">Women in Technology</a></li>    
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
