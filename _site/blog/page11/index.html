<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }
    
    .container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
    }
    .video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }    

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/02/understanding-global-api-performance-at-the-multicloud-level/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-metrics/apimetrics-cloud-location-performance-map.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/02/understanding-global-api-performance-at-the-multicloud-level/">Understanding Global API Performance At The Multi-Cloud Level</a></h3>
			<p><em>02 Aug 2017</em></p>
			<p>APIMetrics has a pretty addictive map showing the performance of API calls between multiple cloud providers, spanning many global regions. The cloud location latency map “shows relative performance of a standard, reference GET request made to servers running on all the Google locations and via the Google global load balancer. Calls are made from AWS, Azure, IBM and Google clouds and data is stored for all steps of the API call process and the key percentiles under consideration.” It is interesting to play with the destination of the API calls, changing the region, and visualizing how API calls begin to degrade to different regions. It really sets the stage for how we should start thinking about the deployment, monitoring, and testing of our APIs. Region, by region, getting to know where our consumers are, and making sure APIs are deployed within the cloud infrastructure that delivers the best possible performance. It’s not just testing your APIs in a single location from many locations, it is also rethinking where your APIs are deployed, leveraging a multi-cloud reality and using all the top cloud provider, while also making API deployment by region a priority. I’m a big fan of what APIMetrics is doing with the API performance visualizations and mapping. However, I think their approach to using HTTPbin is a significant part of this approach to monitoring and visualizing API performance at the multi-cloud level, while also making much of the process and data behind it all public. I want to put some more thought into how they are using HTTPbin behind this approach to multi-cloud API performance monitoring. I feel like there is potential her for applying this beyond just API performance, and think about other testing, security, and critical aspects of reliability and doing business online with APIs today. After thinking where else this HTTPbin approach to data gathering could be applied, I want to think more about how the data behind APIMetrics cloud location...[<a href="/2017/08/02/understanding-global-api-performance-at-the-multicloud-level/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/02/making-sure-your-api-service-connects-to-other-stops-along-the-api-lifecycle/"><img src="https://s3.amazonaws.com/kinlane-productions2/runscope/runscope-connected-services.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/02/making-sure-your-api-service-connects-to-other-stops-along-the-api-lifecycle/">Making Sure Your API Service Connects To Other Stops Along The API Lifecycle</a></h3>
			<p><em>02 Aug 2017</em></p>
			<p>I am continuing my integration platform as a service research, and spending a little bit of time trying to understand how API providers are offering up integrations with other APIs. Along the way, I also wanted to look at how API service providers are doing it as well, opening themselves up to other stops along n API lifecycle. To understand how API service providers are allowing their users to easily connect to other services I’m taking a look at how my partners are handling this, starting with connected services at Runscope. Runscope provides ready to go integration of their API monitoring and testing services with twenty other platforms, delivering a pretty interesting Venn diagram of services along the API lifecycle: Slack - Slack to receive notifications from Runscope API test results and Traffic Alerts. Datadog - Datadog to create events and metrics from Runscope API test results. Splunk Cloud - Splunk Cloud to create events for API test results. PagerDuty - A PagerDuty service to trigger and resolve incidents based on Runscope API test results or Traffic Alerts. Amazon Web Services - Amazon Web Services to import tests from API Gateway definitions. Ghost Inspector - Ghost Inspector to run UI tests from within your Runscope API tests. New Relic Insights - New Relic Insights to create events from Runscope API test results. Microsoft Teams - Microsoft Teams to receive notifications from Runscope API test results. HipChat - HipChat to receive notifications from Runscope API test results and Traffic Alerts. StatusPage.io - StatusPage.io to create metrics from Runscope API test results. Big Panda - Big Panda to create alerts from Runscope API test results. Keen IO - Keen IO to create events from Runscope API test results. VictorOps - A VictorOps service to trigger and resolve incidents based on Runscope API test results or Traffic Alerts. Flowdock - Flowdock to receive notifications from Runscope API test results and Traffic Alerts. AWS CodePipeline - Integrate your Runscope...[<a href="/2017/08/02/making-sure-your-api-service-connects-to-other-stops-along-the-api-lifecycle/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/02/learning-about-realtime-advertising-bidding-transparency-using-ads-txt/"><img src="https://s3.amazonaws.com/kinlane-productions2/ads-txt/ads.txt-about.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/02/learning-about-realtime-advertising-bidding-transparency-using-ads-txt/">Learning About Real-Time Advertising Bidding Transparency Using Ads.txt</a></h3>
			<p><em>02 Aug 2017</em></p>
			<p>I was learning about real-time bidding transparency using Ads.txt from Lukasz Olejnik. The mission of the ads.txt project is to “increase transparency in the programmatic advertising ecosystem. Ads.txt stands for Authorized Digital Sellers and is a simple, flexible and secure method that publishers and distributors can use to publicly declare the companies they authorize to sell their digital inventory.” While Ads.txt isn’t an API, it is an open, machine readable definition that is working to make advertising more transparent and observable to everyone, not just people in the ad-tech space. Ads.txt works similar to robots.txt, and is a simple text file that lives in the root of a domain, listing the companies that have permission to sell advertising. The format is new, so there isn’t a lot of adoption yet, but you can see one in action over at the Washington Post. Helping make platforms observable is why I perform as the API Evangelist. I see them as one of the important tools we have for making systems, algorithms, and platforms more observable, and less of a black box. I see ads.txt having similar potential for the world of advertising, and something that eventually could have an API, for helping make sense of the very distributed, brokered, and often dark world of online advertising. Honestly, I know almost nothing about online advertising. I have a basic level of understanding of Google Analytics, Adwords, and Adsense, as well as reading the blogs, and documentation for many advertising APIs I come across in my regular monitoring of the API space. I am just interested in ads.txt as an agent of observability, and pulling back the current on who is buying and selling our digital bits online. I am adding ads.txt to my API definitions research. This will allow me to keep an eye on the project, see where it goes, and think about the API level for aggregation of the ad network data, on maybe Github or...[<a href="/2017/08/02/learning-about-realtime-advertising-bidding-transparency-using-ads-txt/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/01/u-s-energy-information-administration-excel-addin-and-google-addon/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/eia/eia-spreadsheet-add-ons.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/01/u-s-energy-information-administration-excel-addin-and-google-addon/">U.S. Energy Information Administration Excel Add-In and Google Add-On</a></h3>
			<p><em>01 Aug 2017</em></p>
			<p>I was looking through a number of federal government API implementations last week in preparation of a talk I did in Washington DC. The result of research like this is always a notebook full of interesting stories to tell about what federal agencies are up to with APIs. Today’s story is out of the U.S. Energy Information Administration (EIA), with their Excel Data Add-In and Google Add-On tooling which allows you to download energy data from EIA’s data API and economic data from the St. Louis Federal Reserve’s Economic Data (FRED) API directly into your spreadsheet(s). I’m regularly looking out for innovative uses of spreadsheets when it comes to deploying, as well as consuming APIs, because I believe it is the best way we have to turn average business users into API consumers, by piping in data into the environment they are already using each day. EIA’s data API contains 1.6 million energy series, and the St. Louis Federal Reserve’s API contains 240,000 economic series. Making valuable federal agency maintained data available within spreadsheets like this using APIs is something ALL other agencies should be emulating. First, agencies need to be doing public APIs, then they need to make sure they are also investing in spreadsheet tooling like the EIA is. I’m adding this example of using Microsoft Excel and Google Sheets as an API client for not just federal government, but also for such valuable commerce and energy data, to my APIs and spreadsheets research. I’m also going to be on the hunt for open source solutions for delivering spreadsheet API connectivity like this. There should be a wealth of open source tooling that federal agencies can put to work when it comes to delivering data to spreadsheets, both internally, and externally with private sector partners. In a time where it is easy to get pretty depressed on a daily basis about APIs in the federal government, it makes me happy to find shining...[<a href="/2017/08/01/u-s-energy-information-administration-excel-addin-and-google-addon/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/01/the-trusted-automated-exchange-of-intelligence-information-taxii/"><img src="https://s3.amazonaws.com/kinlane-productions2/taxii/taxii-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/01/the-trusted-automated-exchange-of-intelligence-information-taxii/">The Trusted Automated Exchange of Intelligence Information (TAXII)</a></h3>
			<p><em>01 Aug 2017</em></p>
			<p>I recently wrote about the opportunity around developing an aggregate threat information API, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of the Trusted Automated Exchange of Intelligence Information (TAXII), as one possible approach to defining a common set of API definitions and tooling for the exchange of threat intelligence. The description of TAXII from the project website describes it well: Trusted Automated Exchange of Intelligence Information (TAXII) is an application layer protocol for the communication of cyber threat information in a simple and scalable manner. TAXII is a protocol used to exchange cyber threat intelligence (CTI) over HTTPS. TAXII enables organizations to share CTI by defining an API that aligns with common sharing models. TAXII is specifically designed to support the exchange of CTI represented in STIX. I breezed through the documentation for TAXII version 2.0, and it looks pretty robust, and a project that has made some significant inroads towards accomplishing what I’d like to see out there for sharing threat intelligence. I’m still understanding the overlap of TAXII, the transport mechanism for sharing cyber threat intelligence, and STIX, the structured language for cyber threat intelligence, but it looks like a robust, existing approach defining the schema and an API for sharing threat intelligence. Next, I am going to gather my thoughts around both of these existing definitions, and look at establishing an OpenAPI that represents STIX and TAXII, providing a machine readable definition for sharing threat intelligence. I think having an OpenAPI will provide a blueprint that can be used to define a handful of server side implementations in a variety of programming languages. I was happy to be directed to this existing work, saving me significant time and energy when it comes to this conversation....[<a href="/2017/08/01/the-trusted-automated-exchange-of-intelligence-information-taxii/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/01/craft-an-openapi-for-an-existing-threat-intelligence-sharing-api-specification/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-threat-info-sharing-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/01/craft-an-openapi-for-an-existing-threat-intelligence-sharing-api-specification/">Craft An OpenAPI For An Existing Threat Intelligence Sharing API Specification</a></h3>
			<p><em>01 Aug 2017</em></p>
			<p>I wrote about the opportunity around developing an aggregate threat information API, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of the Structured Threat Information Expression (STIX), a structured language for cyber threat intelligence, and Trusted Automated Exchange of Intelligence Information (TAXII), and transport mechanism for sharing cyber threat intelligence. This is why I write about my projects openly like this, so that my readers can help me identify existing approaches for tackling whatever I am focusing on. I prefer to never reinvent the wheel, and build on top of any existing work that is already available. I’m thinking the next step is to craft an OpenAPI fo TAXII, and STIX. Creating a machine readable blueprint for deploying, managing, and documenting a threat intelligence API. I couldn’t find any existing work on an OpenAPI definition, so this seems like a logical place to begin working to build on, and augment the work of the Cyber Threat Intelligence Technical Committee. Clearly, the working group has created a robust set of specifications, but I’d like to help move it closer to implementation with an OpenAPI. I have created a Github organization to help organize any work on this project. I have forked the project for STIX and TAXII there, as well as started a planning repository to coordinate any work I’m contributing to the conversation. I have also created a repository for working on and publishing the OpenAPI that will define the project. Once we have this, I’d like to start thinking about the development of a handful of server side implementations in maybe Node.js, Python, PHP, or other common programming language. Here are the next steps I’d like to see occur around this project: OpenAPI - Create an OpenAPI for...[<a href="/2017/08/01/craft-an-openapi-for-an-existing-threat-intelligence-sharing-api-specification/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/01/api-sdk-licensing-notifications-using-versioneye/"><img src="https://s3.amazonaws.com/kinlane-productions2/versioneye/01-veye-licenses-7679418f2968513011985476db94b59b0ab65abd5c030a6a92189fc0e1170722.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/01/api-sdk-licensing-notifications-using-versioneye/">API SDK Licensing Notifications Using VersionEye</a></h3>
			<p><em>01 Aug 2017</em></p>
			<p>
I have been watching VersionEye for a while now. If you aren’t familiar, they provide a service that will notify you of security vulnerabilities, license violations and out-dated dependencies in your Git repositories. I wanted to craft a story specifically about their licensing notification services, which can check all your open source dependencies against a license white list, then notify you of violations, and changes at the SDK licensing level.

The first thing I like here, is the notion of an API SDK licensing whitelist. The idea that there is a service that could potentially let you know which API providers have SDKs that are licensed in a way that meets your integration requirements. I think it helps developers who are building applications on top of APIs understand which APIs they should or shouldn’t be using based upon SDK licensing, while also providing an incentive for API providers to get their SDKs organized, including the licensing–you’d be surprised at how many API providers do not have their SDK house in order.

VersionEye also provides CI/CD integration, so that you can stop a build based on a licensing violation. Injecting the politics of API operations, from an API consumers perspective, into the application lifecycle. I’m interested in VersionEye’s CI/CD, as well as security vulnerabilities, but I wanted to make sure this approach to keeping an eye on SDK licensing was included in my SDK, monitoring, and licensing research, influencing my storytelling across these areas. Some day all API providers will have a wide variety of SDKs available, each complete with clear licensing, published on Github, and indexed as part of an APIs.json. We just aren’t quite there yet, and we need more services like VersionEye to help build awareness at the API SDK licensing level to get us closer to this reality.

[<a href="/2017/08/01/api-sdk-licensing-notifications-using-versioneye/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/carryload_diego_rivera1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/">You See Duplicate Work While I See Common Patterns</a></h3>
			<p><em>31 Jul 2017</em></p>
			<p>Someone asked me on Twitter recently how I deal the duplicate work required to manage a large volume of OpenAPIs. All the same things you have to do when crafting the headers, parameters, responses, and schema across every OpenAPI you are crafting. My response was that I don’t see these things as repetitive or duplicate work, I see these things as common patterns across the resources I am making available. They main reason I think they seem repetitive is the tooling we are currently using needs to play catch up, and help us better apply common patterns across all our APIs–dealing with the duplicate, repetitive work for us. I’m confident that open source API design tooling like Apicurio are going to help us better manage the common patterns we should be applying across our OpenAPIs. I’m also hopeful that OpenAPI 3.0 contributes to reuse of parameters, schema, errors, and other common building blocks across the request and response surface of our API. I’m counting on OpenAPI + Apicurio as well as other API definition and design tooling to step up and do the hard work wen it comes to helping us manage the common patterns across our APIs, and make the reuse of common patterns across our APIs a good thing, and never a burden. This reuse shouldn’t just being within any company, and we should be reusing and sharing patterns from across the space, including common web concepts and standards. The fact that you use ISO 8601 for all your dates, while employing a handful of date field names over and over across your systems isn’t repetition or duplicate work, that is consistency, and sensible reuse of common API patterns. It is the job of API design service and tooling providers to help us get over this hump, and craft notebooks, catalogs, collections, dictionaries, and stores of the common patterns we will need to be applying (over and over again) across our API definitions–OpenAPI....[<a href="/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/31/professional-api-deployment-templates/"><img src="https://s3.amazonaws.com/kinlane-productions2/gsa/gsa-prototype-api-portal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/31/professional-api-deployment-templates/">Professional API Deployment Templates</a></h3>
			<p><em>31 Jul 2017</em></p>
			<p>I wrote about the GSA API prototype the other day. It is an API prototype developed by the GSA, providing an API that is designed in alignment with GSA API design guidelines, complete with an API portal for delivering documentation, and other essential resources any API deployment will need. The GSA provides us with an important approach to delivering open source blueprints that other federal agencies can fork, reverse engineer and deploy as their own custom API implementation. We need more of this in the private sector. We need a whole buffet of APIs that do a variety of different things, in every language, and platform or stack that we can imagine. Need a contact directory API, or maybe a document storage API, URL shortener API–here is a forkable, downloadable, open source solution you can put to work immediately. We need the WordPress for APIs. Not the CMS interface WordPress is known for, just a simple API that is open source, and can be easily deployed by anyone in any common hosting, and serverless environments. Making the deployment of common API patterns a no-brainer, and something anyone can do, anywhere in the cloud, on-premise, or on-device. Even though these little bundles of API deployment joy would be open source, there would be a significant amount of opportunity routing folks to other add-on, and premium services on top of any open source API deployment, as well as providing cloud deployment opportunities for developers–similar to the separation between WordPress.com and WordPress.org. I could see a variety of service providers emerge that could cater to the API deployment needs of various industries. Some could focus on more data specific solutions, while others could focus on content, or even more algorithmic, machine learning-focused API deployment solutions. If linked data, hypermedia, gRPC, and GraphQL practitioners want to see more adoption in their chosen protocol, they should be publishing, evolving, and maintaining robust, reverse engineer-able, forkable, open source examples of the...[<a href="/2017/07/31/professional-api-deployment-templates/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/31/making-the-business-of-apis-more-modular-before-you-do-the-tech/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/31/making-the-business-of-apis-more-modular-before-you-do-the-tech/">Making The Business Of APIs More Modular Before You Do The Tech</a></h3>
			<p><em>31 Jul 2017</em></p>
			<p>I have been immersed in how APIs are being done in the federal government for the last week or so, looking for positive API behavior I can showcase and focus on in my storytelling. I was walking through each step of my API lifecycle, sizing up the federal government for each area I track the private sector on when it comes to APIs. I was taking a look at the areas of microservices, containerization, and serverless. You know the modularization of IT infrastructure in government? I couldn’t find much rubber meeting the road when it comes to microservices or containerization in my research, but I did see hints of modularizing the business aspects of doing APIs in the federal government. Over at 18F you can find some interesting discussion around micro-procurement, “a procurement model that breaks what would traditionally be a large, monolithic contract into several shorter-term, lower dollar amount contracts.” I feel that breaking down the business of defining, designing, deploying, managing, and even testing your APIs into small projects is an important first step for many companies, organizations, institutions, and agencies. While not all organizations will be the same, many will need to break down the business of procuring API design, deployment, and management services, before they can even getting to work breaking down the technical components of what is needing to be delivered. I have been working with my partners at Skylight on our approach to breaking down and delivering API projects, discussing the technical, business, and political aspects of doing things in as modular, bite-size chunks as we possibly can. This involves exploring the other side of the micro-procurement coin, with micro-consulting, providing API related services in small, modular projects. I’m exploring the delivery of API training and curriculum, as well as white paper, guide, and other content-centric services using a micro-consulting approach–keeping engagements small, focused, and delivering additional services that support one or many individual API implementations. Micro procurement and...[<a href="/2017/07/31/making-the-business-of-apis-more-modular-before-you-do-the-tech/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/31/balancing-domain-expertise-with-the-disruptive-power-of-upstarts-who-do-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/shipping-energy-trucking.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/31/balancing-domain-expertise-with-the-disruptive-power-of-upstarts-who-do-apis/">Balancing Domain Expertise With The Disruptive Power Of Upstarts Who Do APIs</a></h3>
			<p><em>31 Jul 2017</em></p>
			<p>APIs aren’t good, or bad, nor are they neutral. APIs do the bidding of their providers, and sometimes their consumers. In my experience APIs are more often used for bad than they are ever used for good, something I try to be as vocal as I can about, while working hard to shine a light on the good that is possible. After many years of trying to help folks understand APIs, one of the biggest challenges I face involves the unrealistic rhetoric of startups. The overoptimistic vision and promises of what APIs will do, coupled with an an often limiting awareness of the challenges and complexity of industries where APIs are targeting, making for a pretty toxic, non-cooperative environment for actually getting anything done. I work hard to keep APIs alive in a variety of industries that have seen multiple waves of startups trumpeting their disruption and change horns, while also often belittling and underestimating the people within the industry. I recently came across a post recently that captures the challenge we all face when we are looking to make change within established, and often entrenched industries using APIs. I feel this paragraph captures it well: The new players, and the venture capital/private equity money backing them, think they are entering a world full of Luddites. Yet the brokers we’ve talked to—and we know it’s not everybody—are quite IT-oriented. In a world where visibility is paramount, they are keenly aware of technology’s role in keeping them competitive. They are investing in IT and will continue to do so as prices drop. Meanwhile, many bring vast experience in mastering the physical part of the solution that the startups can’t touch. It is interesting to come across this friction in the freight brokerage industry. It is something I’ve seen in industry after industry, and with each wave of startups doing APIs. In some spaces startups will find success, but in others they will find themselves stopped cold...[<a href="/2017/07/31/balancing-domain-expertise-with-the-disruptive-power-of-upstarts-who-do-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/27/state-of-apis-in-the-federal-government/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/state-2017/bw-government.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/27/state-of-apis-in-the-federal-government/">State of APIs In The Federal Government</a></h3>
			<p><em>27 Jul 2017</em></p>
			<p>This is my talk from Washington DC with Steve Willmott of 3Scale by Red Hat about transforming enterprise IT with containers, APIs, and integration, where I assess the current state of APIs in the federal government, and the opportunity in the private sector when it comes to working with government data. API Evangelist My name is Kin Lane. I am the API Evangelist. I have studied the technology, business, and politics of Application Programming Interfaces, or more commonly known as APIs, full time since 2010. I spend time looking through the growing number of APIs available today, as well as the evolving group of service providers selling their solutions to API providers. I take what I learn across the space and publish as over 80 independent research projects that I run on Github, covering a growing number of stops along the API lifecycle. In 2011, I began studying and writing about federal government APIs. I have long had an interest in politics, and how our government works (or doesn’t work), which was in alignment with thinking about how I could take what I’ve learned about APIs and apply to the federal government. By 2013, my research and storytelling about APIs attracted the attention of folks in government, which led to an invitation to come work on open data and API projects at multiple agencies. This move took my work to new levels, opening up some interesting doors that have opened my eyes to the scope of APIs within federal government. Presidential Fellow In the summer of 2013 I was invited to be part of the round two Presidential Innovation Fellowship, and work at the Department of Veterans affairs doing web service and API inventory, as well as assist with the wider open data efforts of the Obama administration. I worked in DC until the government shutdown in October, when I decided to leave my position so that I could continue doing my work around veterans...[<a href="/2017/07/27/state-of-apis-in-the-federal-government/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/26/we-have-a-hostile-ceo-which-requires-a-shift-in-our-api-strategy/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/raven-fence.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/26/we-have-a-hostile-ceo-which-requires-a-shift-in-our-api-strategy/">We Have A Hostile CEO Which Requires A Shift In Our API Strategy</a></h3>
			<p><em>26 Jul 2017</em></p>
			<p>As I work my way through almost one hundred federal government API developer portals, almost 500 APIs, and 133 Github accounts for federal agencies the chilling effect of the change of leadership in this country becomes clear. You can tell the momentum across hundreds of federal agency built up over the last five years is still moving, but the silence across blogs, Twitter accounts, change logs, and Github repos shows that the pace of acceleration is in jeopardy. When you are browsing agency developer portals you come across phrases like this, “As part of the Open Government Initiative, the BusinessUSA codebase is available on the BusinessUSA GitHub Open Source Repository.” With the link to the Open Government Initiative leading to a a page on the White House website that has been removed–something you can easily find on the Obama archives. I am coming across numerous examples like this of how the change in leadership has created a vacuum when it comes to API and open data leadership, at a time when we should be doubling down on sharing of data, content, and putting algorithms to work across the federal government. After several days immersed in federal government developer areas it is clear we have a hostile CEO that will require us to shift in our API strategy. After six months it is clear that the current leadership has no interest transparency, observability, or even the efficiency in government that is achieved from focusing opening up data via public, but secure APIs. This doesn’t mean the end of our open data and API efforts, it just means we lose the top down leadership we’ve enjoyed for the last eight years when it came to technology in government, and efforts will have to shift to a more bottom up approach, with agencies and departments often setting their own agenda. This is nothing new, and it won’t be the last time we face this working with APIs across...[<a href="/2017/07/26/we-have-a-hostile-ceo-which-requires-a-shift-in-our-api-strategy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/25/i-am-speaking-on-state-of-apis-in-federal-government-thursday-in-dc/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/steve_and_i_apistrat_2016.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/25/i-am-speaking-on-state-of-apis-in-federal-government-thursday-in-dc/">I Am Speaking On State Of APIs In Federal Government Thursday In DC</a></h3>
			<p><em>25 Jul 2017</em></p>
			<p>
I am joining my friend Steve Willmott in DC this week to talk about federal government APIs. We will  be gathering at Tysons’ Biergarten between 1:30 and 5:00 PM this Thursday to talk APIs. Both Steve and I will be speaking individually, with some QA, and a happy hour afterwards as an opportunity for more discussion.

I am looking forward for the opportunity to hanging with my friend Steve, as the last time we’ve hung out and spoke together was APIStrat in Boston, but at APIStat we are always running a conference, and not actually focused on our views of the APIs space. So, I am eager to learn more detail about what 3Scale is up to as part of the Red Hat machine, and specifically some of the containerization, microservices, and virtualization discussions they are leading lately.

Anyways, I will be in DC all day Thursday. Come join the conversation. I won’t have much time in DC, so the gathering will be the best opportunity to grab a moment of my time. I’ll be talking about the state of APIs in federal government, something I’m reminded during my research and preparation for my talk is probably the most important discussion we should be having in the API space right now.

Looking forward to seeing you all in DC. Thanks to Red Hat for bringing me out to DC, and making this conversation possible. I’ll see you Thursday.

Event Details:
Date: Thursday, July 27, 2017

Time: 1:30 p.m. – 5:00 p.m.
Registration: 1:30 – 2:00 p.m.
Presentations: 2:00 – 3:30 p.m.
Happy Hour: 3:30 – 5:00 p.m.

Location:
Tysons’ Biergarten
8346 Leesburg Pike
Tysons, VA 22182

[<a href="/2017/07/25/i-am-speaking-on-state-of-apis-in-federal-government-thursday-in-dc/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/25/api-management-across-all-government-agencies/"><img src="https://s3.amazonaws.com/kinlane-productions2/18f/9302707420_dbc7c2c437_o.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/25/api-management-across-all-government-agencies/">API Management Across All Government Agencies</a></h3>
			<p><em>25 Jul 2017</em></p>
			<p>This isn’t a new drum beat for me, but is one I wanted to pick it up again as part of the federal government research and speaking I’m doing this month. It is regarding the management of APIs across federal government. In short, helping agencies successfully secure, meter, analyze, and develop awareness of who is using government API resources. API management is a commodity in the private technology sector, and is something that has been gaining momentum in government circles, but we have a lot more work ahead to get things where we need them. The folks over at 18F have done a great job of helping bake API management into government APIs using API Umbrella, resulting in these twelve federal agencies: BusinessUSA.gov API Department of Agriculture Department of Commerce Department of Education Federal Communications Commission Federal Election Commission Food and Drug Administration General Services Administration National Aeronautics and Space Administration National Institutes of Health National Renewable Energy Laboratory Regulations.gov API This doesn’t just mean that each of these agencies are managing their APIs. It also means that all of these agencies are managing their APIs in a consistent way, using a consistent tool. Something that is allowing these agencies to effectively manage: API Key Usage - How to use your API key after signing up. Web Service Rate Limits - Daily and hourly rate limits on accessing api.data.gov APIs. General Web Service Errors - General error codes that can be returned by any api.data.gov API. HTTPS Usage - Information about HTTPS usage on api.data.gov. I know that both 18F and USDS are working are hard on this, but this is an area we need agencies to step up in, as well as the private sector. We need any vendor doing API deployment projects for any agency to work together to make sure their agency is using a standardized approach. This means that vendors should make the investment when it comes to reaching out to...[<a href="/2017/07/25/api-management-across-all-government-agencies/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/25/adding-vulnerability-disclosure-to-my-api-building-block-recommendations/"><img src="https://s3.amazonaws.com/kinlane-productions2/18f/vulnerabilities-disclosure-policy.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/25/adding-vulnerability-disclosure-to-my-api-building-block-recommendations/">Adding Vulnerability Disclosure To My API Building Block Recommendations</a></h3>
			<p><em>25 Jul 2017</em></p>
			<p>I am working through the almost 100 federal government agency developer portals and the almost 500 APIs that exist across these agencies, looking for the good and bad of APIs in government at this level. One of interesting building blocks I’ve stumbled across, that I would like to shine a light on for other public and private sector API providers to consider in their own operations is a vulnerability disclosure. I feel that 18F description of their vulnerability disclosure says it best: As part of a U.S. government agency, the General Services Administration (GSA)’s Technology Transformation Service (TTS) takes seriously our responsibility to protect the public’s information, including financial and personal information, from unwarranted disclosure. We want security researchers to feel comfortable reporting vulnerabilities they’ve discovered, as set out in this policy, so that we can fix them and keep our information safe. This policy describes what systems and types of research are covered under this policy, how to send us vulnerability reports, and how long we ask security researchers to wait before publicly disclosing vulnerabilities. This should be default across all federal, state, county, and municipal government agencies. Hell, it should be default across all companies, organizations, and institutions. One of the reasons we have so much dysfunction in the security realm that elevates the discussion to theatrical levels with cybersecurity is that we aren’t having honest conversations about the vulnerabilities that exist. Few platforms want these conversations to occur, let alone set the tone of the conversation in such an open way. Without any guidance, and fear of retaliation, developers and analysts who find vulnerabilities will continue to hold back on what they find. Vulnerability disclosure seems like something that ALL API provides should possess. There is no reason you can’t fork the GSA vulnerability policy and share it as the official tone of the vulnerability disclosure conversation on your platform. Encouraging all API developers to understand what the tone of the conversation...[<a href="/2017/07/25/adding-vulnerability-disclosure-to-my-api-building-block-recommendations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/25/a-lack-of-communication-around-federal-government-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/census/census-api-updates.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/25/a-lack-of-communication-around-federal-government-apis/">A Lack Of Communication Around Federal Government APIs</a></h3>
			<p><em>25 Jul 2017</em></p>
			<p>I personally understand the challenges with communicating publicly when you work for the federal government. It is one of the top reasons I do not work in federal government anymore. It would kill me if I couldn’t blog each day without friction–it is how I create and ideate. Even with this understanding I find myself regularly frustrated with the lack of communication by owners of APIs across federal government agencies. There are numerous agencies who do successfully communicate around their APIs and open data projects, but the majority of APIs I come across have little, or no communication around their API operations. Have a blog, Twitter, or Github account might seem like a nice to have, but in reality they are often the only sign that anyone is home, and an API is reliable, and make the the difference between choosing to integrate with an API, or not. A blog or Twitter account, and whats new feature box on the home page of an API developer portal can send the winning (or losing) signal that an API is actually active and alive. Developers come across a lot of APIs that are dormant or abandoned, and the presence of common communication channels (blog, Twitter, Facebook, LinkedIn, Github) are the signal we often need before we are willing to invest the time into learning another new API, or signing up for yet another developer account. I know that it is possible to handle API communications in a healthy way at government agencies–18F, Census, and others are doing it right. There is some serious storytelling friction occurring in government. I see the same illness in corporate and other institutional API platforms–geeks and IT folks aren’t always the best at getting the word out about what they are doing. However, I think there is additional friction at the government level. We’ve seen a significant increase in blogging, and social network usage usage across government agencies, we need to investigate...[<a href="/2017/07/25/a-lack-of-communication-around-federal-government-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/24/the-hack-education-gates-foundation-grant-data-has-an-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/hack-education/hack-education-personalize-learning-and-the-power-of-the-gates-foundation-to-shape-education-policy.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/24/the-hack-education-gates-foundation-grant-data-has-an-api/">The Hack Education Gates Foundation Grant Data Has An API</a></h3>
			<p><em>24 Jul 2017</em></p>
			<p>I have been helping my partner in crime Audrey Watters (@audreywatters) adopt my approach to managing data project(s) using Google Sheets and Github, as part of her work on ed-tech funding. She is going through many of the leading companies, and foundations behind the funding of technology used across the education sector, and doing the hard work of connecting the dots behind how technology gets funded in this critical layer of our society. I want Audrey (and others), to be self-sufficient when it comes to managing their data projects, which is why I’ve engineered it to use common services (Google Sheets, Github), with any code and supporting elements as self-contained as possible–something Github excels at when it comes to managing data, content, and code. While Audrey is going to town creating spreadsheets and repos, I wanted to highlight a single area of her research into the grants that the Gates Foundation are handing out. She has worked hard to normalize data across many years (1998-2017) of PDF and HTML data into a single Google Sheet, then she has published as individual YAML files which live on Github–making her work forkable and reusable by anyone. Once published, Audrey is the first person to fork the YAML, and put to work in her storytelling around ed-tech funding, but each of her project repos also come with an API for her research by default. Well, ok, it isn’t a full-blown searchable API, but in addition to being able to get data in YAML format, she has a JSON API for each year of the Gates Foundation grants (ie. 2016). Increasing the surface area when it comes to collaborating and building on top of her work, which can be forked using Github, or accessed via the machine readable YAML and JSON files. While she is busy creating new Google Sheets and repos for other companies, I wanted to add one more tool to her toolbox, an APIs.json index for...[<a href="/2017/07/24/the-hack-education-gates-foundation-grant-data-has-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/"><img src="https://s3.amazonaws.com/kinlane-productions2/lessons/google-sheet-to-github.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/">First Handful Of Lessons Using My Google Sheet Github Approach</a></h3>
			<p><em>24 Jul 2017</em></p>
			<p>With my recent shift to using Google Sheets as my data backend for my research, and my continued usage of Github as my data project publishing platform, I started pushing out some new API related lessons. I wanted to begin formalizing my schema and process for this new approach to delivering lessons with some simple topics, so I got to work taking my 101, and history of APIs work, and converting them into a multi-step lesson. Some of my initial 101 API lessons are: API 101 (Website) (Github Repo) (Google Sheet) - Just a general overview of what is API, targeting average user. API Provider 101 (Website) (Github Repo) (Google Sheet) - Working to evolve an opening pitch to would be API providers. API Consumer 101 (Website) (Github Repo) (Google Sheet) - Working to get better at providing information for API consumers. The History of APIs (Website) (Github Repo) (Google Sheet) - Continuing to expand on my history of APIs story. I will keep working those 101 lessons. Editing, polishing, expanding, and as I found out with this revision–removing some elements of APIs that are fading away. While my 101 stories are always working to reach as wide as possible, my wider research is always based in two sides of the API coin, with information about providing APis, while also keep my API consumer hat on, and thinking about the needs of developers and integrators. Now that I have the 101 lessons under way I wanted to focus on my API life cycle research, and work on creating a set of high level lessons for each of the 80+ stops I track on along a modern API life cycle. So I got to work on the lesson for API definitions, which I think is the most important stop along any API life cycle–one that actually crosses with every other line. Definitions (Website) (Github Repo) (https://docs.google.com/spreadsheets/d/13WXRAA30QMzKXRu-dH8gr-UrAQlLLDAD9pBAmwUPIS4/edit#gid=0) After kicking off a lesson for my API life cycle...[<a href="/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/24/finding-things-i-want-to-write-about-when-apis-are-dumb/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/training/gargoyle_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/24/finding-things-i-want-to-write-about-when-apis-are-dumb/">Finding Things I Want To Write About When APIs Are Dumb</a></h3>
			<p><em>24 Jul 2017</em></p>
			<p>You ever wake up some days, and find yourself not caring about APIs, or much else in the realm of technology? No? Well, I do. Regularly. I find myself in this headspace on this fine Monday morning, and without a weeks worth of stories scheduled, it is a very bad place to be as the API Evangelist. Part of this problem is me–I am a pain in my ass. However, a another portion of it is just about staying motivated, engaged, and producing compelling (ha) content on a regular basis for the blog, and other projects I’m working on. There are almost a hundred stories in my notebook and all of them seem really, really dumb to me this morning. I can’t seem to muster up the energy to take any of them and turn into even a three paragraph API blah blah blah story. It’s just words right? I should be able to do it. I churn out meaningless API words all the time, non-stop for the last seven years! I should be able to do it today. What is wrong with you man? C’mon, you should be able to just turn it on, and the words will flow. Not today. Like many days before I am going to need to trick myself into turning on the faucet. The best place to start (for me) when I have lost my writing mojo, is to find a project I truly care about 100%. This is why I work on the human services API project, and look for ways that I can help my partner in crime Audrey Watters (@audreywatters) with her Hack Education work, as she is always focused on the most critical area we face when it comes to our use of technology–education. Understanding how technology is helping, or hurting us when it comes to educating every human on earth is serious business, and something that might just help pull me from my writing...[<a href="/2017/07/24/finding-things-i-want-to-write-about-when-apis-are-dumb/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/24/federal-government-apis-in-a-trump-administration/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/24/federal-government-apis-in-a-trump-administration/">Federal Government APIs In A Trump Administration</a></h3>
			<p><em>24 Jul 2017</em></p>
			<p>I haven’t written much about APIs in the federal government since the election. I’m still having conversations, and investing time into monitoring what is going on in the federal government, but honestly in the name of self-care I have to turn my head from what is going on with the current administration. It’s no secret that I’m not a Trump supporter, and honestly I have trouble not getting angry with Trump supporters when it comes to making the federal government more transparent and observable with data and APIs. The current tone the administration is taking when it comes to transparency, observability, and accountability will take us decades to recover from, making conversations about federal government APIs very difficult to have in many scenarios. Luckily, I’m regularly reminded that there are MANY good people at government agencies who are doing amazing things, allowing me find more energy for thinking about APIs in federal government. I’ve been preparing for a talk I’m doing in DC this week with 3Scale by Red Hat, which is priming the pump for a presentation I’m doing for the General Services Administration later in August. Both of these talks give me the chance to think about federal government, and invest some energy into finding the good that is going on in the federal government when it comes to APIs. It will also give me some time to take a look at what challenges exist when doing APIs at the federal level of government, with some acknowledgement of the current leadership in the White House. First, I’m going to go agency by agency, taking a fresh look at anything API going on at the top level agencies, with a quick secondary look at some of the lesser known agencies. After this, I want to take a look at who is behind any API project that I’m coming across–understanding what I can about the internal groups doing APIs, any inter-agency efforts, including efforts out...[<a href="/2017/07/24/federal-government-apis-in-a-trump-administration/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/21/structured-threat-information-expression-stix/"><img src="https://s3.amazonaws.com/kinlane-productions2/stix/stix-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/21/structured-threat-information-expression-stix/">Structured Threat Information Expression (STIX)</a></h3>
			<p><em>21 Jul 2017</em></p>
			<p>I wrote about the opportunity around developing an aggregate threat information API, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of the Structured Threat Information Expression (STIX), as one possible schema for definining and sharing the information I’m talking about. Here is a quick summary of STIX is from the website: Structured Threat Information Expression (STIX™) is a language for describing cyber threat information in a standardized and structured manner to enable the exchange of cyber threat intelligence (CTI). STIX characterizes an extensive set of CTI to include indicators of adversary activity, as well as contextual information characterizing cyber adversary motivations, capabilities, and activities and best courses of action for defense and mitigation. I haven’t dug into STIX too much, so I’m not making recomendations on the value it brings to the table yet, but I want to make sure we take a look at any existing work that was already on the, and make sure we aren’t reinventing the wheel with any part of an aggregated threat information API. At first glance STIX looks like a pretty damn good start for a potential API schema, that speaks a common language, and is seeing adoption with other existing threat information storage and sharing providers. I am adding STIX to my research into threat information sharing, and wider API security research. I am currently diving deeper into API security thanks to investment from Elastic Beam, and I will be publishing a guide, as well as an API security white paper as part of the work. I’m going to try and provide some intelligence to a group of folks who expressed interest in developing an aggregate threat information sharing API. I’m hoping to better flesh out my thoughts on how API...[<a href="/2017/07/21/structured-threat-information-expression-stix/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/21/requiring-all-platform-partners-use-the-api-so-there-is-a-registered/"><img src="https://s3.amazonaws.com/kinlane-productions2/google/google-apps-connected.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/21/requiring-all-platform-partners-use-the-api-so-there-is-a-registered/">Requiring ALL Platform Partners Use The API So There Is A Registered</a></h3>
			<p><em>21 Jul 2017</em></p>
			<p>I wrote a story about Twitter allowing users to check or uncheck a box regarding sharing data with select Twitter partners. While I am happy to see this move from Twitter, I feel the concept of information sharing being simply being a checkbox is unacceptable. I wanted to make sure I praised Twitter in my last post, but I’d like to expand upon what I’d like to see from Twitter, as well as ALL other platforms that I depend on in my personal and professional life. There is no reason that EVERY platform we depend on couldn’t require ALL partners to use their API, resulting in every single application of our data be registered as an official OAuth application. The technology is out there, and there is no reason it can’t be the default mode for operations. There just hasn’t been the need amongst platform providers, as as no significant demand from platform users. Even if you don’t get full access to delete and adjust the details of the integration and partnership, I’d still like to see companies, share as many details as they possibly can regarding any partner sharing relationships that involve my data. OAuth is not the answer to all of the problems on this front, but it is the best solution we have right now, and we need to have more talk about how we can make it is more intuitive, informative, and usable by the average end-users, as well as 3rd party developers, and platform operators. API plus OAuth is the lowest cost, widely adopted, standards based approach to establishing a pipeline for ALL data, content, and algorithms operate within that gives a platform the access and control they desire, while opening up access to 3rd party integrators and application developers, and most importantly, it gives a voice to end-users–we just need to continue discussing how we can keep amplifying this voice. To the folks who will DM, email, and Tweet...[<a href="/2017/07/21/requiring-all-platform-partners-use-the-api-so-there-is-a-registered/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/21/misconceptions-about-what-openapi-isnt-still-slowing-conversations/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/desert_dragon_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/21/misconceptions-about-what-openapi-isnt-still-slowing-conversations/">Misconceptions About What OpenAPI Is(nt) Still Slowing Conversations</a></h3>
			<p><em>21 Jul 2017</em></p>
			<p>I’ve been pushing forward conversations around my Human Services Data API (HSDA) work lately, and hitting some friction with folks around the finer technical details of the API. I feel the friction around these API conversations could be streamlined with OpenAPI, but with most folks completely unaware of what OpenAPI is and does, there is friction. Then for the handful of folks who do know what OpenAPI is and does, I’m seeing the common misconceptions about what they think it is slowing the conversation. Let’s start with the folks who are unaware of what OpenAPI is. I am seeing two main ways that human services data vendors and implementations have conversations about what they need: 1) documentation, and 2) code. The last wave of HSDA feedback was very much about receiving a PDF or Word documentation about what is expected of an application and an API behind it. The next wave of conversations I’m having are very much here are some code implementations to demonstrate what someone is looking to articulate. Both very expensive waves of articulating and sharing what is needed for the future, or to develop a shared understanding. My goal throughout these conversations is to help folks understand that there are other more efficient, low costs ways to articulate and share what is needed–OpenAPI. Beyond the folks who are not OpenAPI aware, the second group of folks who see OpenAPI as a documentation tool, or code generation tool. Interestingly enough a vantage point that is not very far evolved beyond the first group. Once you know what you have, you document it using OpenAPI, or you generate some code samples from it. Relinquishing OpenAPI to a very downstream tool, something you bring in after all the decisions are made. I had someone say to me, that OpenAPI is great, but we need a way to be having a conversation about each specific API request, the details of the that request, with a...[<a href="/2017/07/21/misconceptions-about-what-openapi-isnt-still-slowing-conversations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/20/the-plivo-support-portal-and-knowledge-base/"><img src="https://s3.amazonaws.com/kinlane-productions2/plivo/plivo-support-knowledge-base.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/20/the-plivo-support-portal-and-knowledge-base/">The Plivo Support Portal And Knowledge Base</a></h3>
			<p><em>20 Jul 2017</em></p>
			<p>
I’m always watching out for how existing API providers are shifting up their support strategies in their communities as part of my work. This means staying into tune with their communications, which includes processing their email newsletters and developer updates. Staying aware of what is actually working, and what is not working, based upon active API service providers who are finding ways to make it all work.

Plivo opted out to phase out direct emails at the end of the month, and pushing developers to use the Plivo support portal, and the ticketing system. The support portal provides a knowledge base which provides a base of self-service support before any developer actually uses the support ticketing system to:


  Create, manage, respond to and check the status of your support ticket(s)
  Select improved ticket categories for more efficient ticket routing and faster resolution
  Receive resolution suggestions from our knowledge base before you submit a ticket to help decrease resolution time


Email only support isn’t always the most optimal way of handling support, and using a ticketing system definitely provides a nice trail to follow for both sides of the conversations. The central ticketing system also provides a nice source of content to feed into the self-service support knowledge base, keeping self-service support in sync with direct support activity.

I’m going to continue to track on which API providers offer a ticketing solution, as well as a knowledge base. I’m feeling like these are what I’m going to recommend to new API providers as what I consider to be default support building blocks that EVERY API platform should be starting with, covering the self-service and direct support requirements of a platform. I’m going to start pushing 1-3 support solutions like ZenDesk, also giving API providers some options when it comes to quickly delivering adequate support for their platforms.

[<a href="/2017/07/20/the-plivo-support-portal-and-knowledge-base/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/20/managing-platform-terms-of-service-in-a-site-policy-repository/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-site-policy.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/20/managing-platform-terms-of-service-in-a-site-policy-repository/">Managing Platform Terms of Service In A Site Policy Repository</a></h3>
			<p><em>20 Jul 2017</em></p>
			<p>Github is releasing an update to their platform Terms of Service and Corporate Terms of Service. Guess what platform their are using to manage the evolution, and release of their terms of service? Github of course! They are soliciting feedback, along with clarifications and improvements to their terms of service, with an emphasis on helping making things more readable! #nice Github has provided a deadline for everyone to submit comments by the end of the month, then they’ll spend about a week going through the comments before making any changes. It provides a pretty useful way for any platform to manage their terms of service in a way that gives the community a voice, and provides some observability into the process for everyone else who might not feel confident enough to chime in on the process. This can go a long way towards building trust with the community, even if they don’t directly participate in the process. Managing terms of service using Github makes sense for all providers, not just Github. It provides an open, transparent, and participatory way to move forward one of the most important documents that is governing API consumption. It is logical that the drafting, publishing, and evolution of platform terms be done out in the open, where the community can watch and participate. Pushing forward the design of the legal document in sync with the design, deployment, management, SDKs and other aspects of API operations. Bringing the legal side of things out of the shadows, and making it part of the conversation within the community. Eventually, I’d like to see the terms of service, privacy policies, service level agreements, and other legal documents that govern API operations managed and available on Github like this. It gives the wider API community the chance to play a more significant role in hammering out the legal side of API operations, ensuring this are easier to follow and understand, and maybe even standardized across...[<a href="/2017/07/20/managing-platform-terms-of-service-in-a-site-policy-repository/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/20/charles-proxy-generated-har-to-openapi-using-api-transformer/"><img src="https://s3.amazonaws.com/kinlane-productions2/charles-to-openapi/har-conversion.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/20/charles-proxy-generated-har-to-openapi-using-api-transformer/">Charles Proxy Generated HAR To OpenAPI Using API Transformer</a></h3>
			<p><em>20 Jul 2017</em></p>
			<p>I was responding to Jean-Philippe M. (@jpmonette) tweet regarding whether or not I had moved forward my auto generation of OpenAPIs from traffic captured by Charles Proxy. It is one of many features of my internal systems I have not gotten around to finishing, but thankfully he actually answered his own question, and found a better solution than even I had–using my friends over at API Transformer. I had been exploring ways for speeding up the process of generating OpenAPI specs for the APIs that I’m reviewing, something that becomes very tedious when working with large APIs, as well as just profiling the sheer number of APIs I am looking profile as part of my work. I haven’t been profiling many APIs lately, but the approach Jean-Philippe M. came up is petty damn easy, leaving me feeling pretty silly that I hadn’t connected the dots myself. Here is what you do. Fire up Charles Proxy: Then open up Postman, and make any API calls. Of course you could also proxy mobile application or website API calls through your Charles Proxy, but Postman is a great way to for a majority of the APIs I depend on. After you’ve made the calls to all the APIs you are looking to generate an OpenAPI for, save your Charles Proxy session as a .har file, which is the last option on the dropdown menu available while saving. Then you head over to API Transformer and upload your .har file, and select OpenAPI (Swagger) 2.0 as the output–push convert. API Transformer will then push a fresh OpenAPI to your desktop, or allow you to publish via a portal, and generate an SDK using APIMATIC. Automated (mostly) generation of OpenAPI definitions from API traffic you generate through your browser, Postman, Restlet Client, mobile application, or other tooling. I have abandoned my internal systems, except for my stack of APIs, and depending mostly on 3rd party services like Charles Proxy, Postman,...[<a href="/2017/07/20/charles-proxy-generated-har-to-openapi-using-api-transformer/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/20/100k-view-of-bot-space-from-the-api-evangelist-perspective/"><img src="https://s3.amazonaws.com/kinlane-productions2/bots-satellites.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/20/100k-view-of-bot-space-from-the-api-evangelist-perspective/">100K View Of Bot Space From The API Evangelist Perspective</a></h3>
			<p><em>20 Jul 2017</em></p>
			<p>I had a friend ask me for my thoughts on bots. It is a space I tend to rant about frequently, but isn’t an area I’m moving forward any meaningful research in, but it does seem to keep coming up and refuses to ever go way. I think bots are a great example of yet another thing that us technologists get all worked up about and think is the future, but in reality, while there will only be a handful of viable use cases, and bots will cause more harm, than they ever will do any good, or fully enjoy a satisfactory mainstream adoption. First, bots aren’t new. Second, bots are just automation. Sure, there will be some useful automation implementations, but more often than not, bots will wreak havoc and cause unnecessary noise. Conveniently though, no matter what happens, there will be money to be made deploying and defending against each wave of bot investment. Making bots is pretty representative of how technology is approached in today’s online environment. Lot’s of tech. Lot’s of investment. Regular waves. Not a lot of good sense. Top Bot Platforms Ok, where can you deploy and find bots today? These are the dominant platforms where I am seeing bots emerge: Twitter - Building bots on the public social media platform using their API. Facebook - Building Facebook messenger bots to unleash on the Facebook Graph. Slack - Building more business and productivity focused bots on Slack. There are other platforms like Telegram, and folks developing interesting Github bots, but these three platforms dominate the conversation when it comes to bots in 2017. Each platform brings it’s own tone when it comes to what bots are capable of doing, and who is developing the bots. Another important thing to note across these platforms is that Slack is really the only one working to own the bot conversation on their platform, while on Facebook and Twitter allow the developer community...[<a href="/2017/07/20/100k-view-of-bot-space-from-the-api-evangelist-perspective/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/19/the-most-important-aspect-of-the-api-discussion-is-learning-to-think-outside/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-box.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/19/the-most-important-aspect-of-the-api-discussion-is-learning-to-think-outside/">The Most Important Aspect Of The API Discussion Is Learning To Think Outside</a></h3>
			<p><em>19 Jul 2017</em></p>
			<p>
There are many good things to come out of doing APIs properly. Unfortunately there are also many bad things that can come out of doing APIs badly, or with misaligned expectations. It is easy to focus on the direct benefits of doing APIs like making data resources available to partners, or maybe developing a mobile application. I prefer looking for the more indirect benefits, which are more human, more than they are ever technical.

As I work with different groups on a variety of API definitions and strategies, one very significant part of the process I see, is people being forced to think outside their box. APIs are all about engaging around data, content, and algorithms on the web, with 3rd parties that operate outside your box. You are forced to lookup, and outward a bit. Not everyone I engage with is fully equipped to do this, for a variety of reasons, but overall the API process does make folks just a little more critical than they do with even their websites.

The web has come with a number of affordances. Those same affordances aren’t always present in API discussions forcing folks to have more conversations around why we are doing APIs (an answer shouldn’t always be yes), and discussing the finer details not just storing your data, and managing your schema, but doing in a way that will play nicely with other external systems. You may be doing things one way internally, and it might even be working for you, but it is something that can only get better with each outside partner, or consumer you are exposed to along your journey. Even with all of the internal politics I encounter in my API conversations, the API process always leaves me enjoying almost any outcome.

[<a href="/2017/07/19/the-most-important-aspect-of-the-api-discussion-is-learning-to-think-outside/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/19/more-investment-in-api-security/"><img src="https://s3.amazonaws.com/kinlane-productions2/elastic-beam/elasticbeam-security.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/19/more-investment-in-api-security/">More Investment In API Security</a></h3>
			<p><em>19 Jul 2017</em></p>
			<p>I’m getting some investment from ElasticBeam to turn up the volume on my API security research, so I will be telling more stories on the subject, and publishing an industry guide, as well as a white paper in coming weeks. I want my API security to become a first class area of my API research, along side definitions, design, deployment, management, monitoring, testing, and performance. Much of my API security research is built on top of OWASP’s hard work, but honestly I haven’t gotten very far along in it. I’ve managed to curated a handful of companies who I’ve come across in my research, but haven’t had time to dive in deeper, or fully process all the news I’ve curated there. It takes time to stay in tune with what companies are up to, and I’m thankful for ElasticBeam’s investment to help me pay the bills while I’m heads down doing this work. I am hoping that my API security research will also help encourage you to invest more into API security. As I do with my other partners, I will find ways of weaving ElasticBeam into the conversation, but my stories, guides, and white papers will be about the wider space–which Elastic Beam fits in. I’m hoping they’ll compliment Runscope as my partner when it comes to monitoring, testing, and performance (see how I did that, I worked Runscope in too), adding the security dimension to these critical layers of operating a reliable API. One thing that attracted me to conversations with ElasticBeam was that they were developing a solution that could augment existing API management solutions like 3Scale and Amazon Web Services. I’ll have a talk with the team about integrating with Tyk, DreamFactory, and Restlet–my other partners. Damn I’m good. I got them all in here! Seriously though, I’m thankful for these partners investing in what I do, and helping me tell more stories on the blog, and produce more guides and...[<a href="/2017/07/19/more-investment-in-api-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/19/does-your-platform-have-an-integrations-page/"><img src="https://s3.amazonaws.com/kinlane-productions2/airtable/airtable-integrations-page.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/19/does-your-platform-have-an-integrations-page/">Does Your Platform Have An Integrations Page?</a></h3>
			<p><em>19 Jul 2017</em></p>
			<p>
I’m continuing to come across more dedicated integration pages for the API platforms I’m test driving, and keeping an eye on. This time it is out of spreadsheet and database hybrid AirTable, that allows you to easily deploy an API complete with a portal, with a pretty robust integrations page for their platform. Airtable’s dedicated integrations page is made easier since they use Zapier, which helps them aggregate over 750+ APIs for possible integration.

Airtable is pretty slick all by itself, but once you start wiring it up to some of the other API driven platforms we depend on, it becomes a pretty powerful tool for data aggregation, and then publishing as an API. I don’t understand why a Zapier-driven API integrations page isn’t default for every API platform out there. API consumption today isn’t just about deploying web or mobile applications, it is about moving data and content around the web–making sure it is where we need it, when we need it.

I’m playing with different variations of the API integrations page lately. I’m exploring the idea of how I can encourage some higher education folks I know, and government open data folks I know to be Zapier advocates within their organizations, and publish a static integrations page, showing the integrations solutions available around the platforms they depend on. Dedicated integration pages help API developers understand the potential of any API, and they help non-developers also understand the potential, but in a way they can easily put into action to solve problems in their world. I’m going to keep beating the API integration page drum, and now that Zapier has their partner API you will also hear me talking about Zapier a lot more.

[<a href="/2017/07/19/does-your-platform-have-an-integrations-page/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/19/containerized-microservices-monitoring-driving-api-infrastructure/"><img src="https://s3.amazonaws.com/kinlane-productions2/netsil/1-P8w_-2-oCz0QfV6OENawJQ.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/19/containerized-microservices-monitoring-driving-api-infrastructure/">Containerized Microservices Monitoring Driving API Infrastructure</a></h3>
			<p><em>19 Jul 2017</em></p>
			<p>While I track on what is going on with visualizations generated from data, I haven’t seen much when it comes to API driven visualizations, or specifically visualization about API infrastructure, that is new and interesting. This week I came across an interesting example in a post from Netsil about mapping microservices so that you can monitor them. They are a pretty basic visualization of each database, API, and DNS element for your stack, but it does provide solid example of visualizing not just the deployment of database and API resources, but also DNS, and other protocols in your stack. Netsil microservices visualization is focused on monitoring, but I can see this type of visualization also being applied to design, deployment, management, logging, testing, and any other stop along the API lifecycle. I can see API lifecycle visualization tooling like this becoming more common place, and play more of a role in making API infrastructure more observable. Visualizations are an important of the storytelling around API operations that moves things from just IT and dev team monitoring, making it more observable by all stakeholders. I’m glad to see service providers moving the needle with helping visualize API infrastructure. I’d like to see more embeddable solutions deployed to Github emerge as part of API life cycle monitoring. I’d like to see what full life cycle solutions are possible when it comes to my partners like deployment visualizations from Tyk and Dreamfactory APIs, and management visualizations with 3Scale APIs, and monitoring and testing visualizations using Runscope. I’ll play around with pulling data from these provides, and publishing to Github as YAML, which I can then easily make available as JSON or CSV for use in some basic visualizations. If you think about it, thee really should be a wealth of open source dashboard visualizations that could be embedded on any public or private Github repository, for every API service provider out there. API providers should be able to...[<a href="/2017/07/19/containerized-microservices-monitoring-driving-api-infrastructure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/18/specialized-collections-of-machine-learning-apis-could-be-interesting/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorithmia/algorithmia-enterprise.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/18/specialized-collections-of-machine-learning-apis-could-be-interesting/">Specialized Collections Of Machine Learning APIs Could Be Interesting</a></h3>
			<p><em>18 Jul 2017</em></p>
			<p>I was learning more about CODEX, from Algorithmia, their enterprise platform for deploying machine learning API collections on premise or in the cloud. Algorithmia is taking the platform in which their algorithmic marketplace is deployed on and making it so you can deploy it anywhere. I feel like this is where the algorithmic-centered API deployment is heading, potentially creating some very interesting, and hopefully specialized collections of machine learning APIs. I talked about how the economics of what Algorithmia is doing interests me. I see the potential when it comes to supporting machine learning APIs that service an image or video processing pipeline–something I’ve enjoyed thinking about with my drone prototype. Drone is just one example of how specialized collections of machine learning APIs could become pretty valuable when they are deployed exactly where they are needed, either on-premise or in any of the top cloud platforms. Machine learning marketplaces operated by the cloud giants will ultimately do fine because of their scale, but I think where the best action will be at is delivering curated, specialized machine learning models, tailored to exactly what people need, right where they need them–no searching necessary. I think recent moves by Google to put TensorFlow on mobile phones, and Apple making similar moves show signs of a future where our machine learning APIs are portable, operating on-premise, on-device, and on-network. I see Algorithmia having two significant advantages right now. 1) they can deploy their marketplace anywhere, and 2) they have the economics, as well as the scaling of it figured out. Allowing for specialized collections of machine learning APIs to have the metering, and revenue generation engines built into them. Imagine a future where you can deploy and machine learning and algorithmic API stack within any company or institution, or the factory floor in an industrial setting, and out in the field in an agricultural or mining situation–processing environmental data, images, or video. Exploring the possibilities with real...[<a href="/2017/07/18/specialized-collections-of-machine-learning-apis-could-be-interesting/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/18/one-api-development-partner-every-api-provider-should-have/"><img src="https://s3.amazonaws.com/kinlane-productions2/zapier/4b21d50900beffcc0bcfa2c09bcc7bfe.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/18/one-api-development-partner-every-api-provider-should-have/">One API Development Partner Every API Provider Should Have</a></h3>
			<p><em>18 Jul 2017</em></p>
			<p>
Yet another reason to be making sure Zapier is part of your API operations–issue management. Zapier is now providing an important window into how people are integrating with your API(s)–now any public API connected to Zapier can see filtered, categorized feedback from their users with Zapier Issues, and use that information to improve upon their APIs and integrations. This is the biggest movement I’ve seen in my API issues research since I first started doing it on April of 2016.

Zapier Issues doesn’t just provide you with a look at the issues that arise within API integrations (the bad news), it also provides you with a feedback look where you can engage with Zapier users who have integrated with your API, and hear feature requests (the good news), and other road map influencing suggestions. Zapier sees, “thousands of app combinations and complex workflows from more than 1.5 million people—and we want to give you more insight into how your best customers use your app on Zapier.”

It is another pretty big reason that ALL API providers should be baking Zapier into their platforms. Not only will you be opening up API consumption to the average business user, you can now get feedback from them, and leverage the wisdom Zapier has acquired integrating with over 750 APIs. As an API provider you should be jumping at this opportunity to get this type of feedback on your API resources. Helping you make sure your APIs more usable, stable, reliable, and providing the solutions that actual business users are needing to solve the problems they encounter in their daily lives.

[<a href="/2017/07/18/one-api-development-partner-every-api-provider-should-have/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/18/http-status-codes-are-an-essential-part-of-api-design-and-deployment/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/runscope/runscope-200-ok.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/18/http-status-codes-are-an-essential-part-of-api-design-and-deployment/">HTTP Status Codes Are An Essential Part Of API Design And Deployment</a></h3>
			<p><em>18 Jul 2017</em></p>
			<p>It takes a lot of work provide a reliable API that people can depend on. Something your consumers can trust, and will provide them with consistent, stable, meaningful, and expected behavior. There are a lot of affordances built into the web, allowing us humans to get around, and make sense of the ocean of information on the web today. These affordances aren’t always present with APIs, and we need to communicate with our consumers through the design of our API at every turn. One area I see IT and developer groups often overlook when it comes to API design and deployment are HTTP Status Codes. That standardized list of meaningful responses that come back with every web and API request: 1xx Informational - An informational response indicates that the request was received and understood. It is issued on a provisional basis while request processing continues. 2xx Success - This class of status codes indicates the action requested by the client was received, understood, accepted, and processed successfully. 3xx Redirection - This class of status code indicates the client must take additional action to complete the request. Many of these status codes are used in URL redirection. 4xx Client errors - This class of status code is intended for situations in which the client seems to have errored. 5xx Server error - The server failed to fulfill an apparently valid request. Without HTTP Status codes, application won’t every really know if their API request was successful or not, and even if an application can tell there was a failure, it will never understand why. HTTP Status Codes are fundamental to the web working with browsers, and apis working with applications. HTTP Status Codes should never be left on the API development workbench, and API providers should always go beyond just 200 and 500 for every API implementation. Without them, NO API platform will ever scale, and support any number of external integrations and applications. The most...[<a href="/2017/07/18/http-status-codes-are-an-essential-part-of-api-design-and-deployment/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/18/diagramming-the-components-of-api-observability/"><img src="https://s3.amazonaws.com/kinlane-productions2/observable/api-observability-venn.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/18/diagramming-the-components-of-api-observability/">Diagramming The Components Of API Observability</a></h3>
			<p><em>18 Jul 2017</em></p>
			<p>I created a diagram of the politics of APIs sometime ago that has really held true for me, and is something I’ve continue to reference as part of my storytelling. I wanted to do a similar thing to help me evolve my notion of API observability. Like the politics of APIs, observability overlaps many areas of my API life cycle research. Also like the politics of APIs, observability involves many technical, business, and legal aspects of operating a platform online today.

Here is my first draft of a Venn diagram beginning to articulate what I see as the components of API observability:



The majority of the API observability conversation in the API space currently centers around logging, monitoring, and performance–driven by internal motivations, but done in a way that is very public. I’m looking to push forward the notion of API observability to transcend the technical, and address the other operational, industry, and even regulatory concerns that will help bring observability to everyone’s attention.

I do not think we should always be doing API, AI, ML and the other tech buzzwords out there if we do not have to–saying no to technology can be done. In the other cases where the answer is yes, we should be doing API, AI, and ML in an observable way. This is my core philosophy. The data, content, algorithms, and networks we are exposing using APIs, and using across web, mobile, device, and network applications should be observable by internal groups, as well as partners, and public stakeholders as it makes sense. There will be industry, community, and regulatory benefits for sectors that see observability as a positive thing, and go beyond just the technical side of observability, and work to be more observable in all the areas I’ve highlight above.

[<a href="/2017/07/18/diagramming-the-components-of-api-observability/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/17/writing-api-stories-that-speak-to-but-also-influences-their-view-of-technology/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-storytelling.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/17/writing-api-stories-that-speak-to-but-also-influences-their-view-of-technology/">Writing API Stories That Speak To But Also Influences Their View Of Technology</a></h3>
			<p><em>17 Jul 2017</em></p>
			<p>I know that some of my friends who follow API Evangelist shake their heads when I talk about API business models, partner programs, and many of the business sides of API operations. Much of my work will have an almost delusional attraction towards the concept of an API. Heavily doused in a belief in technology as a solution. This isn’t accidental. This is API Evangelist. A persona I have developed to help me make a living, and help influence where we go (or don’t go) with technology. I am delusional enough to think I can influence change in how the world uses technology. I’m borderline megalomaniac, but there really is not sufficient ego to get me quite all the way there. While still very, very, very minor, I feel I have influenced where technology has flowed over my seven years as the API Evangelist. Even if it just slowing the speed (seconds) at which the machines turn on us, and kills us all. If nothing else, I know there are few folks out there who I have touched, and shaped how they see, use, and allow technology in their lives (cause they told me so). Through my storytelling on API Evangelist, I am always looking for the next convert–even if it takes years and hundreds of stories. A significant portion of this outreach involves telling stories that reach my intended audience–usually startups, business, institutional, and government agency workers and influencers. To reach them I need to tell stories that speak to them, and feed their current goals around finding success in their startup, or their role within businesses, institutions, and government agencies. With this in mind, I am always trying to bend my stories in their direction, talking about topics that they’ll care about, and tune into. Once I have their attention, I will work on them in other ways. I’ll help them think about their business model, but also help them understand transparency and...[<a href="/2017/07/17/writing-api-stories-that-speak-to-but-also-influences-their-view-of-technology/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/17/making-all-subresources-available-within-the-core-set-of-human-service-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/3D-Printing/regions/api-regions-global-map-from-google.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/17/making-all-subresources-available-within-the-core-set-of-human-service-apis/">Making All Sub-Resources Available Within The Core Set Of Human Service APIs</a></h3>
			<p><em>17 Jul 2017</em></p>
			<p>I had recently taken the Human Services Data Specification (HSDS) and exposed it as a set of API paths that provide access to about 95% of the schema, which we are calling the Human Services Data API (HSDA). When you make a call to the /organizations/ path, you receive an array collection of organizations that each match the HSDA organization schema. The same applies when you make a call to the /locations, /contacts, and /services, opening up access to the entire schema–minus three objects I pushed off until future releases. After the core set of API paths /organization, /service, /location, /contact, there are a set of sub-resources available across those as it makes sense–including /phone, /programs, /physical_address, /postal_address, /regular_schedule, /holiday_schedule, /funding, /eligibility, /service_area, /required_document, /payment_accepted, /language, /accessiblity_for_disabilities, and /service_at_location_id. I took the HSDA schema, and published API paths for each sub-resource so that it exactly returned, and accepted HSDA compliant schema–making all aspects of the schema accessible via an API, with POST and PUT requests accepting compliant schema, and GET returning compliant schema. One of the “stoppers” we received from several folks in the HSDS community during the feedback cycle going from version 1.0 of the API to version 1.1, was that the design was overly complex, and that it would serve any of the human services use cases on the table currently, unless you could get at all the sub resources directly with each core API path, eliminating the need for making additional call(s) to each sub-resource. You could get at everything about an /organization, /service, /location, and /contact in a single API URL. Currently the core four API paths accept and return the following schema: Organization Field Name Type (Format) Description Required? Unique? id string (uuid) Each organization must have a unique identifier. True True name string The official or public name of the organization. True False alternate_name string Alternative or commonly used name for the organization. False False description string A brief...[<a href="/2017/07/17/making-all-subresources-available-within-the-core-set-of-human-service-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/17/bot-observability-for-every-platform/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-bot-showcase.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/17/bot-observability-for-every-platform/">Bot Observability For Every Platform</a></h3>
			<p><em>17 Jul 2017</em></p>
			<p>I lightly keep an eye on the world of bots, as APIs are used to create them. In my work I see a lot of noise about bots usually in two main camps: 1) pro-bot - bots are the future, and 2) anti-bot - they are one of the biggest threats we face on the web. This is a magical marketing creating formula, which allows you to sell products to both sides of the equation, making money off of bot creation, as well as bot identification and defense–it is beautiful (if you live by disruption). From my vantage point, I’m wondering why platforms do not provide more bot observability as a part of platform operations. There shouldn’t be services that tell us which accounts are bots, the platform should tell us by default, which users are real and which are automated (you know you know). Platforms should embrace automation and providing services and tooling to assist in their operation, which includes actual definitions of what is acceptable, and what unacceptable bot behavior. Then actually policing this behavior, and being observable in your actions around bot management and enforcement. It feels like this is just another layer of technology that is being bastardized by the money that flow around technology so easily. Investment in lots of silly useless bots. Investment in bot armies that inflate customer numbers, advertising, and other ways of generating attention (to get investment), and generate revenue. It feels like Slack is the only leading bot platform that has fully embraced the bot conversation. Facebook and Twitter lightly reference the possibilities, and have made slight motions when it comes to managing the realities of bots, but when you Google “Twitter Bots” or “Facebook Bots”, neither of them dominate the conversation around what is happening–which very telling around how they view the world of bots. Slack has a formal bots directory, and has defined the notion of a bot user, separating them from users–setting...[<a href="/2017/07/17/bot-observability-for-every-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/13/quantifying-the-difference-between-human-services-data-specification-hsds/"><img src="https://s3.amazonaws.com/kinlane-productions2/3D-Printing/regions/api-regions-global-map-from-google.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/13/quantifying-the-difference-between-human-services-data-specification-hsds/">Quantifying The Difference Between Human Services Data Specification (HSDS)</a></h3>
			<p><em>13 Jul 2017</em></p>
			<p>To help quantify the move from version 1.0 to 1.1 of the Human Services Data API (HSDA) definition I took the existing Ohana API and created an OpenAPI definition to describe what was present in version 1.0 of the HSDA. Then I took version 1.1 of the Human Services Data Specification (HSDS) and made sure as much of HSDS was returned as part of API responses, as well as allowing adding, updating, and deleting across the schema. During the vendor API review portion of our process I took the documentation for four of the vendors APIs and created OpenAPI for each of them. I then laid all the vendor OpenAPIs alongside the current draft I had of the HSDA definition. I then consider each path, the parameters, body, and responses for inclusion as part of the HSDA definition. This allowed me to consider the existing vendor API implementations that are already serving human service implementations. OpenAPI plays a central role in defining what is, what might be, while opening up a forum for having a conversation about the specific detail of the HSDS/A definition. I’m using OpenAPI to establish a definition of what both HSDS and HSDA are. It will be the contract that gets hammered out as part of the Open Referral governance process, so you will see me use it regularly to articulate specific aspects of what is going. With this in mind, I’d like to use a distilled OpenAPI, articulated just a single API path for GET /organizations. I won’t go into to much detail on the OpenAPI, I recommend learning more about the specification on the GitHub repository, and at the OpenAPI Initiative (OAI). What I’d like to articulate for this story is to help quantity the separation and connection between the Human Services Data Specification (HSDA), and the Human Services Data API (HSDA), using this single OpenAPI, describing a single HSDA path–organizations. When you take the schemes: located at line...[<a href="/2017/07/13/quantifying-the-difference-between-human-services-data-specification-hsds/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/13/moving-the-human-services-api-specification-from-version-1-1-to-1-2/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-referral/hsda-organizations-documentation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/13/moving-the-human-services-api-specification-from-version-1-1-to-1-2/">Moving The Human Services API Specification From Version 1.1 to 1.2</a></h3>
			<p><em>13 Jul 2017</em></p>
			<p>I am preparing for the recurring governance meeting for the Open Referral Human Services Data API standard–which I’m the technical lead for. I need to load up every detail of my Human Services Data API work into my brain, and writing stories is how I do this. I need to understand where the definition is with v1.1, and encourage discussion around a variety of topics when it comes to version 1.2. Constraints From Version 1.0 To v1.1 I wasn’t able to move as fast as I’d like from 1.0 to 1.1, resulting in me leaving out a number of features. The primary motivation to make sure as much of the version 1.1 of Human Services Data Specification (HSDS) was covered as possible–something I ended up doing horizontally with new API paths, over loading up the core paths of /organizations, /locations, and /services. There were too many discussion on the table regarding the scope and filtering of data, and schema for these core paths. Something which led to a discussion, about /search–resulting in me pushing off API design discussions on how to expand vertically at the core API path level to future versions. There were just too many decisions to make at the API request and response level for me to make a decision in all the areas–warranting more discussion. Additionally, there were other API design discussion regarding operational, validation, and more utility APIs to discuss for inclusion in future versions expanding the scope and filtering discussions to the API path, and now API project level. In preparation for our regular governance meeting I wanted to run through all of the open API design issues, as well as additional projects the community needs to be thinking about. API Design As part of my Human Services Data API (HSDA) work we have opened up a pretty wide API design conversation regarding where the API definition could (should) be going. I’ve tried to capture the conversations going on...[<a href="/2017/07/13/moving-the-human-services-api-specification-from-version-1-1-to-1-2/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/12/providing-solid-examples-that-api-consumers-can-learn-from-like-slack-app/"><img src="https://s3.amazonaws.com/kinlane-productions2/slack/slack-app-blueprints.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/12/providing-solid-examples-that-api-consumers-can-learn-from-like-slack-app/">Providing Solid Examples That API Consumers Can Learn From Like Slack App</a></h3>
			<p><em>12 Jul 2017</em></p>
			<p>
People often learn through example. Before I’d ever consider myself a software engineer, I’d consider myself a reverse software engineer. 93% of what I know has been extracted from the work of others. Even with 7% being of my own creation, it is always heavily influenced by the work of others. People emulate what they know, what they see, and use. This is why as an API provider you should be showcasing best practices, positive examples, and healthy blueprints of what API consumers could (should) be doing.

You can see this in action with Slack’s best practice blueprints page, where they provide six blueprints of applications that API consumers should be learning from. Slack doesn’t just provide a title, description and image of example applications, it is truly a blueprint–providing diagrams, links to documentation, code samples, and other essential knowledge you will need to successfully develop an application on Slack. Providing six solid examples that anyone can reverse engineer to understand how Slack application development could (should) work.

Slack app blueprints is just one component of a pretty sophisticated getting started section offered as part of the Slack API ecosystem. I am adding application blueprint as a building block to my getting started API research, and adding it as a dimension to my API documentation &amp; SDK research–the overlap in these areas seem like it should be strong to me. Coming across Slack app blueprints, and writing this story has reminded me that I also need to write another piece on the Slack ecosystem, and generate an outline of all the building blocks they are using in their API ecosystem, and create an updated blueprint for successful API operations that other API providers can emulate.

[<a href="/2017/07/12/providing-solid-examples-that-api-consumers-can-learn-from-like-slack-app/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/12/challenges-when-aggregating-data-published-across-many-years/"><img src="https://s3.amazonaws.com/kinlane-productions2/hack-education/ed-tech-investment-research.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/12/challenges-when-aggregating-data-published-across-many-years/">Challenges When Aggregating Data Published Across Many Years</a></h3>
			<p><em>12 Jul 2017</em></p>
			<p>My partner in crime is working on a large data aggregation project regarding ed-tech funding. She is publishing data to Google Sheets, and I’m helping her develop Jekyll templates she can fork and expand using Github when it comes to publishing and telling stories around this data across her network of sites. Like API Evangelist, Hack Education runs as a network of Github repositories, with a common template across them–we call the overlap between API Evangelist, Contrafabulists. One of the smaller projects she is working on as part of her ed-tech funding research involves pulling the grants made by the Gates Foundation since the 1990s. Similar to my story a couple weeks ago about my friend David Kernohan, where he was wanting to pull data from multiple sources, and aggregate into a single, workable project. Audrey is looking to pull data from a single source, but because the data spans almost 20 years–it ends up being a lot like aggregating data from across multiple sources. A couple of the challenges she is facing trying to gather the data, and aggregate as a common dataset are: PDF - The enemy of any open data advocate is the PDF, and a portion of her research data data is only available in PDF format which translates into a good deal of manual work. Search - Other portions of the data is available via the web, but obfuscated behind search forms requiring many different searches to occur, with paginated results to navigate. Scraping - The lack of APIs, CSV, XML, and other machine readable results raises the bar when it comes to aggregating and normalizing data across many years, making scraping a consideration, but because of PDFs, and obfuscated HTML pages behind a search, even scraping will have a significant costs. Format - Even once you’ve aggregated data from across the many sources, there is a challenge with it being in different formats. Some years are broken down by...[<a href="/2017/07/12/challenges-when-aggregating-data-published-across-many-years/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/12/a-zapier-advocate-and-dedicated-api-resources-page-for-your-company/"><img src="https://s3.amazonaws.com/kinlane-productions2/zapier/zapier-icons.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/12/a-zapier-advocate-and-dedicated-api-resources-page-for-your-company/">A Zapier Advocate And Dedicated API Resources Page For Your Company</a></h3>
			<p><em>12 Jul 2017</em></p>
			<p>I am spending time going through some of the most relevant APIs I know of online today, working to create some 101 training materials for average folks to take advantage of. I’m looking through these APIs: Twitter, Google Sheets, Github, Flickr, Instagram, Facebook, YouTube, Slack, Dropbox, Paypal, Weather Underground, Spotify, Google Maps, Reddit, Pinterest, NY Times, Twilio, Stripe, SendGrid, Algolia, Keen, Census, Yelp, Walgreens. I feel they are some of the most useful solutions in the average business person who is API curious. With these new lessons I’m trying to continue my work evangelizing APIs amongst the normals, helping them understand what APIs are, and what is possible when you put them to work. Once I introduce folks to each API I’m left with the challenge of how do I actually onboard them with each API when they aren’t actually a programmer. The number one way I’m helping alleviate this problem is by including Zapier examples with each of my API lessons, helping folks understand that they can quickly get up and running with each API using the Zapier integration platform as a service (iPaaS). I will be including one or more Zapier examples along with each of my API 101 lessons, helping normal folk put what they’ve learned about APIs to use–hopefully making each lesson a little more sticky. One of the primary targets for my lessons is the average worker at small, medium, and enterprise businesses, trying to help them understand that APIs aren’t just for developers, and that they can be putting APIs to use in their world. I tried to pick a handful of APIs that are relevant and useful in their daily lives, and helping them become aware of useful Zapier recipes they can adopt in their daily work. I’m looking to encourage users to become more API-literate, and begin connecting and orchestrating using APIs in their daily work. I’m hoping that eventually they will become confident enough by leverage...[<a href="/2017/07/12/a-zapier-advocate-and-dedicated-api-resources-page-for-your-company/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/12/20k-40k-60k-and-80k-foot-levels-of-industry-api-design-guidance/"><img src="https://s3.amazonaws.com/kinlane-productions2/definition-of-high-altitude.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/12/20k-40k-60k-and-80k-foot-levels-of-industry-api-design-guidance/">20K, 40K, 60K, and 80K Foot Levels Of Industry API Design Guidance</a></h3>
			<p><em>12 Jul 2017</em></p>
			<p>I am moving my Human Services Data API (HSDA) work forward and one of the top items on the list to consider as part of the move from version 1.1 to 1.2 is all around the scope of the API design portion of the standard. We are at a phase where the API design still very much reflects the Human Services Data Specification (HSDS)–basically a very CRUD (Create, Read, Update and Delete) API. With version 1.2 I need to begin considering the needs of API consumers a little more, looking to vendors and real world practitioners to help understand what the next version(s) of the API definition will/should contain. The most prominent discussion in the move from version 1.1 to 1.2 centers around scope of API design at four distinct levels of this work, where we are looking to move forward a variety of API design concerns for a large group of API consumers: Data Scope / Filtering - Discussions around how to filter data, allowing API consumers to search across the contents of any HSDA implementation, getting exactly the data they need, no more, no less. Schema Scope / Filtering - Considering the design of simple, standard, or full schema responses that can specified using a prefer header, parameter, or path levels. Path Scope / Filtering - How are API paths going to be group and organized, allowing a large surface area to be shared via documentation (APIs.json) in a way that new API consumers can start simple, advanced users can get what they need, and serving as many needs in between as we can. Project Scope / Filtering - Adding the fourth dimension to this scope / filtering discussion, I’m proposing we discuss how projects are defined and isolated, which can allow them to move forward at different rates, and be reflected in documentation, code, and other resources–allowing for filtering by consumers, as well as prioritization by vendors involved in the API design...[<a href="/2017/07/12/20k-40k-60k-and-80k-foot-levels-of-industry-api-design-guidance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/11/when-you-publish-a-google-sheet-to-the-web-it-also-becomes-an-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/google-sheets/google-sheets-icon.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/11/when-you-publish-a-google-sheet-to-the-web-it-also-becomes-an-api/">When You Publish A Google Sheet To The Web It Also Becomes An API</a></h3>
			<p><em>11 Jul 2017</em></p>
			<p>When you take any Google Sheet and choose to publish it to the web, you immediately get an API. Well, you get the HTML representation of the spreadsheet (shared with the web), and if you know the right way to ask, you also can get the JSON representation of the spreadsheet–which gives you an interface you can program against in any application. Articles I curate, the companies, institutions, organizations, government agencies, and everything else I track on lives in Google Sheets that are published to the web in this way. When you are viewing any Google Sheet in your browser you are viewing it using a URL like: https://docs.google.com/spreadsheets/d/[sheet_id]/edit Of course, [sheet_id] is replaced with the actual id for your sheet, but the URL demonstrates what you will see. Once you publish your Google sheet to the web you are given a slight variation on that url: https://docs.google.com/spreadsheets/d/[sheet_id]/pubhtml This is the URL you will share with the public, allowing them to view the data you have in your spreadsheet in their browsers. In order to get at a JSON representation of the data you just need to learn the right way to craft the URL using the same sheet id: https://spreadsheets.google.com/feeds/list/[sheet_id]/default/public/values?alt=json Ok, one thing I have to come clean on is that the JSON available for each Google sheet is not the most intuitive JSON you will come across, but once you learn what is going on you can easily consume the data within a spreadsheet using any programming languages. Personally, I use a JavaScript library called tabletop.js that quickly helps you make sense of a spreadsheet and get to work using the data in any (JavaScript) application. The fastest, lowest cost way to deploy an API is to put some data in a Google Sheet, and hit publish to the web. Ok, its not a full blown API, it’s just JSON available at a public URL, but it does provide an interface you can...[<a href="/2017/07/11/when-you-publish-a-google-sheet-to-the-web-it-also-becomes-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/11/locking-down-drones-and-iot-devices-by-manufacturers/"><img src="https://s3.amazonaws.com/kinlane-productions2/drones/drone-rock-outdoors.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/11/locking-down-drones-and-iot-devices-by-manufacturers/">Locking Down Drones And IoT Devices By Manufacturers</a></h3>
			<p><em>11 Jul 2017</em></p>
			<p>I have been following stories about, as well as personally experiencing DJI restricting where their drones can fly, going beyond just warning you about restricted areas and actually locking down or restricting your drone capabilities. So it was interesting to also read a post in Motherboard about the company also locking down drones to prevent against hacking, modifying, and tweaking your DJI drones as you wish. Drones for me are a poster child for the entire Internet of Things (IoT), and I think DJI’s approach is a sign of what is to come for all Internet connected devices. In coming years, there will be a lot that the IoT community can learn from the drone space. From the technical to regulatory, drones will be pushing forward conversations about our networks, cameras, security, privacy, surveillance, and corporate and government control over us, and our devices. Drones stimulate some interesting emotions within people associated with the industry, but more importantly people who know nothing about drones, and will be weighing in on regulation at the municipal, all the way up to the federal and international levels. I thought it was interesting when DJI began enforcing the recommendations I get in the dashboard for my drones, and requiring that I update my drones, RC controller, and mobile applications to reduce their liability regarding what I an actually doing with my devices. However, locking down drones so people can’t modify, augment, or fix their own drones is a whole other layer to this discussion that isn’t just about stopping ISIS from strapping bombs to their drones, it is also about maintaining sovereignty over their creations, and limiting what we can do as owners when it comes to fixing our devices. We already see the right to fix conversation bubble up in the John Deere ecosystem, but it is something we will continue to see showing up in IoT ecosystems across many different business sectors. The bold entry into our...[<a href="/2017/07/11/locking-down-drones-and-iot-devices-by-manufacturers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/11/either-you-own-the-conversation-around-your-apis-or-someone-else-will/"><img src="https://s3.amazonaws.com/kinlane-productions2/rogue/tinder-api-google-search.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/11/either-you-own-the-conversation-around-your-apis-or-someone-else-will/">Either You Own The Conversation Around Your APIs Or Someone Else Will</a></h3>
			<p><em>11 Jul 2017</em></p>
			<p>I was looking at how many of the top mobile applications in the iTunes story actually had a public API presence, and was finding it very telling what came up in the Google search results for each company when I searched [company name] + API. It tells a lot about how a company sees the world, when they don’t have a public API presence, but they have a very public mobile application that uses APIs. An example of this is with Tinder, where the top listings are all Github rogue API repositories, when you Google “Tinder API”. Tinder doesn’t own the conversation when it comes to their own APIs. While the Tinder APIs are public, and well documented, Tinder prefers acting like they are private–they aren’t. Pinterest uses SSL pinning, but there is even a good amount of information out there at how to get around that, making the mapping out and documenting of Tinder APIs a pretty doable thing. Honestly, I don’t care about Tinder’s APIs. They are just an easy example to point a finger at and use as a poster child. I don’t even expect them to have fully public APIs that any developer could use without permission. Sure, lock that shit down, but provide a sandbox, and make sure every application gets approval before they can more access to live data. Make sure that you own the API conversation by having a developers portal, and provide information regarding what it takes to get access, and maybe some day actually become an approved partner. I’m not saying that every company should have freely available public APIs. I’m saying every company should own the public conversation around their APIs, no matter what their strategy for developing applications around a platform’s APIs. Have a presence. Own the conversation. Have a door for application developers to walk, even if there is a waiting room. Not all applications will be competing with your own web, mobile,...[<a href="/2017/07/11/either-you-own-the-conversation-around-your-apis-or-someone-else-will/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/11/each-airtable-datastore-comes-with-complete-api-and-developer-portal/"><img src="https://s3.amazonaws.com/kinlane-productions2/airtable/airtable-api-for-or2.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/11/each-airtable-datastore-comes-with-complete-api-and-developer-portal/">Each Airtable Datastore Comes With Complete API and Developer Portal</a></h3>
			<p><em>11 Jul 2017</em></p>
			<p>I see a lot of tools come across my desk each week, and I have to be honest I don’t alway fully get what they are and what they do. There are many reasons why I overlook interesting applications, but the most common reason is because I’m too busy and do not have the time to fully play with a solution. One application I’ve been keeping an eye on as part of my work is Airtable, which I have to be honest, I didn’t get what they were doing, or really I just didn’t notice because I was too busy. Airtable is part spreadsheet, part database, that operates as a simple, easy to use web application, which with a push of a button, you can publish an API from. You don’t just get an API by default with each Airtable, you get a pretty robust developer portal for your API complete with good looking API documentation. Allowing you to go from an Airtable (spreadsheet / database) to API and documentation–no coding necessary. Trust me. Try it out, anyone can create an Airtable and publish an API that any developer can visit and quickly understand what is going on. As a developer, API deployment still feels like it can be a lot of work. Then, once I take off my programmers hat, and put on my business user hat, I see that there are some very easy to use solutions like Airtable available to me. Knowing how to code is almost slowing me down when it comes API deployment. Sure, the APIs that Airtable publishes aren’t the perfectly designed, artisanally crafted API I make with my bare hands, but they work just as well as mine. Most importantly, they get business done. No coding necessary. Something that anyone can do without the burden of programming. Airtable provides me another solution that I can recommend that my readers and clients should consider using when managing their data,...[<a href="/2017/07/11/each-airtable-datastore-comes-with-complete-api-and-developer-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/10/opportunity-to-develop-a-threat-intelligence-aggregation-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/facing-cannon_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/10/opportunity-to-develop-a-threat-intelligence-aggregation-api/">Opportunity To Develop A Threat Intelligence Aggregation API</a></h3>
			<p><em>10 Jul 2017</em></p>
			<p>I came across this valuable list of threat intelligence resources and think that the section on information sources should be aggregated and provided as a single threat intelligence API. When I come across valuable information repos like this my first impulse is to go through them, standardize and upload as JSON and YAML to Github, making all of this data forkable, and available via an API. Of course if I responded to every impulse like this I would never get any of my normal work done, and actually pay my bills. A second option for me is to put things out there publicly in hopes that a) someone will pay me to do the work, or b) someone else who has more time, and the rent paid will tackle the work. With this in mind, this list of sources should be standardized, and publish to Github and as an API: Alexa Top 1 Million sites - Probable Whitelist of the top 1 Million sites from Amazon(Alexa). APT Groups and Operations - A spreadsheet containing information and intelligence about APT groups, operations and tactics. AutoShun - A public service offering at most 2000 malicious IPs and some more resources. BGP Ranking - Ranking of ASNs having the most malicious content. Botnet Tracker - Tracks several active botnets. BruteForceBlocker - BruteForceBlocker is a perl script that monitors a server’s sshd logs and identifies brute force attacks, which it then uses to automatically configure firewall blocking rules and submit those IPs back to the project site, http://danger.rulez.sk/projects/bruteforceblocker/blist.php. C&amp;C Tracker - A feed of known, active and non-sinkholed C&amp;C IP addresses, from Bambenek Consulting. CI Army List - A subset of the commercial CINS Score list, focused on poorly rated IPs that are not currently present on other threatlists. Cisco Umbrella - Probable Whitelist of the top 1 million sites resolved by Cisco Umbrella (was OpenDNS). Critical Stack Intel - The free threat intelligence parsed and aggregated by Critical...[<a href="/2017/07/10/opportunity-to-develop-a-threat-intelligence-aggregation-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/10/having-the-right-communications-pipeline-for-your-api-platform/"><img src="https://s3.amazonaws.com/kinlane-productions2/matts-blog.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/10/having-the-right-communications-pipeline-for-your-api-platform/">Having The Right Communications Pipeline For Your API Platform</a></h3>
			<p><em>10 Jul 2017</em></p>
			<p>My friend Matthew Reinbold, formerly of Vox Pop, and now the Lead for the Capital One API Center of Excellence, as well as the maintainer of web API events has shifted his blogging platform to use Github, using Jekyll. Ok, yawn, why is this news? Someone is shifting the underlying platform for their blog. Well, first Matt is one of the leading API practitioners in the space, who is also a storyteller. Second, his approach highlights a set of tools that other API providers should be considering for their API communications pipeline. Matt is using a pretty potent formula for his communications platform in my opinion, with a handful of essential ingredients: Github - Using a Github repository as the open source folder for your website. Github Pages - Using Github Pages to publish the front-end for your website. Jekyll - The content management system that sits in the folder for your website. CloudFlare - The DNS and SSL front-end for your website, complete with analytics. Hover - The registrar for the domain which you offload DNS management to CloudFlare. Matt is taking advantage of the benefits of static website development, which some of the benefits are, as Matt describes: SPEED - There’s no processing server side; posts have already been reduced to the essential atomic units of the web: HTML, Javascript, and CSS. There’s something poetic to me about that. Security - While not so much an issue with my own coded CMS, I lived in constant fear of missing a zero-day Wordpress exploit patch and finding myself, along with clients, compromised. Reducing the number of moving parts significantly decreases the places where something might go wrong. Hosting - Rather than having to find, research, and deploy to increasingly rare ColdFusion hosts (or port to another language), I can post my content to anywhere that supports HTTP/JS/CSS. hosting. This becomes very compelling given that Github Pages, one option, is free. This is the cheapest...[<a href="/2017/07/10/having-the-right-communications-pipeline-for-your-api-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/10/github-serverless/"><img src="https://octodex.github.com/images/daftpunktocat-thomas.gif" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/10/github-serverless/">Github Serverless</a></h3>
			<p><em>10 Jul 2017</em></p>
			<p>I run the entire front-end of my online presence using Github. All my API Evangelist research lives as open repositories on Github, with the website running Jekyll, hosted on Github Pages. My front-end is all HTML, JavaScript, and CSS, that leverages YAML data, and displayed using Liquid. It provides me a nice way to offload the public side of my operations to Github. I am increasingly doing this with all of my data, by publishing it as YAML, and rendering a dynamic (static) API representation in JSON–all done with the same approach I’m using to publish my website(s). You can get at all of the data I use across my API research in a single API Evangelist developer portal, which just aggregates all of the JSON APIs I’ve published across my network almost 100 Github repositories, and supporting sites. Another thing I’m experimenting with is publishing simple JavaScript functions to individual pages within Github repositories. These scripts do a range of things from pulling items I’ve curated from the Feedly API, fresh data from Google Sheets that I am using as data stores, and a variety of other jobs across my network of research sites, data projects, and API tooling. Some of these scripts I’m running manually, while others I run on a variety of schedules using EasyCron. The approach definitely has some significant limitations, but I find that I’m able to get quite a bit done with JavaScript by pulling data from external APIs and other feeds, and using each Github repo as storage, and the Github API as the read/write layer for this storage. I do not store any API keys, tokens, or other secrets in the Github repositories, I’m passing them all in via the URL, which isn’t the most secure, and could in theory be abducted in transit even though I’m using SSL–something I’d like to improve upon by passing a single token to unlock a private store. I have access...[<a href="/2017/07/10/github-serverless/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/10/being-first-with-any-technology-trend-is-hard/"><img src="https://s3.amazonaws.com/kinlane-productions2/iron-io/Iron-io-Platform_Diagram_V3-05.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/10/being-first-with-any-technology-trend-is-hard/">Being First With Any Technology Trend Is Hard</a></h3>
			<p><em>10 Jul 2017</em></p>
			<p>I first wrote about Iron.io back in 2012. The are an API-first company, and they were the first serverless platform. I’ve known the team since they first reached out back in 2011, and I consider them one of my poster children for why there is more to all of this than just the technology. Iron.io gets the technology side of API deployment, and they saw the need for enabling developers to go serverless, running small scalable scripts in the cloud, and offloading the backend worries to someone who knows what they are doing. Iron.io is what I’d consider to be a pretty balanced startup, slowly growing, and taking sensible amounts of funding they needed to grow their business. The primary area I would say that Iron.io has fallen short is when it comes to storytelling about what they are up to, and generally playing the role of a shiny startup everyone should pay attention to. They are great storytellers, but unfortunately the frequency and amplification of their stories has fallen short, allowing other strong players to fill the void–opening the door for Amazon to take the lion share of the conversation when it comes to serverless. Demonstrating that you can rock the technology side of things, but if you don’t also rock the storytelling and more theatrical side of things, there is a good chance you can come in second. Storytelling is key to all of this. I always love the folks who push back on me saying that nobody cares about these stories, the markets only care about successful strong companies–when it reality, IT IS ALL ABOUT STORYTELLING! Amazon’s platform machine is good at storytelling. Not just their serverless group, but the entire platform. They blog, tweet, publish press releases, whisper in reporter ears, buy entire newspapers, publish science fiction patents, conduct road shows, and flagship conferences. Each AWS platform team can tap into this, participate, and benefit from the momentum, helping them dominate...[<a href="/2017/07/10/being-first-with-any-technology-trend-is-hard/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/07/when-json-schema-is-seen-as-power/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/power-lines-empty-space_sunday.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/07/when-json-schema-is-seen-as-power/">When JSON Schema Is Seen As Power</a></h3>
			<p><em>07 Jul 2017</em></p>
			<p>In a 30 year career as a database professional I’ve seen some extraordinary ways in which owning and controlling data is associated with power. Those who have the data leverage it against those who do not have it. Losing control means losing power, so people do whatever they can to stay in control, protecting the spreadsheets and databases at all costs. After 30 years of seeing this play out over and over again, I thought I’d seen it all, but sadly in an API era I’m just seeing new incarnations of data being wielded by those in power. I recently came across an example where a company was holding back a series of JSON schema for a variety of public datasets, and standards in use as part of some government systems. From what I can tell company had been brought in to handle the systems and open data work a few years back, and with each version of the software and schema they slowly began to maintain tighter control over the schema, while they were also being mandated to be more open with the data–shifting from being controlling over the data, to being controlling of the schema. They see the ability to be able to validate data, API requests and responses as something only a handful of people should be able to do. If you have the ability to validate, and say, “yes that data or API is compliant”, you are now in a position of power. This groups was mandated to be open with the data, allowing it flow freely between open source and proprietary systems, keeping in sync with laws and regulations, but they had found another way to remain as gatekeeper–I think this is what some folks call innovation, and thinking out of the box. In my world, it is just another example of how power will always find ways to keep data from flowing, no matter how it learns to be...[<a href="/2017/07/07/when-json-schema-is-seen-as-power/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/06/the-essential-api-elements-in-my-world/"><img src="https://s3.amazonaws.com/kinlane-productions2/periodic-elements.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/06/the-essential-api-elements-in-my-world/">The Essential API Elements In My World</a></h3>
			<p><em>06 Jul 2017</em></p>
			<p>In 2017 there seems to be an API for just about everything. You can make products available via an API, messing, images, videos, and any of the digital bits that make up our lives. I still get excited by some new APIs, but APIs have to have real usage, and deliver real value before I’ll get too worked up about them. I’m regularly looking down the list of my digital bits thinking about which are the most important to me, which ones I’ll keep around, and the services I’ll adopt to help me define and manage these bits. This process has got me thinking really deeply about what I’d consider to be the three most important types of APIs in my life: Compute - In my world compute is all about AWS EC2 instances, but when I think about it, Github really handles the majority of the compute for my front-end, but EC2 is the scalable compute for the backend of my world that is driving my APIs. Storage - Primarily storage is all about Amazon S3, but I also depend on Dropbox, Google Drive, and I also put Github into the storage bucket because I store quite a bit of JSON, YAML, and other data there. DNS - apievangelist.com and kinlane.com are very important domains in my world–they are how I make my living, and share my stories. CloudFlare is how I manage this frontline of my world, making DNS an extremely important element in my world. I leverage compute, storage, and DNS APIs regularly throughout each day–making them very important APIs in my existence. However, these are also the essential ingredients of my APIs as well. I consume these APIs, but I also deploy my APIs with these three elements. Each API has a compute and storage layer, with DNS as the naming, addressing, and discovery for these valuable resources in my world. This makes these three aspects of operating online, the three...[<a href="/2017/07/06/the-essential-api-elements-in-my-world/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/06/standardizing-and-templatizing-api-design-editor-validation-tips/"><img src="https://s3.amazonaws.com/kinlane-productions2/apicurio/apicurio-feedback-loop.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/06/standardizing-and-templatizing-api-design-editor-validation-tips/">Standardizing and Templatizing API Design Editor Validation Tips</a></h3>
			<p><em>06 Jul 2017</em></p>
			<p>I’ve been playing with Apicurio, the open source API design editor I’ve been waiting for, and saw a potential opportunity for design time collaboration, instruction, and feedback loop. When you are designing an API in Apicurio it gives you alerts based upon JSON schema validation of the underlying OpenAPI, providing a nice visual feedback loop–forcing you to complete your API definition until it properly validates. Visual alerts and feedback based upon JSON schema validation isn’t really new or that interesting–you see it in the Swagger Editor, and many other JSON tooling. Where I see an opportunity is specifically when it comes to an open source visual API design editor like Apicurio, and when the JSON schema engine for the validation responses is opened up as part of the architecture. Allowing users to import and export JSON schema that goes beyond the default OpenAPI schema, which gets us to a minimum viable OpenAPI definition–while this is good, we can do better. I’d like to see a marketplace of JSON schema to emerge helping API designers and architects push the completeness and precision of their OpenAPI definitions beyond the speed at which the core OpenAPI spec can move, and go in directions, and cover niche definitions that the core OpenAPI schema will never cover. I want to be able to load a schema that will help me push forward my API responses beyond just a default 200. I want to be able to load custom JSON schema crafted by API design experts who have more skills than I do, and learn from them. I want my API design editor to help me take my APIs to the next level, while also be pushing my API design skills forward along the way. Apicurio takes does a good job at giving plain english responses to validation errors–much better than some tools I’ve used. You can click on the detail of each alert, to get more information about what is...[<a href="/2017/07/06/standardizing-and-templatizing-api-design-editor-validation-tips/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/06/openapi-leading-the-open-banking-api-conversation/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-blue-icons.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/06/openapi-leading-the-open-banking-api-conversation/">OpenAPI Leading The Open Banking API Conversation</a></h3>
			<p><em>06 Jul 2017</em></p>
			<p>I’ve been looking through the ecosystems of banking API platforms trying to understand the technical, business, and political approach of banks when it comes to the API conversation. While Capital One is definitely leading the conversation in the U.S., I’ve also been looking to better understand what is happening around the PSD2 banking API conversation in the EU and UK. I was pleased to find OpenAPI present in the OpenBankProject PSD2 API Explorer, as well as leading the specification standards conversation over at Open Banking in the UK. The existence of the OpenAPI allows analysts like me to quickly load up the OpenAPI in an API client like Postman or Restlet, and become more intimate with what paths, and definitions are available–developing my awareness of where banking API standards are headed. OpenAPI is proving to be a great way to facilitate a conversation about an API at the team, as well as industry level. While the learning curve involved with OpenAPI adoption is real, I’m finding it to be an essential diplomatic tool when it comes to harmonizing the industry level conversation around my Human Services Data API work. OpenAPI provides a central reference that business stakeholders can reference at the 100K view, while also enabling developers and architects can discuss at the nitty gritty technical level. I’m bookmarking all the OpenAPIs I find around PSD2, and I’m on the hunt for more OpenAPIs around FHIR. These are the two leading API standards conversation going on at the industry level, helping define a common API interface within two heavily regulated industries–banking and healthcare. While there is still a HUGE amount of work within these communities to truly achieve the adoption everyone is envisioning, I find the fact that their are OpenAPIs being used as a positive sign. It shows that we are moving towards more substance than just talk, and acknowledges the conversation stimulating powers of OpenAPI, in addition to the potential for delivering API...[<a href="/2017/07/06/openapi-leading-the-open-banking-api-conversation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/06/does-your-api-sandbox-have-malicious-users/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/sand-hand_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/06/does-your-api-sandbox-have-malicious-users/">Does Your API Sandbox Have Malicious Users?</a></h3>
			<p><em>06 Jul 2017</em></p>
			<p>I have been going through my API virtualization research, expanding the number of companies I’m paying attention to, and taking a look at industry specific sandboxes, mock APIs, and other approaches to virtualizing APIs, and the data and content they serve up. I’m playing around with some banking API sandboxes, getting familiar with PSD2, and learning about how banks are approaches their API virtualization–providing me with an example within a heavily regulated industry. AS I’m looking through Open Bank Project’s PSD2 Sandbox, and playing with services that are targeting the banking industry with sandbox solution, I find myself thinking about Netflix’s Chaos Monkey, which is “a resiliency tool that helps applications tolerate random instance failures.” Now I am wondering if there are any API sandboxes out there that have simulated threats built in, pushing developers to build more stable and secure applications with API resources. There are threat detection solutions that have APIs, and some interesting sandboxes for analyzing malware that have APIs, but I don’t find any API sandboxes that just have general threats available in them. If you know of any sandboxes that provide simulations, or sample data, please let me know. Also if you know of any APIs that specifically provide API security threats in their sandbox environments so that developers can harden their apps–I’d love to hear more about it. I depend on my readers to let me know of the interesting details from API operations like this. I’m on the hunt for APIs that have sandboxes that assist application developers think about the resiliancy and security of their applications built on top of an API. Eventually I’d also love to see a sandbox emerge to emerge that could help API providers think about the resiliancy and security of their APIs. I’m feeling like this aspect of API virtualization is going to become just as critical as guidance on API design best practices, but helping API operaters better understand the threats...[<a href="/2017/07/06/does-your-api-sandbox-have-malicious-users/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/05/enhancing-your-api-seo/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-portal/enhance-seo-and-interoperability.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/05/enhancing-your-api-seo/">Enhancing Your API SEO</a></h3>
			<p><em>05 Jul 2017</em></p>
			<p>One question I’m regularly getting from my readers is regarding how you can increase the search engine optimization (SEO) for your APIs–yes, API SEO (acronyms rule)! While we should be investing in API discoverability by embracing hypermedia early on, I feel in its absence we should also be indexing our entire API operations with APIs.json, and making sure we describe individual APIs using OpenAPI, the world of web APIs is still very hitched to the web, making SEO very relevant when it comes to API discoverability. While I was diving deeper into “The API Platform”, a VERY forward leaning API deployment and management solution, I was pleased to see another mention of API SEO using JSON-LD (scroll down on the page). While I wish every API would adopt JSON-LD for their overall design, I feel we are going to have to piece SEO and discoverability together for our sites, as The API platform demonstrates. They provide a nice example of how you can paste a JSON-LD script into the the page of your API documentation, helping amplify some of the meaning and intent behind your API using JSON-LD + Schema.org. I have been thinking about Schema.org’s relationship to API discovery for some time now, which is something I’m hoping to get more time to invest in further during 2017. I’d like to see Schema.org get more baked into API design, deployment, and documentation, as well as JSON-LD as part of underlying schema. To help build a bridge from where we are at, to where we need to be going, I’m going to explore how I can leverage OpenAPI tags to help autogenerate JSON-LD Schema.org tags as part of API documentation. While I’d love for everyone to just get the benefits of JSON-LD, I’m afraid many folks won’t have the bandwidth, and could use an assist from the API documentation solutions they are already using–making APIs more SEO friendly by default. If you are starting a...[<a href="/2017/07/05/enhancing-your-api-seo/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/05/bringing-the-api-deployment-landscape-into-focus/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-api-deployment.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/05/bringing-the-api-deployment-landscape-into-focus/">Bringing The API Deployment Landscape Into Focus</a></h3>
			<p><em>05 Jul 2017</em></p>
			<p>I am finally getting the time to invest more into the rest of my API industry guides, which involves deep dives into core areas of my research like API definitions, design, and now deployment. The outline for my API deployment research has begun to come into focus and looks like it will rival my API management research in size. With this release, I am looking to help onboard some of my less technical readers with API deployment. Not the technical details, but the big picture, so I wanted to start with some simple questions, to help prime the discussion around API development. Where? - Where are APIs being deployed. On-premise, and in the clouds. Traditional website hosting, and even containerized and serverless API deployment. How? - What technologies are being used to deploy APIs? From using spreadsheets, document and file stores, or the central database. Also thinking smaller with microservices, containes, and serverless. Who? - Who will be doing the deployment? Of course, IT and developers groups will be leading the charge, but increasingly business users are leveraging new solutions to play a significant role in how APIs are deployed. The Role Of API Definitions While not every deployment will be auto-generated using an API definition like OpenAPI, API definitions are increasingly playing a lead role as the contract that doesn’t just deploy an API, but sets the stage for API documentation, testing, monitoring, and a number of other stops along the API lifecycle. I want to make sure to point out in my API deployment research that API definitions aren’t just overlapping with deploying APIs, they are essential to connect API deployments with the rest of the API lifecycle. Using Open Source Frameworks Early on in this research guide I am focusing on the most common way for developers to deploy an API, using an open source API framework. This is how I deploy my APIs, and there are an increasing number of open...[<a href="/2017/07/05/bringing-the-api-deployment-landscape-into-focus/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/05/an-api-change-log-and-road-map-visualization/"><img src="https://s3.amazonaws.com/kinlane-productions2/qlik/qlik-api-insights.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/05/an-api-change-log-and-road-map-visualization/">An API Change Log And Road Map Visualization</a></h3>
			<p><em>05 Jul 2017</em></p>
			<p>I saw a blog post come across my feeds from the analysis and visualizaiton API provider Qlik, about their Qlik Sense API Insights. It is a pretty interesting approach to trying visualize the change log and road map for an API. I like it because it is an analysis and visualization API provider who has used their own platform to help visualize the evolution of their API. I find the visualization for Qlik Sense API Insights to be a little busy, and not as interactive as I’d like to see it be, but I like where they are headed. It tries to capture a ton of data, showing the road map and changes across multiple versions of sixteen APIs, something that can’t be easy to wrap your head around, let alone capture in a single visualization. I really like the direction they are going with this, even though it doesn’t fully bring it home for me. Qlik Sense API Insights is the first approach I’ve seen like this to attempt to try and quantify the API road map and change log–it makes sense that it is something being done by a visualization platform provider. With a little usage and user experience (UX) love I think the concept of analysis, visualizaitons, and hopefully insights around the road map, change log, and even open issues and status could be significantly improved upon. I could see something like this expand and begin to provide an interesting view into the forever changing world of APIs, and keep consumers better informed, and in sync with what is going on. In a world where many API providers still do not even share a road map or change log I’m always looking for examples of providers going the extra mile to provide more details, especially if they are innovating thike Qlik is with visualizations. I see a lot of conversations about how to version an API, but very few conversations about how...[<a href="/2017/07/05/an-api-change-log-and-road-map-visualization/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/07/05/a-bot-that-actually-does-useful-things-for-me/"><img src="https://s3.amazonaws.com/kinlane-productions2/hashicorp/hashicorp-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/07/05/a-bot-that-actually-does-useful-things-for-me/">A Bot That Actually Does Useful Things For Me</a></h3>
			<p><em>05 Jul 2017</em></p>
			<p>I’m not a fan of the unfolding bot universe. I get it, you can do interesting things with them–the key word being interesting. Most of what I’ve seen done via Twitter, Facebook, and Slack Bots really isn’t that interesting. Maybe it’s that I’m old and boring, or maybe because people aren’t doing interesting things. When you hear me complain about bots, just remember it isn’t because I think the technology approach is dumb, it’s because I think the implementations are dumb. After several dives into the world of bots, looking to understand how bots are using APIs, I’ve found some interesting Twitter bots, and an even smaller number of Slack bots I found to be useful–I have yet to find an interesting Facebook Bot. Honestly, I think it is the constaints of each platform that are incentivizing interesting things to be done, and also the not interesting, and even dangerous things to be done. So I find it interesting when the bot conversation moves to other platforms, bringing with it a new sets of constraints, like I just saw with a new bot out of Hashicorp. Hashicorp’s Bot does mundane Github janitorial work for me! This is automation (aka bot) activity I can get behind. I feel like much of the Slack automation I’ve seen is doing things that wouldn’t actually benefit me, and would be creating more noise than any solution it would bring–this is due to how I use Slack, or rather how I don’t use Slack. I’m a HEAVY Github user, and there are MANY tasks that are left undone. Things like tagging repos, README files, licensing, and the other things we either forget about, or just don’t have the time for. You fire up a bot to help me with these things, my ears are going to perk up a bit when it comes to the bot conversation. In the end, I just need to remember that it is not bots...[<a href="/2017/07/05/a-bot-that-actually-does-useful-things-for-me/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/29/the-growing-importance-of-geographic-regions-in-api-operations/"><img src="https://s3.amazonaws.com/kinlane-productions2/3D-Printing/regions/api-regions-global-map-from-google.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/29/the-growing-importance-of-geographic-regions-in-api-operations/">The Growing Importance of Geographic Regions In API Operations</a></h3>
			<p><em>29 Jun 2017</em></p>
			<p>I have been revisiting my earlier work on an API rating system. One area that keeps coming up as I’m working is around the availability of APIs in a variety of regions, and the cloud platforms that are driving them. I have talked about regional availability of APIs for some time now, keeping an eye on how API providers are supporting multiple regions, as well as the expanding world of cloud computing that is powering these regional examples of providing and consuming APIs. I have been watching Amazon rapidly expand their available regions, as well as Google and Microsoft racing to catch up. But I am starting to see API providers like Digital Ocean providing APIs for getting at geographic region information, and Amazon provides API methods for getting the available regions for Amazon EC2 compute–I will have to check if this is standard across all services. Twilio has regions for their API client, and Runscope has a region API for managing how you run API tests from a variety of regions. The role of geographic regions when it comes to providing APIs, as well as consuming APIs is increasingly part of the conversation when you visit the most mature API platforms, and something that keeps coming up on my radar. We are still far from the average company being able to easily deploy, deprecate, and migrate APIs seamlessly across cloud providers and geographic regions, but as APIs become smaller and more modular, and cloud providers add more regions, and APIs to support automation around these regions, we will begin to see more decisions being made at deploy and run time regarding where you want to deploy or consume your API resources. To be able to do this we are going to need a lot more data and common schema regarding the what geographic regions are available for deployment, what services operate in which regions, and other key considerations about exactly where our resources should...[<a href="/2017/06/29/the-growing-importance-of-geographic-regions-in-api-operations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/29/making-an-account-activity-api-the-default/"><img src="https://s3.amazonaws.com/kinlane-productions2/twitter/twitter-account-activity-api-.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/29/making-an-account-activity-api-the-default/">Making An Account Activity API The Default</a></h3>
			<p><em>29 Jun 2017</em></p>
			<p>
I was reading an informative post about the Twitter Account Activity API, which seems like something that should be the default for ALL platforms. In today’s cyber insecure environment, we should have the option to subscribe to a handful of events regarding our account or be able to sign up for a service that can subscribe and help us make sense of our account activity.

An account activity API should be the default for ALL the platforms we depend on. There should be a wealth of certified aggregate activity services that can help us audit and understand what is going on with our platform account activity. We should be able to look at, understand, and react to the good and bad activity via our accounts. If there are applications doing things that don’t make sense, we should be able to suspend access, until more is understood.

The Twitter Account Activity API Callback request contains three level of details:


  direct_message_events: An array of Direct Message Event objects.
  users: An object containing hydrated user objects keyed by user ID.
  apps: An object containing hydrated application objects keyed by app ID.


The Twitter Account Activity API provides a nice blueprint other API providers can follow when thinking about their own solution. While the schema returned will vary between providers, it seems like the API definition, and the webhook driven process can be standardized and shared across providers.

The Twitter Account Activity API is in beta, but I will keep an eye on it. Now that I have the concept in my head, I’ll also look for this type of API available on other platforms. It is one of those ideas I think will be sticky, and if I can kick up enough dust, maybe other API providers will consider. I would love to have this level of control over my accounts, and it is also good to see Twitter still rolling out new APIs like this.

[<a href="/2017/06/29/making-an-account-activity-api-the-default/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/28/i-have-two-apis-i-am-interested-in-and-i-am-not-a-developerwhat-do-i-do/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-documentation-unistats.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/28/i-have-two-apis-i-am-interested-in-and-i-am-not-a-developerwhat-do-i-do/">I Have Two APIs I Am Interested In And I Am Not A Developer--What Do I Do?</a></h3>
			<p><em>28 Jun 2017</em></p>
			<p>My friend David Kernohan (@dkernohan) emailed me the other day asking me for some advice on where to get started working with some data APIs he had been introduced to. This is such a common question for me, and surprisingly seven years into API Evangelist they are questions I still do not have easy answers for. Partly because I spend the majority of my time writing about providing APIs, but also because API consumption is often times inconsistent, and just hard. David provided me with two sources of data he wanted to work, which I think help articulate the differences between APIs, that can make things hard to work with when you are just getting started with any API. Let’s break down the two APIs he wants to work with: UNISTATS Description: Compare official course data from universities and colleges. URL: http://dataportal.unistats.ac.uk/Pages/ApiDocumentation Details: It is an API with 8 separate paths to get what you need. Resources: Institution, Course, Stages, Accreditations, Locations, Statistics Data Type: XML &amp; JSON Authentication: Basic Auth Higher Education Funding Council for England (HEFCE) Register of Higher Education Providers Description: The HEFCE Register is a searchable tool that shows how the Government regulates higher education providers in England. URL: http://www.hefce.ac.uk/reg/register/data/ Details: Downloadable files with 6 urls available. Resources: Providers, Courses Data Type: XML, CSV Authentication: Auth: NONE Here you have two sources of data that overlap. One is actually an API, which you can change paths, parameters, and get different JSON or XML results. The other is just a download of an XML or CSV file. One has authentication using BasicAuth, which is a standard way of logging into websites, which often is reappropriated for accessing web APIs. You can start to see why API consumption can become pretty overwhelming, pretty quickly. CSV Is Easier So where do we start? Well with the HEFCE downloads you get the results in CSV, something you can quickly upload into a spreadsheet and get...[<a href="/2017/06/28/i-have-two-apis-i-am-interested-in-and-i-am-not-a-developerwhat-do-i-do/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/28/continue-to-explore-restaurant-menu-as-an-analogy-for-api-copyright-and/"><img src="https://s3.amazonaws.com/kinlane-productions2/burger-guild-menu1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/28/continue-to-explore-restaurant-menu-as-an-analogy-for-api-copyright-and/">Continue To Explore Restaurant Menu as an Analogy for API Copyright and</a></h3>
			<p><em>28 Jun 2017</em></p>
			<p>While working on my feedback to the EFF for the first response to the Oracle v Google API copyright case, one of the stories I published used the restaurant menu as an analogy for API copyright. This example was used in the most recent response by Google’s lawyers as they defended themselves in court, and as I’m working on my API patent research, I wanted to revisit this analogy, in the same way, helping focus attention on why API patents are such a bad idea. Building on my previous analogy, as a restaurant, imagine your restaurant specialty is delivering meat-centric dishes. Your burgers and steaks are da bomb! You literally have several “secret sauces”, some unique preparation processes, as well as some very appealing ways of naming and describing your dishes. Not that different from many API providers, who have some “secret sauces”, some unique process, as well as some very appealing ways of naming and describing the very useful APIs they are offering. In regards to copyright, why would you want to lock up the naming and ordering of what you are offering? Even if your competitor copies the exact wording on their menu (documentation), their burgers and steaks do not have your secret sauce or unique processes. Also, why would you want to burden food delivery services from aggregating your menu (documentation) alongside other restaurants using copyright? Don’t restrict how the local paper or food rag can reference your menu (documentation), and publish it on and offline–it is unnecessary and will do nothing to protect your business. In regards to patents, why would you want to lock up the menu to your burgers and steaks alongside your secret sauce(s) and unique process? Could you imagine if McDonalds sued everyone for patent infringement because they had a burger section on their menu? Someone comes up with a unique burger, and now nobody can have a specific meat dish sections on their menu? The menu...[<a href="/2017/06/28/continue-to-explore-restaurant-menu-as-an-analogy-for-api-copyright-and/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/28/api-preparation-at-the-bureau-for-the-2020-census/"><img src="https://s3.amazonaws.com/kinlane-productions2/census/census-2020-mobile-preparation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/28/api-preparation-at-the-bureau-for-the-2020-census/">API Preparation At The Bureau For The 2020 Census</a></h3>
			<p><em>28 Jun 2017</em></p>
			<p>I was reading about what the Census is doing to prepare for the 2020 census over at GCN. I’ve been invested in what they are doing at Census for some time now, so it makes me happy to see where they are headed with their preparation for the 2020 census. From what I’ve read, and what I’ve seen with their existing API efforts, they have really taken API to heart and are working to bake APIs into everything they do. According to GCN: Through the site’s application programming interface, users will be able to download and manipulate the data to serve their own purposes; ensuring that the API can drive all of data.census.gov’s core functions means outside users will have more power as well. “The more that we make this API capable, then we can serve our customers better by providing them with ways to extend the API in their own platforms for their customer base” – said Census Bureau Chief Data Officer Zach Whitman. Continuing to show that the folks at Census get APIs. The Census Bureau is the most important API out there for helping us understand the people of the United States, and how the economy is working, or not working for us. When you look at the landing page there are working on in preparation of the 2020 Census you can tell they continue to work hard to find new ways of exploring and visualization the huge amount of data they have gathered through the censuses of the past. I’m glad the Census Bureau has been on their API journey for several years now, as what they have learned will go a long way towards making the 2020 census make a more meaningful impact. APIs are not just about providing access to data. They are also about allowing many 3rd parties to add, update, as well as access and put to use data. Having the infrastructure and practice will contribute to...[<a href="/2017/06/28/api-preparation-at-the-bureau-for-the-2020-census/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/28/algorithmic-observability-in-predictive-policing/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/crypto-machine-bletchley_copper_circuit.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/28/algorithmic-observability-in-predictive-policing/">Algorithmic Observability In Predictive Policing</a></h3>
			<p><em>28 Jun 2017</em></p>
			<p>As I study the world of APIs I am always on the lookout for good examples of APIs in action so that I can tell stories about them, and help influence the way folks do APIs. This is what I do each day. As part of this work, I am investing as much time as I can into better understanding how APIs can be used to help with algorithmic transparency, and helping us see into the black boxes that often are algorithms. Algorithms are increasingly driving vital aspects of our world from what we see in our Facebook timelines, to whether or not we would commit a crime in the eyes of the legal system. I am reading about algorithms being used in policing in the Washington Monthly, and I learned about an important example of algorithmic transparency that I would like to highlight and learn more about. A classic argument regarding why algorithms should remain closed is centered around intellectual property and protecting the work that gives you your competitive advantage–if you share your secret algorithm, your competitors will just steal it. While discussing the predictive policing algorithm, Rebecca Wexler explores the competitive landscape: But Perlin’s more transparent competitors appear to be doing just fine. TrueAllele’s main rival, a program called STRmix, which claims a 54 percent U.S. market share, has an official policy of providing defendants access to its source code, subject to a protective order. Its developer, John Buckleton, said that the key to his business success is not the code, but rather the training and support services the company provides for customers. “I’m committed to meaningful defense access,” he told me. He acknowledged the risk of leaks. “But we’re not going to reverse that policy because of it,” he said. “We’re just going to live with the consequences.” And remember PredPol, the secretive developer of predictive policing software? HunchLab, one of PredPol’s key competitors, uses only open-source algorithms and code, reveals...[<a href="/2017/06/28/algorithmic-observability-in-predictive-policing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/27/the-open-service-broker-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-service-broker-api/osbapi_logo_concept3_wtm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/27/the-open-service-broker-api/">The Open Service Broker API</a></h3>
			<p><em>27 Jun 2017</em></p>
			<p>Jerome Louvel from Restlet introduced me to the Open Service Broker API the other day, a “project allows developers, ISVs, and SaaS vendors a single, simple, and elegant way to deliver services to applications running within cloud-native platforms such as Cloud Foundry, OpenShift, and Kubernetes. The project includes individuals from Fujitsu, Google, IBM, Pivotal, RedHat and SAP.” Honestly, I only have so much cognitive capacity to understand everything I come across, so I pasted the link into my super secret Slack group for API super heroes to get additional opinions. My friend James Higginbotham (@launchany) quickly responded with, “if I understand correctly, this is a standard that would be equiv to Heroku’s Add-On API? Or am I misunderstanding? The Open Service Broker API is a clean abstraction that allows ‘services’ to expose a catalog of capabilities, as well as the ability to create, use and delete those services. Sounds like add-on support to me, but I could be wrong[…]But seems very much like vendor-to-vendor. Will be interesting to track.” At first glance, I thought it was more of an aggregation and/or discovery solution, but I think James is right. It is an API scaffolding that SaaS platforms can plug into their platforms to broker other 3rd party API services. It allows any platform to offer an environment for extending your platform like Heroku does, as James points out. It is something that adds an API discovery dimension to the concept of offering up plugins, or I guess what could be an embedded API marketplace within your platform. Opening up wholesale and private label opportunities for API providers to sell their warez directly on other people’s platforms. The concept really isn’t anything new. I remember developing document print plugins for Box back when I worked with the Mimeo print API in 2011. The Open Service Broker API is just looking to standardize this approach so hat API provider could bake in a set of 3rd party...[<a href="/2017/06/27/the-open-service-broker-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/27/patent-us-20170153932-adapting-legacy-endpoints-to-modern-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/jetpack-patent.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/27/patent-us-20170153932-adapting-legacy-endpoints-to-modern-apis/">Patent #US 20170153932: Adapting Legacy Endpoints To Modern APIs</a></h3>
			<p><em>27 Jun 2017</em></p>
			<p>I made my API patent inventory a little more explorable this week, allowing me to more easily discover new and interesting patents that will affect the world of APIs, which I can include in my research. An interesting patent from eBay quickly floated up to the top as a questionable idea for a patent. Adapting legacy endpoints to modern APIs: Example methods and systems are directed to adapting legacy endpoints to modern application protocol interfaces (APIs). A legacy endpoint may provide a powerful and complex API. A modern application may desire access to the legacy endpoint. One or more layers may be added between the modern application and the legacy endpoint. Each layer may provide a different API. These layers of APIs may transform the interface from a powerful and complex interface to a more limited but simpler and easier to use interface. In some example embodiments, a proxy layer, an adapter layer, a facade layer, and a service layer may be used. Pub Date: 2016/27/06 Number: 09576314 Owner: eBay Inc. Location: San Jose, US Details: Visit USPTO Adapting legacy endpoints to modern APIs is a fundamental aspect of doing APIs in the first place. It is something that is useful and completely obvious. This is one of those patents that makes me question the competency of folks reviewing patents at the USPTO. If you at all are acquainted with the concept of web APIs, you know that this is something that is already done, and not worthy of the non-obvious aspect of being a patent. APIs have no place in patents. I think this patent pretty clearly shows the system is broken, and the reasons why many companies are filing patents have reached an unhealthy level, as well as how broken the USPTO is. If they are approving a patent for adapting legacy endpoints to modern APIs in 2016 and 2017, they are pretty out of touch with the digital world that is unfolding...[<a href="/2017/06/27/patent-us-20170153932-adapting-legacy-endpoints-to-modern-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/27/i-published-60k-patents-to-github-as-part-of-my-api-patent-research/"><img src="https://s3.amazonaws.com/kinlane-productions2/patents/api-evangelist-patent-listing-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/27/i-published-60k-patents-to-github-as-part-of-my-api-patent-research/">I Published 60K Patents To Github As Part Of My API Patent Research</a></h3>
			<p><em>27 Jun 2017</em></p>
			<p>I’ve been migrating from my own homebrew CMS system over the last couple of weeks, ditching it for a variety of existing services, balancing my operations across a diverse set of platforms I’ve identified as useful. I’m using Github and Jekyll to manage my content system, storing thinks like blog, news, notes, and patents there. Github repos are well suited for storing this type data for free (if it is public), and Jekyll is well suited for helping me manage small to large repositories of content I need to use across my platform. This last week I migrated 60K patents I had filtered out of all the XML dumps of patents I downloaded, between the years of 2005 and 2016. I filtered out any patent with API or application programming interface in the title, abstract, or body of the patent. Once done importing, I clean up the body a little bit and publish each one as a Jekyll post within a Github repo. I had to break them down by year, as Github started getting wonky after 10K file, but the Jekyll structure works well for managing up to 10K patents. Once committed, I find the Jekyll post and data structure pretty easy to navigate and use for some API storytelling. I like having the full listing of title and abstract available to search in a single public HTML page. I have also started publishing keyword filters for each area of my research. It is taking me a while to build the index for each JSON API, but it is allowing me to quickly get at cached searches for machine learning, API management, and other patents that affect the areas of the API lifecycle I’m keeping a close eye on. I will keep adding filters and enhancing the ones that I have, but ultimately it takes me reading a large number of patents and then actually tagging each one, that will help me build a...[<a href="/2017/06/27/i-published-60k-patents-to-github-as-part-of-my-api-patent-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/27/api-environment-portability/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/runscope/1-runscope-env.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/27/api-environment-portability/">API Environment Portability</a></h3>
			<p><em>27 Jun 2017</em></p>
			<p>
I was reading the post from Runscope on copying environments using their new API. I was looking through the request and response structure for their API, it looks like a pretty good start when it comes to what I’d call API environment portability. I’m talking about allowing us to define, share, replicate, and reuse the definitions for our API environments across the services and tools we are depending on.

If our API environment definitions shared a common schema, and API like Runscope provides, I could take my Runscope environment settings, and use them in my Stoplight, Restlet Client, Postman, and other API services and tooling. It would also help me templatize and standardize my development, staging, production, and other environments across the services I use. Assisting me in keeping my environment house in order, and also something that I can use to audit and turn over my environments to help out with security.

It is just a thought. An API environment API, possessing an evolving but common schema just seems like one of those things that would make the entire API space work a little smoother. Making our API environments exportable, importable, and portable just seems like it would help us think through when it comes setting up, configuring, managing, and evolving our API environments–who knows maybe someday we’ll have API service providers who help us manage our API environments, dictating how they are used across the growing number of API services we are depending on.

Disclosure: Runscope and Restlet are API Evangelist partners.

[<a href="/2017/06/27/api-environment-portability/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/26/three-new-api-industry-groups-on-the-horizon/"><img src="https://s3.amazonaws.com/kinlane-productions2/industry/industry-work-group.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/26/three-new-api-industry-groups-on-the-horizon/">Three New API Industry Groups On The Horizon</a></h3>
			<p><em>26 Jun 2017</em></p>
			<p>Along with the growth of industry level API events for machine learning, healthcare, and beyond, I’m starting to see the emergence of more API specific working groups, something I’ve been asking for, for some time now. The API universe is expanding and we will need API specialists with domain expertise to help push forward the conversations in leading industries like healthcare, banking, education, transportation, and beyond. I’ve been keeping an eye out for any movement within industries beyond FHIR and PSD2, and now I”m adding three more efforts to my list: Artificial Intelligence - NTT DOCOMO’s new docomo AI Agent Open Partner Initiative to facilitate collaborative development of all-new offerings implemented with a service-agnostic, device-agnostic speech interface based on AI Agent API, a newly developed artificial intelligence (AI) application programming interface (API) that DOCOMO plans to incorporate into a new AI agent service to be launched in early fiscal 2018. Payments - The National Automated Clearing House Association (NACHA) API Industry Standardization Group looking to “help improve the safety and transparency of transactions, increase efficiencies and speed of communications, and enhance support of payments innovations.” Hospitality - The Hospitality Technology Next Generation (HTNG) API Registry looking to address a number of inefficiencies in the Hospitality API space, including finding potential technology partners whose products and/or services that could add value to an hotelier’s offerings, by creating a registry of Hospitality API’s to help modernize this space. These three efforts provide some interesting models for how companies and existing industry organizations are stepping up to provide leadership when it comes to API definitions, standards, and discovery in a particular business sector. There are no signs of whether these organizations will get off the ground and become sustainable, or that their stewards understand the wider API space, and have their industries best interest in mind. It does provide us with a glimpse at what is coming though, and the opportunity for companies, organizations, institutions, and government agencies...[<a href="/2017/06/26/three-new-api-industry-groups-on-the-horizon/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/26/the-competitive-advantage-of-api-agility-over-any-secret-sauce/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/machine-road_blue_circuit_5_bw.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/26/the-competitive-advantage-of-api-agility-over-any-secret-sauce/">The Competitive Advantage Of API Agility Over Any Secret Sauce</a></h3>
			<p><em>26 Jun 2017</em></p>
			<p>
I was talking to a VC about one of my favorite API upstarts the other day, and one of the closing questions I received was if the API upstart had a secret sauce that made their position defensible. To which I responded, no…but they are API first, and API definition-driven in everything they do, so they will ultimately move faster than any competitor can.

Agility is one of the classic things you hear people tell companies regarding why they should be doing APIs. The benefit is definitely overused and overstated in situations it shouldn’t be, but when APIs are fully embraced, and done properly, the agility is real. I’ve seen companies be able to shift, pivot, and add new features in a fraction of the time of their competitors, allowing them to in new ways that nobody had intended just months before–APIs allow for the type of shape shifting you need to remain competitive in today’s environment.

APIs do not automagically mean a company, institution, organization, or agency will be agile by default. Organizationally, and culturally the entity behind an API needs to be in sync with the API frontline, or agility will never be fully realized. However, when you can dial all this in I’ve seen something that is more potent than any secret sauce or proprietary approach, allowing you to move more confidently and flexibly. I don’t think API agility is a competitive advantage that all companies or investors fully grasp, but once they see in action, I think they’ll realize APIs are more effective than locking something up and keeping it secret.

[<a href="/2017/06/26/the-competitive-advantage-of-api-agility-over-any-secret-sauce/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/26/healthcare-api-interoperability-at-hl7-fhir-dev-days-in-amsterdam/"><img src="https://s3.amazonaws.com/kinlane-productions2/fhir/fhir-devdays-2017-amsterdam.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/26/healthcare-api-interoperability-at-hl7-fhir-dev-days-in-amsterdam/">Healthcare API Interoperability At HL7 FHIR Dev Days In Amsterdam</a></h3>
			<p><em>26 Jun 2017</em></p>
			<p>
I wrote about a machine learning specific API event a couple weeks back, and today I wanted to highlight the growth of conferences dedicated to FHIR, or the Fast Healthcare Interoperability Resources. FHIR is larger than just it’s API, but it is one very important example of how APIs and open industry API specifications can help move forward the API conversation in large industries.

As part of my healthcare API research, I bookmarked HL7 FHIR DEVDAYS 2017, a conference dedicated to standardizing healthcare APIs, and increasing interoperability across the global healthcare system. I went through the three-day schedule for the conference, following speakers on Twitter, and learning more about what being presented when it came to healthcare interoperability–expanding my awareness of what is going on when it comes to connecting healthcare systems and pushing forward this important API definition.

FHIR is right there with PSD2 for banking. While not perfect examples, they are the two most significant examples we have when it comes moving forward an API standard for a large industry that are critical to our society. I don’t have the time to become an expert on every detail of FHIR or PSD2, but I do think it is important as an API professional to understand what is going on in these communities. Much like we’ve done with the API sector with the API Strategy &amp; Practice Conferenc, we are going to need more industry level API-focused conferences to emerge, helping industries come together and hammer out a common API definition, as well as the services and tooling that will be putting these definitions to work on the ground across the API economy.

[<a href="/2017/06/26/healthcare-api-interoperability-at-hl7-fhir-dev-days-in-amsterdam/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/26/budget-transparency-at-the-county-level-with-open-data-and-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/socrata/douglas-county-operating-budget-768x485.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/26/budget-transparency-at-the-county-level-with-open-data-and-apis/">Budget Transparency At The County Level With Open Data And APIs</a></h3>
			<p><em>26 Jun 2017</em></p>
			<p>
As we find ourselves darker times when it comes to transparency within the federal government in the United States, I’m always on the hunt for any positive signs of transparency at the other levels of government. I can usually depend on my friends over at Socrata to help out, and they came through this week with a story on Douglas County, Kansas publishing their budget online.

“This week, Douglas County, Kansas, home to 117,000 residents, launched an Open Budget site, which provides the public with access to one of the county’s most crucial documents: the annual operating budget.” Jill Jolicoeur, Assistant to the County Administrator stated that “our goal is for Open Budget to replace the time-intensive process of developing and publishing the annual budget in a hard-copy format.” Open data and APIs is one way for resource-strapped companies can open things up, allowing external groups help share the load when it comes to auditing, analysis, visualization, and many other areas county government could use some assistance.

Douglas County provides us with another example of how we can make government more observable. There is no reason that county government can’t open up their budgets like this, and let a little sunlight into the operations. In my experience, the primary reason you want to keep things closed is when you are looking to hide corruption, incompetence, and poor technological and business decisions. I’m concerned with the bar being set by the White House right now when it comes to transparency and observability, but it doesn’t mean we can’t resist at the local level, and leverage open data and APIs to get things done more efficiently–like Douglas County is doing, and Socrata is enabling.

[<a href="/2017/06/26/budget-transparency-at-the-county-level-with-open-data-and-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/22/i-would-love-to-reference-github-data-across-repos-with-org-data-object/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-github-icon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/22/i-would-love-to-reference-github-data-across-repos-with-org-data-object/">I Would Love To Reference Github Data Across Repos With [org].data.[object]</a></h3>
			<p><em>22 Jun 2017</em></p>
			<p>I am a big fan of Jekyll and Github when it comes to managing data-driven projects. All of my research runs on Github, and I use Jekyll to serve up YAML and JSON representations of my research for a variety of purposes. I store all data that supports my research in the _data folder for each research project’s repository. From there I will create HTML, Atom, and JSON representations for use in my API Evangelist storytelling. When I am referencing any YAML data store I have in the _data folder I just use site.data.[object] to reference it. From there I can loop through collections, filter and show fields and other elements on the page using Liquid syntax. I love having all the data at my fingertips, but I’m thinking about the next step of data management at scale, as I work to build more data-driven repositories, housed within Github organizations, I want to be able to reach outside of each repo, into other repos stored within a single organization. I would love it if Github allowed me to reference my data within an organization using [org].data.[object]. This way I could reference my API design research within my hypermedia API research. I have my API industry research segregated by topic for several reasons. Often it is just a logical separator, but sometimes the research is too big to fit into a single repository, and I’ll shard thing by year, topic, or another logical separator. It would be nice if I could reach across the repository line, into other organizations within an organization, or maybe someday reach out into other organizations as well. Just some thoughts as I’m working with YAML data at scale across hundreds of Github repositories. I handle this cross-repo access using JavaScript currently but would love it if it was natively built into Jekyll, allowing me to publish in a static way. The amount of Liquid YAML and JSON voodoo I could unleash...[<a href="/2017/06/22/i-would-love-to-reference-github-data-across-repos-with-org-data-object/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/22/api-plans-are-not-sustainable-for-my-small-business/"><img src="https://s3.amazonaws.com/kinlane-productions2/imageoptim/imageoptim-api-pricing.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/22/api-plans-are-not-sustainable-for-my-small-business/">API Plans Are Not Sustainable For My Small Business</a></h3>
			<p><em>22 Jun 2017</em></p>
			<p>I’ve already written about how I just don’t like class pricing tiers for API consumption, but I want to keep beating this drum until service providers hear what I’m singing. I think pricing tiers worked well to onboard the world with SaaS but for an API-driven world we a lot more flexibility and scalability when it comes to t he business model. As a small business I just can’t take another monthly payment, without some deep consideration, and bank account consultation. I’m looking at the ImageOptim API, which is already a desktop tool I use, but the opportunity to automate my image optimization is very appealing. However, their entry level pricing tier is $9 a month for $1,000.00 calls. I might be able to add another monthly fee, but it damn well better be generating some value, or directly bringing in some new revenue. My Amazon bill is always around $350.00, and I just downsized my Dropbox from $75 to $15 a month, and my CloudFlare is up to $75, and on and on. I have a whole list of monthly bills for a variety of services I depend on. The pricing tier based services are almost always the first to go when I have to downsize–Dropbox, gone. However, the ones that let me pay for what I use, tend to actually grow and are sustained. Overall I have about $1K a month to spend on SaaS and API solutions, but increasingly I’m favoring utility-based services over plan-based ones. I think it is easy for API providers to look at their plans and pricing in a silo, exclusively from their perspective, and by itself the structure makes sense, but when you look at in the context of an entire portfolio of services for a small business, it just doesn’t make a lot of sense when you are using tens or hundreds of services. I know that all you business owners are looking for some stability...[<a href="/2017/06/22/api-plans-are-not-sustainable-for-my-small-business/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/21/validating-my-api-schema-as-part-of-my-api-security-practices/"><img src="https://s3.amazonaws.com/kinlane-productions2/fcc/fcc-makes-net-neutrality-commenters-email-addresses-public-through-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/21/validating-my-api-schema-as-part-of-my-api-security-practices/">Validating My API Schema As Part of My API Security Practices</a></h3>
			<p><em>21 Jun 2017</em></p>
			<p>I am spending more time thinking about the unknown unknowns when it comes to API security. This means thinking beyond the usual suspects when thinking about API security like encryption, API keys, and OAuth. As I monitor the API space I’m keeping an eye out for examples of what might be security concerns that not every API provider is thinking about. [I found one recently in ARS Technica, about the Federal Communication Commission (FCC) leaking the email addresses through the CC API for anyone who submitted feedback as part of any issues like the recent Net Neutrality discussion. It sounds like the breach with the FCC API was unintentional, but it provides a pretty interesting example of a security risk that could probably be mitigated with some basic API testing and monitoring, using common services like Runscope, or Restlet Client. Adding a testing and monitoring layer to your API operations helps you look beyond just an API being up or down. You should be validating that each endpoint is returning the intended/expected schema. Just this little step of setting up a more detailed monitor can give you that brief moment to think a little more deeply about your schema–the little things like whether or not you should be sharing the email addresses of thousands, or even millions of users. I’m working on a JSON Schema for my Open Referral Human Services API right now. I want to be able to easily validate any API as human services compliant, but I also want to be able to setup testing and monitoring, as well as security checkups by validating the schema. When it comes to human services data I want to be able to validate every field present, ensuring only what is required gets out via the API. I am validating primarily to ensure an API and the resulting schema is compliant with HSDS/A standards but seeing this breach at the FCC has reminded me that taking...[<a href="/2017/06/21/validating-my-api-schema-as-part-of-my-api-security-practices/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/21/patent-us9639404-api-matchmaking-using-feature-models/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/21/patent-us9639404-api-matchmaking-using-feature-models/">Patent US9639404: API Matchmaking Using Feature Models</a></h3>
			<p><em>21 Jun 2017</em></p>
			<p>Here is another patent in my series of API related patents. I’d file this in the category as the other similar one from IBM–Patent US 8954988: Automated Assessment of Terms of Service in an API Marketplace. It is a good idea. I just don’t feel it is a good patent idea. Title: API matchmaking using feature models Number: 09454409 Owner: International Business Machines Corporation Abstract: Software that uses machine logic based algorithms to help determine and/or prioritize an application programming interface’s (API) desirability to a user based on how closely the API’s terms of service (ToS) meet the users’ ToS preferences. The software performs the following steps: (i) receiving a set of API ToS feature information that includes identifying information for at least one API and respectively associated ToS features for each identified API; (ii) receiving ToS preference information that relates to ToS related preferences for a user; and (iii) evaluating a strength of a match between each respective API identified in the API ToS feature information set and the ToS preference information to yield a match value for each API identified in the API ToS feature information set. The ToS features include at least a first ToS field. At least one API includes multiple, alternative values in its first ToS field. Honestly, I don’t have a problem with a company turning something like this into a feature, and even charging for it. I just wish IBM would help us solve the problem of making terms of service machine readable, so something like this is even possible. Could you imagine what would be possible if everybody’s terms of service were machine readable, and could be programmatically evaluated? We’d all be better off, and matchmaking services like this would become a viable service. I just wish more of the energy I see go into these patent would be spent actually doing things in the API space. Providing low cost, innovative API services that businesses can use,...[<a href="/2017/06/21/patent-us9639404-api-matchmaking-using-feature-models/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/21/making-machine-learning-accessible-to-spreadsheet-power-users/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorithmia/AlgorithmiaSpreadsheets.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/21/making-machine-learning-accessible-to-spreadsheet-power-users/">Making Machine Learning Accessible To Spreadsheet Power Users</a></h3>
			<p><em>21 Jun 2017</em></p>
			<p>
My friends over at Algorithmia are up to some good things–making their algorithms available within a spreadsheet. Algorithmia has created a set of open source scripts and walkthrough to help you inject the algorithms from their marketplace into your Google Spreadsheets.

They have seven useful algorithms to inject into spreadsheets:


  Linear Detrend – removes increasing or decreasing trends in time series
  Autocorrelate – used to analyze the seasonality of a time series
  Outlier Detection – flags unusual data points
  Forecast – predict a given time series into the future
  Summarizer – creates a text summary by extracting key topic sentences
  Social Sentiment Analysis – assigns sentiment ratings of “positive”, “negative” and “neutral”
  Count Social Shares – returns the number of times that URL has been shared on various social media sites


The Google scripts are available on Github, thanks to the hard work of Ken Burcham. It provides yet another interesting example of how a spreadsheet can be used as an API client but it also provides an interesting example of how machine learning API providers can get their ML warez in front of the average business user. Developers building applications with your ML APIs is one thing but getting the average business spreadsheet power user to put your ML API to work in their everyday workflow is a whole other world of API integration opportunity.

I have been preaching the spreadsheet to API connection for a while now. I know that many API developers want to do away with the spreadsheet, but I think they would be better off focusing on injecting their API solutions into spreadsheets like Algorithmia is doing. When you are designing your algorithmic-centric APIs, providing access to your machine learning models, make sure you keep your APIs simple and doing one thing well like Algorithmia does, then your ML APIs can be injected into the spreadsheets around the globe that are driving business each day.


[<a href="/2017/06/21/making-machine-learning-accessible-to-spreadsheet-power-users/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/21/i-am-working-with-elastic-beam-to-help-define-api-security/"><img src="https://s3.amazonaws.com/kinlane-productions2/elastic-beam/infographic-products.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/21/i-am-working-with-elastic-beam-to-help-define-api-security/">I Am Working With Elastic Beam To Help Define API Security</a></h3>
			<p><em>21 Jun 2017</em></p>
			<p>Security is the number one concern companies, organizations, institutions, and government agencies have when I’m talking with them about doing APIs. Strangely it is also one of the most deficient, and underinvested areas of API operations. Companies are just learning to design, deploy, and manage their APIs, and monitoring, testing, and security are still on the future road map for many API providers I know. Security is one of the important areas I’ve been trying to find more time and resources to invest into my research, and I’ve been on the hunt for interesting providers to partner with when it comes to defining security as it applies to APIs. There are a number of web and infrastructure security companies out there, but there aren’t enough that are only focused on just APIs. With the number of APIs increasing, we need more eyeballs on the problem, and even more services and tools to stay ahead of the curve. I finally found a worthwhile partner to help explore API security as part of my regular work as the API Evangelist, a new API security focused startup called Elastic Beam. They are a brand new effort exclusively focused on API security, who are hitting all the buzzworthy areas of the tech space (Artificial Intelligence, Machine Learning, etc), while also doing one thing and doing it well–API security. ElasticBeams core products are: API Behavioral Security - Keeping an eye on the unknown unknowns when it comes to API threats. Deep API Insight - Access insights extracted from data ingested across all Elastic Beam implementations. API Security Enforcer - Actually blocking and providing a smokescreen for threats against an API. Hybrid Cloud API Security - Plugs into existing API service and infrastructure providers. Secure MQTT Proxy - Focused on API security when it comes to the Internet of Things. I’ve seen Elastic Beam in action, and have a copy to play with as I’m exploring a variety of scenarios in...[<a href="/2017/06/21/i-am-working-with-elastic-beam-to-help-define-api-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/20/tightly-coupled-to-our-mobile-phones/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-mobile-apps.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/20/tightly-coupled-to-our-mobile-phones/">Tightly Coupled To Our Mobile Phones</a></h3>
			<p><em>20 Jun 2017</em></p>
			<p>
I had ditched my phone last year after being with AT&amp;T for just shy of 20 years. Not having a phone made me realize how much you need a phone number to exist online these days. Facebook, Twitter, Google, all needed me to have a phone number which I can verify from time to time, to keep my accounts active.

In addition to just needing it for an account, I also need it regularly to secure my world via two-factor authentication. Sometimes I need it for SMS, but mostly I just need the authenticator app–both requiring at least having the mobile device in my presence. I’m not very tightly coupled with my phone, but it feels like it increasingly like it is always coupled to me.

I’m guessing that if it isn’t our mobile phones, in the future there will always be at least one device we will be required to have as part of our identity, and be helping us secure both our physical and digital worlds. It isn’t something I enjoy but like pretty everyone else, it is not a cord I am going to be able to cut anytime soon.

[<a href="/2017/06/20/tightly-coupled-to-our-mobile-phones/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/20/the-unknown-unknowns-of-api-security/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-security-unknown.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/20/the-unknown-unknowns-of-api-security/">The Unknown Unknowns Of API Security</a></h3>
			<p><em>20 Jun 2017</em></p>
			<p>I am trying to wrap my head around the next steps in the evolution of API security. I am trying to help separate some of the layers of what we collectively call API security, into some specific building blocks I can reference in my storytelling. I’m ramping up my API security research as I onboard a new API service provider partner, and will have more resources to invest in the API security discussion. Let’s start with the easy “Known Knowns”: Encryption - Using encryption across all aspects of API operations from portal to base URL. API Keys - Making sure everyone who is accessing an API is keying up and passing along with each request. OAuth - Getting more granular with access by applying OAuth when there is access to 3rd party data and content. Rate Limiting - Limiting how much of a resource any single API consumer can access using rate limiting. These are the first things that come to mind when the topic of API security is brought up. I’d say most conversation begin with encryption, but as someone progresses on their API journey they begin to understand how they can key things up, using OAuth, rate limiting, and other aspects of API management to secure their API operations. Next, I want to think about some the “Known Unknowns”, things like: Freeloaders - People with multiple accounts, and generally aren’t healthy consumers. Errors - Errors, misconfigured, and malformed requests, responses, and anything that generally gums up the gears of operations. Vulnerabilities - Making sure all known software and hardware vulnerabilities are patched and communicated as part of operations. Attacks - Defending against the known attacks, staying in tune with OWASP and other groups who are actively identifying, studying, and sharing attacks. These are the things we might not totally understand or have implemented, but they are known(ish). With the right amount of resources and expertise, any API team should be able to mitigate...[<a href="/2017/06/20/the-unknown-unknowns-of-api-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/20/the-general-services-administration-api-strategy-considers-how-to-generate-revenue/"><img src="https://s3.amazonaws.com/kinlane-productions2/gsa/gsa-api-strategy-diagram.pngg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/20/the-general-services-administration-api-strategy-considers-how-to-generate-revenue/">The General Services Administration API Strategy Considers How To Generate Revenue</a></h3>
			<p><em>20 Jun 2017</em></p>
			<p>
The General Services Administration(GSA) has an API strategy, which describes “GSA’s future direction for agency­wide API management including design, development, architecture, operations, and support, and security.” Ok, let’s pause there. I want to point out that this isn’t just an API design guide. That is a portion of it, but it also touches on some of the most obvious (deployment), and the most critical aspects (security) of API operation–important stuff.

The objectives for the GSA crafting an API strategy are:

­* Harness API management to maximize customer value and technical efficiencies.
­* Adopt industry best practices with API design, development, and management.
­* Consider opportunities for revenue through API offerings.

I always enjoy seeing federal agencies talk about efficiencies and best practices, but it gives me hope that all of this might actually work when I see federal agencies actually “considering opportunities for revenue through API offerings”. Especially at the GSA, who provides technical guidance to the other 400+ federal agencies, as well as at the state and municipal level. I am not excited about our government charging money for digital resources, I am excited about our government exploring how it will generate revenue to sustain itself in the future.

I know there are a lot of open data advocates who can’t wrap their mind around this, but this is how the government will generate needed tax revenue to operate in the future–using APIs. Our government currently generates a portion of its revenue from the sale of physical resources like timber, mining, and agriculture, why should things be different when it comes to taxing and regulating digital resources being made available via the web, mobile, and device applications. While there is still lots to figure out on this front, I am glad to see the GSA putting some thought into the role APIs will play in the future of funding the government services we all depend on each day.

[<a href="/2017/06/20/the-general-services-administration-api-strategy-considers-how-to-generate-revenue/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/20/api-wrappers-to-help-bring-machine-learning-into-focus/"><img src="https://s3.amazonaws.com/kinlane-productions2/tensorflow/kites_detections_output.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/20/api-wrappers-to-help-bring-machine-learning-into-focus/">API Wrappers To Help Bring Machine Learning Into Focus</a></h3>
			<p><em>20 Jun 2017</em></p>
			<p>I was taking a look at the Tensorflow Object Detection API, and while I am interested in the object detection, the usage of API is something I find more intriguing. It is yet another example of how diverse APIs can be. This is not a web API, but an API on top of a single dimension of the machine learning platform TensorFlow. “The TensorFlow Object Detection API is an open source framework built on top of TensorFlow that makes it easy to construct, train and deploy object detection models.” It is just a specialized code base helping abstract away the complexity of one aspect of using TensorFlow, specifically for detecting objects in images. You could actually wrap this API with another web API and run on any server or within a single container as a proper object recognition API. For me, it demonstrates one possible way of wrapping a single or cross section of a machine learning implementation to abstract away the complexity and helping you train and deploy ML models in this particular area. This approach to deploying an API on top of ML shows that you can use to APIs to help simplify and abstract ML for developers. This can be done to help satisfy business, regulatory, privacy, security, and other real or perceived concerns when it comes to artificial intelligence, machine learning, or any other digital voodoo that resembles magic. No matter how complex the inputs and outputs of an algorithm are, you can always craft an API wrapper, or series of API wrappers that help others make sense of those inputs, from a technical, business, or even political perspective. I just wanted to highlight this example of ML being wrapped with an API, even if it isn’t for all the same reasons that I would be doing it. It’s just part of a larger toolbox I’m looking to create to help me make the argument for more algorithmic transparency in the...[<a href="/2017/06/20/api-wrappers-to-help-bring-machine-learning-into-focus/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/19/three-rules-of-my-api-communication-strategy/"><img src="https://s3.amazonaws.com/kinlane-productions2/three-rules-for-a-communication-strategy.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/19/three-rules-of-my-api-communication-strategy/">Three Rules Of My API Communication Strategy</a></h3>
			<p><em>19 Jun 2017</em></p>
			<p>
Communicating effectively around API operations is the number one illness I see across the API space. Engineers are good at writing code and devopping their way to a usable API, but often fall short when it comes to telling the story of what the API does, and consistently beating this drum until people become familiar with what is going on.

An effective API communication strategy is more art than it is science, and I’d like to share three of my rules when it comes to telling stories on the API Evangelist platform.


  Honesty - Be honest with yourself, you’re readers, and those you are writing about. If you can’t find a way to be honest in your writing go find a new job–it won’t be sustainable.
  Consistent - Communicate every day. Ok, maybe every other day. Regardless of frequency, make sure you are communicating on a consistent basis, setting the tone for what your audience can expect.
  Compelling - Make it compelling. No, not every single post will be compelling, but make it your primary goal to tell a compelling story that you would read yourself, if it was on someone else’s blog.


That is it. Don’t sweat all the technical details. Just write on the blog, spend the time on Twitter, participate in threads on Github, and regularly dive into the bowels of the Slack. Don’t spend all your time worrying about your communication strategy, just make sure you give it a sensible amount of time, and follow these three rules–the rest will come.

As of this moment, there are 2,680 blog posts on API Evangelist, with the first entry in September 2010. These three rules have kept me on track when I was taking money from bigcos like Intel and Mulesoft, and kept me from losing my shit when I was traveling and drinking too heavily. These three rules are what keep me doing this effectively after seven years.

[<a href="/2017/06/19/three-rules-of-my-api-communication-strategy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/19/the-six-dimensions-of-api-patents-i-dwell-on/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/supreme-court-statues.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/19/the-six-dimensions-of-api-patents-i-dwell-on/">The Six Dimensions Of API Patents I Dwell On</a></h3>
			<p><em>19 Jun 2017</em></p>
			<p>Each story I publish about API patents will usually get a comment, Tweet, LinkedIn, or other comments letting me know that the owner of the patent is only doing it in a defensive pattern. I fully grasp that this is the predominant stance when it comes to defending a patent portfolio, but I prefer seeing six dimensions to this discussion–looking beyond this single position. When thinking about why a patent exists I see it in six dimensions: Idea - That someone has an idea, thinks it is theirs and feels that this should exist as a patent. Patent- That some have the resources to craft the patent application, and file it with the patent office. Filing - That the patent authority thinks an idea is patent-worthy, and something that should be approved. Litigation - When someone wields a patent as part of litigation, either in an offensive, or defensive stance. Public Deals - When patent shows up as part of an acquisition, partnership, and wielded publicly as part of some deal. Backroom Deals - When patents are leveraged as part of backroom deals when negotiating with companies and investors, and sizing up the value on the table. We only have insight into the middle four dimensions. We really don’t have any view into the individual reasons behind a patent, and we do not get access to how they are wielded and leveraged behind closed doors. I think that defending yourself as part of any court case makes sense, but I’m discussing API patents to help shine a light and understand what is happening within the other dimensions. Overall I find that the existence of patents to speak volumes about a company’s motivations. The way they showcase or do not showcase their API portfolio contributes to this conversation in important ways. Sadly we will never get a full picture of the individual and backroom aspects of how API patents are used. However, I’m confident I can...[<a href="/2017/06/19/the-six-dimensions-of-api-patents-i-dwell-on/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/19/idk-is-always-the-first-step-to-api-integration/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-question-mark.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/19/idk-is-always-the-first-step-to-api-integration/">IDK Is Always The First Step To API Integration</a></h3>
			<p><em>19 Jun 2017</em></p>
			<p>
I spend a lot of time trying to figure out what technology does. I read press releases, pitch emails, documentation, and marketing materials trying to get an idea of what is possible. While many of the APIs I come across are intuitive, and just make sense there is still a significant portion of them that leave me scratching my head regarding what it even does.

As developers, it can be easy to think about the SDKs you will need to support API integration with your API, but I think you are making a lot of assumptions about your consumers when you focus your initial energy here. The first step in any API integration begins with IDK and not the SDK. When a potential API consumer comes across your API, the first question to be answered is: what does this API do? If the answer is I Don’t Know (IDK), we have a problem. A problem no amount of SDKs will solve, no matter how many languages you create them in.

Every API begins with an IDK. What does the Twilio API do? What does the Stripe API do? The answer to this initial question for your API cannot be IDK! As soon as I read your press release, land on the home page of your developer area, I should know what your API does in 5 seconds or less. If I walk away from our first interaction with an IDK, the chances I’ll be coming back are pretty slim. Make sure you always address the IDK, before you get to work on your SDK, for your API(s).

[<a href="/2017/06/19/idk-is-always-the-first-step-to-api-integration/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/19/i-would-like-to-see-more-api-test-drives/"><img src="https://s3.amazonaws.com/kinlane-productions2/c627d90c-1615-4750-b45c-9658b45596bc.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/19/i-would-like-to-see-more-api-test-drives/">I Would Like To See More API Test Drives</a></h3>
			<p><em>19 Jun 2017</em></p>
			<p>
The Azure Marketplace has the ability to test drive anything that is deployed in the Azure Marketplace. As someone who has to sign up for an endless number of new accounts to be able to play with APIs and API services, I’m a big fan of the concept of a test drive–not just for web applications, or backend infrastructure, but specifically for individual APIs and microservices.

From the Azure site:  Test Drives are ready to go environments that allow you to experience a product for free without needing an Azure subscription. An additional benefit with a Test Drive is that it is pre-provisioned - you don’t have to download, set up or configure the product and can instead spend your time on evaluating the user experience, key features, and benefits of the product.

I like it. I want more of these. I want to be able to test drive, then deploy any API I want. I don’t want to sign up for an account, enter my credit card details, talk to sales, or signup for 30 day trial–I want to test drive. I want it to have data in it, and be pre-configured for a variety of use cases. Helping me understand what is possible.

I want all the friction between me finding an API (discovery via marketplace) understanding what an API does, test driving, then deployment of the API in any cloud I want. I think we are still a little bit off from this being as frictionless as I envision in my head, but I hope with enough nudging we will get there very soon.

[<a href="/2017/06/19/i-would-like-to-see-more-api-test-drives/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/16/serverless-blueprints-for-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/lambda_find_box_blue_1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/16/serverless-blueprints-for-your-api/">Serverless Blueprints For Your API</a></h3>
			<p><em>16 Jun 2017</em></p>
			<p>
Serverless is spreading across the API sector, and is something that leading API providers are beginning to embrace as part of their operations. I saw an interesting example of this out of AWS and Box lately, with the announcement of Lambda blueprints and code for integrating with the Box API via the AWS platform.

The Box serverless blueprints show you how to call the Box APIs and connect a Box webhook to a Lambda function via the Amazon API Gateway–providing some pretty interesting use cases for using Box via serverless functions:


  Manage User Authentication with Box Platform using Amazon Cognito – How to use Amazon Cognito to power a login page for application users.
  Add Deep Learning-based Image Recognition to your Box App with Amazon Rekognition – How to build an image tagging application that is powered by Amazon Rekognition.


They are some pretty basic use cases, but it is the approach that opens up an entirely new door for API integration for me–Serverless Development Kits (SDK). Every API providers should have a whole catalog of open source serverless scripts that are deployable to Lambda, and other serverless platforms. Of course, there are one-click buttons deploy each individual script to the cloud platform of your choice.

I’m diving into the other side of this story for me, where Box is embracing a tighter coupling with the AWS platform as part of their operations. I am looking at how Box is providing a copy of the Box API for deployment on AWS. This all reflects how I see things working in the future, where you can deploy individual API integration scripts, as well as deploy APIs to a serverless environment like this–empowering anyone to become both API consumer and provider via the AWS, or any other cloud ecosystem.

[<a href="/2017/06/16/serverless-blueprints-for-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/16/publishing-your-api-in-the-aws-marketplace/"><img src="https://s3.amazonaws.com/kinlane-productions2/box/box-platform-cloud-content-management-apis-aws-marketplace.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/16/publishing-your-api-in-the-aws-marketplace/">Publishing Your API In The AWS Marketplace</a></h3>
			<p><em>16 Jun 2017</em></p>
			<p>I’ve been watching the conversation around how APIs are discovered since 2010 and I ave been working to understand where things might be going beyond ProgrammableWeb, to the Mashape Marketplace, and even investing in my own API discovery format APIs.json. It is a layer of the API space that feels very bipolar to me, with highs and lows, and a lot of meh in the middle. I do not claim to have “the solution” when it comes to API discovery and prefer just watching what is happening, and contributing where I can. A number interesting signals for API deployment, as well as API discovery, are coming out of Amazon Marketplace lately. I find myself keeping a closer eye on the almost 350 API related solutions in the marketplace, and today I’m specifically taking notice of the Box API availability in the AWS Marketplace. I find this marketplace approach to not just API discovery via an API marketplace, but also API deployment very interesting. AWS isn’t just a marketplace of APIs, where you find what you need and integrate directly with that provider. It is where you find your API(s) and then spin up an instance within your AWS infrastructure that facilitates that API integration–a significant shift. I’m interested in the coupling between API providers and AWS. AWS and Box have entered into a partnership, but their approach provides a possible blueprint for how this approach to API integration and deployment can scale. How tightly coupled each API provider chooses to be, looser (proxy calling the API), or tighter (deploying API as AMI), will vary from implementation to implementation, but the model is there. The Box AWS Marketplace instance dependencies on the Box platform aren’t evident to me, but I’m sure they can easily be quantified, and something I can get other API providers to make sure and articulate when publishing their API solutions to AWS Marketplace. AWS is moving towards earlier visions I’ve had of...[<a href="/2017/06/16/publishing-your-api-in-the-aws-marketplace/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/16/github-helping-set-the-bar-for-your-api-community-code-standards/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-community-standards.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/16/github-helping-set-the-bar-for-your-api-community-code-standards/">Github Helping Set The Bar For Your API Community Code Standards</a></h3>
			<p><em>16 Jun 2017</em></p>
			<p>
Github has released an interesting new feature to help users better manage some of the community elements of the repositories they use to manage code, definitions, data, and content across API operations. For each repository, you now have a community profile tab, where you’ll see a checklist showing how your project compares to Github recommended community standards.

If you are lacking one of these common elements, it gives you an option to quickly add one of the missing pieces. I still have some repositories where I don’t properly have licensing dictated, even a handful without a README (I know). Almost none of my repositories have a code of conduct or contributing agreement. The new feature adds another task to my list of maintenance items I’ll be tackling to help standardize the projects I manage on Github (which is everything I do).

I like where Github is going with this. It resembles what I am trying to do with API projects by identifying the common building blocks of API deployments, providing a checklist that API providers can follow when publishing their APIs–which is often done using Github. One way that API providers can help standardize these common elements across groups is to create a working API portal prototype that is forkable on Github, similar to what the GSA is doing for the federal government with their working API prototype.

Most of the time API architects and eveloper just need a friendly reminder, or possibly a working example they can follow when it comes to standardizing how we deploy and manage the community elements of our API operations. I hope what Github is doing with their community standards baseline will keep evolving, spreading, and eventually be something that Github organizations can templatize, standardize, and define their own criteria regarding the minimum viable aspects of community operations for the repositories they have on Github.

[<a href="/2017/06/16/github-helping-set-the-bar-for-your-api-community-code-standards/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/15/zooming-out-to-the-100k-level-then-back-to-api-sea-level-with-openapi-and-apis-json/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/drone_control_sunset.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/15/zooming-out-to-the-100k-level-then-back-to-api-sea-level-with-openapi-and-apis-json/">Zooming Out To The 100K Level Then Back To API Sea Level With OpenAPI And APIs.json</a></h3>
			<p><em>15 Jun 2017</em></p>
			<p>I’m wrestling with the different levels of conversations I’m having around my human services API work. Some of my audience are more technical and are pushing for discussion at the granular level, while other parts of my audience are more about the business of things at the 100K. I appreciate these types of projects, but when there are many different conversations going on at many different levels, it is a lot of work to wrestle things into something coherent that everyone involved will appreciate. One day I’m thinking about which individual fields are required, then next I will considering how multiple human services API integrators will be syndicating and sharing information between clusters of human service API implementations. While I’m relying on Github, and Slack to facilitate conversations that going on, I am ultimately relying on OpenAPI and APIs.json to help me hammer out the contract that will speak to the developers at the granular level but can also communicate the business and political terms of the API contract. It will describe which fields are required as well as describe the webhooks I need to define how to syndicate and share between implementations. OpenAPI is pretty focused on helping me with things happening at API sea level, but I’m exploring using APIs.json to help me organize conversations all the way up to the 100K foot level. Things like, where do I signup for my API keys, access partnership levels of access, find the terms of service, or possibly someone to contact and answer a question. Then using the OpenAPI I can publish documentation for developers to understand the surface area of the API (sea level), and while the APIs.json includes a pointer to this discussion, it also provides pointers to other discussions going on around support, communications, changes, privacy, security, so that I can generate documentation for business and partner stakeholders as well. I’m working on an example of doing this for my Open Referral...[<a href="/2017/06/15/zooming-out-to-the-100k-level-then-back-to-api-sea-level-with-openapi-and-apis-json/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/15/my-api-communication-stack-for-the-human-services-api-specification/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-referral/human-services-data-specification-draft-snapshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/15/my-api-communication-stack-for-the-human-services-api-specification/">My API Communication Stack For The Human Services API Specification</a></h3>
			<p><em>15 Jun 2017</em></p>
			<p>
I’m refining my approach to moving forward the discussion around the Human Services Data Specification and API in an attempt to include more vendors and implementors in the conversation. Part of this work is to streamline how we move forward an increasing number of conversations regarding the schema and API definition.

I am looking help solidify our communication strategy around the human services API, and help make clear which channels participants can tune into:


  Github - Github Issues is where the specific conversation around a variety
  Slack - A variety of Slack channels for discussing the evolution of API.
  Blog - Storytelling via API Evangelist, and specific project level blogs.
  GHangouts - Virtual gatherings to discuss the API via video conferencing.


These are the channels where the HSDS/A conversations are occurring. It is spread unevenly across these synchronous and asynchronous digital channels. We are using a variety of signals including Github issues, Slack messaging as well as video conference calls, blog posts, and semi-regular virtual gatherings.

I am heavily using the blog post to organize my ideas, distilling down the explosion of information, ideas, and technical details in smaller, coherent, bite-size chunks. This helps me organize and better communicate what’s going on, which includes having a single URL to share with new players. In fact, this blog post is part of me pulling together my communication around the API communications strategy for the human services API project and will be the most current URL I share with people.

[<a href="/2017/06/15/my-api-communication-stack-for-the-human-services-api-specification/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/15/a-community-approval-dimension-when-adding-updating-and-deleting-via-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-community-join.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/15/a-community-approval-dimension-when-adding-updating-and-deleting-via-api/">A Community Approval Dimension When Adding, Updating, And Deleting Via API</a></h3>
			<p><em>15 Jun 2017</em></p>
			<p>
One of the projects I’m working on as part my Human Services API work is trying to define the layer that allows developers to add, update, and delete data via the API. We ultimately want to empower 3rd party developers, and external stakeholders to help curate and maintain critical human services data within a community, through trusted partners.

The Human Services API allows for the reading and writing of organizations, locations, and services for any given area. I am looking to provide guidance on how API implementors can allow for POST, PUT, PATCH, and DELETE on their API, but require approval before any changing transaction is actually executed. Requiring the approval of an internal system administrator to ultimately give the thumbs up or thumbs down regarding whether or not the change will actually occur.

A process which immediately begs for the ability to have multiple administrators or even possibly involving external actors. How can we allow organizations to have a vote in approving changes to their data? How can multiple data stewards be notified of a change, and given the ability to approve or disprove, logging every step along the way? Allowing any change to be approved, reviewed, audited, and even rolled back. Making public data management a community affair, with observability and transparency built in by default.

I am doing research into different approaches to tackling this, ranging from community approaches like Wikipedia, to publish and subscribe, and other events or webhook models. I am looking for technological solutions to opening up approval to the API request and response structure, with accompanying API and webhook surface area for managing all aspects of the approval of any API changes. If you know of any interesting solutions to this problem I’d love to hear more, so that I can include in my research, future storytelling, and ultimately the specification for the Open Referral Human Services Data Specification and API.

[<a href="/2017/06/15/a-community-approval-dimension-when-adding-updating-and-deleting-via-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/14/the-yes-i-would-like-to-talk-button-when-signing-up-for-an-api-platform/"><img src="https://s3.amazonaws.com/kinlane-productions2/reprezen/the-yes-id-like-to-talk-button.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/14/the-yes-i-would-like-to-talk-button-when-signing-up-for-an-api-platform/">The Yes I Would Like To Talk Button When Signing Up For An API Platform</a></h3>
			<p><em>14 Jun 2017</em></p>
			<p>
There are never enough hours in the day. I have an ever growing queue of APIs and API related services that I need to play with for the first time, or just make sure and take another look at. I was FINALLY making time to take another look at the RepreZen API Studio again when I saw that they were now supporting OpenAPI 3.0.

I am still driving it around the block, but I thought the second email I got from them when I was signing up was worth writing about. I had received a pretty standard getting started email from them, but then I got a second email from Miles Daffin, their product manager, reminding me that I can reach out, and providing me with a “Yes I Would Like To Talk Button”. I know, another pretty obvious thing, but you’d be surprised how a little thing like this can actually break us from our regular isolated workspace, and make the people behind an API, or API related service more accessible. The email was pretty concise and simple, but what caught my eye when I scanned was the button.

Anyways, just a small potential building block for your API communication strategy. I’ll be adding the list I am cultivating. I’m not a big hard sell kind of guy, and I appreciate soft outreach like this–leaving it up to me when I want to connect. I’ll keep playing with RepreZen API Studio and report back when I have anything worth sharing. I just wanted to make sure this type of signup email was included in my API communication research.

[<a href="/2017/06/14/the-yes-i-would-like-to-talk-button-when-signing-up-for-an-api-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/14/the-successes-and-mostly-failures-of-a-developer-evangelist/"><img src="https://s3.amazonaws.com/kinlane-productions2/stitch-data/the-evangelism-compendium.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/14/the-successes-and-mostly-failures-of-a-developer-evangelist/">The Successes And (Mostly) Failures Of A Developer Evangelist</a></h3>
			<p><em>14 Jun 2017</em></p>
			<p>I am a big fan of companies who share their API journey publicly. The comment I hear from readers, as well as attendees of @APIStrat often, is that they want to hear more honest stories from API practitioners regarding every stop along the API lifecycle from defining to deprecation. I encourage API providers to actively share their stories publicly on their blog, and even semi-privately via email newsletters. Ash Hathaway (@ash_hathaway) over at Stich Data asked me what I thought about her doing an evangelism email newsletter based on her experiences–to which I responded with, “hell yeah, it is a great idea!”. So she has launched The Evangelism Compendium, the successes and (mostly) failures of a developer evangelist email newsletter. She will be sharing her regular thoughts from the trenches, as she is evangelizing for the data integration platform. Sharing stories from the API trenches like this is a great way to generate content for your operations, while also working through your thoughts on what is working (or not), when it comes to evangelism and outreach for your platform. I think the email newsletter has two audiences 1) data stewards looking to learn more about managing your data in today’s online cloud environment, and 2) other data and API service providers who are looking to learn, and hopefully share thoughts on evangelizing your platform. Many folks think I’m crazy for encouraging this type of real-time transparency into your operations, something that can make some feel vulnerability and exposed, but I find it to the best way to generate honest and compelling content. API Evangelist is the result of me doing this for the last seven years. Sharing my thoughts as I do my work. I find it is a helpful part of my regular workflow to tell stories in this way, as it helps me refine and articulate my approach. It also generates valuable SEO and SMM exhaust for my platform, while also hopefully helping...[<a href="/2017/06/14/the-successes-and-mostly-failures-of-a-developer-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/14/the-gsa-api-standards-with-a-working-prototype-api-and-portal/"><img src="https://s3.amazonaws.com/kinlane-productions2/gsa/gsa-prototype-api-portal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/14/the-gsa-api-standards-with-a-working-prototype-api-and-portal/">The GSA API Standards With A Working Prototype API And Portal</a></h3>
			<p><em>14 Jun 2017</em></p>
			<p>One way to help API developers understand API design is to provide them with a design guide, helping set a standard for how APIs should be designed across an organization or group. Another way to help developers follow best practices when it comes to API design is to provide them with a working example they can follow when developing their API(s). In my experience people learn API design best practices through following what they know–emulating what they see. Hang on to that thought, cause now I’m going to blow your mind. Guess how API providers learn how to provide API design guide and working examples? By showcasing working examples of companies, institutions, and government agencies publishing API design guides, working APIs, and portal prototypes. So that other API providers can learn by example! BOOM! Mind blown. :-) An example of this can be found over at the General Service Administration (GSA), with their API standards guide, API prototype, and forkable API portal and documentation–complete with the essential API building blocks. The GSA’s simple approach to providing a working example of their API standards is refreshing. They have taken an existing GSA data set and launched a prototype API, then they published the API in a complete, working API developer portal with all the essential building blocks of a basic API presence. Getting Started - What you need to get started with the API. Documentation - OpenAPI driven interactive API documentation. Schema - A reference of the fields / schema used for API responses. FAQ - Some basic questions about the API, and more importantly the API prototype. Contact Info - Using Github issues for support, with an accompanying email. The whole things runs on Github so it is forkable. What is great, is that they also have the source code for the API on Github, essentially providing a working, forkable representation of what is expected in the GSA API design guide. This is how you...[<a href="/2017/06/14/the-gsa-api-standards-with-a-working-prototype-api-and-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/14/proprietary-views-of-your-taxonomy/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-taxonomy.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/14/proprietary-views-of-your-taxonomy/">Proprietary Views Of Your Taxonomy</a></h3>
			<p><em>14 Jun 2017</em></p>
			<p>I’ve been investing a lot more energy into open data and APIs involved with city government, something I’ve dabbled in as long as I’ve been doing API Evangelist, but is something I’ve ratcheted up pretty significantly over the last couple of years. As part of this work, I’ve increasingly come across some pretty proprietary stances when it comes to data that is part of city operations–this stuff has been seen as gold, long before Silicon Valley came along, with long lines of folks looking to lock it up and control it. Helping civic data stakeholders separate the licensing layers around their open data and APIs is something I do as the API Evangelist. Another layer I will be adding to this discussion is around taxonomy. How city data folks are categorizing, organizing, and classifying the valuable data needed to make our cities work. I’ve been coming across more vendors in the city open data world who feel their taxonomy is their secret sauce and falls under intellectual property protection. I don’t have any wisdom regarding why this is a bad idea, but I will keep writing about as part of my wider API licensing work to help flesh out my ideas, and create a more coherent and precise argument. I understand that some companies put a lot of work into taxonomies, and the description behind how they organize things, but like API definitions and schema, these are aspects of your open data and API operations you want to be a widely understood, shared, and reusable knowledge within your systems, as well as the 3rd party integration of your partners and customers. Making your taxonomy proprietary isn’t going to help your brand, or get you ahead in the game. I recommend focusing on other aspects of the value you bring to the table and keep your taxonomy as openly licensed as you possibly can, encouraging adoption by others. I’ll work on a more robust argument as...[<a href="/2017/06/14/proprietary-views-of-your-taxonomy/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page10" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page12" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home Page</a></li>
    <li><a href="/blog/">The Blog</a></li>
    <li><a href="https://101.apievangelist.com/">API 101</a></li>
    <li><a href="http://history.apievangelist.com">History of APIs</a></li>
    <li><a href="https://women-in-tech.apievangelist.com/">Women in Technology</a></li>    
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
