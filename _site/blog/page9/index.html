<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/09/treating-your-apis-like-they-are-infrastructure/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripe-api-versioning.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/09/treating-your-apis-like-they-are-infrastructure/">Treating Your APIs Like They Are Infrastructure</a></h3>
			<p><em>09 Oct 2017</em></p>
			<p>We all (well most of us) strive to deliver as stable of an API presence as we possibly can. It is something that is easier said than done. It is something that takes caring, as well as the right resources, experience, team, management, and budget to do APIs just right. It is something the API idols our there make look easy, when they really have invested a lot of time and energy into developing a agile, yet scalable approach to ensuring APIs stay up and running. Something that you might able to achieve with a single API, but can easily be lost between each API version, as we steer the ship forward. I spend a lot of time at the developer portals of these leading API providers looking for interesting insight into how they are operating, and I though Stripe’s vision around versioning their API is worth highlighting. Specifically their quote about treating your API like they are real life physical infrastructure. “Like a connected power grid or water supply, after hooking it up, an API should run without interruption for as long as possible.Our mission at Stripe is to provide the economic infrastructure for the internet. Just like a power company shouldn’t change its voltage every two years, we believe that our users should be able to trust that a web API will be as stable as possible.” This is possible. This is how I view Amazon S3, and Pinboard. These are two APIs I depend on to make my business work. Storage and bookmarking are two essential resources in my world, and both these APIs have consistently delivered stable API infrastructure, that I know I can depend on. I think it is also interesting to note that one is a tech giant, while the other is a viable small business (not startup). Demonstrating for me that there isn’t a single path to being a reliable, stable, API provider, despite what some folks might...[<a href="/2017/10/09/treating-your-apis-like-they-are-infrastructure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/09/publishing-your-api-road-map-using-trello/"><img src="https://s3.amazonaws.com/kinlane-productions2/tyk/tyk-road-map.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/09/publishing-your-api-road-map-using-trello/">Publishing Your API Road Map Using Trello</a></h3>
			<p><em>09 Oct 2017</em></p>
			<p>I consider a road map for any API to be an essential building block, whether it is a public API or not. You should be in the business of planning the next steps for your API in an organized way, and you should be sharing that with your API consumers so that they can stay up to speed on what is right around the corner. If you want to really go the extra mile I recommend following what Tyk is up to, with their public road map using Trello. With the API management platform Tyk, you don’t just see a listing of their API road map, you see all the work and conversation behind the road ma using the visual collaboration platform Trello. Using their road map you can see proposed features, which is great to see if something you want has already been suggested, and you can get at a list of what the next minor releases will contain. Plus using the menu bar you can get at a history of the changes the Tyk team has made to the platform, going back for the entire history of the Trello board. Using Trello you can subscribe to, or vote up any of the message boards. If you want to submit something you need to sign-up and post something to the Tyk community. Then they’ll consider adding it to the proposed road map features. It is a pretty low cost, easy to make public, approach to delivering a road map. Sometimes this stuff doesn’t need a complex solution, just one that provides some transparency, and help your customers understand what is next. Tyk provides a nice way to provide a road map that any other API provider, or service provider can follow. Another interesting approach to delivering an API road map that I can add to my research. I’m a big fan of having many different ways of delivering the essential building blocks of API...[<a href="/2017/10/09/publishing-your-api-road-map-using-trello/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/09/learning-about-api-governance-from-capital-one-devexchange/"><img src="https://cdn-images-1.medium.com/max/800/0*DOAJXcR7VcCN3iOh.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/09/learning-about-api-governance-from-capital-one-devexchange/">Learning About API Governance From Capital One DevExchange</a></h3>
			<p><em>09 Oct 2017</em></p>
			<p>I am still working through my notes from a recent visit to Capital One, where I spent time talking with Matthew Reinbold (@libel_vox) about their API governance strategy. I was given a walk through their approach to defining API standards across groups, as well as how they incentivize, encourage, and even measure what is happening. I’m still processing my notes from our talk, and waiting to see Matt publish more on his work, before I publish too many details, but I think it is worth looking at from a high level view, setting the bar for other API governance conversations I am engaging in. First, what is API governance. I personally know that many of my readers have a lot of misconceptions about what it is, and what it isn’t. I’m not interesting in defining a single definition of API governance. I am hoping to help define it so that you can find it a version of it that you can apply across your API operations. API governance is at its simplest form, about ensuring consistency in how you do API across your development groups, and a more robust definition might be about having an individual or team dedicated to establishing organization-wide API standards, helping train, educate, enforce, and in the case of capital one, measure their success. Before you can begin thinking about API governance, you need to start establishing what your API standards are. In my experience this usually begins with API design, but should also quickly also be about consistent, API deployment, management, monitoring, testing, SDKs, clients, and every other stop along the API lifecycle. Without well-defined, and properly socialized API standards, you won’t be able to establish any sort of API governance that has any sort of impact. I know this sounds simple, but I know more API providers who do not have any kind of API design, or other guide for their operations, than I know API providers who have...[<a href="/2017/10/09/learning-about-api-governance-from-capital-one-devexchange/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/09/communication-strategy-filler-using-sections-of-your-api-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions2/aws/aws-tweet-documentation-links.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/09/communication-strategy-filler-using-sections-of-your-api-documentation/">Communication Strategy Filler Using Sections Of Your API Documentation</a></h3>
			<p><em>09 Oct 2017</em></p>
			<p>&lt;/a&gt; Coming up with things creative things to write about regularly on the blog, and on Twitter when you are operating an API is hard. It has taken a lot of discipline to keep posts going up on API Evangelist regularly for the last seven years–totaling almost 3K total stories told so far. I don’t expect every API provider to have the same obsessive compulsive disorder that I do, so I’m always looking for innovative things that they can do to communicate with their API communities–something that Amazon Web Services is always good at providing healthy examples that I feel I can showcase. One thing the AWS team does on a regular basis is tweeting out links to specific areas of their documentation, that helps users accomplish specific things with AWS APIs. The AWS security team is great at doing this, with recent examples focusing on securing things with the AWS Directory Service, and API Organizations. Each contains a useful description, attractive looking image, and a link to a specific page in the documentation that helps you learn more about what is possible. I have been pushing myself to make sure all headers, and sub headers in my API documentation have anchors, so that I can not just link to a specific page, but I can link to a specific section, path, or other relevant item within my API documentation. This helps me in my storytelling when I’m looking to reference specific topics, and would help when it comes to tweeting out regular elements across my documentation in tweets. I’m slowly going to push out some of the lower grade tweets of curated news that I push out, and replace with relevant work I do in specific areas of my research–using my own work to fill the cracks over less than exciting things I may come across in the API space. Tweeting out what is possible with your API, with links to specific sections of...[<a href="/2017/10/09/communication-strategy-filler-using-sections-of-your-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/06/thinking-about-why-we-rate-limit-our-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/power-lines-empty-space_sunday.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/06/thinking-about-why-we-rate-limit-our-apis/">Thinking About Why We Rate Limit Our APIs</a></h3>
			<p><em>06 Oct 2017</em></p>
			<p>I am helping a client think through their API management solution at the moment, so I’m working through all the moving parts of how, and why of API management solutions. The API management landscape has shifted since the last time I helped a small company navigate the process of getting up and running, so I wanted to work through each aspect and think critically before I make any recommendations. My client has a content API, which isn’t very complex, but possesses some pretty valuable data they’ve aggregated, curated, and are looking to make available via a simple web API. It is pretty clear that all developers will need a key to be access the API, but I wanted to pause for a moment and think more about API rate limiting. Why do we rate limit? The primary reason is to help manage the compute resources available for all API consumers. You don’t want any single user hitting the server too hard, and taking things down for everyone else. I’d say after that, the next major reason is to enforce API access tiers, and ensure API consumers are only consuming what they should be. Which both seem like pretty dated concepts, that might need re-evaluation in general, but also in the context of this particular project. There is no free access to this API. I believe there will be a public account for test driving (making very limited # of calls), and some that drive their embeddable strategy, but for access to the majority of content, developers will have to register for a key, and provide a credit card to pay for their consumption. Which leaves me with the question, should we be rate limiting at all? If users are paying for whatever they consume, and there is a credit card on file, do we want to rate limit? Why are we so worried about server capacity in a cloud world? It seems like rate limiting...[<a href="/2017/10/06/thinking-about-why-we-rate-limit-our-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/06/the-api-management-landscape-has-shifted-more-than-i-anticipated/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/yellow-tree-in-the-rocks.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/06/the-api-management-landscape-has-shifted-more-than-i-anticipated/">The API Management Landscape Has Shifted More Than I Anticipated</a></h3>
			<p><em>06 Oct 2017</em></p>
			<p>It is interesting to take a fresh look at the API management landscape these days. It has been a while since I’ve looked through all the providers to see where their pricing is at, and what they offer. I’d say the space has definitely shifted from what things looked like 2012 through 2015. There are still a number of open source offerings, which there weren’t in 2012, but the old guard has solidly turned their attention to the enterprise. There are the cloud solutions like Restlet, ad SlashDB which really help you get up and running from existing data sources in the cloud, but for this particular project I am looking for a simple proxy and connector approach to deploying on any infrastructure, and they don’t quite fit the requirements. Apigee, and the other more enterprise offerings have always been out of my league, and 3Scale’s entry level package is up to $750, which is a little out of my reach, but I do know they are open sourcing their offering, now that they are part of Red Hat. There is API Umbrella, APIMan, Fusio, Monarch, and handful of other solutions that will work, but they take certain platform, or specific language commitment that doesn’t work for this project. Everything else is of the enterprise caliber, nothing really that I would recommend to my customers who are just getting started on their API journey. I’m really left with the cloud giants, which I guess is one of the main reasons we are at this junction in the evolution of API management. API management becoming a commodity has really shifted the landscape, making it more difficult to be a strong player like Tyk and Kong are managing to pull off. If my customer was looking to launch a data API from an existing database I’d point them to SlashDb or Restlet. If they are an enterprise customer I’d point them to 3Scale. Tyk is pretty much...[<a href="/2017/10/06/the-api-management-landscape-has-shifted-more-than-i-anticipated/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/06/publish-share-monetize-machine-learning-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/azure/azure-publish-share-monetize-ml.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/06/publish-share-monetize-machine-learning-apis/">Publish, Share, Monetize Machine Learning APIs</a></h3>
			<p><em>06 Oct 2017</em></p>
			<p>I’ve been playing with Tensor Flow for over a year now, specifically when it comes to working with images and video, but it has been something that has helped me understand what things looks like behind the algorithmic curtain that seems to be part of a growing number of tech marketing strategies right now. Part of this learning is exploring beyond Google’s approach, who is behind Tensor Flow, and understand what is going on at AWS, as well as Azure. I’m stil getting my feet wet learning about what Microsoft is up to with their platform, but I did notice one aspect of the Azure Machine Learning Studio emphasized developers to, “publish, share, monetize” their ML models. While I’m sure there will be a lot of useless vapor ware being sold within this realm, I’m simply seeing it as the next step in API monetization, and specifically the algorithmic evolution of being an API provider. As the label says in the three ML models for sale in the picture, this is all experimental. Nobody knows what will actually work, or even what the market will bear. However, this is something APIs, and the business of APIs excel at. Making a digital resource available to consumers in a retail, or even wholesale way via marketplaces like Azure and AWS, then playing around with features, pricing, and other elements, until you find the sweet spot. This is how Amazon figured out the whole cloud computing game, and became the leader. It is how Twilio, Stipe and other API as a product companies figured out what developers needed, and what these markets would bear. This will play out in marketplaces like Azure and Google, as well as startup players like Algorithmia–which is where I’ve been cutting my teeth, and learning about ML. The challenge for ML API entrepreneurs will be helping consumers understand what their models do, or do not do. I see it as an opportunity, because...[<a href="/2017/10/06/publish-share-monetize-machine-learning-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/digital-bits-capital-dc-flag-side-view.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/">A Couple More Questions For The Equifax CEO About Their Breach</a></h3>
			<p><em>06 Oct 2017</em></p>
			<p>Speaking to the House Energy and Commerce Committee, former Equifax CEO Richard Smith pointed the finger at a single developer who failed to patch the Apache Struts vulnerability. Saying that protocol was followed, and a single developer was responsible, shifting the blame away from leadership. It sounds like a good answer, but when you operate in the space you understand that this was a systemic failure, and you shouldn’t be relying on a single individual, or even a single piece of scanning software to verify the patch was applied. You really should have many layers in place to help prevent breaches like we saw with Equifax. If I was interviewing the CEO, I’d have a few other questions for him, getting at some of the other systemic and process failures based upon his lack of leadership, and awareness: API Monitoring &amp; Testing - You say the scanner for the Apache Struts vulnerability failed, but what about other monitoring and testing. The plugin in questions was a REST plugin, that allowed for API communication with your systems. Due to the vulnerability, extra junk information was allowed to get through. Where were your added API request and response integrity testing and monitoring process? Sure you were scanning for the vulnerability, but are you keeping an eye on the details of the data being passed back and forth? API monitoring &amp; testing has been around for many years, and service providers like Runscope do this for a living. What other layers of monitoring and testing were in place? API Management - When you expose APIs like you did from Apache Struts, what does the standardized management approach look like? What sort of metering, logging, rate limiting, and analysis occurs on each endpoint, and verification occurs, ensuring that only required clients should have access? API management has been standard procedure for over a decade now for exposing APIs like this both internally and externally. Why didn’t your API management...[<a href="/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/"><img src="ttps://s3.amazonaws.com/kinlane-productions2/holmes-county/holmes-county-listing.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/">Teaching My Client Three Approaches To Modular UI Design Using Their APIs</a></h3>
			<p><em>05 Oct 2017</em></p>
			<p>I am working with a client to develop a simple user interface on top of a Human Services Data API (HSDA) I launched for them. They want a basic website for searching, browsing, and navigating the organizations, locations, and services available in their API. A part of this work is helping them understand how modular and configurable their web site is, with each page, or portion of a page being a simple API call. It is taking a while for them to fully understand what they have, and the potential of evolving a web application in this way, but I feel like they are beginning to understand, and are taking the reigns a little more when it comes to dictate what they want within this new world. When I first published a basic listing of human services they were disappointed. They had envisioned a map of the listings, allowing users to navigate in a more visual way. I got to work helping them see the basic API call(s) behind the listing, and how we could use the JSON response in any way we wanted. I am looking to provide three main ways in which I can put the API data to work in a variety of web applications: Server-Side - A pretty standard PHP call to the API, taking the results and rendering to the page using HTML. Client-Side - Leveraging JavaScript in the browser to call the API and then render to the page using Jquery. Static Push - Calling the APPI using PHP, then publishing as YAML or JSON to a Jekyll site and rendering with Liquid and HTML. What the resulting HTML, CSS, and JavaScript looks like in all these scenarios is up to the individual who is in charge of dictating the user experience. In this case, it is my client. They just aren’t used to having this much control over dictating the overal user experience. Which path they choose depends...[<a href="/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/05/show-the-api-call-behind-each-dashboard-visualization/"><img src="https://s3.amazonaws.com/kinlane-productions2/kentik/kentik_API_menu-300w.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/05/show-the-api-call-behind-each-dashboard-visualization/">Show The API Call Behind Each Dashboard Visualization</a></h3>
			<p><em>05 Oct 2017</em></p>
			<p>I am a big fan of user interfaces that bring APIs out of the shadows. Historically, APIs are often a footnote in the software as a service (SaaS) world, available as a link way down at the bottom of the page, in the settings, or help areas. Rarely, are APIs made a first class citizen in the operations of a web application, which really just perpetuates the myth that APIs aren’t for everybody, and the “normals” shouldn’t worry their little heads about it. When in reality, EVERYBODY should know about APIs, and have the opportunity to put them to work, so we should stop burying the links to our APIs, and our developer areas. If your API is too technical for a business user to understand what is going on, then you should probably get to work simplifying it, not burying it and keeping it in developer and IT realm. I have written before about how DNS provider CloudFlare provides an API behind every feature in their user interface, and I’ve found another great example of this over at the network API provider Kentik. In their network dashboard visualization tooling they provide a robust set of tooling for accessing the data behind the visuals, allowing you to export, view SQL, show API call, and enter share view. In their post, they proceed to instruction about how you can get your API key as part of your account, as well as providing a pretty robust introduction into why APIs are important. This is how ALL dashboards should work in my opinion. Any user should be introduced to APIs, and have the ability to get at the data behind, and export it, or directly make an API call in their browser or at the command line. Developers like to think this stuff should be out of reach of the average user, but that is more about our own insecurities, and power trips, than it is about the...[<a href="/2017/10/05/show-the-api-call-behind-each-dashboard-visualization/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/05/how-api-evangelist-works/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/crypto-machine-bletchley_copper_circuit.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/05/how-api-evangelist-works/">How API Evangelist Works</a></h3>
			<p><em>05 Oct 2017</em></p>
			<p>I’ve covered this topic several times before, but I figured I’d share again for folks who might have just become readers int he last year. Providing an overview of how API Evangelist works, to help eliminate confusion as you are navigating around my site, as well as to help you find what you are looking for. First, API Evangelist was started in the summer of 2010 as a research site to help me better understand what is going on in the world of APIs. In 2017, it is still a research site, but it has grown and expanded pretty dramatically into a network of websites, driven by a data and a content core. The most import thing to remember is that all my sites run on Github, which is my workbench in the the API Evangelist workshop. apievangelist.com is the front door of the workshop, with each area of my research existing as its own Github repository, at its own subdomain with the apievangelist domain. An example of this can be found in my API design research, where you will find at design.apievangelist.com. As I do my work each day, I publish my research to each of my domains, in the form of YAML data for one of these areas: Organizatons - Companies, organizations, institutions, programs, and government agencies doing anything interesting with APIs. Individuals - The individual people at organizations, or independently doing anything interesting with APIs. News - The interesting API related, or other news I curate and tag daily in my feed reader or as I browse the web. Tools - The open source tooling I come across that I think is relevant to the API space in some way. Building Blocks - The common building blocks I find across the organizations, and tooling I’m studying, showing the good and the bad of doing APIs. Patents - The API related patents I harvest from the US Patent Office, showing how IP is...[<a href="/2017/10/05/how-api-evangelist-works/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/05/big-data-is-not-about-access-using-web-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/dragon_close-up_yellow_collage.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/05/big-data-is-not-about-access-using-web-apis/">Big Data Is Not About Access Using Web APIs</a></h3>
			<p><em>05 Oct 2017</em></p>
			<p>I’m neck deep in research around data and APIs right now, and after looking at 37 of the Apache data projects it is pretty clear that web APIs are not a priority in this world. There are some of the projects that have web APIs, and there a couple projects that look to bridge several of the projects with an aggregate or gateway API, but you can tell that the engineers behind the majority of these open source projects are not concerned with access at this level. Many engineers will counter this point by saying that web APIs can’t handle the volume, and it shows that the concept isn’t applicable in all scenarios. I’m not saying web APIs should be used for the core functionality at scale, I’m saying that web APIs should be present to provide access to the result state of the core features for each of these platform, whatever that is, which something that web APIs excel at. From my vantage point the lack of web APIs isn’t a technical one, it is a business and political motivation. When it comes to big data the objectives are always about access, and it definitely isn’t about the wide audience access that comes when you use HTTP, and the web for API access. The objective is to aggregate, move around, and work with as much data as you possibly can amongst a core group of knowledgable developers. Then you distribute awareness, access, and usage to designated parties via distilled analysis, visualizations, or in some cases to other systems where the result can be accessed and put to use. Wide access to this data is not the primary objective, paying forward much of the power and control we currently see around database to API efforts. Big data isn’t about democratization. Big Data is about aggregating as much as you can and selling the distilled down wisdom from analysis, or derived as part of machine learning...[<a href="/2017/10/05/big-data-is-not-about-access-using-web-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/top-aws-iam-documentation-pages-so-far-in-2017.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your/">Sharing Top Sections From Your API Documentation As Part Of Your</a></h3>
			<p><em>04 Oct 2017</em></p>
			<p>I’m always learning from the API communication practices from out of the different AWS teams. From the regular storytelling coming out of the Alexa team, to the mythical tales of leadership at AWS that have contributed to the platform’s success, the platform provides a wealth of examples that other API providers can emulate. As I talked about last week, finding creative ways to keep publishing interesting content to your blog as part of your API evangelism and communications strategy is hard. It is something you have to work at. One way I find inspiration is by watching the API leaders, and learning from what they do. An interesting example I recently found out of the AWS security team, was their approach to showcasing the top 20 AWS IAM documentation pages so far in 2017. It is a pretty simple, yet valuable way to deliver some content for your readers, that can also help you expose the dark corners of your API documentation, and other resources on your blog. The approach from the AWS security team is a great way to generate content without having to come up with good ideas, but also will help with your SEO, especially if you can cross publish, or promote through other channels. It’s pretty basic content stuff, that helps with your overall SEO, and if you play it right, you could also get some SMM juice by tweeting out the store, as well as maybe a handful of the top links from your list. It is pretty listicle type stuff, but honestly if you do right, it will also deliver value. These are the top answers, in a specific category, that your API consumers are looking for answers in. Helping these answers rise to the top of your blog, search engine, and social media does your consumers good, as well as your platform. One more tool for the API communications and evangelism toolbox. Something you can pull out when...[<a href="/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/flower-barbed-wire_clean_view.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/">Letting Go In An API World Is Hard To Do</a></h3>
			<p><em>04 Oct 2017</em></p>
			<p>I encounter a number of folks who really, really, really want to do APIs. You know, because they are cool and all, but they just can’t do what it takes to let go a little, so that their valuable API resources can actually be put to use by other folks. Sometimes this happens because they don’t actually own the data, content, or algorithms they are serving up, but in other cases it is because they view their thing as being so valuable, and so important that they can’t share it openly enough, to be accessible via an API. Even if your APIs are private, you still have to document, and share access with folks, so they can understand what is happening, and have enough freedom to put to use in their application as part of their business, without too much constraint and restrictions. Some folks tell me they want to do API, but I can usually tell pretty quickly that they won’t be able to go the distance. I find a lot of this has to do with perceptions of intellectual property, combined with a general distrust of EVERYONE. My thing is super valuable, extremely unique and original, and EVERYONE is going to want it. Which is why they want to do APIs, because EVERYONE will want it. Also, once it is available to EVERYONE via an API, competitors, and people we don’t want getting at it, will now be able to reverse engineer, and copy this amazing idea. However, if we don’t make accessible, we can’t get rich. Dilemna. Dilemna. Dilemna. What do we do? My answer is you probably that you shouldn’t be doing APIs. You see, doing APIs, whether public or privately requires letting go a bit. Sure, you can dial in how much control you are willing to give up using API management solutions, but you still have to let go enough so that people can do SOMETHING with your valuable...[<a href="/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/04/apis-used-to-give-us-access-to-resources-that-were-out-of-our-reach/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/mountainlake/clean_view/file-00_00_58_86.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/04/apis-used-to-give-us-access-to-resources-that-were-out-of-our-reach/">APIs Used To Give Us Access To Resources That Were Out Of Our Reach</a></h3>
			<p><em>04 Oct 2017</em></p>
			<p>I remember when almost all the APIs out there gave us developers access to things we couldn’t ever possibly get on our own. Some of it was about the network effect with the early Amazon and eBay marketplaces, or Flickr and Delicious, and then Twitter and Facebook. Then what really brought it home was going beyond the network effect, and delivering resources that were completely out of our reach like maps of the world around us, (seemingly) infinitely scalable compute and storage, SMS, and credit card payments. In the early days it really seemed like APIs were all about giving us access to something that was out of our reach as startups, or individuals. While this still does exist, it seems like many APIs have flipped the table and it is all about giving them access to our personal and business data in ways that used to be out of their reach. Machine learning APIs are using parlour tricks to get access to our internal systems and databases. Voice enablement, entertainment, and cameras are gaining access to our homes, what we watch and listen to, and are able to look into the dark corners of our personal lives. Tinder, Facebook, and other platforms know our deep dark secrets, our personal thoughts, and have access to our email and intimate conversations. The API promise seems to have changed along the way, and stopped being about giving us access, and is now about giving them access. I know it has always been about money, but the early vision of APIs seemed more honest. It seemed more about selling a product or service that people needed, and was more straight up. Now it just seems like APIs are invasive. Being used to infiltrate our professional and business worlds through our mobile phones. It feels like people just want access to us, purely so they can mine us and make more money. You just don’t see many Flickrs, Google...[<a href="/2017/10/04/apis-used-to-give-us-access-to-resources-that-were-out-of-our-reach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/">API Providers Should Provide Observability Into Government Developer Accounts</a></h3>
			<p><em>04 Oct 2017</em></p>
			<p>I’ve talked about this before, but after reading several articles recently about various federal government agencies collecting, and using social media accounts for surveillance lately, it is a drum I will be beating a lot more regularly. Along with the transparency reports we are beginning to see emerge from the largest platform providers, I’d like to start seeing more observability regarding which accounts, both user and developer are out of government agencies. Some platforms are good at highlighting how government of all shapes and sizes are using their platform, and some government agencies are good at showcasing their social media usage, but I’d like to understand this from purely an API developer account perspective. I’d like to see more observability into which government agencies are requesting API keys. Maybe not specific agencies ad groups, and account details, although that would be a good idea as well down the road. I am just looking for some breakdown of how many developer accounts on a platform are government and law enforcement. What does their API consumption look like? If there is Oauth via a platform, is there any bypassing of the usual authentication flows to get at data, any differently than regular developers would be accessing, or requiring user approval? From what I am hearing, I’m guessing that there are more government accounts out there than platforms either realize, or are willing to admit. It seems like now is a good time to start asking these questions. I would add on another layer to this. If an application developer is developing applications on behalf of law enforcement, or as part of a project for a government agency, there should be some sort of disclosure at this level as well. I know I’m asking a lot, and a number of people will call me crazy, but with everything going on these days, I’m feeling like we need a little more disclosure regarding how government(s) are using our platforms,...[<a href="/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/03/temporal-logic-of-actions-for-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/status-berlin_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/03/temporal-logic-of-actions-for-apis/">Temporal Logic of Actions For APIs</a></h3>
			<p><em>03 Oct 2017</em></p>
			<p>I’m evolving forward my thoughts on algorithmic observability and transparency using APIs, and I was recently introduced to TLA+, or the Temporal Logic of Actions. It is the closest I’ve come to what I’m seeing in my head when I think about how we can provide observability into algorithms through existing external outputs (APIs). As I do with all my work here on API I want to process TLA+ as part of my API research, and see how I can layer it in with what I already know. TLA+ is a formal specification language developed by Leslie Lamport, which can be used to design, model, document, and verify concurrent systems. It has been described as exhaustively-testable pseudocode which can provide a blueprint for software systems. In the context of design and documentation, TLA+ can be viewed as informal technical specifications. However, since TLA+ specifications are written in a formal language of logic and mathematics it can be used to uncover design flaws before system implementation is underway, and are amenable to model checking for finding all possible system behaviours up to some number of execution steps, and examines them for violations. TLA+ specifications use basic set theory to define safety (bad things won’t happen) and temporal logic to define liveness (good things eventually happen). TLA+ specifications are organized into modules.Although the TLA+ standard is specified in typeset mathematical symbols, existing TLA+ tools use symbol definitions in ASCII, using several terms which require further definition: State - an assignment of values to variables Behaviour - a sequence of states Step - a pair of successive states in a behavior Stuttering Step - a step during which variables are unchanged Next-State Rlation - a relation describing how variables can change in any step State Function - an expression containing variables and constants that is not a next-state relation State Predicate - a Boolean-valued state function Invariant - a state predicate true in all reachable states Temporal Formula...[<a href="/2017/10/03/temporal-logic-of-actions-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/03/looking-at-the-37-apache-data-projects/"><img src="https://s3.amazonaws.com/kinlane-productions2/apache/apache-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/03/looking-at-the-37-apache-data-projects/">Looking At The 37 Apache Data Projects</a></h3>
			<p><em>03 Oct 2017</em></p>
			<p>I’m spending time investing in my data, as well as my database API research. I’ll have guides, with accompanying stories coming out over the next couple weeks, but I want to take a moment to publish some of the raw research that I think paints an interesting picture about where things are headed. When studying what is going on with data and APIs you can’t do any search without stumbling across an Apache project doing something or other with data. I found 37 separate projects at Apache that were data related, and wanted to publish as a single list I could learn from. Airvata** - Apache Airavata is a micro-service architecture based software framework for executing and managing computational jobs and workflows on distributed computing resources including local clusters, supercomputers, national grids, academic and commercial clouds. Airavata is dominantly used to build Web-based science gateways and assist to compose, manage, execute, and monitor large scale applications (wrapped as Web services) and workflows composed of these services. Ambari - Apache Ambari makes Hadoop cluster provisioning, managing, and monitoring dead simple. Apex - Apache Apex is a unified platform for big data stream and batch processing. Use cases include ingestion, ETL, real-time analytics, alerts and real-time actions. Apex is a Hadoop-native YARN implementation and uses HDFS by default. It simplifies development and productization of Hadoop applications by reducing time to market. Key features include Enterprise Grade Operability with Fault Tolerance, State Management, Event Processing Guarantees, No Data Loss, In-memory Performance &amp; Scalability and Native Window Support. Avro - Apache Avro is a data serialization system. Beam - Apache Beam is a unified programming model for both batch and streaming data processing, enabling efficient execution across diverse distributed execution engines and providing extensibility points for connecting to different technologies and user communities. Bigtop - Bigtop is a project for the development of packaging and tests of the Apache Hadoop ecosystem. The primary goal of Bigtop is to build...[<a href="/2017/10/03/looking-at-the-37-apache-data-projects/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/database-to-database-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/">Database To Database Then API, Instead Of Directly To API</a></h3>
			<p><em>03 Oct 2017</em></p>
			<p>I am working with a team to expose a database as an API. With projects like this there can be a lot of anxiety in exposing a database directly as an API. Security is the first one, but in my experience, most of the time security is just cover for anxiety about a messy backend. The group I’m working with has been managing the same database for over a decade, adding on clients, and making the magic happen via a whole bunch of databases and table kung fu. Keeping this monster up and running has been priority number one, and evolving, decentralizing, or decoupling has never quite been a priority. The database team has learned the hard way, and they have the resources to keep things up and running, but never seem to have them when it comes to refactoring it and thinking differently, let alone tackling the delivery of a web API on top of things. There will need to be a significant amount of education and training around REST, and doing APIs properly before we can move forward, something there really isn’t a lot of time or interest in doing. To help bridge the gap I am suggesting that we do an entirely new API, with it’s own database, and we focus on database to database communication, since that is what the team knows. We can launch an Amazon RDS instance, with an EC2 instance running the API, and the database team can work directly with RDS (MySQL) which they are already familiar with. We can have a dedicated API team handle the new API and database, and the existing team can handle the syncing from database to database. This also keeps the messy, aggregate, overworked database out of reach of the new API. We get an API. The database team anxiety levels are lowered. It balances things out a little. Sure there will still be some work between databases, but the API...[<a href="/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/03/clearly-designate-api-bot-automation-accounts/"><img src="https://s3.amazonaws.com/kinlane-productions2/shieldsquare/good-v-bad-bots.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/03/clearly-designate-api-bot-automation-accounts/">Clearly Designate API Bot Automation Accounts</a></h3>
			<p><em>03 Oct 2017</em></p>
			<p>I’m continuing my research into bot platform observability, and how API platforms are handling (or not handling) bot automation on their platforms, as I try to make sense of each wave of the bot invasion on the shores of the API sector. It is pretty clear that Twitter and Facebook aren’t that interested in taming automation on their platforms, unless there is more pressure applied to them externally. I’m looking to make sure there is a wealth of ideas, materials, and examples of how any API driven platform can (are) control bot automation on their platform, as the pressure from lawmakers, and the public intensifies. Requiring users clearly identify automation accounts is a good first place to start. Establishing a clear designation for bot users has its precedents, and requiring developers to provide an image, description, and some clear check or flag that identifies an account as automated just makes sense. Providing a clear definition of what a bot is, with articulate rules for what bots should and shouldn’t be doing is next up on the checklist for API platforms. Sure, not all users will abide by this, but it is pretty easy to identify automated traffic versus human behavior, and having a clear separation allows accounts to automatically turned off when they fit a particular fingerprint, until a user can pass a sort of platform Turing test, or provide some sort of human identification. Automation on API platforms has its uses. However, unmanaged automation via APIs has proven to be a problem. Platforms need to step up and manage this problem, or the government eventually will. Then it will become yet another burdensome regulation on business, and there will be nobody to blame except for the bad actors in the space (cough Twitter &amp; Facebook, cough, cough). Platforms tend to not see it as a problem because they aren’t the targets of harassment, and it tends to boost their metrics and bottom line when...[<a href="/2017/10/03/clearly-designate-api-bot-automation-accounts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/beach-rocks-currents_blue_circuit_5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/">The Waves Of API Driven Bots Invading Our Shores</a></h3>
			<p><em>02 Oct 2017</em></p>
			<p>As each wave of technology comes crashing on the shores of the API space you’ll mostly find me silent, listening and watching what is happening. Occasionally you’ll hear me grumble about the aggressiveness of a single wave, or how unaware each wave is of the rest of the beach, or of the waves that came before them. Mostly I am just yelling back to the waves that claim, “we are going to change the beach forever”, and “we are the wave that matters, better than all the waves that came before us”. Mostly, it is the hype, and the unrealistic claims being made by each wave that bothers me, not the waves themselves. I do not think that technology won’t have an impact on the beach. I just think that us technologists tend to over-hype, and over-believe in the power each wave of technology, and that we do not consider the impact on the wider beach, and the amount of sand that ends up in everything. I don’t doubt that there will be some gems found in the sand, and that geologically speaking that the ocean plays a significant role in how the coastline is shaped. I’m just choosing to sit back on the bluff and enjoy my time on the beach, and not choosing to be a three year old playing in each of the waves, super excited by the sound each crash makes on the beach. I’m not saying that playing in the waves is wrong, I’m just choosing to look at the bigger picture from up here on the bluff. You can see one such canvas being painted over the last couple of years with what has become to be known as “bots”. Little automated nuggets of tech goodness, or evil, depending on your location on the beach. People love saying that bots will change everything. They’ll be your assistant. They’ll do everything for you. They’ll automate your life. Take care of...[<a href="/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/02/the-ca-acquisition-of-runscope/"><img src="https://s3.amazonaws.com/kinlane-productions2/runscope/ca-runscope-acquisitions.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/02/the-ca-acquisition-of-runscope/">The CA Acquisition Of Runscope</a></h3>
			<p><em>02 Oct 2017</em></p>
			<p>You won’t find me talking about the acquisition of API startups very often. I’m just not a fan of the game. I am not anti-venture capital, but I find the majority of investment in the API startup ecosystem works against everything we are trying to do with APIs. In my opinion, VC investment shouldn’t be the default, it should be an exception. There are other ways to build a business, and I see too many useful API tools get ruined while playing this game. With that said, I tend to not cover the topic, unless I get really pissed off, or the occasional investment or acquisition that I feel will result in a positive result. Last week we saw the Runscope acquisition by CA. This is an acquisition that doesn’t leave me concerned. Runscope is a partner of mine, run by people I know and care about, and they offer a tool that is useful in the API sector. If they’d had been acquired by many other bigcos I would have been more concerned, or even upset (if it had been certain ones). However, I have experience with CA, and while they are an enterprise beast, I’ve seen them make acquisitions before that weren’t damaging to the services and tooling they acquired. I trust that CA isn’t acquiring Runscope to just eliminate a strong player from the sector, and that they are actually interested in what Runscope does. I have seen CA’s role in the API space through the lens of the API Academy team, as well as through public and private conversations with other CA employees, on a variety of other teams. I’ve gone on-site and participated in API training session, and I have seen evidence that CA is invested in helping evolve their enterprise to be an API aware organization. Something that you can see reflected in how they approach doing business with their customers. I’m currently working to help move forward some...[<a href="/2017/10/02/the-ca-acquisition-of-runscope/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/02/openapi-definitions-for-entire-schema-org-vocabulary-do-not-reinvent-wheel/"><img src="https://s3.amazonaws.com/kinlane-productions2/schema-org/schema-org.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/02/openapi-definitions-for-entire-schema-org-vocabulary-do-not-reinvent-wheel/">OpenAPI Definitions For Entire Schema.org Vocabulary (Do Not Reinvent Wheel)</a></h3>
			<p><em>02 Oct 2017</em></p>
			<p>I am preparing my Schema.org Github repo with a variety of data sources for use across my API tooling and other projects. I’m trying to get better at using a common vocabulary, and not reinventing the wheel each time I start a new project. Schema.org has the most robust vocabulary of shared schema available today–so I am using this existing work as the core of mine. I am slicing and dicing the schema.org vocabulary into several formats that I can use in my OpenAPI-driven editors, and other tooling. I took the JSON-LD representation for Schema.org, and published it as a simpler JSON schema definition format that can be applied quickly to an OpenAPI. It isn’t perfect, and you lose a lot of the semantics in the process, but I think it still provides an important base for API designers, architects, and developers to use across their OpenAPI. It is pretty verbose, with over 150K lines, but it provides a fairly consolidated view of Schema.org classes, in a single set of definitions: You can download a copy via the Gist, or you can find as JSON and YAML in my Github repository for this work. I’m going to be creating complete OpenAPI for each Schema.org class, as well as individual JSON schema files for each class. I just haven’t to figure out how to decouple them into individual files, yet containing all the relevant schema. I have the code, I just need to dial it in, when I have more time. I am going to use this Schema.org JSON schema as an autocomplete in my API design tooling, and using the OpenAPI as the source definition for my API deployment and testing tooling. I’ve been evolving my Human Services Data API work to easily generate server side code using OpenAPI, and I’m going to use the same code base to generate any Schema.org API, and deploy as AWS EC2 instance. I’m not looking to develop a...[<a href="/2017/10/02/openapi-definitions-for-entire-schema-org-vocabulary-do-not-reinvent-wheel/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/"><img src="https://s3.amazonaws.com/kinlane-productions2/hack-education/hack-education-data.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/">Learning About API Design With Resources That Matter To You</a></h3>
			<p><em>02 Oct 2017</em></p>
			<p>I have been helping my partner in crime Audrey Watters (@audreywatters) evolve her data work as part of her Columbia Spencer Education Journalism Fellowship, where she is publishing a wealth of ed-tech funding data to Github. I worked with her to evolve the schema she is using across the Google Sheet, and YAML data stores she is using. Something that will autogenerate APIs (well dynamic JSON) based upon the filename, and the fields she chooses as part of her data stores. I just planted the seeds, and she has been cranking away creating repos, and building data stores since this last summer. She mentioned to me recently that she thought she had been being consistent in her naming conventions across her work, but had recently noticed some inconsistencies–realizing the importance of a consistent design and schema across the projects, something that really could become problematic at scale if she hadn’t caught. Luckily she was able to fix with some work, and was back on track. She isn’t as automated in the replication of data across her projects, but that is a good thing. It is forcing her to think more deeply about the naming and overall design of her static data APIs, which she uses across many repos, and displayed in a variety of lists, outlines, and stories she is telling around her work. Audrey has spent seven years listening to me talk about API design blah blah blah, but until she was working with her own data, that she cared about, she didn’t fully grasp some of the API design and implications of working with the access, reusability, and maintenance of data at scale. I’ve offered to automate more of the maintenance, replication, and standardization of data across her repos, but she’s declined. She said she finds it valuable to work with the design, and naming of her data stores, for us in different projects. She likes keeping here YAML data stores in separate...[<a href="/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/29/tyk-is-conducting-api-surgery-meetups/"><img src="https://s3.amazonaws.com/kinlane-productions2/tyk/tyk-api-surgery-singapore.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/29/tyk-is-conducting-api-surgery-meetups/">Tyk Is Conducting API Surgery Meetups</a></h3>
			<p><em>29 Sep 2017</em></p>
			<p>I was having one of my regular calls with the Tyk team as part of our partnership, discussing what they are up to these days. I’m always looking to understand their road map, and see where I can discover any stories to tell about what they are up to. A part of their strategy to build awarness around their API management solution that I found was interesting, was the API Surgery event they held in Singapore last month, where they brought together API providers, developers, and architects to learn more about how Tyk can help them out in their operations. API surgery seems like an interesting evolution in the Meetup formula. They have a lot of the same elements as a regular Meetup like making sure there was pizza and drinks, but instead of presentations, they ask folks to bring their APIs along, and they walk them through setting up Tyk, and deliver an API management layer for their API operations. If they don’t have their own API, no problem. Tyk makes sure there are test APIs for them to use while learning about how things work. Helping them understand how to deliver API developer onboarding, documentation, authentication, rate limiting, monitoring, analytics, and the other features that Tyk delivers. They had about 12 people show up to the event, with a handful of business users, as well as some student developers. They even got a couple of new clients from the event. It seems like a good way to not beat around the bush about what an API service provider is wanting from event attendees, and getting down to the business at hand, learning how to secure and manage your API. I think the Meetup format still works for API providers, and service providers looking to reach an audience, but I like hearing about evolutions in the concept, and doing things that might bring out a different type of audience, and cut out some of...[<a href="/2017/09/29/tyk-is-conducting-api-surgery-meetups/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/29/the-api-stack-for-disrupting-the-world/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/fredericksburg-downtown-flag.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/29/the-api-stack-for-disrupting-the-world/">The API Stack For Disrupting The World</a></h3>
			<p><em>29 Sep 2017</em></p>
			<p>I know people don’t understand why I’m so obsessed with APIs. Sometimes I ask the same question. When I began in 2010, it was 75% about my belief in the good that APIs can do, and 25% about pushing back on the bad things being done with APIs. In 2017, it is 15% about the good, and 85% about pushing back on the bad things that APIs can do. API driven platforms are being used for some pretty shady things these days, and increasingly they are a force for disruption, and not about making the world a better place. With this in mind, I wanted to take a moment to highlight the API stack right now that is being used to disrupt the world around us. These are the APIs that have shifted the political landscape in the U.S., and are being used to replicate, automate, and scale this disruption around the world. Facebook - The network effect is what brings the troublemakers to Facebook.They are on pace to have 2 billion active users. Something that has the potential to create quite a network effect when sharing stories and links, and when you seed that, target it, and grow it using the Facebook advertising engine–it makes for an excellent engine for disruption. Twitter - Twitter is a different beast. Less of the mainstream population than Facebook enjoys, but still a sizable, and very public audience. You can use the Twitter engine to spin things up, get people sharing, do some of the same sharing of stories and links, seeding, targeting, and growing with advertising. Often times the viral nature will spread to Facebook on take on a life of its own. Reddit - Now Reddit is entirely just an organic engine for disseminating information, which makes it great for propaganda, everything fake, and stoking the haters. The network effect that is Reddit, works very, very well will Twitter and Facebook, making for a perfect storm...[<a href="/2017/09/29/the-api-stack-for-disrupting-the-world/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/29/developing-the-ability-to-repeat-the-same-api-stories-over-and-over/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/kinlane-whiteboard-api-bw-artsy.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/29/developing-the-ability-to-repeat-the-same-api-stories-over-and-over/">Developing The Ability To Repeat The Same API Stories Over And Over</a></h3>
			<p><em>29 Sep 2017</em></p>
			<p>After seven years of telling stories on API Evangelist I’ve had to repeat myself from time to time. Honestly, I repeat myself A LOT. Hopefully I do it in a way that some of you don’t notice, or at least you are good at filtering the stories you’ve already heard from your feed timeline. My primary target audience is the waves of new folks to the world of APIs I catch with the SEO net I’m casting and working on a daily basis. Secondarily, it is the API echo chamber, and folks who have been following me for a while. I try to write stories across the spectrum, speaking to the leading edge API conversations, as well as the 101 level, and everything in between. Ask anyone doing API evangelism, advocacy, training, outreach, and leadership–and they’ll that you have to repeat yourself a lot. It is something you get pretty sick of, and if you don’t find ways to make things interesting, and change things up, you will burn out. To help tell the same story over and over I’m always looking for a slightly different angle. Let’s take API Meetups as an example. Writing a story about conducting an API Meetup has been done. Overdone. To write a new story about it I’ll evaluate what is happening at the Meetup that is different, or maybe the company, or the speaker. Diving into the background of what they are doing looking for interesting things they’ve done. You have to find an angle to wrap the boring in something of value. API documentation is another topic I cover over, and over, and over. You can only talk about static or interactive API documentation so much. Then you move into the process behind. Maybe a list of other supporting elements like code samples, visualizations, or authentication. How was the onboarding process improved? How the open source solution behind it simplifies the process. You really have to work...[<a href="/2017/09/29/developing-the-ability-to-repeat-the-same-api-stories-over-and-over/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-graphql.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/">API Design Industry Guide: GraphQL, A Query Language For APIs</a></h3>
			<p><em>29 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. GraphQL is a query language designed by Facebook to build client applications using a flexible syntax and provide a system for describing the data requirements and interactions required by each application. GraphQL began as a Facebook project that soon began powering all their mobile applications. By 2015, became a formal specification. GraphQL provides a query language for your APIs that allows users to describe how they would like their API requests be fulfilled. The approach shifts the API design process to be more about request flexibility requiring API providers to design all API paths ahead of time. It opts for an augmented query language over investing in static schema that requires specific API paths. REST APIs focus on paths to your resources, but GraphQL is all about fields and data types, with everything accessed through a single API path. GraphQL does a better job of providing a more comprehensive approach access to data stored in a database by offloading design to the query layer for interpretation at query render time. The ability to define what data is returned opens up some interesting approaches to delivering resources, especially when it comes to potentially constrained network environments. When it comes to providing access to data used in responsive web and mobile applications, GraphQL can be successful in allowing application developers to get exactly what they need for an interface and nothing more....[<a href="/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/path-in-the-woods-black-white.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/">How Do We Help Folks Understand That APIs Are A Journey?</a></h3>
			<p><em>28 Sep 2017</em></p>
			<p>I was hanging out with my friend Mike Amundsen (@mamund) in Colorado last month and we ended up discussing folks uncertainty with APIs. You see, many folks that he has been talking to were extremely nervous about all the unknowns in the world of APIs, and were looking for more direction regarding what they should be doing (or not doing). Not all people thrive in a world of unknown unknowns, and not even in a world of known unknowns. Many just want a world of known knowns. This is something that makes the API landscape a very scary thing to some folk, and world where they will not thrive and be successful unless we can all begin to find a way to help them understand that this is all a journey. I love figuring all of this API stuff out, and I know Mike does too. We like thinking about the lofty concepts, as well as figuring out how to piece all the technical elements together in ways that work in a variety of business sectors. Many folks we are pushing APIs on aren’t like us, and just want to be told what to do. They just want the technology solution to their problem. A template. A working blueprint. It freaks them out to have so many options, possibilities, patterns, and directions they take things. I feel like we are setting folks up for failure when we talk them into embarking on an API journey without the proper training, equipment, support, and guidance. I think about the last seven years doing this, and how much I’ve learned. Realizing this makes me want to keep doing APIs, just so I can keep learning new things. I thought I understood REST when I started. I didn’t. I thought I understand the web when I started, I didn’t (still don’t). I was missing a lot of the basics, and no matter what folks told me, or how precise...[<a href="/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/28/apistrat-and-the-openapi-initiative/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-membership-september-2017.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/28/apistrat-and-the-openapi-initiative/">APIStrat And The OpenAPI Initiative</a></h3>
			<p><em>28 Sep 2017</em></p>
			<p>We are getting closer to APIStrat in Portland, Oregon, October 31st through November 2nd. So I’m going to keep crafting stories that help convince you should be there. It is the first APIStrat conference as an OpenAPI event, operated by the Linux Foundation events team. Steve and I are still playing a big part, and will be MC’ing, but like OpenAPI, APIStrat has grown to the point where we need to let it become more than just something Steve, myself, and the 3Scale team can execute by ourselves. APIStrat has always been a place where we gather and talk about OpenAPI, going back to when it was affectionately known as Swagger. Tony, and the team have spoken before, and there has been many other sessions, workshops, and keynotes involving the API specification format. This APIStrat is going to be no different, but there will be an even heavier presence for the specification. Since Tony Tam is stepping away, we are giving a full hour on mainstage for him and folks involved in the evolution of OpenAPI to share their story. Darrel Miller will be holding also be holding a workshop on the first day, where several folks involved in the OAI will be sharing knowledge. There will also be an OAI booth presence, and I know that Jeff ErnstFriedman will be present for OAI membership discussions. If your company is investing in OpenAPI as part of your API operations, and developing tooling around the specification, you should be considering joining the OAI. Take a look at the current membership list. I’m a member, and so are other heavy hitters like Adobe, Google, Microsoft, IBM, and even my partner in crime 3Scale, and Tyk are present. As a member you get in on the Slack channel conversations, participate on marketing and governance calls, and you get invited to participate on the APIStrat crew (if you want). Let me know if you are interested becoming a member,...[<a href="/2017/09/28/apistrat-and-the-openapi-initiative/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/28/api-design-industry-guide-the-restlet-platform-story/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-the-restlet-platform-story.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/28/api-design-industry-guide-the-restlet-platform-story/">API Design Industry Guide: The Restlet Platform Story</a></h3>
			<p><em>28 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. Restlet began as an open source Java API framework over a decade ago and has evolved into an API studio, client, and cloud platform with an API design core. At the center of the API lifecycle management platform is its API designer which gives you a visual view of an API and an OpenAPI or RAML view, providing a machine readable accounting of each API’s contract. The Restlet Studio allows you to design and document your APIs, starting from scratch, or import existing API design patterns using OpenAPI for RAML. Using the Restlet design UI you can shape the paths, parameters, headers and complete requests and responses for any API. Then, take the definition and actually put it to work in development, staging, or production environments. Restlet demonstrates how API design is more than just a momentary phase where you are developing APIs and is actively defining every stop along the API lifecycle from design to deprecation. While designing an API in the Restlet API Studio, you can also work to test and automate using the client, helping ensure a usable and complete API is designed. The Restlet Client provides a dashboard to verify the desired API contract in a way that can be shared across teams, with clients, and across stakeholders. Once the API design process has matured and evolved and is ready for deployment, Restlet empowers production deployment...[<a href="/2017/09/28/api-design-industry-guide-the-restlet-platform-story/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-grpc-open-source-rpc-framework.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/">API Design Industry Guide: gRPC, Open Source RPC Framework</a></h3>
			<p><em>28 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. gRPC is a high-performance open source remote procedure call (RPC) framework that is often used to deploy APIs across data centers that also supporting load balancing, tracing, health checks and authentication. While gRPC excels in more controlled, tightly coupled environments, it is also applicable for delivering resources to web, mobile, and other Internet connected devices. When crafting gRPC APIs, you begin by defining the service using Protocol Buffers, a language and toolset for binary serialization that has support across 10 leading programming languages. Protocol Buffers can be used to generate client and server stubs in these programming languages with tight API/client coupling — delivering a higher level of performance than your average REST API and SDK can. gRPC API design patterns takes advantage of HTTP/2 advances and uses authenticated bi-directional streaming to deliver APIs that can be scaled to millions of RPC calls per second. Its an effective approach for larger, more demanding API platforms that have begun to see the performance limitations of a more RESTful API design approach. gRPC is not ideal for every API implementation, but is definitely an approach providers should consider when high volumes anticipated, especially within the data center or other tightly controlled environment. Google has been using gRPC internally for over a decade now, but has recently committed to delivering all their public APIs using gRPC in addition to RESTful APIs, demonstrating that...[<a href="/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/thinking-beyond-just-distributed-api-scale-towards-federated-api-scale/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/losangelescloudy/dali_three/file-00_00_35_50.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/thinking-beyond-just-distributed-api-scale-towards-federated-api-scale/">Thinking Beyond Just Distributed API Scale Towards Federated API Scale</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>You hear a lot about doing APIs at scale in our space. Many folks dismiss web APIs because they feel they won’t scale, and aren’t performing at the scale they envision. The majority of these discussions focus on how do you scale large operations of Twitter, Facebook, or Google scope. A single organization operating API infrastructure at scale, distributed across many geographical regions, supporting millions of users. There are plenty of discussions going on regarding the technology, business, and politics of doing APIs at this scale. I find myself thinking in similar ways, but more federated version of this, where the latest technology might not always be the right answer. My Human Services Data API (HDSA) work is the best example I have of this. Where I’m having to keep the technology, and API definition bar as low as possible to onboard as many people as I possibly can, but then eventually, be able to aggregate large amounts of data across many federated instance. I have 3,144 counties, and 19,354 cities to consider. They should all be speaking a common schema when it comes to the sharing of human services data. Something that is easier said, than done. When you get on the ground you realize many of them are stuck in 1990s, or early 2000s edition of the web, and just do not have the resources needed to move things forward. They can’t afford the latest SaaS service, and they can’t drop the ball, or thousands, or millions of people will suffer–the stakes are high. When I go into large companies, who have a large teams, and significant number of resources, the conversation around scale is much different. Sure, there is distributed scale. Sure, there is volume scale. However, most times the distribution and volume exists within a single company or organization. A single command and control structure. However, I’m talking about federated distribution and volume, with no single command and control structure. I’m...[<a href="/2017/09/27/thinking-beyond-just-distributed-api-scale-towards-federated-api-scale/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/the-value-of-api-driven-events/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/hermosabeach/dark_dali/file-00_00_11_64.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/the-value-of-api-driven-events/">The Value of API Driven Events</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>I am spending a lot of time lately thinking about event sourcing, evented architecture, real time, and webhooks. I’m revisiting some of the existing aspects of how we move our bits around the Internet in real time and at scale as part of existing conversation I am having, as well as some projects I’m working on. I recently wrote about making sense of API activity with webhook events, and as I’m crafting a list of meaningful events for my Human Services Data API (HSDA) work, I’m thinking about how these events reflect the value that occurs via API platforms. As I’m going through the different APIs I’m exposing via a platform, I am working to identify and catalog events in which folks can subscribe to using webhooks. These are the events that occur, like adding a new organization, updating a service, or completing a batch import–all the things people will care about the most. These are the events and activities that occur because their is an API, which have the most value to API consumers, and platform operators. This is what actually matters, and why we are doing an API in the first place, to enable these events to occur. The more these events are triggered, and the more people we have subscribing and engaging with these events, the more value that is generated using an API. In aggregate, using modern approaches to API management, we might provide analytics and reports that demonstrate all this value being created, to justify the existence of our API. In some implementations, this value created is how we might be charging our API consumers, partners, and other stakeholders. However, in some cases we might even considering paying API consumers when these events occur, incentivizing a certain event-driven behavior that benefits the platform. It is easy to think of API value generation simply as the number of API calls, but I think webhooks has helped establish a new way to...[<a href="/2017/09/27/the-value-of-api-driven-events/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/caching-for-your-api-is-easier-than-you-think-and-something-you-should-invest/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/internet-gauages-3.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/caching-for-your-api-is-easier-than-you-think-and-something-you-should-invest/">Caching For Your API Is Easier Than You Think And Something You Should Invest</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>I’m encountering more API providers who have performance and scalability concerns with their APIs, who are making technical procurement decisions (gateways, proxies, etc) based upon these challenges, but have not invested any time or energy into planning and optimization of caching for their existing web servers that are delivering their APIs. Caching is another aspect of HTTP that I keep finding folks have little or no awareness of, and do not consider more investment in it to assist them in alleviating their scalability and performance concerns. There was a meeting I attended a couple weeks back where an API implementation was concerned about a new project for bulk loading and syncing of data between multiple external systems and their own, because of the strain it put on their database. Citing that they received millions of website, and API calls daily, they said they could not take the added load on their already strained systems during the day, limiting this type of activity to a narrow window at night. I began inquiring regarding caching practices in place on web, and API traffic, and they acknowledged that they new of no such activity or practices in place. This isn’t uncommon in my experiences, and I regularly encounter IT groups who just don’t have the time and HTTP awareness to implement any coherent strategy–this particular one just happened to admit it. My friends over at the API Academy have a great post on caching for RESTful and Hypermedia APIs, so I won’t be addressing the details of HTTP, and how you can optimize your APIs in this way. API caching isn’t an unproven technology, and it is a well known aspect of operating on the web, but it does take some investment and awareness. Like API design in general, you have to get to know the resources you are serving up, understand how your consumers are putting these resources to work, and adjust, dial-in, and tweak your caching...[<a href="/2017/09/27/caching-for-your-api-is-easier-than-you-think-and-something-you-should-invest/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/api-design-industry-guide-applicationlevel-profile-semantics-alps/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-api-stylebook.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/api-design-industry-guide-applicationlevel-profile-semantics-alps/">API Design Industry Guide: Application-Level Profile Semantics (ALPS)</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. Current API design focusses on using schema to help quantify the payload of the request and response structure of our APIs. JSON Schema, MSON, and other data specifications have emerged to help us quantify the bits we are passing back and forth with APIs. Alongside this evolution, another data format has emerged to help us define simple descriptions of our application-level semantics, similar to how we are using HTML microformats to share data on the web, Application-Level Profile Semantics (ALPS). ALPS goes well beyond schema, which provides a representation of a plan or theory in the form of an outline or model. ALPS provides a way to define the meaning behind the data, content, and other resources you are making available via an API. ALPS seeks to establish a shared understanding by illuminating the meaning behind hypermedia interfaces (data and state transitions) such as HTML, Collection+JSON, HAL or Siren. It encourages reusability of common profile documents across the media types we are depending on. Using ALPS you can easily define the common data elements we all use in our API like contacts, todo lists. It can even describe the structure of our APIS for verbose and more useful error responses. What really matters is that you can also define the transitions surrounding these data elements. You can get at the meaning and use behind them, like rolling dice, or playing...[<a href="/2017/09/27/api-design-industry-guide-applicationlevel-profile-semantics-alps/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-api-evangelist-flagged.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/">A Reminder To Always Have A Plan B For Our API Related Github Infrastructure</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>I had a scare this last weekend regarding my Github infrastructure. My Github organization for API Evangelist was flagged as SPAM and taken down. The Github organization contains almost 100 repositories that I use across my platform. These repositories drive the public side of my research, but also contain YAML files that are used in automation across my entire platform, and network of websites. At about 12:00 PM on Saturday, everything came to a screeching halt, with all the data I depend on to make things go around becoming unavailable. I have backups of all the data, and the website templates that produce the public side of API Evangelist. I also have a plan B in place for setting up a Jekyll instance that runs on Amazon EC2, but I hadn’t ever actually ran any drills on plan B. After submitting a ticket to Github, I got to work firing up the AWS EC2 instance, and unloading and unpacking the almost 100 website backups for my API Evangelist research. After getting things setup, and as I was preparing to switch over the DNS, I got an email from Github saying: Sorry for the hassle! It appears your organization had been caught up in a spam filter and was flagged incorrectly. I’ve cleared that flag now, so your account should be back to normal. You shouldn’t see that message again, but let me know if I can help with anything else! Crisis averted. Luckily this was just my own company Github organization. I operate numerous other API developer portals, code repositories, documentation sites, and other API related projects and tooling that lives entirely on Github. If my personal account was frozen, or any of these organizations taken offline, I would have been in a lot more hot water, and accountable to my clients. Overall I was down for a little over six hours. It showed me the fragile nature of depending on Github, not just for...[<a href="/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/why-does-aws-charge-by-usage-and-other-apis-still-use-plans/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_feed_people.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/why-does-aws-charge-by-usage-and-other-apis-still-use-plans/">Why Does AWS Charge By Usage And Other APIs Still Use Plans?</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>Amazon Web Services recently updated their billing for EC2 instances to be by the second, which I really like, because I’ll fire up an instance and run for minutes, then shut things down. I’m just looking to process patent downloads, or other intensive workload projects. Beyond just EC2, the rest of Amazon’s platform is still very usage based. Meaning, you get charged for whatever you use, with unit pricing for each resource designed to compliment how it gets put to use. You get charged for the hard costs of compute, storage, and bandwidth, but you also see per message, job, entry, and other types of billing depending on the type of resource being delivered via API. With this model for doing APIs, I’m wondering why so many API providers still have access plans and tiers. I’ve vented several times that I think service tiers are a legacy of a SaaS way of thinking and does not scale for API consumers. Maybe back when we used a handful of APIs, but the number of APIs I’m using is pushing 50 these days, and I can’t alway justify a monthly subscription to get what I need. I’m looking to just get access to valuable API resources, and get billed for whatever I use. If I don’t use anything for 6 months, I don’t get billed for anything. Also, I want to be able to run large jobs which consume intense amounts of resources without hitting tier and other limits–just charge me for what I use. If I have a $1,000.00 to spend today, let me spend it. Don’t make me jump through hoops. I know the answer to my question regarding why so many API startups do this. It is because the resources being provided via the API isn’t the product, us API consumers are. They are looking to ensure a certain level of headcount, monthly, and annual subscribers, so that they can sell us to their...[<a href="/2017/09/26/why-does-aws-charge-by-usage-and-other-apis-still-use-plans/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/keeping-the-web-api-layer-in-kafka-with-a-rest-proxy/"><img src="https://s3.amazonaws.com/kinlane-productions2/confluent/confluent-kafka-platform.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/keeping-the-web-api-layer-in-kafka-with-a-rest-proxy/">Keeping The Web API Layer In Kafka With A REST Proxy</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>I’m slowly learning more about Kafka, and the other messaging and data streaming solutions gaining traction in the API space. If you aren’t on the Kafka train yet, “Kafka is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.” I’m still learning about how Kafka works, and with no real production experience, it is something that is taking time. As part of my conversations on the subject, I was introduced to Confluent, a platform version of Kafka, which is the quickest way I have seen to get started with real-time data streams. As part of the Confluent offering I noticed they have a REST proxy, which you can find the API documentation here, and the code for the Kafka REST proxy on Github. According to the Github repo, “the Kafka REST Proxy provides a RESTful interface to a Kafka cluster. It makes it easy to produce and consume messages, view the state of the cluster, and perform administrative actions without using the native Kafka protocol or clients.” I’ve noticed that many of the other messaging and data streaming solutions out of Apache these days have diverted from using REST, which makes sense for speed, and scale, but when it comes to reaching a wider audience I can still see the need to have RESTful API. Delivering a kind of multi-speed solution that allows developers to pick their speed based upon their skills, awareness, and need. I’m feeling like the platform approach of Confluent, combined with a RESTful layer, will give them an advantage over other Kafa service providers, or just deploying the open source solution out of the box. REST isn’t always the most efficient, or scalable solution, but when it comes to reaching a wide audience of developers, and allowing consumers to get up and running quickly, REST is still a sensible approach. Honestly, I don’t think it is...[<a href="/2017/09/26/keeping-the-web-api-layer-in-kafka-with-a-rest-proxy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/concerns-around-working-with-the-api-evangelist-at-large-organizations/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/kin-chesapeake-sun_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/concerns-around-working-with-the-api-evangelist-at-large-organizations/">Concerns Around Working With The API Evangelist At Large Organizations</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>I know that I make some tech companies nervous. They see me as being unpredictable, with no guarantees regarding what I will say, in a world where the message should be tightly controlled. I feel it in the silence from many of the folks that are paying attention to me at large companies, and I’ve heard it specifically from some of my friends who aren’t concerned with telling me personally. These concerns keep them from working with me on storytelling projects, and prevent them from telling me stories about what is happening internally behind their firewall. It often doesn’t stop employees from telling me things off the record, but it does hinder official relationships, and on the record stories from being shared. I just want folks to know that I’m not in the scoop, or gotcha business. I only check-in on my page views monthly to help articulate where things are with my sponsors. I’m more than happy to keep conversations off the record, anonymize sources and topics. Even the folks in the space who have pissed me off do not get directly called out by me. Well, most of them. I’ve gone after Oracle a couple of times, but they are the worst of the worst. There are other startups and bigcos who I do not like, and you don’t ever hear me talking trash about them on my blog. Most of my rants are anonymized, generalized, and I take extra care to ensure no enterprise egos, careers, or brands are hurt in the making of API Evangelist. If you study my work, you’ll see that I talk regularly with federal government agencies, and large enterprise organizations weekly, and I never disclose things I shouldn’t be. If you find me unpredictable, I’m guessing you really haven’t been tuning into what I’ve been doing for very long, or your insecurities run deeper than anything to do with me. I’m not in the business of making...[<a href="/2017/09/26/concerns-around-working-with-the-api-evangelist-at-large-organizations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/apis-are-not-just-about-mobile-make-sure-you-are-considering-the-bigger-picture/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/space-suit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/apis-are-not-just-about-mobile-make-sure-you-are-considering-the-bigger-picture/">APIs Are Not Just About Mobile, Make Sure You Are Considering The Bigger Picture</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>When I started API Evangelist in 2010, API usage in mobile phones was the biggest factor contributing to me quitting my job, and becoming a independent voice for all APIs. I was being asked to deliver APIs to drive mobile applications on the iPhone, and while helping run technology for Google I/O I saw an increased need for resources to be delivered to this emerging platform. I knew that APIs were going to play an essential role in ensuring data, content, and algorithms could be put to use in mobile applications. Even with the importance of mobile, it wasn’t the only reason I knew that APIs were going to be important, which is something that still resonates today. In 2007, I saw the growing importance of social media APIs, and how messaging, images, and video were being made more distributed using APIs. Then in 2008, I saw that I could deliver global infrastructure using web APIs, demonstrating that web APIs weren’t a toy, and that you could operate a real business using web APIs. Then the whole mobile thing was just the tipping point, which demonstrated that the web was maturing beyond just websites, and it would be how we’d be doing business for some time to come. Every day I see people with blinders on focusing in on one slice of the API pie, seeing APIs as purely about commerce, social, cloud, mobile, IoT, messaging, or other growing aspect of the API economy. People are good at seeing things through the lens of their products, services, and industry. It is easy for them to ignore those people over there, or the other aspects of why leveraging the web is so important to all of this working. They get excited about a new open source solution, protocol, or pattern, and focus in exclusively on a single aspect of how we deliver technology–sometimes at the cost of other areas of their operations, or the future. If...[<a href="/2017/09/26/apis-are-not-just-about-mobile-make-sure-you-are-considering-the-bigger-picture/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/api-design-industry-guide-api-stylebook/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-api-stylebook.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/api-design-industry-guide-api-stylebook/">API Design Industry Guide: API Stylebook</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. Arnaud Lauret (@arno_di_loreto), the API Handyman (@apihandyman), has been developing an API Stylebook that provides a collection of resources for API designers. It is a brilliant aggregation of thirteen API design guides from Atlassian, Cisco, Cloud Foundry, Google, Haufe, Heroku, Microsoft, PayPal, Red Hat, The White House, and Zalando. It highlights best practices used by leading API providers. “The API Stylebook aims to help API Designers to solve API design matters and build their API design guidelines by providing quick and easy access to selected and categorized resources”, says Lauret. A unique community resource, it provides deep linking to specific topics within publicly available API design guidelines. Instead of reinventing the wheel or searching Google for hours, API designers quickly can find solutions and inspiration from these existing best practices. More than just a list of guidelines, it is a machine readable distillation of the thirteen API design guides into a master list of API design topics you can consider when crafting your own API design guide. It is slick. I like Arnaud’s approach to analyzing the existing API design patterns across the API platforms who have shared their guides. I also really like the YAML approach and it’s presented as a very good looking website using Github, and Github Pages. This is how API literacy tools should be constructed and it provides a valuable lesson in API design. You...[<a href="/2017/09/26/api-design-industry-guide-api-stylebook/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/talking-with-more-federal-agencies-about-api-micro-consulting/"><img src="https://s3.amazonaws.com/kinlane-productions2/skylight/services-infographic.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/talking-with-more-federal-agencies-about-api-micro-consulting/">Talking With More Federal Agencies About API Micro Consulting</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I have been having more conversations with federal agencies as part of my work with my Skylight partners about API related microconsulting. One recent conversation, which I won’t mention the agency, because I haven’t gotten approval, involved bug bounties on top of an API they are rolling out. The agency isn’t looking for the regular technology procurement lifecycle around this project, they are just looking for a little bit of research and consulting to help ensure they are on the right track when it comes to hardening their API approach. Micro consulting like this will usually not exceed $5,000.00 USD, and will always be a short term commitment. From my vantage point micro consulting will always be API related, and in this particular case involves studying how other API providers in the private sector are leveraging bug bounties to help harden their APIs either before they go public, or afterwards in an ongoing fashion. After I do the research I will be taking this work back to my team of consultants at Skylight, and we’ll put together formal report and presentation that we will bring back to the federal government agency to put into motion. This approach to doing APIs in the federal government (or any government) is a win-win. It fits with my approach to doing research at API Evangelist, and it provides API expertise for federal agencies in small, affordable, and bite-size chunks. Government agencies do not have to wait months, or years, and spend massive amounts of money to gain access to API expertise. For Skylight, it gets our foot in the door within government, and helps demonstrate the expertise we bring to the table. Something that will almost always turn into additional micro procurement relationships, as well as potentially larger scale, ongoing project relationships. Personally, I like my API consulting just like my APIs, small, and doing one thing well. I don’t like consulting contracts that try to do too much....[<a href="/2017/09/25/talking-with-more-federal-agencies-about-api-micro-consulting/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/providing-embeddable-api-integrations-for-nondevelopers-with-zapier/"><img src="https://s3.amazonaws.com/kinlane-productions2/zapier/zapier-embeddable-zaps.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/providing-embeddable-api-integrations-for-nondevelopers-with-zapier/">Providing Embeddable API Integrations For Non-Developers With Zapier</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I’m regularly working to make APIs more accessible to non-developers, and Zapier is the #1 way I do this. Zapier provides ready-to-go API integration recipes for over 750 APIs, providing IFTTT-like functionality, but in a way that actual pays the whole API thing forward (Zapier has APIs, IFTTT does not). One of the benefits of having APIs is you can build embeddable tooling on top of them, and Zapier has some basic embeddable tools available to anyone, with some more advanced options for partners via their partner API. Using the Zapier basic embeddable widget you can list one or many Zaps, providing recipes for any user to integrate with one or many APIs, that can be embedded into a web page, or within an application: &#x3C;script type="text/javascript" src="https://zapier.com/zapbook/embed/widget.js?guided_zaps=2618,1035,977"&#x3E;&#x3C;/script&#x3E; All you do is add the id for each of the Zaps you wish to list, under the “guided_zaps” variable, and it will list the icon, title, and “use this zap” functionality, all wrapped in the appropriate powered by Zapier branding. I’m developing lists of useful Zaps ranging from working with Google Sheets, to managing social media presence on Twitter and Facebook. Everyday, useful things that the average user might find valuable when it comes to automating, and taking control over their online presence. Anytime I reference possible API integration use cases in a story, I’m going to start embedding a widget of actual Zaps you can use to accomplish whatever I’m talking about below. I’m also trying to carve out time to develop some of my own Zaps, and sign up to become a Zapier partner, so I can begin to develop some more advanced editions of embeddable tooling. I want to make my own JavaScript library that will spider any text in a story, and turn references to API integration into popup tooltips, with API literacy, training, and action links. I have a handful of API 101 style solutions I’d like to see exist, but...[<a href="/2017/09/25/providing-embeddable-api-integrations-for-nondevelopers-with-zapier/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/latest-copy-of-the-api-evangelist-api-design-industry-guide/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-guide-2017-09-25.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/latest-copy-of-the-api-evangelist-api-design-industry-guide/">Latest Copy Of The API Evangelist API Design Industry Guide</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I’ve been struggling to get the latest edition of my industry guides out the door. I have a new Adobe Indesign format which I really like as a constraint, but is also pushing my desktop publishing skills. What is really kicking my ass though, is the editing. This latest copy was professionally edited, but I ran out of money to pay him on future guides, and I ended up making some slight changes to this one as well. I am very self-conscious of my grammar and spelling mistakes. I’m capable of editing my own stuff, and my grammar and spelling is high quality. The problem is that I’m too close to the content, and with each edit I make changes, which then introduce new mistakes. Also my brain moves too fast sometimes, and I just make silly mistakes, and overlook things by just reading it the way my brain intended. Anyways, I’m over stressing on it all. I just want to get my guides out. I have too much of a back log, and since I can afford a professional editor to shadow my work, I’m just going to put them out there. If you find mistakes, feel free to submit a Github issue on the repo for my API design research. I have too many guides to get out, and it is more important to me that my research moves forward, I spend the time distilling things down into a guide, and hitting publish. I can’t wait for perfect. If folks discount my work because I’m moving so fast, too bad. It is more important that the knowledge is in my head. If you want to help fund me so I can properly afford an editor, I welcome that as well–I have one who will work with me full time, I just need the cash! Anyways, I’m finally getting around to publishing this edition of the API design industry guide, which I hope provides...[<a href="/2017/09/25/latest-copy-of-the-api-evangelist-api-design-industry-guide/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/data-streaming-in-the-api-landscape/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/goldsilverfalls/creativity/file-00_00_40_84.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/data-streaming-in-the-api-landscape/">Data Streaming In The API Landscape</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I was taking a fresh look at my real time API research as part of some data streaming, and event sourcing conversations I was having last week. My research areas are never perfect, but I’d say that real time is still the best umbrella to think about some of the shifts we are seeing on the landscape recently. They are nothing new, but there has been renewed energy, new and interesting conversation going on, as well as some growing trends that I cannot ignore. To support my research, I took a day this week to dive in, have a conversation with my buddy Alex over at the TheNewStack.io, and the new CEO of WSO2 Tyler Jewell around what is happening. The way I approach my research is to always step back and look at what is happening already in the space, and I wanted to take another look at some of the real time API service providers I was already keeping eye on in the space: Pubnub - APIs for developers building secure realtime Mobile, Web, and IoT Apps. StreamData - Transform any API into a real-time data stream without a single line of server code. Fanout.io - Fanout’s reverse proxy helps you push data to connected devices instantly. Firebase - Store and sync data with our NoSQL cloud database. Data is synced across all clients in real time, and remains available when your app goes offline. Pusher - Leaders in real time technologies. We empower all developers to create live features for web and mobile apps with our simple hosted API. I’ve been tracking on what these providers have been doing for a while. They’ve all been pushing to boundaries of what is streaming, and real time APIs for some time. Another open source solution that I think is worth noting, which I believe some of the above services have leverages is Netty.io. Netty - Netty is an asynchronous event-driven network application framework for...[<a href="/2017/09/25/data-streaming-in-the-api-landscape/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/considering-the-future-of-the-openapi-initiative/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/OpenAPI_Pantone.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/considering-the-future-of-the-openapi-initiative/">Considering The Future Of The OpenAPI Initiative</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I’m a member of the OpenAPI Iniative (OAI). I’m not very active on the governance or marketing, but I enjoy hanging out in the hallways of the Slack channel, and being part of the conversation. I’m pretty confident in the core group’s ability to steer the direction of the specification, and leave my influence to be more about storytelling externally, and planting seeds in the minds of folks who are putting the API specification to use. I have a much different style to influencing the API space than many of the companies I share membership within the OAI–it is just my way. I am working with more groups to help them craft, maintain, and evangelize around a specific OpenAPI definition, for use in a specific industry. THe primary one on the table for me is the Human Services Data API (HSDA). Which is an OpenAPI for helping cities, municipalities, and non-profit organizations that help deliver information around human services, speak a common language. This is just one example of industry specific API definitions emerging. I am seeing OpenAPI emerge for PSD2, FHIR, helping guide the conversation going on in the financial and healthcare sectors. The OpenAPI as a top level API specification standard is maturing, and is something that reached version 3.0, and once the services and tooling catch up, we’ll see another boom in industry specific API definitions emerge. This is when we are going to see the need to start harmonizing, standardizing, and merging many disparate standards into a single specification, or at least interoperable specifications. You see this happening right now with OpenAPI, API Blueprint, and RAML–they are all part of the OpenAPI Initiative (OAI). In the next five years you will see this same thing begin occurring for other industry specific APIs, and we’ll eventually need governing bodies to help move forward these independent efforts, as well as feed needs back up the supply chain to OpenAPI. Maybe not right away,...[<a href="/2017/09/25/considering-the-future-of-the-openapi-initiative/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/22/not-everyone-needs-api-scale-some-just-need-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-yellow-house.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/22/not-everyone-needs-api-scale-some-just-need-api/">Not Everyone Needs API Scale, Some Just Need API</a></h3>
			<p><em>22 Sep 2017</em></p>
			<p>I know that catering to the enterprise is where the money is at. I know that playing with all the cool new containerized, event sourcing, continuously integrated and deployed solutions are where you can prove you know your stuff. However, in my world I come across so many companies, organizations, and government agencies that just need things to work. They don’t have the skills, resources, or time to play with everything cool, and really could just use some better access to their data and content across their business, with trusted partners, and maybe solicit the help of 3rd party developers to help carry the load. Many of the conversation I am having within startup and tech circles often focus on scale, and the latest tech. I get that this is the way things work in alpha tech circles, and this is applicable in your worlds, always moving forward, pushing the scope of what we are doing, and making sure we are playing with the latest tools is how it’s done. However, not everyone has this luxury, and many companies can’t afford to hire the talent needed, or pay the cost associated with doing things the most modern approach, or even the right way. Remember, when you are talking about Kafka, Kuburnetes, Docker, GraphQL, and other leading edge solutions, you are talking from a place of privilege. Meaning you probably have the time, resources, and space to implement the most modern approach, and have the team to do it right. I’m not trying to stop you from having fun, and doing what you do. I am just trying to share what I’m seeing on the ground at companies, organizations, and government agencies I’m talking to. I’m spending a lot of time trying to help get folks up to speed on everything I’m seeing, and many of them are intimidated by the pace at which things move, the scope of implementations they are reading about across tech...[<a href="/2017/09/22/not-everyone-needs-api-scale-some-just-need-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/22/i-am-a-professional-in-my-industry-where-should-i-begin-with-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/methuselah-mountain.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/22/i-am-a-professional-in-my-industry-where-should-i-begin-with-apis/">I Am A Professional In My Industry, Where Should I Begin With APIs?</a></h3>
			<p><em>22 Sep 2017</em></p>
			<p>A regularly question I get from business folks out in the space, is regarding where they should start with APIs. My world is usually broke into two areas: 1) Providing APIs, and 2) Consuming APIs. I’d say that these business folk I keep coming across could easily span both of these areas, making it significantly more complicated to help them understand where they should be getting started with APIs. With the API landscape being so wide, and APIs becoming so ubiquitous across many industries, helping someone onboard to the concept can get pretty complex and confusing pretty quick. I always try to prime the pump with my API 101 material, and encourage folks to learn about the history of APIs. I find that before you begin getting bombarded with the technical details of APIs it helps to get the lay of the land, and understand what is going on at the highest level, developing a better understanding of how we got here. Before you get working with any single API, you should try to understand why it has begun to be such a big part of everything we know of online today, and via our mobile phones. Most people don’t realize that they are using APIs everyday, as part of their regular business activity, and common things they do in their personal lives–things like buy products from Amazon, and sharing updates with friends on Facebook. Next, I recommend looking at the software you use each day in your work. If you are an architect, look a the CAD software you are using. If you are in healthcare, look at the administrative systems you use, and the devices you put to work. If you are retail, look at the point of sale (POS), and payment systems you use. All of these companies have APIs in one form or another. They might not all be public APIs like Facebook, Twitter, and Google, but they have APIs that...[<a href="/2017/09/22/i-am-a-professional-in-my-industry-where-should-i-begin-with-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/21/the-concept-of-api-management-has-expanded-so-much-the-concept-should-be/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-barn.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/21/the-concept-of-api-management-has-expanded-so-much-the-concept-should-be/">The Concept Of API Management Has Expanded So Much the Concept Should Be</a></h3>
			<p><em>21 Sep 2017</em></p>
			<p>API management was the first area of my research I started tracking on in 2010, and has been the seed for the 85+ areas of the API lifecycle I’m tracking on in 2017. It was a necessary vehicle for the API sector to move more mainstream, but in 2017 I’m feeling the concept is just too large, and the business of APIs has evolved enough that we should be focusing in on each aspect of API management on its own, and retire the concept entirely. I feel like at this point it will continue to confuse, and be abused, and that we can get more precise in what we are trying to accomplish, and better serve our customers along the way. The main concepts of API management at play have historically been about authentication, service composition, logging, analytics, and billing. There are plenty of other elements that have often been lumped in there like portal, documentation, support, and other aspects, but securing, tracking, and generating revenue from a variety of APIs, and consumers has been center stage. I’d say that some of the positive aspects of the maturing and evolution of API manage include more of a focus on authentication, as well as the awareness introduced by logging and analytics. I’d say some areas that worry me is that security discussions often stop with API management, and we don’t seem to be having evolved conversations around service conversation, billing, and monetization of our API resources. You rarely see these things discussed when we talk about GraphQL, gRPC, evented architecture, data streaming, and other hot topics in the API sector. I feel like the technology of APIs conversations have outpaced the business of APIs conversations as API management matured and moved forward. Advancements in logging, analytics, and reporting have definitely advanced, but understanding the value generated by providing different services to different consumers, seeing the cost associated with operations, and the value generated, then charging or...[<a href="/2017/09/21/the-concept-of-api-management-has-expanded-so-much-the-concept-should-be/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/21/that-point-where-api-session-management-become-api-surveillance/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/border-traffic.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/21/that-point-where-api-session-management-become-api-surveillance/">That Point Where API Session Management Become API Surveillance</a></h3>
			<p><em>21 Sep 2017</em></p>
			<p>I was talking to my friends TC2027 Computer and Information Security class at Tec de Monterrey via a Google hangout today, and one of the questions I got was around managing API sessions using JWT, which was spawned from a story about security JWT. A student was curious about managing session across API consumption, while addressing securing concerns, making sure tokens aren’t abused, and there isn’t API consumption from 3rd parties who shouldn’t have access going unnoticed. I feel like there are two important, and often competing interests occurring here. We want to secure our API resources, making sure data isn’t leaked, and prevent breaches. We want to make sure we know who is accessing resources, and develop a heightened awareness regarding who is accessing what, and how they are putting them to use. However, the more we march down the road of managing session, logging, analyzing, tracking, and securing our APIs, we are also simultaneously ramping up the surveillance of our platforms, and the web, mobile, network, and device clients who are putting our resources to use. Sure, we want to secure things, but we also want to think about the opportunity for abuse, as we are working to manage abuse on our platforms. To answer the question around how to track sessions across API operations I recommended thinking about that identification layer, which includes JWT and OAuth, depending on the situation. After that you should be looking other dimensions for identifying session like IP address, timestamps, user agent, and any other identifying characteristics. An app or user token is much more about identification, than it ever provides actual security, and to truly identify a valid session you should have more than one dimension beyond that key to acknowledge valid sessions, as well as just session in general. Identifying what healthy sessions look like, as well as unhealthy, or unique sessions that might be out of the realm of normal operations. To accomplish all...[<a href="/2017/09/21/that-point-where-api-session-management-become-api-surveillance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/21/i-am-not-a-card-carrying-restafarian-i-just-believe-in-the-web/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/statue-face-open-mouth_blue_circuit.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/21/i-am-not-a-card-carrying-restafarian-i-just-believe-in-the-web/">I Am Not A Card Carrying Restafarian I Just Believe In The Web</a></h3>
			<p><em>21 Sep 2017</em></p>
			<p>I am always surprised at the folks who I meet for the first time who automatically assume I’m all about the REST. It is always something that is more telling about the way they see the world (or don’t), than it ever is about me as THE API Evangelist. It is easy to think I’m going to get all RESTY, and start quoting Roy, but I’m no card carrying RESTafarian, like my buddy Darrel Miller (@darrel_miller) (not that is what Darrel does ;-). Really the only thing I get passionate about is making sure we are reusing the web, and I am pretty much be a sellout on almost everything else. I am just looking to understand how folks are exposing interfaces for their digital resources using the web, making them available for use in other applications. I feel like RESTful approaches are always a good start for folks to begin considering, and learning from when beginning their journey, but I’m rarely going to get all dogmatic about REST. There are trade-offs with any approach you take to providing programmatic interfaces using the web, and you should understand what these are whether your are using REST, Hypermedia, (g)RPC, GraphQL, or any other number of protocols and technologies available out there. A RESTful approach using the web just tends to be the lowest common denominator, the cheapest, and widest reaching solution we have on the table. Rarely is it ever the perfect solution–there are no such things. #sorry If you are entering into discussions with me thinking I’m 100% team REST, you are mistaken, and you have profiled yourself considerably for me. It shows me that you haven’t done a lot of (wide) reading on the subject of APIs, and while you may be an expert, you probably are a very siloed expert who doesn’t entertain a lot of outside opinions, and keep an eye on how the space is shifting and changing. When I encounter...[<a href="/2017/09/21/i-am-not-a-card-carrying-restafarian-i-just-believe-in-the-web/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/21/getting-beyond-openapi-being-about-api-documentation/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/yellowtree/clean_view/file-00_00_50_85.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/21/getting-beyond-openapi-being-about-api-documentation/">Getting Beyond OpenAPI Being About API Documentation</a></h3>
			<p><em>21 Sep 2017</em></p>
			<p>Darrel Miller has a thought provoking post on OpenAPI not being what he thought, shining a light on a very important dimension of what OpenAPI does, and doesn’t do in the API space. In my experience, OpenAPI is rarely what people think, and I want to revisit once slice of Darrel’s story, in regards to folks generally thinking OpenAPI (Swagger) as being all about API documentation. In 2017, the majority of folks I talk to think OpenAPI is about documenting your APIs–something that always makes me sad, but I get it, and is something I regularly work to combat this notion. First, and foremost, OpenAPI is a bridge to understanding and being able to communicate around using HTTP as a transport, and our greatest hope for helping developers learn their HTTPs and 123s. I meet developers on a regular basis who are building web APIs, yet do not have a firm grasp on what HTTP is. Hell, I’ve had a career dedicated to web APIs for the last seven years, and I’m still developing my grasp on what it is, learning new things from folks like Erik Wilde (@dret), Darrel Miller (@darrel_miller), and Mike Amundsen (@mamund) on a regular basis. In the API game, you should always be learning, and the web is the center of your existence at the moment as a software engineer, and should be the focus of what you are learning about to push forward your knowledge. Darrel has a great line in his post where he has “a higher chance of convincing developers to stop drinking Mountain Dew than to pry a documentation generator from the hands of a dev with a deadline.” Meaning, most developers don’t have the time or interest to learn about what OpenAPIs, or can do for them in their busy world, they just want the help delivering documentation–a very visual representation of the work they’ve done, and is something they can demonstrate to their boss,...[<a href="/2017/09/21/getting-beyond-openapi-being-about-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/20/what-were-the-main-api-developments-in-2017/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/amusement-park-2_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/20/what-were-the-main-api-developments-in-2017/">What Were The Main API Developments In 2017</a></h3>
			<p><em>20 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The main API development in 2017 has been the continued shift towards mainstream API adoption. The concept has been moving outside of the tech sector for a couple years now, but in 2017 it is very clear that it’s not just something startups are doing. This is having a profound shift in how we talk about APIs, and how we approach the API lifecycle. APIs have historically been something new and smaller companies are doing, but often will deliver at scale (AWS, SalesForce, eBay, etc.). The mainstream shift in the API sector brings a whole new set of challenges, and opportunities as existing companies, with existing technology in place, work to shift towards an API way of doing things. This shift impacts the technology of doing APIs, but really isn’t the main event–things will be business as usual when it comes to the technology of APIs for many years to come. I’d say the main event has been in the business of doing APIs. How these APIs get funded will be entirely different from how startup focused APIs get funded. This shift in financial incentives behind why APIs are developed, operated, and ultimately deprecated, will have profound effects on what is an API. They will have less of the startup shine, and become more robust, providing commercial, and industrial grade digital resources that are more mature than the newer, younger APIs we’ve seen in recent years. Alongside this shift, another development in the business of APIs has occurred. The funding landscape for startups has...[<a href="/2017/09/20/what-were-the-main-api-developments-in-2017/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/20/what-are-the-unsolved-problems-in-the-api-space/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/sand-hand_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/20/what-are-the-unsolved-problems-in-the-api-space/">What Are The Unsolved Problems In The API Space?</a></h3>
			<p><em>20 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. There are always endless numbers of fabricated unsolved problems in the API space. These are unsolved problems that are usually unsolved because they were just made up to get someone to buy a new service or product. They aren’t real problems. Technology is good at being applied to make believe problems, vendor fabricated problems, and solving real problems created by the last couple of waves of technology. I’d say that biggest unsolved problem is how APIs can actually solve the legacy technical debt challenges large companies, institutions, and government agencies face. There is a lot of rhetoric regarding how APis can unwind all of this, but honestly there are few examples of it in practice. With many API efforts in 2017, bogged down in cultural friction, and a web (pun intended) of technical complexity. One aspect of this problem of legacy technical debt is the problem of delivering technology in an Internet age, without actually embracing the web. People doing APIs don’t always have the knowledge of the web, and what makes it work before they get to work doing APIs. Vendors are offering up tools and services that provide web API solutions, but don’t always embrace the web, and the existing standards and protocols that make the web work. APIs are the next evolution of the web, and rely on many of the same concepts for it to work. When you ignore the web when doing APIs, you will always face challenges in interoperability, and reuse, and often end up building siloed solutions...[<a href="/2017/09/20/what-are-the-unsolved-problems-in-the-api-space/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/20/using-jekyll-as-a-hypermedia-client/"><img src="https://s3.amazonaws.com/kinlane-productions2/subway/hypermedia-siren-subway-stop.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/20/using-jekyll-as-a-hypermedia-client/">Using Jekyll As A Hypermedia Client</a></h3>
			<p><em>20 Sep 2017</em></p>
			<p>I am picking up some of my past work, so that I can move forward in a new way. A while ago, I began working on my subway map API to help me articulate aspects of the API lifecycle, and provide a “vehicle” for helping folks explore some often complex API concepts, in a way that would incrementally introduce them to new ideas. I used the subway map as an analogy because it has been historically used to help folks understand complex systems, and help them navigate it, even if they don’t fully understand everything about it. I gave a talk at @APIStrat in Austin, TX on this subject, but something I haven’t moved forward in over a year. My new approach to using the subway map model is still using hypermedia (Siren), but I’m not wanting a single API to control the data for every client. I’m looking to develop a static, federated approach to delivering subway map experience. I want to be able to quickly publish a common map, but then be able evolve them independently, designed for specific implementations and use cases. Since I’m so Jekyll and Github centered in how I deliver projects, I’m looking for a way to do this in a static way, that can be forked. So, I got to work on publishing Siren YAML to Github, and seeing if it is possible to use Liquid and HTML as the client. Again, I want this to be static. All this could easily be building this in JavaScript, but I want things static and forkable. For my proof of concept I published 15 “stops” along the request “line” for my API design “area”. I don’t have the visual elements present for this functionality, as I just wanted to prove that I could use Liquid and HTML for a hypermedia client, using Siren YAML published to Github. I was forced to add a layout: property to my Siren schema, which...[<a href="/2017/09/20/using-jekyll-as-a-hypermedia-client/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/20/looking-to-2024-what-do-apis-look-like/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/redes-fast-flux-623x425_blue_electricity.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/20/looking-to-2024-what-do-apis-look-like/">Looking To 2024, What Do APIs Look Like?</a></h3>
			<p><em>20 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. I’m not a big fan of predictions. It is a game that analysts and investors play to try and shape the world they want to see. I’m usually focused on shaping the world I want to see by understanding where we are, how we got here, and making incremental shifts in our behavior today. I tend to think that technology futurists are more about ignoring the past, and being in denial about today, than they are ever about what is happening in the future. However, with that said, let me share some thoughts about what I think the future will hold when it comes to doing APIs. Mostly, by 2024 things will look much like they do today. Nothing moves as fast we think they will in the tech sector. I’ve done a lot of studying the history of predictions by technology blogs, and analysts firms over the last decade about what they thought 2017 would be like, and it is 98% bullshit. It was more about getting what they wanted in the year they made the prediction, than it was ever about the future. Technology always feels like it is moving faster than ever before but honestly, what has happened in the last decade that is really seismic? I’d say iPhone is the biggest, with mostly everything else being pretty incremental, and slow moving. By 2024, we will still be struggling with technical debt. However, the debt limit ceiling will have been raised significantly. We won’t have decoupled the monolith, all while adding...[<a href="/2017/09/20/looking-to-2024-what-do-apis-look-like/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/19/who-are-the-most-influential-people-and-companies-to-keep-an-eye-on-in-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/19/who-are-the-most-influential-people-and-companies-to-keep-an-eye-on-in-api/">Who Are The Most Influential People And Companies To Keep An Eye On In API</a></h3>
			<p><em>19 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. When it comes to the most influential people and companies in the API space that I am keeping an eye on, it always starts with the API pioneers. This begins with SalesForce, eBay, and Amazon. Then it moves into the social realm with Twitter and Facebook. All of these providers are still moving and shaking the space when it comes to APIs, and operating viable API platforms that dominate in their sector. While I do not always agree with the direction these platforms are taking, they continue to provide a wealth of healthy, and bad practices we should all be considering as part of our own API operations, even if we aren’t doing it at a similar scale. Secondarily, I always recommend studying the cloud giants. Amazon is definitely the leader in this space, with their pioneering, first mover status, but Google is in a close second, and enjoys some API pioneering credentials with Google Maps, and other services in their stack. Even though Microsoft waiting so long to jump into the game I wouldn’t discount them from being an API mover and shaker with their Azure platform making all the right moves in the last couple of years as they played catch up. These three API providers are dictating much of what we know as being APIs in 2017, and will continue to do so in coming years. They will be leading the conversation, as well as sucking the oxygen out of other conversations they do not think are worthy. If you aren’t...[<a href="/2017/09/19/who-are-the-most-influential-people-and-companies-to-keep-an-eye-on-in-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/19/what-is-the-biggest-challenge-for-big-companies-doing-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/castle-wall-circuits.JPG" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/19/what-is-the-biggest-challenge-for-big-companies-doing-apis/">What Is The Biggest Challenge For Big Companies Doing APIs?</a></h3>
			<p><em>19 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The biggest challenge for big companies doing APIs is always about people and culture. Change is hard. Decoupling things at large companies is difficult. While APIs can operate at scale, they excel when they do one thing well, and aren’t burdened with the scope, and complexity of much of the software systems we see already operating within large companies. These large systems take large teams of people to operate, and shifting this culture, and displacing these people isn’t going to happen easily. People are naturally skeptical of new approaches, and get very defensive when it comes to their data, content, and other digital assets, as it can be seen as a threat to their livelihood–opening up and sharing these resources outside their sphere of influence. The culture that has been established at large companies won’t be easily undone. It is a culture that historically has had a pretty large gulf between business groups, and the IT groups who delivered the last generation of APIs (web services), that weren’t meant to be accessible, and understandable by business users. Web APIs have become simpler, more intuitive, and have the potential to be owned, consumed, and even in some cases deployed by business users. Even with this potential, many of the legacy rifts still exist, and business users feel this isn’t their domain, and IT and developer groups often feel APIs are something that should stay in their domain–perpetuating and confusing existing challenges already in place. While there may be small API success within large companies, they...[<a href="/2017/09/19/what-is-the-biggest-challenge-for-big-companies-doing-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/19/what-has-been-the-biggest-change-in-the-industry-since-i-started-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/beach-rocks-currents_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/19/what-has-been-the-biggest-change-in-the-industry-since-i-started-api/">What Has Been The Biggest Change In The Industry Since I Started API</a></h3>
			<p><em>19 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The biggest change in the industry since I started doing API Evangelist in 2010 is who is doing APIs. In 2010 it was 95% startups doing APIs, with a handful of enterprise, and small businesses doing them. I’d say over the last couple years the biggest change is that this had spread beyond the startup community and is something we see across companies, organizations, institutions, and government agencies of all shapes and sizes. Granted, there are a variety when it comes to the level they are doing them, and the quality, but APIs are something that has been moving mainstream over the last seven years, and becoming more commonplace in many different industries. In 2010 it was all about Twitter, Facebook, Amazon, and many of the API pioneers. This has been rapidly shifting to each wave of startups like Twilio, Stripe, Slack, and others. However, now in 2017 I am seeing insurance companies, airlines, car companies, universities, cities, and federal agencies with API programs. I mean, c’mon, Capital One has an API program (wink, wink). While I still hold influence with each wave of API service providers looking to sell to the space, and many of the API startup providers, my main audience is folks on the frontline of the enterprise, and government agencies at all levels. I also have a growing number of people at higher educational institutions tuning into what I’m writing as they look to evolve their approach to technology. APIs were mainly a startup thing in 2010, and in 2017...[<a href="/2017/09/19/what-has-been-the-biggest-change-in-the-industry-since-i-started-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/19/what-apis-excite-me-and-fuels-my-research-and-writing/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-gas-pumps.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/19/what-apis-excite-me-and-fuels-my-research-and-writing/">What APIs Excite Me And Fuels My Research And Writing</a></h3>
			<p><em>19 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The number API that gets me out of bed each day, with an opportunity to apply what I’ve learned in the API sector is with the Human Services Data API (HSDA). Which is an open API standard I am the technical lead for which helps municipalities, and human service organizations better share information that helps people find services in their communities. This begins with the basics like food, housing, and healthcare, but in recent months I’m seeing the standard get applied in disaster scenarios like Hurricane Irma to help organize shelter information. This is why I do APIs. The project is always struggling for funding, and is something I do mostly for free, with small paychecks whenever we receive grants, or find projects where we can deliver an actual API on the ground. Next, I’d say it is government APIs at the municipal, state, but mostly at the federal levels. I was a Presidential Innovation Fellow in the Obama administration, helping federal agencies publish their open data assets, take inventory of their web services. I don’t work for the government anymore, but it doesn’t mean the work hasn’t stopped. I’m regularly working on projects to ensure RFPs, and RFIs have appropriate API language in them, and talking with agencies about their API strategy, helping educate them what is going on in the private sector, and often times even across other government agencies. APIs like the new FOIA API, Recreational Information Database API, Regulations.gov, IRS APis, and others will have the biggest impact on our...[<a href="/2017/09/19/what-apis-excite-me-and-fuels-my-research-and-writing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/18/why-was-my-week-of-api-rants-so-well-received/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/36349140070_d5ec39cb34_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/18/why-was-my-week-of-api-rants-so-well-received/">Why Was My Week of API Rants So Well Received?</a></h3>
			<p><em>18 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. A couple weeks back I spent the entire week ranting on API Evangelist, instead of my usual lineup of API stories. Normally these types of stories end up on KinLane.com, or my rants edition, and usually don’t get tweeted out. I’m just venting. However on this particular week, I had enough people “piss in my cheerios”, that I felt the space needed to hear my rants, instead of the usual “nice guy” tone I tend to take on here. Granted, I can be pretty outspoken and blunt in my storytelling, but I usually work very hard to keep a professional tone, and be as nice as I possibly can. There are plenty of assholes in the API space, and I really don’t want to be one of them–even though it comes pretty naturally for me. ;-) I actually got tired of the tone by mid-week, but I had so many people Tweet, email, and post on LinkedIn, Facebook, and Slack that they were enjoying the rants, I kept it going until that Friday. I was moving to New York, and really didn’t have much time to do the normal amount of research it takes to write stories, and the rants were an easy want to get content up that took me about 10 minutes to write. So why did people like the posts? First, I have to say that not everyone did. I heard from a number of other people that thought I was being a diva, and found the tone offensive. I also...[<a href="/2017/09/18/why-was-my-week-of-api-rants-so-well-received/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/18/why-did-we-need-the-api-evangelist/"><img src="http://s3.amazonaws.com/kinlane-productions2/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/18/why-did-we-need-the-api-evangelist/">Why Did We Need The API Evangelist?</a></h3>
			<p><em>18 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. You needed the API Evangelist because there was nobody paying attention to the big picture of the API space. Sure, there are many vendors who pay attention to the big picture, and there are analysts who are paid to pay attention to the bigger picture to help validate the vendors, but there is nobody independent. At least there wasn’t in 2010 when I started. Now, there are a number of leading API experts and visionaries who work at different companies, and are able to maintain an independent view of the space, but in 2010 this did not exist. I’d like to think I helped make such a thing possible, but honestly it probably would have happened without me. Developer advocates, and evangelists tend to pay attention to a specific API, set of APIs, or API services and tooling. I pay attention to everything. I keep an eye on as many APIs as I possibly can, and work to evaluate all the services, tools, and technology that emerges on the landscape. I try to remain objective about what is working, and what is not, and share stories about both. I still have my biases, and tend to hold grudges against a few companies for their bad behavior, but for the most part I’m just trying to share an honest view of what is going on at the 100K view. Something that differs from the analysts, because I don’t have a vendor driven agenda, I’m just looking to understand. Another area that I benefit the space...[<a href="/2017/09/18/why-did-we-need-the-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/18/what-it-was-about-web-apis-that-first-captured-my-attention/"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/18/what-it-was-about-web-apis-that-first-captured-my-attention/">What It Was About Web APIs That First Captured My Attention?</a></h3>
			<p><em>18 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. In the spring of 2010 I was ready for a career shift. I was running North American event for SAP, and had also taken up running events for Google, which included Google I/O and Developer Days. I was the VP of Technology, and made all the decisions around usage of tech, from email blasts, to registration, session scanning, and follow-up reporting. When I took over the role I was dealing with a literal hostage colocation facility for server infrastructure, and massive hardware expenditure on servers that I didn’t need most of the year. Then in 2007 I began using the Amazon Cloud, and got to work re-engineering systems to be more API-centric, leverage AWS APIs to orchestrate my operations. By 2007 I had been playing around with web APIs for some time. I had incorporated payment and shipping APIs into commerce systems, and integrated Flickr, Delicious, Twitter, Facebook and other APIs into applications. I had plenty of SOAP web service experience when it came to enterprise infrastructure, but this was the first time I was deploying global infrastructure at scale using web APIs. I realized that web APIs weren’t just hobby toys, as my SAP IT director in Germany called them, they were an actual a tool I could use to operate a business at scale. My success resulted in more work, taking on more events, and scaling operations, which didn’t always pencil out to me actually being happier, even though the events scaled more efficiently, and out-performed what had come before. The two...[<a href="/2017/09/18/what-it-was-about-web-apis-that-first-captured-my-attention/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/18/what-is-the-role-of-an-influencer-in-the-api-industry/"><img src="http://i1.wp.com/restlet.dreamhosters.com/wp-content/uploads/2013/12/kinlane.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/18/what-is-the-role-of-an-influencer-in-the-api-industry/">What Is The Role Of An Influencer In The API Industry?</a></h3>
			<p><em>18 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The idea of an influencer in the API space will mean many things to many different people. I have pretty strong opinions about what an influencer should do, and it is always something that should be as free of product pitches as it possibly can. Influencing someone in the API space should mean that you are not just influencing their decision to buy your product or service. That is sales, which has it’s place, but we are talking about influencing. I would also add that influencing SHOULD NOT be steeped in convincing folks regarding what they should invest in, at the technology purchasing level, all the way up to the venture capital level. The role of an influencer in the API industry should always be about education, awareness, and helping influence how average flks get everyday problems solved. Being an influencer always begins with listening and learning. We are not broadcasting or pitching, we want to influence, so we need to have an idea about who we are influencing, and what will resonate and help them solve the problems they face. I do a significant portion of this by reading blogs, tuning into Twitter, and spending time on Github understanding what folks are building. Next, I engage in conversations with folks who are doing APIs, looking to understand APIs, and listening to what their challenges are, and what matters to them. At this stage I am not influencing anyone. I am being influenced. I’m absorbing what is going on, educating myself about what the...[<a href="/2017/09/18/what-is-the-role-of-an-influencer-in-the-api-industry/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/15/sensibly-thinking-about-where-technology-ends-and-the-human-part-begins-with/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/mosaic-face_blue_circuit_3.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/15/sensibly-thinking-about-where-technology-ends-and-the-human-part-begins-with/">Sensibly Thinking About Where Technology Ends And The Human Part Begins With</a></h3>
			<p><em>15 Sep 2017</em></p>
			<p>Our team at a hackathon I’m participating in this week is working on a data aggregation tool for helping merge multiple hurrican shelter data sources from Irma in Florida. While the need for the data is winding down, the use case for the tool could be something that lives on, and could help communities in the future. This projects aggregates multiple data sources for shelters from FEMA, municipal, and sources pulled together by volunteers. Our team is focused on aggregating, and doing as much heavy lifting to automatically merge and cleanse the data as they can, but then at the right moment render it for humans to step in and finish the work. I was impressed with the balance struck by the team. Knowing where to apply technology, and when to rely on humans. The problem of merging open data from multiple sources is a big and complex one. It is one that I’ve seen many technologists think they can step up and solve simply with their tech toolbox, no humans necessary. Our team quickly saw the scope of the program, discussed at length about what they could accomplish, and what they couldn’t accomplish, then got to work in the code to deliver the functionality, but then also developed a web interface to allow humans to step in at just the right point. Striking a balance between the human and technological aspects of doing this–which is what the Human Services Data Specification (HSDS) is all about. I know there is a significant amount of information out there about User Experience (UX), and also increasingly Developer Experience (DX). However, I think skills to know where to apply technology, and when to step back from using it, and focusing on augmenting, empowering, and putting the humans in charge are seriously deficient in our sector. I regularly encounter developers who think that technology is the solution, and humans are the problem. This contempt always degrades the amount of...[<a href="/2017/09/15/sensibly-thinking-about-where-technology-ends-and-the-human-part-begins-with/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/15/regulations-creeping-in-on-ai-ml-cognitive-and-other-fronts/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/supreme-court-judgement.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/15/regulations-creeping-in-on-ai-ml-cognitive-and-other-fronts/">Regulations Creeping In On AI, ML, Cognitive, And Other Fronts</a></h3>
			<p><em>15 Sep 2017</em></p>
			<p>I wrote an piece earlier today about not fearing AI, but possessing a significant amount of concern when it comes to the people behind. I figured I’d continue with the trend on this Friday afternoon, and talk about the coming regulations when it comes to artificial intelligence (AI), machine learning (ML), and everything cognitive, intelligent, and algorithmic. I am not fully a believer in regulations being the only solution, but I know they are the solutions that bigcos tend to pay attention to. Which is why they spend so much money to distort, and bend them to what they want to see in their industries. We are entering a phase of the Internet where there are going to an increased number of calls for regulations. Whether it’s privacy, security, breaches, or specifically on technology like drones, artificial intelligence, bots, and machine learning, expect more government involvement in the future. This isn’t because government is inheriently bad, and is looking to suffocate business, it is primarily because these areas of technology are being defined by the worst among us. When you bundle with the not so bad folks, and even many of the good folks refusing to reign in their industry partners, and fellow technologists, you end up with more regulations imposed to stablize things. If the tech space was more willing to step up and take the lead regarding acceptable practices, this wouldn’t be necessary. Algorithms are making more decisions in our lives. After seeing what Facebook and Twitter have done during the last US election, and seeing AI and ML continue being applied to important aspects of our lives, there will be more inquirires by the government, and calls for the government to step in. I know that platforms don’t want to be regulated, and with very libertarian stances in much of Silicon Valley, there is a significant undertow of anti-regulatory, and anti-government sentiment. However, if you believe in the wisdom of the crowds,...[<a href="/2017/09/15/regulations-creeping-in-on-ai-ml-cognitive-and-other-fronts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/15/i-will-see-you-at-apistrat-in-portland-this-november/"><img src="https://s3.amazonaws.com/kinlane-productions2/apistrat/apistrat-portland-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/15/i-will-see-you-at-apistrat-in-portland-this-november/">I Will See You At APIStrat In Portland This November</a></h3>
			<p><em>15 Sep 2017</em></p>
			<p>We are putting the finishing touches on the schedule for APIStrat in Portland, OR, October 31st through November 2nd. We have all the workshops, sessions, and keynotes dialed in (not all keynotes announced, wink, wink), and it is all just about making sure y’all show up and participate in the conversation. This is my 2nd favorite part of the event, the build-up for the big day(s). This is the 8th APIStrat we’ve done, and it is the first one we’ve done as part of the Linux Foundation, and with the OpenAPI Initiative. I’m excited. Make sure and take a look at the session schedule. We received over 165 submissions, and had a program committee of almost 30 people vote to decide with 60 would be accepted. I am the program chair and helped make some difficult decisions, but ultimately I”m pretty proud of the lineup we’ve pulled together. It’s much of the same popular topics as you’ve seen at previous events, with new faces, and brands, but there is also some of the leading edge conversations around serverless, gRPC, GraphQL. Of course, there is also going to be a lot of talk about OpenAPI, in workshops, sessions, and on the main stage. So check out the schedule if you haven’t, it’s pretty sweet lineup. I want to personally thank Microsoft, Stoplight, SmartBear, Postman, CapitalOne DevExchange, APIMATIC, Red Hat, Google, and Cloud Elements for sponsoring and making sure APIStrat happens. Of course, thank you to The Linux Foundation, and the OpenAPI Initiative (OAI) for taking the lead on APIStrat as it continues to grow and mature with the API community. I want to also thank my partner in crime, Steve Willmott, and the 3Scale / Red Hat team–without them APIStrat wouldn’t be a thing. Next up for me, now that the schedule is dialed in. Is to just tell stories about what will be happening. I’m going to go through each of the speakers, and companies...[<a href="/2017/09/15/i-will-see-you-at-apistrat-in-portland-this-november/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/15/i-do-not-fear-ai-i-fear-the-people-doing-ai/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/servers-blue-circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/15/i-do-not-fear-ai-i-fear-the-people-doing-ai/">I Do Not Fear AI, I Fear The People Doing AI</a></h3>
			<p><em>15 Sep 2017</em></p>
			<p>There is a lot of FUD out there when it comes to artificial intelligence (AI) and machine learning (ML). The tech press enjoy yanking people’s chain when it comes to the dangers of artificial intelligence. AI is coming for your jobs. AI is racist, sexist, and biased. AI will be lead to World War III. AI will secure and protect us from the bad out there. AI will be the source of all of our worries, and the solution to all of our worries. I’m interested in the storytelling around all of this, and I’m fascinated by the distracting quality of technology when it comes to absolving the humans behind of doing bad things. We have the technology to make this black boxes more observability and accountable. The algorithms feeding us news, judging us in courtrooms, and deciding if we are insurable or a risk, can all be wrapped with APIs, and made more accountable. However, there are many human reasons why we don’t do this. Every AI out there can be held accountable, it isn’t rocket science. The technology exists to keep AI from hurting us, judging us, and impacting our lives in negative ways. However, it is the people behind who do not want it, otherwise their stories won’t work. Their stories won’t have the desired effect and control over our lives. APIs are the layer being wielded for good and for bad on the Internet. Facebook, Twitter, and Reddit, all leverage APIs to be available on our mobile phones. APIs are how people automate, advertise, and fund their activities on their platforms. APIs are how AI and ML are being exposed, wielded, and leveraged. The technology is already there to make them more accountable, we just don’t have the human will to use the technology we have. There is more money to be made in telling wild stories about what is possible. Telling stories that make folks afraid, and in awe of...[<a href="/2017/09/15/i-do-not-fear-ai-i-fear-the-people-doing-ai/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/14/using-3rd-party-apis-to-break-you-out-of-your-enterprise-bubble/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-bubbles.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/14/using-3rd-party-apis-to-break-you-out-of-your-enterprise-bubble/">Using 3rd Party APIs To Break You Out Of Your Enterprise Bubble</a></h3>
			<p><em>14 Sep 2017</em></p>
			<p>I’m participating in a hackathon in Princeton, New Jersey as part of my work on the Human Services Data API (HSDA). We are at a large enterprise financial group’s office, as part of a three day social good hackathon / code sprint. Everybody participating is taking time off from their normal day job as back-end or front-end programmer, and business analyst, to build something for the greater good. Since it is an enterprise developer group the concept of a hackathon is somewhat new to them, and is the first time they’ve worked on external projects, instead of an internally focused hackathon event. I’m enjoying watching the two teams working on human services projects be forced out of their bubble. One of the projects has three separate 3rd party APIs to work with. 1) Simple spreadsheet deployed web API, 2) government agency published web API, and 3) HSDA API operated by a municipal organization. I am sitting here watching them get exposed to the variety of implementations, quality of data and interface, and wrestle with establishing their project requirements. After being pulled from their bubble trying to understand the APIs, they are also finding themselves pulled out of their local development world, having to potentially use 3rd party tools, services, and even reverse engineering a library or codebase in a language they are not familiar with. This is all very, very healthy. No matter what gets built at this hackathon, the fact that they are being pulled out of their bubbles, will benefit their world. They are thinking outside their governance bubble. They are forced to learn about the API best or worst practices of other organizations. They are having to use services, tools, and programming languages they aren’t familiar with. All with the motivation of potentially building something for good. They are exercising their skills and knowledge in ways that they won’t encounter in the routine, and highly structured worlds they exist in. Another layer...[<a href="/2017/09/14/using-3rd-party-apis-to-break-you-out-of-your-enterprise-bubble/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/14/lost-in-api-transit/"><img src="https://s3.amazonaws.com/kinlane-productions2/subway/mta-subway-map.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/14/lost-in-api-transit/">Lost In API Transit</a></h3>
			<p><em>14 Sep 2017</em></p>
			<p>I got on the New York Subway today heading for Penn Station to catch a train (New Jersey Transit) out to Princeton for a hackathon. As I was navigating my way through Metropolitan Transit Authority (MTA) and the New Jersey Transit I was thinking about my usage of API transit instead of API lifecycle. The number one response I had to this concept from readers was in regards the cognitive load experienced when you first look at a subway map that represents API infrastructure, and would anyone even know what I was talking about. It’s true, when you first look at any of the API subway maps I’ve created so far, you scratch your head to figure out what they mean. I haven’t spent a lot of time making them coherent, but I am also just getting going with the work. Truthfully, they’ll get more complicated, over getting simpler. However, each time I first use the subway in NYC there is also a pretty significant cognitive load. I’ve ridden the subway many times, but each time I still have to study the map, learn the portion I need to get what I need done, and accept that much of it I won’t actually ever understand. I usually only learn what applies to me, and the more time I spend riding a transit system, the more time it comes into focus–something that applies to any transit system in the world I’ve used. Think about when you start a new job, or adopt an existing legacy project as an API product manager. You do not immediately understand all the moving parts, absorb any diagrams, or documentation the first time you look at them. It takes time experiencing a system, before you will get acquainted, and become a local, like someone riding the MTA or NJT transit systems. Now that I live in NYC I’m going to spend time learning the transit system so I can get around,...[<a href="/2017/09/14/lost-in-api-transit/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/13/webhook-delivery-headers-from-github-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-circle-icon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/13/webhook-delivery-headers-from-github-api/">Webhook Delivery Headers From Github API</a></h3>
			<p><em>13 Sep 2017</em></p>
			<p>I am continuing my learning about Webhooks, and Github keeps my notebook full with interesting building blocks we can use when crafting our own webhook strategies. I’m not using everything I’m learning from Github in my current strategy, but I like adding each of these building blocks to my webhook research, so that I can use in future guides that I publish. Today’s post overlaps two areas of my research into webhooks, and how headers are being used by a variety of API providers. Github is using HTTP headers as part of the webhook response, providing the recipients of webhooks with more information about what is happening with each outgoing request. They are providing three custom headers along with each payload: X-GitHub-Event - Name of the event that triggered this delivery. X-Hub-Signature - HMAC hex digest of the payload, using the hook’s secret as the key (if configured). X-GitHub-Delivery - Unique ID for this delivery. In addition to these three custom headers, the User-Agent for the requests will have the prefix GitHub-Hookshot–so that your systems can identify these incoming requests more specifically. I like getting the name of the event, and definitely like the example of using the signature to make sure the payload hasn’t been tampered with, or from an untrustworthy source. Additionally you get a unique identifier for the delivery, allowing you to be able to record, and pull up unique webhook receipts. I’m adding these all as building blocks to my webhook research. I still have a notebook full of other Github, Stripe, Twilio, and leading approaches to webhooks. Once I get through this round I’m going to apply what I’ve learned to the project I’m working on, and then see about pushing out the first draft of my webhooks guide–something I’ve never done before. If nothing else, I’m learning a lot. I’m learning from all the leaders in the space, who are several versions into their webhook designs. I’m finding the...[<a href="/2017/09/13/webhook-delivery-headers-from-github-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/13/versioneye-sdk-security-notifications/"><img src="https://s3.amazonaws.com/kinlane-productions2/versioneye/versioneye-logo.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/13/versioneye-sdk-security-notifications/">VersionEye SDK Security Notifications</a></h3>
			<p><em>13 Sep 2017</em></p>
			<p>
I’ve written about VersionEye a couple of times. They help you monitor the 3rd party code you use, keeping an eye on dependencies, license violations, and security issues. I’ve written about the license portion of this equation, but they came up again while doing my API security research, and I wanted to make sure I revisited what they were up to in this aspect of the API lifecycle, floating them up on my radar.

VersionEye is keeping an eye on multiple security databases and helps you monitor the SDKs you are using in your application. Inversely, if you are an API provider generating SDKs for your API consumers to put to use, it seems like you should be proactively leverage VersionEye to help you be the eye on the security aspects of your SDK management. They even help developers within their existing CI/CD workflows, which is something that you should be considering as you plan, craft, and support your APIs. Making it as easy for you to leverage your APIs SDKs in your own workflow, and doing the same for your consumers, while also paying attention to security at each step, breaking your CI/CD process when security is breached.

I also wrote about how VersionEye has open sourced their APIs a while back, highlighting how you can also deploy into any environment you desire. I’m fascinated by the model VersionEye provides for the API space. They are offering valuable services that help us manage our crazy worlds, with a viable commercial and open source offering, that integrates with your existing CI/CD workflow. Next, I’m going to study the dependency portion of what VersionEye offer, then take some time to better understand their business model and pricing. VersionEye is pretty close to what I like to see in a service provider. They don’t have all the shine of a brand new startup, but they have all the important elements that really matter.

[<a href="/2017/09/13/versioneye-sdk-security-notifications/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/13/kubernetes-json-schema-extracted-from-openapi/"><img src="https://s3.amazonaws.com/kinlane-productions2/kubernetes/kubernetes-json-schema.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/13/kubernetes-json-schema-extracted-from-openapi/">Kubernetes JSON Schema Extracted From OpenAPI</a></h3>
			<p><em>13 Sep 2017</em></p>
			<p>I’ve been doing my regular trolling of Github lately, looking for anything interesting. I came across a repository this week that contained JSON Schema for Kubernetes. Something that is interesting by itself, but I also thought the fact that they had autogenerated the individual JSON Schema files from the Kubernetes OpenAPI was worth a story. It demonstrates for me, the growing importance of schema in all of this, and shows that having them readily available on Github is becoming more important for API providers and consumers. Creating schema is an important aspect of crafting an OpenAPI, but I find that many API providers, or the consumers who are creating OpenAPIs and publishing them to Github are not always investing the time into making sure the definitions, or schema portion of them are complete. Another aspect, as Gareth Rushgrove, the author of the Github repo where I found these Kubernetes schema points out, is the JSON Schema in OpenAPI often leaves much to be desired. Until version 3.0 it hasn’t supported everything you need, and many of the ways you are going to use these schema aren’t going to be able to use them in an OpenAPI, and you will need them as individual schema files like Gareth has done. I just published the latest version of the OpenAPI for my Human Services Data API (HSDA) work, and one of the things I’ve done is extracted the JSON Schema into separate files so I can use them in schema validation, and other services and tooling I will be using throughout the API lifecycle. I’ve setup an API that automatically extracts and generates them from the OpenAPI, but I’m also creating a Github repo that does this automatically for any OpenAPI I publish into the data folder for the Github repository. This way all I have to do is publish an OpenAPI, and there is automatically a page that tells me how complete or incomplete my schema...[<a href="/2017/09/13/kubernetes-json-schema-extracted-from-openapi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/13/a-sample-openapi-3-0-file-to-get-started/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/13/a-sample-openapi-3-0-file-to-get-started/">A Sample OpenAPI 3.0 File To Get Started</a></h3>
			<p><em>13 Sep 2017</em></p>
			<p>I am investing more time into my Schema.org work, alongside my learning about OpenAPI 3.0. I’m pretty excited about the components object, and I want to push forward some of my Schema.org dictionary ideas, to help folks get better at reusing common schema throughout their work. Schema.org is the most robust vocabulary out there, and we shouldn’t be reinventing the wheel in this area. I know the most important reason that folks aren’t using is that they either don’t know about it, or they are just lazy. I figure if I create some ready to go schema in an OpenAPI 3.0 components object, maybe people will be more inclined to put common schema to use. To share my components I need basic OpenAPI 3.0 shell to hold all my reusable schema. I really don’t care about the paths, and other elements being their. So I headed over to the OpenAPI 3.0 Github repo and borrowed the sample Petstore OpenAPI 3.0 my friend Darrel Miller created: I will change all the information in this sample to reflect my work, but I figured before I did I would share this example document with my readers. At first glance it doesn’t look much different than version 2.0 of OpenAPI, but once you start studying you see the differences. You see the responses have JSON specific content types inserted in between their schema references. There is also a components object, with a couple of schema present–this is all I need. There are a bunch of other things you can store in your components object, but I think this provides a nice first look at what is going on. If you are looking for some other working examples of OpenAPI 3.0 in action, head over to Mike Ralphson’s repository, he has some additional ones you can play with. I don’t know about you, but I learn from others. I need to reverse engineer API definitions from other people before I...[<a href="/2017/09/13/a-sample-openapi-3-0-file-to-get-started/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/12/the-us-postal-service-wakes-up-to-the-api-management-opportunity-in-new-audit/"><img src="https://s3.amazonaws.com/kinlane-productions2/usps/office-of-inspector-general-united-states-postal-service-api-audit-report.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/12/the-us-postal-service-wakes-up-to-the-api-management-opportunity-in-new-audit/">The US Postal Service Wakes Up To The API Management Opportunity In New Audit</a></h3>
			<p><em>12 Sep 2017</em></p>
			<p>The Office Of Inspector General for US Postal Service published an audit report on the federal agencies API strategy, which has opened their eyes to the potential of API management, and the direct value it can bring to their customers, and their business. The USPS has some extremely high value APIs that are baked into ecommerce solutions around the country, and have even launched an API management solution recently, but until now have not been actively analyzing and using API usage to guide them in any of their business planning decisions. According to the report, “The Postal Service captures customer API usage data and distributes it to stakeholders outside of the Web Tools team via spreadsheets every month. However, management is not using that data to plan for future API needs. This occurred because management did not agree on which group was responsible for reviewing and making decisions about captured usage data.” I’m sure this is common in other agencies, as APIs are often evolved within IT groups, that can have significant canyons between them and any business units. Data isn’t shared, unless a project specifically designates it to be shared, or leadership directs it, leaving real-time API management data out of reach of those business groups making decisions. It is good to see another federal agency wake up to the potential of API management, and the awareness it can bring to business groups. It’s not just some technical implementation with logfiles, it is actual business intelligence that can be used to guide the agency forward, and help an agency better serve constituents (customers). The awareness introduced by doing APIs, and then properly managing APIs, analyzing usage, and building and understanding what is happening, is a journey. It’s a journey that not all federal agencies have even begun (sadly). It is important that other agencies follow USPS lead, because it is likely you are already gathering valuable data, and just passing it on to external...[<a href="/2017/09/12/the-us-postal-service-wakes-up-to-the-api-management-opportunity-in-new-audit/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/12/openapi-3-0-tooling-discovery-on-github-and-social-media/"><img src="https://s3.amazonaws.com/kinlane-productions2/mike-ralphson/openapi_awesome1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/12/openapi-3-0-tooling-discovery-on-github-and-social-media/">OpenAPI 3.0 Tooling Discovery On Github And Social Media</a></h3>
			<p><em>12 Sep 2017</em></p>
			<p>I’ve been setting aside time to browse through and explore tagged projects on Github each week, learning about what is new and trending out there on the Githubz. It is a great way to explore what is being built, and what is getting traction with users. You have to wade through a lot of useless stuff, but when I come across the gems it is always worth it. I’ve been providing guidance to all my customers that they should be publishing their projects to Github, as well as tagging them coherently, so that they come up as part of tagged searches via the Github website, and the API (I do a lot of discovery via the API). When I am browsing API projects on Github I usually have a couple of orgs and users I tend to peek in on, and my friend Mike Ralphson (@PermittedSoc) is always one. Except, I usually don’t have to remember to peek in on Mike’s work, because he is really good at tagging his work, and building interesting projects, so his stuff is usually coming up as I’m browsing tags. He is the first repository I’ve come across that is organizing OpenAPI 3.0 tooling, and on his project he has some great advice for project owners: “Why not make your project discoverable by using the topic openapi3 on GitHub and using the hashtag #openapi3 on social media?” « Great advice Mike!! As I said, I regularly monitor Github tags, and I also monitor a variety of hashtags on Twitter for API chatter. If you aren’t tagging your projects, and Tweeting them out with appropriate hashtags, the likelihood they are going to be found decreases pretty significantly. This is how Mike will find your OpenAPI 3.0 tooling for inclusion in his catalog, and it is how I will find your project for inclusion in stories via API Evangelist. It’s a pretty basic thing, but it is one that I know many...[<a href="/2017/09/12/openapi-3-0-tooling-discovery-on-github-and-social-media/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/12/my-favorite-part-of-openapi-3-0-is-the-components-object/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-30-components-object.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/12/my-favorite-part-of-openapi-3-0-is-the-components-object/">My Favorite Part Of OpenAPI 3.0 Is The Components Object</a></h3>
			<p><em>12 Sep 2017</em></p>
			<p>There were a number of changes made to the structure of Open API in the move to version 3.0 that I am a fan of, but if I had to point at a single seismic shift that I think will move the conversation forward it is the components object. According to the specification the components object, “holds a set of reusable objects for different aspects of the OAS. All objects defined within the components object will have no effect on the API unless they are explicitly referenced from properties outside the components object.” It is the store for for all the common and reusable aspects of defining, and designing your APIs–which will have huge benefits on how we are doing all of this. Here is the laundry list of what you can put into your OpenAPI 3.0 components object, and reference throughout your API definitions: schemas - An object to hold reusable data schema used across your definitions. responses - An object to hold reusable responses, status codes, and their references. parameters - An object to hold reusable parameters you are using throughout your API requests. examples - An object to hold reusable the examples of requests and responses used in your design. requestBodies - An object to hold reusable the bodies that will be sent with your API request. headers - An object to hold reusable headers that define the HTTP structure of your requests. securitySchemes - An object to hold reusable security definitions that protect your API resources. links - An object to hold reusable links that get applied to API requests, moving it towards hypermedia. callbacks - An object to hold reusable callbacks that can be applied. I’ve written about how many API developers see this stuff as duplicate work across our APIs, where I see them as common, resusable patterns that we should be getting organized–the OpenAPI 3.0 components object is the beginning of us getting this house in order. The...[<a href="/2017/09/12/my-favorite-part-of-openapi-3-0-is-the-components-object/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/12/machine-readable-definitions-for-all-things-api-including-your-bots/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-robot-lightning-bold.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/12/machine-readable-definitions-for-all-things-api-including-your-bots/">Machine Readable Definitions For All Things API, Including Your Bots</a></h3>
			<p><em>12 Sep 2017</em></p>
			<p>Every aspect of my business runs as either YAML or JSON. This blog post is YAML stored on Github, viewed as HTML using Jekyll. All the companies, services, tooling, building blocks, patents, and other components of my research all live as YAML on Github. Any API I design is born, and lives as an OpenAPI YAML document on Github. Sure, much of this will be imported, exported, and exported with a variety of other tools, but the YAML and JSON definition is key to every stop along the life cycle of my business, and the work that I do. It isn’t just me. I’m seeing a big shift in how many platforms, services, and tooling operate, with often times YAML, and still in many situations it has JSON, XML, and CSV at its core. Everything you do should have some sort of schema definition, providing you with a template that you can reuse, share, collaborate, and communicate around. Platforms should allow for the creation of these template schema, and enable the exporting, and importing of them, opening up interoperability, and cross-platform functionality–much like APIs do in real-time using HTTP. This is what OpenAPI has done for the API lifecycle, and there should be many complementary, or even competing formats that accomplish the same, but for specific industries, and use cases. You can see this in action over at AWS, with the ability to export your Lex bot schema for use in your Alexa skill. Sure, this is interoperability on the same platform, but it does provide one example of how YAML and JSON definitions can help use share, reuse, and develop common templates for not just APIs, but also the clients, tooling, and other platforms we are engaging with. You’ll see this expand to every aspect of tech as continuous integration and deployment takes root, and Github continues it’s expansion beyond startups, into the enterprise, government, and other institutions. Along the way there will be...[<a href="/2017/09/12/machine-readable-definitions-for-all-things-api-including-your-bots/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/11/version-1-2-draft-of-the-human-services-data-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-referral/hsda-v1-2.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/11/version-1-2-draft-of-the-human-services-data-api/">Version 1.2 Draft Of The Human Services Data API</a></h3>
			<p><em>11 Sep 2017</em></p>
			<p>I have been working on the next version of the Human Services Data API (HSDA) OpenAPI lately, taking all the comments from the Github repository, and pushing forward the specification as far as I can with the minor v1.2 release. I have the Github issues organized by v1.2, and have invested time moving forward the OpenAPI for the project, as well as my demo site for the effort. With this release I am focusing on six main areas, based upon feedback from the group, and what makes sense to move forward without any non-breaking changes: /complete - add an /everything to each core resource, allowing access to all sub resouces. query - Shifting query parameter to be array, allowing for multiple fields to be queried. content negotiation - Allow for JSON, XML, and responses. sorting - Adding sorting. pagination - Adding pagination. status codes - Add more status codes. These were main concerns regarding what was missing from the last release, and were the top items that made sense to push forward this round. I’ve made some other major shifts to the project, but before I go through those, I wanted to provide some more insight into these v1.2 changes to the core HSDA specification. Helping shed some light on why I did what I did, while I am looking to make the API interface as usable as possible for HSDA implementations, vendors servicing the space, as well as developers looking to build web, mobile, voice, and other applications on top of any APIs that support the implementation. Complete Getting access to the entire surface area of the core resources (organizations, locations, and services), as well as all the subresources (phones, physical address, mailing address, etc.) was the most voiced request from v1.1. I had laid out several options to access the entire surface are of HSDA resources, but folks seem focused on a single set of API paths to accomplish what they needed from...[<a href="/2017/09/11/version-1-2-draft-of-the-human-services-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/11/making-sure-definitions-in-openapi-are-robust-for-use-in-schema-validation/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-referral/hsda-schema.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/11/making-sure-definitions-in-openapi-are-robust-for-use-in-schema-validation/">Making Sure Definitions In OpenAPI Are Robust For Use In Schema Validation</a></h3>
			<p><em>11 Sep 2017</em></p>
			<p>I’m working on v1.2 of my Human Sevices Data API (HSDA), and with this wave of work I’m making sue there is a functional API for validating all JSON that gets posted as the body in requests, as well as when it gets returned as part of API responses. To drive my validator I’m using JSON schema, which I already have defined as part of the OpenAPI definition for the project. I want to reuse, and build on top of this work, but I found the definitions for my OpenAPI to be pretty deficient in much of the details I am needing to validate the request and response bodies of my HSDA APIs. The process has showed me the importance of making sure the definitions portion of my OpenAPIs are as robust as I can. Possessing required, default, regex patterns, and other details I’m going to need to make sure my schema validator is as robust as possible. I’m entering the phase of this project where vendors, and implementors are looking for guidance on whether or not their schema are HSDS/A compliant, and they are supporting the fields necessary to get a stamp of approval. The schema validator is essential to this, but the new validation API I’ve created is only as good as the JSON schema that I’m using as part of its engine. I come across a number of OpenAPIs in the wild which do not possess schema definitions, and references for each API. These API providers are only describing enough of the surface area of their API to be able to generate API documentation using Swagger UI. This is something I’ve also been guilty of in the past, where I would only define the surface area of the API, just to get what I needed for my API discovery needs. Over the last year, I’ve spent more time making sure the definitions portion of the OpenAPI is also present, but it isn’t...[<a href="/2017/09/11/making-sure-definitions-in-openapi-are-robust-for-use-in-schema-validation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/11/api-education-is-needed-but-rarely-prioritized-in-the-current-environment/"><img src="https://s3.amazonaws.com/kinlane-productions2/facebook/facebook-blueprint-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/11/api-education-is-needed-but-rarely-prioritized-in-the-current-environment/">API Education Is Needed But Rarely Prioritized In The Current Environment</a></h3>
			<p><em>11 Sep 2017</em></p>
			<p>I wrote about this in a mean way during my rant week, but I wanted to bring up the topic of education and training when it comes to APIs in a more constructive way this week. Amidst the regular requests I get for API architects, developers, product managers, and evangelists I am reminding many companies that they will often need to hire for these roles internally, training and grooming existing employees, as finding seasoned veterans in any of these areas will prove to be difficult. I wish I had my own API school, where I was helping train waves of qualified employees, but sadly most of the folks with existing skills are employed. The challenge of investing in API training and education doesn’t stop with your immediate team, this is something that needs to occur in most cases company-wide. I’ve talk with several groups about developing internal workshops, and training, but I find most of them aren’t truly interested in the investment needed, and are often looking for some free content, or someone they can get to come and speak for free or very low pay. It shows me that many companies aren’t quite ready to make the investment it will take to ensure their staff are ready for the work that lies ahead, and don’t value making sure their workers have the skills they’ll need to be successful in the API-driven world we are finding ourselves in. This isn’t something I’ve just encountered at SMB, SME, and the enterprise. Government agencies are always cashed strapped, under-resourced, and lacking in the skills needed for the next wave. This is also a problem I’m seeing across startups. I’ve had discussions with startup groups selling tools and services to the API space, who are hitting significant challenges once they start selling their solutions outside the mainstream tech ecosystem. Many folks at large companies, small businesses, and government agencies just don’t have some of the basics when it...[<a href="/2017/09/11/api-education-is-needed-but-rarely-prioritized-in-the-current-environment/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/08/when-i-look-at-the-landscape-of-api-services-tooling-i-see-the-future-of/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/servers-hallway-door.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/08/when-i-look-at-the-landscape-of-api-services-tooling-i-see-the-future-of/">When I Look At The Landscape Of API Services & Tooling I See The Future Of</a></h3>
			<p><em>08 Sep 2017</em></p>
			<p>There are a number of API service and tooling providers that I still get excited about in the space. 3Scale, Restlet, Runscope, and Tyk - to begin with my sponsors! ;-) ;-) ;-) However, there are others like Postman, APIMATIC, Materia, OAuth.io, Stoplight, Apicurio, API Platform, API Umbrella, Github, API Science, and others that keep me thinking good thoughts about the things that API service providers are doing. However, I also see a lot of services and tooling that are simply playing the startup game, and have more to do with investment, then they do about APIs. It is these services and tools I see as the next generation of technical debt. When you bundle the vendors who are usually chasing trends as part of their investment and exit strategy, and really don’t care about truly helping you solve your technical, and business challenges, with your existing problems, you are just multiplying your problems. These types of customers only want you as an active customer, preferably locked into a contract, with their services and tools baked into your operations. You know what all of this leads to? Technical debt. When you buy into the vendor stories, and jump on trends, without thinking through the consequences of your actions, and the long term effects on your road map, you end up with a significant amount of technical debt down the road. I have taken a number of IT and developer leadership positions in my career, where I had to come in and clean up the mess from the previous guy (always guys). Nobody was questioning the decisions being made, and allowed someone to make purchasing, and technology decisions that ended up just taking things in a bad direction. That vendor we bought into was acquired, and now that tool we depend on is part of a larger enterprise suite we really don’t need, but because we can’t unwind it from our systems, we are forced to...[<a href="/2017/09/08/when-i-look-at-the-landscape-of-api-services-tooling-i-see-the-future-of/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/08/responding-to-a-webhook/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripe-using-webhooks.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/08/responding-to-a-webhook/">Responding To A Webhook</a></h3>
			<p><em>08 Sep 2017</em></p>
			<p>There are many details of doing APIs you don’t think about until you either a) gain the experience from doing APIs, or b) learn from the API providers already in the space. When you are just getting going with your API efforts you pretty much have to rely on b), unless you have the resources to hire a team with existing API experience. Which many of my readers will not have the luxury to do, so they need as much helping learning from the pioneers who came first, wherever they can. One of the API pioneers you should be learning from is the payment API provider Stripe. I’ve been studying their approach to webhooks lately, and I’ve managed to extract a number of interesting nuggets I will be sharing in separate blog posts. Today’s topic is responding to a webhook, which Stripe provides the following guidance: To acknowledge receipt of a webhook, your endpoint should return a 2xx HTTP status code. Any other information returned in the request headers or request body is ignored. All response codes outside this range, including 3xx codes, will indicate to Stripe that you did not receive the webhook. This does mean that a URL redirection or a “Not Modified” response will be treated as a failure. To be honest, I had never thought I should be responding to the webhooks I’ve setup. I treated them like a UDP request and once they went out the door and I processed, I didn’t need to response at all. How rude! I hadn’t seen any of my existing API providers offer up guidance in this area, or more likely I never noticed it. This is one of the reasons I like going though API providers documentation when I’m not integrating with them, because I tend to have a different eye for what is going on. Anyways, I’m adding webhook responses to my list of building blocks for my webhook research, and will...[<a href="/2017/09/08/responding-to-a-webhook/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/08/i-wish-i-had-time-to-tell-that-api-story/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/Millais_Boyhood_of_Raleigh.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/08/i-wish-i-had-time-to-tell-that-api-story/">I Wish I Had Time To Tell That API Story</a></h3>
			<p><em>08 Sep 2017</em></p>
			<p>If you have followed my work in the API space you know that I consider myself an API storyteller before I ever would an API evangelist, architect, or the other skills I bring to the table. Telling stories about what folks are up to in the space is the most important thing to me, and I feel it is the most common thing people stumble across, and end up associating with my brand. You hear me talk regularly about how important stories are, and how all of this API thing is only a thing, because of stories. Really, telling stories is the most important you should be doing if you are an API provider or API service provider, and something you need to be prioritizing. I was talking with a friend, and client the other day about their API operations, and after they told me a great story about the impact their APIs were making I said, “you should tell that story”! Which they responded, “I wish I had time to tell that story, but I don’t. My boss doesn’t prioritize me spending time on telling stories about what we are doing.” ;-( It just broke my heart. I get really, really busy during the week with phone calls, social media, and other project related activity. However, I always will stop what I’m doing and write 3-5 blog posts for API Evangelist about what I’m doing, and what I’m seeing. I know many of the stories are mundane and probably pretty boring, but they are exercise for me, of my ideas, my words, and how I communicate with other people. The way that enterprise groups and startups operate is something I’m very familiar with. I’ve been scolded by many bosses, and told not read or write on my blog. This is one of the reasons I don’t work in government anymore, or in the enterprise, as it would KILL ME to not be able to...[<a href="/2017/09/08/i-wish-i-had-time-to-tell-that-api-story/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/08/cloud-marketplace-becoming-the-new-wholesale-api-discovery-platform/"><img src="https://s3.amazonaws.com/kinlane-productions2/aws/aws-marketplace.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/08/cloud-marketplace-becoming-the-new-wholesale-api-discovery-platform/">Cloud Marketplace Becoming The New Wholesale API Discovery Platform</a></h3>
			<p><em>08 Sep 2017</em></p>
			<p>I’m keeping an eye on the AWS Marketplace, as well as what Azure and Google are up to, looking for growing signs of anything API. I’d have to say that, while Azure is in close second, that AWS is growing faster when it comes to the availability of APIs in their marketplace. What I find interesting about this growth is it isn’t just about the cloud, it is about wholesale APIs, and as it grows it quickly becomes about API discovery as well. The API conversation on AWS Marketplace has for a while been dominated by API service providers, and specifically the API management providers who have pioneered the space: 3Scale CA WSO2 Akana Strong Loop After management, we see some of the familiar faces from the API space doing API aggregation, database to API deployment, security, integration platform as a service (iPaaS), real time, logging, authentication, and monitoring with Runscope. Cloud Elements (Aggregation) SlashDB (Database) Runscope (Monitoring) Zapier (iPaaS) Peach API Security (Security) Streamdata (Real Time) Auth0 (Authentication) Okta (Authentication) LogEntries (Logging) All rounding off the API lifecycle, providing a growing number of tools that API provides can deploy into their existing AWS infrastructure to help manage API operations. This is how API providers should be operating, offering retail SaaS versions of their APIs, but also cloud deployable, wholesale versions of their offerings that run in any cloud, not just AWS. The portion of this aspect of API operations that is capturing my attention is the individual API providers are moving to offer their API up via AWS marketplace, moving things beyond just API service providers selling their tools to the space. Most notably are the API rockstars from the space: Stripe Twilio Sendgrid After these well known API providers there are a handful of other companies offering up wholesale editions of their APIs, so that potential customers can bake into their existing infrastructure, alongside their own APIs, or possibly other 3rd party APIs....[<a href="/2017/09/08/cloud-marketplace-becoming-the-new-wholesale-api-discovery-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/07/why-i-like-a-service-mindset-over-a-resource-focus-when-it-comes-to-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/human-services/human-services-docs-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/07/why-i-like-a-service-mindset-over-a-resource-focus-when-it-comes-to-apis/">Why I Like A Service Mindset Over A Resource Focus When It Comes To APIs</a></h3>
			<p><em>07 Sep 2017</em></p>
			<p>I am currently crafting a set of services as part of my Human Sevices Data API (API) work. The core set of services for organizations, locations, and services are grouped together as a single service, as this is what I was handed, but all the additional APIs I introduce will be bundled as separate set of individual services. Over the last couple of weeks I’ve introduced seven new services, with a handful more coming in the near future. I’m enjoying this way of focusing on services, over the legacy way that is very resource focused, as I feel like it lets me step back and look at the big picture. When I was defining the core API for this work I was very centered on the resources I was making available (organization, locations, and services), but once I took on a service mindset I began to see a number of things I was missing. With each service I find myself thinking about the full life cycle, not just the APIs that deliver the service. I’m thinking about the easy ones like design, deployment, and management, but I’m also thinking about monitoring, testing, and security. Then I’m delivering documentation, support, communications, and thinking about my monetization strategy, and access plans. I’m not just doing this once, I am thinking about it in the context of each individual service, as well as across all of them, taking care of the business of the services I’m delivering, not just the technical. While some folks I talk to look at some of this as repeat work across my projects, I just see them as common patterns, that I should be reusing, refining, and delivering in consistent ways. I’m thinking about delivering the technology in a consistent way, and the operational, but I’m beginning to think about education, training, and how I can help folks on the provider and consumer side of things learn how things are working. I’m not...[<a href="/2017/09/07/why-i-like-a-service-mindset-over-a-resource-focus-when-it-comes-to-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/07/automatically-generating-openapi-from-a-yaml-dataset-using-jekyll/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-dynamic.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/07/automatically-generating-openapi-from-a-yaml-dataset-using-jekyll/">Automatically Generating OpenAPI From A YAML Dataset Using Jekyll</a></h3>
			<p><em>07 Sep 2017</em></p>
			<p>I was brainstorming with Shelby Switzer (@switzerly) yesterday around potential projects for upcoming events we are attending, looking for interesting ideas we can push forward, and one of the ideas we settled in on, was automatically generating OpenAPIs from any open data set. We aren’t just looking for some code to do this, we are looking for a forkable, reusable way of doing this that anyone could potentially put to work making open data more accessible. It’s an interesting idea that I think could have legs, and compliment some of the existing projects I’m tackling, and would help folks make their open data more usable. To develop a proof of concept I took one of my existing projects for publishing an API integration page within the developer portal of API providers, and replaced the hand crafted OpenAPI with a dynamic one. The project is driven from a single YAML data file, which I manage and publish using Google Sheets, and already had a static API and OpenAPI documentation, making it a perfect proof of concept. As I said, the OpenAPI is currently static YAML, so I got to work making it dynamically driven from the YAML data store. The integrations.yaml data store has eight fields, which I hd published as four separate API paths, depending on which category each entry is in. I was able to assemble the OpenAPI using a handful of variables already in the config.yaml for the project, but the rest I was able to generate by mounting the integrations.yaml, dynamically identifying the fields and the field types, and then generating the API paths, and schema definitions needed in the OpenAPI. It’s totally hacky at the moment, and just a proof of concept, but it works. I’m using the dynamically generated OpenAPI to drive the Swagger UI documentation on the project. I’m not sure why I hadn’t thought of this before, but this is why I spend time hanging with smart folks...[<a href="/2017/09/07/automatically-generating-openapi-from-a-yaml-dataset-using-jekyll/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/07/an-openapi-contract-for-the-freedom-of-information/"><img src="https://s3.amazonaws.com/kinlane-productions2/foia/freedom-of-information-stamp.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/07/an-openapi-contract-for-the-freedom-of-information/">An OpenAPI Contract For The Freedom Of Information</a></h3>
			<p><em>07 Sep 2017</em></p>
			<p>Today’s stories are all based around my preparation for providing some feedback on the next edition of the FOIA.gov API. I have a call with the project team, and want to provide ongoing feedback, so I am loading the project up in my brain, and doing some writing on the topic. The first thing that I do when getting to know any API project, now matter where it is at in it’s lifecycle, is craft an OpenAPI, which will act as a central contract for discussions. Plus, there is no better way, short of integration, to get to know an API than crafting a complete (enough) OpenAPI definition. After looking through the FOIA recommendations for the project, I took the draft FOIA API specification and crafted this OpenAPI definition: The specification is just for a single path, that allows you to POST a FOIA request. I made sure I thought through the supporting schema that gets posted, flushing out using the definitions (JSON schema) portion of the OpenAPI. This helps me see all the moving parts, and connect the dots between the API request and response, complete with definition for three HTTP status codes (200,404,500)–just the basics. Now I can see the technical details of a FOIA request in my head, preparing me for my discussion with the project owners. After loading the technical details in my head, I always like to step back and think about the business, political, and ultimately human aspects of this. This is a Freedom of Information Act (FOIA) API, being used by U.S. citizens to request that information within the federal government be freed. That is pretty significant, and represents why I do API Evangelist. I enjoy helping ensure APIs like this exist, are usable, and become a reality. It is interesting to think of the importance of this OpenAPI contract, and the potential it will have to make information in government more accessible. Providing a potential blueprint that...[<a href="/2017/09/07/an-openapi-contract-for-the-freedom-of-information/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/07/all-federal-government-public-api-projects-should-begin-with-a-github-repo/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/state-2017/68747470733a2f2f662e636c6f75642e6769746875622e636f6d2f6173736574732f3238323735392f313333353931312f32386233656336362d333563302d313165332d386565362d3636323732623966343138362e706e67.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/07/all-federal-government-public-api-projects-should-begin-with-a-github-repo/">All Federal Government Public API Projects Should Begin With A Github Repo</a></h3>
			<p><em>07 Sep 2017</em></p>
			<p>I’m gearing up for a conversation about the next edition of the FOIA API, and in preparation I’ve created an OpenAPI definition to help guide the conversation, which I drafted based upon the specifications published to Github by the FOIA API team at 18F. This was after spending some time reading through the FOIA recommendations for the project, which is also published to Github. Having the project information available on Github, makes it easy for analysts like me to quickly get up to speed on what is going on, and provide valuable feedback to the team. In my opinion, EVERY government API should start with a Github repo flushing out the needs and requirements for the project, exactly like 18F is doing as part of their FOIA work. All the details of the project are there for not just the project team, but for external participants like myself. When it comes to engaging with folks like me, the API project team doesn’t have to do anything, except send me a link to the Github repository, and maybe point out some specifics, but if the README is complete, only the repo link is necessary. This opens up conversation around the project using Github Issues, which leaves a history of the discussions that are occurring throughout the project’s life cycle. Any newcomers can invest the time into digesting the documentation, discussion, and then begin to constructively add value to what is already happening. I know this type of transparent, observable project performance is hard for many folks in government. Hell, it is hard for 18F, and people like myself who do it regularly, by default. It takes a certain fortitude to do things out in the open like this, but this is precisely why you should be doing it. The process injects sunlight into ALL government projects by default. You know your work will be scrutinized from day one, all the way to delivery, so you tend...[<a href="/2017/09/07/all-federal-government-public-api-projects-should-begin-with-a-github-repo/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/06/when-to-build-or-depend-on-an-api-service-provider/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-heart-monitor.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/06/when-to-build-or-depend-on-an-api-service-provider/">When To Build Or Depend On An API Service Provider</a></h3>
			<p><em>06 Sep 2017</em></p>
			<p>I am at that all too familiar place with a project where I am having to decide whether I want to build what I need, or depend on an API service provider. As an engineer it is always easy to think you can just build what you need, but the more experience you have, you begin to realize this isn’t always the smartest move. I’m at that point with API monitoring. I have a growing number of endpoints that I need to make sure are alive and active, but I also see an endless road map of detailed requests when it comes to granularity of what “alive and active” actually means. At first I was just going to use my default cron job service to hit the base url and API paths defined in my OpenAPI for each project, checking for the expected HTTP status code. Then I thought I better start checking for a valid schema. Then I thought I better start checking for valid data. My API project is an open source solution, and I thought about each of my clients and implementations as me for testing and monitoring for their needs. Then I thought, no way!! I’m just going to use Runscope, and build in documentation and processes that each of my clients and implementations can also use Runscope to dial in monitoring and testing of their API on their own terms. Since all of my API projects is OpenAPI driven, and Runscope is an OpenAPI driven API service provider (as ALL should be), I can use this as the seed for setting up testing and monitoring. Not all of my API implementations will be using 100% of the microservices I’m defining, or 100% of the API paths available fo each of the microservices I’m defining. Each microservice has it’s core set of paths that deliver the service, but then I’m also bundling in database, server, DNS, logging and other microservice operational...[<a href="/2017/09/06/when-to-build-or-depend-on-an-api-service-provider/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/06/keeping-things-one-dimensional-to-go-from-api-to-spreadsheet-in-one-step/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-api-to-spreadsheet.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/06/keeping-things-one-dimensional-to-go-from-api-to-spreadsheet-in-one-step/">"Keeping Things One Dimensional To Go From API To Spreadsheet In One Step"</a></h3>
			<p><em>06 Sep 2017</em></p>
			<p>I have been working on the next version of my human services work, which provides a way for cities to make information about organizations, locations, and services available on the web. Part of the feedback from the community around what was missing from the last version, was the number of API calls you needed to make to get a complete representation of a resource, and its sub-resources, as each API response was one dimensional. An example would be that you could get a list of locations, but to get at the list of services you had to make a separate API call. This wasn’t a lapse in API design, it was a result of the schema being born out of a CSV format, and me working to stay true to the original design, and usage of the schema. In the latest version, I did release a handful of paths that provide a complete representation of each resource and it’s sub-resources. However, I have maintained the original one dimension representation of each resource and sub-resources, allowing me to offer an XML, JSON, as well as CSV representation for each API call. This allows API consumers to pull CSV lists of organizations, locations, services, and their sub-resources like address and phone lists. While not something that would be useful in all API implementations, I feel like the audience for municipal level human services data will benefit significantly being able to go from API to spreadsheet in a single step. All the GET paths for organizations, locations, and services are publicly available by default, not requiring authentication, making CSV data available via a single URL–something anyone can make happen. While weighing API design decisions as part of my Human Services Data API (HSDA) work I am having to consider not just the technical of how I should be doing this. I am also deeply considering how the API will be put to use, and who will be doing...[<a href="/2017/09/06/keeping-things-one-dimensional-to-go-from-api-to-spreadsheet-in-one-step/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/06/github-oauth-applications-as-a-blueprint/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-oauth-application.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/06/github-oauth-applications-as-a-blueprint/">Github OAuth Applications As A Blueprint</a></h3>
			<p><em>06 Sep 2017</em></p>
			<p>I was creating a very light-weight API management solution for one of my projects the other day, and I wanted to give my API consumers a quick and dirty way to begin making calls against the API. Most of the API paths are publicly available, but there were a handful of POST, PUT, and DELETE paths I didn’t want to just have open to the public. I didn’t feel like this situation warranted a full blown API management solution like Tyk or 3Scale, but if I could just let people authenticate with their existing Github account, it would suffice. This project has it’s own Github organization, with each of the APIs living as open source API repositories, so I just leveraged Github, and the ability to create Github OAuth applications to do what I needed. You can find OAuth applications under your Github organizational settings, and when you are creating it, all you really need is to give the application a name, description, and a home page and callback URL, then you are given a client id and secret you can use to authenticate individual users with their Github accounts. I didn’t even have to do the complete OAuth dance to get access to resources, or refresh tokens (may will soon), I was just able to implement a single page PHP script to accomplish what I needed for this version: I am wiring this script up to a Github login icon on my developer portal, and each API consumer will be routed to Github to authenticate, and then the page will handle the callback where I capture the valid Github OAuth token, and the login, name, email, and other basic Github information about the user. Right now the API is open to anyone who authenticates, but eventually I will be evaluating the maturity of the Github account, and limiting access based upon a variety of criteria (number of repos, account creation date, etc.). For now,...[<a href="/2017/09/06/github-oauth-applications-as-a-blueprint/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/06/azure-matching-aws-when-it-comes-to-serverless-storytelling/"><img src="https://s3.amazonaws.com/kinlane-productions2/azure/functions/azure-functions-thumbnail.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/06/azure-matching-aws-when-it-comes-to-serverless-storytelling/">Azure Matching AWS When It Comes To Serverless Storytelling</a></h3>
			<p><em>06 Sep 2017</em></p>
			<p>I consume a huge amount of blog and Twitter feeds each week. I evaluate the stories published by major tech blogs, cloud providers, and individual API providers. In my work there is a significant amount of duplicity in stories, mostly because of press release regurgitation, but one area I watch closely is the volume of stories coming out of major cloud computing providers around specific topics that are relevant to APIs. One of these topics I’m watching closely is the new area of serverless, and what type of stories each providers are putting out there. Amazon has long held the front runner position because AWS Lambda was the first major cloud provider to do serverless, coining the term, and dominating the conversation with their brand of API evangelism. However, in the last couple months I have to say that Microsoft is matching AWS when it comes to the storytelling coming out of Azure in the area of serverless and function as a service (FaaS). Amazon definitely has an organic lead in the conversation, but when it comes to the shear volume, and regular drumbeat of serverless stories Microsoft is keeping pace. After watching several months of sustained storytelling, it looks like they could even pass up Amazon in the near future. When you are down in the weeds you tend to not see how narratives spread across the space, and the power of this type of storytelling, but from my vantage point, it is how all the stories we tell at the ground level get seeded, and become reality. It isn’t something you can do overnight, and very few organizations have the resources, and staying power to make this type of storytelling a sustainable thing. I know that many startups and enterprise groups simply see this as content creation and syndication, but that is the quickest way to make your operations unsustainable. Nobody enjoys operating a content farm, and if nobody cares about the content...[<a href="/2017/09/06/azure-matching-aws-when-it-comes-to-serverless-storytelling/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/05/just-waiting-the-graphql-assault-out/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/solidies-planning-attack-blue-matrix.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/05/just-waiting-the-graphql-assault-out/">Just Waiting The GraphQL Assault Out</a></h3>
			<p><em>05 Sep 2017</em></p>
			<p>I was reading a story on GraphQL this weekend which I won’t be linking to or citing because that is what they want, and they do not deserve the attention, that was just (yet) another hating on REST post. As I’ve mentioned before, the GraphQL’s primary strength seems to be they have endless waves of bros who love to write blog posts hating on REST, and web APIs. This particular post shows it’s absurdity by stating that HTTP is just a bad idea, wait…uh what? Yeah, you know that thing we use for the entire web, apparently it’s just not a good idea when it comes to exchanging data. Ok, buddy. When it comes to GraphQL, I’m still watching, learning, and will continue evaluating it as a tool in my API toolbox, but when it comes to the argument of GraphQL vs. Web APIs I will just be waiting out the current assault as I did with all the other haters. The link data haters ran out of steam. The hypermedia haters ran out of steam. The GraphQL haters will also run out steam. All of these technologies are viable tools in our API toolbox, but NONE of them are THE solution. These assaults on “what came before” is just a very tired tactic in the toolbox of startups–you hire young men, give them some cash (which doesn’t last for long), get them all wound up, and let them loose talking trash on the space, selling your warez. GraphQL has many uses. It is not a replacement for web APIs. It is just one tool in our toolbox. If you are following the advice of any of these web API haters you will wake up in a couple of years with a significant amount of technical debt, and probably also be very busy chasing the next wave of technology be pushed by vendors. My advice is that all API providers learn about the web, gain...[<a href="/2017/09/05/just-waiting-the-graphql-assault-out/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/05/api-evangelist-is-a-performance/"><img src="http://s3.amazonaws.com/kinlane-productions2/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/05/api-evangelist-is-a-performance/">API Evangelist Is A Performance</a></h3>
			<p><em>05 Sep 2017</em></p>
			<p>I think I freaked a couple of folks out last week, so I wanted to take a moment and remind folks that API Evangelist is a performance. Sure, it is rooted in my personality, and I keep it as true to my view of the world of APIs as I can, but it is just a performance I do daily. When I sit down at the keyboard and research the world of APIs I am truly (mostly) interested in the technology, but when I craft the words you read here on the blog I am performing a dance that is meant to be interesting to the technology community in a way that draws them in, but then also gives them a swift kick in the pants when it comes to ethics of the technology, business, and politics of doing all of this. Sure, my personality shines through all of this, and I’m being genuine when I talk about my own battles with mental illness, and other things, but please remember API Evangelist is a performance. It is a performance that is meant to counteract the regular stream of fake news that comes out of the Silicon Valley funded technology machine. API Evangelist is a Contrabulist production, pushing back on the often exploitative nature of APIs. Not that APIs are exploitative, it is the people who are doing APIs are exploitative. Back in 2010, I saw that APIs were providing a peek behind the increasingly black box nature of web technology that was invading our lives through our mobile devices, and jumped at the opportunity to jam my foot in the door, even as the VC power brokers continue to look to look for ways to close this door. In 2011, I found my voice as the API Evangelist explaining APIs to the normals, making these often abstract things more accessible. Along the way, I also developed the tone of this voice pushing back on the...[<a href="/2017/09/05/api-evangelist-is-a-performance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/05/acknowledging-the-good-in-the-api-space/"><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/kin-lane-api-evangelist-cartoon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/05/acknowledging-the-good-in-the-api-space/">Acknowledging The Good In The API Space</a></h3>
			<p><em>05 Sep 2017</em></p>
			<p>With such a dark week of blog posts last week I wanted to make sure and start this week off with a brighter post, talking about the good I see in the API space. It can be easy to find than some of the darker things I talked about, but after seven years doing this I see enough good things going on in the API community, that I keep doing this performance I call API Evangelist. It can be easy to rant and rave about the bad, but I find it takes a lot of work to identify the good things going on in the cracks, as they rarely get the attention of the mainstream tech community propaganda engine. First, there are some really smart folks who truly care about human beings and are dedicated to the world of APIs. I do not know of any other layer of technology that sustains a community of people that is not just about startups and mindless moving forward of technology in every industry. I can use all of my fingers counting the folks who truly care about doing APIs, and making a meaningful impact with them. I have had the pleasure of working with these folks, and brining many of them together as part of my APIStrat conference, and regularly enjoy learning from them, reading their stories, and engaging with them on a regular basis as part of this API journey. Second, not all APIs are startup focused. I work with many API providers who are doing very interesting, non-startup, non-VC investment, and most importantly, non-exploitative API things. I regularly work with passionate folks doing APIs at all levels of government, making an impact on the environment, pushing for transparency in our legal system, helping provide human services, and truly making change in a meaningful way using web APIs. APIs are neither good, nor bad, or are they neutral, they are simply a reflection of their creators...[<a href="/2017/09/05/acknowledging-the-good-in-the-api-space/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/05/a-new-minimum-viable-documentationmvd-jekyll-template-for-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/launchany/minimum-viable-documentation-template+for-apis.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/05/a-new-minimum-viable-documentationmvd-jekyll-template-for-apis/">A New Minimum Viable Documentation(MVD) Jekyll Template For APIs</a></h3>
			<p><em>05 Sep 2017</em></p>
			<p>I am a big fan of Jekyll, the static content management system (CMS). All of API Evangelist runs as hundreds of little Jekyll driven Github repositories, in a sort of microservices concert, allowing me to orchestrate my research, data, and the stories I tell across all of my projects. I recommend that API providers launch their API portals using Jekyll, whether you choose to run on Github, or anywhere else using the light-weight portable solution. I have several Jekyll templates I use to to fork and turn into new API portals, providing me with a robust toolbox for making APIs more usable. My friend and collaborator James Higginbotham(@launchany) has launched a new minimum viable documentation (MVD) template for APIs, providing API provides with everything they need out of the gate when it comes to a presence for their API. The MVD solution provides you with a place for your getting started, workflows, code samples, reference material, with OpenAPI as the heartbeat–providing you with everything you need when it comes to API documentation. It all is an open source package available on Github, allowing any API provider to fork and quickly change the content and look and feel to match your needs. Which in my opinion, is the way ALL API documentation solutions should be. None of us should be re-inventing the wheel when it comes to our API portals, there are too many good examples out their to follow. I know that Jekyll is intimidating for many folks. I’m currently dealing with this on several fronts, but trust me when I say that Jekyll will become one of the most important tools in your API toolbox. It takes a bit to learn the structure of Jekyll, and get over some of the quirks of learning to program using Liquid, but once you do, it will open up a whole new world for you. It is much more than just a static content management system (CMS)....[<a href="/2017/09/05/a-new-minimum-viable-documentationmvd-jekyll-template-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/01/you-think-you-are-so-smart-you-did-not-conduct-any-due-diligence-before-launching/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/supreme-court-judgement.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/01/you-think-you-are-so-smart-you-did-not-conduct-any-due-diligence-before-launching/">You Think You Are So Smart You Did Not Conduct Any Due Diligence Before Launching</a></h3>
			<p><em>01 Sep 2017</em></p>
			<p>You know your API stuff. You know it so well, you don’t even need to look at other APIs. There is no reason to Google and look for other APIs because your stuff is that good. Your idea came to you in a flash, and you worked for an entire weekend to bring to life. Your a genius. Everyone has told you so. This stuff just comes to you, and as long as you are left alone, the magic just happens. If people just stay out of your way, do not burden you with outside influences, and unnecessary concerns, you will keep rolling out amazing APIs that everyone will love and need. You consume books, and digest endless blog posts and white papers recommended by your trusted network of friends. You don’t ever notice authorship. They don’t matter. It is all about feeding your mind, and you will decide whether it is worthy or not. You don’t save bookmarks for citations or attributions, once inside your brain ALL ideas becomes yours. If someone’s idea is dumb, you make sure an let them know, making sure they are aware of how they are substandard and beneath you. If your friends let you know your ideas are amazing, you let them know they are great too, and will be rewarded by being in your presence, and part of your team. That one chick that was hired last year made the mistake of blurting out in a meeting, “isn’t that the same thing as that startup that launched last month?”. She isn’t on the core team anymore. You did look at what she was talking about, and their API design is inferior, and the look of their site just turned you off–no need to continue. This is why you don’t conduct due diligence for your API projects. Why spend time looking at so many bad ideas? It takes away from your time to make the magic happen. Why...[<a href="/2017/09/01/you-think-you-are-so-smart-you-did-not-conduct-any-due-diligence-before-launching/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/01/you-have-no-api-imagination-creativity-or-sensibility/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/cyber-api-description-wars/mosaic-face_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/01/you-have-no-api-imagination-creativity-or-sensibility/">You Have No API Imagination, Creativity, Or Sensibility</a></h3>
			<p><em>01 Sep 2017</em></p>
			<p>I know you are used to people telling you that you are creative, and your ideas are great, but I’m here to tell you they aren’t. You lack any imagination, creativity, or sensibility when it comes to your APIs. Some of it is because you are personally lacking in these areas, part of it is because you have no diversity on your team, but it is mostly because you all are just doing this to make money. As creative as you think doing a startups is, they are really just about making money for your bosses, and investors–not a lot of imagination, creativity, or sensibility is required. You could invest the time to come up with good ideas for applications and stories on your blog, but you really don’t want to do the work, or even stand out in the group. It is much easier to just phone it in, follow the group, and let your bosses and the existing industry trends dictate what you do each day. If the business sector you operate within is doing it, you are doing it. If you see something funny online or at a conference you will do it. You have a handful of blogs you read each weekend, that you will rewrite the best posts from and publish on your own blog. Your Twitter account is just retweeting what you find, and you don’t even push out your own stories, because you have already tweeted out the story you copied in the first place. Don’t beat yourself up about this, you come by it honestly. Your privilege affords you never really getting out of your comfort zone, and the people around you make you feel good enough. Everyone on your team is the same, and your bosses really don’t care, as long as you are just creating content, and sending out all the required signals. Just make it look like you are always busy, and keep all...[<a href="/2017/09/01/you-have-no-api-imagination-creativity-or-sensibility/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page8" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page10" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
