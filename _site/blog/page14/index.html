<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/medium.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/02/01/a-limited-medium-api-means-i-do-not-always-curate-what-is-published-there/">A Limited Medium API Means I Do Not Always Curate What Is Published There</a></h3>
			<p><em>01 Feb 2017</em></p>
			<p>One of the deciding factors of whether or not I put a new online service to use in my business depends on whether or not they have an API. Sometimes I have no&nbsp;choice in the matter, but if I have any say, a service must allow me to move data in and out of their system programmatically, keeping in sync with my own systems, otherwise I will not adopt the service as part of my regular operations. One platform I have integrated into my operations is the blogging platform Medium. I handpick some of my content for publishing to Medium, pushing to my account from my own management system using their publishing API. This works well for me, as I require that all of my content, images, and videos are created and originate in my own systems, and then syndicate to other platforms secondarily via APIs--the only downside is that I can't actually pull data from Medium via their API into my management system. The lack of a read API for Medium content doesn't really hurt me, as I employ a POSSE philosophy, but it does hurt other publishers to the platform because I do not curate their content on Medium on a regular basis. I have to manually go to the Medium.com domain to read API related posts (unless it's a known blog RSS feed), which is something I do not always have the time for. I'm will always prioritize content from platforms that allow me to pull into my news curation systems via RSS, Atom, and APIs, and when I have time I will visit siloed platforms like Medium. I understand that Medium is concerned about control and monetization of their platform, but a more complete and thoughtful API strategy that involves read and write APIs, embeddable solutions, and considering other leading content API strategies would actually benefit them, as well as their publishers. Not doing this will actually hurt publishers like me....[<a href="/2017/02/01/a-limited-medium-api-means-i-do-not-always-curate-what-is-published-there/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-news-icon.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/31/the-importance-of-apis-in-journalism-right-now/">The Importance Of APIs In Journalism Right Now</a></h3>
			<p><em>31 Jan 2017</em></p>
			<p>
APIs are playing an increasing role in all aspects of our public life. Our current president has set the precedent that he will be using Twitter as a primary communication channel, cutting off traditional media and other channels--amplifying the importance of the Twitter API when it comes to doing your job as a journalist.
Journalists don't just need to be plugged into to major platform channels like Twitter, Facebook, Instagram, and others, they also need to be able to conduct research using these platform APIs. Journalists should be fluent in synchronous, and asynchronous pulling of social media and other data via leading APIs. Whether it's pulled through custom programming or using existing tools and services, successful journalists will have a robust toolbox for meeting their needs in this area.
Inversely it will be increasingly important that API platforms consider journalist access to APIs, and consider crafting specific service composition, rate limits, and plans for this level of access. Platforms should also be investing in and incentivizing their 3rd party developers to help develop tooling that aid journalists in their work. Ideally, these solutions would be open source&nbsp;or at least think deeply about monetization strategies in the context of access in support of these activities.
The current political climate should remind us in the API sector that APIs are not just about developing the next generation of startup applications, and are also about enabling end-users, as well as power users like journalists. If you need help thinking through journalist access to a platform, feel free to reach out, and I'm happy to help you think through this level of access from onboarding, to rate limits and the design of your APIs. This stuff is important, and I'm happy to help.
[<a href="/2017/01/31/the-importance-of-apis-in-journalism-right-now/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-amplify-audio.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/27/if-you-are-doing-interesting-things-with-apis-please-tell-the-story/">If You Are Doing Interesting Things With APIs Please Tell The Story</a></h3>
			<p><em>27 Jan 2017</em></p>
			<p>
I wish I could write about everything interesting that is going on in the API space, but as a one man show, I struggle to keep up with discovering, reading and understanding what is going on, let alone carving out the time to write something thoughtful about it. Many interesting things get added to my notebook, but I simply do not find the time to finish and publish a story--sadly.
I'm always appreciative of folks who email me ideas for stories based upon interesting things with APIs, but it still doesn't always mean I will find the bandwidth to craft a post. What I really enjoy is when folks ping me about something interesting, but they also write their own story on their own domain, and share via multiple channels like Twitter, LinkedIn, etc. When you do this, it allows me to more easily amplify what you are doing--the chances i will see it are greater.
Even when a company writes about something, I can still choose to augment and add to what has already been written about a subject, via my own blog. Then if I don't have time to craft something I can still amplify what has already been written by a company on Twitter, LinkedIn, and other channels I publish on regularly. I know many companies rely on PR channel to distribute their message, but I don't think this should be instead of publishing on their own blog. Telling stories on your companies blog, reaching out via PR channels, and relying on analysts like me to amplify what you publish makes for&nbsp;a more complete storytelling strategy for your API.
[<a href="/2017/01/27/if-you-are-doing-interesting-things-with-apis-please-tell-the-story/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_26_at_10.26.12_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/27/i-got-a-response-regarding-my-facebook-threat-api-access/">I Got A Response Regarding My Facebook Threat API Access</a></h3>
			<p><em>27 Jan 2017</em></p>
			<p>I am pushing forward my security research, and profiling what threat information APIs and platforms are up to. I rarely ever dive into any API without actually signing up for an API, getting some keys, and actually get to work making API calls. I have come across a number of APIs that are just fronts&nbsp;so that they can get in ProgrammableWeb directory, or just issue a press release that they have an API, so I usually prefer to fire things up and validate an API does what is being advertised. There is no better way to truly get to know an API than to actually make API calls against it and get to work doing some integration. While profiling the Facebook ThreatExchange API I did what I normally do--requested some keys. The platform doesn't allow for self-service access, so I had to wait for a response, which came as an email a couple days later: Hello Kin, Thank you for applying to ThreatExchange! Currently, we are in beta and focused on solving the challenges of companies with dedicated abuse detection or incident response teams seeking to share threat intelligence. At this moment, we don't feel the product is ready for your use-case, but we hope it will be in the near future and will reach back out once it is. We appreciate your patience and understanding. Please reach out to us if you feel this message is in error or if you have ideas on how we can best support your interest(s). Best, Facebook ThreatExchange Team I do not fault companies for not giving away instant self-service access to their API resources. There are plenty of badly behaved 3rd party developers out there. I do encourage them to try and consider other uses cases beyond just their partner implementations, and 3rd party developer integrations. Journalists, researchers, regulators, and analysts&nbsp;like me all need access&nbsp;and can bring a variety of benefits to the platform beyond integrations. When...[<a href="/2017/01/27/i-got-a-response-regarding-my-facebook-threat-api-access/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/API-plans-API-pricing-API-rating.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/27/funding-the-development-of-an-api-ranking-solution/">Funding The Development Of An API Ranking Solution</a></h3>
			<p><em>27 Jan 2017</em></p>
			<p>I have written before about how we are going to create the Standard and Poors or Moodys for the API industry, and how an API ratings could be used as an economic engine. This is a topic I have folks reach out to me about regularly, wanting to create such a rating system, for a variety of business and political reasons. It is something I'd like to continue to get ahead of before someone who is&nbsp;eviler than I am (I am pretty evil), decides to set something in motion that doesn't include me (ego). I may sound elitist saying this (I am), but there are very few people who get the API landscape at this level, understand the scope of this challenge, and can have a productive conversation about how to do this. Within 60 seconds of each Skype (yeah Skype)&nbsp;call I have with individuals about an API rating system I can tell which industry they come from&nbsp;and their understanding of what actually affect the quality of APIs--across the technical, business, and politics layers of operations. You have to have been in the space a while, and seen the positive and negative outcomes of API operations to really get the nuance of how you rank and rate API operations in a meaningful way. Developing an API rating and ranking system will take a lot of work, the involvement of the right folks, money, and time. This is an area that is floating higher up on my priority list and will be actively working to solidify&nbsp;a vision, before some of the folks I have talked with, ever get their way. The only existing model for this out in the open currently is from my friends at the API Rating Agency. They are some fo the handful&nbsp;of folks who I actually trust to execute on a vision of this importance and scale, but alas they ran into some of the common challenges around the scope of making...[<a href="/2017/01/27/funding-the-development-of-an-api-ranking-solution/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-cards-playing.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/25/what-is-at-stake-with-api-definitions-at-the-moment/">What Is At Stake With API Definitions At The Moment</a></h3>
			<p><em>25 Jan 2017</em></p>
			<p>I wrote angrily about Oracle's acquisition of Apiary last week, and this week I find myself deeply considering the API definition landscape, so I wanted to take another look at this event from the 100K view. In 2017, API definitions are touching every aspect of the API lifecycle, from design to deprecation,&nbsp;and are becoming key to defining, automating, and evolving many different industries from cloud computing to human services.&nbsp; I define API definitions as the specifications, schema, and scopes that are being used to map out the world of APIs. Specifications for describing APIs are nothing new, and approaches to defining data schema are well established as well. However by 2012 things were changing and Swagger emerged as an important tool for describing APIs in JSON, then YAML, using JSON Schema to define the underlying data definition. Around the same time, Apiary got to work on their own API Blueprint for describing APIs, and MSON for helping define the data and the relationships in play. Then, not to be left behind, Mulesoft "jumped" into the game with RAML as well--there are others, but this represents the top three. In 2016 Swagger was put into the Linux&nbsp;Foundation, under the&nbsp;Open API Initiative (OAI) and was reborn as the OpenAPI Spec, something that in 2017 is beginning to bear fruit with version 3.0 of the specification. Adding another dimension to this shift over the last year, we just saw Oracle purchase Apiary, and since Apiary is currently in the OAI, this means that Oracle is now in the OAI. I'm sure it is just a matter of time before Mulesoft also makes the move to join the OAI, but I'm not sure what this would mean for RAML--or what it means for API Blueprint. I do know OpenAPI Spec is the dominate format right now, and OAI is the leading group. I want to pause for a moment and also point out that Oracle is the company that...[<a href="/2017/01/25/what-is-at-stake-with-api-definitions-at-the-moment/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/openapi_spec_structural_improvements.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/25/the-openapi-specification-version-3-0-highlights/">The OpenAPI Specification Version 3.0 Highlights</a></h3>
			<p><em>25 Jan 2017</em></p>
			<p>I am impressed with the work that the Open API Initiative (OAI) working group has accomplished with the&nbsp;version 3.0 release of the OpenAPI Specification. I have had zero involvement in moving the specification forward (something I'm changing), and after coming back to the effort I am impressed with what they've prioritized, and accomplished for this release.&nbsp; The highlights in version 3.0 of the OpenAPI Spec for me are: Components - The new components architecture&nbsp;really reflects "APIs" in my opinion, making things modular and reusable. Body - Catching up when it comes to allowing the body to be defined separately from the headers&nbsp;and parameters. Content Negotiation - You can now define content objects to define the&nbsp;relationship between response objects, media types, and schema. Linking - It isn't hypermedia, but it is definitely a nod towards hypermedia, allowing the linking of objects. Webhooks - You can now define callbacks that can be attached to a subscription operation describing an outbound operation. Schema - Increased investment in JSON Schema, including support of `oneOf`, `anyOf` and `not` support, as well allowing for alternative&nbsp;schema now. Hosts - You can now have multiple hosts, allowing you to more narrowly define the host for each path. Examples - Allows you to better describe, and provide examples of APIs responses and requests. Version Identifier - Not a big one, but removing the swagger: "2.0" identifier--it will now just be&nbsp;openapi. Cookies - I'm not a big fan of this being introduced, but it makes sense, and I'm sure is usable for many API operators. The OAI blog provides a five-part series covering the version 3.0 release. These ten areas are the highlights for me. I think they nailed it as far as what was needed, while also pushing into areas like linking and webhooks that I hadn't anticipated. I am looking forward to playing with converting some of my 2.0 specs to be 3.0 compliant--once I am a little more intimate with it, I...[<a href="/2017/01/25/the-openapi-specification-version-3-0-highlights/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_24_at_11.36.10_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/25/pull-the-social-media-accounts-for-gov-using-the-us-digital-registry-api/">Pull The Social Media Accounts For Gov Using The US Digital Registry API</a></h3>
			<p><em>25 Jan 2017</em></p>
			<p>Over the holidays I pulled the data.gov index of federal government data, and the next item on my list was to cache the results of the&nbsp;US Digital Registry API&nbsp;, providing me with a list of agencies, and their social media accounts. I pulled the JSON from the API, and then published to the Github repository for this site, so that I could use for several different applications. Drive Listings Of Federal Agency Social MediaI wanted a quick way to get at the Twitter and Github accounts for the federal government, and have in a single location (Github). I've published YAML data to Github, and using Jekyll I've created listings for the Twitter, Github, Facebook, Pinterest, Instagram, and LinkedIn accounts, making them easier to browse when I need them. Accessible In Machine Readable Format For OthersI have several uses for the listing of government agencies and their social media accounts. I needed the data in a machine-readable format on Github so that I could pull remotely, and even check out the Github repository if I need to. This approach also makes the data available for anyone else to fork and put to use in their own way using Github. Profile The Agencies Using Each Of The Social APIs&nbsp;Now that I have the social media account for these federal gencies&nbsp;pulled using the US Digital Registry API, I'm going to take each of the accounts, and pull their profile details, and any other relevant information using the APIs for each of the social media services. APIs for the win--providing me with a way to quickly profile the US federal government at scale, and stay in tune either real-time or when I think a checkup is required. I'm not sure what I'll do next on this project. I'd like to take a look at how active each account is, and maybe check in each week to see if any accounts have gone dormant, picked up in activity, or possibly...[<a href="/2017/01/25/pull-the-social-media-accounts-for-gov-using-the-us-digital-registry-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_23_at_11.55.57_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/24/making-your-api-portal-speak-to-the-widest-possible-audience/">Making Your API Portal Speak To The Widest Possible Audience</a></h3>
			<p><em>24 Jan 2017</em></p>
			<p>I have the first draft of a developer portal&nbsp;ready for an API project I am working on, and before I move forward polishing it too much I wanted to step back and think about the goals behind the launch of this API portal, and the intended audience I'm targeting with its operation. I do not want this landing page to just speak to developers, I would like it to speak to as wide of an audience as possible.&nbsp; This particular API portal is meant to support the development of human service websites, mobile, and other applications in the Miami area. I do not want to scare off the "normals" with the home page. I want to make sure the language I use speaks to API newbies, while also giving experienced developers what they need to get up and running&nbsp;and solve any problems. While not all APIs will be targeting this wide of an audience, but when it comes to human services at the municipal level, we need to be reaching as many people as we can. I am looking for the developer area to speak to journalists, policy makers, analysts, interns, and even just data curious citizen activists. I think I have all the essential building blocks of an API portal present, things like getting started, documentation, and support, now I just need to polish the titles, text, and other elements so that they are as plain English&nbsp;as they possibly can. As I do this I shouldn't be assuming that a user will ever be actually integrating with an API, they might just need to understand the concept, what is possible with the API, and then engage with someone else who will actually be doing the technical heavy lifting. I'm going to have to put down the technical work on this project for a day or so&nbsp;before I can write content that speaks to non-developers--I am just too close to the code right now. I...[<a href="/2017/01/24/making-your-api-portal-speak-to-the-widest-possible-audience/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_database_relationships.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/24/loss-of-primary-and-foreign-keys-translating-from-data-package-to-openapi-spec/">Loss Of Primary and Foreign Keys Translating From Data Package to OpenAPI Spec</a></h3>
			<p><em>24 Jan 2017</em></p>
			<p>I am keeping a version of an OpenAPI Spec in sync with a Data Package. It's not a perfect sync because the Data Package doesn't describe the surface area of the API, just the underlying data schema used on the backend. During project discussions, one of the concerns that was brought up focused on the loss of primary and foreign key references. During our next discussion, I wanted to have a more coherent explanation of why this was ok, and this post will help me do that. The OpenAPI Spec I've created has each resource in the Data Package&nbsp;represented but leaves out the database relationships represented by those keys in the backend. The API defines the basic CRUD (Create, Read, Update and Delete) for each resource represented, but allows the relationships&nbsp;to be expressed using the URI. Each item in the Data Package has a corresponding path, and each relationship is available as its own path as well--in this case an example might be /locations, and /locations/services/. All the relationships are defined and enforced in the URI given for each API requests, and HTTP takes care of the indexing, performance, and other considerations using caching, and other basic building blocks of the web. My friend James Higginbotham (@launchany) compared this to the concept of views in database backend speak, and OpenAPI Spec describes the HTTP version of&nbsp;OCI (oracle), or TSQL (MS SQL)--depending on what you speak. As an old database guy I like that, "web views", but relying on the request and responses employed as part of the API design. My explanation isn't as coherent as I'd like, but this gives me a start. I'm trying to help database folks who are keepers of the backend, and the schema better understands that what I'm doing with OpenAPI Spec augments and evolves their work. I do not want them to think I am looking to replace or compete with what they are bringing to the table. I'll...[<a href="/2017/01/24/loss-of-primary-and-foreign-keys-translating-from-data-package-to-openapi-spec/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-http2.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/24/amazon-alexa-uses-http2/">Amazon Alexa Uses HTTP/2</a></h3>
			<p><em>24 Jan 2017</em></p>
			<p>I track on the different approaches used by API providers so that I know where to find examples of leading approaches to API design and deployment. Then I write about them so that I have something to reference across my research. I keep an eye out for API providers who employ hypermedia as part of their API design, as well as companies who are putting HTTP/2 to work as part of their design and deployment. The Amazon Alexa Voice Service API employs HTTP/2 as part of their voice-enablement platform. I'm still learning about HTTP/2 so I was pleased to see the amount of education they provide in their documentation, outlining some of the key terms and concepts at play; Frame:&nbsp;The basic protocol unit in HTTP/2; each frame serves a different purpose, for example, HEADERS and DATA frames form the basis of HTTP requests and responses. Stream:&nbsp;An independent, bi-directional sequence of frames exchanged between a client and server within an HTTP/2 connection. For detailed information, see&nbsp;Streams and Multiplexing in RFC 7540. Interfaces:&nbsp;AVS exposes interfaces (SpeechRecognizer, AudioPlayer, SynchronizeState, etc.) that provide your product access to Alexa&rsquo;s built-in skills. Downchannel:&nbsp;A stream you create in your HTTP/2 connection, which is used to deliver directives from the cloud to your client. The downchannel remains open in a half-closed state from the device and open from AVS for the life of the connection. The downchannel is primarily used to send cloud-initiated directives and audio attachments to your client Cloud-initiated Directives:&nbsp;Directives sent from the cloud to your client. For example, when a user adjusts device volume from the Amazon Alexa App, a directive is sent to your product without a corresponding voice request. They also provide details on crafting the&nbsp;HTTP/2 Message Headers, how to construct the&nbsp;HTTP/2 Multipart Messages, and what to expect with&nbsp;HTTP/2 Responses. They have language SDKs in C / C++(nghttp2), C / C++(curl and libcurl), Java (OkHttp), Java Netty), Java(Jetty). I am particularly interested in learning more about how the...[<a href="/2017/01/24/amazon-alexa-uses-http2/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-github-api.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/23/no-database-behind-an-api-and-just-using-files-stored-on-github/">No Database Behind An API and Just Using Files Stored on Github</a></h3>
			<p><em>23 Jan 2017</em></p>
			<p>It is common for an API to just be a facade for a database. Meaning the data, and content served up via the API is read from and written to a database backend. This is probably the most common way to deploy an API, but increasingly I'm working to eliminate the database behind, and storing the content or data being served up via Github repositories.&nbsp; I find it easier to store individual YAML, JSON, and other machine readable files on Github, and just check out the repository as part of each API deployment. Each API has a different refresh rate determining how often I commit or pull a fresh copy of the content or data, but the API does all of its work with a locally checked out copy of the repository. Eliminating the need for a database backend from the required components to make the API operate. Why am I doing this? It helps me solve the database challenges when it comes to deploying in containers, and other more modular approaches to deploying APIs as microservices. The API provides a (hopefully) well-designed&nbsp;facade for the data and content stories, and allows me to use my verbs when reading, writing, and managing resources behind. It also injects the benefits of version control, and user and organizational engagement that Github brings to the table. I'm also using it in on-demand approaches to working with data. I have&nbsp;a lot of government, and other open data stored in Github repositories (free if public), and when I want to work with it, I can spin up a new instance or container, which checks out the latest Github repository, and provides access for reading and writing using a Github OAuth token. When done, the API can be terminated, committing&nbsp;any changes back to the repository, reducing the need for dormant compute resources. This approach also centralizes the data publicly on Github, allowing anyone else to check out and integrate with the JSON...[<a href="/2017/01/23/no-database-behind-an-api-and-just-using-files-stored-on-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/alexa_history.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/23/considering-the-logging-and-observability-layer-for-amazon-alexa-enablement/">Considering the Logging and Observability Layer for Amazon Alexa Enablement</a></h3>
			<p><em>23 Jan 2017</em></p>
			<p>I am going through the Amazon Alexa platform, profiling it as part of my voice API research, and also thinking deeply about the impact voice-enablement, and conversational interfaces will or will not make in our lives. I've been pretty vocal that I am not too excited about voice-enablement in my own world&nbsp;but it is something&nbsp;I understand other folks are, and I'm also interested in Amazon's approach to operating their platform--making it something I'm digging deeper into.&nbsp; I do not own any of the voice enabled&nbsp;Amazon devices, but I am playing with their simulator Echosim.io. I'm not interested in building any skills&nbsp;or applications for Alexa, but I am curious to learn how the platform functions, so I will be developing prototypes&nbsp;so that I can understand things better. One of the things I'm looking to understand as I'm getting up to speed is how the logging for the platform works, so I can evaluate the observability of the voice platform&nbsp;from developers, as well as an end-user perspective. From a developer perspective, I see that you have access to device&nbsp;synchronize state, user inactivity, and exceptions encountered via the Alexa Voice Service System Interface, and from an end-user perspective, there is a history section under device settings. It is a decent start from a logging perspective, but I'm thinking that transparency and observability at this level are going to be important to establish trust between end-users and the platform. With a lot of the speculation around the ambient capabilities of these devices, I'm thinking a robust logging, reporting, and auditing layer to Alexa will be critical to making folks feel like it is something they want in their homes. I'm thinking through the logging layer of my drone platform usage, and what is available to me as end-user, and developer. Alexa provides me another dimension of this logging layer, but this time in the service of voice-enablement. Further rounding off my view of what should be available when it...[<a href="/2017/01/23/considering-the-logging-and-observability-layer-for-amazon-alexa-enablement/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_22_at_8.53.04_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/23/adding-the-webhoseio-search-api-to-stack-of-apis-i-depend-on/">Adding The Webhose,io Search API To Stack Of APIs I Depend On</a></h3>
			<p><em>23 Jan 2017</em></p>
			<p>I have been looking for a decent search engine API to help me uncover new sources of information across the API space. I've always been frustrated with the APIs in this category since all of the good Google search APIs went away. I need to search the web, and specifically for blog and news posts with API related insights. In an effort to find a suitable solution I recently came across and spent time digging into one called&nbsp;Webhose.io, primarily because they had an APIs.JSON file. Webhose.io integrates data from hundreds of&nbsp;thousands of global online sources in the following areas: Social Media Data&nbsp;- Structured data from top social media sites.&nbsp; Forum Monitoring&nbsp;- Post data from across many forums. Blog Search&nbsp;- Pulls the latest from blogs even if no RSS feed. News Search&nbsp;- Access to news from PR sites that don't usually have a feed. Web Search&nbsp;- Gives me access to the latest content from many domains on the&nbsp;web. I'm not a big fan of forum monitoring (too much noise), but access to social media, blog, news, and web&nbsp;data and content is extremely valuable to me. Webhose.io indexes quite a few domains, and I was able to find #mce_temp_url#apievangelist.com, and my girlfriends hackeducation.com in there. Webhose.io allows you to query across categories, using a keyword, and by domain, and they also allow you to set how many days back you'd like to search (up to 30 days)--making it great for staying in tune with the latest content.&nbsp; I like the API mostly because it gets straight to the point, allowing you to quickly sign up for an account, easily craft API calls, then provides valuable, relevant data and content, enriched with tags, sentiment, and other goodies. They also provide a straightforward business model, allowing you to get 1000 free API calls per month, and incremental&nbsp;monthly packages with different quantities from there. I will be wiring up the API to my monitoring system, adding Webhose.io to the stack...[<a href="/2017/01/23/adding-the-webhoseio-search-api-to-stack-of-apis-i-depend-on/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-fist.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/20/reducing-our-hard-work-to-a-transaction-with-apis-and-serverless/">Reducing Our Hard Work To A Transaction With APIs and Serverless</a></h3>
			<p><em>20 Jan 2017</em></p>
			<p>I'm thinking about cloud pricing after my profiling of over 60 of the AWS API resources, as I play with building tools on Algorithmia, and evaluate a variety of serverless options. As someone who is regular blindsided by the devious undercurrents of business, while I'm busy focusing on shiny technological objects, I can't help but feel like us developers are contributing to the commoditization of our (currently) valuable skillset when it comes to APIs, microservices, devops, and serverless. This isn't exclusive to these areas of technology, and I think it is something we've all set into motion with APIs and microservices over the last decade. We are decoupling some very complex, and often large codebases and dependencies that take a certain amount of experience and skills to tackle, and reducing down to individual reusable resources that are automatically scaled, and may not require as many advanced skills to daisy chain and connect together. I'm all for doing this in the spirit of enabling the average business person to accomplish what they need on a daily, but from my experience in the savviness, and deviance of business leaders, I can't help but feel that eventually developer's labor will be devalued in this environment.  If we succeed, all company resources are available via APIs and microservices and all events and actions are present as "serverless" technology, and each individual resource will have a price tag, opening everything up for commoditization and extreme competition--this is good right? There will still be room for a handful of high paid architects in this environment, but new pieces of code will be able to be developed in isolation, at a very reduced price, with a huge amount of competition driving what developers will be paid way down. We are ultimately working against ourselves in this environment, and our best interest by achieving the current vision of "scale".  I do not think that the technology being employed (API, microservices, devops, and serverless) will be entirely...[<a href="/2017/01/20/reducing-our-hard-work-to-a-transaction-with-apis-and-serverless/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_19_at_11.20.03_am.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/19/the-state-of-california-drinking-water-program-repository/">The State of California Drinking Water Program Repository</a></h3>
			<p><em>19 Jan 2017</em></p>
			<p>
One of the side projects I work on regularly is focused on moving forward the conversation around water data. My next wave of work is targeting the State of California&nbsp;Drinking Water Program Repository, and help&nbsp;make some of the valuable spreadsheets and CSV files more usable. Here are the six datasets I'm targeting for processing in coming weeks:

Annual Report Form - PDF of input form used for the Annual Report.
Annual Reports - Excel book of Annual Reports of public water systems from 2011 through 2015.
SDWIS Public Water Systems - CSV of active public water systems including contact, location, water source, and type.
Small Water Supplier Conservation Reports - CSV of small water suppliers reporting conservation results from June through November 2015.
Urban Water Supplier Conservation Reports - CSV of urban water supplier drought monitoring reports from 7/14 to present.
Water Quality Master Results - Zipped CSV file containing 6 M water quality results from 2013 to present including facility name, MCL, DLR, sample date, finding, and water system.

When it comes to public data there are few datasets that are going to as valuable as water data in coming years. I'd put healthcare and education at the top, but water is only going to become increasingly important as the world continues to evolve. If you have an interest in water data let me know, as I could use some help processing these important data sets.
[<a href="/2017/01/19/the-state-of-california-drinking-water-program-repository/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/gorilla_red_circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/19/oracle-acquiring-apiary/">Oracle Acquiring Apiary</a></h3>
			<p><em>19 Jan 2017</em></p>
			<p>Oracle has purchased API design provider Apiary. I'm a big fan of what Apiary does, and what the team has accomplished. I don't trade in Silicon Valley currency, so I'm not going to congratulate the team on their exit. For me, it is just a reminder of how we can't have anything nice in the space. No matter how good your team is, or how good your product or services are, the thousand&nbsp;pound gorillas will always come in the room and fuck things up.&nbsp; I am bummed about the acquisition of Apiary because they are essential to my API design origin story. Jakub, Z, and the Apiary team made API definitions to be more about API design&nbsp;than just API documentation. Pushing the conversation earlier on in the API lifecycle, opening up the concept that API definitions could be used for not just documenting your APIs after they are live, and all about design early on in the process, something that opened up for use across every stop along the API life cycle. This resulted in API definitions being core to mocking, deployment, management, SDKs, testing, monitoring, and much, much more. #thankyou Apiary was something I was proud to support in the API sector--Oracle is not. The negative mark this company has left in the space far outweighs anything good they could possibly do through acquisitions or any new solutions. With this post, I will get the usual waves of people telling me this is "just business", which is code for how people fuck each other over under their capitalist religion. So, go ahead and tell me how I live in a fantasy world, when in reality it is you who live in a fantasy world where it isn't about producing a good product, or service, it is just about your love of money, quest to be rich, and your willingness&nbsp;to allow for good things (and people) to be screwed over along the way. Over the...[<a href="/2017/01/19/oracle-acquiring-apiary/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-gears.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/19/focusing-on-common-api-definitions-schema-scopes-and-specifications/">Focusing On Common API Definitions, Schema, Scopes and Specifications</a></h3>
			<p><em>19 Jan 2017</em></p>
			<p>The API universe is rapidly expanding as more companies, organizations, institutions, and government agencies are sharing data, content, and algorithms using web APIs. It has expanded so much in the last year that I can't keep up with everything that is going on. I can't test new APIs, and the emerging services and tooling from providers who are targeting the space fast enough--that is ok, I'm not sweating it.&nbsp; However, I do have to prioritize and focus on the areas where I can make the biggest impact when it comes to my understanding, and when it comes to helping the API community. While I will still be maintaining a general awareness of all technologies in 2017 I'm going to be heavily focused on three areas: API Definition - The machine-readable definition of an API interface, security and data models. Data Schema - A JSON schema, MSON, or other machine-readable description of data schema. OAuth Scope - Shared definitions of OAuth scopes in play for an API and it's resources. Web Specifications - Web concepts and specifications that make the web go around. Code plays a significant role, and specific implementations are important, but for me, these four areas represent the core of the API space. Defining, employing, discussing, and sharing in these areas is how the API space will scale, and meet the needs of specific industries. Investing in these areas will make the API sector healthier, stronger, and make an impact when it comes to combatting much of the focus on investment, and proprietary of technology that works against things being truly interoperable reusable. In 2017 I'm focusing the majority of my work in these areas. These are not the areas where the money will be in the future, but it will be what enables all the areas that do make startups money and make a difference on the ground at small businesses and the enterprise. Investment in common definitions, schema, scopes, and specifications...[<a href="/2017/01/19/focusing-on-common-api-definitions-schema-scopes-and-specifications/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-trust.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/17/when-we-lose-trust-in-the-reporting-numbers-our-providers-feed-us/">When We Lose Trust In The Reporting Numbers Our Providers Feed Us</a></h3>
			<p><em>17 Jan 2017</em></p>
			<p>What happens when we can't trust the numbers our service providers report to us? I personally do not stress over my analytics and traffic, views, and other numbers we are engineering our worlds to run by, but when you are paying for a service--I definitely have an opinion. Facebook recently had a series of misreporting events around their metrics, leaving us questioning the numbers we are fed by our service providers on a regular basis.
There is no way that we can be 100% sure our service providers are telling us the truth--we have to trust them. However, there are ways that API providers can be more transparent when it comes to the data behind the numbers. It is easy enough to open up the log files, and other data that went into calculating the numbers when operating a platform. Many of the advertising and other service providers in questions have APIs to control the parts of the systems a platform desires you to perform--they just don't like pulling back the curtain, and showing what is truly going on.
If you do not fully trust your provider's numbers make sure and let them know publicly and privately that you would like access to the raw data behind. APIs can be designed to provide access to any data, as well as provide observability into the algorithms that driving any process. This is not a question of whether it's possible technically, it is a question of whether it is possible politically, and whether or not your service provider is willing to be transparent enough with you to develop the required trust needed for all of this to work properly.
[<a href="/2017/01/17/when-we-lose-trust-in-the-reporting-numbers-our-providers-feed-us/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-patent-algorithms.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/17/patent-20150363171-generating-virtualized-api-from-narrative-api-documentation/">Patent #20150363171: Generating Virtualized API From Narrative API Documentation</a></h3>
			<p><em>17 Jan 2017</em></p>
			<p>I like to pick worrisome patents from my API patent research and share them on my blog regularly. Last week I talk about&nbsp;Patent #US9300759 B1: API Calls With Dependencies and today I want to talk about patent #US09471283:&nbsp;Generating virtualized application programming interface (API) implementation from narrative API documentation, which according to its abstract is: A virtualized Application Program Interface (API) implementation is generated based upon narrative API documentation that includes sentences that describe the API, by generating programming statements for the virtualized API implementation based on parsing the narrative API documentation and generating the virtualized API implementation based on upon the programming statements for the virtualized API implementation. The parsing of the narrative documentation may use a natural language parser and a domain-specific ontology for the API that may be obtained or created for the API. The virtualized API implementation may be generated using an API virtualizer. I generally won't talk smack about folks filing patents, except I'm a pretty strong believer that the API interface and the supporting elements that make an API do the API thing should be left out. All the elements present in this patent like virtualization, documentation, and narrative need to be open--this is how things work. Just when we are all beginning to get good when it comes to crafting useful APIs, learning to speak in complete sentences with multiple APIs, we are going to patent the process of telling stories with APIs? Sorry, it just doesn't compute for me. I know this is what ya'll&nbsp;do at bigcos, but it's counterproductive. Instead of filing this patent I wish they would have taken their idea and launched as open source tool, then also launch an accompanying service running in the cloud, and get to work letting us crafting narratives around our APIs. As I've said before, these patents really mean nothing, and it will all come down to keeping an eye on the court filings using the CourtListener API&nbsp;for any...[<a href="/2017/01/17/patent-20150363171-generating-virtualized-api-from-narrative-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/portal_screenshot.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/16/what-are-the-goals-behind-launching-an-api-portal/">What Are The Goals Behind Launching An API Portal?</a></h3>
			<p><em>16 Jan 2017</em></p>
			<p>I was getting ready to do some work on a developer portal for a project I'm working on and I wanted to stop, step back and try to define what the goals in launching this portal are. As the technologist on this project, it is easy for me to impose my belief in why I am launching&nbsp;this portal (to publish documentation), but I feel it is important that we get the perspective of all stakeholders--so, I asked everyone involved what the goals were. In the short term, the goals are to engage these groups around the API resources: Engage Third-Parties - Bring in new, and stimulate existing usage of data made available via APIs. Internal Departments - Ensure the internal groups are also engaged, and part of the outreach. While incentivizing the following things between engaged stakeholders: Aware - Help make people aware of available resources, and what is being done with them. Conversations - Stimulate focused conversations&nbsp;among&nbsp;stakeholders around data and APIs. Issues - Focus on solving tangible issues that actually make an impact in the community. Long term the goals to stimulate projects that matter, and can possibly bring in sustainable relationships and revenue that will help ensure the platform is up and running, and serving the target audiences. It takes money to do APIs properly, keeping the servers running, code evolving, active outreach and support in the community, and a passionate, engaged community to develop valuable projects. I know all of this sounds overly simplified, but it is actually the thought process I'm applying to this project. I would say it is oversimplified to say that we are just launching a portal so that people will use our APIs. We need some basic goals about who we are reaching out to, and what we are trying to accomplish--we can evolve, and continue to define from there. When I lay out the outline for this project's portal I will make sure each element that...[<a href="/2017/01/16/what-are-the-goals-behind-launching-an-api-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_15_at_6.40.58_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/16/profiling-facebook-threatexchange-api/">Profiling Facebook ThreatExchange API</a></h3>
			<p><em>16 Jan 2017</em></p>
			<p>I'm spending some cycles on discovering what "cybersecurity" or "security" API solutions are out there, but specifically looking at threat information related to operating on the web. First up on the list is Facebook's ThreatExchange API, and I wanted to go through and break down what they offer via their API as I work to define an OpenAPI Spec, and their overall API operations as I populate&nbsp;an&nbsp;APIs.json&nbsp;file.This process helps me better understand what Facebook is offering in this area, as well as producing a machine readable definition that I can use across the rest of my research.&nbsp; So, what is Facebook ThreatExchange? Learn about threats. Share threat information back. Everyone becomes more secure. Most threat intelligence solutions suffer because the data is too hard to standardize and verify. Facebook created the ThreatExchange platform so that participating organizations can share threat data using a convenient, structured, and easy-to-use API that provides privacy controls to enable sharing with only desired groups. &nbsp; Scale your intelligence -&nbsp;Threats like malware and phishing will often attack many targets. Safeguard yourself using shared intelligence from other&nbsp;participants. A better way to share -&nbsp;Utilize a structured platform to send and receive information about threats. Join ThreatExchange -&nbsp;Learn about threats. Share threat information back. Everybody becomes more secure. *in beta I'm wanting to understand Facebook's motivations behind doing the ThreatExchange API, and better understand the technical, business, and political aspects of providing a platform like this. When profiling a platform I always start with the endpoints, as I feel like they give the most honest accounting of what is going on. /malware_analyses - Search for malware samples by hash and other metadata. /malware_families - Search for malware families by name and other metadata. /threat_descriptors - Enables searching for indicators of compromise descriptors. /threat_indicators - Enables searching for indicators of compromise. /threat_tags - Enables searching for threat tags. /threat_exchange_members - Returns a list of current members of the ThreatExchange. Next, I look at the underlying...[<a href="/2017/01/16/profiling-facebook-threatexchange-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-terms-of-use.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/16/no-innovation-around-terms-of-service-reveals-true-motives/">No Innovation Around Terms of Service Reveals True Motives</a></h3>
			<p><em>16 Jan 2017</em></p>
			<p>Silicon Valley startups and entrepreneurs love to point out that they are trying to make the world a better place. Over a 25+ year career, I have fallen for the belief that I was improving a situation through technology. Hell, I still do this regularly as the API Evangelist, stating that a certain approach to opening up access to data, content, and algorithms can make things better, when in numerous situations it will not. I walk a fine line with this and I hope that I'm a little more critical about where technology should be applied, and focus primarily on making existing technology more accessible using APIs--not starting new ones. When you are critical of technology in the current climate, there are plenty of folks you like to push back on you, leaning on the fact that they are trying to make the world a better place. Not sure where this line of bullshit first began but is something I should look into. I mean, when you hang out with enough entrepreneurs you really begin see through the bullshit and hype, and you know that this is all about them selling you something, and then selling you and your data as part of an exit via acquisition or going public. It rarely ever has anything to do with making the world a better place, what was promised as part of the marketing, and helping the average end-user. This is where my entrepreneur friends have stopped reading&nbsp;and lean on the fact that they actually believe they are trying to make the world a better place, and their firm belief that Kin is an annoying hippie. You know I love you. You are a&nbsp;special snowflake. But, you are a minority, I am talking about the other 95% of the system you choose to be willfully ignorant of. If you want some evidence of this in the wild, take a look at all the world saving, innovative, revolution...[<a href="/2017/01/16/no-innovation-around-terms-of-service-reveals-true-motives/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_11_28_at_9.58.49_am.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/13/requiring-ssl-for-api-all-calls/">Requiring SSL For API All Calls</a></h3>
			<p><em>13 Jan 2017</em></p>
			<p>
This is one of those regular public service announcements that if at all possible, you should be requiring SSL for all your API calls. I recently got an email from the IBM Watson team telling me that they would be enforcing encryption on all calls to the Alchemy API in February.
SSL is something I've started enforcing on my own internal APIs. I do not have wide usage of my APIs by third-party providers, but I do have a variety of systems making calls to my APIs, transmitting some potentially sensitive&nbsp;information--luckily nothing too serious, as I'm just a simple API Evangelist.
Encryption is an area I research regularly, hoping to stay in tune (as much as I can) with where discussions are going when it comes to encryption and API operations. Much of it doesn't apply to the average API provider&nbsp;but requiring encryption for API calls, and emailing your developers when and if you do begin enforcing, is what I'd consider an essential building block for all API providers.
I'll keep crafting a unique blog post each time I receive on of these emails from the APIs I depend on. Hopefully some day soon, all APIs will be SSL by default.

[<a href="/2017/01/13/requiring-ssl-for-api-all-calls/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/data_fire_bulb.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/13/ifttt-vs-zapier-vs-datafire/">IFTTT vs Zapier vs DataFire</a></h3>
			<p><em>13 Jan 2017</em></p>
			<p>Integration Platform as a Service (iPaaS) solutions are something I've been tracking on for a while, and an area I haven't seen too much innovation in, except by Zapier for most of that time. I'm a big fan of what IFTTT enables, but I'm not a big fan of companies who build services that depend on APIs&nbsp;but do not offer APIs in turn, so you don't find me highlighting them as an iPaaS solution. Instead, you'll find me cheering for Zapier, who has an API, and even though I wish they had more APIs, I am grateful they paying it forward a little bit. I wish we had better solutions, but the politics of API operations seems to slow the evolution of iPaaS, usually leaving me disappointed. That was until recently&nbsp;when some of my favorite&nbsp;API hackers released DataFire: DataFire is an open source integration framework - think Grunt for APIs, or Zapier for the command line. It is built on top of open standards such as RSS and Open API. Flows can be run locally, on AWS Lambda, Google Cloud, or Azure via the Serverless framework, or on DataFire.io. "DataFire natively supports over&nbsp;250 public APIs&nbsp;including: &bull; Slack &bull; GitHub &bull; Twilio &bull; Trello &bull; Spotify &bull; Instagram &bull; Gmail &bull; Google Analytics &bull; YouTube, as well as MongoDB, RSS feeds, and&nbsp;custom integrations." They have a sample&nbsp;flows available as an individual Github repositories.&nbsp;Integrations can be added by the URL of an Open API (Swagger) specification or an RSS feed, you can also specify --raml, --io_docs, --wadl, or --api_blueprint. DataFire is new, so it has a lot of maturing to do as an API framework, but it has EVERYTHING that iPaaS solutions should have at its core in my opinion. It's API definition-driven, its open source,&nbsp;and there is a cloud version that any non-developer user can put to use. DataFire is encouraging everyone to share each of the&nbsp;flows as machine readable templates, each as their own Github...[<a href="/2017/01/13/ifttt-vs-zapier-vs-datafire/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/medium_storytelling_network2.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/13/a-missed-opportunity-with-the-medium-api/">A Missed Opportunity With The Medium API</a></h3>
			<p><em>13 Jan 2017</em></p>
			<p>In addition to using the news of Medium's downsizing as a moment to stop and think about who owns our bits, I wanted to point out what a missed opportunity the Medium API is. Having an API is no guarantee of success, and after&nbsp;$132M&nbsp;in&nbsp;3 Rounds&nbsp;from&nbsp;21 Investors, I'm not sure an API can even help out, but it is fun to speculate about what might be possible if Medium had robust API in operation. Medium has an API, but it is just a Github repository, with reference to a handful of paths allowing you to get details on yourself, the publications you are part of, and post entries to the site. There are no APIs for allowing me to get the posts of other users, or publications, let alone any of the analytics, or traffic for this. I'm guessing there is no API vision or champion at Medium, which results in the simple, restrictive API we see today. Many media and content companies see APIs as giving away all the value they are trying to monetize, and are unaware of the control that modern approaches to API management bring&nbsp;to the table. Many people see the pain that other API pioneers have suffered like Twitter, and want to avoid the same problems, completely unaware that many of Twitter's ecosystem problems were Twitter-induced&nbsp;and not inflicted by 3rd party developer. If Medium had a proper developer portal, complete set of API paths, proper OAuth controls, and other API management tooling, they could open up innovation in content delivery, publishing, analytics, visualizations, voice, bot, and the numerous of other areas where APIs are changing how we consume, create, and engage with information. I get that control over the user experience is key to the Medium model, but there are examples of how this can be done well, and still have an API. The best part is it only costs you the operation of the API operations. I do not think...[<a href="/2017/01/13/a-missed-opportunity-with-the-medium-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/kl_inapiwetrust_500_filtered.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/12/why-i-still-believe-in-apisthe-2017-edition/">Why I Still Believe In APIs--The 2017 Edition</a></h3>
			<p><em>12 Jan 2017</em></p>
			<p>As I approach my seventh year as the API Evangelist&nbsp;and find myself squarely in 2017, I wanted to take a moment to better understand, and articulate why I still believe in APIs. To be the API Evangelist I have to believe in this, or I just can't do it. It is how my personality works--if I am not interested, or believe in something, you will never find me doing it for a living, let alone as obsessively as I have delivered API Evangelist. First, What Does API Mean To Me?There are many, many interpretations, and incarnations of "API" out there. I have a pretty wide definition of what is API, one that spans the technical, business, and politics of APIs. API does not equal REST, although it does employ the same Internet technologies used to drive the web. API is not the latest vendor solution&nbsp;or even standard. The web delivers HTML for humans to consume in the browser, and web APIs deliver machine-readable media types (XML, JSON, Atom, CSV, etc.) for use in other applications. When I say applications, I do not just mean the web, mobile, and devices applications--I mean other applications, as in "the action of putting something into operation".&nbsp;An API has to find harmony between the technical, business, and political aspects of API operations and strike a balance between platform, 3rd party developer, and end-user needs--with every stakeholder being well informed along the way. I Still Believe Early My API VisionWhen I close my eyes I still believe in the API vision that captured my attention in 2005 using the Delicious API, again in 2006 with the Amazon S3 and EC2 APIs, and with the Twitter API in 2007. Although today, I better understand that a significant portion of this early vision was very naive, and too trusting in the fact that people (API providers and consumers) would do the right thing with APIs. APIs use Internet technology to make data, content,...[<a href="/2017/01/12/why-i-still-believe-in-apisthe-2017-edition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/openreferral_logo_green.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/12/using-an-openapi-spec-as-central-truth-in-stakeholder-discussions/">Using An OpenAPI Spec As Central Truth In Stakeholder Discussions</a></h3>
			<p><em>12 Jan 2017</em></p>
			<p>I am working with Open Referral to evolve the schema for the delivery of human services, as well as helping craft a first draft of the OpenAPI Spec for the API definition. The governing organization is looking to take this to the next level, but there are also a handful of the leading commercial providers at the table, as well other groups closer to the municipalities who are implementing and managing Open211 human service implementations. I was working with Open Referral on this before checking out this last summer, and would like to help steward the process, and definition(s) forward further in 2017. This means that we need&nbsp;to speak using a common language when hammering out this specification&nbsp;and be using a common platform where we can record changes, and produce a resulting living document. I will be borrowing from existing work I've done on API definitions, schema, and scope across the API space, and putting together a formal process design specifically for the very public process of defining, and delivering human services at the municipal level. I use OpenAPI Spec (openapis.org) as an open, machine readable format to drive this process. It is the leading standard for defining APIs in 2017, and now is officially part of the Linux Foundation. OpenAPI Spec provides all stakeholders in the process with a common language when describing the Open Referral in JSON Schema, as well as the surface area of the API that handles responses &amp; requests made of the underlying schema. I have an OpenAPI Spec from earlier work&nbsp;on this project, with the JSON version of the machine-readable definition, as well as a YAML edition--OpenAPI Spec allows for JSON or YAML editions, which helps the format speak to a wider, even less technical audience. These current definitions are not complete agreed upon definitions for the human services specification, and are just meant to jumpstart the conversation at this point. OpenAPI Spec provides us with a common language...[<a href="/2017/01/12/using-an-openapi-spec-as-central-truth-in-stakeholder-discussions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_11_at_8.59.54_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/12/the-google-baseline-for-a-user-account-area/">The Google Baseline For A User Account Area</a></h3>
			<p><em>12 Jan 2017</em></p>
			<p>I have a minimum definition for what I consider to be a good portal for an API, and was spending some time thinking about a baseline definition for the API developer account portion of that portal, as well as potentially any other authenticated, and validated platform user. I want a baseline user account definition that I could use as aa base, and the best one out there off the top of my head would be from Google. To support my work I went through my Google account page and outlined the basic building blocks of the Google account: Sign-in &amp; Security - Manage your account access and security settings Signing in to Google - Control your password and account access, along with backup options if you get locked out of your account. Password &amp; sign-in method - Your password protects your account. You can also add a second layer of protection with 2-Step Verification, which sends a single-use code to your phone for you to enter when you sign in.&nbsp; Password - Manage your password. 2-Step Verification - Manage 2-Step verification. App Passwords - Create and manage application passwords. Account recovery options - If you forget your password or cannot access your account, we will use this information to help you get back in. Account Recovery Email - The email to send recovery instructions. Account Recovery Phone - The email to send recovery instructions. Security Question - A secret question to use as verification during recovery. Device activity &amp; notifications - Review which devices have accessed your account, and control how you want to receive alerts if Google thinks something suspicious might be happening. Recent security events - Review security events from the past 28 days. Recently used devices - Check when and where specific devices have accessed your account. Security alerts settings - Decide how we should contact you to let you know of a change to your account&rsquo;s security settings or if we...[<a href="/2017/01/12/the-google-baseline-for-a-user-account-area/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/shutterstock_upload_license.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/11/your-state-issued-id-is-required-to-signup-for-this-online-service/">Your State Issued ID Is Required To Signup For This Online Service</a></h3>
			<p><em>11 Jan 2017</em></p>
			<p>I am evaluating Shutterstock as a new destination for some of my photos and videos. I've been a Shutterstock user for their stock images, but I'm just getting going being a publisher. I thought it was worth noting that as part of their sign up process they require me to upload a copy of my state issued identification&nbsp;before I can sell photos or images as a Shutterstock publisher. This is something I've encountered with other affiliate&nbsp;, partner, and verified solutions. I've also had domains expire, go into limbo, and I have to fax in or upload my identification. It isn't something I haven't seen with many API providers yet, but I'm guessing it will be something we'll see more of with API providers further locking down their valuable resources.&nbsp; I am not sure how I feel about it being a regular part of the partner and developer validation&nbsp;process--I will have to think about it more. I'm just adding to the list of items I consider as part of the API management process. It makes sense to certify and establish trust with developers, but I'm not 100% sure this is the way to do it in the digital age. IDK, I will consider more, and keep an eye out for other examples of this with other API providers. Shutterstock publisher isn't necessarily connected directly to the API, but once I'm approved I will be uploading, and managing my account via their API, so it is a developer validation process for me. The topic of developer validation and trust keeps coming up in other discussions for me, and with the increasing number of APIs we are all developing with, it seems like we are going to need a more streamlined, multi-platform, and an API-driven solution to tackle this.&nbsp; For me, it would be nice if this solution was associated with my Github account, which plays a central&nbsp;role in all of my integrations. When possible, I create my...[<a href="/2017/01/11/your-state-issued-id-is-required-to-signup-for-this-online-service/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_10_at_7.57.42_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/11/intercom-providing-docker-images-of-their-sdks/">Intercom Providing Docker Images Of Their SDKs</a></h3>
			<p><em>11 Jan 2017</em></p>
			<p>
I regularly talk about the evolving world of API SDKs, showcasing what API service providers like APIMATIC are up to when it comes to orchestration, integration, other dimensions of providing client code for API integrations. A new example of this that I have found in the wild is from communication and support API platform Intercom, with their publishing of Docker images of their API SDKs. This overlaps my SDK research with the influence that&nbsp;containerization is having on the the world of providing and integrating with APIs.
Intercom provides Docker images for their Ruby, Node, Go, and PHP API SDKs. It's a new approach to making API code available to API consumers that I haven't seen before, (potentially) making their integrations easier, and quicker. I like their approach to providing the containers and specifically the fact they are looking for feedback on whether or not having SDK Docker containers offer any benefit to developers. I'm guessing this won't benefit all API integrators, but for those who have successfully adopted a containerized way of life, it might streamline the process and overall time to integration.
I just wanted to have a reference on my blog to their approach. I'll keep an eye on their operations, and see if their SDK Docker images become something that gets some traction when it comes to SDK deliver. Since they are sharing the story on their blog, maybe they'll also provide us with an update in a couple months regarding whether developers found it useful or not. If nothing else, their story has reminded me to keep profiling Intercom, and other similar API providers, as part of my wider API communication and support research.

[<a href="/2017/01/11/intercom-providing-docker-images-of-their-sdks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_10_at_1.23.35_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/11/evernote-reaffirming-our-commitment-to-your-privacy/">Evernote: Reaffirming Our Commitment to Your Privacy</a></h3>
			<p><em>11 Jan 2017</em></p>
			<p>A couple of weeks back, the online note-taking platform Evernote made a significant&nbsp;blunder&nbsp;of&nbsp;releasing a privacy policy update that revealed they would be reading our notes to improve their machine learning algorithms. &nbsp;It is something they have since rolled back with the following statement "Reaffirming Our Commitment to Your Privacy": Evernote recently announced a change to its privacy policy and received a lot of customer feedback expressing concerns. We&rsquo;ve heard that feedback and we apologize for the poor communication.We have decided not to move forward with those changes that would have taken effect on January 23, 2017. Instead, in the coming months we will be revising our existing Privacy Policy. The main thing to know is this: your notes remain private, just as they&rsquo;ve always been.Evernote employees have not read, and do not read, your note content. We only access notes under strictly limited circumstances: where we have your permission, or to comply with our legal obligations.Your privacy, and your trust in Evernote are the most important things to us. They are at the heart of our company, and we will continue to focus on that now and in the future. While I am thankful for their change of heart, I wanted to take a moment to point out the wider online environment that incentivizes this type of behavior. This isn't a single situation with Evernote reading our notes, this is the standard mindset for startups operating online, and via our mobile devices in 2017. This is just one situation that was called out, resulting in a change of heart by the platform. Our digital bits are being harvested in the name of machine learning and artificial intelligence across the platforms we depend on daily for our business and personal lives.&nbsp; In these startup's quest for profit, and ultimately their grand exit, they are targeting our individual and business bits. Use this free web application. Use this free mobile application. Plug this device in at home...[<a href="/2017/01/11/evernote-reaffirming-our-commitment-to-your-privacy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_09_at_9.03.01_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/10/the-design-process-helping-me-think-through-my-data-and-content/">The Design Process Helping Me Think Through My Data And Content</a></h3>
			<p><em>10 Jan 2017</em></p>
			<p>I'm working on the next evolution in my API research, and I'm investing more time and energy into the design of the guides I produce as a result of each area of my research. I've long produced a 20+ page PDF dumps of the leading areas of my research like API design, definitions, deployment, and management, but with the next wave of industry guides, I want to polish my approach a little more.  The biggest critique I get from folks about the API industry guides I produce is that they provide too much information, aren't always polished enough, and sometimes contain some obvious errors. I'm getting better at editing, but this only goes so far, and I'm bringing in a second pair of eyes to review things before they go out. Another thing I'm introducing into the process is the use am of professional design software (Adobe InDesign) rather than just relying on PDF's generated from my internal system with a little HTML spit shine. While it is taking me longer to dust off my Adobe skills than I anticipated, I am finding the design process to be extremely valuable. I've often dismissed the fact that my API research needed to look good, and that it is more important that it is just available publicly on my websites. This is fine and is something that will continue, but I'm finding a more formal design process is helping me think through all of the material, better understand what is valuable, what is important, and hopefully better tell a story about why it is relevant to the API space. It is helping me take my messy backend data and content, and present it in a more coherent and useful way. As I'm formalizing my approach using my API definition guide, I'm moving on to my API design guide, and I can't help but see the irony in learning the value of design while publishing a guide on API design, where I highlight...[<a href="/2017/01/10/the-design-process-helping-me-think-through-my-data-and-content/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_09_at_11.11.55_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/10/patent-us9300759-b1-api-calls-with-dependencies/">Patent US9300759 B1: API Calls With Dependencies</a></h3>
			<p><em>10 Jan 2017</em></p>
			<p>
I understand that companies file for patents to build their portfolios, and assert their stance in their industry, and when necessary use patents as leverage in negotiations, and in a court of law. There are a number of things that I feel patents logically apply to, but I have trouble understanding why folks insist on patenting things that make the web work, and this whole API thing work.
One such filing is patent number&nbsp;US9300759 B1: API Calls With Dependencies, which is defined as:
Techniques are disclosed for a client-and-server architecture where the client makes asynchronous API calls to the client. Where the client makes multiple asynchronous API calls, and where these API calls have dependencies (i.e., a result of one call is used as a parameter in a second call), the client may send the server these multiple asynchronous API calls before execution of a call has completed. The server may then execute these multiple asynchronous API calls, using a result generated from one call as a parameter to another call.
Maybe I live in a different dimension than everyone else, but this doesn't sound unique, novel, and really just feels like folks&nbsp;are mapping out all the things that are working on the web and filing for patents. I found this patent while going through the almost 1300 API related patents in Amazon Web Services portfolio. Many of their patents make sense to me, but this one felt like it didn't belong.
When I read these patents I worry about the future of the web. Ultimately I can only monitor the courts for API related patent litigation, and keep an eye out for new cases, as this is where the whole API patent game is going to play out. I'll keep squawking every time I read a patent that doesn't just belong, and when I see any new&nbsp;legal cases I will help shine a light on what is going on.

[<a href="/2017/01/10/patent-us9300759-b1-api-calls-with-dependencies/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/schema_starburst.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/10/hoping-schema-becomes-just-as-important-as-api-definitions-in-2017/">Hoping Schema Becomes Just As Important As API Definitions in 2017</a></h3>
			<p><em>10 Jan 2017</em></p>
			<p>The importance of a machine readable API definition has grown significantly over the last couple of years, with a lot of attention being spent (rightfully so) on helping educate API providers of the value of having an OpenAPI Spec, API Blueprint, or another format. This is something I want to continue contributing to in 2017, but I also want to also shine a light on the importance of having your data schema well defined. When you look through the documentation of many API providers, some of them provide an example request which might give hints at the underlying data model, but you rarely ever see API providers openly share their schema in any usable format. You do come across some of a complete OpenAPI Spec or API Blueprints from time to time, but usually, when you find API definitions, the schema definition portion is incomplete.&nbsp; Not having your schema well defined, shareable, and machine-readable is one of the contributing factors to a lack of common, shared schema in the API space. We have healthy examples of this in action with Schema.org, but for some reason, many of us don't bring schema front and center in our API operations. We are accepting them as input for our API requests, and returning them with API responses, but don't always share examples of this in our documentation, complete sections in our API definitions, or share JSON Schema as part of our developer resources. All contributing to a lack of consistency within a single API operation, as well as the wider industry. I am going to spend more time in 2017 talking to people about the schema they use in their API operations and shining a light on existing schema that has been published by API providers. I will be also continuing to support important schema like Open Referral that helps streamline how our world works. It is no secret that when we speak using common schema things work...[<a href="/2017/01/10/hoping-schema-becomes-just-as-important-as-api-definitions-in-2017/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/kinlane_physical_digital_self_publish.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/09/the-api-driven-marketplace-that-is-my-digital-self/">The API Driven Marketplace That Is My Digital Self</a></h3>
			<p><em>09 Jan 2017</em></p>
			<p>I spend a lot of time studying and thinking about the "digital bits" that we move around the Internet. Personally, and professionally I am dedicated to quantifying, and understanding those bits that are the most important to us as individuals, professionals, and business owners. Like many other folks who work in the tech sector I have always been good at paying attention to the digital bits, I am just not as good at others when monetizing these bits, adding to my own wealth. When you talk about this world in the world as much as I have, you see just a handful of responses. Most "normals" aren't very interested in things at this level--they just want to benefit from the Internet&nbsp;and aren't really interested in how it works. People who are associated with the tech sector and understand the value of these bits, often do not see them as "your" bits, they seem them as their bits--something they can extract value from, and generate revenue. Then there are always a handful of "normals" who are interested in understanding more, because of security and privacy concerns, as well as a handful of tech sector folks who actually care about the humans enough to balance the desire to just make profits. The Imbalance In All OF This Is What Fascinates Me&nbsp;The majority of the "normals" don't care about the complexity of the bits they generate, as well as who has access to them. Folks in the tech sector love to tell me regularly that people don't care about this stuff, they just want the convenience, and for it all to work. However, they are also overwhelmingly interested in the bits you generate each day&nbsp;because there is plenty of money to be made extracting insights from your bits, and selling those insights, as well as the raw bits to other companies so they can do the same. This is why EVERYTHING is being connected to the Internet--the convenience...[<a href="/2017/01/09/the-api-driven-marketplace-that-is-my-digital-self/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-service-level-agreements.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/09/service-level-agreements-for-researchers-who-depend-on-apis/">Service Level Agreements for Researchers Who Depend On APIs</a></h3>
			<p><em>09 Jan 2017</em></p>
			<p>
I came across a pretty interesting post on using APIs for research, and the benefits, and challenges that researchers face when depending on APIs. It was another side of API stability and availability that I hadn't considered too much lately. Social media platforms like Twitter and Facebook are rich with findings to be studied across almost any discipline. I regularly find social media API studies at universities from areas like healthcare and Zika virus,&nbsp;algorithmic intellectual property protection, all the way up to US Navy surveillance programs that are studying Twitter.
APIs are being used for research, but there are rarely API platform plans crafted with research in mind. Flexible rate limits, custom terms of service, that give them access to the data they need. I'm assuming that some companies have behind the scenes deals with some universities, or larger enterprise research groups (IBM, etc), as well as government agencies, and police agencies. The problem with this, is that 1) there is no virtual public front door to walk through and understand research levels of access, and 2) the details of partnerships are not publicly, equitable, and auditable by journalists, and other groups.
The author of this essay provides a lot of details regarding what it is like to depend on APIs for your research. Some of them could put your career in jeopardy if the terms of service, and access levels change before you could finish your research, or dissertation. I'm not sure what the responsibility of API providers should be when it comes to making their resources available for research, but it is something I will be exploring further. I will be reaching out to researchers about their API usage, but will also be helping encourage API providers to share their side of things, and maybe eventually formalize how API providers make their valuable resources available for important research.
[<a href="/2017/01/09/service-level-agreements-for-researchers-who-depend-on-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/shutterstock_developers_home.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/09/evaluating-a-new-channel-for-publishing-my-bits/">Evaluating A New Channel For Publishing My Bits</a></h3>
			<p><em>09 Jan 2017</em></p>
			<p>I have used Shutterstock for some time now when it comes stock images but I've only recently started playing around with their publishing program, hoping to make some money from some of my photos and videos. As with any other channel&nbsp;that I am considering for inclusion in my&nbsp;line-up of tools and services, I am spending time going through their platform and evaluate the tech, business, and political considerations of adding any new service to work into my world.&nbsp; First, a service should always have an API. This isn't just because of what I do for a living&nbsp;and my obsession with APIs. This is so that I can integrate seamlessly with my existing operations. Another side of this&nbsp;argument&nbsp;is that I will always be able to get my data and content out of a system, but I am working to be a little more proactive than that. I want my system, that operates within my domain to be the lead, and any new channel I adopt only play second fiddle. In this scenario, each photo or video that I publish to Shutterstock will live within my image and video systems&nbsp;and then&nbsp;with the Shutterstock API, I will publish to the Shutterstock domain as I deem worthy.&nbsp; The Shutterstock API (potentially) gives me more access and control over my digital bits&nbsp;and allows me to do more with fewer resources. I do not have to depend on APIs, or a platform's data portability to get my data and content out, I've always possessed this control and ownership from the beginning. Then this control and ownership is&nbsp;now exercised and strengthened in three areas: technology, business, and politics. I technical have control over my bits. I have business control over where they are sold, by whom, and how much of the action they get. I have political control, and when I want to change, evolve, or end the relationship I can do what I think is best for me, and my API...[<a href="/2017/01/09/evaluating-a-new-channel-for-publishing-my-bits/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_06_at_11.14.26_am.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/06/algorithmia039s-multiplatform-data-storage-solution-for-machine-learning-workflows/">Algorithmia&#039;s Multi-Platform Data Storage Solution For Machine Learning Workflows</a></h3>
			<p><em>06 Jan 2017</em></p>
			<p>I've been working with Algorithmia to manage a large number of images as part of&nbsp;my algorithmic rotoscope side project, and they have a really nice omni-platform approach to allowing me to manage my images and other files I am using in my machine learning workflows. Images, files, and the input and output of heavy object is an essential part of almost any machine learning task, and Algorithmia makes easy to do across the storage platforms we use the most (hopefully).&nbsp; Algorithmia provides you with local data storage--pretty standard stuff, but they also allow you to connect your Amazon S3 account, or your Dropbox account, and connect to specific folders, buckets, while helping you handle all of your permissions. Maybe I have my blinders on with this because I heavily use Amazon S3 as me default online storage, and Dropbox is my secondary store, but I think the concept still is worth sharing.. This allows me to seamlessly manage the objects, documents, files, and other images I store across my operation as part of my machine learning workflow. &nbsp;Algorithmia even provides you with an intuitive way of referencing files, by allowing each Data URI to uniquely identifies files and directories, with each composed of a protocol and a path, with each service having its own unique protocol: data:// Algorithmia hosted data dropbox:// Dropbox default connected accounts S3:// Amazon S3 default connected account This approach dramatically simplifies my operations when working with files, and allows me to leverage the API driven storage services I am already putting to work, while also taking advantage of the growing number of algorithms available to me in Algorithmia's catalog. In my algorithmic rotoscope project I am breaking videos into individual images, producing 60 images per second of video, and uploading to Amazon S3. Once images are uploaded, I can then run Algorithmia's Deep Filter algorithm against all images, sometimes thousands of images, using their text models, or any of the 25+...[<a href="/2017/01/06/algorithmia039s-multiplatform-data-storage-solution-for-machine-learning-workflows/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/company/logos/amazon-aws-logo.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/05/what-i-learned-crafting-api-definitions-for-66-of-the-amazon-web-services/">What I Learned Crafting API Definitions For 66 Of The Amazon Web Services</a></h3>
			<p><em>05 Jan 2017</em></p>
			<p>I just finished crafting API definitions for 66 of the Amazon Web Services. You can find it all on Github, indexed with an APIs.json. While I wish all API providers would do this hard work on their, I do enjoy the process because it forces me to learn a lot of each API, and the details of what providers are up to. I learned quite a bit about Amazon Web Services going through the over 2000 paths that are available across the 66 services.  The Importance Of Consistency Across TeamsWhen you bounce from service to service within the AWS ecosystem you can tell that consistency is a challenge for Amazon. Consistency is lacking in API design, documentation, and other critical areas. This is something that is actually getting worse with some of their newer projects. While the older AWS APIs aren't the best possible design because they are: "?Action= based", at least they are consistent, and the documentation is using the same template. Some of the newer APIs are better designed, but their documentation is all over the place, and they are deviating from the consistency that seemed to exist with some of the older API efforts.   Clear Picture Of Essential Building BlocksThere are a variety of building blocks employed in support of AWS APIs, but there is a pretty clear definition of what are considered to be the essential building blocks that exist across ALL AWs APIs: Documentation - Overall, developer, and API documentation to support the services. Getting Started - What you need to get up and going with any of the AWS solutions. Frequently Asked Questions - A list of the frequently asked questions asked of each service. Pricing - The pricing for using each service, with some providing a calculator to assist. Amazon also provides a centralized blog, code, support, and what I'd consider to be essential building blocks, and some of the individual services do a good job linking to these...[<a href="/2017/01/05/what-i-learned-crafting-api-definitions-for-66-of-the-amazon-web-services/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-broken-gears.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/04/explaining-to-normals-why-every-api-is-different/">Explaining To Normals Why Every API Is Different</a></h3>
			<p><em>04 Jan 2017</em></p>
			<p>I enjoy having conversations with "normals" about APIs, especially when they approach me after doing a great deal of research, and are pretty knowledgeable about the landscape, even if they may lack deeper awareness around the technical details. These conversations are important to me&nbsp;because it is these folks that will make the biggest impact with APIs--it won't be the entrepreneurs, developers, architects, and us true believers. While having one of these conversations yesterday, the topic of API design came up, and we were talking about the differences between seemingly similar APIs like Flickr and Instagram, or maybe Twitter and Facebook. I was asked, "why are these APIs are so different? I thought the whole thing with APIs is that they are interoperable, and make integration easier?" &lt;&lt; I love getting asked this&nbsp;because it helps me see the API space for what it is, not the delusion that many of us API believers are peddling.&nbsp; So why are the designs of APIs so different, even between seemingly similar APIs? Integration - APIs make integration into web, mobile, and devices apps easier. It will also make integration with other systems easier. However, very few API providers truly want their APIs to work seamlessly with the competition! Silos - Many API providers operate in silos, and I have encountered teams who do almost no due diligence on existing API design patterns, standards, or even looking at the potential competition, and what already exists before crafting their API design strategy.&nbsp; Intellectual Property - Not many folks see the separate between their API design, the naming, ordering, and structure of the interface, and their backend API code, resulting in some distorted views of what is proprietary and what is not. Venture Capital - The investors in many of the companies behind APIs are not interested in being open and interoperable with others in their industry, resulting in a pretty narrow, and selfish focus when it comes to API design patterns....[<a href="/2017/01/04/explaining-to-normals-why-every-api-is-different/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-traffic-light.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/04/api-calls-as-opposed-to-api-traffic/">API Calls as Opposed to API Traffic</a></h3>
			<p><em>04 Jan 2017</em></p>
			<p>
I was doing some planning around a potential business model for commercial implementations of OpenReferral, which provides Open211 open data and API services for cities, allowing citizens to find local services, and I had separated out two types of metrics: 1) API calls &nbsp;2) API traffic. My partner in crime on the project asked me what the difference was, looking for some clarification on how it might possibly contribute to the bottom line of municipalities looking to fund this important open data work.
So, what is the difference between API call and API traffic in this context?

API Call - This is the measurement of each call made to the API by web, mobile, and device applications.
API Traffic - This is the measurement of each click made via URLs / URIs served up as part of any API response.

In this context, we are looking to provide municipalities, non-profit organizations, and even commercial efforts that are delivering 211 services in cities around the world. I am not suggesting that every bit of revenue and profit be squeezed out of the operation of these important services, I am simply suggesting that there are ways to generate revenue that can become important in keeping services up and running, and impact the quality of that services--it takes money to do this stuff right.
Think of API traffic like an affiliate program or in service of lead generation. This approach requires the usage of some sort of URL shortener services&nbsp;so that you can broker, and measure each click made on a link served up by an API. This opens up other security and privacy concerns we should think about, but it does provides a potential layer for generating valuable traffic to internal, and partner web and mobile applications. This is just one of several approaches I am considering when we are thinking about monetization of open data using APIs.


[<a href="/2017/01/04/api-calls-as-opposed-to-api-traffic/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_04_at_12.13.05_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/04/a-glimpse-at-minimum-bar-for-business-api-operations-in-2017/">A Glimpse At Minimum Bar For Business API Operations in 2017</a></h3>
			<p><em>04 Jan 2017</em></p>
			<p>
I look at a lot of API portals and developer areas , and experience a number of innovative approaches from startups, as well as a handful of leading API providers, but the Lufthansa Airlines API portal (which recently came across on my radar) I feel represents the next wave of API providers, as the mainstream business world wakes up to the importance of doing business online in a machine readable way. Their developer program isn't anything amazing, &nbsp;it's pretty run of the mill, but I think it represents the minimum bar for SMB and SMEs out there in 2017.
The Lufthansa developer portal has all the basics including documentation, getting started, an application showcase, blog, and they are using Github, Stack Overflow, Twitter, and have a service status page. They provide APIs for their core business resources including cities, airports, aircraft, and the schedule of their flights. Honestly, it is a pretty boring, mundane representation of an API, something you are unlikely to find written up in Techcrunch, but this is why I like it. In 2017, we are getting down to the boring business of doing business on the web (maybe security will come soon?).
I'm hoping this is what 2017 is all about when it comes to APIs--where average small businesses and enterprises getting their API operations up and running. Its is like 2002 for websites, and 2012 for mobile--APIs are what you do if you are doing business online in 2017. They aren't the latest tech trend or fad, it is about acknowledging there are many applications and systems that will need to integrate with your resources, and having simple, low cost, machine-readable APIs is how you do this. Let's all get down to business in 2017, and leave behind the hype when it comes to the API life cycle.
[<a href="/2017/01/04/a-glimpse-at-minimum-bar-for-business-api-operations-in-2017/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/valleyrivertreeline/clean_view/file-00_01_06_83.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/03/learning-about-machine-learning-apis-with-my-algorithmic-rotoscope-work/">Learning About Machine Learning APIs With My Algorithmic Rotoscope Work</a></h3>
			<p><em>03 Jan 2017</em></p>
			<p>I was playing around with Algorithmia for a story about their business model back in December, when I got sucked into playing with their DeepFilter service, resulting in a 4-week long distraction which ultimately became what I am calling my&nbsp;algorithmic rotoscope&nbsp;work. After weeks of playing around, I have a good grasp of what it takes to separate videos into individual images, applying the&nbsp;Algorithmia&nbsp;machine learning filters, and reassembling them as videos. I also have several of my own texture filters created now using the AWS AMI and process provided&nbsp;Algorithmia--you can learn more about algorithmic rotoscope, and details of what I did via the Github project updatese. The project has been a great distraction from what I should be doing. After the election, I just did not feel like doing my regular writing, scheduling of Tweets, processing of press releases, and the other things I do on a regular basis. Algorithmic Rotoscope provided a creative, yet a&nbsp;still very API focused project to take my mind off things during the holidays. It was a concept I couldn't get out of my head, which is always a sign for me that I should be working on a project. The work was more involved than I anticipated, but after a couple weeks of tinkering, I have the core process for applying filters to videos working well, allowing me to easily apply the algorithmic textures. Other than just being a distraction, this project has been a great learning experience for me,&nbsp;with several aspects keeping me engaged: Algorithmia's Image Filters&nbsp; - Their very cool DeepFilter service, which allows you to apply artistically and stylish filters to your images using their API or CLI, providing over 30 filters you can use right away. Training Style Transfer Models - Firing up an Amazon GPU instance, look through art books and find interesting pieces that can be used to train the machine learning models, so you can define your own filters. Applying Filters To Images...[<a href="/2017/01/03/learning-about-machine-learning-apis-with-my-algorithmic-rotoscope-work/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/algorithmia_pricing_how_it_works.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/01/03/exploring-the-economics-of-wholesale-and-retail-algorithmic-apis/">Exploring The Economics of Wholesale and Retail Algorithmic APIs</a></h3>
			<p><em>03 Jan 2017</em></p>
			<p>I got sucked into a month long project applying machine learning filters to video over the holidays. The project began with me doing the research on the economics behind Algorithmia's machine learning services, specifically the DeepFilter algorithm in their catalog. My algorithmic rotoscope work applying Algorithmia's Deep Filters to images and drone videos has given me a hands-on&nbsp;view of Algorithmia's approach to algorithms, and APIs, and the opportunity to think pretty deeply about the economics of all of this. I think Algorithmia's vision of all of this has a lot of potential for not just image filters, but any sort of algorithmic and machine learning API. Retail Algorithmic and Machine Learning APIsUsing Algorithmia is pretty straightforward. With their API or CLI you can make calls to a variety of algorithms in their catalog, in this case their DeepFilter solution. All I do is pass them the URL of an image, what I want the new filtered image to be called, and the name of the filter that I want to be applied. Algorithmia provides an API explorer you can copy &amp; paste the required JSON into, or they also provide a demo application for you to use--no JSON required.&nbsp; Training Your Own Style Transfer Models Using Their AWS AMIThe first "rabbit hole" concept I fell into when doing the research on Algorithmia's model was their story on creating your own style transfer models, providing you step by step details on how to train them, including a ready to go AWS AMI that you can run as a GPU instance. At first, I thought they were just cannibalizing their own service, but then I realized it was much more savvier than that. They were offloading much of the costly compute resources needed to create the models, but the end product still resulted in using their Deep Filter APIs.&nbsp; Developing My Own API Layer For Working With Images and VideosOnce I had experience using Algorithmia's deep filter...[<a href="/2017/01/03/exploring-the-economics-of-wholesale-and-retail-algorithmic-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/target_v_pandora.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/11/16/pandora-vs-target-when-considering-how-public-to-be-with-your-api-operations/">Pandora vs Target When Considering How Public To Be With Your API Operations</a></h3>
			<p><em>16 Nov 2016</em></p>
			<p>I am reworking the API Evangelist developer area, and shifting most of my content to be available as YAML and JSON data on the Github repositories that drive my network of sites. I'm doing this partly because I am not in the business of managing and growing an API community, and because there are some really badly behaved people out there that I'm just not really interested in having keys to my internal network. I am happy to open up read only access to my work publicly and write capabilities to my trusted partners, but having self-service access to my server(s) just isn't fun in the current online climate.&nbsp; I get it when folks want to keep their valuable data, content, and algorithm under lock and key, and require developers to build a relationship before they get access&nbsp;or increased levels of consumption. However, this hasn't always been the tune I'm whistling. There are plenty of examples of me telling API providers to provide self-service access to their resources in the past--well, we've fucked that off, with our bad behavior as API consumers. It's not that the concept won't work, it is just that it won't work with the number of assholes on the web these days it just isn't a good idea. Even with keeping the lock and key on our API resources, we can still be public with our API operations--there are many positive reasons for doing so. SEO is probably the first, I mean how are people going to find your APIs if you can't Google for them. Providing information to the press, and making the resources that support your APIs self-service can reduce&nbsp;the workload when you do give people access. Some examples of this can be found when looking at the Target vs. Pandora developer programs, both required approval to get access, but Target is much more open than Pandora with their overall story. Target really doesn't have that much more than...[<a href="/2016/11/16/pandora-vs-target-when-considering-how-public-to-be-with-your-api-operations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-focused-targed.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/11/16/focusing-on-a-single-example-of-what-an-api-is-when-onboarding-folks/">Focusing On A Single Example Of What An API Is When On-Boarding Folks</a></h3>
			<p><em>16 Nov 2016</em></p>
			<p>
Talking to people, and telling stories on a regular basis always pushes me to evolve my understanding of how people see (or don't see) APIs, and pushes me to keep shifting the way I tell stories. I've always felt that that education about APIs should be relevant to the users, but usually this centers around making it familiar,&nbsp;and speak to whoever I am helping onboard. After talking to some folks recently at @APIStrat, I'm adding to my thinking on this, and focusing on making my stories more precise for folks I talk with.
One of the reasons I really like APIs is that they are so versatile. You can take and a piece of data, content, or an algorithm (code), and wrap with an API, and provide read and write access via the Internet. However, I think the average person does not thrive in this environment and need an explanation that is more precise. It doesn't help them to know that APIs can do anything, they need it to be relevant, but also providing a single solution that they can wrap their heads around, and apply in their world.
I am just sharing this thinking as I'm working to add it to my storytelling toolbox. I'm really committed to helping people understand what APIs are, so they can help push back on platforms to have APIs, as well as be more open with existing APIs. It doesn't do any good to confuse people with an unlimited number of API scenarios. We should be dialing in our storytelling so that we can help onboard them with the concept, and increase the number of folks who understand that they can take control over their own information on the platforms they depend on each day.
[<a href="/2016/11/16/focusing-on-a-single-example-of-what-an-api-is-when-onboarding-folks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_11_14_at_11.34.48_am.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/11/14/the-taiwanese-government-posts-an-apis-json-index/">The Taiwanese Government Posts An APIs.json Index</a></h3>
			<p><em>14 Nov 2016</em></p>
			<p>
My friend and partner in crime Nicolas Greni&eacute; (@picsoung), and operator of our open source API search engine APIs.io, just let me know that the Taiwanese government just added an APIs.json&nbsp;file for their government open data site. Adding to the other authoritative (added by owner) government API indexes like from Trade.gov in the United States.
We haven't' had a lot of time to move forward the APIs.json&nbsp;index lately, but honestly it doesn't need much pushing forward at this point in time. Our primary objective is to continue getting adoption like this, and not radically shift the specification until we have more feedback from the community, across a large number of API operators.
You can learn more about the specification we've been working on over at apisjson.org, and see other existing implements from Fibit, Plivo, and Actuity&nbsp;Scheduling. Once you've created an APIs.json&nbsp;definition for your API operations you can add to the APIs.io search engine using the submission form, or using the APIs.io API. We are gearing up for another push forward regarding tooling being developed on the specification over the holidays and will have more information in 2017.&nbsp;
[<a href="/2016/11/14/the-taiwanese-government-posts-an-apis-json-index/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://s3.amazonaws.com/kinlane-productions2/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/11/14/the-api-evangelist-mission-continues/">The API Evangelist Mission Continues</a></h3>
			<p><em>14 Nov 2016</em></p>
			<p>I tried to get back to normal last week on API Evangelist -- I failed. The previous week was @APIStrat in Boston, which was a success. It was the Presidential election that caused me to swerve and put things into the ditch. I was devastated and saddened by the results. Not because my party lost, but because we chose someone who ran on such a racially, and religiously&nbsp;charged platform, that was so threatening to women.&nbsp; It is easy to mistake what I do as the API Evangelist, as being a voice for the startup community -- cheerleading APIs in the service of the seemingly&nbsp;endless wave of tech companies coming out of Silicon Valley. While I do pay attention to the technology, and business of how these companies are using APIs, making sure the content, data, and algorithms they employ are transparent and observable by partners, 3rd party developers, end-users, and industry regulators is my primary objective. My mission is not about open APIs because open always makes things better, or simply open for business. I believe corporate, government and institutional data, content, and algorithmic resources should be as transparent and observable as possible, by those who are impacted by their existence. Meaning if you are collecting data about me, I should know what you are collecting, and have access to it. I should be able to move it around or delete it. Trusted regulators and auditors should also be able to peek behind the curtains of the algorithms that are increasingly impacting our world, and make sure they size up with the claims being made. I have spent six years pushing on startups and the enterprise to be more transparent and inclusive with their resources by employing APIs. During this time I did the same for the city, state, and the federal government. I've also extended this to higher educational institutions. With Donald Trump in office, this does not change, it just ups the stakes...[<a href="/2016/11/14/the-api-evangelist-mission-continues/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/stoplight_scenarios.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/11/14/daisy-chaining-multiple-api-paths-using-stoplight-scenarios/">Daisy Chaining Multiple API Paths Using Stoplight Scenarios</a></h3>
			<p><em>14 Nov 2016</em></p>
			<p>There aren't too many startups doing interesting things in the API space right now. One of the exceptions is Stoplight.io. I am working really hard to find some of the good things in the API space to focus on, in hopes of not being too dark with my writing, after the election. Stoplight's new Scenarios is something new, something progressive, and I think could have a larger impact down the road--making it worth covering here on API Evangelist. Stoplight Scenarios is billed as "Test, automate, and debug web APIs + AWS Lambda". You can make API and AWS Lambda calls, and test, automate, and debug the responses -- pretty standard stuff we are seeing across several API service providers. Where it gets interesting for me is that you can daisy chain these requests together into a variety of scenarios. You can do this with a single API, or you can do it across a variety of disparate APIs--making for some pretty valuable "juju" in my opinion. As I am learning about a new API, I am often frustrated by having to connect the dots between many different paths, adding, then pulling, then updating, and other common "scenarios" I will want to accomplish. I would love for API providers to do this heavy lifting for me, and provide me with a variety of the common scenarios I need as an API consumer to get up and going. This is just the local possibilities around using scenarios, I'll explore the possibilities between many distributed APIs in future posts. If you haven't played with Stoplight before, I recommend heading over there. Their API design, definition, mocking, and documentation tooling is leading edge stuff in the world of APIs right now. The Stoplight Scenarios just ups the value their platform brings to the table, continuing to make them on of the few bright spots in the API world for me these days. Let me know what you think after...[<a href="/2016/11/14/daisy-chaining-multiple-api-paths-using-stoplight-scenarios/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/steve_and_i_apistrat_2016.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/11/10/maintaining-the-api-community-at-scale-apistrat/">Maintaining The API Community At Scale #APIStrat</a></h3>
			<p><em>10 Nov 2016</em></p>
			<p>The 7th edition of API Strategy &amp; Practice wrapped up last week. It has been difficult to gather my thoughts with the election going on, but I wanted to shift my attention back to the API community for a bit. Steve Willmott and I started APIStrat back in 2012 to help establish an open, vendor-neutral community to discuss the technology, business, and politics of APIs -- we more than succeeded! It always warms my heart when people come up to me and share that they didn't see anything different about APIStrat, from other events before they attended&nbsp;and that they do now. Steve and I worked our asses off to make it a rich, inclusive, and meaningful conversation around APIs. It makes me very happy to hear people seeing the event as we intended, and going home with a headful of API knowledge. One thing I heard from several of the core members who have been there since the first one, and is something that has lingered in my mind, not just because of APIStrat, but also because of where are at in the wider API space -- is that the API world is expanding, and there are concerns regarding how we can maintain community. I consider our hard work in evangelizing APIs to be a success, as space is expanding like never before. Everybody is doing APIs, it isn't just a niche anymore. We are at the point we were with the web in 2001, and folks aren't asking if they should be doing APIs anymore--they are just doing them. Many of us in the original group are tired. We've been at this for a while. Many of the younger ones are still easily excited and distracted by new technology. There are many new entrants who just need to hear the same old stories we've been telling for years so that they can just get up to speed. Many people expressed their concerns around maintaining...[<a href="/2016/11/10/maintaining-the-api-community-at-scale-apistrat/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://apistrat.com/wp-content/uploads/2014/12/apisrtrat-logo.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/11/10/how-do-you-work-towards-a-more-diverse-inclusive-tech-conference/">How Do You Work Towards A More Diverse Inclusive Tech Conference?</a></h3>
			<p><em>10 Nov 2016</em></p>
			<p>The 7th edition of API Strategy &amp; Practice Conference happened last week. While I wasn't fully engaged throughout the planning process for this edition, due to my summer being disrupted, I wanted to take the time to share some of what happened to make it more of an inclusive technology event. There is a lot of people who are "interested" in making their events more diverse and inclusive, but APIStrat is "committed" to this (thanks, Charles Ashley III @CAsh_The3rd), and here are some of what we did. Strong Female Lead - Put a woman in charge. Period. She will set a good tone. Invite Women To Speak - Work to ONLY invite women when getting started. Invite People of Color To Speak - Work to ONLY invite people of color when getting started. Have Code Of Conduct - Make there is a code of conduct present, and communicated well. Enforce Code of Conduct - Sadly, we had to do&nbsp;this&nbsp;round, but it sets the right tone. No Manels - Do not have any panels where you only have men. Numbers&nbsp;- Know your numbers, and work every moment to increase them wherever you can. Repeat, Rinse - Repeat all of this at all levels, session chairs, keynotes, reg counter, etc. We are nowhere near where we want to be with making #APIStrat a truly inclusive event, but we are making improvements with each edition. We have our checklist and have been building on it with each event. It takes a leadership team that is committed to this. Steve, Lorinda, Amelia, and the team delivered this round -- I wish I could take credit. I saw more women, diverse faces, and topics at #APIStrat this round -- leaving me very, please. If you are running a tech conference, please put in the extra work. You can't just give this 5 or 10% effort. You literally have to invite NO WHITE MEN to your event for the first couple...[<a href="/2016/11/10/how-do-you-work-towards-a-more-diverse-inclusive-tech-conference/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/kin_drinking.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/11/04/drone-recovery-in-the-attention-economy/">Drone Recovery In The Attention Economy</a></h3>
			<p><em>04 Nov 2016</em></p>
			<p>Difficult To Keep My AttentionWhen I was young I was always curious when it came to technology. I set up the entire computer lab for my 7th-grade math teacher back in 1983. I programmed computers all through high school, even having a job programming the software used by schools in the State of Oregon in COBOL. I was really good at&nbsp;school&nbsp;until I wasn't interested. If I got bored, which I did growing up in rural Oregon, I tended not to pay&nbsp;attention&nbsp;in school. Eventually, it got kicked out of high school in my senior year, and I ended up getting into drugs, and a lot of trouble. From 1990 through 1995 I spent my time traveling the country partying and dealing drugs until I finally hit a wall and needed a way out. To get my life out of the ditch I turned to what I already knew, the outdoors (I grew up out in the woods), and computers. I was good at paying&nbsp;attention&nbsp;to the bits and bytes in the emerging world of personal computing, and I would leverage technology to get me out of this mess I found myself in. Your browser does not support the video tag. Attention To Family &amp; CareerAfter spending a summer in the Oregon wilderness getting clean and healthy, I moved to the nearest city&nbsp;and got to work building a career. By the first&nbsp;dot com bubble, I had found success, married a young lady, and had a beautiful baby girl. I had left my troubled past behind. It was a period in my life where there the harder I worked, the better I felt. I had a good&nbsp;job and bought a house (two), but my attention always seemed to be on finding further business success, a sort&nbsp;of chronic entrepreneurial condition,&nbsp;resulting in having at least one, and often times multiple startup projects going on at any point in time. In total, I had almost 14 separate startups with only two I...[<a href="/2016/11/04/drone-recovery-in-the-attention-economy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_28_at_12.52.27_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/28/we-will-need-machine-readable-transparency-report-info-via-apis/">We Will Need Machine Readable Transparency Report Info Via APIs</a></h3>
			<p><em>28 Oct 2016</em></p>
			<p>
I was reading the latest Yahoo transparency report, as well as the Tumblr. When a company releases their latest version of this data, it tends to prompt me to take a look at some of the other providers who have them, like Google and Twitter. I am interested in what I can learn from these reports, about what the government is up to, as well as the incentives behind each platform publishing their reports. I fascinated by studying the process and approach of each company, tracking on what some of the common building blocks so that I can include in my API transparency research.
The addition of CSV downloads of information requests by Twitter is worth noting, and adding to my list of building blocks. Platform providers are going to have to consider adding machine-readable versions of their transparency reports to help address the growing number of requests they will bet getting in this area. Providing CSV, JSON, or even YAML access to transparency report information is going to become important for us to understand how this legal and very political layer of our worlds is evolving.

The number of government and court requests for information are up. The number of companies publishing transparency reports in response to public demand for transparency is increasing. Providing machine readable representations of this data, as well as evolving a common API and data schema definition is the only way we are going to be able to manage this growth, and make sense of the data across platform providers. I will keep adding company transparency reports to my research, and take the time to aggregate the common elements in play across them, and see if I can't contribute to this common API and schema definition for use by providers
[<a href="/2016/10/28/we-will-need-machine-readable-transparency-report-info-via-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/copy_of_united_states_data_map.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/28/a-drone-law-api-for-use-in-planning-and-at-flight-time/">A Drone Law API For Use In Planning And At Flight Time</a></h3>
			<p><em>28 Oct 2016</em></p>
			<p>&nbsp; Photo: Drones and Society I went down to the police department in Hermosa Beach and filed my application for a drone permit. It's been two weeks and I haven't heard back. When I get done with @APIStrat I will go down there and talk to them again, and probably have to resubmit my application. Hermosa Beach is purported to have some of the strictest laws. I'm submitting mine so I can just play with my drone, and program it on the beach without getting into trouble.&nbsp; Before I fly my drone I use the B4UFLY mobile app from the Federal Aviation Administration to know whether I should fly my drone or not. It tells me whether I'm near an airport, military base, other location or event. This helps, but it doesn't help me with the legal side of things, what the laws are in my area, and any assistance in acquiring permits and approval. The industry is young, I'm sure eager startups out there are already working their asses off to aggregate the data, and serving it up via APIs for use in mobile and drone applications. As an operator, I am going to need the laws applying to the drone, bu also laws applying to video, and other data gathered. Can I be filming? What are private / public property data and privacy laws? I am gathering video, weather, temperature, and other crop data over commercial farms, am I allowed to keep it? What are the rules with other types of imaging&nbsp;like infrared? Are there any noise ordinances? There are numerous considerations when flying a drone for both personal and commercial scenarios. To be an informed pilot I need all this at run(flight)time. I guess I might also need some of it before I head out to my flight location, but also whatever I can also get at flight time. The drone sector seems to begin the process of tackling many of the...[<a href="/2016/10/28/a-drone-law-api-for-use-in-planning-and-at-flight-time/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/pricing_and_faq_nanoscale_io.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/27/why-upgrade-to-a-paid-plan/">Why upgrade to a paid plan?</a></h3>
			<p><em>27 Oct 2016</em></p>
			<p>
I thought the microservices platform Nanoscale.io have an interesting argument for why you would upgrade to a paid plan. On their pricing page, after they break down each of the pricing plans they provide you with four reasons of why you would want to upgrade from their free tier.

More powerful APIs - Your nanoscale.io hosted microservices can run longer and perform more complex operations, or access slower source systems, without timing out.
Get premium support - Having a problem with nanoscale.io? Think you may have found a bug? Submit your inquiry and get a guaranteed response from our technical team.
Influence product roadmap - We are open to new feature consideration, and give preference to paid accounts to influence which ones get built sooner.
Deploy anywhere - Upgrade your downloadable nanoscale.io server with a production license to deploy your microservices on any infrastructure you choose.

More power and support seems like no-brainers. Being able to influence the roadmap is a compelling reason and something I would pay money for! ;-) The deploy anywhere I think is a sign of the future, not just for how you will buy services for your API, this is how you will deploy your APIs for your consumers. In cloud. On-premise. On-device. You can consume our API anywhere you want--if you pay for it!
I have been aggregating the plans and pricing of API providers, and service providers for a while now. People are getting better at providing a decent breakdown of their API plans, but they aren't always that good at articulating the reasons why you would go from free to a paid plan. I think this is an area of stress, and concern for many API providers, and an area they could use more examples to follow from out in the wild.
[<a href="/2016/10/27/why-upgrade-to-a-paid-plan/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/68747470733a2f2f7261772e6769746875622e636f6d2f4e6574666c69782f64656e6f6d696e61746f722f6d61737465722f64656e6f6d696e61746f722e6a7067.jpeg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/27/tooling-to-help-aggregate-dns-across-multiple-service-providers/">Tooling To Help Aggregate DNS Across Multiple Service Providers</a></h3>
			<p><em>27 Oct 2016</em></p>
			<p>Adrian Cockroft (@adrianco) turned me on to a DNS aggregation solution the other day while I was working on updating the API definitions for the API providers that are included in my API DS research. It was a very appropriate day for thinking deeply about aggregate DNS, with the DDOS attack against Dyn going on.

DNS provider redundancy: the idea behind @denominatorOSS - one API/tool for many providers to allow switching. /cc @adrianfcole
&mdash; adrian cockcroft (@adrianco) October 21, 2016
Denominator&nbsp;is a portable Java library for manipulating DNS clouds. It has pluggable backends, including AWS Route53, Neustar Ultra, DynECT, Rackspace Cloud DNS, and OpenStack Designate. Here is a good post on it from back in 2013, describing it as a multi-vendor interface for DNS.&nbsp;

There doesn't look to be a lot of activity around the project in the last year, but it provides a good model for what I'd like to eventually see across all the major stops along the API lifecycle. I picture a wealth of aggregate tooling like Denominator that can act as a broker between API service providers&nbsp;and help switch, migrate, and sync between providers whether you are deploying, managing, testing, monitoring, or dialing in your DNS.
As I read the multiple investigations into what happened with the DDOS attack on Dyn last week, it seems relevant to learn more about aggregate DNS API solutions like Denominator. I will spend some time looking for other similar open tooling that is vendor-neutral, as well as vendor-switchable. We are going to need open source circuit breakers like this to help route, switch, migrate, and sync DNS across many service providers in this volatile landscape.
[<a href="/2016/10/27/tooling-to-help-aggregate-dns-across-multiple-service-providers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_26_at_9.05.40_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/27/allowing-for-relationships-between-api-developers-at-the-app-level/">Allowing For Relationships Between API Developers At The App Level</a></h3>
			<p><em>27 Oct 2016</em></p>
			<p>
Managing developers access to an API is API management 101. Managing the relationships between developers, and allowing for multiple users associated with an API application isn't something I have seen before. Slack just added the ability to add what they call app collaborators--"adding any existing member of your Slack team as a Collaborator, including members and&nbsp;Guest Accounts."
The functionality felt a lot like the social aspects that made Github more attractive than just Git. When it comes to developing messaging apps and bots I can see a social layer doing pretty well. "Once a Collaborator is added, they&rsquo;ll receive a notification from Slackbot letting them know they now have access to your app." Smells like an opportunity for API management providers to bake into user management solution, and if you wanted to take it to the next level, add a Slack messaging layer as option.
Its a small feature, but I think it is one of the things that made Github work, and could have a similar impact at the API management level, allowing for more engagement between users who are working together on API integration. I am going to add it as a building block for my API management research, even if I haven't seen it elsewhere. It is something I'd like to see more of, and maybe if I plant the bug, more providers will implement.
[<a href="/2016/10/27/allowing-for-relationships-between-api-developers-at-the-app-level/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-device-data.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/26/transparent-data-transfer-control-apis-at-the-iot-device-level/">Transparent Data Transfer Control APIs At The IoT Device Level</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>I am diving deep into the DJi drone developer platform, and one of the elements of the DJi Drone Guidance API that caught my attention was the data transfer control methods. In this situation, the transfer control methods are designed for just the data being sent as part of the drone guidance systems, but I think it provides a blueprint that can be used across almost any IoT device connectivity. DJI provides four methods for managing the drone data transfer control: Start Transfer -&nbsp;Inform guidance to start data transfer. Stop Transfer -&nbsp;Inform guidance to stop data transfer. Release Transfer -&nbsp;Release the data transfer thread. Wait For Board Ready -&nbsp;Set callback function handler for hen data from guidance comes, it will be called by data transfer thread. The "wait for board ready" method acts as a sort of web hook, that can notify any application build on the API that data is now being transferred, opening up the possibilities for notifying a device owner, and operator that data is being transferred. To me, this can be a critical aspect of building trust that our devices have our best interests in mind, providing some essential transparency in the data layer of the IoT space. Data transfer control APIs for IoT devices like this will not ensure healthy data practices. This implementation is designed to provide transfer control capabilities to the developer, it is now up to the developer to include the end users in this process. Many current mobile application business models do not incentivize this type of transparency, as you do not want end users, and often times 3rd party developers involved in data gathering, and revenue generation this valuable "exhaust" of Internet-connected devices. I am hoping this evolves and changes as the Internet matures, and the number of connected devices increases. We need transparency at the device data transfer level, and we need all humans involved / impacted to be a literate, active, and will...[<a href="/2016/10/26/transparent-data-transfer-control-apis-at-the-iot-device-level/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_25_at_1.08.16_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/26/potential-for-apis-to-target-us-online-by-adding-more-context/">Potential For APIs To Target Us Online By Adding More Context</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>Many folks see me simply as a cheerleader for APIs&nbsp;when in reality I am more of an evangelist for the bad that can happen with APIs. I believe that sharing of data, content, and algorithms using web APIs has the potential for good, but in reality, they are often be used for doing some pretty shady shit.&nbsp; An example of this is found in my inbox this morning, and I'm sure is something everyone will encounter at some point in their daily lives. It is an email for an undelivered Fedex package, which I know better than to click on, but sadly I think it is one that many folks will fall for. Why do they fall for this? Because the email potentially has relevance, as I just ordered a handful of packages from Amazon, which were being shipped via Fedex (I do not order much online). Using the FedEx&nbsp;API, anyone can query the status of a&nbsp;package. I'm assuming that there are&nbsp;folks out there who are scanning for the presence of delivering notifications--I'm not up to speed on the details of how you can do this. I'm unsure if they can get my email alongside this information, but I don't think this matters. I think they can correlate data about where I live, and the fact I'm receiving packages--whether the email came from API, or through other forms intelligence, it doesn't matter.&nbsp; My point is more around the fact that APIs are increasingly opening up signals about our daily lives, providing a wealth of context for phishing campaigns, increasing the chance that people will fall for these attacks. My solution to this problem does not involve a knee-jerk response to providing APIs, I am just looking to just warn API providers that they should be&nbsp;monitoring for this type of behavior on top of an API, and we should help the average email users&nbsp;and Amazon&nbsp;package&nbsp;receiver that these dangers exist. Everyone should pause and think deeply about...[<a href="/2016/10/26/potential-for-apis-to-target-us-online-by-adding-more-context/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/dji_phantom_3_pro.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/26/learning-the-dimensions-of-the-dji-drone-sdks-and-apis/">Learning The Dimensions Of The DJI Drone SDKs And APIs</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>I am going through the DJi DJI drone developer area which has three distinct SDKs, which allow us to leverage a variety of APIs that make the drone magic happen. I'm still wrapping my head around the intersection of drones and APIs, and this is my attempt to distil down what I'm finding in their developer area, and absorb some of what is going across the industry. This is not &nbsp;meant to be a complete list. It is meant for my learning, and hopefully yours along the way. There are a variety of devices being connected to the Internet, but other than the automobile I don't think there is another object that is as complex as the drone. I'm fascinated by what is possible with this device, and the variety of APIs it has, the interaction with the RC controller, mobile device, and with other resources the clouds. I personally fly a DJI drone, so I am going through the DJI developer area, learning about their three SDKs, as they seem to be the ecosystem furthest along in their understanding the API potential -- think Twitter for IoT. The DJI Onboard SDK&nbsp;This SDK allows for communication with the DJI flight controller over a direct serial connection, to monitor and control aircraft flight behavior with the Onboard API&nbsp;while utilizing the built-in Intelligent Navigation Modes to create autonomous flight paths and maneuvers. Some of the actions for the onboard SDK are: Activation -&nbsp;Before you start exploring DJI Onboard SDK functionality via our ROS examples, you will need to go through the "Activation" process. Obtain/Release Flight Control - Managing the process to get flight control. Take Off - Initiate a take-off&nbsp;for the drone. Landing - Tell the device to land. Go Home - Tell the device to go home. Gimbal Control - Manage gimbal for camera. Altitude Control - Manage the altitude for the drone. Photo Taking - Allow for taking photos Start/Stop Video Recording - Start and...[<a href="/2016/10/26/learning-the-dimensions-of-the-dji-drone-sdks-and-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-anti-social.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/26/asynchonous-conversational-interfaces-for-us-anti-social-folks/">Asynchonous Conversational Interfaces For Us Anti Social Folks</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>I get why people are interested in voice-enabled&nbsp;solutions like Alexa and Siri. I'm personally not a fan of speaking to get what I want, but I get the attraction for others. Similarly,&nbsp;I get why people are interested in bot enabled solutions like Facebook and Slack are bringing to the table, but I'm personally not a fan of the human-led noise in both of these channels, let alone automating this mayhem with bots. In short, I'm not 100% on board that voice and bots will be as revolutionary as promised. I do think they will have a significant impact and are worthy of paying attention to, but when it comes to API driven conversational interfaces, I'm putting my money on push driven approaches to making API magic happen. Approaches like Push by Zapier, and Webtask.io, where you can initiate a single, or chain of API driven events from the click on a button in the browser, in a web page, on my mobile phone, or hell, using the Amazon Dash button approach. These web tasks operate in an asynchronous way, making them more conversational-esque. Allowing those of us who are anti-social, and have adequate air gapped our social and messaging channels, and haven't fully subscribed to the surveillance economy, alternate solutions. These mediums could even facilitate a back and forth, passing machine readable values, until the desired result has been achieved. Some conversations could be predefined or saved, allowing me to trigger using a button at any point (ie. reorder that product from Amazon, retweet that article from last week). I'm not saying I don't want to have an API-enabled conversation, I'm just not sure I want a speaker or bot always present to get what I need to get done in my day. I understand that I am not the norm. There are plenty of folks who have no problem with devices listening around their home or business, and are super excited when it comes...[<a href="/2016/10/26/asynchonous-conversational-interfaces-for-us-anti-social-folks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/gamaya_agriculture_analytics.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/26/apis-helping-drones-generate-alpha-used-in-high-frequency-trading/">APIs Helping Drones Generate Alpha Used In High Frequency Trading</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>One of the things I love about my world as the API Evangelist is the time I get diving into rabbit holes and learning about different areas where technology is being applied. I do not always agree with the business motivations behind what is going on, which can result in some often pretty shady situations, but I enjoy stepping back and understanding the data, API and approaches behind what is going on. I was doing some research on drone APIs recently, and as I was falling down the rabbit hole,&nbsp;I found myself reading about drones being a source of alpha? WTF is alpha? I had no idea and wanted to learn more about what an alpha generation platform was, and how drones and APIs are playing a role--here is the definition I found: An alpha generation platform is a technology solution used in algorithmic trading to develop quantitative financial models, or trading strategies, that generate consistent alpha, or absolute returns. The process of alpha generation refers to generating excess returns. The article that triggered this is about drones generating alpha was focused on the data generation, and communication capabilities of drones, and how they can be used in trading algorithms. Companies are looking to fly over crops, and predict yields, aggregate data, and add to the resources that available in the "alpha generation platform". I'm not convinced of the reality of this approach, but it does provide for some interesting scenarios as I am learning about the data drones can gather, how this transfer occurs to the cloud, and be put to work using existing approaches to video, image, analysis, and visualization APIs. Also, it provides fuel for my alternate design fiction writing, where I explore the possibilities&nbsp;of technology, both good and bad. From what I understand, this type of data would be considered "dirty" in the alpha workflow, in the same category of other data, you might gather from Amazon sales, Twitter sentiment, and...[<a href="/2016/10/26/apis-helping-drones-generate-alpha-used-in-high-frequency-trading/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-revenue-share.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/25/with-mobile-we-are-the-product-with-iot-lets-get-a-piece-of-the-action/">With Mobile We Are The Product -- With IoT Lets Get A Piece Of The Action</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>Internet-connected devices generate data. The most recent wave of mobile devices has opened up an unprecedented world of data generation and harvesting from the network, device, and application layers. The location data, photos, videos, and other valuable exhaust from these devices is why there has been so much investment in technology, and why we are seeing continued investment in the Internet of Things (IoT). When it came to mobile phones this opportunity was new, and it isn't always clear that we are the product when it comes to making money off connecting these devices to the Internet. People aren't always aware of how much data they are generating, and how much this data is generating revenue for the latest generation of entrepreneurs--because it's new. Things have moved along, and it's not a secret anymore that devices connected to the Internet generating data has the potential to be very valuable in the right situation. I have been historically frustrated with people's lack of awareness of this, but I'm hoping that with each wave of technology that comes in the future, we will get smarter about this, and stop being the product when we can, and begin demanding a piece of this action (if we do it at all). If our wearable fitness device is used in any healthcare study, if that weather, water, or pollution sensor in our yard is generating revenue, we should get a piece of the action. If a device in our homes or businesses is generating data, we should be a more aware of, and willing participant in this new supply chain. The average person may not always care about their privacy&nbsp;in the surveillance economy, but maybe they'll care about lost opportunities for making money. At the consumer level this isn't always a coherent&nbsp;argument, but as you approach the work at home world, and into the professional territory, it can begin making more sense. Not all weather and pollution monitors might make...[<a href="/2016/10/25/with-mobile-we-are-the-product-with-iot-lets-get-a-piece-of-the-action/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/datadog_observability_dashboard.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/25/thinking-about-an-api-observability-stack/">Thinking About An API Observability Stack</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>I am learning about observability from reading Stripes post on Veneur, a high performance and global aggregation for Datadog. While the math of it all is over my head, the definition makes a lot of sense and provides me with a nice Venn&nbsp;diagram overlap across several areas of my API research, including testing, monitoring, logging, analysis, and visualization. The Wikipedia definition for observability is: Formally, a system is said to be observable if, for any possible sequence of state and control vectors, the current state can be determined in finite time using only the outputs (this definition is slanted towards the state space representation). Less formally, this means that from the system's outputs it is possible to determine the behavior of the entire system. If a system is not observable, this means the current values of some of its states cannot be determined through output sensors. Stripe provides a great technical breakdown of the tools, services used to establish observability as part of their system operations, but I wanted to step back, and think about observability through a business and political lens. The business imperative for observability might seem clear, as you want as much visibility and awareness into your API operations as you possibly can, so you can provide a reliable level of service. I am thinking about the incentives for extending this observability beyond internal groups, to partners, developers, regulators, or the public--encouraging transparent observability. This moves into the area of API and algorithmic transparency you hear me ranting about regularly, and the benefits APIs bring to the table when you apply in areas like policing and surveillance, or other more regulated sectors of our society. When you take the assertions applied as part of API testing and monitoring practices, and you link them up to this observability stack definition, and open things up beyond just core technical groups, I think we are moving into new territory where we can begin to observe...[<a href="/2016/10/25/thinking-about-an-api-observability-stack/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-certification.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/25/looking-at-the-latest-api-related-certification-stories/">Looking At The Latest API Related Certification Stories</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>As I curate the interesting news from across the API space each week I tag things to put them into different buckets. At the end of each week, I look through each bucket, deciding which area(s) I will be writing about each week. I am always trying to identify patterns and evolve the different areas of my research. One of the areas I'm considering adding as a formal area of API research is when it comes to certification.&nbsp; I have been seeing the area come up quite a bit, making me think it is something I should be thinking more about, and researching&nbsp;as its own project. This week there were four items thought caught my attention: BigML Certifications are Here! - Professional certification on machine learning APIs, from a leading API provider. Better Business Bureau yanks Wells Fargo's accreditation - &nbsp;Ok, not quite certification, but a classic representation and story to consider. Why cybersecurity certifications suck - A look into why certifications can be better, and applied in a critical area of the online world. Eligible Announces SOC2 Certification - Certification than an insurance API provider on their availability, processing integrity, confidentiality, or privacy. These posts came after another four items from last week, providing, even more, to think about when it comes to APIs and certification: A new certification program for Open Source Hardware - I certify that I am truly open source, not that faux open source you see a lot of--we are the real deal! Online Dante Certification Program Now Available - Professional certification on audio APIs, from an older school software provider. Transform your business; become a Google Certified Professional Data Engineer - Professional certification that you know your stuff when it comes to data, specifically data in the Google ecosystem--data engineers are hot. IBM launches Watson application developer certification - Another professional certification for machine learning and artificial intelligence, this time from big blue. There is a lot going on...[<a href="/2016/10/25/looking-at-the-latest-api-related-certification-stories/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-marching-shouting.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/25/api-technology-does-not-have-to-mindlessly-march-forward/">API Technology Does Not Have To Mindlessly March Forward</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>I am seeing more people asking that we put on the brakes when it comes to technology, looking to slow the adoption of new technology, in favor of mastery of the existing, and getting our house in order with the technology we already in play. One of the core tenets of my message as the API Evangelist centers on the importance of doing what we are already doing&nbsp;and doing it better. You can see this message present in my 2014 recommendation on an API strategy for the US federal government--do more of what is already in motion, don't disrupt by just doing the new. I have seen a lot of API technology float by in the last six years of doing API Evangelist. I can still get excited by some of it, but far less than I did in 2010 when I first started. This is partly because I'm tired, but mostly it is because I've seen a lot of shit float by, and it has to be meaningful in some way to get me excited. I am fairly willing to keep an open mind when it comes to microservices, DevOps, GraphQL, voice, bots, drones, and the other technological frontlines, but I refuse to accept that technology has to mindlessly march forward. This is more about selling us new things than it is every about truly bringing us real solutions to everyday problems. Whether it's addressing technical debt and monoliths, the security concerns of the Internet of Things, or anything in between, we should always work to step back and ask if the new technology will actually provide a solution, or create three new problems for each solution it brings. I am not anti new tech, I love new shiny tech objects, but I think for our own sanity we should learn to be more thoughtful. New technology trends can be exciting, and fun to play with, but when it comes to production environments, and...[<a href="/2016/10/25/api-technology-does-not-have-to-mindlessly-march-forward/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/company/logos/uspto.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/25/adding-new-dimension-by-including-patents-in-my-dns-api-research/">Adding New Dimension By Including Patents In My DNS API Research</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>I have been tracking on API related patents for some time. I regularly pull XML dumps from the US Patent Office, a process in which I am getting more refined, so that I am able to easily tag, and organize them alongside the rest of my research. I spent some time this last week diving into my DNS API research, and after updating the rest of the data behind, I added some DNS related patents. The patent information is already available in my API monitoring system, I just needed to be able to tag the patents, and write a script to publish the tagged patents&nbsp;to each of my Github projects. Now that I have this in place, it is pretty easy for me to spend an hour or two looking through the patents that come each week, and putting them into each area of the API space I study--which is why I have organized my API research the way that I have. The patent information provides shines another light on each layer of the space for me. Understanding the companies, tools, and individual API endpoints provide me with important insight on what is going on, but the patents being submitted are an extremely important indicator of what is actually going on behind the curtain within industries, and companies. With this addition, my research is finally reaching the levels I originally envisioned when I first began organizing my work in this way, going beyond what is available on the "features" page for each company, organization, institution, or government agency is doing with APIs. I can extract a lot of information from the product pages of the companies who are doing APIs, but these pages are designed to tell a specific story. I find that API definitions, press releases, and patent filings often tell a different story than the company's main product page. I would say that all these areas lie, but marketing, API definitions, press releases,...[<a href="/2016/10/25/adding-new-dimension-by-including-patents-in-my-dns-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_23_at_11.14.58_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/24/the-api-behind-every-feature-in-the-user-interface/">The API Behind Every Feature In The User Interface</a></h3>
			<p><em>24 Oct 2016</em></p>
			<p>I have blogged about this topic in the last 60 days, but I predict it is an area you will hear from me about regularly until I see it baked into more software solutions. CloudFlare, one of my favorite DNS API providers&nbsp;has what I think is the best approach to linking to an API in the bottom corner of every UI element in their dashboard. If you look in the bottom right corner, next to the help icon you will see an API link. When I click on the API link I'm given the API path that corresponds to the UI element. In this scenario it allows me to purge the cache for my domain. I am also given a link to the full CloudFlare API documentation. I have always been an advocate for companies making sure to have an "API" or "Developer" link in the footer of their main website. If you can make sure and have it front and center in the global navigation--all the better. Now I"m going to be advocating for an inline approach like CloudFlare. If all software as a service (SaaS) providers provided API transparency and access in this way, it would be a much different landscape. The more people that know about APIs, the more people are putting them to work developing web and mobile applications, to assist us in moving our valuable bits and bytes around the cloud using iPaaS services like Zapier, and the increasing number of other applications APIs are being put to work in. When you make APIs accessible in this way, it transforms API solutions like Push by Zapier into some pretty empowering solutions for helping the average business user put APIs to work, whenever and wherever they need. Inline access to the API resources behind ALL UI elements is something I will be writing about regularly. Hopefully, I won't have to just keep talking about CloudFlare, and some other SaaS providers step...[<a href="/2016/10/24/the-api-behind-every-feature-in-the-user-interface/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_23_at_10.59.54_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/24/prototype-api-design-guide-builder-developed-on-top-of-api-stylebook/">Prototype API Design Guide Builder Developed On Top Of API Stylebook</a></h3>
			<p><em>24 Oct 2016</em></p>
			<p>I was pretty happy when my friend Arnaud Lauret&nbsp;(@arno_di_loreto) developed&nbsp;API Stylebook. I want to see his work expand and grow into someday containing hundreds or thousands of API design guides. To help contribute to his work I took the&nbsp;YAML core of the design topics&nbsp;he's aggregated and began developing an API design guide builder that runs 100% on Github, allowing anyone to fork, and use to build their own API Stylebook on top of Arnaud's work. Currently, I have two screens for API design guide builder: Editor&nbsp;- Allows for the editing of the YAML API Design Stylebook stored in the Github repository. View&nbsp;- Allows you to view the API Design Stylebook stored in the Github repository. I'm a big fan of this approach to developing little machine readable (YAML / JSON) micro tools that are simply HTML, CSS, and JavaScript, with a data core. In this scenario, Arnaud's design topics act's as the machine readable YAML core. I just developed the self-contained editor, and viewer, allowing anyone to fork and use to manage their own API design guide. This is just a prototype. I am just getting started. I am looking to add autocomplete suggestions based on the other API design guides that Arnaud has aggregated in his API Stylebook. I am adding this, and some other features to the&nbsp;Github issues for the project, if you have any feedback or suggestions feel free to submit an issue to the Github repository.&nbsp; My objective here is to allow anyone to quickly fork, and build their own corporate or project API design guide, built on top of the existing best practices out there. I envision a future where every company, organization, institution, and an agency has their own API design guide, building on the best approaches available. Arnaud can continue to aggregate and merge the best practices (using Github) out there, and API providers can keep forking, and building on top of the best practices of the APIs...[<a href="/2016/10/24/prototype-api-design-guide-builder-developed-on-top-of-api-stylebook/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_24_at_3.54.55_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/24/preserving-the-twitter-api-field-guide/">Preserving The Twitter API Field Guide</a></h3>
			<p><em>24 Oct 2016</em></p>
			<p>I'm a fan of the human elements of this technological shift that is going on in our world. We tend to focus on the technology, and the dudes who do the technologies (the cyber is HUGE), but what will really matter in 50 or 100 years will be the more human aspects of how we did all of this. Trust me, it is hard to make this boring ass API shit human in any way, so I am always excited when there are people&nbsp;who soften the hard edges of the gears as they grind forward. One of the moments that has stood out for me in the last decade&nbsp;was the Twitter API Field Guide created by Taylor Singletary (@episod) during his time as an evangelist at Twitter. This field guide was removed by Twitter (not sure when as I just noticed today), but the original blog post remained. Thank the Internet gods for the Internet Archive (and Tyler pointing it out), because a copy of his work still lives on for us all to enjoy. I wanted to make sure a copy of it lives on beyond Twitter, and the Internet Archive, so I copied (cleaned up) and published on Github (ha). While this work may seem out of date, and irrelevant, it is art and will be something we will look back on fondly in the future--it shouldn't simply be deleted. This is one of the problems with the constant change we have embraced in the tech space--many things we care about will just be deleted and lost forever. For me, the Twitter Field Guide represents a specific time in the history of the web, and as I write this post I realize this was a dark time for API optimism in the Twitter API ecosystem, something that was having an effect on the wider API movement. I think this field guide was a very creative human response as we are struggling with not...[<a href="/2016/10/24/preserving-the-twitter-api-field-guide/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/squirrelbin_arch.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/24/amazon-groups-should-share-more-api-design-patterns/">Amazon Groups Should Share More API Design Patterns</a></h3>
			<p><em>24 Oct 2016</em></p>
			<p>The sharing of common API design patterns is something we are really bad at in the API space. I'm not a believer that there is one API design pattern to rule them all, but I am a believer in learning from what works, consuming other people's APIs, and sharing design tips over the cubicle wall. I don't believe that everyone should be 100% REST-compliant in the crafting of their APIs, but you should be picking your head up from time to time, and learning from what the rest of the world is up to, especially across the other groups within your own company. I tend to shy away on critiquing companies on API design, and prescribing any specific approach, but I can't help but point out inconsistencies in any approach, when it is clear that they aren't tuning into some of the common patterns out there, especially between their own internal groups. An example of this can be found at one of the API gods, Amazon Web Services. Amazon isn't known for their RESTful APIs, which is something I can overlook, but&nbsp;when it comes to their lack consistency between their different APIs, I think there are lessons for all of us to learn from. I have not hacked against all of the Amazon APIs, but here are the four distinct patterns I've seen: S3&nbsp;- /?{method} EC2&nbsp;- /?Action={method} Route 53&nbsp;- /2013-04-01/{methodname} Route 53 Domains&nbsp;-&nbsp;route53domains.us-east-1.amazonaws.com/ header: x-amz-target:Route53Domains_v20140515.{method} There might be additional patterns employed over at other Amazon APIs, but these are the four that I'm exposed to in my own integrations. The presence of two separate patterns within the Route 53 team was what prompted me to write this post. While I'm not a fan of the action={method} approach, which is the most common AWS pattern I have seen used, the passing of method as part of custom header just seems even wackier to me.&nbsp; I do not get dogmatic about specific API design patterns, but I...[<a href="/2016/10/24/amazon-groups-should-share-more-api-design-patterns/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/parquetrename.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/21/icons-to-describe-each-of-your-api-resources-like-aws/">Icons To Describe Each Of Your API Resources Like AWS</a></h3>
			<p><em>21 Oct 2016</em></p>
			<p>Amazon Web Service teams sure have been rocking their architectural icons across their storytelling lately. They standardized a set of icons for each of their cloud services and published in a variety of formats as the&nbsp;AWS Simple Icons for Architecture Diagrams. I am a big fan of the Noun Project API for use across my storytelling&nbsp;and find that having a standardized library of meaningful images and icons to be extremely valuable in helping quickly convey meaning (or entertain).
I was reading&nbsp;optimizing Amazon S3 for high concurrency in distributed workloads from AWS, and the diagram they provided, daisy chaining each of the AWS services at play, really brought the concept home for me. I read a lot, but I also scan a lot, and meaningful diagrams like this can go a long way to help me get up to speed as quickly as possible.

I have been an advocate for establishing icon sets for common API technologies, and even service providers, and I think I will add individual API resources to the stack. It would be valuable to have icon sets for common API resources like images, video, storage, etc. Icons that spoke to what they did, but also clearly identifies them as an API resource&nbsp;and allowing them to possibly be daisy chained together like Amazon does.
API industry icons will have to wait until some benefactor steps up to invest in this crazy idea. I'm graphically challenged, I know what looks good, but I can't craft anything graphical to save my life--look at my logo. Even if we don't have an industry-wide set of icons, it might be something individual API providers would want to consider creating for their API resources, like Amazon does. It is something that could help make your storytelling more impactful, and provide a common set of icons for everyone to use when referencing specific API resources.
[<a href="/2016/10/21/icons-to-describe-each-of-your-api-resources-like-aws/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-site/blog/platform-nanoscale-io-screenshot.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/21/does-your-business-model-reflect-where-your-api-deployment-is-going/">Does Your Business Model Reflect Where Your API Deployment Is Going</a></h3>
			<p><em>21 Oct 2016</em></p>
			<p>
I've been thinking about the concept of a wholesale API for some time. Going beyond how we technically deploy our APIs, and focusing more on how we can provide a wholesale version of the same API resources, with accompanying terms of services that go beyond just a retail level of API access in the cloud. Not all APIs fit into this category of API, but with the containerization of everything, and the evolving world of Internet of Things (IoT), there are many new ways in which API resources are being deployed.
You can see this evolution in how we are deploying APIs present in one of the latest API deployment platforms I added to my API deployment research, Nanoscale.io. This image is just a portion of their platform, but the separation of deployment concerns articulates the technical side of what I'm talking about, we just need to add in considerations for the business and political side of how this works.
We've seen API deployment move from on-premise and back again, and now we are seeing it move onto everyday objects like cameras, printers, routers, and other everyday objects. I'm watching service providers like Nanoscale.io emerge to help us deploy our APIs exactly where we need them. I'm guessing that the companies who have their business models in similar order, allowing for API service composition from the management layer to further slide down the stack to the deployment layer, will come out ahead.

[<a href="/2016/10/21/does-your-business-model-reflect-where-your-api-deployment-is-going/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_20_at_7.37.43_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/21/an-api-discovery-api-for-your-api-with-tyk/">An API Discovery API For Your API With Tyk</a></h3>
			<p><em>21 Oct 2016</em></p>
			<p>
If you are selling services to the API space you should have an API, it is just how this game works (if you are savvy). I was going through Tyk's&nbsp;API for their open source API management solution and came across their API definitions API, which gives you a list of APIs for each Tyk deployment--baking in API discovery into the open source API management solution by default.
The API API (I still enjoy saying that) gives you the authentication, paths, versioning, and other details about each API being managed. I'm writing about this because I think that an API API should be the default for all API service providers. If you are selling me API services you should have an API for all your services, especially one that allows me to discover&nbsp;and manage all the APIs I'm applying your service to.&nbsp;
I am expanding my definition of a minimum viable blueprint for API service providers, and adding an API API as one of the default APIs. I'm going to be adding the account, billing, and a handful of other essential APIs to my default definition. If I'm using your service to manage any part of my API operations, I need to be automating discovery, management, and billing in our relationship.
It seems obvious to me but I'm looking to provide a simple checklist that other API service providers can consider as they craft their strategy. My goal is to help make sure each stop along the lifecycle can be orchestrated in a programmatic way like Tyk.
Disclosure: Tyk is an API Evangelist partner.

[<a href="/2016/10/21/an-api-discovery-api-for-your-api-with-tyk/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-forward.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/20/what-is-more-important-helping-new-users-be-aware-of-apis-or-pushing-concept-forward/">What Is More Important? Helping New Users Be Aware Of APIs Or Pushing Concept Forward?</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>
This is a&nbsp;topic that has come up in several discussions lately&nbsp;and is a topic I struggle with on a regular basis. What is more important, helping new users, both developer and non-developer be more aware of APIs, or is helping push forward the concept of APIs amongst those who are already tuned in? While I don't think there is a perfect answer, I think it is an important concept to explore and discuss.
I started API Evangelist on the premise of helping educate new users about the importance of APIs&nbsp;and is something I regularly strive to do, but if you read my blog I definitely do not always keep things simple and accessible&nbsp;to a wide audience. I keep coming back to this founding premise of API Evangelist&nbsp;and work hard to write about 101, and 201 level concepts, despite definitely being a card carrying member of the core API community.
There are a number of leading-edge topics in the space right now from hypermedia to GraphQL, as well as leading edge implementations like bot and voice enablement, but should we be careful to not focus too heavily on these leading issues, and make sure we all invest in new users education as well. I'm feeling like we should keep a core academic group focusing on the big issues, but also enlist armies of folks to help translate down the food chain.
I do not feel there is a right answer to this question. You ask me on separate days, I will answer differently. Sometimes the topics that push forward the API conversation seems most important, but then sometimes educating the developer masses, and the average business users about APIs seems like what will truly push forward the conversation. Even with my flip-flopping, which is why I will never run for president, I think this topic is something we should ask ourselves regularly, and try to understand where we stand as time marches forward.
[<a href="/2016/10/20/what-is-more-important-helping-new-users-be-aware-of-apis-or-pushing-concept-forward/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_19_at_4.41.44_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/20/what-are-your-intentions-with-my-api/">What Are Your Intentions With My API?</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>While it can be easy to bash on API providers for being tight with their API resources, it can be very difficult to be an API provider operating in today's online environment. Some developers are just badly behaved and hold some pretty unrealistic expectations when it comes to opening up access to valuable content, data, and algorithms. This is why I work to be supportive of providers when locking down their API resources, as long as they do it a constructive way. One positive approach I came across recently was from the Oxford Dictionaries API, who ask a lot of questions as part of their API signup form, but I felt it was a positive experience, and provided no barriers to me gaining access to their valuable API resources. They asked me details about what type of application I'm developing, as well as the platform and language I am employing--allowing me to provide a summary of what it was I would be doing. They provide me with a summary of the terms of service, and the ability to opt into a platform email list, and they also let me know that they will not be selling my personal details to third parties. I have never been a fan of lengthy API signup forms, as I tend to feel like I'm just a lead in their sales funnel, but this left me feeling the opposite. The Oxford Dictionary API signup form felt more like it was about making sure everyone was informed&nbsp;and in the service of protecting valuable API assets. I'm a supporter of API providers requesting more details on what API developers are up to, as long as they do it in the right way. If you don't make me feel like I'm a criminal, that I'm just one of many users in your sales funnel, and that you are just looking to extract value from me, I'm more than happy to share more information...[<a href="/2016/10/20/what-are-your-intentions-with-my-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_19_at_9.04.05_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/20/the-potential-of-the-openapi-spec-parameters-object/">The Potential Of The OpenAPI Spec Parameters Object</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>
I enjoy learning from the OpenAPI Specs of the API providers I track on. Just having an OpenAPI Spec present tells a lot about an API provider in my book, but the level of detail some providers put into their API definitions adds another level to this for me. While reviewing the OpenAPI Spec for the Oxford Dictionaries API, I noticed their robust usage of the OpenAPI Spec parameters definitions collection, which provides an interesting overview of the surface area of the API, augmenting the benefits brought to the table by the definitions collection of an APIs underlying data schema.
When you are defining each path for an API you can either define the parameters using each paths parameters, or you can add them to the overall parameters definition object, allowing them to be reused across all paths. This object provided me with a centralized place to learn about the parameters used when making calls to the Oxford Dictionary API, and I'm assuming it helped them be more organized in how they defined the surface area of their APIs.

I can see how the processing of defining each path's parameters, and centrally organizing them for reuse can be a healthy thing. The more you lift yourself out of the individual&nbsp;definition&nbsp;of each path&nbsp;and consider the parameter patterns that have been used for other paths, the chances you will have a better view of the landscape will increase. I am optimistic about this OpenAPI Spec object, and curious about how it can be evolved as part of other conversation around GraphQL--something I'll work to understand better in the future.
[<a href="/2016/10/20/the-potential-of-the-openapi-spec-parameters-object/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_19_at_8.25.43_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/20/the-open-guide-to-amazon-web-services/">The Open Guide to Amazon Web Services</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>I keep an eye on things that are trending daily and weekly on Github&nbsp;because it is a great way to discover new companies and individuals doing interesting things with APIs. While looking at this earlier this week I came across the open guide to Amazon Web Services, a pretty robust, and well organized&nbsp;getting started guide to everything AWS. Here is their description of this resource out of the leading cloud computing platform: A lot of information on AWS is already written. Most people learn AWS by reading a blog or a &ldquo;getting started guide&rdquo; and referring to the&nbsp;standard AWS references. Nonetheless, trustworthy and practical information and recommendations aren&rsquo;t easy to come by. AWS&rsquo;s own documentation is a great but sprawling resource few have time to read fully, and it doesn&rsquo;t include anything but official facts, so omits experiences of engineers. The information in blogs or&nbsp;Stack Overflow&nbsp;is also not consistently up to date. This guide is by and for engineers who use AWS. It aims to be a useful, living reference that consolidates links, tips, gotchas, and best practices. It arose from discussion and editing over beers by&nbsp;several engineers&nbsp;who have used AWS extensively. I find it interesting when API providers invest in authoritative&nbsp;solutions like this, acknowledging the scattered and often out of date nature of blogs, QA sites, forums, and the wealth of other self-service resources available for APIs. Amazon is getting seriously organized with their approach to provider resources for developers--they have been doing this a while, and know where the pain points are.&nbsp; Amazon's organized approach, the breaking down by service, and the usage of Github are all interesting things I think are worth noting as part of my research. AWS is a tough API pioneer to showcase because they have way more resources than the average API provider, but as one of the early leaders in the API&nbsp;space they possess some serious&nbsp;wisdom&nbsp;and practices that are worth emulating. I'll keep going through their open...[<a href="/2016/10/20/the-open-guide-to-amazon-web-services/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_19_at_7.24.09_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/20/reducing-friction-for-api-developers-with-enums-in-api-definitions/">Reducing Friction For API Developers With Enums In API Definitions</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>
I am going through the Oxford Dictionaries API, learning about this valuable resource. Their onboarding process for registration, and learning about what the API does using interactive documentation, is very smooth. One of the things that really cuts the rough edges off learning about each API are the enums that are available for each path.
The parameters required for making calls to many of the paths, like language and country, have their enum values populated as part of their API definition. I look at numerous OpenAPI Specs in the course of my work&nbsp;and they rarely have values present for enum, providing critical default values for developers to use--eliminating some often serious frustration.
Not having the right values available when making even the simplest of API calls can be a significant point of friction when trying to get up and running using an API. While it may seem like a small thing, the work the Oxford Dictionaries API team has put into this level of detail for their API definitions will go a long way towards making their API resources more accessible and usable.
[<a href="/2016/10/20/reducing-friction-for-api-developers-with-enums-in-api-definitions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-organized.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/19/we-want-to-do-apis-you-already-are-it-is-just-not-in-an-organized-way/">We Want To Do APIs -- You Already Are, It Is Just Not In An Organized Way</a></h3>
			<p><em>19 Oct 2016</em></p>
			<p>
I have a number of folks at companies, organizations, institutions, and government agencies come to me saying that they want to do APIs, and they need some help. In many of these discussions, the first task centers around addressing the motivations behind this declaration and helping folks realize they are already doing APIs, they are just not doing it in any sort of coherent and organized fashion.
Any business, organization, institution and government agency is moving machine readable data between internal, and with external systems in 2016, for use in a variety of applications. This is just done in a variety of often proprietary, ad-hoc, data dump, and other approaches that are often dictated without any coherent vision. API does not always mean 100% REST, it is about establishing interfaces between systems for programmatic usage across a variety of applications--the trick is to do all of this in a simplified, standardized, consistent, and low-cost way modeled on what has been working for the web.&nbsp;
You are already doing API. You need to identify all the different ways you are already exporting, importing, migrating, syncing, and putting data to work across operations and begin to establish a coherent roadmap and approach to communicating about all of this. Then get to work identifying where the opportunities for standardizing how everything is defined, how&nbsp;access is needed, and a baseline for communication around these critical integrations on an ongoing basis. It's not about whether we should do APIs or not, you are already there, it is how do start doing it in a more organized and coherent way.
[<a href="/2016/10/19/we-want-to-do-apis-you-already-are-it-is-just-not-in-an-organized-way/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-flame.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/19/how-do-we-keep-the-fire-alive-in-api-space/">How Do We Keep The Fire Alive In API Space?</a></h3>
			<p><em>19 Oct 2016</em></p>
			<p>It is tough to keep a sustained fire burning in the world of technology, at the individual, organizational, and community level. I have been doing API Evangelist full time for six years, and it is no secret that I have had several moments where I've experienced a crisis of faith, and I do not doubt that there will be&nbsp;many more of these in my future--there is no perfect solution. It takes hard work, creativity, and a myriad of other considerations to keep going, stay energized, and keep other folks doing the same. I have spent a great deal of time this fall thinking about all of the factors that influence me, and contribute to the fire burning, or acting as a flame retardant to me and the API space. When exploring these contributing factors, it is only logical we start with the external forces right? Because this all sucks because of everything external, aka all of you! Couldn't possibly be me? So what are some of the external forces out there that contribute to the fire burning brightly, or possibly being dampened across API space are? People Aren't Always Nice - For some reason, the Internet has brought the worst out in many of us. I personally feel this is the nature of technology -- it isn't human, and the more time you spend with it, the less human we are, and less empathy we will have for other people. Everyone Takes A Little - Until you've spent a great deal of time in the spotlight writing, speaking, or otherwise you don't fully grasp this one. Every person you talk to, every hand you shake, takes a little bit from you -- making it super critical for people to give back -- it all takes a toll, whether we acknowledge it or not. Few Ever Give Back - The general tone set by startup culture and VC investment is to take, take, take, and rarely...[<a href="/2016/10/19/how-do-we-keep-the-fire-alive-in-api-space/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/twitter_logo_blue.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/18/the-twitter-branding-page-provides-minimum-bar-for-api-providers/">The Twitter Branding Page Provides Minimum Bar For API Providers</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>API branding is an area that I find to be contradictory in the space, with the loss of brand control being in the top concerns for companies when doing APIs, while simultaneously one of the most deficient areas of API operations, with most API providers have no branding guidance in their developer portal whatsoever. I think it is just one of those telling aspects of how dysfunctional many companies are, and how their concerns are out of alignment with reality, and where they are investing their resources. Every API should have some sort of branding page or area for their API operations--I even have a branding page. ;-) If you are looking for a healthy example to consider as a baseline of your branding page, take a look at Twitters branding page, which provides the essential building blocks you should be considering: Simple URL - Something easy to find, easily indexed by search engines, even a subdomain like Twitter does. Logos &amp; Assets - Provide us with at least a logo for your company, if not a wealth of assets&nbsp;to put to use. Branding Guidelines - Offer up some structure, helping guide us, and show you've put some thought into branding. Link to Embeddables - If you have any buttons, badges, and widgets, point us to your embeddable resources. Link to Terms of Service - Provide us with a quick link to the terms of service as it applies to branding. Contact Information - Give me an email, or another channel for asking a question if I get confused at any time. I do not agree with all of Twitter's branding enforcement, but I respect that they have set a bar, and provide the ecosystem with guidance. At the very least, it makes sure all of us are using the latest logo, and when done right it can help encourage consistency across every integration that is putting API resources to work. I find it hard...[<a href="/2016/10/18/the-twitter-branding-page-provides-minimum-bar-for-api-providers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_relevant.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/18/learning-about-apis-has-to-be-relevant-and-interesting/">Learning About APIs Has To Be Relevant And Interesting</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>I am working on a project with a 16-year-old young lady to extract and tell a story using the YouTube API. I'm pretty excited about the project because the young lady happens to be my daughter Kaia Lane. If you've ever seen my API Blah Blah Blah t-shirt, you've seen her work. Historically she could care less about APIs, until recently when pulling data about one of her favorite YouTube stars came up--suddenly she is interested in learning more about APIs. During regular chatting with my daughter, I shared a story on&nbsp;the entire history of Kickstarter projects broken down by a city. She is a little geeky and likes Kickstarter, so I figured I'd share the approach to telling stories with data, and said that if she ever wanted help telling a story like this using YouTube, Instagram, or another platform, that she should let me know. She came back to me a couple days later asking to learn more about how she could tell a story like this using data pulled from one of the YouTube stars she follows. &nbsp; Ok. I have to stop there for a moment. My 16-year-old daughter&nbsp;just asked me to learn more about APIs. :-) As an old goofy dad who happens to be the API Evangelist, I am beside myself. I'm not 100% sure where this project will go. Right now I'm just seeing what data I can pull on Dan &amp; Phil's video game YouTube channel, and from there we'll talk more about what type of story we want to tell about their followers&nbsp;and get to work pulling and organizing the data we need. I couldn't think of a tougher audience to be trying to get her interested in APIs. She isn't going to care about APIs, wants to learn about APIs, let alone become proficient with APIs unless they are relevant and interesting to her world.&nbsp; I do not think this lesson is exclusive to teaching...[<a href="/2016/10/18/learning-about-apis-has-to-be-relevant-and-interesting/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_splitter.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/18/are-api-docs-amp-definition-formats-a-single-thing-or-separate/">Are API Docs &amp; Definition Formats A Single Thing Or Separate?</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>I was reading a virtual panel: document and description formats for web APIs,&nbsp;and thought the conversation was very productive when it comes to helping bring the world of API documentation and definitions into better focus. I encounter daily reminders that folks do not see the many dimensions of API definitions, and the role they play in almost every stop along the life cycle. This virtual panel helps move this discussion forward for me, providing some clarification for when it comes to the separation between API definitions and API documentation. One of the questions asked of the panels was "Do you see API Documentation and Description formats as a single thing? Or multiple things?" Which I found&nbsp;Zdenek Nemec (@zdne) answer to be a great introduction for folks when it comes to understanding the importance of this separation: There are definitely two different things. But truth be told, the initial incentive for the use API description formats was definitely the vision of API documentation without much work. However, the tide is turning as more and more people are discovering the benefits of the upfront design, API contracts, and quick prototyping Many people still see machine readable definitions as purely something that drives API documentation. OpenAPI Specs are just for deploying Swagger UI, and API Blueprint is just for using Apiary. When in reality, the why and how you are doing API definitions is much, much deeper. As Z from Apiary points out, it is key to the API design and prototyping process, and critical to establishing the API contract. Realizing that crafting machine readable API definitions is not just about API documentation, and that it is essential to establishing a meaningful technical, business, and legal contract internally, with partners, and maybe the public, early on in this API life cycle is empowering. I would say that I didn't fully appreciate API design, and understanding the depth of it until I had OpenSpec providing me with a scaffolding...[<a href="/2016/10/18/are-api-docs-amp-definition-formats-a-single-thing-or-separate/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/restlet_icons.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/18/all-the-right-channel-icons-in-support-of-your-api-platform/">All The Right Channel Icons In Support Of Your API Platform</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>I look at a lot of websites for companies who are providing APIs and selling services to the API space. When I find a new company, I can spend upwards of 10 minutes looking for all the relevant information I need to connect. Elements like where their Twitter and Github accounts are. These are all the key channels I am looking for so that I can better understand what a company does and stay in tune with any activity, but they are also the same channels that developers will be looking for so that they can stay in tune a platform as well. I spend a great deal of time looking for these channels, so I'm always happy when I find companies who provide a near complete set of icons for all the channels that matter. Restlet, the API design, deployment, management,and&nbsp;testing&nbsp;platform&nbsp;has a nice example of this in action, providing the following channels: Facebook Twitter Google+ LinkedIn Vimeo Slideshare Github Stack Overflow Email All of these channels are available as orderly icons in the footer of their site. Making my job easier, and I'm sure making it easier for other would be API developers. They also provide an email newsletter signup along with the set of icons. While this provides me with a nice set of channels to tune into, more than I usually find, I would still like to have a blog and atom feed icons, as well as maybe an AngelList or Crunchbase, so that i can peak behind the business curtain a little. I know. I know. I am demanding, and never happy. I am just trying to provide an easy checklist for companies looking to do interesting things APIs of the common channels they should consider offering. You should only offer up channels that you can keep active, but I recommend that you think about offering up as many of these as you possibly can manage. No matter which ones you...[<a href="/2016/10/18/all-the-right-channel-icons-in-support-of-your-api-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-contract.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/18/a-more-honest-and-flexible-api-contract-using-hypermedia/">A More Honest And Flexible API Contract Using Hypermedia</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>One of the reasons I write so much on API Evangelist is to refine how I tell stories about APIs and hopefully make a bigger impact by being more precise in what I'm saying. I feel like one of the reasons why hypermedia API concepts have to take longer than we anticipated to spread is because many (not all) of the hypermedia elite suck at telling stories. I am sorry, but you have done a shit job selling the importance of hypermedia, and often times were just confrontational and turning many folks off to the subject. I am working on playing around with telling different stories about hypermedia, hoping to soften some of the sharp edges of the hypermedia stories we tell. One of the core elements of hypermedia APIs is they provide us with links as part of each API response, emulating much of what works with the web, in the system to system, and application layers of the web. One of the benefits of these links is they help facilitate&nbsp;the evolution and change that is inevitable in our API infrastructure. The default argument&nbsp;for hypermedia folk is often about versioning&nbsp;and change-resistant API clients. I'm looking to help make these concepts more human, and that employing hypermedia&nbsp;as part of our API contracts is more about providing an honest and flexible view of the business relationship we are entering into. As API provider and consumer, we are acknowledging that there will be change and evolution in the resources that are being delivered, and being more honest that this exists, as we craft this API contract. As an API provider I am not just going to dump this JSON product response on you, and expect you to know what to do, refusing to have a discussion with my consumers that this product will change. it might be out of inventory, and might be replaced by another product, and any number of evolution changes that may occur in...[<a href="/2016/10/18/a-more-honest-and-flexible-api-contract-using-hypermedia/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-conversational-interfaces.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/17/thinking-more-about-api-driven-conversational-interfaces/">Thinking More About API Driven Conversational Interfaces</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>I am spending a lot of time thinking about conversational interfaces, and how APIs are driving the voice and bot layers of the space. While I am probably not as excited about Siri, Alexa and the waves of Slack bots being developed as everyone else, I am interested in the potential when it comes to some of the technology and business approaches behind them. When it comes to these "conversational interfaces", I think voice can be interesting, but not always practical for actually interacting with everyday systems--I just can't be talking to devices to get what I need done&nbsp;each day, but maybe that is just me. I'm also not very excited about the busy, chatty bots in my Slack channels, as I'm having trouble even dealing with all the humans in there, but then again maybe this is just me.&nbsp; I am interested in the interaction between these conversational interfaces and the growing number of API resources I track on, and how the voice and bot applications which are done thoughtfully, might be able to do some interesting things&nbsp;and enable some healthy interactions. I am also interested in how webhooks, iPaaS, and push approaches like we are seeing out of Zapier, can influence the conversation around conversational interfaces.&nbsp; Conceptually I can be optimistic about voice enablement, but I work in the living room across from my girlfriend, I'm just not going to be talking a lot to Siri, Alexa or anyone else...sorry. Even if I move back to our home office, I'm really not going to be having a conversation with Siri or Alex to get my work done, but then again maybe its just me. I'm also really aware of the damaging effects of too much messaging, chat, and push notification channels open, so the bot thing just doesn't really work for me, but then again maybe it's me.&nbsp; I am more of a fan of asynchronous conversations than I am of the synchronous...[<a href="/2016/10/17/thinking-more-about-api-driven-conversational-interfaces/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-spotlights.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/17/people-do-not-know-what-your-api-does-if-you-do-not-showcase-it/">People Do Not Know What Your API Does If You Do Not Showcase It</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>
With a lot of my storytelling, I feel like captain obvious, but I also recognize the importance of simple, and sometimes repetitive storytelling to help reach my audience of time and resource-strapped&nbsp;API providers. Sometimes API providers are just too busy to remember the small things, and this is where I come in to help you remember some of the most obvious aspects of providing APIs that can be essential to success.
This morning's reminder is that nobody will know the cool things your API does if you do not showcase what is being done with it. This is why I added my API showcase research, to highlight the approach of the successful API providers, and give me a regular reminder to write about the topic. I understand you are busy, but so are your API consumers, and there is a good chance they need a little help understanding what can be done with your super valuable API resource(s).
Showcasing your successful API integrations is where the rubber meets the road with API operations. Tell us all about the cool things people are doing with your APIs. It doesn't have to be a full blown&nbsp;case study (although that would be nice too), it can be 250 words, plus some bullets, and another 250 words, helping us understand the possibilities in 2 minutes or less. From my experience in the space, people eat up these simple, easy to read examples of APIs solving problems, and providing solutions.
Ideally, you are showcasing the cool things being done with your API on a regular basis via your blog, but if for some reason, you are too busy, make sure and at least share your thoughts with me via email. You never know, if it is worthy, I might take the time to share here on API Evangelist.
[<a href="/2016/10/17/people-do-not-know-what-your-api-does-if-you-do-not-showcase-it/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/cf60890864f3b19978e7a9a6a9c84152_400x400.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/17/discovering-new-apis-through-security-alerts/">Discovering New APIs Through Security Alerts</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>
I tune into a number of different channels looking for signs of individuals, companies, organizations, institutions, and government agencies doing APIs. I find APIs using Google Alerts, monitoring Twitter and Github, using press releases and&nbsp;via patent filings. Another way I am learning to discover APIs is via alerts and notifications about security events.
An example of this can be found via the Industrial Control Systems Cyber Emergency Response Team out of the U.S. Department of Homeland Security (@icscert), with the recent issued advisory ICSA-16-287-01 OSIsoft PI Web API 2015 R2 Service Acct Permissions Vuln to ICS-CERT website, leading me to the OSIsoft website. They aren't very forthcoming with their API operations, but this is something I am used to, and in my experience, companies who aren't very public with their operations tend to also cultivate an environment where security issue go unnoticed.
I am looking to aggregate API related security events and vulnerabilities like the feed coming out of Homeland Security. This information needs to be shared more often, opening up further discussion around API security issues, and even possibly&nbsp;providing an API for sharing real-time updates and news. I wish more companies, organizations, institutions, and government agencies would be more public with their API operations&nbsp;and be more honest about the dangers of providing access to data, content, and algorithms via HTTP, but until this is the norm, I'll continue using API related security alerts and notifications to find new APIs operating online.
[<a href="/2016/10/17/discovering-new-apis-through-security-alerts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_16_at_2.35.09_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/17/defining-oauth-scope-inline-within-the-api-documentation/">Defining OAuth Scope Inline Within The API Documentation</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>I am working on a project using the Youtube API, and came across their inline OAut 2.0 scopes, allowing you to explore what the API does as you are browsing the API docs. I am a huge fan of what interactive documentation like Swagger UI, and Apiary brought to the table, but I'm an even bigger fan of the creative ways people are evolving&nbsp;upon the concept, making learning about APIs a hands-on, interactive experience wherever possible.
To kick off my education of the YouTube API I started playing with the search endpoint for the Youtube Data API. As I was playing with I noticed the had an API explorer allowing me to call the search method and see the live data.

Once I clicked on the "Authorize requests using OAuth 2.0" slider I got a popup that gave me options for selecting OAuth 2.0s copes, that would be applied by the API explorer when I make API calls.

The inline OAuth is simple, intuitive, and what I needed to define my API consumption, in line within the Youtube API documentation. I didn't have to write any code&nbsp;or jump through a bunch of classic OAuth hoops. It gves me what I need for OAuth, right in the documentation--simple OAuth is something you don't see very often.
I'm a supporter of more API documentation being an attractive static HTML layout like this, with little interactive modules embedded throughout the API docs. I'm also interested in seeing more web literacy being thrown in at this layer as well, pulling common web concepts and specification details, and providing popups, tooltips, and other inline API design learning opportunities.
I'm adding YouTube's approach to OAuth to my list of approaches to a modular approach to delivering interactive API documentation, for use in future storytelling.
[<a href="/2016/10/17/defining-oauth-scope-inline-within-the-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/widdershins_logo.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/17/convert-openapi-spec-to-slate-shins-markdown-api-docs/">Convert OpenAPI Spec to Slate / Shins Markdown API Docs</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>
Someone turned me on to an OpenAPI Spec to Slate / Shins compatible markdown converter on Github this last week. I have been an advocate for making sure we are still using machine readable API definitions for our API documentation, even if we are deploying the more attractive Slate. I've been encouraging folks to develop an attractive option for API documentation driven by OpenAPI Spec for some time, so I am happy to add this converter to my API documentation research and toolbox.
The OpenAPI Spec to markdown converter also introduced me to a version of Slate that is ported to JavaScript / Node.js called Shins. I'm going to add Shins to my API documentation research, and "widdershins" the OpenAPI Spec to markdown converter to my API definition research. The auto-generation of attractive API documentation like Slate and Shins seems like a valid approach to getting things done, and worth including in my research.
I am increasingly publishing&nbsp;YAML editions of my OpenAPI Specs which drive API documentation that operates on Jekyll, using Liquid. So I am all about having many different ways to skin the API documentation beast, allowing it to be easily deployed as part of any CI flow, and enabling the publishing of API docs for many different APIs, in many different developer portals or embedded on any device as part of IoT deployments. I think that a diverse range of approaches are optimal, as long as we do not lose our machine readable core.

[<a href="/2016/10/17/convert-openapi-spec-to-slate-shins-markdown-api-docs/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/facebook_instagram_and_twitter_provided_data_access_for_a_surveillance_product_marketed_to_target_activists_of_color__american_civil_liberties_union.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/14/transparency-in-police-access-to-social-platforms-using-oauth-and-apis/">Transparency In Police Access To Social Platforms Using OAuth And APIs</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>I was learning about&nbsp;Geofeedia providing law enforcement access to social media data from Twitter, Facebook, and Instagram via their API(s) this week. Geofeedia was making money by selling surveillance services to law enforcement build on top of these social APIs and is something that I guess Facebook and Instagram have cut-off access, but they could still have Twitter access through a reseller (Gnip?).&nbsp; This isn't something that will just go away. If law enforcement wants access to user's data on Facebook, Twitter, and Instagram, they are going to get it. I am guessing that the rules regarding what law enforcement can or can't do aren't clear (I will have to learn more), and something that is just left up to platforms to enforce via their terms of service. It is a problem that modern approaches to API authentication, management, and analytics are well designed to help make sense of--we just have to come up with a new layer defined specifically for law enforcement. Law enforcement should be able to fire up any standard, or customized solution they desire to search against social media data via APIs. However, they should be required to obtain an application key, and obtain the OAuth tokens that any other developer would need to. Rather than law enforcement being the customer of companies like Geofeedia, they should each get their own app id and keys, providing an identifying&nbsp;application that represents a specific law enforcement agency. They can still buy the software from providers, they just need the unique identifier when it comes to API consumption. Along with this access, we also need to begin to define an auditable or regulatory layer, where other government agencies or 3rd party auditors can get access to the access logs for all applications registered to law enforcement&nbsp;agencies. A kind of real time FOIA access to the API management layer, allowing for a window into how law enforcement agencies are searching and putting social media data...[<a href="/2016/10/14/transparency-in-police-access-to-social-platforms-using-oauth-and-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-puzzle-four-pieces.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/14/the-monitoring-layer-of-the-devops-aggregation-api-platform/">The Monitoring Layer Of The DevOps Aggregation API Platform</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>
While spending some time going through my API monitoring research I found myself creating an OpenAPI spec and APIs.json&nbsp;index for the DataDog API, and had the realization that this is the beginning of what I'm looking for when I was talking about a DevOps aggregation API platform. DataDog is just the monitoring layer of this vision I have, but it has many of the other elements I'm looking for.
DataDog has all the monitoring elements present in their API platform, and they have all the platform integrations I'm envisioning in a DevOps aggregate API. We just need the same thing for design, deployment, virtualization, serverless, DNS, SDK, documentation, and the other critical stops along a modern API life cycle.
I'll keep profiling the APIs for the service providers in my life cycle research&nbsp;until I get more of this DevOps aggregate API definition mapped out. Hopefully, I will stumble across other providers like DataDog who are doing such an interesting job with the choreography, and orchestration that will be needed to work across so many platforms. I appreciate API aggregation service providers who 1) have an API, and 2) share so much of the definition behind their work.
The next thing I will work on is profiling the metrics that DataDog has defined across the platforms they integrate with. Take a look at the metrics they have defined for each integration, there are some valuable patterns available in their work. I'd love to see a common set of API monitoring metrics emerge from across providers, something that if we standardize and share in a machine readable way, others will emulate--making&nbsp;interoperability much smoother when it comes to monitoring.
I just wanted to keep beating my drum about the fact that APIs aren't just about building applications, they are also critical to the API life cycle, and making sure there are stable, scalable APIs to build applications on top of in the first place.
[<a href="/2016/10/14/the-monitoring-layer-of-the-devops-aggregation-api-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/twitter_developers.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/14/taking-a-fresh-look-at-the-twitter-api/">Taking A Fresh Look At The Twitter API</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>I am working on profiling the Twitter API again, and I thought their stack of APIs have evolved significantly beyond what we tend to think of as the Twitter API, and was worth taking another look at. It is easy to think of Twitter API being about tweeting, friends, and following people, and #hashtags, but they have an interesting mix that I think tells its own story about Twitter's journey. Here is the current Twitter API stack: Public REST API - The public REST APIs provide programmatic access to read and write the Twitter data -- what we think of when we talk about the Twitter API. Media API - The APi for managing photo, videos or animated GIFs, that are used by other Twitter API endpoints when tweeting, direct messaging, and others. Collections API - The API for managing collections of tweets to tell specific stories, providing a single URL that represents each Twitter collection. The TON (Twitter Object Nest) API - Allowing implementers to upload media and various assets to Twitter, allowing for resumable, and single file uploads. Curator API - Provides broadcasters their curator-created streams for on-air graphics systems, or other digital displays.&nbsp; Streaming APIs - Deliver new responses to REST API queries over a long-lived HTTP connection, providing a regular stream of tweets from the platform. Ads API - The Ads API gives partners a way to integrate Twitter advertising management in their product. Selected partners have the ability create their own tools to manage Twitter Ad campaigns while easily integrating into existing, cross-channel advertising management solutions. Gnip -&nbsp;Gnip is Twitter&rsquo;s enterprise API platform, delivering real-time and historical Twitter firehose data for large use applications. It is interesting to think about Twitter's long API evolution that got them here. I hear people often reference Twitter as the most extreme example of a public API out there. Granted, it is definitely the original example and has a very public element to it,...[<a href="/2016/10/14/taking-a-fresh-look-at-the-twitter-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/1_z2iwyx5ebaipra4gsrrljw.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/14/slack-shares-their-view-on-bot-advertising/">Slack Shares Their View On Bot Advertising</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>
I was reading the hard questions on bot ethics from Slack, and their thoughts on bot advertising grabbed my attention. Trying to understand how bots will be monetizing things has been something I'm learning about, so I found Slack's post rather timely, and relevant to this fast growing layer of the API world.&nbsp;
Here was Slack's view on advertising with bots by developers:
A bot should not serve ads unless it has a strong, expressed purpose that benefits the user in doing so, and even then only on B2C platforms. I would hate to see bots becoming the new tracking pixel. Bots should not be prompting users to click on things and buy things unless explicitly asked to do so.
They continue by adding that, "ads in apps are against the&nbsp;Slack API&nbsp;terms of service, and that makes me rather proud". I'm hoping that the Slack's business model is solid enough that they'll never need to consider bot advertising. I think it is an interesting constraint upon the community, and one that I'm curious how they'll work around when it comes to making money with bots. Could we be looking at a post-advertising world, when it comes to generating revenue?
From what I can tell, bot monetization will either be about mining data from users,&nbsp;paying for premium features, or a little of both. I'm still a few weeks off from having more examples of how the bots are generating revenue, but I wanted to make sure I recorded Slack's stance on the subject, for reference in future work.
[<a href="/2016/10/14/slack-shares-their-view-on-bot-advertising/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_13_at_9.17.46_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/14/including-the-twitter-object-nest-api-as-a-file-upload-api-example/">Including The Twitter Object Nest API As A File Upload API Example</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>One request I get from folks on a regular basis, is an example of file upload APIs. Each time I get one of these requests I regret that I do not have more file upload and storage APIs profiled, allowing me to share a list of examples. So file upload APIs are high on my list to keep an eye out for as I'm doing my regular monitoring and mapping of the API universe.&nbsp; An API I wanted to add to this list was the&nbsp;TON (Twitter Object Nest) API, which "allows implementers to upload media and various assets to Twitter".&nbsp;The TON API is an interesting model for me because it supports resumable, and non-resumable uploads--with all files over 64MB required to be resumable. I wanted to profile the API in a story, and add some of the key aspects to my research on file upload APIs, so that I could reference in future conversations. Some of the core features of how the TON API operates are: The Content-Type of requests cannot be application/x-www-form-urlencoded. The Content-Type of requests&nbsp;are&nbsp;a valid media type as defined by&nbsp;IANA. Chunks should be in integer multiples of X-TON-Min-Chunk-Size (except the last). The Location header after upload needs to be saved to be used in other Twitter API calls. Here is the basic makeup of the initial request to kick off a resumable upload: Authorization: See 3-legged authorization Content-Length: Set to 0 Content-Type: The Content-Type of the asset to be uploaded. X-TON-Content-Type: Identical to Content-Type X-TON-Content-Length: Total number of bytes of the asset to be uploaded The initialization response contains a Location which can then be used in other calls to the Twitter API. After you make the resumable upload initialization call, you can make each of the follow-up chunk uploads for the file--here is an example resumable video upload request: PUT /1.1/ton/bucket/{bucket}/SzFxGfAg_Zj.mp4?resumable=true&amp;resumeId=28401873 HTTP/1.1 Authorization: // oAuth1.0a (3-legged) details here Content-Type: video/mp4 Content-Length: {number of bytes transferred in this request} Content-Range: bytes {starting...[<a href="/2016/10/14/including-the-twitter-object-nest-api-as-a-file-upload-api-example/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_12_at_11.19.25_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/13/the-open-skills-api-from-dept-of-labor-amp-university-of-chicago/">The Open Skills API From Dept of Labor &amp; University of Chicago</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>
We like to talk about the API economy in this space. It is kind of the grand dream of API obsessed, that helps us articulate how big of a deal we think APIs are going to be. We know APIs are going to be big, but in reality, the impact most APIs make don't size up. This is one of the reasons I'm such a big support of APIs in the public sector, as the potential for a positive impact tends to be greater in my opinion.
One of the APIs that has the potential to contribute at the API economy scale is out of a partnership between the Department of Labor and the University of Chicago--the DataAtWork Open Skills API. Providing "a complete and standard data store for canonical and emerging skills, knowledge, abilities, tools, technologies, and how they relate to jobs". Opening up a pretty useful API from their collaborative work to "map the DNA or genome of the U.S. labor market."
The Open Skills API uses Swagger UI for the documentation, which always makes me happy because it means there is an OpenAPI Spec behind, for use in Postman, APIMATIC, and other API solutions. It's a simple, open API, that has some serious potential for use in web, and mobile apps, as well as visualizations, analysis, and other types of applications--making it worthy to add the APIs in my list that I think could actually make an impact at API economy scale.
[<a href="/2016/10/13/the-open-skills-api-from-dept-of-labor-amp-university-of-chicago/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_12_at_6.28.48_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/13/saving-and-versioning-api-definitions-in-editor-using-github-gists/">Saving and Versioning API Definitions In Editor Using Github Gists</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>
My friend Jordan Walsh (@jordwalsh) just released a new take on the Swagger editor, that inches closer to my vision of a dream API sketchbook and portfolio. His&nbsp;swagger-gist.io tool allows you to open and save your API definitions to Github Gists, allowing you to use the snippet sharing solution to manage your API definitions, and their evolution.
While it isn't my entire vision for an API sketchbook and portfolio, swagger-gist.io's usage of Github Gists is a move in the right direction. This is just the first draft of his&nbsp;tool, and it looks like he plans on building in more of the API definition management features I am looking for--leveraging Github Gists as the book, in my sketchbook definition. #Creative
I like this model, especially when it comes to collaboration and storytelling around the API design process. I could see offering more sharing features for API definitions within the editor, enabling you to email, Slack, and share throughout an API's life cycle. I can also see more copy and paste opportunities, embedding API definitions using Github Gists in blog, knowledge base, and forum posts--grabbing the embed code from within the editor.
I'm curious to see where Jordan takes it. I have lots of ideas, but will just keep an eye on his work. My only critique at the moment is to not couple the functionality too tightly with the word "Swagger", as that is a trademarked product. I recommend relying on "OpenAPI Spec" or even better, some other way of identifying Gists that contain an OpenAPI Spec definition. ;-)
Cool stuff Jordan, keep up the good work.
[<a href="/2016/10/13/saving-and-versioning-api-definitions-in-editor-using-github-gists/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/ok_800.jpeg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/13/preserving-the-sunlight-on-github/">Preserving The Sunlight On Github</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>
I'm following along as the Sunlight Foundation winds down their operations and gathering any lessons along the way, that can help&nbsp;us open data and transparency folks can learn from as we do our work. I wrote earlier that we should be learning from the Sunlight Foundation situation and that we are making sure we bake transparency into our projects&nbsp;and wanted to continue to extract wisdom we can reuse as they turn out the lights.
The Sunlight Foundation shared that they are working with the Internet Archive and Github to preserve their projects, and that they are "trying to ensure the open source community can understand and use our projects in the future" by:

adding documentation
standardizing licenses&nbsp;
scrubbing sensitive info

This is the benefits of being transparent and open by default is that you tend&nbsp;to do all of this in real-time. If you are in the business of opening up data, making it accessible with open APIs, you should be using Github, documenting, and telling the story as you go along. Then you do not have to do it all when you are walking away--everything is open by default.
This is why I work out in the open on Github each day, it allows people to take my tools like the CSV Converter, and the API Stack, and put them to work, even as I'm still evolving them. When I step away from a project, it can continue to live on with all the code, definitions, schema, data, licensing, and the story behind in a nice forkable package--no extra work necessary.
Anyways, just a couple of nuggets to consider as we are working on open data and API project across the space.

[<a href="/2016/10/13/preserving-the-sunlight-on-github/">Read More</a>]</p>
			<p><hr /></p>
	  

		<!-- Pagination links -->
		<table width="100%">
			<tr>
				<td align="left" width="33%">
				  
				    <a href="/blog/page13" class="previous">
				      &#8592; Previous
				    </a>
				  
			</td>
			<td align="center" width="33%">
			  <span class="page_number ">
			    Page: 14 of 37
			  </span>
			</td>
			<td align="right" width="33%">
		  
		    <a href="/blog/page15" class="next">Next &#8594;</a>
		  
			</tr>
			</tr>
		</table>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
	<hr>
	<p align="center">
		relevant work:
		<a href="http://apievangelist.com">apievangelist.com</a> |
		<a href="http://adopta.agency">adopta.agency</a>
	</p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
