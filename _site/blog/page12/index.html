<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }
    
    .container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
    }
    .video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }    

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/13/setting-the-rules-for-api-automation/"><img src="https://s3.amazonaws.com/kinlane-productions2/twitter/twitter-automation-rules.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/13/setting-the-rules-for-api-automation/">Setting The Rules For API Automation</a></h3>
			<p><em>13 Jun 2017</em></p>
			<p>Twitter released some automation rules this spring, laying the ground rules when it comes to building bots using the Twitter API. Some of the rules overlap with their existing terms of service, but it provides an interesting evolution in how platform providers need to be providing some direction for API consumers in a bot-driven conversational landscape. They begin by laying the ground rules for automation using the Twitter API: Do! Build solutions that automatically broadcast helpful information in Tweets Run creative campaigns that auto-reply to users who engage with your content Build solutions that automatically respond to users in Direct Messages Try new things that help people (and comply with our rules) Make sure your application provides a good user experience and performs well — and confirm that remains the case over time Don’t! Violate these or other policies. Be extra mindful of our rules about abuse and user privacy! Abuse the Twitter API or attempt to circumvent rate limits Spam or bother users, or otherwise send them unsolicited messages Twitter is just evolving their operation by providing an automation update to the Twitter rules and the developer agreement and policy, outlining what is expected of automated activity when it comes to engaging with users account, when bots are tweeting, direct messages, and other actions you take when it comes to Tweets or Twitter accounts. Providing an interesting look at the shift in API platform terms of service as the definition of what is an application continues to evolve. While there were may automated aspects to the classic interpretation of web or mobile applications, bots are definitely bringing an entirely new meaning to what automation can bring to a platform. I think any API driven platform that is opening up their resources to automation is going to have to run down their list of available resources and think deeply about the positive and negative consequences of automation in the current landscape. Whether it is bots,...[<a href="/2017/06/13/setting-the-rules-for-api-automation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/13/more-evangelism-will-be-needed-to-move-banking-api-conversation-forward/"><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/api-evangelist-speaking.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/13/more-evangelism-will-be-needed-to-move-banking-api-conversation-forward/">More Evangelism Will Be Needed To Move Banking API Conversation Forward</a></h3>
			<p><em>13 Jun 2017</em></p>
			<p>I was reading what’s behind the hold up of API adoption at credit unions and I’m reminded (again) of the critical need for API evangelists in the space. I am not talking about advocates for a single API, but more evangelists that reflect my mission as the API Evangelist, but dialed in for specific industries. To set the stage for you, let me share why I started API Evangelist seven years ago. I began writing about the business, and eventually the politics of APIs because I saw the potential with APIs, but I also saw that things were not evolving as fast as they could because technologists were dominating the API conversation. We needed more discussion around the business of doing APIs, and many of the finer political details like security, terms of service, branding, and other concerns of the business leaders who actually controlled the purse strings that would move the space forward at the speed and scale everyone desired. To help break the log jam within the federal government, healthcare, and other industries someone needed to help assure business and technical stakeholders of the benefits of doing APIs, with sensible discussion regarding how they could mitigate the risk involved. The problem in the banking space is that you only have the vendors and hype pundits dominating the conversation, and much-needed trust is never established. Banks are extremely risk adverse because of money, but also because of regulations, and when you have just technologist, vendors, and hype analysts stirring the pot, nobody ever really helps alleviate bank’s actual concerns around security, privacy, and other areas that keep them up at night. I remember sitting down at a table in Vegas with Anthem, Kaiser Permanente, and other health insurance providers who were all about business when I sat down, but after an hour or so I convinced them they weren’t in my sales pipeline, and the tone changed completely. They are so used to be...[<a href="/2017/06/13/more-evangelism-will-be-needed-to-move-banking-api-conversation-forward/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/13/i-wish-usa-facts-had-a-more-sophisticated-api-embeddable-and-sharing-strategy/"><img src="https://s3.amazonaws.com/kinlane-productions2/usa-facts/usa-facts-federal-business-spending.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/13/i-wish-usa-facts-had-a-more-sophisticated-api-embeddable-and-sharing-strategy/">I Wish USA Facts Had A More Sophisticated API Embeddable And Sharing Strategy</a></h3>
			<p><em>13 Jun 2017</em></p>
			<p>I love what the folks over at USAFacts have done with their effort to educate everyone regarding how the US works (or doesn’t). I commend Steve Ballmer for the money he’s put into the project and the obviously huge amount of work they have put into making some pretty complex things understandable. However, I just have one critique: I wish they had an API, accompanied with a more sophisticated sharing and embeddable approach to publishing the wealth of valuable information contained within the site. You can share links to specific sections of USAFacts, but it is just a generic image with a link to each area of the site. The site is exactly what we need in a Trump era, and is full of valuable factoids about how things work, but we need more eye candy for sharing, and the ability to share more granular level details about what is contained within the project. Journalists should be able to craft stories around finance and population via a graph, chart, or other detail that links back to the site. In the current state, you have to be pretty motivated and truly care about this stuff to visit the site and get involved–this represents a pretty light portion of the US population (maybe we could get a chart ;-). All it would take to accomplish this is a pretty simple JSON API with some D3.js or other API visualization magic. With an API, data-savvy journalists, and other 3rd party developers could help carry the load when it comes to developing storytelling tooling that could help USAFacts make a bigger impact. Think about the impact that Facebook and Twitter cards had on the election when it came to sharing news (or fake news) with the public–we need USAFacts to be richly embedded in everyone’s timeline, with meaningful storytelling behind when a user is looking to know more. The facts have to be portable, shareable, visual, and tell a...[<a href="/2017/06/13/i-wish-usa-facts-had-a-more-sophisticated-api-embeddable-and-sharing-strategy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/12/tweeting-out-your-api-forum-conversations/"><img src="https://s3.amazonaws.com/kinlane-productions2/oxford-dictionaries/oxford-dictionaries-support-forum-tweet.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/12/tweeting-out-your-api-forum-conversations/">Tweeting Out Your API Forum Conversations</a></h3>
			<p><em>12 Jun 2017</em></p>
			<p>
It is a lot of work to keep the API evangelism drumbeat going each day on your blog, Twitter, and other social media channels you use for your API operations. Each Tweet, Facebook or LinkedIn Post is one possible signal that might reach existing developers, or possibly reach a potentially new API consumer–educating them about what your API does.

My friends over at the Oxford Dictionaries APIs are getting really good at this API evangelism song and dance, and one of the tactics in their toolbox is regularly Tweeting out relevant threads from their API forum. It is a great way to expose conversations that are going on within your API support forum, and help make other developers aware that these conversations are going on in a way that will also boost your overall SEO, making your API support operations more visible to the public.

Another benefit of sending out these regular API signals is that there is always the potential that I will write up what you are doing, and you’ll get the additional exposure of being on API Evangelist. When people ask me what is the #1 thing they can do to be more successful with evangelism for their API, it is always consistency. Regular, consistent drumbeats about what is going on with your platform, the problems it solves is always the best way to make sure your valuable API resources will be found and put to use in meaningful ways.

[<a href="/2017/06/12/tweeting-out-your-api-forum-conversations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/12/revisiting-graphql-as-part-of-my-api-toolbox/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-tools-school.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/12/revisiting-graphql-as-part-of-my-api-toolbox/">Revisiting GraphQL As Part Of My API Toolbox</a></h3>
			<p><em>12 Jun 2017</em></p>
			<p>I’ve been reading and curating information on GraphQL as part of my regular research and monitoring of the API space for some time now. As part of this work, I wanted to take a moment and revisit my earlier thoughts about GraphQL, and see where I currently stand. Honestly, not much has changed for me, to move me in one direction or another regarding the popular approach to providing API access to data and content resources. I still stand by my cautionary advice for GraphQL evangelist regarding not taking such an adversarial stance when it comes to the API approach, and I feel that GraphQL is a good addition to any API architect looking to have a robust and diverse API toolbox. Even with the regular drumbeat from GraphQL evangelists, and significant adoption like the Github GraphQL API I am not convinced it is the solution for all APIs and is a replacement for simple RESTful web API design. My current position is that the loudest advocates for GraphQL aren’t looking at the holistic benefits of REST, and too wrapped in ideology, which is setting them up for similar challenges that linked data, hypermedia, and even early RESTafarian practitioners have faced. I think GraphQL excels when you have a well educated, known and savvy audience, who are focused on developed web and mobile applications–especially the latest breed of single page applications (SPA). I feel like in this environment GraphQL is going to rock it, and help API providers reduce friction for their consumers. This is why I’m offering advice to GraphQL evangelists to turn down the anti-REST, and complete replacement/alternative for REST–it ain’t helping your cause and will backfire for you. You are better to off educating folks about the positive, and being honest about the negatives. I will keep studying GraphQL, understanding the impact it is making, and keeping an eye on important implementations. However, when it comes to writing about GraphQL you are...[<a href="/2017/06/12/revisiting-graphql-as-part-of-my-api-toolbox/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/12/recent-api-paths/"><img src="https://s3.amazonaws.com/kinlane-productions2/box/box-recent-item-object.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/12/recent-api-paths/">Recent API Paths</a></h3>
			<p><em>12 Jun 2017</em></p>
			<p>
I was learning about a new API path for the document platform Box, that was designed specifically for showing recently updated objects. I think that the concept of having API paths dedicated to showing recently changed elements makes sense, helping eliminate the need for API consumers to learn about which parameters are needed to achieve their immediate goals, helping expose useful aspects of the platform through API design.

As an API consumer, it can be a lot of work to get at the meaningful and relevant context of an API Platform if you do not know all the right knobs and levers to pull on. This is where API design comes in handy, helping surface the most relevant and contextual aspects of what is going on. While this may not be the approach all API providers should be taking, especially if you have a savvy API consumer audience, in times when you have a wider, more unknown audience who may be looking to just get at what is going on with their users platform integrations, this type of API design can be a good way to reduce friction.

I am writing about the concept of recent API paths, to think about how this can be added to my API design toolbox. While not a REST, hypermedia (wait maybe) design pattern, I think that in some scenarios, having a recent API path makes a lot of sense. Some platforms are only relevant for a specific time period, and the rest of what is happening can be dealt with through search, taxonomy, and other ways of archiving. Once I write about an API design pattern I come across it kinds of activates it in my brain and makes it something I will be thinking about as I review other APIs, as well as design my own APIs, and who knows, maybe it will do the same for you!

[<a href="/2017/06/12/recent-api-paths/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/12/if-oracle-wants-to-be-taken-seriously-with-its-api-campaign-it-needs-to-drop/"><img src="https://s3.amazonaws.com/kinlane-productions2/oracle/the-oracle-in-the-matrix.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/12/if-oracle-wants-to-be-taken-seriously-with-its-api-campaign-it-needs-to-drop/">If Oracle Wants To Be Taken Seriously With Its API Campaign It Needs To Drop</a></h3>
			<p><em>12 Jun 2017</em></p>
			<p>Oracle is investing a serious amount resources to become a contender in the API space lately. [They've acquired Apiary](http://apievangelist.com/2017/01/19/oracle-acquiring-apiary/), and are beating a regular PR drum regarding API design, deployment, management, and everything API. The tech giant shows up on my API monitoring daily with new waves of messaging about how it is the platform of choice when it comes to APIs. The problem is these are always from meaningless outlets who publish every press release they get, not the sources the community looks to for answers. The company is facing an uphill battle because it is extremely late to the game, but also because of it's ongoing API copyright lawsuit against Google. I know from their vantage point they think they can just change their tune when it comes to marketing and PR, and many enterprise zombies will gladly follow along. They also can flex their financial resources and the venture capital behind startups like Apiary will also fall in line, but when it comes to gaining significant mind share amongst the core API community, they will never be taken seriously until they drop their lawsuit, and make a public announcement demonstrating their commitment to the community. I'll get the regular wave of haters for this post asking who I think I am. I'm nobody. It's true. Although I must be a micro blip on the radar because Oracle's API team had reached out to me. I asked them if they knew who I was, and had read my work? They said they had, but I think they really hadn't because after a while they stopped responding to me emails regarding their request for a sit-down. I get it. I'm nobody at the Oracle scale, but I am listened to in the API community, which is a different software community than the enterprise world. I'm under no delusions that the enterprise community gives a shit about what I have to say, but many in...[<a href="/2017/06/12/if-oracle-wants-to-be-taken-seriously-with-its-api-campaign-it-needs-to-drop/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/09/the-apis-json-for-trade-gov/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/09/the-apis-json-for-trade-gov/">The APIs.json For Trade.gov</a></h3>
			<p><em>09 Jun 2017</em></p>
			<p>
There are a growing number of API providers who have published an APIs.json for their API operations, providing a machine-readable index of not just their API, but for their API entire operations. My favorite example to use in my talks and conversations when I’m showcasing the API discovery format is the one for the International Trade Administration at developer.trade.gov.

The International Trade Administration (ITA) is the government agency that “strengthens the competitiveness of U.S. industry, promotes trade and investment, and ensures fair trade through the rigorous enforcement of our trade laws and agreements”, provides an index of where you can find their developer portal, documentation, terms of service, as well as a machine readable OpenAPI for their trade APIs.

I couldn’t think of a more shining example of APIs when it comes to talking about the API economy. I am pleased to have helped influenced their API efforts and helping them see the importance of providing a machine readable index of their API operations with APIs.json, as well as their APIs using OpenAPI. If you need a well maintained, and meaningful example of how APIs.json works head over to developer.trade.gov and take a look.

[<a href="/2017/06/09/the-apis-json-for-trade-gov/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/09/i-like-the-apicurio-road-map/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/09/i-like-the-apicurio-road-map/">I Like The Apicurio Road Map</a></h3>
			<p><em>09 Jun 2017</em></p>
			<p>
I have been learning more about Apicurio, which is the open source API design editor I have been waiting for. There are a number of things I’m interested in when it comes to Apicurio, but one side element that caught my attention was their road map.

I am a big fan of encouraging folks to share their roadmap. It is an important part of helping establish a shared future between API provider and API consumer. Apicurio is an API tool, without any APIs (yet), but the roadmap purpose remains the same. I like how Apicurio shares their tech preview, beta, and 1.x plan, in a coherent and organized way–you do not have to be a developer to understand what they are planning.

As I was using Apicurio I had a lot of questions about what it didn’t do. I had a lot of ideas about what it should do, and before I set out writing these ideas on my blog I spent some time with their road map, syncing items on my list with items on their roadmap. After I was done, I had reduced my list of questions and ideas to just a handful of items–which I will write about shortly. Their proactive, coherent, and complete road map saved me time, but also will save them time when it comes to listening to my feedback (if they do).

Complete, coherent, and plain English road maps are another one of those super simple, captain obvious ideas that over 50% of the APIs I review DO NOT HAVE. Which is why I’m writing about it and showcasing a positive example like Apicurio. Please do not forget to share road map with your community–it will save us both time.

[<a href="/2017/06/09/i-like-the-apicurio-road-map/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/08/the-support-elements-of-your-api-service-level-agreement/"><img src="https://s3.amazonaws.com/kinlane-productions2/zendesk/keeping-your-word-the-support-sla.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/08/the-support-elements-of-your-api-service-level-agreement/">The Support Elements Of Your API Service Level Agreement</a></h3>
			<p><em>08 Jun 2017</em></p>
			<p>
Zendesk gave me some valuable building blocks to add to both my API support and API service level agreement research, with their support SLA. This is why I keep an eye on not just how API providers are handling their support, but also how leading support software as a service API providers are setting the bar for how we do support.

The Zendesk support SLA provides us with some valuable information about setting a service level objective, developing support SLA workflow, dealing with a breach, and even some key performance indicators (KPIs) to help you measure success. I will be taking the bullet points from each area and adding to the overlap of my API support and service level research, and I’ll even begin flushing out my API breach research with its first handful of building blocks regarding how to handle a really bad situation.

I’m seeing an uptick in the number of SLAs with leading API providers, so it makes sense to start considering how other aspects of API operations should be reflected in our API service level agreements. How you support and communicate with your customers can be just as important as the technical bullets of your SLA. Most of the SLAs I’ve read in the API space focus on the technical, business, and legal considerations of integration, but Zendesk reminds us of the actual human elements of setting and meeting a specific level of service when it comes to API integration.

[<a href="/2017/06/08/the-support-elements-of-your-api-service-level-agreement/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/08/temporary-interaction-limits/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/769c0c3c-34af-11e7-9b76-96396c81f051.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/08/temporary-interaction-limits/">Temporary Interaction Limits</a></h3>
			<p><em>08 Jun 2017</em></p>
			<p>I spend a lot of time thinking about API rate limits. How they can hurt API providers, or as my friend Tyler Singletary (@harmophone) says incentivize creativity. I think your view on rate limits will vary depending on which side of the limit you stand, as well as your own creative potential and limitations. I agree with Tyler that they can incentivize creativity, but it doesn’t mean that all limitations imposed will ultimately be good, or all creativity will be good. I found myself contemplating Github’s recent introduction of temporary interaction limits which means “maintainers can temporarily limit who can comment, create pull requests, and open issues among existing users, collaborators, and prior contributors.” While this isn’t directly about API rate limiting, it does overlap, and provide us with some thoughts we can apply to our world of API consumption, and how we sensibly moderate the access to the digital resources we are making available online. When it comes to real-time fetishism around the digital world those with the loudest bullhorn often get heard and think real-time is good, while I am becoming less convinced that anything gets done in a 24-hour time frame. Despite what many want you to believe, real-time does not always mean good. Sometimes it might do you some good to chill out for 24 hours before you continue commenting, posting, or increase your consumption of a digital resource, whether you want to admit it or not. Our digital overlords have convinced us that more is better and real time is always ideal. Temporary interaction limits may not be the right answer in all situations, but it does give us another example of rate limiting by a major provider that we can consider and follow when it comes to crafting limitations around our digital resources. This is what rate limitations are all about for me, thoughtful consideration about how much of a good thing you will need each second, minute, day,...[<a href="/2017/06/08/temporary-interaction-limits/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/08/patents-as-a-measure-of-individual-success/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/raven-fence.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/08/patents-as-a-measure-of-individual-success/">Patents As A Measure Of Individual Success</a></h3>
			<p><em>08 Jun 2017</em></p>
			<p>I read a lot of patents as part of my work as the API Evangelist, and I tend to stalk and tune into the social media accounts of some of the authors. I have noticed that some of them work at large companies, and are counting each patent they file and are announcing each one like it is a badge of honor. I’m fascinated by this. Each company’s approach to showcasing or downplaying their patent portfolio tells a lot about the company, something that I feel trickles down to each individual author. The theater of showcasing the number of patents is fascinating to me. I’m not saying it’s a bad thing, just something I think is worthy of more discussion in the modern age. I don’t showcase the number of patents I have filed because 1) I don’t have any patents 2) I cannot afford to file any patents 3) I don’t showcase my ideas, I showcase things I do and the stories I tell. Ok, maybe a patent is a story right? A story about what is possible, that you’ve paid a fee to file with the government, and convinced them that the story is true? I’m just trying to get at the thinking behind this theater production, and why some folks feel that it is a badge of honor. The biggest differentiator here for me is that I cannot afford to file patents. It is a rich man’s game. Is this why people showcase? To declare they are part of the elite? Even if I could afford to file one patent, I definitely cannot afford to file many patents, and I cannot ever afford to litigate and defend a patent in a court of law. Making patents completely useless to me, even if I wanted to legally define my stories and ideas like this. Another thing that I notice is that there are no individuals filing patents, it is always an individual filing...[<a href="/2017/06/08/patents-as-a-measure-of-individual-success/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/08/patent-us-8954988-automated-assessment-of-terms-of-service-in-an-api-marketplace/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-terms-conditions.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/08/patent-us-8954988-automated-assessment-of-terms-of-service-in-an-api-marketplace/">Patent US 8954988: Automated Assessment of Terms of Service in an API Marketplace</a></h3>
			<p><em>08 Jun 2017</em></p>
			<p>I’m reading a lot of API patents lately trying to understand the variety of approaches these “innovative” patent authors are using to help define the API space. Many of the API patents I have historically objected to tend to patent the technical detail that make the web work or significantly contributes to the integration benefits that an API delivers. Today’s patent does all of this but is focused on patenting the legal details that are needed to make this whole API thing work at scale. Title: Automated assessment of terms of service in an API marketplace Number: 08954988 Owner: International Business Machines Corporation Abstract: An embodiment of the invention comprising a method is associated with an API marketplace, wherein one or more API providers can each supply an API of a specified type, and each provider has a set of ToS for its API of the specified type. The method includes, responsive to an API consumer having a need for an API of the specified type, obtaining the ToS of each of the API providers. The method further includes implementing an automated process to determine differences between the ToS of a given API provider, and a ToS required by the API consumer. The ToS differences determined for respective API providers are used to decide whether to select a particular one of the API providers to supply an API of the specified type to the API consumer. Don’t get me wrong. I think this is an innovative idea. I just don’t think it is something that should be patented. It should be an open standard, with a wealth of open (and proprietary) tooling developed to enable it to be a reality. If you are patenting the thing we need to make the legal partnership details in API marketplace, and ideally on the open web across API implementations more streamlined, you are just slowing meaningful API adoption and integration–it isn’t something you are going to get rich...[<a href="/2017/06/08/patent-us-8954988-automated-assessment-of-terms-of-service-in-an-api-marketplace/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/08/kdl-a-graphical-notation-for-kubernetes-api-objects/"><img src="https://s3.amazonaws.com/kinlane-productions2/openshift/kubernetes-kdl-deploy-image19-24.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/08/kdl-a-graphical-notation-for-kubernetes-api-objects/">KDL: A Graphical Notation for Kubernetes API Objects</a></h3>
			<p><em>08 Jun 2017</em></p>
			<p>
I am learning about the Kubernetes Deployment Language (KDL) today, trying to understand their approach to defining their notion of Kubernetes API objects. It feels like an interesting evolution in how we define our infrastructure, and begin standardizing the API layer for it so that we can orchestrate as we need.

They are standardizing the Kubernetes API objects into the following buckets:

Cluster - The orchestration level of things.
Compute - The individual compute level.
Networking - The networking layer of it all.
Storage - Storage behind our APIs.

This has elements of my API lifecycle research, as well as a containerized, clustered, BaaS 2.0 in my view. Standardizing how we define and describe the essential layers of our API and application infrastructure. I could also see standardizing the testing, monitoring, performance, security, and other critical aspects of doing this at scale.

I’m also fascinated at how fast YAML has become the default orchestration template language for folks in the cloud containerization space. I’ll add KDL to my API definition and container research and keep an eye on what they are up to, and keep an eye out for other approaches to standardizing the API layer for deploying, managing, and scaling our increasingly containerized API infrastructure.

[<a href="/2017/06/08/kdl-a-graphical-notation-for-kubernetes-api-objects/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/07/transparency-around-every-company-who-has-access-to-our-social-data-via-an-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/university-of-cambridge/university-of-cambridge-api-transparency.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/07/transparency-around-every-company-who-has-access-to-our-social-data-via-an-api/">Transparency Around Every Company Who Has Access To Our Social Data Via An API</a></h3>
			<p><em>07 Jun 2017</em></p>
			<p>I believe that APIs can bring some important transparency to the web, mobile, and device applications that seem to be invading our life. I hesitate using the word transparency because it has been weaponized by Wikileaks and others in the current cyber(in)secure landscape, but for the purposes of this story, it will work. APIs by default do not mean transparency, but when done in the right way they can pull back the curtain a little on what is going on when a company, organization, institution, or agency behind is truly committed to transparency. I’ve long had a portion of my research dedicated to studying intentional transparency efforts by API providers, giving me a place to publish any organizations, links, and stories that I publish on the subject of API transparency. As part of my API research I was looking at some university API efforts the other day when I came across the Apply Magic Sauce API, a personalisation engine that accurately predicts psychological traits from digital footprints of human behavior, which had a pretty interesting section dedicated to the subject of transparency. Here is some background on their approach: Our methods have been peer-reviewed and published in open access journals since 2013, and new services that sound similar to Apply Magic Sauce API (AMS) are springing up every day. As this technology becomes more accessible and its impact increases, we would like to ensure that citizens have clarity on who we do and do not work with. We are therefore committed to keeping an up to date list of every organisation that we have formally authorised to use AMS for commercial purposes. These clients are advised to follow our ethical guidelines and are bound by our terms and conditions regarding the need to obtain the informed consent of individuals about whom predictions are made. We encourage other providers of predictive technologies to honour the principles of privacy, transparency and relevance and publish a similar list...[<a href="/2017/06/07/transparency-around-every-company-who-has-access-to-our-social-data-via-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/07/examples-of-the-openapi-specification-used-for-government-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/cms/cms-quality-payment-program.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/07/examples-of-the-openapi-specification-used-for-government-apis/">Examples Of The OpenAPI Specification Used For Government APIs</a></h3>
			<p><em>07 Jun 2017</em></p>
			<p>
I was answering some questions for my partners over at DreamFactory when it comes to APIs in government, and one of the questions they asked was about some examples of the OpenAPI specification being used in government. To help out, I started going through  my list of government API looking for any examples in the wild–here is what I found:


  
    Federal Election Commission (FEC) (OpenAPI)
  
  
    System for Award Management (SAM) (OpenAPI)
  
  
    US Digital Registry (OpenAPI)
  
  
    18F Open Source Micro Purchasing API (OpenAPI)
  
  
    NASA (Couldn’t find OpenAPI in less than 30 seconds)
  
  
    Centers for Medicare &amp; Medicaid Services (CMS) API for Quality Payment Program Measures (OpenAPI)
  
  
    National Renewal Energy Labratory Transportation Laws and Incentives API (OpenAPI)
  


I am sure there are more OpenAPI in use across government, but this is what I could find in a five-minute search of my API database. It provides us with seven quality examples of OpenAPI being used for documenting government APIs. I don’t see the OpenAPI used for much beyond documentation, but it is still a good start.

If you know of any government APIs that use OpenAPI feel free to let me know. I’d love to keep adding examples to my research so I can pull up quickly when I am asked questions like this in the future, and be able to highlight best practices for API operations in city, county, state, and federal levels of government.

[<a href="/2017/06/07/examples-of-the-openapi-specification-used-for-government-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/07/apis-for-monitoring-the-performance-of-your-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/runscope/1-dashboard-test-performance.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/07/apis-for-monitoring-the-performance-of-your-apis/">APIs For Monitoring The Performance Of Your APIs</a></h3>
			<p><em>07 Jun 2017</em></p>
			<p>I am a big fan of API providers who also have APIs. It may sound silly to say, but you would be surprised how many companies are selling services to API providers and do not actually have an API themselves. So, anytime I find a good example of API service providers launching new APIs that help API providers be more successful, I’m all over it with a story. Today’s example is from my friends over at Runscope with their API Metrics API that lets you “retrieve your API tests performance metrics for each individual test, keep a pulse on your API’s performance over time, and create custom internal or external dashboards with it”. You can filter the request by using 3 different parameters: region - The service region you’re using to run your tests (e.g. us1, us2, eu1, etc.) timeframe - Hour, day, week, or month. Depending on the timeframe you use, the interval between the response times will be different. environment_uuid - Filter by a specific environment, such as test, production, etc. That is a pretty healthy example of everything that is API for me–an API that helps you make sure your APIs are performing as expected. You can not just understand how well your API responds, you can dial that in by region, and paint a clear picture of how well you are doing over time. I like that you can create internal dashboards for communicating this with your organization, but I also like their approach to providing external API performance dashboards so much I am going to add it to my list of building blocks I track on as part of my API performance research. Aight. That concludes today’s showcase of an API service provider making sure they are practicing what they preach and providing APIs for their valuable services. Honestly, I find this to be a fascinating layer of the API sector–the API layer that can orchestrate APIs. I enjoy thinking...[<a href="/2017/06/07/apis-for-monitoring-the-performance-of-your-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/07/a-conference-focused-on-machine-learning-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/papis/papis-api-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/07/a-conference-focused-on-machine-learning-apis/">A Conference Focused On Machine Learning APIs</a></h3>
			<p><em>07 Jun 2017</em></p>
			<p>
I try to pay attention to events going on in the API space beyond just APIStrat in Portland this fall (submit your CFP!!), and I saw a notification for PAPIs in São Paulo in two weeks, as well as Boston in October. I’m glad we’ve always kept @APIStrat a wider community thing, but if I had to pick one vertical to focus on in 2017 and on, it would definitely be machine learning APIs.

PAPIs has been on my radar for a while now, but I think their foresight is going to start paying off this year. While there are a number of trends moving the API space forward, things like microservices, serverless, and GraphQL, nothing will compare to what is happening with machine learning (ML). I think 90% of the ML will be BS, but there will be 5-10% of it that will actually move industries forward in any meaningful way, and the scope of the investment into everything ML is going to be dizzying for the foreseeable future.

Conferences like PAPIs are going to become increasingly important to help us sit down and have conversations about what ML and AI APIs do, or do not do. I see machine learning, cognitive, artificial intelligence and the buzzwords everybody likes to use just as the algorithmic evolution of the API industry. Where we will be moving beyond just data and content APIs as the default, and having a robust toolbox of algorithmic resources to bake into all of our applications will become standard operating procedure. I’m guessing we’ll see an increased presence of PAPIs conferences in cities around the globe, as well as waves of other ML and AI API-focused events pop up.

[<a href="/2017/06/07/a-conference-focused-on-machine-learning-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/06/the-effect-of-visual-design-and-information-content-on-readers-assessments/"><img src="https://s3.amazonaws.com/kinlane-productions2/documentation/api-documentation-research-visualization.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/06/the-effect-of-visual-design-and-information-content-on-readers-assessments/">The Effect of Visual Design and Information Content on Readers’ Assessments</a></h3>
			<p><em>06 Jun 2017</em></p>
			<p>
I have seen a number of research projects looking at API documentation, but this is the most detailed study into how people are seeing, or not seeing the API documentation and other resources we are providing. It is a dissertation for Robert Bennett Watson out of the University of Washington on the Effect of Visual Design and Information Content on Readers’ Assessments of API Reference Topics.

I gave the research paper a read through and it is some lofty academic stuff, but it touches on a number of the things I write about on API Evangelist when it comes to the cognitive load associated with understanding what an API does. I found the resulting conversation from the research to be the most interesting part, discussing how we can improve the flow with our API documentation and reduce interruption time, or as I often call it, “friction”. There are a wealth of ideas in there for helping us think more critically about our API documentation, which has been repeatedly identified as the number one problem area for our developers.

If you are in the business of creating any new API documentation startup your team should be digesting Mr. Watson’s work. This is the first official academic work I’ve seen on the subject of API documentation and is something I’ll be revisiting regularly, attempting to distil down any words of wisdom for my readers. I feel like this work is a sign of larger movements towards the API space beginning to get more coherent in how we approach our API operations. I’m hoping it is something that will lay the groundwork for some more useful API documentation services and tooling.

[<a href="/2017/06/06/the-effect-of-visual-design-and-information-content-on-readers-assessments/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/06/patent-us-8997069-api-descriptions/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-definition.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/06/patent-us-8997069-api-descriptions/">Patent US 8997069: API Descriptions</a></h3>
			<p><em>06 Jun 2017</em></p>
			<p>
There are so many API patents out there, I’m going to have to start posting one a day just to keep up. Lucky for you I begin to get really depressed by all the API patents I lose interest in reading them and begin to work harder looking for positive examples of API in the world, but until then here is today’s depressing as fuck API patent.

Title: API descriptions
Number: US 8997069
Owner: Microsoft Technology Licensing, LLC
Abstract: API description techniques are described for consumption by dynamically-typed languages. In one or more implementations, machine-readable data is parsed to locate descriptions of one or more application programming interfaces (APIs). The descriptions of the one or more application programming interfaces are projected into an alternate form that is different than a form of the machine-readable data.

I don’t mean to be a complete dick here, but why would you think this is a good idea? I get that companies want their employees to develop a patent portfolio, but this one is patenting an essential ingredient that makes APIs work. If you enforce this patent it will be worthless because this whole API thing won’t work, and if you don’t enforce it, it will be worthless because it does nothing–money well spent on the filing fee.

I just need to file my patent on patenting APIs and end all of this nonsense. I know y’all think I’m crazy for my beliefs that APIs shouldn’t be patented, but every time I dive into my patent research I can’t help but think y’all are the crazy ones, and I’m actually sane. I just do not understand how this patent is going to help anyone and represents any of the value that APIs and even a patent can bring to the table.

[<a href="/2017/06/06/patent-us-8997069-api-descriptions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/06/expanding-on-the-api-acronym/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/server-racks-clouds_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/06/expanding-on-the-api-acronym/">Expanding On The API Acronym</a></h3>
			<p><em>06 Jun 2017</em></p>
			<p>
I really dislike acronyms, so the irony surrounding me being the API Evangelist is always present for me. API isn’t just about RESTful APIs to me. API is much more than just the technical, it is also the business and politics of our digital world–something that doesn’t come across in three letters.

As part of my storytelling, I enjoy unpacking the complexity that acronyms are often used to shadow, hopefully making the world of technology a little less intimidating for folks. While space out at lunch the other day I unpacked API and wrote this:


  A - application - the action of putting something into operation.
  P - programming - the action or process of writing computer programs.
  I - interface - interact with another system, person, or organization.


I feel like this has helped unpack not just the acronym, but also the words behind them, helping better speak to what APIs actually do in my opinion. I feel like mobile applications always take the lion share of meaning when it comes to the meaning behind the word application. I also feel like humans are left out of the interface discussion. Storytelling around the acronym helps me provide a little more depth regarding what is API, giving me some easy definitions I can recall throughout my storytelling and API conversations.

Telling stories as allowed me to evolve as a technologist. I used to never see the problem with using acronyms as shorthand for technical complexity, but after telling thousands of stories about a single acronym, I’ve come very acquainted with how they are used to exclude and obfuscate not just technical complexity, but the business and political undertow that exist in technological circles. I really have come to dislike acronyms, and being the API Evangelist keeps this dislike front and center for me, helping me to remember to unpack the complexity whenever I can.

[<a href="/2017/06/06/expanding-on-the-api-acronym/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/06/apis-are-how-our-digital-selves-are-learning-to-speak-with-each-other/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/06/apis-are-how-our-digital-selves-are-learning-to-speak-with-each-other/">APIs Are How Our Digital Selves are Learning To Speak With Each Other</a></h3>
			<p><em>06 Jun 2017</em></p>
			<p>I know this will sound funny to many folks, but when I see APIs, I see language and communication, and humans learning to speak with each other in this new digital world we are creating for ourselves. My friend Erik Wilde (@dret) tweeted a reminder for me that APIs are indeed a language. APIs are languages. show me one #API aspect that cannot be adequately framed in the context of language design practices and challenges.&mdash; Erik Wilde (@dret) June 4, 2017 Every second on our laptops and mobile phone we are communicating with many different companies and individuals. With each wall post, Tweet, photo push, or video stream we are communicating with our friends, family, and the public. Each of these interactions is being defined and facilitated using an API. An API call just for saying something in text, in an image, or video. API is the digital language we use to communicate online and via our mobile devices. Uber geeks like me spend their days trying to map out and understand these direct interactions, as well as the growing number of indirect interactions. For every direct communication, there are usually numerous other indirect communications with advertisers, platform providers, or maybe even law enforcement, researchers, or anyone else with access to the communication channels. We aren’t just learning to directly communicate, we are also being conditioned to participate indirectly in conversations we can’t see–unless you are tuned into the bigger picture of the API economy. When we post that photo, companies are whispering about what is in the photo, where it was taken, and what meaning it has. When we share that news link of Facebook, companies have a discussion about the truthfulness and impact of the link, maybe the psychological profile behind the link and where we fit into their psychological profile database. In some scenarios, they are talking directly about us personally like we are sitting in the room, other times they are...[<a href="/2017/06/06/apis-are-how-our-digital-selves-are-learning-to-speak-with-each-other/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/05/patent-us9462011-determining-trustworthiness-of-api-requests/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-patent-algorithms.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/05/patent-us9462011-determining-trustworthiness-of-api-requests/">Patent US9462011: Determining trustworthiness of API requests</a></h3>
			<p><em>05 Jun 2017</em></p>
			<p>
I’m always fascinated by the patents that get filed related to APIs. Most just have an API that is part of the equation, but some of the patents are directly for an API process. It’s no secret that I’m a patent skeptic. I’m not anti-patent, I just think the process is broken when it comes to the digital world, and specifically when it comes to APIs and interoperability. Here is one of those API patents that show just how broken things are:

Title: Determining trustworthiness of API requests based on source computer applications’ responses to attack messages
Number: US9462011
Owner: CA, Inc.

Abstract: A method includes receiving an application programming interface (API) request from a source computer application that is directed to a destination computer application. An attack response message that is configured to trigger operation of a defined action by the source computer application is sent to the source computer application. Deliverability of the API request to the destination computer application is controlled based on whether the attack response message triggered operation of the defined action. Related operations by API request risk assessment systems are disclosed.

I get that you might want to patent some of the secret sauce behind this process, but when it comes to APIs, and API security I’m thinking we need to keep thinks open, reusable, and interoperable. Obviously, this is just my not so the business savvy view of the world, but from my tech savvy view of how we secure APIs, patented process help nobody.

When it comes to API security you gain an advantage by providing actual solutions and doing it better than anyone else. Then you do not need to defend anything, everyone will be standing in line to buy your services because securing your APIs is critical to doing business in 2017.

[<a href="/2017/06/05/patent-us9462011-determining-trustworthiness-of-api-requests/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/05/extending-your-apps-using-embeddable-serverless-webhooks/"><img src="https://s3.amazonaws.com/kinlane-productions2/auth0/auth0-extensions-screenshot-editor.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/05/extending-your-apps-using-embeddable-serverless-webhooks/">Extending Your Apps Using Embeddable Serverless Webhooks</a></h3>
			<p><em>05 Jun 2017</em></p>
			<p>Auth0 has released a pretty interesting way to extend your web applications using what is an embeddable, serverless, webhooks environment–for lack of a better description. It’s a pretty interesting way to extend applications in a scrappy, hackable, scriptable, webhooky kind of way. The extensions are definitely not for non-developers, but provide a kind of scriptable view source that any brave user could use to get some interesting things done within an existing web application interface. Here are some of the selling features of Auth0 extensions: They are deployed outside of your product and managed externally. They run securely and in isolation from your SaaS application. The SaaS will not go down due to a faulty Webhook. They are generally easy for a developer to create, whether it’s your own engineers, customers, or partners. They can be authored in a number of programming languages. They can use whatever third-party dependencies they need. I think it is an interesting approach to extending existing applications using Webhooks. I’m guessing some users might be intimidated by it, but I could see it be something that developers and tech savvy users could hack together some pretty interesting implementations. Then when you start saving these interesting scripts, making them available to power users via a catalog–I could see some useful things emerge. I remember several jobs I’ve had that had some sort of universal SQL text area within a system, allowing power users to craft and reuse useful SQL scripts–this seems like a similar approach, but for the API age. I’m curious to see where this kind of solution goes. It is a quick way to extend SaaS functionality, allowing users to get more from an application without expensive developer cycles, and offloading the compute to external services. I think it is a creative convergence of what I see as embeddable, serverless, and webhooks–all part of an effective API strategy. I’m hoping it injects some creativity and extensibility into existing apps,...[<a href="/2017/06/05/extending-your-apps-using-embeddable-serverless-webhooks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/05/an-api-you-should-consider-emulating-when-crafting-your-saas-api-business/"><img src="https://blog.pinboard.in/2017/06/pinboard_acquires_delicious/" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/05/an-api-you-should-consider-emulating-when-crafting-your-saas-api-business/">An API You Should Consider Emulating When Crafting Your SaaS / API Business</a></h3>
			<p><em>05 Jun 2017</em></p>
			<p>
The social bookmarking API Pinboard is my favorite API. I feel like it is a model we should all be considering crafting our API-focused businesses. I’ve used Pinboard to curate what I do as the API Evangelist ever since 2011, and it has been one of the most stable and versatile APIs in my stack, doing one thing, and doing it well, reflecting everything that is API from a business perspective.

I feel that Pinboard provides entrepreneurs with a positive model for not just a SaaS business, and API operations, but showing startups that you don’t always need to scale to achieve success. Pinboard acquired their rival Delicious bookmarking site this last week, which has been bought and sold five times now, demonstrating the volatility of startup culture, as well as the viability and potential stability a well-run API business can bring to the table. Providing a model that won’t necessary work in all business scenarios, but does provide us with plenty to consider for our API ideas that probably aren’t VC scale.

I know there will always be startups who are obsessed with the VC model for launching, scaling, and selling a business. I am fine with dealing with some of the volatility brought by these types of companies, but I feel like the lion share of the actual API business will be conducted on the shoulders of giants like Amazon, Google, and Microsoft, and via smaller operators like Pinboard. I’m going to make Pinboard into a textbook example of how to do APIs, helping entrepreneurs understand what healthy patterns already exist out there, providing them with something they can emulate when crafting their SaaS / API business.

[<a href="/2017/06/05/an-api-you-should-consider-emulating-when-crafting-your-saas-api-business/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/05/algorithmia-invests-more-resources-into-machine-learning-apis-for-working-with/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorithmia/algorithmia-cube2-png-1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/05/algorithmia-invests-more-resources-into-machine-learning-apis-for-working-with/">Algorithmia Invests More Resources Into Machine Learning APIs For Working With</a></h3>
			<p><em>05 Jun 2017</em></p>
			<p>
I got my regular email from Algorithmia this last week and I like where they are going with some of their machine learning APIs. They have been heavily investing in machine learning applied to video, allowing for the extraction of information from video, as well as applying interesting transformations to your videos.

Here are some of the video tools they have been working on:


  Introduction to Video Transform: apply image transformations to every frame of a video automatically.
  Introduction to Video Meta-Data Extraction: apply any image recognition algorithms to every frame of a video automatically.
  Deep Dive into Parallelized Video Processing: Check out how we built a highly parallelized video processing pipeline.


These are all things I’m interested in using as part of my drone and other video work that I’ve been working on as a hobby. I’m interested in the video pipeline aspect because it’s fun to work with the video I capture, but I also see the potential when it comes to drones in agriculture and mining, and I am also curious the business models associated with this type of a video pipeline. I think video, images, plus APIs, coupled with the API monetization strategy Algorithmia already has in place is their formula for success.

I’m keeping an eye on what Amazon, Google, and Microsoft are up to, but I think Algorithmia has a first mover advantage when it comes to the economic of all of this. I’m glad they are investing more into their video resources. I think there are endless uses for API-driven pipelines that process images and video and apply machine learning models using APIs, then metered, and made available via an algorithmic catalog like Algorithmia offers.

[<a href="/2017/06/05/algorithmia-invests-more-resources-into-machine-learning-apis-for-working-with/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/02/the-github-repo-stripe-uses-to-manage-their-openapi/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripes-openapi-specification-on-github.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/02/the-github-repo-stripe-uses-to-manage-their-openapi/">The Github Repo Stripe Uses To Manage Their OpenAPI</a></h3>
			<p><em>02 Jun 2017</em></p>
			<p>I’m beating a drum every time I find a company managing their OpenAPI on Github, like we would the other code elements of our API operations. Today’s drumbeat comes from my friend Nicolas Grenié (@picsoung), who posted Stripe’s Github repository for their OpenAPI in our Slack channel for the super cool API Evangelists in the sector. ;-)

Along with the New York Times, Box, and other API providers, Stripe has a dedicated Github repo for managing their OpenAPI definition. This opens up the Stripe API for easily loading in client tools like Restlet Client, and Postman, as we as generating code samples and SDKs using services like APIMATIC. Most importantly, it allows for developers to easily understand the surface area of the Stripe API, in a way that is machine-readable, and portable.

It makes me happy to see leading API providers manage their own OpenAPI using Github like this. The API sector will be able to reach new heights when every single API provider manages their API definitions like this. I know, I know hypermedia folks–everyone should just do hypermedia. Yes, they should. However, we need some initial steps to occur before that is possible, and API providers being able to effectively communicate their API surface area to API consumers in a way that scales and can be used across the API lifecycle is an important part of this evolution. With each OpenAPI I find like this, I get more optimistic that we are getting closer to the future that RESTafarians and hypermedia folks envision–providers are doing the hard work of thinking about the definitions used in their APIs in the context of the entire API lifecycle, and the API consumers who exist along the way.

[<a href="/2017/06/02/the-github-repo-stripe-uses-to-manage-their-openapi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/02/the-depth-and-dimensions-of-monitoring-api-operations/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/02/the-depth-and-dimensions-of-monitoring-api-operations/">The Depth And Dimensions Of Monitoring API Operations</a></h3>
			<p><em>02 Jun 2017</em></p>
			<p>When I play with my Hitch service I am always left thinking about the many dimensions of API monitoring. When you talk about API monitoring in the tech sector conversations almost always start with the API providers and the technical details monitoring of individual APIs. Hopefully, these discussions also focus on API monitoring from the API consumers point of view, but I wanted to also shine a light on companies like Hitch who are adding an additional dimension from the API service provider view of things–which is closer to my vantage point as an analyst. I am an advisor to Hitch because they are a different breed of API monitoring service, that isn’t just focused on the APIs. Hitch brings in the wider view of monitoring the entire operations of an API–if documentation changes, an SDK on Github, or update via Twitter, or a pricing change, you get alerted. As a developer I enjoy being made aware of what is going on across operations, keeping me in tune with not just the technical, but also the business and politics of API platform operations. Another reason I like Hitch, and really the reason behind me writing this post, is that they are helping API providers think about the bigger picture of API monitoring. Helping them think deeply, as well as getting their shit together when it comes to regularly sending out the critical signals us API consumers are tuning into. When you are down in the trenches of operating an API at a large company, it is easy to get caught up in the internal vacuum, forgetting to properly communicate and support your community–Hitch helps keep this bubble from forming, assisting you in keeping an external focus on your community. If you are just embarking on your API journey I recommend tuning into API Evangelist first. ;-) However, if you are unsure of how to properly communicate and support your community I recommend you talk to...[<a href="/2017/06/02/the-depth-and-dimensions-of-monitoring-api-operations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/02/centers-for-medicare-medicaid-services-api-for-quality-payment-program-measures/"><img src="https://s3.amazonaws.com/kinlane-productions2/cms/cms-quality-payment-program.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/02/centers-for-medicare-medicaid-services-api-for-quality-payment-program-measures/">Centers for Medicare & Medicaid Services API for Quality Payment Program Measures</a></h3>
			<p><em>02 Jun 2017</em></p>
			<p>I am regularly using APIs to slice and dice large datasets to help make sense of what is contained within the database behind in a way that other folks can then develop visualizations, reporting, and other applications for use by folks who are closest to the problem we are trying to solve–this opportunity is one of the reasons I have been evangelizing APIs at all level of government over the last seven years. After many years of hard work in the federal government by smart folks at 18F, and at the agencies they serve, we are beginning to see some tools emerge that begin to help us make sense of the overwhelming amount of data that comes out of the government on a regular basis. You can see this in action with the API for Quality Payment Program Measures, out of the Centers for Medicare &amp; Medicaid Services (CMS)–helping make sense of how spending is working, or not working when it comes to healthcare. It has taken years for projects like this to get approved and rolled out. It represents the future of how we make sense of big government, and begin to understand where the waste is, while still also understanding the good that it is occurring. Sadly, as I tell the story, and we see this first wave of APIs coming out of government, the current administration is slashing the budgets behind this type of work. In 2017 I am very concerned for projects like this, as well as future iterations of these API efforts, at a time where we need increased investment in important areas like APIs cracking open health care spending. If you are in healthcare, make sure and spend some time playing with the API, providing CMS with feedback on what works and what doesn’t. If you have the skills, and the resources, maybe you can also invest in developing some visualizations, applications, or other storytelling around the CMA API...[<a href="/2017/06/02/centers-for-medicare-medicaid-services-api-for-quality-payment-program-measures/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/01/managing-your-postman-collection-using-github/"><img src="https://s3.amazonaws.com/kinlane-productions2/apigee/apigee-postman-collection-on-github.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/01/managing-your-postman-collection-using-github/">Managing Your Postman Collection Using Github</a></h3>
			<p><em>01 Jun 2017</em></p>
			<p>
I have been encouraging API providers to publish and manage their API definitions using Github similar to how you’d manage any code. Companies like Box and NY Times are publishing their OpenAPI definitions to a single repository, allowing partners and API consumers to pull the latest version of the API definition and use throughout the API lifecycle.

I stumbled across another example of managing your API definitions using Github, but this time it is the management of your Postman Collections in a Github repo from API management provider Apigee (now Google). The Postman Collection provides a complete description of the Apigee API management surface area, allowing API providers to easily automate or orchestrate their API operations using Apigee.

The Github repository providers a complete Postman Collection, along with instructions on how to load, as well as a Run in Postman button allowing any consumer to instantly load the entire surface area of the Apigee API management solution into their Postman Client. I am a big fan of managing your Postman Collections, as well as OpenAPI definitions in this way, managing the definitions for your API similar to how you manage your code, but also making available for forking, checking out, and integration of these machine-readable definitions anywhere across the API lifecycle.

[<a href="/2017/06/01/managing-your-postman-collection-using-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/01/heavier-investment-in-api-training-will-be-necessary/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/01/heavier-investment-in-api-training-will-be-necessary/">Heavier Investment In API Training Will Be Necessary</a></h3>
			<p><em>01 Jun 2017</em></p>
			<p>&lt;/a&gt;
I was learning about the virtual classes that Github are offering, as I was working on some basic API curriculum for some of my clients, and I was reminded of how important training and education is when it comes to technological adoption. Not everyone learns the same way, and not everyone is an autodidact, and providing training around any technology, platform, or service your company, institution, or government agency is adopting is important.

If you look at the historic spending of leading API companies like Apigee, you’ll see a large chunk of the budget going to educating and bringing would be or existing clients up to speed with the technology in play. Training and education will be a significant portion of each of the trends you see in play like DevOps, Microservices, and Serverless. A core aspect of all of these movements hinges on unwinding legacy technical debt and moving often large groups of people forward with a new way of doing things–if education isn’t a significant portion of your strategy, you will fail.

If you see any interesting API training out there on LinkedIn (Lynda), or on the open web, please let me know, I’d like to start curating more resources for people to use. I am also working with groups to develop their own internal API training and curriculum to match not just the wider API space, but also what is going on internally within an organization. In coming months I will also be going through each of my research areas that might help API providers think about how they are going to tackle API education and help my readers understand that a heavier investment in API training will be necessary to steer the ship where you want it to go.

[<a href="/2017/06/01/heavier-investment-in-api-training-will-be-necessary/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/06/01/data-visualization-and-storytelling-around-museum-collections-using-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/1958.0055_banner_crop.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/06/01/data-visualization-and-storytelling-around-museum-collections-using-apis/">Data Visualization And Storytelling Around Museum Collections Using APIs</a></h3>
			<p><em>01 Jun 2017</em></p>
			<p>I spend my days looking for interesting API stories to tell. Many days I work REAL hard to find anything truly interesting, as there is a lot of repetition and reuse in the API space, both for good and bad. So when I find stories that reflect what I see in my mind when I think API, I’m always very happy. One of these stories is out of the University of Kansas where a fellow named James Miller has teamed up with museum staff as a faculty research fellow for the Integrated Arts Research Initiative (IARI) “to engage researchers across the sciences and humanities in hybrid projects,” said Joey Orr, the museum’s curator for research who coordinates fellowships for IARI. “We’re using database-driven visualization to tell the stories of the Spencer Museum of Art — from its original founding gift to all the items they’ve obtained since then,”…“The term we like to use is ‘storytelling.’ We’re trying to take large amounts of data and tell stories that relate the history and current impact of the Spencer.” How are they going to do this you might ask? Well, APIs of course. They have developed an API on top of the Spencer Museum of Art database, to enable visualization and storytelling. This is what I see in my head when I think API, not the waves of API startups I am seeing come across my desk. APIs allowing access to valuable resources, and enabling storytelling and discussion–this is API. “One of our initial storytelling ideas relates to studying the impact of immigrants and the art they created after settling in the U.S.,” he said. “There are so many pieces in the collection by such artists, and it has been a fun exploratory project.” Sorry, for just posting a bunch of their quotes, but I thought it was worth sharing a few nuggets from the post. Head over to the University of Kansas to read more, and make...[<a href="/2017/06/01/data-visualization-and-storytelling-around-museum-collections-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/31/an-example-of-api-ethics-out-of-cambridge-university/"><img src="https://s3.amazonaws.com/kinlane-productions2/university-of-cambridge/apply-magic-sauce-prediction-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/31/an-example-of-api-ethics-out-of-cambridge-university/">An Example Of API Ethics Out Of Cambridge University</a></h3>
			<p><em>31 May 2017</em></p>
			<p>
I was doing some research into what was going on with the API landscape at universities and I came across the Trait Prediction API from the University of Cambridge. I’m still studying what they have going on from a social API perspective, but I thought their approach to API ethics stood out as something I wanted to explore some more.

The University of Cambridge, “encourage all of our collaborators to adhere to the following ethical principles, in addition to the applicable legal restrictions”:


  Control: Nobody should have predictions made about them without their prior informed consent
  Transparency: The results of any predictions should be shared with individuals in a clear and understandable format
  Benefit: Predictions should be used to improve services and provide a clear benefit to users
  Relevance: It should be clear why the data requested is relevant to the prediction being made


I do not have formal areas of my API research dedicated to API ethics, but I think I just found my first couple of building blocks to add to it when I do fire it up. I’m seeing more discussion of ethics in computing going on in this era of artificial intelligence, machine learning, big data, and surveillance capitalism. It is a conversation I want to help encourage, by finding examples of ethics being injected into the API lifecycle, either at the provider or the consumer level.

With our current track record when it comes to ethics and technology, I’m thinking we are going to need plenty of examples in the future of ethics being a priority, to help shine a light on. Providing clear examples of how to do technology without exploiting and screwing people over, which many people seem to not get.

[<a href="/2017/05/31/an-example-of-api-ethics-out-of-cambridge-university/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/31/adding-three-apimatic-openapi-extensions-to-the-openapi-toolbox/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/apimatic_dx_kits.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/31/adding-three-apimatic-openapi-extensions-to-the-openapi-toolbox/">Adding Three APIMATIC OpenAPI Extensions To The OpenAPI Toolbox</a></h3>
			<p><em>31 May 2017</em></p>
			<p>
I’ve added three OpenAPI extensions from APIMATIC to my OpenAPI Toolbox, adding to the number of extensions I’m tracking on that service providers and tooling developers are using as part of their API solutions. APIMATIC provides SDK code generation services, so their OpenAPI extensions are all about customizing how you deploy code as part of the integration process.

These are the three OpenAPI extensions I am adding from them:


  x-codegen-settings - These settings are globally applicable to all operations and schema definitions.
  x-operation-settings - These settings can be specified inside an "operation" object.
  x-additional-headers - These headers are in addition to any headers required for authentication or defined as parameters.


If you have ever used APIMATIC you know that you can do a lot more than just “SDK generation”, which often has a bad reputation. APIMATIC provides some interesting ways you can use OpenAPI to dial in your SDK, script, and code generation as part of any continuous integration lifecycle.

Providing another example of how you don’t have to live within the constraints of the current OpenAPI spec. Anyone can augment, and extend the current specification to meet your unique needs. Then who knows, maybe it will become useful enough, and something that might eventually be added to the core specification. Which is part of the reason I’m aggregating these specifications, and including them in the OpenAPI Toolbox.

[<a href="/2017/05/31/adding-three-apimatic-openapi-extensions-to-the-openapi-toolbox/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/31/a-perception-of-patent-and-copyright-overlapping-when-it-comes-to-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/oracle-v-google/1991-Patent-Copyright-Overlap-Study.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/31/a-perception-of-patent-and-copyright-overlapping-when-it-comes-to-apis/">A Perception Of Patent And Copyright Overlapping When It Comes To APIs</a></h3>
			<p><em>31 May 2017</em></p>
			<p>I just read an interesting piece by Dennis Crouch over at on Patentlyo asking, “Are Copyright and Patent Overlapping or Mutually Exclusive in Protecting Software Innovations?” The article is challenging the most recent decision in the ongoing Oracle v Google copyright case using a study on “The Patent-Copyright Laws Overlap Study”, prepared at the behest of the House Subcommittee on Intellectual Property and the Administration of Justice in May of 1991. Among the most significant of the Study’s software findings is that there is “no overlap in subject matter: copyright protects the authorship in a set of statements that bring about a certain result in the operation of a computer, and patents cover novel and nonobvious computer processes.” Letter from Ralph Oman and Harry F. Manbeck to the Hon. William J. Hughes, July 17, 1991 (transmitting the Study to the then Chair of the House Subcommittee). I’m interested in this level of discussion because it helps me see the many layers of how patent and copyright law applies, or does not apply in the digital world, and specifically to APIs. Personally, I do not feel that copyright or patents apply to the API definition layer of API operations. Maybe patents can apply to the software processes behind, and maybe copyright could be applied to some more complex, creative software orchestrations using APIs, but really this layer acts like a menu to what is possible, something akin to a restaurant menu, which I have argued in the past. At a time in the growth of the API industry where we should be standardizing, sharing, and ensuring our API definitions are interoperable, we are going backward and further locking up these definitions, and keeping them in our proprietary safe. Instead of locking them up we need to be having more conversation like this about further defining the licensing layers of API operations, separating the backend code, data, schema, API definitions, front-end, and rules that surround them. I...[<a href="/2017/05/31/a-perception-of-patent-and-copyright-overlapping-when-it-comes-to-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/30/exploring-the-possibilities-with-my-drone-prototype-api/"><img src="https://s3.amazonaws.com/drone-recovery/bolanlake21.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/30/exploring-the-possibilities-with-my-drone-prototype-api/">Exploring The Possibilities With My Drone Prototype API</a></h3>
			<p><em>30 May 2017</em></p>
			<p>I enjoy playing with what is possible when it comes to APIs, without all the overhead of actually operating the APIs. I’ve been exploring the world of drones over the last year and it is something that has inevitably collided with my API research, leaving me intrigued by what is possible with drone APIs, as I learn what existing drone providers are doing when it comes to APIs. Drones are the poster child for the Internet of Things. They collect video, take pictures, track location, and best of all–they fly!! Drones have APIs and consume APIs. You can deploy APIs in the cloud, on mobile devices and radio controllers, as well as on the drones themselves. Adding an entirely new dimension, you can also connect a variety of other IoT devices to the drones themselves, things like infrared and network detection, further pushing forward the possibilities. Drones represent the IoT opportunity–both good and bad. The fun stuff, and the really, really scary stuff. Like the rest of the API space, I am fascinated by drones and APIs, and I want to understand what all the providers are doing. I want to aggregate all of this knowledge as part of my API research and share what I learn as stories on API Evangelist, while I also play with ideas of what I would like to see emerge in the drone API space, without actually having to be responsible for making it happen–I needed a safe space I can play. I’m pushing forward my usage of Jekyll and Github to help me play with a new way to prototype APIs, mock their functionality, but also emulate the operations around the APIs. I created version 1.0 of my API Evangelist API prototyping toolkit, where I use Google Spreadsheets, Github, and Jekyll to emulate the definition, design, deployment, management, testing, monitoring, portal, discovery, and other stops along the API lifecycle. The approach allows me to quickly stand up an...[<a href="/2017/05/30/exploring-the-possibilities-with-my-drone-prototype-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/30/apicurio-is-the-open-source-visual-api-design-editor-i-was-looking-for/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/30/apicurio-is-the-open-source-visual-api-design-editor-i-was-looking-for/">Apicurio Is The Open Source Visual API Design Editor I Was Looking For</a></h3>
			<p><em>30 May 2017</em></p>
			<p>I’ve been wanting someone to create an open source API editor for some time, and now the folks over at Red Hat / 3Scale have delivered one called Apicurio. It is a web-based Angular2 app, for visually designing your APIs using OpenAPI, with a Github focus. Apicurio is that blend of visual designer, and code view that I was hoping for, letting you manage all your paths, and definitions using OpenAPI via Github. It doesn’t have all the bells and whistles I’d love to see in my perfect API design editor, but they are just getting going, and I think it is an excellent start. Using Apicurio you can start a new API, or begin with an existing API by importing an OpenAPI (as it should be). When you are editing each path, it breaks up your verbs and has grayed out placeholders for adding any verbs you are missing–great inline API design literacy, helping folks quickly expand the design of their API. It is a slick, intuitive, API design interface which takes seconds to grasp what’s going on and begin expanding the surface area of an API. After making changes you can save it to Github, helping center the API design and definition process around Github, which can then be applied to the center of any API lifecycle. I really like how the design tool is a visual interface but you can always get at the machine readable definition behind, and edit it directly if you prefer. I feel like it is an interface that both developers and non-developers can put to work while still keeping OpenAPI at the center. You can see where they are headed with APIcurio by checking out the roadmap, as well as hints in the interface–like grayed out buttons for testing and documentation. I can see the tool turning into a full blown lifecycle management solutions, allowing you to design, deploy, manage, document, test, and many other useful areas...[<a href="/2017/05/30/apicurio-is-the-open-source-visual-api-design-editor-i-was-looking-for/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/30/api-providers-localizing-compute-for-developers-using-serverless/"><img src="https://s3.amazonaws.com/kinlane-productions2/twilio/twilio-sms-message.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/30/api-providers-localizing-compute-for-developers-using-serverless/">API Providers Localizing Compute For Developers Using Serverless</a></h3>
			<p><em>30 May 2017</em></p>
			<p>
Twilio launched their Twilio Function this last week, localizing serverless infrastructure for Twilio API consumers, when it comes to powering key functionality that Twilio brings to the table. This seems like a logical move for mature API providers, keeping in tune with shifts in how developers are integrating with APIs, and deploying their applications in a DevOps, continuous integration world.

I could see other API providers following Twilio’s lead, jumping on the serverless bandwagon, and localizing compute within their API ecosystems. I can see this approach converging with other movements in the SDK space where service providers like APIMATIC are enabling the continuous deployment of SDKs, samples, and other scripts for API integration. Allowing developers to quickly deploy integration scripts, in the programming language of choice–all baked into their existing API platform developer arrangement.

It makes sense that some of these common approaches that are emerging across the API space like containerization, webhooks, serverless, evented and other real-time technologies make their way to being baked in, or at least augmenting existing API operations. I don’t think that every API provider should be following Twilio’s lead in every area, but they do provide a pretty interesting example consider when we think about where the API space might be headed–I find the most mature API providers are just as important to keep an eye on as much as each wave of startups.

I’ll keep an eye on serverless being localized like this with other API providers. It seems like an opportunity for some provider, to develop a white label solution to help API providers deliver scripting, events, webhooks, and other emerging ways to orchestrate and integrate with APIs like Twilio is doing.

[<a href="/2017/05/30/api-providers-localizing-compute-for-developers-using-serverless/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/25/every-api-should-begin-with-a-github-repository/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/open-referral/open-referral-github-repo-api-spec.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/25/every-api-should-begin-with-a-github-repository/">Every API Should Begin With A Github Repository</a></h3>
			<p><em>25 May 2017</em></p>
			<p>
I’m working on my API definition and design strategy for my human services API work, and as I was doing this Box went all in on Opening, adding to the number of API providers I track on who not just have an OpenAPI but they also use Github as the core management for their API definition.

Part of my API definition and design advice for human service API providers, and the vendors who sell software to them is that they have an OpenAPI and JSON schema defined for their API, and share this either publicly or privately using a Github repository. When I evaluate a new vendor or service provider as part of the Human Services Data API (HSDA) specification I’m beginning to require that they share their API definition and schema using Github–if you don’t have one, I’ll create it for you. Having a machine-readable definition of the surface area of an API, and the underlying schema in a Github repo I can checkout, commit to, and access via an API is essential.

Every API should begin with a Github repository in my opinion, where you can share the API definition, documentation, schema, and have a conversation around these machine readable docs using Github issues. Approaching your API in this way doesn’t just make it easier to find when it comes to API discovery, but it also makes your API contract available at all steps of the API design lifecycle, from design to deprecation.

[<a href="/2017/05/25/every-api-should-begin-with-a-github-repository/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/25/craft-your-api-design-guide-so-you-can-move-to-other-areas-of-the-lifecycle/"><img src="https://s3.amazonaws.com/kinlane-productions2/talks/november-2015/api-lifecycle-tag-cloud.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/25/craft-your-api-design-guide-so-you-can-move-to-other-areas-of-the-lifecycle/">Craft Your API Design Guide So You Can Move To Other Areas of The Lifecycle</a></h3>
			<p><em>25 May 2017</em></p>
			<p>I am working on an API definition and design guide for my human services API work, helping establish a framework for approaching API design as part of the human services data and API specification, but also for implementers to follow in their own individual deployments. Every time I work on the subject of API design, I’m reminded of how far behind the API sector is when it comes to standardizing what it is we do. Every month or so I see a new company publicly share their API design guide. When they do my friend Arnaud always adds to his API Stylebook, adding it to the wealth of information available in his work. I’m happy to see each API design guide release, but in reality, ALL API providers should have an API design guide, and they should also be open to publishing it publicly, showing their consumers they have their act together, and sharing with the wider API community the best practices in play. The lack of companies sharing their API design practices and their API definitions is why we have such a deficiency when it comes to common API patterns in use. It is why we have so many variations of web APIs, as well as the underlying schema. We have an API industry because early practitioners like SalesForce, Amazon, eBay, Flickr, Delicious, Twitter, Youtube, and others were open with their API operations. People emulate what they see and know. Each wave of the API sector depends on the previous wave sharing what they do publicly–it is how this all works. To demonstrate even further about how deficient we are, I do not find companies sharing their guides for API deployment, management, testing, monitoring, clients, and other stops along the API lifecycle. I’m glad we are seeing an uptick in the number of API design guides, but we need this practice to spread to every other stop. We need successful providers to share how...[<a href="/2017/05/25/craft-your-api-design-guide-so-you-can-move-to-other-areas-of-the-lifecycle/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/24/spreadsheet-to-github-for-sample-data-ci/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/24/spreadsheet-to-github-for-sample-data-ci/">Spreadsheet To Github For Sample Data CI</a></h3>
			<p><em>24 May 2017</em></p>
			<p>I’m needing data for use in human service API implementations. I need sample organizations, locations, and services to round off implementations, making it easier to understand what is possible with an API, when you are playing with one of my demos. There are a number of features that require there to be data in these systems, and is always more convincing when it has intuitive, recognizable entries, not just test names, or possibly latin filler text. I need a variety of samples, in many different categories, with a complete phone, address, and other specific data points. I also need this across many different APIs, and ideally, on demand when I set up a new demo instance of the human services API. To accomplish this I wanted to keep things as simple as I can so that non-developer stakeholders could get involved, so I set up a Google spreadsheet with a tab for each type of test data I needed–in this case, it was organizations and locations. Then I created a Github repository, with a Github Pages front-end. After making the spreadsheet public, I pull each worksheet using JavaScript, and write to the Github repository as YAML, using the Github API. It is kind of a poor man’s way of creating test data, then publishing to Github for use in a variety of continuous integration workflows. I can maintain a rich folder of test data sets for a variety of use cases in spreadsheets, and even invite other folks to help me create and manage the data stored in spreadsheets. Then I can publish to a variety of Github repositories as YAML, and integrated into any workflow, loading test data sets into new APIs, and existing APIs as part of testing, monitoring, or even just to make an API seem convincing. To support my work I have a spreadsheet published, and two scripts, one for pulling organizations, and the other for pulling locations–both which publish YAML...[<a href="/2017/05/24/spreadsheet-to-github-for-sample-data-ci/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/24/my-api-design-checklist-for-this-version-of-the-human-services-data-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-design-checklist.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/24/my-api-design-checklist-for-this-version-of-the-human-services-data-api/">My API Design Checklist For This Version Of The Human Services Data API</a></h3>
			<p><em>24 May 2017</em></p>
			<p>I am going through my API design checklist for the Human Services Data API work I am doing. I’m trying to make sure I’m not forgetting anything before I propose a v1.1 OpenAPI draft, so I pulled together a simple checklist I wanted to share with other stakeholders, and hopefully also help keep me focused. First, to support my API design work I got to work on these areas for defining the HSDS schema and the HSDA definition: JSON Schema - I generated a JSON Schema from the HSDS documentation. OpenAPI - I crafted an OpenAPI for the API, generating GET, POST, PUT, and DELETE methods for 100% of the schema, and reflective its use in the API request and response. Github Repo - I published it all in a Github repository for sharing with stakeholders, and programmatic usage across any tooling and applications being developed. Then I reviewed the core elements of my API design to make sure I had everything I wanted to cover in this cycle, with the resources we have: Domain(s) - Right now I’m going with api.example.com, and developer.example.com for the portal. Versioning - I know many of my friends are gonna give me grief, but I’m putting versioning in the URL, keeping things front and center, and in alignment with the versioning of the schema. Paths - Really not much to consider here as the paths are derived from the schema definitions, providing a pretty simple, and intuitive design for paths–will continue adding guidance for future APIs. Verbs - A major part of this release was making sure 100% of the surface area of the HSDS schema add the ability to POST, PUT, and DELETE, as well as just GET a response. I’m not addressing PATCH in this cycle, but it is on the roadmap. Parameters - There are only a handful of query parameters present in the primary paths (organizations, locations, services), and a robust set for use...[<a href="/2017/05/24/my-api-design-checklist-for-this-version-of-the-human-services-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/24/how-twitter-handles-sorting-for-their-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/24/how-twitter-handles-sorting-for-their-api/">How Twitter Handles Sorting For Their API</a></h3>
			<p><em>24 May 2017</em></p>
			<p>
I was looking into some of the common approaches by API providers for sorting of data in API responses. I’m not in the business of finding the right answer, I am in the business of finding successful examples from APIs(brands) that people are familiar with–I thought Twitter’s page in their API documentation dedicated to sorting was worth noting.

When you craft your Twitter API request you just append sort_by=[attribute name]-[asc/desc] where the attribute is a valid attribute that is returned in the JSON of your GET request. An example of this is using ?name-asc to sort by name alphabetically or ?name-desc to sort in reverse. Providing a pretty basic approach that API providers can consider when designing sort functionality in their API.

I’ll be documenting all the approaches I find from known providers, developing a catalog of options for a variety of use cases. I’ll also spend some time looking at how GraphQL tackles the problem, providing a much more holistic approach to managing data using APIs. When I go through my API design checklist each round, I like adding a variety of diverse tooling to it, based upon examples I find from the strongest API providers. Healthy diversity in your API toolbox will be increasingly important to assist in tackling a variety of data, content, or algorithmic challenges.

[<a href="/2017/05/24/how-twitter-handles-sorting-for-their-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/24/considering-using-http-prefer-header-instead-of-field-filtering-for-this-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/http-prefer-header.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/24/considering-using-http-prefer-header-instead-of-field-filtering-for-this-api/">Considering Using HTTP Prefer Header Instead Of Field Filtering For This API</a></h3>
			<p><em>24 May 2017</em></p>
			<p>I am working my way through a variety of API design considerations for the Human Services Data API (HSDA)that I’m working on with Open Referral. I was working through my thoughts on how I wanted to approach the filtering of the underlying data schema of the API, and Shelby Switzer (@switzerly) suggested I follow Irakli Nadareishvili’s advice and consider using RFC 7240 -the Prefer Header for HTTP, instead of some of the commonly seen approaches to filtering which fields are returned in an API response. I find this approach to be of interest for this Human Services Data API implementation because I want to lean on API design, over providing parameters for consumers to dial in the query they are looking for. While I’m not opposed to going down the route of providing a more parameter based approach to defining API responses, in the beginning I want to carefully craft endpoints for specific use cases, and I think the usage of the HTTP Prefer Header helps extend this to the schema, allowing me to craft simple, full, or specialized representations of the schema for a variety of potential use cases. (ie. mobile, voice, bot) It adds a new dimension to API design for me. Since I’ve been using OpenAPI I’ve gotten better at considering the schema alongside the surface area of the APIs I design, showing how it is used in the request and response structure of my APIs. I like the idea of providing tailored schema in responses over allowing consumers to dynamically filter the schema that is returned using request parameters. At some point, I can see embracing a GraphQL approach to this, but I don’t think that human service data stewards will always know what they want, and we need to invest in a thoughtful set design patterns that reflect exactly the schema they will need. Early on in this effort, I like allowing API consumers to request minimal, standard or full...[<a href="/2017/05/24/considering-using-http-prefer-header-instead-of-field-filtering-for-this-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/23/thinking-about-the-privacy-and-security-of-public-data-using-api-management/"><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/kin-lane-api-days-berlin-respect-privacy.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/23/thinking-about-the-privacy-and-security-of-public-data-using-api-management/">Thinking About The Privacy And Security Of Public Data Using API Management</a></h3>
			<p><em>23 May 2017</em></p>
			<p>When I suggest modern approaches to API management be applied to public data I always get a few open data folks who push back saying that public data shouldn’t be locked up, and needs to always be publicly available–as the open data gods intended. I get it, and I agree that public data should be easily accessible, but there are increasingly a number of unintended consequences that data stewards need to consider before they publish public data to the web in 2017. I’m going through this exercise with my recommendations and guidance for municipal 211 operators when it comes to implementing Open Referral’s Human Services Data API (HSDA). The schema and API definition centers around the storage and access to organizations, locations, services, contacts, and other key data for human services offered in any city–things like mental health resources, suicide assistance, food banks, and other things we humans need on a day to day basis. This data should be publicly available, and easy to access. We want people to find the resources they need at the local level–this is the mission. However, once you get to know the data, you start understanding the importance of not everything being 100% public by default. When you come across listings Muslim faith, and LGBTQ services, or possibly domestic violence shelters, and needle exchanges. They are numerous types of listings where we need to be having sincere discussions around security and privacy concerns, and possibly think twice about publishing all or part of a dataset. This is where modern approaches to API management can lend a hand. Where we can design specific endpoints, that pull specialized information for specific groups of people, and define who has access through API rate limiting. Right now my HSDA implementation has two access groups, public and private. Every GET path is publicly available, and if you want to POST, PUT, or DELETE data you will need an API key. As I consider my...[<a href="/2017/05/23/thinking-about-the-privacy-and-security-of-public-data-using-api-management/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/23/some-thoughts-on-openapi-not-being-the-solution/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/losangelescloudy/blue_circuit/file-00_00_35_50.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/23/some-thoughts-on-openapi-not-being-the-solution/">Some Thoughts On OpenAPI Not Being The Solution</a></h3>
			<p><em>23 May 2017</em></p>
			<p>I get regular waves of folks who chime in anytime I push on one of the hot-button topics on my site like hypermedia and OpenAPI. I have a couple of messages in my inbox regarding some recent stories I’ve done about OpenAPI recently, and how it isn’t sustainable, and we should be putting hypermedia practices to work. I’m still working on my responses, but I wanted to think through some of my thoughts here on the blog before I respond–I like to simmer on these things, releasing the emotional exhaust before I respond. When it comes to the arguments from the hypermedia folks, the short answer is that I agree. I think many of the APIs I’m seeing designed using OpenAPI would benefit from some hypermedia patterns. However, there is such a big gap between where people are, and where we need to be for hypermedia to become the default, and getting people there is easier said than done. I view OpenAPI as a scaffolding or bridge to transport API designers, developers, and architects at scale from where we are, to where we need to be–at scale. I wish I could snap my fingers and everyone understood how the web works and understood the pros and cons of each of the leading hypermedia types. Many developers do not even understand how the web works. Hell, I’m still learning new things every day, and I’ve been doing this full time for seven years. Most developers still do not even properly include and use HTTP status codes in their simple API designs, let alone understand the intricate relationship possibilities between their resources, and the clients that will be consuming them. Think about it, as developer, I don’t even have time, budget or care to articulate the details of why a response failed, and you expect that I have the time, budget, are care about link relations, and the evolution of the clients build on top of my...[<a href="/2017/05/23/some-thoughts-on-openapi-not-being-the-solution/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/23/avoid-moving-too-fast-for-my-api-audience/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/freeway_atari_missle.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/23/avoid-moving-too-fast-for-my-api-audience/">Avoid Moving Too Fast For My API Audience</a></h3>
			<p><em>23 May 2017</em></p>
			<p>I am stepping back to today and thinking about a pretty long list of API design considerations for the Human Services Data API (HSDA), providing guidance for municipal 211 who are implementing an API. I’m making simple API design decisions from how I define query parameters all the way to hypermedia decisions for the version 2.0 of the HSDA API. There are a ton of things I want to do with this API design. I really want folks involved with municipal 211 operations to be adopting it, helping ensure their operations are interoperable, and I can help incentivize developers to build some interesting applications. As I think through the laundry list of things I want, I keep coming back to my audience of practitioners, you know the people on the ground with 211 operations that I want to adopt an API way of doing things. My target audience isn’t steeped in API. They are regular folks trying to get things done on a daily basis. This move from v1.0 to v.1 is incremental. It is not about anything big. Primarily this move was to make sure the API reflected 100% of the surface area of the Human Services Data Specification (HSDS), keeping in sync with the schema’s move from v1.0 to v1.1, and not much more. I need to onboard folks with the concept of HSDS, and what access looks like using the API–I do not have much more bandwidth to do much else. I want to avoid moving too fast for my API audience. I can see version 2,3, even 4 in my head as the API architect and designer, but am I think of me, or my potential consumers? I’m going to seize this opportunity to educate my target audience about APIs using the road map for the API specification. I have a huge amount of education of 211 operators, as well as the existing vendors who sell them software when it comes...[<a href="/2017/05/23/avoid-moving-too-fast-for-my-api-audience/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/22/keen-io-pushing-forward-the-data-schema-conversation/"><img src="https://s3.amazonaws.com/kinlane-productions2/keen/1-6YVuTRnIIEM7o6XQO-QtMA.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/22/keen-io-pushing-forward-the-data-schema-conversation/">Keen IO Pushing Forward The Data Schema Conversation</a></h3>
			<p><em>22 May 2017</em></p>
			<p>
I wrote earlier this year that I would like us all to focus more on our schema and definitions of our data we use across API operations. Since then I’ve been keeping an eye out for any other interesting signs in this area like Postman with their data editor, and now I’ve come across the Streams Manager for inspecting the data schema of your event collections in Keen IO..

With Streams Manager you can:


  Inspect and review the data schema for each of your event collections
  Review the last 10 events for each of your event collections
  Delete event collections that are no longer needed
  Inspect the trends across your combined data streams over the last 30-day period


Keen IO provides us with an interesting approach to getting in tune with the schema across your event collections. I’d like to see more of this across the API lifecycle. I understand that companies like Runscope, Stoplight, Postman, and others already let us peek inside of each API call, which includes a look at the schema in play. This is good, but I’d like to see more schema management solutions at this layer helping API providers from design to deprecation.

In 2017 we all have an insane amount of bits and bytes flowing around us in our daily business operations. APIs are only enabling this to grow, opening up access to our bits to our partners and on the open web. We need more solutions like Keen’s Stream Manager, but for every layer of the API stack, allowing us to get our schema house in order, and make sense of the growing data bits we are producing, managing, and sharing.

[<a href="/2017/05/22/keen-io-pushing-forward-the-data-schema-conversation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/22/box-goes-all-in-on-openapi/"><img src="https://s3.amazonaws.com/kinlane-productions2/box/embracing-openapi-at-box" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/22/box-goes-all-in-on-openapi/">Box Goes All In On OpenAPI</a></h3>
			<p><em>22 May 2017</em></p>
			<p>
Box has gone all in on OpenAPI. They have published an OpenAPI for their document and storage API on Github, where it can be used in a variety of tools and services, as well as be maintained as part of the Box platform operations. Adding to the number of high-profile APIs managing their OpenAPI definitions on Github, like Box, and the NY Times.

As part of their OpenaPI release, Box published a blog post that touches on all the major benefits of having an OpenAPI, like forking on Github for integration into your workflow, generating documentation and visualizations, code, mock APIs, and even monitoring and testing using Runscope. It’s good to see a major API provider drinking the OpenAPI Kool-Aid, and working to reduce friction for their developers.

I would love to not be in the business of crafting complete OpenAPIs for API providers. I would like every single API provider to be maintaining their own OpenAPI on Github like Box and [NY Times(http://apievangelist.com/2017/03/01/new-york-times-manages-their-openapi-using-github/) does. Then I (we) could spend time indexing, curating, and developing interesting tooling and visualizations to help us tell stories around APIs, helping developers and business owners understand what is possible with the growing number of APIs available today.

[<a href="/2017/05/22/box-goes-all-in-on-openapi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/19/on-device-machine-learning-api-stack/"><img src="https://s3.amazonaws.com/kinlane-productions2/google/tensorflow/tensorflow-lite-images.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/19/on-device-machine-learning-api-stack/">On Device Machine Learning API Stack</a></h3>
			<p><em>19 May 2017</em></p>
			<p>
I was reading about Google’s TensorFlowLite in Techcrunch, and their mention of Facebook’s Caffe2Go, and I was reminded of a conversation I was having with the Oxford Dictionaries API team a couple months ago.

The OED and other dictionary and language content API teams wanted to learn more about on-device API deployment, so their dictionaries could become the default. I have asked when we will have containers natively on our routers a while ago, but I’d also like to add to that request–when will we have a stack of containers on device where we can deploy API resources that can be used by applications, and augment the existing on-device hardware and OS APIs?

API providers should be able to deploy their APIs exactly here they are needed. API deployment, management, monitoring, logging, and analytics should exist by default in these micro-containerized environments on any devices. Whether it’s our mobile phones, our automobiles, or the weather, solar, or other industrial device integration, we are going to new API-driven data, ML, AI, augmented, and other resources on-device, in a localized environment.

[<a href="/2017/05/19/on-device-machine-learning-api-stack/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/19/its-not-just-the-technology-api-monitoring-means-you-care/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-api-hold.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/19/its-not-just-the-technology-api-monitoring-means-you-care/">Its Not Just The Technology: API Monitoring Means You Care</a></h3>
			<p><em>19 May 2017</em></p>
			<p>
I was just messing around with a friend online about monitoring of our monitoring tools, where I said that I have a monitor setup to monitor whether or not I care about monitoring. I was half joking, but in reality, giving a shit is actually a pretty critical component of monitoring when you think about it. Nobody monitors something they don’t care about. While monitoring in the world of APIsn might mean a variety of things, I’m guessing that caring about those resources is a piece of every single monitoring configuration.

This has come up before in conversation with my friend Dave O’Neill of APIMetrics, where he tells stories of developers signing up for their service, running the reports they need to satisfy management or customers, then they turn off the service. I think this type of behavior exists at all levels, with many reasons why someone truly doesn’t care about a service actually performing as promised, and doing what it takes to rise to the occasion–resulting in the instability, and unreliability that APIs that gets touted in the tech blogosphere.

There are many reasons management or developers will not truly care when it comes to monitoring the availability, reliability, and security of an API. Demonstrating yet another aspect of the API space that is more business and politics, than it is ever technical. We are seeing this play out online with the flakiness of websites, applications, devices, and the networks we depend on daily, and the waves of breaches, vulnerabilities, and general cyber(in)security. This is a human problem, not a technical, but there are many services and tools that can help mitigate people not caring.

[<a href="/2017/05/19/its-not-just-the-technology-api-monitoring-means-you-care/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/19/evolving-api-sdks-at-google-with-storage-logging-and-analytics/"><img src="https://s3.amazonaws.com/kinlane-productions2/google/sdk/google-sdk-essential-tools.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/19/evolving-api-sdks-at-google-with-storage-logging-and-analytics/">Evolving API SDKs at Google With Storage, Logging and Analytics</a></h3>
			<p><em>19 May 2017</em></p>
			<p>
One layer of my API research is dedicated to keeping track on what is going on with API software development kits (SDK). I have been looking at trends in SDK evolution as part of continuous integration and deployment, increased analytics at the SDK layer, and SDKs getting more specialized in the last year. This is a conversation that Google is continuing to move forward by focusing on enhanced storage, logging, and analytics at the SDK level.

Google provides a nice example of how API providers are increasing the number of available resources at the SDK layer, beyond just handling API requests and responses, and authentication. I’ll try to carve out some time to paint a bigger picture of what Google is up to with SDKs. All their experience developing and supporting SDKs across hundreds of public APIs seems to be coming to a head with the Google Cloud SDK effort(s).

I’m optimistic about the changes at the SDK level, so much I’m even advising APIMATIC on their strategy, but I’m also nervous about some of the negative consequences that will come with expanding this layer of API integration–things like latency, surveillance, privacy, and security are top of mind. I’ll keep monitoring what is going on, and do deeper dives into SDK approaches when I have the time and $money$, and if nothing else I will just craft regular stories about how SDKs are evolving.

Disclosure: I am an advisor of APIMATIC.

[<a href="/2017/05/19/evolving-api-sdks-at-google-with-storage-logging-and-analytics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/18/the-parrot-sequoia-api-is-nice-and-simple-for-iot/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/18/the-parrot-sequoia-api-is-nice-and-simple-for-iot/">The Parrot Sequoia API Is Nice And Simple For IoT</a></h3>
			<p><em>18 May 2017</em></p>
			<p>
I’m profiling a number of drone APIs lately and I came across some interesting APIs out of Parrot. Not all of the APIs are for drones, but I thought they were clean and simple examples of what IoT APIs can look like.

The API for the Parrot Sequoia camera can be controlled over USB, WIFI, allowing you to change settings, calibrate the sensors, trigger image capture and manage memory, and files.

Here are the paths for the device:


  /capture: to get the Sequoia capture state, start and stop a capture
  /config: to get and set the configuration of the camera
  /status: to get all information about the Sequoia physical state
  /calibration: to get the calibration status, start and stop a calibration
  /storage: to get informations about memory
  /file: to get files and folders information
  /download: to download files
  /delete: to delete files and folders
  /version: to get serial number and software version
  /wifi: to get the Sequoia SSID
  /manualmode: to get and set ISO and exposure manually
  /websocket: to use WebSocket notifications on asynchronous events


I like the simple use of API design to express what is possible with an IoT device and that a small hand-held deployable camera and sensor can be defined in this way. While you still need some coding skills to bring any integration life, anyone could land on the API page and pretty quickly understand what is possible with the device API.

When I come across simple approaches to IoT devices using web APIs I try to write about them, adding them to my research when it comes to IoT APIs. It gives me an easy way to find it again in the future, but also hopefully provides IoT manufacturers some examples of how you can do this as simply and effectively as you can.

I need as many examples of how APIs can be in the cloud, or even on a device like the Parrot Sequoia API.

[<a href="/2017/05/18/the-parrot-sequoia-api-is-nice-and-simple-for-iot/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/18/my-google-sheet-driven-product-api-and-web-page/"><img src="http://apievangelist.com/images/product-square.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/18/my-google-sheet-driven-product-api-and-web-page/">My Google Sheet Driven Product API And Web Page</a></h3>
			<p><em>18 May 2017</em></p>
			<p>I am in the process of eliminating the MySQL backend behind much of my research, eliminating a business expense, as well as an unnecessary complexity in my architecture. There really is no reason for the data I use in my business to be in a database. Nothing I track on tends to go beyond 10K rows, with most of the tables actually being less than 100 rows–perfect for spreadsheets, and my new static approach to delivering APIs, and websites for my research. The time had come to update some of the products on my website, and I thought my product page was a perfect candidate for this approach, providing me with the following elements: Products Google Sheet - I have a simple spreadsheet with all of my products in it. Jekyll YAML Data Store - I have a YAML data store in the _data folder for API Evangelist. Google Sheet to YAML Sync - I have a JavaScript function that pulls the data from the Google Sheet, converts it to YAML, and writes to the _data folder in the Jekyll repository. Products Web Page - I have a page that lists all the products in the YAML file as HTML using Liquid. Products API - I have a JSON page that lists all the products in the YAML file as JSON using Liquid. This simple approach to publishing static APIs using Google Sheets and Github is working well for little data like this–I am all about the little data, while everyone else is excited about big data. ;-) I even have the beginning of some documentation and an updated APIs.json for my website. Next, I’ll work through the rest of my projects, organizations, tools, and other data I track on as part of my API research. I’ll be publishing a complete snapshot of this data at API Evangelist, as well as subsets of it at each of the individual research projects. When I’m done I’ll...[<a href="/2017/05/18/my-google-sheet-driven-product-api-and-web-page/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/18/15-topics-to-help-folks-see-the-business-potential-of-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-business-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/18/15-topics-to-help-folks-see-the-business-potential-of-apis/">15 Topics To Help Folks See The Business Potential Of APIs</a></h3>
			<p><em>18 May 2017</em></p>
			<p>One of my clients asked me for fifteen bullet points of what I’d say to help convince folks at his company that APIs are the future, and have potentially viable business models. While helping convince people of the market value of APIs is not really my game anymore, I’m still interested in putting on my business of APIs hat, and playing this game to see what I can brainstorm to convince folks to be more open with their APIs. Here are the fifteen stories from the API space that I would share with folks to help them understand the potential. Web - Remember asking about the viability about the web? That was barely 20 years ago. APIs are just the next iteration of the web, and instead of just delivering HTML to humans for viewing in the browser, it is about sharing machine-readable versions for use in mobile, devices, and other types of applications. Cloud - The secret to Amazon’s success has been APIs. Their ability to disrupt retail commerce, and impact almost every other business sector with the cloud was API-driven. Mobile - APIs are how data, content, and algorithms are delivered to mobile devices, as well as provides developers with access to other device capabilities like the camera, GPS, and other essential aspects of our ubiquitous mobile devices. SalesForce - SalesForce has disrupted the CRM and sales market with it’s API-driven approach to software since 2001, generating 50% of its revenue via APIs. Twitter - Well, maybe Twitter is not the poster child of revenue, but I do think they provide an example of how an API can create a viable ecosystem where there is money to be made building businesses. There are numerous successful startups born out of the Twitter API ecosystem, and Twitter itself is a great example of what is possible with APIs–both good and bad. Twilio - Twilio is the poster child for how you do APIs right, build...[<a href="/2017/05/18/15-topics-to-help-folks-see-the-business-potential-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/17/studying-how-providers-are-supporting-batch-api-requests/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-bulk-requests.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/17/studying-how-providers-are-supporting-batch-api-requests/">Studying How Providers Are Supporting Batch API Requests</a></h3>
			<p><em>17 May 2017</em></p>
			<p>

A recent addition to my API research is the concept of making batch API requests. I was reminded of this during a webinar I did with Cloud Elements when they cited batch API requests as an area needing improvement in their State of API Integration report. I had also recently come across several batch APIs while profiling the Google API stack, so I already had the topic in my notebook, but Cloud Element pushed me to add the topic to my research.

Here are a handful of batch API implementations I am working through, to better understand how providers are approaching the problem:

Facebook Graph API
[Google Cloud Storage](https://cloud.google.com/storage/docs/json_api/v1/how-tos/batch
Full Contact
Zendesk
SalesForce
Microsoft Office
Amazon
MailChimp
Meetup

As I do, in my approach to API research, I will process the common patterns I come across in each of these implementations, then add as building blocks in my API design research, hopefully providing some details API providers can consider early on in the API lifecycle. I’m not looking to tell people how to deliver batch APIs–I am just looking to shine a light on how the successful APIs are already doing it.

I feel like batch APIs are a response to more APIs, and less direct database access or full data download availability. While modular, simple APIs that do one thing well works in many situations, sometimes you need to move large amounts of resources around and make API requests that do more than just update a single resource or database record. I’ll file this research under API design, but I’ll migrate make a mention of it at the database and other levels, as I identify the variances in how bulk API requests are being made and the solutions they are providing.

[<a href="/2017/05/17/studying-how-providers-are-supporting-batch-api-requests/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/17/json-schema-for-openapi-version-3-0/"><img src="https://s3.amazonaws.com/kinlane-productions2/json/json-schema.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/17/json-schema-for-openapi-version-3-0/">JSON Schema For OpenAPI Version 3.0</a></h3>
			<p><em>17 May 2017</em></p>
			<p>
We are inching closer to a final release of version 3.0 for the OpenAPI specification, with the official version currently set at 3.0.0-rc1. We are beginning to see tooling emerge, and services like APIMATIC are already supporting version 3.0 when it comes to SDK generation, as well their API Transformer conversion tool.

I am working on an OpenAPI validation solutions tailored specifically for municipal API deployments and was working with the JSON Schema for version 2.0 of the API specification.  I wanted to help make my work be as ready for the future of the API specification and wanted to see if there was a JSON Schema for version 3.0 of the OpenAPI specification. I couldn’t find anything in the new branch of the repository, so I set out seeing if anyone else has been working on it.

While I was searching I saw Tim Burks share that he and Mike Ralphson were working on one over at the Google Gnostic project. It looks like it is still a work in progress, but it provides us with a starting point to work from.

I will keep an eye out on the OpenAPI repo for when they add an official JSON Schema for version 3.0. As I’ve learned more about JSON Schema, the more I’m learning about how it helps harmonize and stabilizing tooling being developed around any schema. Without it, you get some pretty unreliable results, but with it, you can achieve some pretty scalable consistency across API tooling and services–something the API space needs as much as it can get in 2017.

[<a href="/2017/05/17/json-schema-for-openapi-version-3-0/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/17/focusing-on-what-you-do-best-while-leveraging-apis-to-not-reinvent-the-wheel/"><img src="https://s3.amazonaws.com/kinlane-productions2/keen_logo_dark.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/17/focusing-on-what-you-do-best-while-leveraging-apis-to-not-reinvent-the-wheel/">Focusing On What You Do Best While Leveraging APIs To Not Reinvent The Wheel</a></h3>
			<p><em>17 May 2017</em></p>
			<p>There are some pretty proven API solutions out there these days. I had to explain to someone a call the other day that in 2017 you shouldn’t ever roll your own API signup, registration, rate limiting, reporting, logging, and other API management features–there are too many proven API management solutions on the market these days (cough, 3Scale, Restlet, DreamFactory, or Tyk) As a penny-pinching small business owner who is also a programmer, I am always struggling with the question of whether I should be buying or building. However, when it comes to some of the more proven, well-laid API sectors–I know better. One of these areas I will never develop my own tooling is when it comes to analytics. There are just too many quality analytics solutions available out there who are API-driven. One of these is Keen.io, who describe themselves as “APIs for capturing, analyzing, and embedding event data in everything you build”, provided an example of this in action, in their unstoppable API era post: One of Keen’s largest customers stealthily uses Keen’s APIs to white label an in-app analytics suite for their Fortune 500 customers. Their delivery manager probably made the point most succinctly: “It would have taken our team two years to deliver what we’ve done with Keen in nine weeks.” As a developer, we like to think we can doing anything on our own, and we often also underestimate the time it will take to accomplish something. I think I’m going to go through the high grade, well-established resources available in my API Stack research, and see which areas I feel contain solid API providers, and I would think twice about reinventing the wheel in. Something like analytics always seems like it is easy to do at first, but then once you are down the rabbit hole trying to do at scale, and things begin to change. I prefer to not get distracted by the nuts and bolts of things...[<a href="/2017/05/17/focusing-on-what-you-do-best-while-leveraging-apis-to-not-reinvent-the-wheel/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/16/the-human-services-schema-defines-the-storage-and-the-api-defines-access/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/16/the-human-services-schema-defines-the-storage-and-the-api-defines-access/">The Human Services Schema Defines The Storage And The API Defines Access</a></h3>
			<p><em>16 May 2017</em></p>
			<p>
I’m comparing five separate vendor API implementations with the Human Services API standard I’m working on at the moment. I’m looking to push version 1.0 of the API towards a 1.1 with some incremental, forward-thinking changes.

During This phase of the project, I’m looking to get as much feedback on the API interface from commercial vendors. The Human Services schema is being moved forward by a separate, but overlapping group, and has already gone through a feedback phase, and has officially released version 1.1 of the schema–I’m looking to do the same for the API.

Even though the Human Services schema is present, the purpose of the API definition is to open up discussion about what access to that data looks like, with the OpenAPI for the Human Services API acting as a distributed and repeatable contract governing how we access publicly available human services data.

The contract provided by the Human Services API defines how stakeholders can access organizations, locations, services. The Human Services schema defines how human services data is stored, and with the assistance of the API will be defined in transit for every request made, as well as the response that is given.

If we are going to get thousands of municipalities exchanging data with each other, as well as with the growing number of applications and systems they are using to serve the public, we will need a shared definition for how data is stored, as well as accessed by everyone involved. As I prepare responses to vendors involved in the feedback loop, I just wanted to gather my thoughts regarding the separation between the schema efforts and the API efforts.

[<a href="/2017/05/16/the-human-services-schema-defines-the-storage-and-the-api-defines-access/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/16/key-factors-determining-who-succeeds-in-the-api-and-ml-marketplace-game/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-marketplace.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/16/key-factors-determining-who-succeeds-in-the-api-and-ml-marketplace-game/">Key Factors Determining Who Succeeds In The API and ML Marketplace Game</a></h3>
			<p><em>16 May 2017</em></p>
			<p>I was having a discussion with an investor today about the potential of algorithmic-centered API marketplaces. I’m not talking about API marketplaces like Mashape, I’m more talking about ML API marketplaces like Algorithmia. This conversation spans multiple areas of my API lifecycle research, so I wanted to explore my thoughts on the subject some more. I really do not get excited about API marketplaces when you think just about API discovery–how do I find an API? We need solutions in this area, but I feel good implementations will immediately move from useful to commodity, with companies like Amazon already pushing this towards a reality. There are a handful of key factors for determining who ultimately wins the API Machine Learning (ML) marketplace game: Always Modular - Everything has to be decoupled and deliver micro value. Vendors will be tempted to build in dependency and emphasize relationships and partnerships, but the smaller and more modular will always win out. Easy Multi-Cloud - Whatever is available in a marketplace has to be available on all major platforms. Even if the marketplace is AWS, each unit of compute has to be transferrable to Google or Azure cloud without ANY friction. Enterprise Ready - The biggest failure of API marketplaces has always been being public. On-premise and private cloud API ML marketplaces will always be more successful that their public counterparts. The marketplace that caters to the enterprise will do well. Financial Engine - The key to markets are their financial engines. This is one area AWS is way ahead of the game, with their approach to monetizing digital bits, and their sophisticated market creating pricing calculators for estimating and predicting costs gives them a significant advantage. Whichever marketplaces allows for innovation at the financial engine level will win. Definition Driven - Marketplaces of the future will have to be definition driven. Everything has to have a YAML or JSON definition, from the API interface, and schema defining inputs...[<a href="/2017/05/16/key-factors-determining-who-succeeds-in-the-api-and-ml-marketplace-game/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/16/google-spanner-is-a-database-with-an-api-core/"><img src="https://s3.amazonaws.com/kinlane-productions2/google/spanner/google-spanner.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/16/google-spanner-is-a-database-with-an-api-core/">Google Spanner Is A Database With An API Core</a></h3>
			<p><em>16 May 2017</em></p>
			<p>

I saw the news that Google’s Spanner Database is ready for prime time, and I wanted to connect it with a note I took at the Google Analyst Summit a few months back–that gRPC is the heart of the database solution. I’m not intimate with the Spanner architecture, approach, or codebase yet, but the API focus, both gRPC core, and REST APIs for a database platform are very interesting.

My first programming job was in 1987, developing COBOL databases. I’ve watched the database world evolve, contributing to my interest in APIs, and I have to say Google Spanner isn’t something I anticipated. Databases have always been where you start deploying an API, but Spanner feels like something new, where the database and the API are one, and the way the database does everything internally and externally is done via APIs (gRPC).

Now that Spanner Database is ready for prime time, I will invest some more time in standing up an instance of it and get to work playing with what is possible with the REST APIs. I also want to push forward my grPC education by hacking on this side of the database’s interface. Spanner feels like a pretty seismic shift in how we do APIs, and how we do them at scale–when you combine this with the elasticity of the cloud, and the simplicity of RESTful interfaces I think there is a lot of potential.

[<a href="/2017/05/16/google-spanner-is-a-database-with-an-api-core/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/16/an-openapi-generator-for-publishing-to-github/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/16/an-openapi-generator-for-publishing-to-github/">An OpenAPI Generator For Publishing To Github</a></h3>
			<p><em>16 May 2017</em></p>
			<p>The folks behind the OpenAPI Spec driven, interactive API documentation ReDoc, have also developed an OpenAPI generator that helps you manage your OpenAPI Spec deployment using Yeoman. If you aren’t familiar with Yeoman, it is modern scaffolding for web apps, which is all about helping you manage the quick and consistent deployment of APIs following existing best practices. ReDoc’s Yeoman generator leverages all the benefits of pushing your OpenAPI Specs and API documentation on Github: Community Engagement - Allows for engagement with API consumers via Github’s native infrastructure, and issue management. Hosting on GitHub Pages - You are offloading the hosting and bandwidth to Github, and their CDN, significantly reducing overhead. Exposure in Github Community - Your API documentation and presence is available on Github, via trending, search, and other discovery mechanisms. Life Cycle Management - Github provides you with version history, branching, and continuous integration options, matching with your existing workflows. Familiar Environment - People increasingly understand Github as an environment where you can engage with code, as well as the community around the tooling and content published in this environment. ReDoc’s Yeoman generator helps to create a GitHub repo with the following features: Possibility to split a big Swagger spec into smaller files and bundle it for deployment Continuous integration/deployment on Travis Code samples as separate files Swagger spec is validated after each commit Swagger spec + ReDoc deployed to Github Pages Live editing in your editor or swagger-editor All the documentation for ReDoc’s Yeoman generator is available on the Github repository. The Yeoman generator demonstrates how OpenAPI Spec operates as more than just a machine readable core to API documentation. It acts as a machine-readable core to all stops along the API life cycle defining how you design, deploy, manage, and document your APIs. ReDoc shows us the role OpenAPI Spec can play in driving your APIs documentation, but then turns things up a notch or two, demonstrating that when you publish...[<a href="/2017/05/16/an-openapi-generator-for-publishing-to-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/15/one-layer-of-an-api-ranking-system-will-need-to-be-domain-scoring/"><img src="https://s3.amazonaws.com/kinlane-productions2/whoapi/Domain-score-API-integration-in-WhoAPI.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/15/one-layer-of-an-api-ranking-system-will-need-to-be-domain-scoring/">One Layer Of An API Ranking System Will Need To Be Domain Scoring</a></h3>
			<p><em>15 May 2017</em></p>
			<p>
I saw that WhoAPI launched a Domain Score API recently, helping put a value on whether or not you can trust a domain. The example they have in their blog post applies a domain score to the email addresses for any developer signing up for the Domain Score API–pretty useful stuff.

I do not know anything about the algorithm behind the domain scoring API, or what data it pulls from, but I think the concept is definitely applicable in today’s online environment. I wrote about Best Buy requiring their developers to register with an email at their business domain, not some general email service, and this seems like another layer of security you could add to this process.

With the current climate online, services like these are going to be increasingly valuable in the day to day operations of businesses, and not just for registering for an API. I’m going to start building a collection of these types of APIs and publish them as an individual stack–not sure what I’ll call it yet, but hopefully, it will be something I can leverage as part of a future API ranking system.

[<a href="/2017/05/15/one-layer-of-an-api-ranking-system-will-need-to-be-domain-scoring/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/15/my-new-cms-for-manging-my-network-of-github-sites/"><img src="https://s3.amazonaws.com/kinlane-productions2/siteleaf/siteleaf-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/15/my-new-cms-for-manging-my-network-of-github-sites/">My New CMS For Manging My Network Of Github Sites</a></h3>
			<p><em>15 May 2017</em></p>
			<p>
All of my websites have run 100% on Github for the last three years. The core of my API industry research is always in JSON or YAML, stored in individual project-based Github repositories. I leverage Jekyll for the page and other content collections (blogs, news, etc.). Since 2011 I’ve used my own homebrew CMS system, making it accommodate the switch to a more static presence on Github.

Over the weekend I ditched my CMS and lit up a new CMS I came across called Siteleaf, which has all the core features I need: Github, Jekyll, Amazon S3, and API. This is how I manage a couple hundred API research sites, and the images, video, and other heavy objects I store using Amazon S3–these services and tools are critical to my business.

I am writing this post in Siteleaf. I do not have it setup for publishing across all of my websites, but so far it has worked for getting up 3 blog posts on API Evangelist, providing me with a successful test run. Siteleaf represents how I think software tools should be built, providing a simple useful application, that leverages Github, common storage like AWS S3, Dropbox, and others, and of course, they should ALL have an API.

[<a href="/2017/05/15/my-new-cms-for-manging-my-network-of-github-sites/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/15/generating-revenue-from-the-remarketing-tags-on-api-evangelist/"><img src="https://s3.amazonaws.com/kinlane-productions2/google/remarketing_products_sm.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/15/generating-revenue-from-the-remarketing-tags-on-api-evangelist/">Generating Revenue From The Remarketing Tags On API Evangelist</a></h3>
			<p><em>15 May 2017</em></p>
			<p>I am going through my entire infrastructure lately, quantifying the products and services that API Evangelist offers, and the partnerships that make everything go round. As I do in my work as the API Evangelist, I’m looking to work through my thoughts here on the blog, and this week I have an interesting topic on the workbench–the API Evangelist remarketing tags. According to Google, remarketing tags are: “To show ads to people who have visited your desktop or mobile website, add the remarketing tag to your website. The tag is a short snippet of code that adds your website visitors to remarketing lists; you can then target these lists with your ads. If your website has a Google Analytics tag, you can use this tag instead and skip adding the AdWords remarketing tag.” Over the last couple of years, I’ve had 3Scale’s remarketing tags on my site. 3Scale has been my number one supporter for the last three years, and API Evangelist wouldn’t even be a thing without them. They have provided me with financial support each month for a variety of reasons. We’ve never fully itemized what is in exchange for this support, primarily it has been to just invest in me being API Evangelist and help 3Scale be successful, but one of the things on the table for the last couple of years has been that I published the 3Scale remarketing tags on my network of sites. So, if you visited API Evangelist in the last three years, it is likely you saw 3Scale ads wherever you went on the web. As I’m evaluating all the products and services I offer, quantify my partnerships, and identify the value that API Evangelist brings to the table, I find myself thinking deeply about this practice. Should I be selling my visitors data like this? If you visit API Evangelist, should I allow my partners to target you with advertising on other sites you visit? As...[<a href="/2017/05/15/generating-revenue-from-the-remarketing-tags-on-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/12/helping-standardize-how-we-communicate-about-the-api-integration-possibilities/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_integrations_icon_page.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/12/helping-standardize-how-we-communicate-about-the-api-integration-possibilities/">Helping Standardize How We Communicate About The API Integration Possibilities</a></h3>
			<p><em>12 May 2017</em></p>
			<p>Showcasing the integrations that are possible with your API via your API developer portal is an increasingly important way to demonstrate the usefulness of your API resources. Companies like Amazon, DataDog,&nbsp;Intercom, and other leading providers showcase other systems their solution is already integrated with. When it comes to API solutions, applications aren't just web and mobile, they are often system to system integrations with many of the SaaS and other software solutions that companies are already using in their operations. As I do with other types of signals coming out of the API space I'd like to see more API providers publish a listing of possible integrations, and I've created a simple API solution for managing an API integration page that can run in any Jekyll environment and possesses an API core. Using my approach to managing simple datasets, and publishing a human and machine interface using Github, I published an API integration page solution. Anyone can fork the project, reskin the look &amp; feel of the website, and update the listing of integrations that are possible using a Google spreadsheet, then publishing the latest data using a Github repo and site. My objective is to help API providers better manage and communicate the integrations that are possible via their API platform. I want them doing this in an organized and standardized way, helping their API consumers understand what is possible. I also want there to be a machine-readable definition of these integrations, so that analysts&nbsp;like me can more easily aggregate and connect the dots when it comes to what is possible with APIs. Right now, I am just tracking the URL of API providers integration pages, but I'd love it if the contents were also machine-readable and available via Github and API for forking and integration into other applications. My API integration page and API template can be forked and run standalone, or you can copy the listing or icon pages, APIs, and YAML...[<a href="/2017/05/12/helping-standardize-how-we-communicate-about-the-api-integration-possibilities/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/12/api-integration-service-providers/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/left_hook_custom_integrations_and_worfklow_intelligence.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/12/api-integration-service-providers/">API Integration Service Providers</a></h3>
			<p><em>12 May 2017</em></p>
			<p>I spend a lot of time talking about API providers, companies who have a public APIs. Occasionally, you will also hear me talk integration platform as a service (iPaaS) providers, companies like Zapier and Datafire&nbsp;who focus on providing a platform that connects you with many different API integration possibilities. These companies are a valuable player in the API ecosystem because they acknowledge that we usually do not just need one API, we will almost always need to integrate with many APIs, and they provide tools for developers, and non-developers to deliver API solutions that can leverage multiple individual&nbsp;APIs in a variety of business workflows. I just got off a call with Sean Matthews of Left Hook Digital, an integration service provider who "efficiently build, maintain, and grow their integration options through a diversified iPaaS presence." This is the other half of the API integration coin I have been looking for, actual people who will help you navigate the crazy world of API integration, as well as the growing number of API integration and aggregation platforms and tooling that have been emerging, and evolving. I've been looking for people to help my readers navigate this world of API integration gray space in between full automation and custom development. I've been looking for people to help small businesses, organizations, institutions, and government agencies understand how they can better leverage API aggregation providers like Cloud Elements, and integration platform as a service provider (iPaaS) like Zapier. Both platforms provide a wealth of services and tooling, but there still needs to be a person who is knowledgeable of these platforms who are&nbsp;willing to talk to a company or organization about which API-driven services they use, and what the possibilities and limitations around integrations are. In addition to talking to your average company about integration, I've also been in need of knowledgeable folks to help API providers better leverage aggregation and integration platforms in their own operations. API providers are...[<a href="/2017/05/12/api-integration-service-providers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/12/a-new-look-for-api-evangelist/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_evangelist_new_look.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/12/a-new-look-for-api-evangelist/">A New Look For API Evangelist</a></h3>
			<p><em>12 May 2017</em></p>
			<p>I was trying to fit some new content into my website, and I couldn't make it fit within the layout. Then I remembered I had also taken off a section of my work because it looked like crap on my iPad a couple weeks back. It is a sign I've outgrown the current layout of my website when I can't publish my new work, as well as be an adequate archive for my historical research. Thankfully, my website is a pretty modular Jekyll implementation, so once I found the right look, it was only a couple hours worth of work to give it a full overhaul. The new look and feel for API Evangelist remind me of the original look for the site back in the day, but with a more modern touch. It's responsive and has&nbsp;one of those little icon menus that follow you as your scroll. Something that bothered me at first, but has grown on me a little bit. While my old site was responsive, over the last couple years I managed to screw a few things up as I worked to make it fit all my crazy ideas. This new layout feels bigger, with much more sensible information architecture--something I'm hoping will accommodate&nbsp;what I'm looking to do with API Evangelist over the next couple of months. The overhaul of the UI for API Evangelist also gave me an opportunity to shift how I manage the YAML data across all my research, moving from a MySQL database to Google Sheets. I'm using my evolved approach to using Github, Jekyll, and Google Sheets to deploy new data-driven API research, complete with HTML, YAML, Atom, and JSON output, depending on who I am looking to serve. All my short form content (blog posts) is managed using Jekyll within each repository, and my long form content (guides) are available via PDF--it's a nice marriage between content and data, that jives with my workflow. This new...[<a href="/2017/05/12/a-new-look-for-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/11/using-public-lands-as-an-analogy-when-talking-about-public-data-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/anza_borrego_desert_state_park.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/11/using-public-lands-as-an-analogy-when-talking-about-public-data-apis/">Using Public Lands As An Analogy When Talking About Public Data APIs</a></h3>
			<p><em>11 May 2017</em></p>
			<p>I have used the analogy of public lands when talking about access to, and monetization around public data resources, for a number of years. While not a perfect analogy, it provides me with a very tangible, and relatable way to help people understand access to, and the value of public data resources that can often be very abstract and difficult to see. Conveniently, some of the stories about public&nbsp;data, and policy I've worked on in the Federal Government involved public data that was actually about public lands, and more specifically national parks, and other resources in the Recreational Information Database (RIDB). While discussing this work on a conference call the other day, someone thought that using the analogy of public lands when talking about public data didn't always work because public lands were a limited resource--a national park is only so big. It was not endless like digital data and content can be. I urged them to think beyond just the land, but the elements of the land, and a variety of resources made available via its existence. Things like water, minerals, fish, timber, and other resources that are often very valuable. While not all these resources are endless, some are, and others&nbsp;are renewable if managed properly, sharing many useful considerations for applying to how we manage access to our digital public resources. I feel like there are many lessons present in applying the public lands analogy&nbsp;to public data. Depending on who you talk to, you will receive different&nbsp;views over ownership and access to public lands in the United States. Similarly, depending on who you talk to, you will receive different views over ownership and access to public data. It is easy to think of digital public assets as infinite, but similar to public lands, this will vary from resource to resource, and in some cases public data could be very limited, involving things like quarterly budgets, executive summaries, and other aggregate views of what...[<a href="/2017/05/11/using-public-lands-as-an-analogy-when-talking-about-public-data-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/11/my-challenges-when-taking-money-from-startups-as-the-api-evangelist/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/desert_dragon_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/11/my-challenges-when-taking-money-from-startups-as-the-api-evangelist/">My Challenges When Taking Money From Startups As The API Evangelist</a></h3>
			<p><em>11 May 2017</em></p>
			<p>It is a hustle to do API Evangelist. I've been lucky to have the support of 3Scale since 2013, without them API Evangelist would not have survived. I'm also thankful for the community stepping up last year to keep the site up and running, keeping it community focused thing, and not just yet another vendor mouthpiece. I make my money providing four ad slots on the site, by selling guides and white papers, and by consulting and creating content for others. It is a hustle that I enjoy much more than having a regular job, even though it is often more precarious, and unpredictable regarding what the future might hold. Taking money from companies always creates a paradox for me. People read my stories because they tend to be vendor neutral and focus on ideas, and usable API-centric concepts. While I do write about specific companies, products, services, and tooling, I primarily try to talk about the solutions they provide, the ideas and stories behind them, steering clear of just being a cheerleader for specific vendor solutions. It's hard, and something I'm not always successful at, but I have primarily defined my brand by sticking to this theory. This approach is at odds with what most people want to give me money for. 3Scale has long supported me and invested in me being me, doing what I do--which is rare. Most companies just want me to write about them, even if they understand the API Evangelist brand. They are giving me money, and in exchange, I should write about them, and their products and services. They often have no regard to the fact that this will hurt my brand, and run my readers off, falling short of actually achieving what they are wanting to achieve. I get the desire to advertise and market your warez, but the ability for companies to be their own worst enemy in this process is always fascinating to me. I...[<a href="/2017/05/11/my-challenges-when-taking-money-from-startups-as-the-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/11/a-handful-of-microsoft-flow-openapi-extensions/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/ms_flow_icon.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/11/a-handful-of-microsoft-flow-openapi-extensions/">A HandFul Of Microsoft Flow OpenAPI Extensions</a></h3>
			<p><em>11 May 2017</em></p>
			<p>
I used to keep track of Swagger vendor extensions are part of my previous research around what was formerly known as Swagger. It is something I'm reviving as part of my OpenAPI Toolbox work, profiling the OpenAPI extensions I come across in the course of my work.
While profiling the Azure as part of my API Stack research I came across Microsoft Flow, and noticed that they use OpenAPI as part of the configuration of the integration platform as a service (iPaaS) solution, and have four specific extensions defined:

x-ms-summary - Title of the entity.
x-ms-visibility - Determines the user facing visibility of the entity.
x-ms-dynamicvalues - Enables populating a dropdown for collecting input parameters to an operation.
x-ms-dynamic-schema - This is a hint to the flow designer that the schema for this parameter or response is dynamic in nature.

According to the Microsoft Flow website: "To use custom APIs in Microsoft Flow, you must provide a Swagger definition, which is a language-agnostic machine-readable document describing the API's operations and parameters. In addition to the out-of-the-box Swagger specification, there are some extensions available when creating a custom API for Microsoft Flow."
I like that they use OpenAPI for this layer, and think that their extension of the OpenAPI specification is worth noting. My goal is to aggregate all the extensions I can as part of the OpenAPI Toolbox, encouraging reuse of existing patterns that have not made it into the specification. I also want providers to understand that they can extend the specification from outside the working group, delivering interesting features in the services and tooling you develop. I don't think that folks always understand the role they can play in helping define the OpenAPI specification just by developing interesting&nbsp;implementations that use OpenAPI as the common way to define and model your APIs.
[<a href="/2017/05/11/a-handful-of-microsoft-flow-openapi-extensions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/10/simple-apis-with-jekyll-and-github-with-data-managed-via-google-spreadsheets/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/openapi_toolbox_home_page.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/10/simple-apis-with-jekyll-and-github-with-data-managed-via-google-spreadsheets/">Simple APIs With Jekyll And Github With Data Managed Via Google Spreadsheets</a></h3>
			<p><em>10 May 2017</em></p>
			<p>I'm always looking for simpler, and cheaper ways of doing APIs that can help anyone easily manage data while making it available in both a human and machine readable way--preferably&nbsp;something developers and non-developers both will find useful. I've pushed forward my use of Github when it comes to managing simple datasets, and have a new approach I want to share, and potentially use across other projects. You can find a working example of this in action with my OpenAPI Toolbox, where I'm looking to manage and share a listing of tooling that is built on top of the OpenAPI specification. Like the rest of my API research, I am looking manage the data in a simple and cheap way that I can offload&nbsp;the storage, compute, and bandwidth to other providers, preferably&nbsp;ones that don't cost me a dime. While not a solution that would work in every API scenario, I am pretty happy with the formula I've come up with for my OpenAPI Toolbox. Data Storage and Management In Google SheetsThe data used in the OpenAPI Toolbox comes from a public Google Sheet. I manage all the tools in the toolbox via this spreadsheet, tracking title, description, URL, image, organization, types, tags, and license using the spreadsheet. I have two separate worksheets, one of which tracks details on the organizations, and the other keeping track of each individual tool in the toolbox. This allows for the data to be managed by anyone I give access to the sheet using Google Docs, offloading storage and data management to Google. Sure, it has its limitations, but for simple datasets, it works better than a database in my opinion. Website and Basic API Hosting Using GithubFirst, and foremost the data in the OpenAPI Toolbox is meant to be accessible by any human on the web. Github, using their Github Pages solution, combined with the static website tool Jekyll, provides a rich environment for managing this data-driven toolbox. Jekyll provides...[<a href="/2017/05/10/simple-apis-with-jekyll-and-github-with-data-managed-via-google-spreadsheets/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/10/in-search-of-some-funding-for-my-machine-learning-api-research/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/machine_learning_gun_patent.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/10/in-search-of-some-funding-for-my-machine-learning-api-research/">In Search Of Some Funding For My Machine Learning API Research</a></h3>
			<p><em>10 May 2017</em></p>
			<p>
I am wanting to profile the world of machine learning APIs, similar to what I've done with Amazon, Google, Microsoft, Facebook, and the rest of my API Stack research, but I'm in need of some investment to help make sure I can properly carve out the time to conduct the research, and publish a resulting guide that provides an overview of the space, when done. After profiling the tech giants, I'm seeing some interesting shifts in the landscape when it comes to machine learning and would like to spend time profiling the rest of the landscape beyond just the bigcos.
While I am interested in mapping out the landscape of the machine learning API space, I don't have the time to make every project happen, and also pay the bills. To help make sure this project happens, I am looking for someone to step up and sponsor the research. In exchange for your brand and link on the resulting guide and any blog posts along the way, I'm looking for some cash investment to help me make ends meet, while I am doing the work.
When I profile a segment of the API space I focus on documenting the company behind, including all their relevant signals like Twitter and Github, but I also document the available API using OpenAPI, and the pricing, and other relevant building blocks. The resulting short form (blog posts), and long form (guide) provides an overview of the landscape when it comes to how APIs are being used as part of the latest machine learning wave. If your company is looking to better understand machine learning, and how APIs are playing a role, or maybe just looking to get some exposure with my audience who would read the blog posts and the resulting guide, let me know--I could use your help.
[<a href="/2017/05/10/in-search-of-some-funding-for-my-machine-learning-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/10/adding-an-extensions-category-to-the-openapi-toolbox/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/openapi_extensions_in_the_openapi_toolbox.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/10/adding-an-extensions-category-to-the-openapi-toolbox/">Adding An Extensions Category To The OpenAPI Toolbox</a></h3>
			<p><em>10 May 2017</em></p>
			<p>I added another type of tool to my OpenAPI Toolbox, this time it is extensions. They used to be called Swagger vendor extensions, and now they are simply called OpenAPI extensions, which allow any implementor to extend the schema outside the current version of the API specification. All you do to add an OpenAPI extension is prepend x- to any value&nbsp;that you wish to include in your OpenAPI, and the validator will overlook it as part of the specification. I have a whole list of vendor extensions I'd like to add, but I've started with a handful from Microsoft Flow, and my friends over at APIMATIC. Two great examples of how OpenAPI extensions can be used in the API lifecycle. In this case, one is for integration platform as a service (iPaaS), and the other is SDK generation and continuous integration. Both vendors needed to extend the specification to meet a specific need, so they just extended it as required--you can find the extensions in the new section of the toolbox. My goal in adding the section to the OpenAPI toolbox is to highlight how people are evolving the specification outside the core working group. While some of the extensions are pretty unique, some of them have a potential common purpose. I will be adding some discovery focused extensions next week from the OpenAPI directory APIs.guru, which I will be adopting and using in my own definitions to help me articulate the provenance of any OpenAPI definition in my catalog(s). Plus, I find it to be a learning experience to see how different vendors are putting them to work.&nbsp; If you know of any OpenAPI extensions that are not in the toolbox currently feel free to submit an issue on the Github repository for the project. I'd like to evolve the collection to be a comprehensive look at how OpenAPI extensions are being used across the sector, from a diverse number of providers. I'm going...[<a href="/2017/05/10/adding-an-extensions-category-to-the-openapi-toolbox/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/09/the-list-of-api-signals-i-track-on-in-my-api-stack-research/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-signals.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/09/the-list-of-api-signals-i-track-on-in-my-api-stack-research/">The List Of API Signals I Track On In My API Stack Research</a></h3>
			<p><em>09 May 2017</em></p>
			<p>I keep an eye on several thousand companies as part of my research into the API space&nbsp;and publish over a thousand of these profiles in my API Stack project. Across the over 1,100 companies, organizations, institutions, and government agencies I'm regularly running&nbsp;into a growing number of signals that tune me into what is going on with each API provider, or service provider.&nbsp; Here are the almost 100 types of signals I am tuning into as I keep an eye on the world of APIs, each contributing to my unique awareness of what is going on with everything API. Account Settings&nbsp;(x-account-settings) - Does an API provider allow me to manage the settings for my account? Android SDK&nbsp;(x-android-sdk) - Is there an Android SDK present? Angular&nbsp;(x-angularjs)&nbsp;- Is there an Angular SDK present? API Explorer&nbsp;(x-api-explorer) - Does a provider have an interactive API explorer? Application Gallery&nbsp;(x-application-gallery) - Is there a gallery of applications build on an API available? Application Manager&nbsp;(x-application-manager) - Does the platform allow me to management my APIs? Authentication Overview&nbsp;(x-authentication-overview) - Is there a page dedicated to educating users about authentication? Base URL for API&nbsp;(x-base-url-for-api) - What is the base URL(s) for the API? Base URL for Portal&nbsp;(x-base-url-for-portal) - What is the base URL for the developer portal? Best Practices&nbsp;(x-best-practices) - Is there a page outlining best practices for integrating with an API? Billing history&nbsp;(x-billing-history) - As a developer, can I get at the billing history for my API consumption? Blog&nbsp;(x-blog) - Does the API have a blog, either at the company level, but preferably at the API and developer level as well? Blog RSS Feed&nbsp;(x-blog-rss-feed) - Is there an RSS feed for the blog? Branding page&nbsp;(x-branding-page) - Is there a dedicated branding page as part of API operations? Buttons&nbsp;(x-buttons) - Are there any embeddable buttons available as part of API operations. C# SDK&nbsp;(x-c-sharp) -&nbsp;Is there a C# SDK present? Case Studies&nbsp;(x-case-studies) - Are there case studies available, showcasing implementations on top of an API? Change...[<a href="/2017/05/09/the-list-of-api-signals-i-track-on-in-my-api-stack-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/09/public-and-private-sector-hybrid-data-marketplaces/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/copenhagens_innovative_big_data_marketplace.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/09/public-and-private-sector-hybrid-data-marketplaces/">Public And Private Sector Hybrid Data Marketplaces</a></h3>
			<p><em>09 May 2017</em></p>
			<p>I have seen a number of incarnations when it comes to making public data available on the Internet, from startup implementations like earlier InfoChimps, U.S. Federal Government efforts like Dataa.gov, and Socrata. Recently, Andrew Nicklin (@technickle), the Director of Data Practices at the Center for Government Excellence at Johns Hopkins University pointed out&nbsp;a version I haven't come across yet, the public / private sector hybrid.&nbsp; Publicly-operated data markets. This is an extremely interesting approach, because it provides a few other benefits beyond making government data accessible. With this approach, a government offers a public data market as a platform on which it and third-parties make a variety of data available, some for free and some at a premium. Because they are operating it, the government gains the ability to apply taxes or fees to data-access transactions (and this could be through any or all of the models suggested above), but it also gets an opportunity to regulate the market itself by establishing ground rules to protect privacy, public interest, and so on. Smart Copenhagen appears to be moving in this direction, and Smart Dubai may evolve towards this as well. (These platforms also present the opportunity for revenue generation through advertising, even if it&rsquo;s just advertising other datasets to their repeat customers.) You can see this in action at the City Data Exchange in Copenhagen, Denmark, where the city has partnered with the Hitachi Insight Group to provide a data marketplace where public and private sector data can coexist, and as Andrew suggests, even allow for fee-based access&nbsp;(key point). Providing us with an evolving definition of what data marketplaces can be. I've seen a lot of data marketplace come and go, and this model, with a realistic business model for both public and private sector partners, provides me with a little bit of hope for providing access to public data in a sustainable way. I will be keeping an eye on the City Data Exchange,...[<a href="/2017/05/09/public-and-private-sector-hybrid-data-marketplaces/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/09/pricing-tiers-works-for-saas-but-not-really-for-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_plans_pricing_tiers.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/09/pricing-tiers-works-for-saas-but-not-really-for-apis/">Pricing Tiers Works For SaaS But Not Really For APIs</a></h3>
			<p><em>09 May 2017</em></p>
			<p>I get why SaaS, and API providers offer a handful of pricing plans and tiers for their platforms, but it isn't something I personally care for as an API consumer. I've studied thousands of plans and pricing for API providers, and have to regularly navigate 50+ plans for my own API operations, and I just prefer having access to a wide range of API resources, across many different companies, with a variety of usage limitations and pricing based upon each individual resources. I really am getting tired of having to choose between bronze, gold, or platinum, and often getting priced out completely because I can scale to the next tier as a user. I understand that companies like putting users into buckets, something that makes revenue predictable from month to month, or year to year, but as we consumer more APIs from many different providers, it would help reduce the complexity for us API consumers if you flattened the landscape. I really don't want to have to learn the difference between each of my provider's tiers. I just want access to THAT resource via an API, at a fair price--something that scales infinitely if at all possible (I want it all). Ultimately, I do not feel like API plans and tiers will scale to API economy levels. I think as API providers, we are still being pretty self-centered, and thinking about pricing as we see it, and we need to open up and think about how our API consumers will view us in a growing landscape of service providers--otherwise, someone else will. As I pick up my earlier API pricing work, which has two distinct components: 1) all API resources and pricing available for a platform 2) the details of plans and tiers which a complex list of resources, features, and pricing fit into. It would be much easier to just track resources, the features they have, and the unit price available for each API....[<a href="/2017/05/09/pricing-tiers-works-for-saas-but-not-really-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/08/regional-availability-when-it-comes-to-api-access/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/azure_regions.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/08/regional-availability-when-it-comes-to-api-access/">Regional Availability When It Comes To API Access</a></h3>
			<p><em>08 May 2017</em></p>
			<p>I have been profiling the Microsoft Azure platform over the last couple of weeks, and I found their approach to talking about the regions that were available was worth taking note of. I haven't actually assessed who has more regions, but Azure's approach seems to be pretty advanced, even if AWS might possess more regions (gut feeling). By profiling these cloud services&nbsp;and their available APIs using OpenAPI I am hoping to eventually develop a&nbsp;machine-readable approach to comparing which providers are available within which regions. Google has a regions page, but it doesn't feel as forward leaning as AWS and Azures. It is interesting to watch how each of these providers is handling the availability of API services in a variety of regions across North and South America, Europe, Asia, Africa, and the Middle East. I've been watching how providers are thinking about the availability of API resources in different geographic regions for a while, but after seeing Azure evolve in this area, it is something I'll keep a closer eye on it moving forward. Increasing the number of available regions is definitely the biggest concern for providers, something that small providers will be able to piggyback on and expand using as the top cloud providers grow and expand their regions. API providers and API service providers should be expanding the number of regions available, but everyone involved needs to also get more organized about how they communicate with customers about which regions available--region availability should be communicated at the highest level, like we see with the AWS, Google, and Azure deployment pages, but should also work to articulate which regions are available at the individual API level. As data and algorithmic nationalism continue to grow, we are going to see more focus from providers when it comes to enabling their customer's deployment and operation of APIs into exactly the region they need. I"m guessing with the evolution of software-defined networking (SDN), we are going to...[<a href="/2017/05/08/regional-availability-when-it-comes-to-api-access/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/08/participating-in-the-openapi-feedback-loop/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/oai_silverbadge_text.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/08/participating-in-the-openapi-feedback-loop/">Participating In The OpenAPI Feedback Loop</a></h3>
			<p><em>08 May 2017</em></p>
			<p>When you are an individual in a sea of tech giants, and startups who are moving technical conversations forward, it can be easy to just sit back, stay quiet, and go with the flow. As a single person, it feels like our voice will not be heard, or even listened to when it comes to moving forward standards and specifications like the OpenAPI, but in reality, every single voice that speaks up is important, and has the potential to bring a new perspective regarding what the future should hold when it comes to the roadmap. If you are building any services or tooling that supports version 2.0 of the OpenAPI specification and will be looking to evolve your services or tooling to support version 3.0, you need to make sure and share your views. No matter where you are in the development of your tooling, planning or even deployment, you should make sure you gather and share your thoughts with the OpenAPI Initiative (OAI)--they have a form for tooling developers to submit their feedback and details about what you are up to. Whether or not you submit your&nbsp;OAI tooling and service plans via the form they provide, you should be also telling your story on your blog. You don't have to have a big audience for your blog, you just need to make sure and publicly share the details of your tools and services, and your perspective of both the OpenAPI 2.0 and 3.0 versions. If you tell your story on your&nbsp;blog, and Tweet or email a link to me, I may even craft my own story based on your perspective, and publish to API Evangelist, and put in my&nbsp;OpenAPI Toolbox. Storytelling around the specification plays an important role in helping evolve the specification, as well as help onboard other folks to the API specification format. As the only individual in the OAI, I can testify that I often feel like my voice is too...[<a href="/2017/05/08/participating-in-the-openapi-feedback-loop/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/08/openapidriven-documentation-for-your-api-with-redoc/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/redoc/redoc-demo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/08/openapidriven-documentation-for-your-api-with-redoc/">OpenAPI-Driven Documentation For Your API With ReDoc</a></h3>
			<p><em>08 May 2017</em></p>
			<p>ReDoc is the responsive, three-panel, OpenAPI specification driven documentation for your API that you were looking for. Swagger UI is still reigning king when it comes to API documentation generated using the OpenAPI Spec, but ReDoc provides a simple, attractive, and clean alternative to documentation. ReDoc is deployable to any web page with just two tags--with the resulting documentation looking attractive on both web and mobile devices. Now you can have it all, your API documentation looking good, interactive, and driven by a machine-readable definition that will help you keep everything up to date. All you need to fire up ReDoc is two lines of HTML on your web page: The quickest way to deploy ReDoc is using the CDN step shown above, but they also provide bower or npm solutions, if that is your desire. There is also a Yeoman generator to help you share your OpenAPIs that are central of your web application operation, something we will write about in future posts here on the blog. ReDoc leverages a custom HTML tag, and provides you with a handful of attributes for defining, and customizing their documentation, including specurl, scroll-y-offset, suppress-warnings, lazy-rendering, hid-hostname, and expand-responses--providing some quick ways to get exactly what you need, on any web page. There is a handful of APIs who have put ReDocs to use as API documentation for their platform: Rebilly Docker Engine Zuora Shopify Draft Orders Discourse APIs.guru There also provide a live demo of ReDoc, allowing you to kick the tires some more before you deploy, and make sure it does what you will need it to before you fork. ReDoc provides a simple, OpenAPI spec compliant way of delivering attractive, interactive, responsive and up to date documentation that can be deployed anywhere, including integration into your existing continuous integration, and API lifecycle. ReDoc reflects a new generation of very modular, plug and play API tooling that can be put to use immediately as part of...[<a href="/2017/05/08/openapidriven-documentation-for-your-api-with-redoc/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/05/the-value-of-operational-level-api-exhaust-systems/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_data_exhaust_system.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/05/the-value-of-operational-level-api-exhaust-systems/">The Value Of Operational Level API Exhaust Systems</a></h3>
			<p><em>05 May 2017</em></p>
			<p>When thinking about generating revenue generated from APIs it is easy to focus on directly charging for any digital resource being made available via the API. If it's an image, we charge per API call, and maybe the amount of MB transferred. If it's messaging, we charge per message. There are plenty of existing examples out there regarding how you directly charge for data, content, or algorithms using APIs, and an API way of doing business--look to Amazon, Twilio, and other pioneers. Where there are fewer examples and less open discussions, is around the value of the operation level of APIs,Â and making these data available via APIs--yes APIs for APIs. Modern approaches to doing APIs are all about requiring each application to use an API key with each call they make, the logging of each request and response, possessing the identifying key for each application. This is how API providers are developing an awareness of who is accessing resources, how they are being put them to use, and specific details about each application, and maybe even the users involved. Sometimes the value generated at this layer doesn't exist. Due to restrictive access models, and direct revenue models, there isn't much going on operationally, so there isn't much value generated. However, when there is heavy usage around APIs, the exhaust of the API management layer can become increasingly valuable. What are people searching for? What are applications most popular? Which geographic regions are the most active? There is a pretty lengthy laundry list of valuable data points being applied across modern API operations, that are helping API providers better understand what is going on, that aren't often being included as part of the API road map, and future revenue models. Ok, let me pause here for a moment. I identify the value being generated at this layer because I see existing providers reaching this realization in their operations, as well as wanting to help other providers...[<a href="/2017/05/05/the-value-of-operational-level-api-exhaust-systems/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/05/taxation-on-public-data-via-the-api-management-layer/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_percentage_api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/05/taxation-on-public-data-via-the-api-management-layer/">Taxation On Public Data Via The API Management Layer</a></h3>
			<p><em>05 May 2017</em></p>
			<p>I'm involved in some very interesting conversations with public data folks who are trying to push forward the conversation around sensible revenue generation by cities, counties, state, and the federal government using public data. I'm learning a lot from these conversations, resulting in the expansion and evolution my perceptions of how the API layer can help the government develop new revenue streams through making public data more accessible.Â  I have long been a proponent of using modern API management infrastructure to help government agencies generate revenue using public data. I would also add that I'm supportive of the crafting of sensible approaches to developing applications on top of public data and API in ways that generate a fair profit for private sector actors. I am also in favor of free and unfettered access to data, and observability into the platform operations, as well as ALL commercial interests developing applications on top of public data and APIs. I'm only in favor of this, when the right amount of observability is present--otherwise digital good olÂ boy networks form, and the public will lose. API management is the oldest area of my API research, expanding into my other work to eventually defineÂ documentation, SDKs, communication, support, monetization, and API plans. This is where you define the business of API operations, organizing APIs into coherent catalogs, where you can then work to begin establishing a wider monetization strategy, as well as tiers and plans that govern access to data, content, and algorithms being made available via APIs. This is the layer of API operations I'm focusing on when helping government agencies better understand how they can get more in tune with their data resources, and identify potential partnerships and other applications that might establish new revenue streams. A portion of thisÂ conversation that I am having was involved in the story from Anthony Williams about maybe government data shouldn't always be free, where the topic of taxation came up. One possible analogy...[<a href="/2017/05/05/taxation-on-public-data-via-the-api-management-layer/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/05/quantifying-the-data-a-company-possesses-using-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/facebook_schema_feed_files.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/05/quantifying-the-data-a-company-possesses-using-apis/">Quantifying The Data A Company Possesses Using APIs</a></h3>
			<p><em>05 May 2017</em></p>
			<p>Profiling APIs always provides me with a nice bulleted list of what a company does or doesn't do. In my&nbsp;work as the API Evangelist, I can read marketing and communications to find out what a company does, but I find that profiling their APIs provides a more honest view of what is going on. The lack of a public API always sets the tone for how I view what a company is up to, but when there is a public API, profiling it always provides a nice distillation of what a company does, in a nice bulleted list I can share with my readers. When I profile the APIs of companies like Amazon, Google, and Microsoft, I come out of it with a nice bulleted list of what is possible, but when I go even further, making sure each API profile has accompanying&nbsp;schema definitions, a nice list of what data company&nbsp;begins to emerge. When I profile an API using OpenAPI I always start by profiling the request layer of an API, the paths, parameters, and other elements. Next, I get to work describing the schema definitions of data used in these requests, as well as the structure of the responses--providing me with a nice bulleted list of the data that a company has.&nbsp; You can see this in action with my Facebook API profiling work. There is a bulleted list of what is possible (API definition), as well as what data is sent, received, and stored (API schema). This work provides me with a&nbsp;nice look at the data Facebook gathers and stores about everyone. It is FAR from a complete picture of the data Facebook gathers, but it does provide us with a snapshot to consider, as well as a model we can ask Facebook to share more schema about the data points that they track. API and data specification formats like JSON Schema, and OpenAPI provides us with a toolbox to help us quantify...[<a href="/2017/05/05/quantifying-the-data-a-company-possesses-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/04/i-pushed-1173-api-definitions-to-the-api-stack/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_stack_screen.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/04/i-pushed-1173-api-definitions-to-the-api-stack/">I Pushed 1173 API Definitions To The API Stack</a></h3>
			<p><em>04 May 2017</em></p>
			<p>It has been over a year since I've pushed any API definitions to my API Stack research, but I finally was able to prioritize time this week to make sure it was updated with the latest profiles I have in my API monitoring system. I pushed 1,173 companies who are doing interesting things with APIs. Not all of them have a traditional API program, but most of them do. It isn't all of the API related companies in my tracking system, but it's definitely the core group of what I'm watching. Each API is profiled with an APIs.json file, providing an index of the name, description, tags, and other metadata, but also provides the URLs for documentation, Github, Twitter, and other key aspects of API operations. When there is an OpenAPI present, I publish a copy of it, with a URL for the resource, within each index. I have a bunch more OpenAPI definitions to publish, but they will take some considerable cleaning up before they are ready for prime time--look for regular updates now that I have the bulk of it updated. My API Stack runs 100% on Github, like the rest of my research. All the data behind is available in APIs.jsonÂ and OpenAPI, both as JSON and YAML--take your pick. I prefer YAML being the core of everything and stored in the _data folder, but I tend to also put up JSON and HTML facades on this data when it makes sense. The homepage for the project is just an HTML listing done in Liquid, driven from the YAML in the _data folder, making it easy to play with different views of the APIs indexed as part of the collection. At over 1000 APIs the repository is getting pretty unwieldy to commit each time, and i've begun the process of breaking up most of the larger APIs into their own repositories for individual management, projects like AWS, Google, Microsoft, and Facebook. I will...[<a href="/2017/05/04/i-pushed-1173-api-definitions-to-the-api-stack/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/02/open-discussions-about-funding-api-startups/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-business-api-response.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/02/open-discussions-about-funding-api-startups/">Open Discussions About Funding API Startups</a></h3>
			<p><em>02 May 2017</em></p>
			<p>It made me happy to read the Rise of Non &ldquo;VC compatible&rdquo; SaaS Companies, and see that there are more sensible discussions going on around how to develop SaaS business, something that I hope spreads into the specifically API as a product startups and API service providers. I know that many of my readers think I'm anti-VC--I am not. Or may I'm anti-startup--I am not. I'm anti-VC and anti-startup ideology becoming the dominant religion, pushing out a lot of really good people and ideas who can't play that game. When it comes to Silicon Valley, if you push back, you get excluded from the club, and there are waves of people who step up to tell you "not all startups are bad" or "not all VCs are bad"--I wish I could help you understand how this response makes you look. Of course, they aren't all bad, but there are bad ones, and there is a lot of rhetoric that this is the ONLY way you can build technology when it isn't. There are plenty of ways you can develop technology, and build a business without the VC approach, or the cult of the startup.&nbsp; There are more instructions you should follow in the rise of the non-VC compatible SaaS companies story, but the author outlines four types of SaaS companies, which I think applies nicely to APIi companies, as many of them will be SaaS &nbsp;providers: Funded SaaS: companies which finance their business with VCs a.k.a equity against money. From early stage startups with no revenue to companies going public with hundreds of millions of dollars of ARR, the range is extremely wide. Bootstrapped &ldquo;scaling&rdquo; SaaS companies:&nbsp;SaaS companies which manage to pass the $10M ARR threshold without VC money. Ex: Mailchimp or Atlassian (which raise VC money but at a very late stage) have reached the hundreds of millions of dollars of ARR without VC money. These &ldquo;unicorns among unicorns&rdquo; are very rare. Bootstrapped SaaS...[<a href="/2017/05/02/open-discussions-about-funding-api-startups/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/02/my-new-api-vendor-evaluation-checklist/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_evaluation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/02/my-new-api-vendor-evaluation-checklist/">My New API Vendor Evaluation Checklist</a></h3>
			<p><em>02 May 2017</em></p>
			<p>I am helping a customer think through their decision-making process around the adoption of a new API service, and while I am doing this I am spending the time to think through my own API adoption process. I like having checklists to consider when making new purchasing and integration decision. Sometimes I have an immediate need which is driven by emotion, and it can help to step back and think through a more rational checklist I established for myself on a previous date. When I am approaching a new API that I think might move beyond just playing around, and actually have a viable business use case in my operations, I think through the following areas: Define Value - What value is being created by an API I'm looking to use. Define Needs - What needs do I have which using an API will solve. Define Options - What other solutions are there beyond this API. Think About Today - Is this an immediate need I have with days or weeks. Think About Tomorrow - Is this a long term need that will go on for years. Vet Company &amp; People - More detail about the company, people, and investors. Define Partners - What does my the partnership landscape look like for the API. What Things Cost - What are things going to cost be for using an API. What You Can Afford - Can I really afford using this service, or not use. Develop Continuity Plan - What is the plan for ensuring stable operations using API. Develop Exit Plan - How will I severe a relationship and replace or deprecate need. Sometimes I do not have everything in order before I pull the trigger. I may not completely though through what I will do when an API goes away, but I like at least having some notes written down about my frame of mind when I was integrating with any API. Having the...[<a href="/2017/05/02/my-new-api-vendor-evaluation-checklist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/02/defining-the-surface-area-of-the-facebook-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/the_facebook_api_index_screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/02/defining-the-surface-area-of-the-facebook-api/">Defining The Surface Area Of The Facebook API</a></h3>
			<p><em>02 May 2017</em></p>
			<p>I learn a lot by studying APIs. One of the ways I get to know what an API does is by creating an OpenAPI for the API, which helps define all of the paths available for an API--helping me understand what is possible. After defining the API requests that are possible, ensuring there are simple descriptions for each path, I also like to make sure all the data that is being passed back and forth via an API is also defined in the OpenAPI--giving me a good snapshot of what data is stored behind an API. In my current Facebook API definition, there is a total of 271 paths, with 90 objects defined using 654 data points. The machine-readable OpenAPI definition tells me what data is stored and transmitted, and what actions I can take involving these items. The Facebook API definition isn't complete, but it does provide me with a nice view of the Facebook Graph API landscape, and since the&nbsp;project is hosted on Github anyone can help me continue adding to and expanding on the definition, as well as the documentation and other tools I'm building on top of the definition. There are three parts to my research: 1) to define Facebooks API operation from a technical and business perspective 2) understand which of my bits Facebook tracks about me and my business(es), and how I can take more control over these bits with the Facebook API, and 3) understand how Facebook can be orchestrated and manipulated as part of a social engineering toolbox. Facebook isn't on my list of top places I hang out online, but I grasp its magnitude, and importance to my business and my career, and our democracy--so I work to understand WTF is going on with it. I currently post stories to Facebook from my content management system. When I blog something, if I want it on Facebook, I can schedule it, and my system posts the story...[<a href="/2017/05/02/defining-the-surface-area-of-the-facebook-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/01/quantifying-the-api-landscape-across-amazon-google-and-microsoft/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/microsoft_graph_api_surface.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/01/quantifying-the-api-landscape-across-amazon-google-and-microsoft/">Quantifying The API Landscape Across Amazon, Google, and Microsoft</a></h3>
			<p><em>01 May 2017</em></p>
			<p>I work to develop OpenAPI definitions for 3rd party APIs because it helps me understand what is being offered by a company. Even when I'm able to autogenerate an OpenAPI for an API, or come across an existing one, I still spend time going through the finer details of what an API does, or doesn't do. I find the process to be one of the best ways to learn about an API, stopping short of actually integrating with it. Over the last couple of months, I've aggregated, generated, and crafted OpenAPI and APIs.json definitions for the top three cloud API providers out there. I wanted to be able to easily see the surface area for as many of the APIs as I could find for these three companies: Amazon - 2222 paths (or methods) across&nbsp;65 of the Amazon Web Services - you can find the APIs.json, and OpenAPI behind in the Github repository. Google - 2089 paths across&nbsp;75 of the Google services I'm profiling&nbsp;- you can find the APIs.json, and OpenAPI behind in the Github repository. Microsoft - 2109 paths across&nbsp;41 of the Microsoft and Azure services.&nbsp;- you can find the APIs.json, and OpenAPI behind in the Github repository. I learned a lot about all three providers in the process. I filled my notebook with stories about their approaches. I also ended up with three separate Github repositories with APIs.json&nbsp;indexed OpenAPI definitions for as many of their APIs as I could process. They are far from complete, but I feel like they paint a pretty interesting picture of the API landscape across these three tech giants. So far there are 6,420 paths across&nbsp;181 individual services. I'm still working on the summary and tags for each path, which are the two most important elements for me. I think the list of 6,420 actions you can take via an API across three of the biggest cloud providers gives us a lot of insight into what these companies...[<a href="/2017/05/01/quantifying-the-api-landscape-across-amazon-google-and-microsoft/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/01/expressing-what-an-api-does-as-well-as-what-is-possible-using-openapi/"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/tool/openapi-spec.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/01/expressing-what-an-api-does-as-well-as-what-is-possible-using-openapi/">Expressing What An API Does As Well As What Is Possible Using OpenAPI</a></h3>
			<p><em>01 May 2017</em></p>
			<p>I am working to update my OpenAPI definitions for AWS, Google, and Microsoft using some other OpenAPIs I've discovered on Github. When a new OpenAPI has entirely new paths available, I just insert them, but when it has an existing path I have to think more critically about what is next. Sometimes I dismiss the metadata about the API path as incomplete or lower quality than the one I have already. Other times the content is actually more superior than mine, and I incorporate it into my work. Now I'm also finding that in some cases I want to keep my representation, as well as the one I discovered, side by side--both having value. This is one reason I'm not 100% sold on the fact that just API providers should be crafting their own OpenAPis--sure, the API space would be waaaaaay better if ALL API providers had machine readable OpenAPIs for all their services, but I would want it to end here. You see, API providers are good (sometimes) at defining what their API does, but they often suck at telling you what is possible--which is why they are doing APIs. I have a lot of people who push back on me creating OpenAPIs for popular APIs, telling me that API providers should be the ones doing the hard work, otherwise it doesn't matter. I'm just not sold that this is the case, and there is an opportunity for evolving the definition&nbsp;of an API by external entities using OpenAPI. To help me explore this idea, and push the boundaries of how I use OpenAPI in my API storytelling, I wanted to frame this in the context of the Amazon EC2 API, which allows me to deploy a single unit of compute into the cloud using an API, a pretty fundamental component of our digital worlds. To make any call against the Amazon EC2 I send all my calls to a single base URL: ec2.amazonaws.com With...[<a href="/2017/05/01/expressing-what-an-api-does-as-well-as-what-is-possible-using-openapi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/05/01/communication-with-consumers-through-the-design-of-our-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/the_sunset_http_header.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/05/01/communication-with-consumers-through-the-design-of-our-apis/">Communication With Consumers Through The Design Of Our APIs</a></h3>
			<p><em>01 May 2017</em></p>
			<p>
Many of the problems that APIs are often associated with API adoption can often be mitigated via more communication. I track on a number of ways the successful API providers are communicating around their API efforts, but I also like it when API designers and architects communicate through the actual technical design of their APIs. One example of this can be found&nbsp;in the IETF draft submission for The Sunset HTTP Header, by Erik Wilde.
"This specification defines the Sunset HTTP response header field, which indicates that a URI is likely to become unresponsive at a specified point in the future."
In his original post, nothing lasts Forever (and some things disappear according to a schedule), Erik shows us that a little bit of embedded communication like this in the design of our APIs can help make API consumers more in tune with what is going on. It is tough to get people's attention sometimes, and I think sometimes when us engineers are heads down in the code we tune out the regular channels, and baking things in the request and response structure of the API can help out a lot.
I like the concept of baking API communication and literacy into our operations through good API design. This is one aspect of hypermedia API design that I think often gets overlooked by the haterz. I'll keep adding these little building blocks to my API communications research, helping develop ways we can better communicate with our API consumers through our API design--in real time.&nbsp;
[<a href="/2017/05/01/communication-with-consumers-through-the-design-of-our-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/28/where-do-i-start-with-apis-your-website/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_website_scrape.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/28/where-do-i-start-with-apis-your-website/">Where Do I Start With APIs? Your Website!</a></h3>
			<p><em>28 Apr 2017</em></p>
			<p>I was firing up my low hanging fruit engine for a customer today, pulling down their entire website, giving them suggestions on where they should start with their API efforts, so I felt it was a good time to blog about this process again. Getting started with APIs can be a challenging question for some organizations, especially when they don't have executive and/or IT buy-in to the concept. To assist folks who find themselves in this situation, I have a process which I call "low-hanging fruit", designed to help jumpstart the API conversation in a safer way. When someone approaches me with the question: Where do I start with APIs at our company, organization, institution, or government agency? If they do not have a specific project in mind or have the buy-in of IT yet, I always recommend started with the low hanging fruit on your website, and targeting these four areas: Existing Spreadsheets - If you there is a CSV, Microsoft Excel Spreadsheet, or Google sheet on the public website, it should be available as an API for searching, and usage in other applications. Existing Data Files - If there are existing XML, JSON, or other machine-readable data files already available on the public website, they should be available as an API for searching, versioning, and usage in other applications. Existing Tables - Any page that has a table over 25 rows should be reviewed, and consider for publishing as a simple API, for use in all applications. Existing Forms - Any page that has a form should be reviewed, and consider for publishing the content available within as a&nbsp;simple API, for use&nbsp;in all applications. Begin here. Target these four areas across your websites. Identify the data source. See if you can get access to the source. If not, scrape the data. Either way, get the data published as a simple API. As soon as you launch your first API, make sure you also...[<a href="/2017/04/28/where-do-i-start-with-apis-your-website/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/28/where-do-i-start-with-apis-your-existing-software-usage/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_software_usage.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/28/where-do-i-start-with-apis-your-existing-software-usage/">Where Do I Start With APIs? Your Existing Software Usage!</a></h3>
			<p><em>28 Apr 2017</em></p>
			<p>When I get asked by folks about where they should start with APIs, I always start with the low hanging fruit on their websites--if it is publicly available as HTML on your website, it should be available as JSON or YAML via an API. After that, I recommend people start with the software and systems a company is already using and might have APIs or data outputs. Your existing technology most likely has APIs or the ability to publish machine-readable data, and you are just not taking advantage of their capabilities as part of any larger API strategy. I recommend launching a new developer portal using Github&nbsp;and get to work inventorying all of the software, systems, website, and services you use. It doesn't have to be public, you can make your repository private if you have any concerns. Profile each of the technological solutions you are using on a regular basis, and see if it has an API or not. If it doesn't, why the hell are you using it? Ok, well maybe that is another discussion. If it has an API, add it to your API list, and set it aside for profiling in more detail, developing a better understanding what is possible. After reviewing&nbsp;all your existing software, and now possessing a list of companies who do have APIs, let's get to work profiling them, and bring together into a coherent 3rd party API strategy for your operations: Developer Portal - Where do we find documentation and other resources for each API? Pay for Service - Are we paying for a service? What are the pricing and plans for each API? Documentation - Where is the API documentation for each API we are including in our catalog? OpenAPI Definition - Create or obtain an OpenAPI definition for each API, allowing it to be used in operations. Support Information - Where do we get support for an API? Are there multiple options, or possibly paid...[<a href="/2017/04/28/where-do-i-start-with-apis-your-existing-software-usage/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/28/a-view-of-the-api-lifecycle-from-james-higginbotham/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-lifecycle.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/28/a-view-of-the-api-lifecycle-from-james-higginbotham/">A View Of The API LIfecycle From James Higginbotham</a></h3>
			<p><em>28 Apr 2017</em></p>
			<p>
Us API Evangelists have a super secret Slack group where we talk about super interesting API Evangelist things, and one of the folks I regularly learn from in this group is James Higginbotham (@launchany). James is a highly skilled enterprise API architect and curator of the popular API Developer Weekly email newsletter. James is always dropping wisdom in the group, but I found a recent API lifecycle list to be particularly worth sharing, as many of my readers are looking to bring some coherence to their own API operations.

Identify Desired Outcome (maps to Discover)
Assess Capabilities Required to Satisfy the Need (activities to achieve the outcome)
Capture Actors/Participants Using the Capabilities to Achieve the Outcomes (activity steps)
Determine Capability Gaps (what don't I have yet, along with what I have - might be various 3rd parties to fill the need)
Find Capability Boundaries (when the problem space is larger than a bounded context for a team, line of business, or some other appropriate boundary)
Identify API Resources
Design API Resource Lifecycle (endpoints that will be offered, to whom)
Document the API Design (OpenAPI, Landing Page for a developer portal section, etc)
Consume the API Design using a mock
Development/Automated Testing
Deploy/Manage/Monitor/Market/Evangelize/Improve/Support

It is a short, but powerful way to look at your life cycle, which often means many different things to many different folks. I particularly like how much focus he puts on the solutions brought to the table, as well as thinking about the realities of actually getting the work done. I come across a lot of companies who know they want APIs but don't have any real plan for getting there, beyond just putting a developer on it. I'm keeping his list handy, augmenting my own, and use to help folks developer their own, custom API lifecycle flow.
[<a href="/2017/04/28/a-view-of-the-api-lifecycle-from-james-higginbotham/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/27/wearing-my-tech-vendor-hat-when-it-comes-to-public-data/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_magic_hat.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/27/wearing-my-tech-vendor-hat-when-it-comes-to-public-data/">Wearing My Tech Vendor Hat When It Comes To Public Data</a></h3>
			<p><em>27 Apr 2017</em></p>
			<p>This is a multipart story on monetizing public data using APIs. I have&nbsp;spent&nbsp;the last seven years studying over 75+ aspects of the API delivery&nbsp;lifecycle&nbsp;across companies, organizations, institutions, and government agencies. This project is designed to be a distillation of my work to help drive a conversation around sensible and&nbsp;pragmatic&nbsp;revenue generation using public data--allowing the&nbsp;city, county, state, and federal government agencies to think critically about how open data efforts can exist and grow. It lives as a standalone repository, as well as individual stories that are meant to stand on their own, while also contributing to an overall narrative about public data monetization. While my primary income is not derived from developing software for sale, I have developed commercial software throughout my career, and actively maintain my own API driven technology platform for tracking on the API industry. This is my best attempt to put on my technology vendor hat on for a bit to better understand the concerns and perspective of the software vendors involved with the public data sector. There is a wide spectrum of technology vendors servicing the space, making this exercise difficult to generalize, but I wanted to take a shot at defending and jumpstarting the conversation at the commercial vendor level. Commercial tech vendors are always at the frontline of discussion around monetization of public data, for better or worse. When open data activists push back on my work to understand how public data can be monetized, the most common response I have is that public data is already being monetized by commercial vendors, and my work is about shining a light on this, and not being in denial that it is already occurring everywhere. Here are some of my thoughts from the public data commercial vendor landscape: Investment In Data -&nbsp;As a technology company I am investing a significant amount of resources into our data, and the data of our customers. While views may greatly vary on how much ownership...[<a href="/2017/04/27/wearing-my-tech-vendor-hat-when-it-comes-to-public-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/27/sdk-generation-api-validation-and-transformation-using-the-apimatic-cli/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/introducing_apimatic_cli.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/27/sdk-generation-api-validation-and-transformation-using-the-apimatic-cli/">SDK Generation, API Validation And Transformation Using The APIMATIC CLI</a></h3>
			<p><em>27 Apr 2017</em></p>
			<p>Continuing a growing number of command line interfaces (CLI) being deployed side by side with APIs, SDK generation provider APIMATIC just released a CLI for their platform, continuing their march towards being a continuous&nbsp;integration provider. There was a great post the other day on Nordic APIs about CLI, highlighting one way API providers seem to be investing in CLIs to help increase the chances that their services will get baked into applications and system integrations. "APIMatic CLI is a command line tool written in Python which serves as a wrapper over our own Python SDK. It is available in the form of a small windows executable so you can easily plug it into your build cycle. You no longer have to write your own code or set up a development environment for the consumption of our APIs." SDK generation, API validation and API transformation baked into your workflow, all driven by API definitions available via any URL. This is a pretty important layer of your API lifecycle, something that isn't easily done if you are still battling the monolith system, but when you've gone full microservices, DevOps, Continous Integration Kung Fu (tm), it provides you with a pretty easy way to define, deploy, and validate API endpoints, as well as the system integrations that consume those APIs--all driven and choreographed by your API definitions. I'm still very API-centric in my workflows and use the APIMATIC API to generate SDKs, and API Transformer to translate API definitions from one format to the other, but I understand that a CLI is more practical for the way some teams operate. API service providers seem to be getting the message and responding to developers with a fully functional CLI, as well as API&nbsp;like Zapier did the other day with the release of their CLI--pushing the boundaries of what is a continuous integration platform as a service. I asked Adeel of APIMATIC when they would have a CLI generator based...[<a href="/2017/04/27/sdk-generation-api-validation-and-transformation-using-the-apimatic-cli/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/27/api-definitions-should-be-done-by-the-api-provider/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-puzzle-piece-gear.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/27/api-definitions-should-be-done-by-the-api-provider/">API Definitions Should Be Done By The API Provider</a></h3>
			<p><em>27 Apr 2017</em></p>
			<p>I talk to a lot of API service and tooling providers about API definitions. I've long been an advocate for API service providers supporting OpenAPI, as well as a variety of API definition formats--if you are having trouble doing this, check out API Transformer. While service providers are an important link in the API definition chain, support of API specification by API providers themselves, and the availability of definitions for all of their APIs is another very critical link in this API supply chain. During a discussion with an iPaaS provider this week about the availability of OpenAPI definitions, a comment was made about there not being enough good sources of usable definitions, specifically from API providers themselves. While it is true, and there is not as much adoption by leading API providers as I would like to see, it still is pretty easy to find numerous proactive efforts by APIs providers like SendGrid, NY Times, and Azure-- just to name a few. Of course, I want ALL API providers to maintain an accurate, comprehensive set of API definitions for the operations, but I don't think is the reality we live in, or even where all API definition creation should occur. It is part of the API DNA for the lion share of the innovation to come from external sources. Sure, I think every API provider would be better off if they maintained their own OpenAPI, generating documentation, code, and other resources. But, I also think it is perfectly acceptable for the community to do some of this heavy lifting. Not all API providers are going to have the skills, time, and other resources to make this happen--often times this is precisely why they are doing APIs, to outsource, and share the innovation load. I see API definitions and discovery as a community thing, something API providers, consumers, as well as API service providers should all be contributing to. No matter where you operate in...[<a href="/2017/04/27/api-definitions-should-be-done-by-the-api-provider/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/26/zapier-was-pretty-savvy-in-their-approach-to-launching-their-partner-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/21f48611824b752981e51eff1c1dfbe1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/26/zapier-was-pretty-savvy-in-their-approach-to-launching-their-partner-api/">Zapier Was Pretty Savvy In Their Approach To Launching Their Partner API</a></h3>
			<p><em>26 Apr 2017</em></p>
			<p>One of the areas of the API sector I've been pretty critical of service providers is in the area of integration platform as a service, or iPaaS. If This Then That emerged on the scene, and began enabling some pretty interesting orchestration between popular APIs, but their approach is something I've been critical of, because I don't feel like they pay forward the API way of doing things, by keeping their partnerships closed, and not offering an API on top of their own API-driven platform. Because of my stance on IFTTT, I've always chosen to support the Zapier team, who are in direct competition with IFTTT, but they also have an API for developing, deploying and managing your platform integrations. My only critique of Zapier has long been that they don't have an API that lets me get at their catalog of API integrations, and workflows (zaps). I'm sure Wade Foster, Zapier's CEO is sick of hearing me complain about this, and I have to finally quit my complaining because they have released the Zapier Partner API, which is exactly the API I wanted--minus one little detail. The Zapier Partner API is not available to everyone, it is as it is named, a "partner API". Meaning I cannot just sign up and get access to the catalog without first being a partner. The open API developer activist in me yells, "WTF, this should be an open API!!--This is not fair!!". Then I stop and think about the realities of doing APIs on the web, today open to the public, with a valuable resource like the catalog of over 750 API integrations that Zapier possesses, and I say "whoooaaa&nbsp;to my high horse", and step off it for a moment, and standing firmly on the ground, I have to admit their approach to releasing the partner API is pretty smart. To use the API you have to have built a Zapier App, and become a partner--you have...[<a href="/2017/04/26/zapier-was-pretty-savvy-in-their-approach-to-launching-their-partner-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/26/simple-api-design-interface-features/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/restlet_api_design_studio.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/26/simple-api-design-interface-features/">Simple API Design Interface Features</a></h3>
			<p><em>26 Apr 2017</em></p>
			<p>I am on a quest to help improve and standardize the available API design tooling out there, and one aspect of doing this is spending time highlighting the API service providers who have interesting approaches to design embedded in their service. Top of my list is Restlet with their studio. There are a couple of things going on in Restlet Studio that I think are significant to the API design conversation, and would like to see become commonplace across service providers, and possibly even part of some sort of open source offering. Restlet Studio has a nice human interface for designing APIs. When you load up an API you are given a simple, clean, yet comprehensive&nbsp;user interface for adding and updating API paths, and other finer details of your design, no developer skills necessary--allowing anyone to step up and help define the API contract. Next, when you look at the bottom of your API design in Restlet Studio you are given a design, OpenAPI 2.0, and RAML 1.0 view of your API, allowing you to switch from user interface to a machine-readable definition of the API contract in two leading formats. This API contract will continue to define your API at every stop along the life cycle, and it significantly helps that this contract is human readable in YAML and Markdown. All API design editors should abstract away as much&nbsp;as they possibly can regarding transformations between API definition formats (use the&nbsp;API Transformer API). I like Restlet's simple approach to delivering design tooling at the center of their platform. I'm keeping an eye on a variety of service providers who are innovating in this way, providing usable tooling when it comes to API design. If you are looking for other examples of this in the wild, take a look at APIMATIC or Stoplight--they are both designing high-quality API design interfaces and tooling. When crafting your own API design user interfaces make sure you take the time...[<a href="/2017/04/26/simple-api-design-interface-features/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/26/my-concerns-as-a-public-data-steward/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/data_steward.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/26/my-concerns-as-a-public-data-steward/">My Concerns As A Public Data Steward</a></h3>
			<p><em>26 Apr 2017</em></p>
			<p>This is a multipart story on monetizing public data using APIs. I have&nbsp;spent&nbsp;the last seven years studying over 75+ aspects of the API delivery&nbsp;lifecycle&nbsp;across companies, organizations, institutions, and government agencies. This project is designed to be a distillation of my work to help drive a conversation around sensible and&nbsp;pragmatic&nbsp;revenue generation using public data--allowing the city, county, state, and federal government agencies to think critically about how open data efforts can exist and grow. It lives as a standalone repository, as well as individual stories that are meant to stand on their own, while also contributing to an overall narrative about public data monetization. I am a database person. I have had a professional career working with databases since 1987 when I began working with COBOL databases as part of student information systems in the State of Oregon. After the Internet became a thing in 1996 I began to architect a variety of database driven web applications. Ten years later, as the cloud began to form, I began architecting distributed data-driven systems using web technology, aka APIs. I understand data. I understand the challenges&nbsp;of being a data administrator, operator, and steward. I wanted to take this experience and awareness&nbsp;and apply it to helping data operators and stewards become more successful when it comes to achieving their mission.&nbsp; As part of this exercise, I wanted to put on my data steward hat for a few, and think about my needs when it comes to monetization of my data. While this project is focused on the monetization of public data, in reality, much of the logic can also be applied to any type of data, it just depends on your view of the data landscape--here is what comes to mind:&nbsp; Hard Work&nbsp;- I have invested a lot of time and resources into my data. This wasn't just a one-time thing. I am perpetually investing in my data, and I would like to see this reflected and respected in...[<a href="/2017/04/26/my-concerns-as-a-public-data-steward/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/04/25/your-wholesale-api-for-sale-in-the-major-api-marketplaces/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/announcing_multi_year_contracts_for_saas_and_apis.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/04/25/your-wholesale-api-for-sale-in-the-major-api-marketplaces/">Your Wholesale API For Sale In The Major API Marketplaces</a></h3>
			<p><em>25 Apr 2017</em></p>
			<p>I have been talking about selling wholesale APIs for some time now, allowing your potential customers to pick and choose exactly the API infrastructure they need, and develop their own virtualized API stacks. I'm not talking about publishing your retail API into marketplaces like Mashpe, I'm talking about making your API deployable and manageable on all the major cloud providers.&nbsp; You see this shift in business with a recent AWS email I got telling me about multi-year contracs for SaaS and APIs. Right now there are&nbsp;70 SaaS products on AWS Marketplace, but from the email I can tell that Amazon is really trying to expand it's API offerings as well. When you deploy an API solution using the AWS Marketplace, and a customer signs up for a one, two, or three year contract, they don't pay for the underlying AWS infrastructure, just for the SaaS, or API solution. I will have to expore more to see if this is just absorbed by the API operator, or AWS working to incentivize this type of wholesale API deployment in their marketplace, and locking in providers and consumers. I'm still learning about how Amazon is shifting the landscpe for deploying and managing APIs in this wholesale, almost API broker type of way. I recently came across the AWS Serverless API Portal, which is meant to augment the delivery of SaaS or API solutions in this way. With this model you could be in the business of deploying API developer portals for companies, and fill ingthe catalog with a variety of wholesale API resources, from a varietiy of providers--opening up a pretty interesting opportunity for white label APIs, and API brokers. As I'm studying this new approach to deploying and managing APIs using marketplaces like this, I'm also noticing a shift towards deliving more algorithmic APIs, with machine learning, artificial intelligence, and other voodoo as the engine--resulting in a shift towards&nbsp;machine learning API marketplaces. I really need to carve...[<a href="/2017/04/25/your-wholesale-api-for-sale-in-the-major-api-marketplaces/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page11" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page13" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home Page</a></li>
    <li><a href="/blog/">The Blog</a></li>
    <li><a href="https://101.apievangelist.com/">API 101</a></li>
    <li><a href="http://history.apievangelist.com">History of APIs</a></li>
    <li><a href="https://women-in-tech.apievangelist.com/">Women in Technology</a></li>    
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
