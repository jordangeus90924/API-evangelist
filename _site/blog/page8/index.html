<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/03/the-transit-feed-api-is-a-nice-blueprint-for-your-home-grown-api-project/">The Transit Feed API Is A Nice Blueprint For Your Home Grown API Project</a></h3>
        <span class="post-date">03 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/transit-feeds-api/transit-feeds-api-home-page.png" align="right" width="45%" style="padding:15px;" /></p>
<p>I look at a lot of APIs. When I land on the home page of an API portal, more often than not I am lost, confused, and unsure of what I need to do to get started. Us developers are very good at complexifying things, and making our APIs implementations as messy as our backends, and the API ideas in our heads. I suffer from this still, and I know what it takes to deliver a simple, useful API experience. It just takes time, resources, as well as knowledge to it properly, and simply. Oh, and caring. You have to care.</p>

<p>I am always on the hunt for good examples of simple API implementations that people can emulate, that aren’t the API rockstars like Twilio and Stripe who have crazy amounts of resources at their disposal. One good example of a simple, useful, well presented API can be found with <a href="http://transitfeeds.com/">the Transit Feeds API, which aggregates the feeds of many different transit providers around the world</a>. When I land on the home page of Transit Feeds, I immediately know what is going on, and I go from home page to making my first API call in under 60 seconds–pretty impressive stuff, for a home grown API project.</p>

<p>While there are still some rough edges, Transit Feeds has all the hallmarks of a quality API implementation. Simple UI, with a clear message about what it does on the home, but most importantly an API that does one thing, and does it well–providing access to transit feeds. The site uses Github OAuth to allow me to instantly sign up and get my API key–which is how ALL APIs should work. You land on the portal, you immediately know what they do, and you have your keys in hand, making an API call, all without having to create yet another API developer account.</p>

<p>The Transit Feed API <a href="http://transitfeeds.com/api/transitfeeds-api.yaml">provides an OpenAPI for their API</a>, and uses it to drive <a href="http://transitfeeds.com/api/swagger/">their Swagger UI API documentation</a>. I wish the API documentation was embedded onto the docs page, but I’m just thankful they are using OpenAPI, and provide detailed interactive API documentations. Additionally, they have a great <a href="http://transitfeeds.com/news">updates page</a>, providing recent site, feed, and data updates across the project. To provide support they wisely <a href="https://transitfeeds.com/issues">use Github Issues to help provide a feedback loop</a> with all their API consumers.</p>

<p>It isn’t rocket surgery. Transit Feed makes it look easy. They provide a pretty simple blueprint that the rest of us can follow. They have all the essential building blocks, in an easy to understand, easy to get up and running format. They leverage OpenAPI and Github, which should be the default for any public API. I’d love to see some POST and PUT methods for the API, encouraging for more engagement with users, but as I said earlier, I’m pretty happy with what is there, and just hope that the project owners keep investing in the Transit Feed API. It provides a great example for me to use when working with transit data, but also gives me a home grown example of an API project that any of my readers could emulate.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/03/the-transit-feed-api-is-a-nice-blueprint-for-your-home-grown-api-project/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/03/alexa-voice-skills-is-the-poster-child-for-your-enterprise-api-efforts/">Alexa Voice Skills Are The Poster Child For Your Enterprise API Efforts</a></h3>
        <span class="post-date">03 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/alexa/alexa-skills.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was sitting in an IT architectural planning meeting for a large enterprise organization the other day, and one of the presentation from one of the executives contained a complex diagram of their IT infrastructure, with a column to the right showing a simple five step Alexa conversation, asking a specific question from customer. Each question posed as part of the Alexa conversation theoretically accessed a different system, weaving a pretty complex web of IT connections, to enable this simple conversation.</p>

<p>This presentation reflects why I feel that Alexa Skills development poses some interesting questions in the API world, and why the platform becomes interesting to so many business users. It reflects the end goal of why we are doing all of this (in theory), but then quickly illustrates how complicated we’ve actually made all of this, demonstrating how challenging delivering conversational interfaces will be in reality. There are many conversational challenges in enabling our system to be able to talk with humans, but I think many of the most daunting challenges companies will face in coming years will be to actually get at the right data to provide a relevant answer to questions poised in voice, bot, and other conversationally-enabled solutions.</p>

<p>Being able to quickly respond to information requests is why many companies, organizations, institutions, and government agencies are doing APIs. Being able to respond to them in real time conversations is definitely a question of doing APIs, but I’m finding in most organizations it is more about solving human and political questions, than it is just a technical one. Sure, you can envision the most beautiful stack of microservices reaching into every aspect of your organization(s), and even develop a robust conversational layer for answering questions posed across that stack, but delivering it all consistently, at scale, across multiple teams of human beings will never be easy, or quick.</p>

<p>I think that conversational interfaces provide an excellent exercise for companies, to help them map out the complexities of their backend systems, and try to understand how to deliver more real time solutions. Personally, I’m not a big fan of bot or voice-enablement, but I know others are. I’m more interested in them because of the technical challenges in delivering, and the business and cultural hurdles they put in front of development teams. It isn’t easy to deliver meaningful, relevant, and intelligent conversations via these new mediums, and I think the Alexa Skills framework provides a useful way for us to hang these conversations regarding our IT resources on.</p>

<p>While the majority of APIs are still about delivering data and content to the web and mobile applications, I think conversational interfaces are showing the future of where things are headed. I don’t think we’ll get there as fast as we would like, or as quickly as the vendors are promising us, but I do think we will make movements towards delivering more meaningful conversational interfaces in coming years. Mostly it will be due to the availability of API resources. If we can get at the data and content, we can usually answer questions regarding that data content. The problem will be that not everything is digitized, and easily accessible. Despite the promises of artificial intelligence, and voice-enabled platforms like Alexa, humans will prove to be the biggest obstacle to realizing the visions of business leaders to answer even the most basic questions we are looking to answer.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/03/alexa-voice-skills-is-the-poster-child-for-your-enterprise-api-efforts/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/03/the-api-transit-basics-api-definitions/">API Transit Basics: API Definitions</a></h3>
        <span class="post-date">03 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-definition.png" align="right" width="40%" style="padding: 15px;" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>Defining an API is the first stop along any API journey. When I say definitions, I’m not just talking about OpenAPI (fka Swagger), and specifically definitions for the surface area of your API. I’m talking about defining your idea, your goals, and the standard aspects of doing business with APIs. By API definitions, I mean having a robust toolbox of definitions for everything that is going into your API operations, from standardized dates and currencies, to common data schema, and yes to making sure there is an active OpenAPI definition for every single one of your APIs.</p>

<p>I’d say that 75% of the companies, organizations, institutions, and government agencies I’m talking with about APIs begin API development by coding. A very costly, and rigid approach to defining a solution to a problem. Many of the groups I know who are using OpenAPI in their operations still rely on it being generated from systems and code, and do not actually hand-define, or hand-craft the definitions for their APIs, which should be being applied across API operations, not just for delivering documentation. When it comes to establishing a robust API definition strategy for operations, I recommend starting with a handful of tools and concepts.</p>

<ul>
  <li><a href="https://www.openapis.org/">OpenAPI</a> - Ensuring there are OpenAPI definitions for ALL APIs / microservices.</li>
  <li><a href="http://json-schema.org/">JSON Schema</a> - Ensuring there are robust JSON schema for all data in use.</li>
  <li><a href="https://www.getpostman.com/docs/postman/collections/creating_collections">Postman Collections</a> - Postman’s proprietary format for defining APIs, which can be translated to and from OpenAPI.</li>
  <li><a href="https://apimatic.io/transformer">API Transformer</a> - Opening up the ability to transform APIs across formats.</li>
  <li>Multi-Format - Being able to speak XML, JSON, and YAML fluently and seamlessly across groups.</li>
</ul>

<p>There are many other tools to assist you in crafting, generating, managing, and evolving the definitions as part of your API operations. API definitions isn’t just one stop along this API journey, and I will be exploring ideas for how API definitions can be applied to each stop along this API journey, in a separate line of thought that runs parallel to what I consider to be my API Transit basics. These five areas represent what I think are the basics of API definitions for ANY API operations, and should be where any API provider begins their journey–by defining the moving parts of each API, and what it will do from define to deprecation.</p>

<p>The biggest threat to properly defining APIs is too much automation, and thinking that they only apply to one stop of the API Transit process. OpenAPI is not just about generating documentation. JSON Schema is not just about completing your OpenAPI definition. Not all APIs are purely JSON, and teams should be multi-lingual when it comes to the definitions they use across API operations. API definitions are essential to not just delivering your APIs, but also communicating and supporting them, and evolving them as part of your road map. API definitions are essential to establishing healthy API operations, and without them, things will easily break down for a single API, and be near impossible to deliver APIs consistently at scale across any organization.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/03/the-api-transit-basics-api-definitions/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/02/my-evolving-definition-of-a-robust-and-diverse-api-toolbox/">My Evolving Definition Of A Robust And Diverse API Toolbox</a></h3>
        <span class="post-date">02 Jan 2018</span>
        <p>It is always telling when folks assume I mean REST when I say API. While the web dominates my definition of API, and REST is definitely a leading architectural style, these assumptions always define the people who bring them to the table, more than they ever do me. I’m in the business of studying how people are applying programmatic interfaces using the web. To reflect my research I’ve been evolving a diagram of my toolbox that I’ve been publishing as part of workshops, presentations, and some talks I’m preparing for 2018. It reflects what I’m seeing as the evolving API toolbox that I’m seeing companies working with, and a diversity in which I’m encouraging others to think about more, as we choose to ignore the polarizing forces in the API sector.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/api-toolbox/API+Toolbox.png" width="100%" style="padding: 15px;" /></p>

<p>To set the tone for any API conversation I am participating in, I prefer to introduce the concept of the API toolbox including more tools than just REST, acknowledging that there are a growing number of tools in our API infrastructure toolbox which can be applied to different APIs, to solve a variety of problems and challenges we face. Also we need to be more honest about the fact that there are many legacy solutions still in use across large organizations, even as we consider adopting the latest in leading edge approaches to API deployment in newer projects.</p>

<ul>
  <li>HTTP - Leverage the web, and the HTTP standard across ALL API efforts.</li>
  <li>SOAP - Acknowledging there are still a number of SOAP services in use.</li>
  <li>RPC - Understand how and why RPC APIs still might be viable in production.</li>
  <li>REST - Making REST, and a resource-centered approach the focus of the operations.</li>
  <li>Microservices - Emphasis on independently deployable and module API services.</li>
  <li>Verbs - Knowing, and putting to use HTTP verbs across API implementations.</li>
  <li>Content-Type - Understanding the negotiation between XML, JSON, and other types.</li>
  <li>Hypermedia - Considering how hypermedia design, and content types play a role.</li>
  <li>GraphQL - Thinking about GraphQL when it comes to data intensive API projects.</li>
  <li>HTTP/2 - Understanding and embracing the evolution of the HTTP standard.</li>
  <li>gRPC - Considering two-speed APIs, and using gRPC for higher volume API implementations.</li>
  <li>Webhooks - Seeing APIs as a two-way street, and pushing data to APIs as well as receiving.</li>
  <li>Server-Sent Events (SSE) - Leveraging HTTP push technology to make things real time.</li>
  <li>Websockets - Opening up two streams that allow for bi-directional API interactions.</li>
  <li>PubSubHubbub - Considering a distributed publish-subscribe approach to API interactions.</li>
  <li>Apache - Being aware of the Apache stack which includes Spark, Kafka, and other real time data solutions.</li>
</ul>

<p>While HTTP and REST are definitely the focal point of many API conversations I am in, SOAP and RPC are legacy realities we must accept are still getting the job done in many environments, and we shouldn’t be shaming the folks who own this infrastructure. At the same time I’m helping folks unwind this legacy infrastructure, I also find myself participating in discussions around event-driven architecture, streaming, and HTTP/2 which represent where API architecture is headed. I’m needing a toolbox that reflects this spectrum of API tooling, as well as where we’ve been, and find ourselves still supporting in 2018.</p>

<p>I’m still evaluating the Apache stack, as well as GraphQL and gRPC, to better understand how they fit in my definition. This work, as well as part new partnership with <a href="http://streamdata.io">Streamdata.io</a> is pushing me to re-evaluate exactly what is real time and streaming using webhooks and server-sent events, alongside a more event-driven approach I am seeing emerge within many leading organizers. People love to say that APIs are done. I wish I could show how silly this way of thinking makes y’all look. The idea that using the web to exchange data, content, and algorithms in a machine readable formats is going anywhere is laughable. My objective is to keep tracking on the tools people are using to get this job done, and help folks ensure their toolbox is as robust and diverse as possible, not traffic in silly dogmatic fantasies about API trends and religions.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/02/my-evolving-definition-of-a-robust-and-diverse-api-toolbox/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/02/treating-all-apis-like-they-are-public/">Treating All APIs Like They Are Public</a></h3>
        <span class="post-date">02 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/45_78_800_500_0_max_0_1_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was talking with the Internal Revenue Service (IRS) about their internal API strategy the week before Christmas, sharing my thoughts on the strategy that they were pitching internally when it comes to the next phase of their API journey. One topic that kept coming up is the firm line of separate between public and private APIs, which you kind of get at an organization like the IRS. It isn’t really the type of organization you want to be vague about this line, making sure everyone understands where an API should be consumed, and where it should not be consumed.</p>

<p>Even with that reality, I still made the suggestion that they should be treating ALL APIs like they are public. I clarified by saying you shouldn’t be getting rid of the hard line dictating whether or not an API is internal or external, but if you treat them all like they are public, and act like they are all under threat, you will be better off for it. This peaked their interest, was something they did not expect to hear from me, and was something they would be adding to their recommendations for the next version of their API strategy.</p>

<p>The first benefit of treating your internal APIs like they are public is when it come to security, logging, and overall API management. You have the tools in place to catch any threats, and develop awareness regarding how an API is being used, both good and bad. While the threats might be minimized internally, developing the same awareness, and having the tools to identify who is using what, and respond accordingly will benefit operations. API security isn’t just about firewalls, it is about an awareness of who is using what.</p>

<p>The next benefit is about the future of your APIs. If you treat APIs like they are public, and you ever want to make it public, you will be in much better shape. You will have proper authentication, management, logging, and security controls already in place. You can cross the line between internal and external with much less friction. When you are ready to work with partners on a project, the time to make resources available can be significantly reduced, making things more efficient and agile when it comes to working with partners.</p>

<p>I get the hard line between internal and external. However I don’t get having two separate API strategies. Have one strategy. Treat everything like they are public, but then be very strict, and explicit about who has access to an API, and monitor, audit, analyze, and report on who has access to API resources in real time. These are web APIs. Let’s treat them all the same, and expect that there will be threats and misuse of varying degrees. Let’s treat all APIs equal, and reduce the chance people will become complacent with API management and security just because it is an “internal” API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/02/treating-all-apis-like-they-are-public/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/02/the-national-transit-database-ntd-needs-to-be-an-api/">The National Transit Database (NTD) Needs To Be An API</a></h3>
        <span class="post-date">02 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/government/federal-transit-agency/the-national-transit-database.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been looking for sources of transit data as part of some research I’m doing with Streamdata.io. Like most industries I study as part of my API research, it is a mess. There is no single source of truth, lack of robust open source solutions, government PDFs acting as databases, and tech companies extracting as much value as they can, and giving as little in return as they possibly can. Todays frustration centers around the unfortunately common federal government PDF database, or more specifically, <a href="https://www.transit.dot.gov/ntd">the National Transit Database (NTD)</a>.</p>

<p>In 2017, when you publish something to the web as a “database”, it should be machine readable. There is some valuable data in the agency profile reports for the 800+ transit agencies available in the database, but this information is locked up in PDFs. You can find machine readable, historic versions of this data up to 2015 in data.gov, but for 2016, and 2017, the data is only available in individual PDFs for each agency profile. To make things more difficult, the listing of transit agencies uses some Ajax voodoo for its pagination and detail pages, making it even harder to scrape, on top of rendering each agencies detail useless by storing it as a PDF.</p>

<p>I understand why government is stuck in this mode. The systems they use only provide them with PDF as a their primary output. Staff hasn’t been trained on the importance of making data available in machine readable formats. People just don’t understand the negative impact they are making on the life of their data, and how it restricts people putting it to work. In some cases, people are fully aware of this, and want to limit how the data gets used, interpreted, keeping them as the definitive source of truth. I’m not saying this is what the Federal Transit Agency (FTA) is up to, but I’m saying it is the effect of their actions, which is having a chilling effect on folks like me using the valuable data to help communities served by these transit agencies.</p>

<p>I emailed the FTA asking if they have a machine readable copy of the database. This information should be published by default as CSV to the agencies Github account. I’m sure the data is available in a spreadsheet somewhere, before it becomes a PDF. It wouldn’t be very hard to save this data as CSV and publish to Github, which could then be easily converted into JSON, or other machine readable formats. I’m happy with CSV. I’m just not happy with PDF being called a database. Database implies that I can put the data to work, and in its current format the National Transit Database (NTD) is’t usable as data–hence not a database. It is just too much work to get out of the PDFs and make usable again, forcing me to step away from my project to understand how communities are investing in transit–I am hoping I can find the data some other place.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/02/the-national-transit-database-ntd-needs-to-be-an-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/01/api-transit-the-basics/">API Transit - The Basics</a></h3>
        <span class="post-date">01 Jan 2018</span>
        <p align="center"><a href="http://basics.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/api-transit/api-transt-subway.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I have been evolving my approach to mapping out all the stops along my API research, using a subway map approach lately. It has been something I’ve been <a href="http://apievangelist.com/2014/12/01/my-turkey-holiday-project-a-subway-map-api/">working on since 2014</a>, and had <a href="https://apievangelist.com/2015/11/29/the-api-lifecycle-my-talk-from-defrag-and-apistrat/">developed as a keynote talk in 2015</a>. My goal is to be able to lay out simple, as well as increasingly complex aspects of consistently operating an API. Something I’ve historically called the API life cycle, but <a href="http://apievangelist.com/2017/08/17/testing-out-the-concept-of-api-transit-instead-of-api-lifecycle/">will work to call API transit in the future</a>.</p>

<p>Right now, I have two main approaches to delivering the API Transit maps. 1) API Life Cycle, and 2) API Documentation. The first is about applying consistent practices to API operations, and the second is about understanding API operations as they happen. In my mind, both these types of API Transit maps will eventually work in sync, but I have to work my way up to that. Right now, I’m focusing on the API Life Cycle version, which is becoming more about API governance, but I’m going to try and rebrand as API Transit. I’m using transit as a verb, “pass across or through” a standard, and consistent way of doing APIs. What some might consider API design, or governance, but I’m considering more holistically.</p>

<p>To support a couple of my consulting projects I am working on at the moment, <a href="http://basics.apievangelist.com/">I have published a simple API Transit project to help navigate some API teams through what I’d consider to be the basics they should be considering as they look to standardize how they deliver APIs across teams</a>. It’s a basic single line, 19 stop API Transit map. It is something I will keep adding stops to, and expand many into their own lines, serving up much more detail, but for this first project I wanted to keep simple, and speaking to a specific enterprise audience. I don’t want to overwhelm them with information as they are just getting started on their API journey. They still have so much work to do in these 19 areas, I don’t them to get distracted with other areas, or feel like they are drowning in information.</p>

<p align="center"><a href="http://basics.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/api-transit/api-transit-basics.png" align="center" width="90%" style="padding: 5px;" /></a></p>

<p>My API Transit maps all run on Github, using Jekyll as the client. Each transit line, and stop is stored as Siren hypermedia stored in a Jekyll Collection. The resulting transit map, and details of each stop is just a simple HTML client which uses Liquid to render the data. This allows me to add stops, and lines as I need, expanding the API journey for each API Transit implementation. I still have routing challenges for the lines on the map. I have an editor for helping me plot where each line should go, but there are no easy answers when it comes to transit map layout, and is something that is proving to be more art than science, so I’m refraining from automating too much at the moment. I’m working on a routing algorithm, but just don’t have the time to perfect it at the moment.</p>

<p>Next, I’m working on more complex iterations of existing APIs, so more about documentation than governance, life cycle, or transit. <a href="http://working.laneworks.net/psd2/psd2-v1.html">I’m doing this with PSD2 as an exercise</a>. Once I’ve done some more complex transit and specific API maps, I will work on combining the two, and applying the governance that exists in the transit map to a specific API, or set of APIs. Not sure where all of this is going, it is just a work in progress right now. It has been for almost three years, and I expect it will continue for many more years. If you are interested in having an API Transit map created for an existing API, or for a specific API governance process, feel free to reach out. I’m looking for more paid work to help push this work forward. Otherwise, it will just move along at whatever pace I can on my own steam!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/01/api-transit-the-basics/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/20/api-discovery-will-be-about-finding-companies-who-do-what-yu-need-and-api-is-assumed/">API Discovery Will Be About Finding Companies Who Do What You Need And API Is Assumed</a></h3>
        <span class="post-date">20 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/27_127_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>While I’m still investing in defining the API discovery space, and I’m seeing some improvements from other API service and tooling providers when it comes to finding, sharing, indexing, and publishing API definitions, I honestly don’t think in the end API discovery will ever be a top-level concern. While API design, deployment, management, and even testing and monitoring have floated to the top as primary discussion areas for API providers, and consumers, the area of API discovery never has quite become a priority. There is always lots of talk about API discovery, mostly about what is broken, rarely about what is needed to fix, with regular waves of directories, marketplaces, and search solutions emerging to attempting to fix the problem, but always falling short.</p>

<p>As I watch more mainstream businesses on-board with the world of APIs, and banks, healthcare, insurance, automobile, and other staple industries work to find their way forward, I’m thinking that the mainstreamification of APIs will surpass API discovery. Meaning that people will be looking for companies who do the thing that they want, and that API is just assumed. Every business will need to have an API, just like every business is assumed to have an website. Sure there will be search engines, directories, and marketplaces to help us find what we are looking for, but when we just won’t always be looking for APIs, we will be looking for solutions. The presence of an API be will be assumed, and if it doesn’t exist we will move on looking for other companies, organizations, institutions, and agencies who do what we need.</p>

<p>I feel like this is one of the reasons API discovery really became a thing. It doesn’t need to be. If you are selling products and services online you need a website, and as the web has matured, you need the same data, content, media, and algorithms available in a machine readable format so they can be distributed to other websites, used within a variety of mobile applications, and available in voice, bot, device, and other applications. This is just how things will work. Developers won’t be searching for APIs, they’ll be searching for the solution to their problem, and the API is just one of the features that have to be present for them to actually become a customer. I’ll keep working to evolve my APIs.json discovery format, and incentivize the development of client, IDE, CI/CD, and other tooling, but I think these things will always be enablers, and not ever a primary concern in the API lifecycle.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/20/api-discovery-will-be-about-finding-companies-who-do-what-yu-need-and-api-is-assumed/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/20/basic-api-design-guidelines-are-you-first-step-towards-api-governance/">Basic API Design Guidelines Are Your First Step Towards API Governance</a></h3>
        <span class="post-date">20 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/54_32_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working with a group that has begun defining their API governance strategy. We’ve discussed a full spectrum of API lifecycle capabilities that need to be integrated into their development practices, and CI/CD workflow, as well as eventually their API governance documentation. However, they are just getting going with the concept of API governance, and I want to make sure they don’t get ahead of themselves and start piling in too much into their API governance documentation, before they can get buy in, and participation from other groups.</p>

<p>We are approaching the first draft of an API governance document for the organization, and while it has lofty aspirations, the first draft is really nothing more than some basic API design guidelines. It is basically a two-page document that explains why REST is good, provides guidance on naming paths, using your verbs, and a handful of other API design practices. While I have a much longer list of items I want to see added to the document, I feel it is much more important to get the basic first draft up, circulated amongst groups, and establishing feedback loops, than making sure the API governance document is comprehensive. Without buy-in from all groups, any API governance strategy will be ignored, and ultimately suffocated by teams who feel like they don’t have any ownership in the process.</p>

<p>I am lobbying that the API governance strategy be versioned and evolved much like any other artifact, code, or documentation applied across API operations. This is v1 of the API governance, and before we can iterate towards v2, we need to get feedback, accept issues, comments, and allow for pull requests on the strategy before it moves forward. It is critical that ALL teams feel like they have been part of the conversation from day one, otherwise it can be weakened as a strategy, and any team looking to implement, coach, advise, report on, and enforce will be hobbled. API governance advocates always wish for things to move forward at a faster speed, but the reality within large organizations will require more consensus, or at least involvement, which will move forward at a variety of speeds depending on the size of the organization.</p>

<p>This process has been a reminder for me, and hopefully for my readers who are looking to get started on their API governance strategy. Always start small. Get your first draft up. Start with the basics of how you define and design your APIs, and then begin to flesh out the finer details of design, deployment, management, testing, and the other stops along your lifecycle. Just get your basic version documentation and guidance published. Maybe even consider calling it something other than governance from day one. Come up with a much more friendly name, that might not turn your various teams off, and then once it matures you can call it what it is, after everyone is participating, and has buy-in regarding the overall API governance strategy for your platform.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/20/basic-api-design-guidelines-are-you-first-step-towards-api-governance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/20/i-am-now-realizing-that-streamdata-io-is-not-just-for-api-providers/">I Am Now Realizing That Streamdata.io Is Not Just For API Providers</a></h3>
        <span class="post-date">20 Dec 2017</span>
        <p><a href="https://streamdata.io"><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/streamdata-push.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>When I first started diving into what <a href="https://streamdata.io">Streamdata.io</a> does, and thinking of their role in the wider API landscape, I was pretty exclusively focused API providers. Meaning, if you are an API provider, depending on the resources you are serving up, you should consider augmenting it with a real time stream using Streamdata.io. This still holds true, but after using Streamdata.io more as a developer, it is becoming clear of Streamdata.io’s value in my toolbox as an API consumer, and thinking about how I can make my applications more efficient, real time, and event-driven.</p>

<p>Right now, I’m just taking a wide variety of existing web APIs and running through the Streamdata.io proxy, and seeing what comes out the other end. I’m in the phase where I’m just understanding what Server-Sent Events (SSE) combined with JSON Patch does to existing web APIs, and their resources. This process is helping me understand the possibilities with streaming existing web APIs, but as I fire up each API I’m seeing it also reveal a new layer of events that exist in between providing APIs, and consuming APIs. I feel like this layer isn’t always evident to API providers, who haven’t made it very far in their API journey.</p>

<p>While I study how the bleeding, and leading edge developers are deploying event-driven architecture, mining for the event value that exists within big data, I’m thinking there is also a pretty interesting opportunity in mining the event layer for existing web APIs. Once I turn on streaming for a web API, the immediate value you see is when a new resource is added. However, this really isn’t that amazing beyond just subscribing to a webhook, or polling an API. I feel like the valuable events we don’t fully see without Server-Sent Events (SSE) is the changes. When a price changes. When a link is modified. When content is refreshed. The subtle events that occur that might not be noticed in regular operations.</p>

<p>I’ve had this conversation with Nicolas Rigaud, the VP Marketing &amp; Partners for Streamdata.io several times recently. That there is unrealized value in these changes to any system. The more they are known, recognized, and responded to, the more value they will possess. I feel like this is potentially the value that is driving the wider event-driven architecture movement at the moment. Understanding the subtle, but important changes that exist across systems and the data that is generated. Not just individual events, but also aggregate events at scale, which equal something much, much bigger. While I feel like “hoovering” up all the data you can find, and dialing in Kafka, or some other event-driven, streaming solution, is how you mine this value at scale, I think there is an equally great opportunity to tune into web APIs, and the unrealized events that happen via everyday platforms.</p>

<p>I’m working on a target list of around 100 APIs to proxy with Streamdata.io so that I can get a handle on the types of events that are occurring within some of the most used APIs out there. I’m guessing that the API providers who have the resources and skills on staff are already jumping at this opportunity, but I’m guessing there are many other APIs that have a significant amount of untapped potential for defining the event layer. This is where I see the potential for Streamdata.io as a tool in the hands of API consumers, and the average developer. To step in from an external vantage point and identify the most meaningful events that are occurring, and make them accessible to other systems, and within applications. Depending on the industry, I’m guessing there will become a growing number of monetization opportunities to emerge from these newfound events as we discover them in the real time streams.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/20/i-am-now-realizing-that-streamdata-io-is-not-just-for-api-providers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/20/understanding-server-sent-events-sse-as-part-of-the-api-landscape/">Understanding Server-Sent Events (SSE) As Part Of The API Landscape</a></h3>
        <span class="post-date">20 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/server-racks-clouds_clean_view.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m continuing to break down the technology stack as I get to know <a href="http://streamdata.io">my new partner Streamdata.io</a>. Yesterday <a href="http://apievangelist.com/2017/12/19/javascript-object-notation-json-patch/">I wrote about their use of JSON Patch for returning partial responses of changes made to an API that has been proxied through the service</a>, and today I want to focus on understanding <a href="https://en.wikipedia.org/wiki/Server-sent_events">Server-Sent Events (SSE)</a>, which Streamdata.io uses to stream those events in real time to any consumer. In my experience, SSE is a lesser known of the real time technologies out there, but is one that holds a lot of potential, so I wanted to spend some time covering it here on the blog.</p>

<p>As opposed to technology that delivers a two-way stream, Server-sent events (SSE) is all about a client receiving automatic updates from a server via HTTP connection. The technology is a standard, with the Server-sent events (SSE) EventSource API being standardized as part of the HTML5 specification out of the W3C. Similar to Streamdata.io’s usage of JSON Patch, SSE is all about efficiency. Making web APIs real time isn’t always about having a two-way connection, and SEE is a great way to make things streaming in a one-way direction, only sending you what has changed in real-time using JSON Patch. Efficiency in direction, delivery, and in message.</p>

<p>Server-sent events (SSE) definitely shines when you look at how it can be used to constantly push and refresh data in any web UI using JavaScript. It’s HTML5 roots makes it a first-class citizen in the browser, but I also think there are a huge number of scenarios to play with when it comes to system integration, and reducing polling on APIs. I think the news, currency, stock, and other financial data scenarios are the low hanging fruit, but I feel like Streamdata.io as a rapid deploy proxy that developers can throw in between any API and a system integration is where the killer use cases of Server-sent events (SSE) could be.</p>

<p>To help me validate this theory I will keep playing with <a href="http://streamdata.io">Streamdata.io</a> and proxying any API I can get my hand on to see what is possible when you replace basic web API requests and responses with Server-sent events (SSE), and begin streaming only what changes after that initial request. I’m guessing that a whole new world of events will begin to emerge, allowing us to think about look at common web API resources differently. I feel like there is a lot of opportunity in deploying real-time, event-driven solutions like Kafka, and other Apache solutions, but I feel like there will be even more opportunity when it comes to getting intimate with the events that are already occurring across existing web APIs, even if the providers are fully tuned into what is going on, or have the resources to tackle event-driven architecture yet.</p>

<p><em><strong>Disclosure:</strong> <a href="http://streamdata.io">Streamdata.io</a> is an API Evangelist partner.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/20/understanding-server-sent-events-sse-as-part-of-the-api-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/19/javascript-object-notation-json-patch/">JavaScript Object Notation (JSON) Patch</a></h3>
        <span class="post-date">19 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/rfc/6902/javascript-object-notation-json-patch.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m continuing my studying into what <a href="http://streamdata.io">my new partner in crime Streamdata.io does</a>, and part of this research is understanding the details of their technology stack. Today’s work involves understanding their usage of <a href="https://tools.ietf.org/html/rfc6902">JavaScript Object Notation (JSON) Patch</a>. When you <a href="http://streamdata.io">proxy any existing web API using Streamdata.io</a>, the first thing you get back is a complete JSON representation of the response, but then with each change you just get back a JSON Patch response with only the details of what has changed. JSON Patch is used for expressing a sequence of operations to apply to a any JSON object or document and you’ll find used with the HTTP PATCH method.</p>

<p>The introduction for <a href="https://tools.ietf.org/html/rfc6902">JSON Patch from RFC [RFC4627]</a> describes it this way:</p>

<blockquote>
  <p>JavaScript Object Notation (JSON) [RFC4627] is a common format for the exchange and storage of structured data.  HTTP PATCH [RFC5789] extends the Hypertext Transfer Protocol (HTTP) [RFC2616] with a method to perform partial modifications to resources.
JSON Patch is a format (identified by the media type “application/json-patch+json”) for expressing a sequence of operations to apply to a target JSON document; it is suitable for use with the HTTP PATCH method.
This format is also potentially useful in other cases in which it is necessary to make partial updates to a JSON document or to a data structure that has similar constraints (i.e., they can be serialized as an object or an array using the JSON grammar).</p>
</blockquote>

<p>JSON Patch is an efficient way to only get the details from an API regarding only what has changed, instead of sending everything over the pipes each time. It makes sense that Streamdata.io has used it in conjunction with Server-Sent Events (SSE) to efficiently cache and stream data from existing web APIs. I have to admit I never put this together with the PATCH method for API responses. Most of the APIs I’ve seen that use PATCH, do not actually implement JSON PATCH, so this was a learning moment for me. Something I’m always thankful for, constantly reminding me just how much I do not know in the space, even with my experience studying APIs.</p>

<p>Next, I’m going to invest more time understanding how to write code that navigates and applies JSON Patch in real-world situations. I’ve got my handful of Streamdata.io enabled APIs streaming me data, but I don’t have the experiencing applying the changes to a UI, or existing system integration as the data flows in. It all makes a lot more sense to me now, and provides me with a efficient model for managing changes that occur across APIs. It is also yet ANOTHER reminder for me of how important it is that I study up on RFCs, and be knowledgable regarding existing patterns like this that exist, so that I’m not reinventing the wheel when it comes to my API design.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/19/javascript-object-notation-json-patch/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/19/from-ci-cd-to-a-continous-everything-ce-workflow/">From CI/CD To A Continuous Everything (CE) Workflow</a></h3>
        <span class="post-date">19 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/27_93_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am evaluating an existing continuous integration and deployment workflow to make recommendations regarding how they can evolve to service their growing API lifecycle. This is an area of my research that spans multiple areas of my work, but I tend to house under what I call <a href="http://orchestration.apievangelist.com/">API orchestration</a>. I try to always step back and look at an evolving area of the tech space as part of the big picture, and attempt to look beyond any individual company, or even the wider industry hype in place that is moving something forward. I see the clear technical benefits of CI/CD, and I see the business benefits of it as well, but I haven’t always been convinced of it as a standalone thing, and have spent the last couple of years trying understand how it fits into the bigger picture.</p>

<p>As I’ve been consulting with several enterprise groups working to adopt a CI/CD mindset, and having similar conversations with government agencies, I’m beginning to see the bigger picture of “continuous”, and starting to decouple it from just deployment and even integration. The first thing that is assumed, not always evident for newbies, but is always a default–is testing. You alway test before you integrate or deploy, right? As I watch groups adopt I’m seeing them struggle with making sure there are other things I feel are an obvious part of the API lifecycle, but aren’t default in a CI/CD mindset, but quickly are being plugged in–things like security, licensing, documentation, discovery, support, communications, etc. In the end, I think us technologists are good at focusing on the tech innovations, but often move right past many of the other things that are essential for the business world. I see this happening with containers, microservices, Kubernetes, Kafka, and other fast moving trends.</p>

<p>I guess the point I want to make is that there is more to a pipeline than just deployment, integration, and testing. We need to make sure that documentation, discovery, security, and other essentials are baked in by default. Otherwise us techies might be continuously forgetting about these aspects, and the newbies might be continuously frustrated that these things aren’t present. We need to make sure we are continuously documenting, continuously securing, and continuously communicating around training, and our continuously evolving (and sharing) our road maps. I’m sure what I’m saying isn’t anything new for the CI/CD veterans, but I’m trying to onboard new folks with the concept, and as with most areas of the tech sector I find the naming and on-boarding materials fairly deficient in possessing all the concepts large organizations are needing to make the shift.</p>

<p>I’m thinking I’m going to be merging my API orchestration (CI/CD) research with my overall API lifecycle research, thinking deeply about how everything from definition to deprecation fits into the pipeline. I feel like CI/CD has been highly focused on the technology of evolving how we deploy and integrate (rightfully so) for some time now, and with adoption expanding we need to zoom out and think about everything else organizations will need to be successful. I see CI/CD as being essential to decoupling the monolith, and changing culture at some of the large organizations I’m working with. I want these folks to be successful, and not fall into the trappings of only thinking about the tech, but also consider the business and political implications involved with being able to move from annual or quarterly deployments and integrations, to where they can do things in weeks, or even days.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/19/from-ci-cd-to-a-continous-everything-ce-workflow/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/19/no-more-scraping-of-banking-data-in-europe-according-to-psd2/">No More Scraping Of Banking Data In Europe According to PSD2, Only APIs</a></h3>
        <span class="post-date">19 Dec 2017</span>
        <p><a href="http://europa.eu/rapid/press-release_MEMO-17-4961_en.htm"><img src="https://s3.amazonaws.com/kinlane-productions/psd2/european-commission-press-release-psd2-scraping.png" align="right" width="45%" style="padding: 15px" /></a></p>
<p>Part of my partnership with <a href="Streamdata.io">http://streamdata.io</a> centers around me investing more time into studying the banking industry, starting with the rollout of PSD2 in Europe next month. I’ll be working through each aspect of the regulations for the banking industry when it comes to APIs, but I wanted to highlight a recent change regarding scraping that is pretty monumental. In <a href="http://europa.eu/rapid/press-release_MEMO-17-4961_en.htm">a recent press release from the European Commission they further clarified guidance for third party payment services providers (TPPs)</a>, and whether or not they can be scraping data from bank still, instead of using the APIs being mandated by the commission.</p>

<p>Here is the section from the press release specifically addressing “what data can TPPs access and use via screen scraping”:</p>

<blockquote>
  <p>PSD2 prohibits TPPs from accessing any other data from the customer payment account beyond those explicitly authorised by the customer. Customers will have to agree on the access, use and processing of these data.
With these new rules, it will no longer be allowed to access the customer’s data through the use of the techniques of “screen scraping”. Screen scraping means accessing the data through the customer interface with the use of the customer’s security credentials. Through screen scraping, TPPs can access customer data without any further identification vis-à-vis the banks.
Banks will have to put in place a communication channel that allows TPPs to access the data that they need in accordance with PSD2. The channel will also be used to enable banks and TPPs to identify each other when accessing these data. It will also allow them to communicate through secure messaging at all times.
Banks may establish this communication channel by adapting their customer online banking interface. They may also create a new dedicated interface that will include all necessary information for the relevant payment service providers.
The RTS specifies the contingency safeguards that banks shall put in place if they decide to develop a dedicated interface. This will ensure fair competition and business continuity for TPPs.</p>
</blockquote>

<p>Banks will have to provide APIs for aggregators to access data. Aggregators will not be allowed to scrape, and are being forced to use the APIs. While there will be a rolling out period, and I’m sure there will still be the bad actors on both sides of the equation, it is the groundwork for a much more sensible and secure approach to providing access to banking customers data–cleaning up the current mess. It is an important step for the banking sector, as well as a significant precedent for the API space when it comes to requiring API access to users data, allowing them to take advantage of valuable 3rd party services.</p>

<p>I’m seeing hints of similar language out of the CFPB regarding banking in the United States, but we are still years behind this kind of regulations. While I would like optimistic that the EU regulations will have an impact on the US when it comes to banks who do business overseas, I’m not holding my breathe. Where I’m going to be placing bets is when it comes to forward thinking banks like Capital One leading the charge when it comes to access to customer data via APIs because it makes sense, not because the government is mandating it. I’m not a big fan of government dictating that industries do APIs, I’m more about companies doing APIs because they make sense.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/19/no-more-scraping-of-banking-data-in-europe-according-to-psd2/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/19/robust-public-storytelling-around-your-ap-process-is-sign-of-maturity/">Robust Public Storytelling Around Your API Process Is Sign Of Maturity</a></h3>
        <span class="post-date">19 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/68_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Sharing stories around your API is something you hear me talk about a lot. Many of my readers like to let me know how they are serious API people, and my storytelling emphasis is silly. Just do APIs. Storytelling is unnecessary fluff. When in reality, storytelling has real, direct benefits on your business bottom line, but also have many other indirect aspects, and its presence is a sign of the overall health of an organization from my vantage point. When you are actively telling stories about your operations, in my experience, it is a sign of the overall maturity of your API process.</p>

<p>I’m working through my storytelling around what Capital One is up to with their DevExchange, studying their approach to API governance, as well as the wider role they are playing in the banking, and even API regulation game here in the United States. I can find stories about each of the topics I’m looking for on their public blog(s), and out in the open. This type of storytelling isn’t accidental, it is an intentional part of a maturing internal and external API strategy. Sure, they have a lot of work ahead of them, but based upon my internal conversations with them, and their external storytelling, I’m aware of how far along they are in their API journey–compared to other banks I’m talking to.</p>

<p>Take a look at <a href="https://medium.com/capital-one-developers">Capital One’s storytelling on Medium</a>. Tune into <a href="https://developer.capitalone.com/community/">the storytelling within the DevExchange community</a>. This isn’t Capital One just being confident in what they do, and are able to tell their story publicly. This is part of what you do to work through your API processes and break down the monolith. Check out <a href="https://dzone.com/guides/microservices-breaking-down-the-monolith">the Breaking Down the Monolith guide from my friend Irakli Nadareishvili over at DZone</a>. When you know your stuff, you are able tell the story of how you are unwinding the enterprise mess publicly like this. You aren’t embarrassed to tell these stories publicly. You are able to get it pass legal. Everyone involved benefits from your storytelling. This is how you do APIs. This is how you begin to shift behavior internally, and set your organization up as a leader in the public sphere. Through storytelling. Sharing your API journey in real time, in a very public way.</p>

<p>The reason you are unable to tell stories at your organization in this way is the reason your API efforts aren’t seeing the success you envisioned. Sure you can blame this on legal, but that is just a symptom of the greater illness. Sure, you can say that you don’t have the skills to write these types of stories, but that is also a symptom of the greater illness. Many folks just don’t see the benefits or value of storytelling, which also means you will never see the benefits or value in doing APIs properly. All of this goes hand in hand, and enables your organization to play nicely with other organizations, and able to share data without friction, and rapidly develop new products and applications. Robust storytelling around your API processes is a sign of the overall maturity of your efforts, something I see play out over and over across the API space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/19/robust-public-storytelling-around-your-ap-process-is-sign-of-maturity/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/18/definition-driven-api-lifecycle-instead-of-code-driven-apis/">Definition-Driven API Lifecycle Instead Of Code-Driven API Deployment</a></h3>
        <span class="post-date">18 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/beach-rocks-currents_internet_numbers.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>You hear a lot about being API design first out of the API echo chamber these days. I’m finding that concept to be challenging for many groups I’m working with due to some of uninformed perceptions around REST, leaving many unable to move towards a design first approach because they are worried if they are doing it correctly. I shifted my own thinking a while back to be more about define-first, requiring that I thoroughly define each API project before I begin moving it along whatever API lifecycle I’ve quantified for a project.</p>

<p>One thing I’m finding pretty common across the enterprise groups who have adopted OpenAPI (fka Swagger) as part of their operations is that many aren’t truly using the API specification format to its full potential as an API definition, let alone applying across multiple stops along the API lifecycle. Over and over I see groups “using Swagger”, but when you dig deeper you see the documentation being autogenerated as part of existing development, out of .JAR files, and (thankfully) evolving continuous deployment workflows. While this is progress, it’s not the definition-driven API lifecycle that organizations should be investing in, is is just code-driven APIs—not actually using OpenAPI to its full potential as a driver across all stops along the API lifecycle.</p>

<p>Getting your hands dirty in the defining, designing, and crafting of OpenAPI definitions is where rubber really begins to meet the road in implementing the API specification. Sure, you can still be autogenerating the specifications from your services and tooling, but you should be actively polishing, and rounding off the rough edges in an API design tool, and putting your definitions to work in an API client like Restlet or Postman to truly define what your API does, or what it doesn’t do. Then taking your definitions and generating server side code, importing into your API gateway, building SDKs, and publishing your documentation and tests. If you skip over getting your hands in there, and actually getting intimate with the requests, responses, and schema of an API, you really aren’t definition-driven, and really just code-driven, which is a much more costly, and inflexible way of doing business with APIs.</p>

<p>One of the biggest challenges in achieving a definition-driven approach to APIs I’m seeing is that many groups still think OpenAPI (fka Swagger) is SwaggerUI (aka Documentation). Most don’t even see the definition behind what they are autogenerating, and have never loaded it into an API design editor like Swagger Editor or Apicurio, let alone within a client tool like Postman. This is something that is only exacerbated with the confusion between what is Swagger and OpenAPI, now that the specification is in the OAI. Code-driven APIs are much more costly, and rigid than definition-driven APIs. While you can still use the resulting API definition throughout the API lifecycle, each iteration, and change will be significantly more costly than if you are working directly from a definition, then mocking, and iterating using a client tool like Postman. Something it will take large enterprises a while to fully realize, as they struggle to realize why their API efforts are returning the results they envisioned.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/18/definition-driven-api-lifecycle-instead-of-code-driven-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/18/what-you-can-expect-as-client-from-soap-to-grprc/">What You Can Expect As A Client From SOAP To gRPC</a></h3>
        <span class="post-date">18 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/57_64_800_500_0_max_0_-5_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m working hard on what I consider to be my definition of a robust API deployment toolbox, and was enjoying the 100K perspective. As I explore, I wanted to share some of my thoughts about by you might expect to receive as a client in each of these scenarios.</p>

<ul>
  <li><strong>SOAP</strong>: You get what the vendor says we can send to you in very structured way.</li>
  <li><strong>REST</strong>: Is this what you want? Let us know if it wasn’t via StackOverflow.</li>
  <li><strong>Hypermedia</strong>: We are prepared to send you whatever we want at any point in the future.</li>
  <li><strong>Microservices</strong>: You are just going get a little bit of this one thing.</li>
  <li><strong>GraphQL</strong>: You get exactly what you want, you better know what to ask for!</li>
  <li><strong>Websockets</strong>: Here you get that, and this, and that, and that…</li>
  <li><strong>PubSub</strong>: You get only the topic you wish to subscribe to.</li>
  <li><strong>Webhooks</strong>: Here you asked us to send this to you–here you go.</li>
  <li><strong>Event Architecture</strong>: You get something whenever that something happens.</li>
  <li><strong>gRPC</strong>: You get what we want really fast, and can accept what we want really fast!!</li>
</ul>

<p>It is fun to step back and think about the motivations, ideology, and pros/cons of each of these API deployment scenarios. I’d love to hear your additions or perspective on what you think the client view of the conversation might be. As I see the API universe continue to expand, I’m curious to see how others are seeing it.</p>

<p>In coming months you’ll hear me write more about event driven architecture, gRPC, and how the pace of things are picking up when it comes to API consumption. I’m working with <a href="https://streamdata.io">Streamdata.io</a> to help try and map out this landscape, as well as some of the usual areas I focus on as the API Evangelist.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/18/what-you-can-expect-as-client-from-soap-to-grprc/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/18/reducing-polling-of-your-existing-api-using-streamdata-io/">Reducing Polling Of Your Existing API Using Streamdata.io</a></h3>
        <span class="post-date">18 Dec 2017</span>
        <p><a href="https://streamdata.io"><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/cloud-architecture-1024x519.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p><a href="https://streamdata.io">I’ve partnered with Streamdata.io</a>, resulting in me getting more acquainted with their API solutions, and telling the story of that process here on API Evangelist. I figured I would dive right in and start with the basics of what Streamdata.io does–turning your existing web API into a real-time stream. Streamdata.io acts as a reverse proxy that translates REST API polling into a stream of data. Instead of constantly polling your API for changes, your API clients will poll Streamdata.io and get a <a href="https://tools.ietf.org/html/rfc6902">JSON Patch update</a> if anything has changed, and reducing the impact of the requests your clients will make to your API.</p>

<p>When thinking about what Streamdata.io does it is easy to get caught up on the real time and streaming nature of what they do, but the most immediate value they bring to the table is about making your relationship with your API clients more efficient. Streamdata.io reduces the costs associated with operating your API, stepping in between you and your demanding clients, and act as a buffer that will reduce the load on your servers. Eliminating one of the biggest headaches for API providers, and reigning in the behavior by our most demanding, and demanding clients.</p>

<p>I’m always surprised by the answers I get from API providers when I ask them why they rate limit their APIs. I’d say that 80% of the time it is based upon reducing the overhead and impact on backend systems, and dealing with the bad behavior of API consumers. Streamdata.io provides a pretty compelling solution to help alleviate this reality of operating APIs for most API providers. It isn’t just about making things real-time, it is more about cost savings, and minimizing the impact of API consumption on our back-end solutions. Making rate limiting irrelevant, unless you have some other specific business needs behind your decision.</p>

<p>There are numerous other benefits Streamdata.io brings to the table, but reducing the load on your APIs probably the most relevant to ALL of my readers who operate APIs. We can always do better when it comes to making our APIs more efficient, and <a href="https://streamdata.io">Streamdata.io</a> is a way we can do this with minimal costs, in minutes, not days, weeks, or months. Which is one of the primary reasons I am partnering with Streamdata.io. It is a service I find easy to push as part of my API storytelling here on the blog, and happy to have become part of the team.</p>

<p><em><strong>Disclosure:</strong> <a href="https://streamdata.io">Streamdata.io</a> is the primary partner for the API Evangelist website.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/18/reducing-polling-of-your-existing-api-using-streamdata-io/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/17/streaming-data-from-the-google-sheet-json-api-and-streamdata-io/">Streaming Data From The Google Sheet JSON API And Streamdata.io</a></h3>
        <span class="post-date">17 Dec 2017</span>
        <p><a href="https://streamdata.io"><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/streamdata-google-sheet.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I am playing with Streamdata.io as I learn how to use my new partner’s service. Streamdata.io proxies any API, and uses <a href="https://streamdata.io/blog/server-sent-events/">Server-Sent Event (SSE)</a> to push updates using <a href="https://tools.ietf.org/html/rfc6902">JSON Patch</a>. I am playing with making a variety of APIs real time using their service, and in my style, I wanted to share the story of what I’m working on, here on the blog. I was making updates to some data in a Google Sheet that I use to drive some data across a couple of my websites, and thought…can I make this spreadsheet streaming using Streamdata.io? Yes. Yes, I can.</p>

<p>To test out my theory I went and created a basic Google Sheet with two columns, one for product name, and one for price. Simulating a potential product pricing list that maybe I’d want to stream across multiple website, or possibly within client and partner portals. Then I published the Google Sheet to the web, making the data publicly available, so I didn’t have to deal with any sort of authentication–something you will only want to do with publicly available data. I’ll play around with an authenticated edition at some point in the future, showing more secure examples.</p>

<p>Once I made the sheet public I grabbed the unique key for the sheet, which you can find in the URL, and placed into this URL: https://spreadsheets.google.com/feeds/list/[sheet key]/od6/public/basic?alt=json. The Google Sheet key takes a little bit to identify in the URL, but it is the long GUID in the URL, which is the longest part of the URL when editing the sheet. Once you put the key in the URL, you can take the URL and paste in the browser–giving you a JSON representation of your sheet, instead of HTML, basically giving you a public API for your Google Sheet. The JSON for Google Sheets is a little verbose and complicated, but once you study a bit it doesn’t take long for it to come into focus, showing eaching of the columns and rows.</p>

<p>Next, I created a Streamdata.io account, verified my email, logged in and created a new app. Something that took me about 2 minutes. I take the new URL for my Google Sheet and publish as the target URL in my Streamdata.io account. The UI then generates a curl statement for calling the API through the Streamdata.io proxy. Before it will work, you will have to replace the second question mark with an ampersand (&amp;), as Streamdata.io assumes you do not have any parameters in the URL. Once replaced, you can open up your command line, paste in the command and run. Using Server-Sent Event (SSE) you’ll see the script running, checking for changes. When you make any changes to your Google Sheet, you will see a JSON Patch response returned with any changes in real time. Providing a real-time stream of your Google Sheet which can be displayed in any application.</p>

<p>Next, I’m going to make a simple JavaScript web page that will take the results and render to the page, showing how to navigate the Google Sheets API response structure, as well as the JSON Patch using the Streamdata.io JavaScript SDK. All together this took me about 5 minutes to make happen, from creating the Google Sheet, to firing up a new Streamdata.io account, and executing the curl command. Sure, you’d still have to make it display anywhere, but it was quicker than I expected to make a Google Sheet real-time. I’ll spend a little more time thinking about the possibilities for using Google Sheets in this way, and publishing some UI examples to Github, providing a forkable use case that anyone can follow when making it all work for them.</p>

<p><em><strong>Disclosure:</strong> <a href="https://streamdata.io">Streamdata.io</a> is an API Evangelist partner, and sponsors this site.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/17/streaming-data-from-the-google-sheet-json-api-and-streamdata-io/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/11/cost-savings-analysis-for-washington-metropolitan-area-transit-authority-wmata-data-apis/">Cost Saving Analysis For Washington Metropolitan Area Transit Authority (WMATA) Data APIs</a></h3>
        <span class="post-date">11 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/transit/washington-metropolitan-area-transit-authority-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>Even before I engaged with Streamdata.io on our current partnership, I was working with them to quantity the value they bring to the table with their service. As I was working on my story regarding <a href="http://apievangelist.com/2017/11/30/licensing-over-dc-transit-data/">the roubling terms of service changes From Washington Metropolitan Area Transit Authority (WMATA) data APIs</a>, the Streamdata.io team was running a cost savings analysis on the WMATA APIs. This is where they take their web API, and see what they could save if they used Streamdata.io, and turned it into a streaming API.</p>

<p>The Streamdata.io team took <a href="https://developer.wmata.com/docs/services/5476365e031f590f38092508/operations/5476365e031f5909e4fe331e">the WMATA Real-Time Bus PredictionsAPI</a>, and assessed the efficiency gains for WMATA when it comes to their most demanding API consumers. Here are the bandwidth and CPU savings:</p>

<ul>
  <li>Client Bandwidth (BW) Savings - 88%</li>
  <li>Server Bandwidth (BW) Savings - 99%</li>
  <li>Server CPU Savings - 87%</li>
</ul>

<p>Streamdata.io does this by being stood up in front of the WMATA web API and caching the results, then only showing changes to clients–in real-time. This isn’t just about making something real-time, it is about reducing the number of times API consumers need to be polling an API. When it comes to transit data you can imagine that the client is probably polling every second to see what has changed.</p>

<p>I’m learning about Streamdata.io’s process for calculating these savings, which is why I’m writing this story. I’m going to work to help apply this to many other APIs, as well as look at productizing the tool so that maybe it can become a self-service tool for other API providers to evaluate their own cost savings, if they went to a real-time way of doing things. To help me understand the savings beyond WMATA, I’m going to be doing benchmarks across all the other US transit provides, and see what kind of numbers I can generate.</p>

<p><em><strong>Disclosure:</strong> Streamdata.io is a paid API Evangelists partner.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/11/cost-savings-analysis-for-washington-metropolitan-area-transit-authority-wmata-data-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/11/api-evangelist-and-streamdata-io/">API Evangelist And Streamdata.io</a></h3>
        <span class="post-date">11 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/083a1a91-3495-4fc8-ae2e-b5b6819548c6-original.jpeg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Some of you in my backchannels know that I’ve been shopping around for a job lately. I’m looking to make a shift in API Evangelist, as I’ve written about some (and will write about more), and I’m also looking for a shift in how I fund my world. During a multi-week search I opened up conversations about a couple of different roles, and one particularly interesting partnership came my way from a company I’ve been working with for a while now. I’ve entered into a partnership with the <a href="https://streamdata.io/">Streamdata.io</a> team, to help them chart the course for their real-time, streaming, event-sourced future, and they’ll continue to invest in me being the API Evangelist. In the past I’ve had several partners at any one time, but moving forward I’m going to limit it to being with a single partner–changing the formula a little bit.</p>

<p>I’ll talk about why the older model wasn’t working in other posts on my personal blog, but moving forward API Evangelist will continue to be about <a href="http://apievangelist.com/api-lifecycle/">my research into the wider API space</a>, but it will increasingly have a focus about what I’m also doing with Streamdata.io. I’m interested in helping Streamdata.io understand that API sector, while I am helping my readers understand what Streamdata.io does. I will be actively telling stories about what I’m up to on a daily basis via API Evangelist, and I will continue to research the wider API space–this is valuable to Streamdata.io, and hopefully it is also valuable to you.  I will also be applying this knowledge to helping the Streamdata.io team help define their place in the API sector, and telling the stories in real-time on API Evangelist–this is valuable to Streamdata.io, and hopefully it is also valuable to you.</p>

<p>Streamdata.io is all about helping define the real-time, streaming layer of the API space–something I have been focused on for a number of years. However, they also invested into helping define the data streaming, event-sourced future of the API space, something I’ve lightly invested in with my webhooks research, but keen to further understand when it comes to Kafka, and other emerging trends. It fits well with what I”m seeing happen in the API space, as well as my interests when it comes to technology. Honestly, it also fits with my interests in having a stable income, and not having to chase stories in the name of page views, or other random projects just because they might bring in some money to pay the bills. I’m not too proud to say that I was ready for Streamdata.io’s help, and it is greatly appreciated.</p>

<p>This partnership wasn’t the next step I envisioned for API Evangelist, but it was the quickest, most natural, and positive offer to show itself–right when I needed it. To the API echo chamber I think what I offer via API Evangelist will change substantially, however to API providers, and API service providers, I’m guessing they probably won’t even notice a difference. I’m looking forward to partnering with Streamdata.io to help get the word out about the dead simple real-time streaming solutions they provide on top of existing APIs, and I’m also excited that they’ll continue to invest in me being the API Evangelist. Here’s to a great partnership Streamdata.io, and I look forward to the stories we can tell together in 2018. I’m looking forward to continuing to help define the API space, with a focus on the perspective that Streamdata.io brings to the table. Something that goes well beyond just streaming data–stay tuned!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/11/api-evangelist-and-streamdata-io/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/10/warming-up-api-providers-we-are-targeting-for-using-streamdata/">Warming Up API Providers We Are Targeting For Using Streamdata.io With Storytelling</a></h3>
        <span class="post-date">10 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/damian-stream-data.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://www.linkedin.com/in/damianodoemena/">My new partner in crime Damian Odoemena the technical account manager for Streamdata.io</a> has said he is ready to work with me to deliver on the road map for the real-time streaming API. I explained to him that I will help fill his head with my knowledge of the API space, as well as be completely transparent around our strategy through storytelling here on API Evangelist. This is one of the reasons I jumped at the opportunity to partner with the team, because of their willingness to let me share what I do with the team, but also tell the story in real-time, streaming (pun intended) our day to day activities here on API Evangelist. I’ll be working 50% of my time on website copy, white papers, as well as evangelism and platform strategy for them, but the other 50% of the time I will be telling the story of what I did for them, here on the blog for me readers to learn from.</p>

<p>This is essentially what I’ve been doing for API Evangelist for the last seven years, except I’ve  crafted my storytelling based upon rolling waves of many partners, as well as ongoing private conversations I’ve had with API providers, and service providers across the space. I’m going to keep this up, but now it will be centered around the work I’m doing for Streamdata.io, as well as talking to their customers to better understand what they need when it comes to delivering regular web APIs, as well as streaming, real-time APIs. I know that Damian nodded his head when I explained this, but I’m guessing he doesn’t fully get how he is going to be thrust into the center of my storytelling, as we work together to develop Streamdata.io’s evangelism strategy, build prototypes, and the other fun things we have planned–he will soon enough! ;-)</p>

<p>I’m working right now with Damian to create a list of API providers who would immediately benefit from Streamdata.io’s service. Even once I have the list established, I’m not a big fan of just cold calling people, or pinging them via social networks, even if I know them personally. It just isn’t my style. I’m a little more passive aggressive, then ever being directly sales aggressive. One way I work, is to warm them up a little bit by writing about them. Crafting one, or many stories about a potential target is a good way for me and Damian to better understand how the service might benefit them, develop the sales pitch we might be taking with them, while publishing and generating some search and social media exhaust that might benefit the platform. Who knows, maybe the target will even read it, and contact us–no sales outreach needed!</p>

<p>Any example of this can be found with an earlier story on <a href="http://apievangelist.com/2017/11/30/licensing-over-dc-transit-data/">the Washington Metropolitan Area Transit Authority (WMATA) terms of service change from a couple of weeks ago</a>, as well as <a href="http://apievangelist.com/2017/12/11/cost-savings-analysis-for-washington-metropolitan-area-transit-authority-wmata-data-apis.markdown">another one today on what WMATA would save if they augmented their web APIs using Streamdata.io</a>. WMATA is on our target list, and we want to be able to quantify how the service would benefit them, and develop an understanding of the transit API–no better way than crafting stories about them. Getting the attention of a transit authority isn’t easy, and these stories are no guarantee that they’ll be tuning in, but you never know. If nothing else, it will heighten our understanding of them, and generate some good SEO juice that maybe other transit groups will be tuning into. Let’s get to work on crafting stories for the rest of our target list Damian–you ready?</p>

<p><em><strong>Disclosure:</strong> Streamdata.io is a paid API Evangelists partner.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/10/warming-up-api-providers-we-are-targeting-for-using-streamdata/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/06/will-apis-still-be-relevant/">Will APIs Still Be Relevant?</a></h3>
        <span class="post-date">06 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/32_119_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I named my blog, company, as well assumed my own title as “API Evangelist” in 2010. Every year since making that decision I’ve questioned it, and wonder if the concept and acronym will fade away. First of all, I have to admit its a bullshit concept in the first place. Its an acronym. It’s a pretty wide umbrella that allows us (me) to assemble a wide variety of technological concepts underneath. However, I made an investment in it, I was going to continue. I found some meaning that I was able to articulate to others, that would make an impact on businesses, organizations, institutions, and government agencies. It works. I am going to run with it, and in 2017, I’m renewing that perspective, and keeping it as my brand, title, and the central message I’m peddling in the tech sector.</p>

<p>This all contributes to a significant under tow on my reality, pushing me to question reality on a regular basis. However, I’d say the strongest current I struggle with in this area is when it comes to endless waves of trends that crash on the shore. Maybe API is irrelevant because of microservices? Wait, maybe it is because of GraphQL? Sure, it will become irrelevant because of Kafka? Web APIs can’t do everything, and it something that will surely render them the wrong choice, just around the corner. There is truth in all of these statements. However, these statements are also just the nature of the game. Web APIs played these same cards when it came onto the scene. REST replaced SOAP, and JSON replaced XML. It is how the technology game is played.</p>

<p>The frontline of this sector will always be developing and evangelizing new tools. It is how it disrupts, and builds new markets. I’m complicit in this. However, the mainstream world won’t ever move as fast as the frontline. No matter how much we want it to. Change just doesn’t happen that quick at scale. I’m confident that my definition of APIs, using the web to make data, content, and algorithms available in a machine readable way isn’t going anywhere soon. I’m confident that the acronym API has enough mindshare, that it can occasionally rise above the trends. Although, the trends actually help refine, and further define the concept, even if they go by other names. It all moves forward at a glacial pace, even though often times it feels like everything always moving so fast–it does this by design.</p>

<p>After seven years of doing this, not a lot has changed. We are still doing web APIs. Most of the time pretty poorly. We are doing them smaller. Occasionally folks are doing them real time, as they have a lot going on. There are more of them, and more people doing them. Not much else is really that revolutionary. It just feels like everything is moving fast, changing at a break neck pace, and there are always new technologies threatening to make APIs irrelevant. If you are in the business of selling new things you believe things are. If you are in the business of buying new technology, you believe things are moving real fast. However, if you are just in the business of understanding what is happening, and getting business done, you see that things aren’t really moving that fast, and APIs will be relevant for some time to come.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/06/will-apis-still-be-relevant/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/06/the-shifting-api-landscape/">The Shifting API Landscape</a></h3>
        <span class="post-date">06 Dec 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/canyon/yellow_collage/file-00_02_34_62.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been watching, and trying to move forward the API conversation across all business sectors for seven years now. I’m not a startup. I’m not an API service provider. I’m not steering an enterprise group. I’m not an investor. I’m a software architect and storyteller who saw the potential for leveraging web infrastructure to deliver data, content, media, and algorithms across the web, to our mobile phones, as well as the seemingly endless number of devices we are connecting to the Internet in our personal, professional, and industrial worlds. I’m not studying the landscape so I sell to it. I am studying the landscape so I can understand it. While most of my readers will not grasp that difference, it gives me a fundamentally different view of what is going on across the space.</p>

<p>In the last seven years I’ve had a focus on helping individuals at SMB, SME, enterprise, organizations, institutions, and government agencies understand what APIs are, and why they should be doing them. In 2017, I feel that mission becoming irrelevant based upon the shifting API landscape. As I work on my third API-first strategy for a top level federal agency in response to an RFI in recent months, prepare for an all week API workshop at Mutual of Omaha in Nebraska, and bookmark the job postings for API architect at almost every major bank in the US and UK, my cute little mission to help understand people understand what APIs are clearly needs to be retired. While there are plenty of people who still need to be educated what APIs are, and that they should be doing them, I’m going to leave it to the waves of other pundits, advocates, evangelists, and analysts to help onboard them. I’ve done my time.</p>

<p>There are many changes on the horizon for API Evangelist which I’ll cover in future posts, but one significant one for me will be to lose “the mission”. As much as I’d like to think people care, in this investment fueled startup world, bundled with an endlessly uncritically belief in technology, I’m not convinced people do. I feel like I was bullshitting myself, right along with other entrepreneurs that I was doing the “good work”, and trying to make a difference in the world (it is a white dude condition). People like to rally around the little campfire I’ve built, but after seven years I can count on two hands the people who actually follow through on this vision. So, you’ll see this delusion disappear from my storytelling, and along with it the belief that everyone should be doing APIs. I’ll still hold on to a belief that EVERYONE should understand what APIs are, as they are an essential aspect of digital literacy in this online world we’ve assembled for ourselves, but I’ll be losing the social good aspect of my API Evangelism in the future.</p>

<p>The API landscape is shifting to be more mainstream. While API providers haven’t made all the choices I would have liked to see in the space when it comes to transparency, observability, privacy, security, and communication, I feel like I’ve had some influence on the space–even if it was just some better storytelling than the marketing that is pumped out of startup and enterprise factories on a daily basis, and the mouthpieces that are the tech blogs. While I will still be talking to the serious startups who are building real tools, and are willing to pay for my consulting services, all the other waves of predatory, exit-building startups that emerge will probably not even know who I am. That is just fine. I’m going to be shifting away from startup-land, in an effort to minimize my frustrations, depression, and to help eliminate the rantiness of my storytelling on the blog. While it may be amusing for some, it is a symptom of a larger illness that plagues not just me, but the wider sector–it is something I’ll be distancing myself from.</p>

<p>I’m working to keep API Evangelist alive and relevant after all these years, as well as pay my bills. I want to keep it up and running. I want to keep it telling relevant stories on a regular basis. This isn’t as easy as it sounds. After 3,060 blog posts I can say that finding the mojo to do it each week, and cover things that are valuable to readers isn’t straightforward. I’m going to shift things to be more relevant to organizations of all sizes to do API right. It will still be a blend of my focus on the technology, business, and politics, but it will be more grownup and mature. Less ranty. Less accessible. More professional. More about helping those doing APIs do them better. I’m just not convinced my helping folks “do APIs” was ever any good for anyone. It was my delusion. Sharing my skills and expertise as a professional might have value to some, but the mission thing was more about my ego, than it was about anyone else. The API space is shifting. It is expanding and growing. It is definitely going mainstream. I’m continuing to study it. Understand it. Report on it. And I am looking forward to my work shifting and evolving with it in 2018. I very curious where it will all lead, and how things will look from my perch.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/06/the-shifting-api-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/05/what-is-more-important-having-an-api-or-having-a-well-designed-api/">What Is More Important? Having An API? Or Having A Well-Designed API?</a></h3>
        <span class="post-date">05 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/16_38_600_500_0_avg_1_1_1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I  got some expected flack this week for some stories on database to API deployments, and allowing folks to just auto-generate APIs from database structures. This approach is notorious for producing very badly designed APIs, which is something that just reflects whatever legacy infrastructure you have as a backend. It is something that drives many of API design, architects, and pundits crazy. Just do things properly!!! Follow good design practices! Put some thought into your API, and have some pride in this interface you are putting out there. All of this is easy for us to declare from our vantage point, but when your entrenched within an existing organization, battling for every movement forward, and often times just to not go backwards, this isn’t always the reality.</p>

<p>As technologists we are always looking forward, and have a really hard time empathizing with folks who are stuck in positions that aren’t as forward leaning as ours. I know we have a well of experience we want everyone to see eye to eye with, but that isn’t always the reality. You can’t convince someone who is just trying to stay afloat within an organization that they should be investing in all of these possibilities in a future they aren’t tuned into. Not everyone holds the privileged position that many of us enjoy in the technology space, and I feel we can do a better job empathizing with some of them. I’m not saying we should give up on leading, and telling stories of a better future, but we need to work to build bridges to many who are less fortunate than we are.</p>

<p>You know what is worse than being in an organization where you are battling for every bit of budget, resources, skills, and other things that help you stay afloat? Having people in more privileged positions making you feel stupid for what you do not understand, or have the time to learn. I wish folks at startups, and bigcos would spend more time investing in the knowledge transfer to smaller, more underserved organizations. Not teaching them to use their software, but actually investing in their staff becoming more web, and API literate. Instead, of making people feel like they don’t have the knowledge, skills, and resources to do things right. In my experience, most of these folks are well aware of this, and they don’t need to be reminded on it.</p>

<p>I’m investing in organizations just doing APIs. Sure, I would like them to do it as well as possible, but I’m more invested in people just doing them. Making their data, content, and other resources more accessible so they can be just a little bit more successful in what they are doing. There will be some pain to go along with this approach, but I feel like it will ultimately be worth it. I can’t shield data stewards, and other would-be API providers from all the pain of doing APIs. I feel it is more important to me that folks have an API, and be on their journey, than having a perfectly designed API. This is where the learning comes, and hopefully I can convince more technologists, startups, and bigcos to invest in this journey, rather than shame people for not being well-equipped when it comes to doing APIs, and quite possibly never even doing them at all. That is much worse, than a poorly designed API in my book.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/05/what-is-more-important-having-an-api-or-having-a-well-designed-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/05/the-picture-we-pain-with-each-api-release/">The Picture We Paint With The Stories We Tell Around Each API Version Release</a></h3>
        <span class="post-date">05 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/facebook/facebook-version-211-release.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I fell down the rabbit hole of the latest Facebook version release, trying to understand the deprecation of their User Insights API. The story of the deprecation of the API isn’t told accurately as part of the the regular release process, so I found myself thinking more deeply about how we tell stories (or don’t) around each step forward of our APIs. I have dedicated areas of my API research for the <a href="http://road-map.apievangelist.com/">road map</a>, <a href="http://issues.apievangelist.com/">issues</a>, and <a href="http://change-log.apievangelist.com/">change log</a> for API operations, because their presence tell a lot about the character of an API, and their usage I feel paints and accurate painting of each moment in time for an API.</p>

<p><a href="https://developers.facebook.com/docs/graph-api/changelog">Facebook has a dedicated change log for their API platform</a>, as well as an <a href="https://developers.facebook.com/status/dashboard/">active status</a> and <a href="https://developers.facebook.com/status/issues/">issues</a> pages, but they do not share much about what their road map looks like. They provide a handful of elements with each releases change log:</p>

<ul>
  <li><strong>New Features</strong> — New products or services, including new nodes, edges, and fields.</li>
  <li><strong>Changes</strong> — Changes to existing products or services (not including Deprecations).</li>
  <li><strong>Deprecations</strong> — Existing products or services that are being removed.</li>
  <li><strong>90-Day Breaking Changes</strong> — Changes and deprecations that will take effect 90 days after the version release date.</li>
</ul>

<p>The presence, or lack of presence, of a road map, change log, status and issue pages for an API paints a particular picture of a platform in my mind. Also, the stories they tell, or do not tell with each release paint an evolving picture of where a platform is headed, and whether or not we want to participating in the journey. Facebook does better than most platforms I track on when it comes to storytelling, by also releasing a blog post telling the story of each release, providing separate posts for <a href="https://developers.facebook.com/blog/post/2017/11/07/graphapi-v2.11/">the Graph API</a>, as well as <a href="https://developers.facebook.com/ads/blog/post/2017/11/07/marketing-api-v211/">the Marketing API</a>. It is too bad that <a href="https://developers.facebook.com/ads/blog/post/2017/11/07/marketing-api-v211/">they omitted the deprecation of the Audience Insight API</a>, which occurred at the time of this story.</p>

<p>While I consider the presence of building blocks like a change log, road map, issues and status page a positive sign for platforms. It still always requires reading between the lines, and staying in tune with each release to really get a feel for how well a platform puts these building blocks to work for the platform. Regardless, I think these building blocks do adequately paint a picture of the current state of a platform, it just usually happens to be the picture that platform wants you to see, not necessary the picture the platform consumers would like to see.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/05/the-picture-we-pain-with-each-api-release/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/05/api-deployment-templates-as-part-of-a-wider-api-governance-strategy/">API Deployment Templates As Part Of A Wider API Governance Strategy</a></h3>
        <span class="post-date">05 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/server-cloud1_internet_numbers.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>People have been asking me for more stories on API governance. Examples of how it is working, or not working at the companies, organizations, institutions, and government agencies I’m talking with. Some folks are looking for top down ways of controlling large teams of developers when it comes to delivering APIs consistently across large disparate organizations, while others are looking for bottom ways to educate and incentivize developers to operate APIs in sync, working together as a large, distributed engine.</p>

<p>I’m approach my research into API governance as I would any other area, not from the bottom up, or top down. I’m just assembling all the building blocks I come across, then began to assemble them into a coherent picture of what is working, and what is not. One example I’ve found of an approach to helping API providers across the federal government better implement consistent API patterns is out of the General Services Administration (GSA), with <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">the Prototype City Pairs API</a>. The Github repository is a working API prototype, documentation and developer portal that is in alignment with the GSA API design guidelines, providing a working example that other API developers can reverse engineer.</p>

<p>The <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">Prototype City Pairs API</a> is a forkable example of what you want developers to emulate in their work. It is a tool in the GSA’s API governance toolbox. It demonstrates what developers should be working towards in not just their API design, but also the supporting portal and documentation. The GSA leads by example. Providing a pretty compelling approach to model, and a building block any API provider could add to their toolbox. I would consider a working prototype to be both a bottom up approach because it is forkable, and usable, but also top down because it can reflect wider organizational API governance objectives.</p>

<p>I could see mature API governance operations having multiple API design and deployment templates like the GSA has done, providing a suite of forkable, reusable API templates that developers can put to use. While not all developers would use, in my experience many teams are actually made up of reverse engineers, who tend to emulate what they know. If they are exposed to bad API design, they tend to just emulate that, but if they are given robust, well-defined examples, they will just emulate healthy patterns. I’m adding API deployment templates to my API governance research, and will keep rounding off strategies for successful API governance, that can work at a wide variety of organizations, and platforms. As it stands, there are not very many examples out there, and I’m hoping to pull together any of the pieces I can find into a coherent set of approaches folks can choose from when crafting their own approach.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/05/api-deployment-templates-as-part-of-a-wider-api-governance-strategy/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/04/narrowing-in-on-my-api-governance-using-api-transit-to-map-out-psd2/">Narrowing In On My API Governance Strategy Using API Transit To Map Out PSD2</a></h3>
        <span class="post-date">04 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/talks/november-2015/subway-map-15.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://apievangelist.com/2017/08/17/testing-out-the-concept-of-api-transit-instead-of-api-lifecycle/">I’m still kicking around my API Transit strategy in my head</a>, trying to find a path forward with applying to API governance. <a href="https://apievangelist.com/2015/11/29/the-api-lifecycle-my-talk-from-defrag-and-apistrat/">I started moving it forward a couple years ago as a way to map out the API lifecycle</a>, but in my experience, managing APIs are rarely a linear lifecycle. I have been captivated by the potential of the subway map to help us map out, understand, and navigate complex infrastructure since I learned about <a href="https://en.wikipedia.org/wiki/Tube_map">Harry Beck’s approach to the London Tube map which has become the standard for quantifying transit around the globe</a>.</p>

<p>I am borrowing from Beck’s work, but augmenting for a digital world to try and map out the API practices I study in my research of the space in a way that allow them to be explored, but also implemented, measured, and reported upon by all stakeholders involved with API operations. While I’m still pushing forward this concept in the safe space of my own API projects, I’m beginning to dabble with applying at the industry level, by applying to PSD2 banking, and seeing if I can’t provide an interactive map that helps folks see, understand, and navigate what is going on when it comes to banking APIs.</p>

<p>An API Transit map for PSD2 would build upon the framework I have derived from my API research, applied specifically for quantifying the PSD2 world. Each of the areas of my research broken down into separate subway lines, that can be plotted along the map with relative stops along they way:</p>

<ul>
  <li><strong>Definition</strong> - Which definitions are used? Where are the OpenAPI, schema, and other relevant patterns.</li>
  <li><strong>Design</strong> - What design patterns are in play across the API definitions, and what is the meaning behind the design of all APIs.</li>
  <li><strong>Deployment</strong> - What does deployment look like on-premise, in the cloud, and from region to region.</li>
  <li><strong>Portals</strong> - What is the minimum viable standard for an API portal presence with any building blocks.</li>
  <li><strong>Management</strong> - Quantify the standard approaches to managing APIs from on-boarding to analysis and reporting.</li>
  <li><strong>Plans</strong> - How are access tiers and plans defined, providing 3rd party access to APIs, including that of aggregators and application developers.</li>
  <li><strong>Monitoring</strong> - What does monitoring of web APIs look like, and how is data aggregated and shared.</li>
  <li><strong>Testing</strong> - What does testing of web APIs look like, and how is data aggregated and shared.</li>
  <li><strong>Performance</strong> - What does performance evaluation of web APIs look like, and how is data aggregated and shared.</li>
  <li><strong>Security</strong> - What are the security practices in place for the entire API stack?</li>
  <li><strong>Breaches</strong> - When there is a breach, what is the protocol, and practices surrounding what should happen–where is the historical data as well.</li>
  <li><strong>Terms of Service</strong> - What does terms of service across many APIs look like?</li>
  <li><strong>Privacy Policy</strong> - How is privacy protected across API operations?</li>
  <li><strong>Support</strong> - What are all the expected support channels, and where are they located?</li>
  <li>Road Map - What is expected, and where do we find the road map and change log for the platform?</li>
</ul>

<p>These are just a handful of the lines I will be laying out as part of my subway map. I have others I want to add, but this provides a nice version of what I”d like to see as an API Transit map of the PSD2 universe. Each line would have numerous stops that would provide resources and potentially tooling to help educate, quantify, and walk people through each of these areas in detail, but in the context of PSD2, and the banking industry. This where I’m beginning to push the subway map context further to help make work in a virtualized world, and augmenting with some concepts I hope will add new dimensions to how we understand, and navigate our digital worlds, but using the subway map as a skeuomorph.</p>

<p>To help make the PSD2 landscape I’m mapping out more valuable I am playing with adding a “tour” layer, which allows me to craft tours that cover specific lines, hitting only the stops that matter, bridges multiple lines, and creates a meaningful tour for a specific audience. Here are a handful of the tours I’m planning for PSD2:</p>

<ul>
  <li><strong>Introduction</strong> - A simple introduction to the concepts at play when it comes to the PSD2 landscape.</li>
  <li><strong>Provider Training</strong> - A detailed training walk-through for anyone looking to provide a PSD2 compliant platform.</li>
  <li><strong>Provider Certification</strong> - A detailed walkthrough that gathers information and detail to map out, quantity, and assess a specific PSD2 API / platform.</li>
  <li><strong>Executive</strong> - A robust walk-through of the concepts at play for an executive from the 100K view, as well as those of their own companies PSD2 certified API, and possibly those of competitors.</li>
  <li><strong>Regulator</strong> - A comprehensive walk through the entire landscape, including what is required, as well as the certification of individual PSD2 API platforms, with real-time control dashboard.</li>
</ul>

<p>These are just a few of the areas I’m looking to provide tours through this quantified PSD2 API Transit landscape. I am using Github to deploy, and evolve my maps, which leverages Jekyll as a Hypermedia client to deliver the API Transit experience. While each line of the API Transit map has it’s own hypermedia flow for storing and experiencing each stop along the line, the tours also have its own hypermedia flows which can augment existing lines and stops, as well as inject their own text, images, audio, video, links and tooling along the way.</p>

<p>The result will be a single URL which anyone can land on for the PSD2 API Transit platform. You can choose from any of the pre-crafted tours, or just begin exploring each line, getting off at only the stops that interest you. Some stops will be destinations, while others will provide transfers to other lines. I’m going to be investing some cycles into my PSD2 API Transit platform over the holidays. If you have any questions, comments, input, or would like to invest in my work, please let me know. I’m always looking for feedback, as well as interested parties to help fund my work and ensure I can carve out the time to make them happen.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/04/narrowing-in-on-my-api-governance-using-api-transit-to-map-out-psd2/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/04/being-able-to-see-your-database-in-xml-json-and-csv/">Being Able To See Your Database In XML, JSON, and CSV</a></h3>
        <span class="post-date">04 Dec 2017</span>
        <p><a href="https://www.slashdb.com/documentation/api-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions/slashdb/slashdb-content-negotiation.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p>I remember making the migration from XML to JSON. It was hard for me to understand that difference between the formats, and that you accomplish pretty much the same things in JSON that you could in XML. I’ve been seeing similarities in my migration to YAML from JSON. The parallels in each of these formats isn’t 100%, but this story is more about our perception of data formats, than it is about the technical details. CSV has long been a tool in my toolbox, but it was until this recent migration from JSON to YAML that I really started seeing the importance of CSV when it comes to helping onboard business users with the API possibilities.</p>

<p>In my experience API design plays a significant role in helping us understand our data. Half of this equation is understanding our schema, and what the dimensions, field names, and data types of the data we are moving around using APIs. As I was working through some stories on how my friends over at SlashDB are turning databases into APIs, I saw that they were translating database, tables, and field names into API design, and that <a href="https://www.slashdb.com/documentation/api-documentation/">they also help you handle content negotiation between JSON, XML, CSV</a>. Which I interpret as an excellent opportunity for learning more about the data we have in our databases, and getting to know the design aspects of the data schema.</p>

<p>In an earlier post about what SlashDB does I mentioned that many API designers cringe at translating database directly into a web API. While I agree that people should be investing into API design to get to know their data resources, the more time I spend with SlashDB’s approach to deploying APIs from a variety of databases, the more I see the potential for teaching API design skills along the way. I know many API developers who understand API design, but do not understand content negotiation between XML, JSON, and CSV. I see an opportunity for helping publish web APIs from a database, while having a conversation about what the API design should be, and also getting to know the underlying schema, then being able to actively negotiate between the different formats–all using an existing service.</p>

<p>While I want everyone to be as advanced as they possibly can with their API implementations, I also understand the reality on the ground at many organizations. I’m looking for any possible way to just get people doing APIs, and begin their journey, and I am not going to be to heavy handed when it comes to people being up to speed on modern API design concepts. The API journey is the perfect way to learn, and going from database to API, and kicking of the journey is more important than expecting everyone to be skilled from day one. This is why I’m partnering with companies like SlashDB, to help highlight tools that can help organizations take their existing legacy databases and translate them into web APIs, even if those APIs are just auto-translations of their database schema.</p>

<p>Being able to see your database as XML, JSON, and CSV is an important API literacy exercise for companies, organizations, institutions, and government agencies who are looking to make their data resources available to partners using the web. It is another important step in understanding what we have, and the naming and dimensions of what we are making available. I think the XML to JSON holds one particular set of lessons, but then CSV possesses a set of lessons all its own, helping keep the bar low for the average business user when it comes to making data available over the web. I’m feeling like there are a number of important lessons for companies looking to make their databases available via web APIs over at SlashDB, with automated XML, JSON, and CSV translation being just a notable one.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/04/being-able-to-see-your-database-in-xml-json-and-csv/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/04/facebook-quietly-deprecates-the-audience-insight-api-used-to-automate-targeting-during-the-election/">Facebook Quietly Deprecates The Audience Insight API Used To Automate Targeting During The Election</a></h3>
        <span class="post-date">04 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/facebook/audience-insights/facebook-audience-insights-api-affinity.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://www.adweek.com/digital/facebook-is-shutting-down-its-api-that-marketers-lean-on-for-research/#/">According to AdWeek, Facebook is quietly shutting down its Audience Insights API by the end of the year</a>. They have a statement from Facebook stating, “We have decided to focus marketers on our more broadly available Audience Insights tool, so we are winding down the Audience Insights API by end of year. We’ll continue testing different ways to provide valuable insights to advertisers and agencies through the tool and across other destinations on Facebook.” which I assume they got directly from Facebook, because I can find no other communication regarding the deprecation of the API through normal <a href="https://newsroom.fb.com/">newsroom</a>, or <a href="https://developers.facebook.com/docs/graph-api/changelog">API change log</a> channels. It could be that I’m missing it, but it is clear they are trying to minimize chatter around this.</p>

<p>According to <a href="https://www.facebook.com/business/help/304781119678235">the Facebook help page</a>, Audience Insights, “shows you data about your target audiences so that you can create more relevant advertisements for them”. The platform uses native Facebook data to show you audience features such as: Age and gender, Relationship status, Education level, Job role, Top categories, Page likes, Top cities, Top countries, Top languages, Frequency of activities, and Device users. Then using third-party data (data come from sources like Acxiom, Datalogix and Epsilon) they show you audience features such as: Lifestyle, Household income, Home ownership, Household size, Home market value, Spending methods, Retail spending, Online purchases, Purchase behavior, and whether they are in market for a vehicle. You can still get at this via <a href="https://www.facebook.com/ads/audience-insights/">the Facebook Audience Insights web interface</a>, but the APIs for automating this aspect of Facebook has mostly disappeared, or is in the process of disappearing.</p>

<p>There are three layers to the Faceook Audience Insights API deprecation. You can still access some insights for ads, pages, and other objects, as well as one audience insight still available:</p>

<ul>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/v2.11/insights"><strong>/{object-id}/insights</strong></a>  - Facebook Insights is a product available to all Pages and Apps on Facebook using the Insights dashboard.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-rule/"><strong>Audience Insights Rule</strong></a> - Definition of an audience insight rule.</li>
</ul>

<p>Then there are a handful of API paths related to Audience Insights that are still there, but not listed off the main navigation:</p>

<ul>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-rule-component/"><strong>Audience Insights Rule Component</strong></a> - Rule component of a study rule.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-post/"><strong>Audience Insights Post</strong></a> - Represents a sample post.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-group-by-summary/"><strong>Audience Insights Group By Summary</strong></a> - Overall summary for audience insights query insights.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/insights-value/"><strong>Insights Value</strong></a> - The value for one insights metric given a timestamp.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/insights-result/"><strong>Insights Result</strong></a> - The result of an Insights query.</li>
</ul>

<p>Then there are the core Audince Insights APIs that are completely gone, with all documentation removed:</p>

<ul>
  <li><strong>Audience Insights Lifestyles</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-lifestyles/">URL</a>) (<a href="https://webcache.googleusercontent.com/search?q=cache:zcVvTrCjRTYJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-lifestyles/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Insights about lifestyles for you audience.</li>
  <li><strong>Audience Insight Education Level</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-education-level/">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:AxUtpiQ0vuIJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-education-level/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Information about the education level of your audience</li>
  <li><strong>Audience Insights Home Owners</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-home-owners/">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:8KLILtYG3KYJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-home-owners/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Information about home ownership.</li>
  <li><strong>Audience Insights Household Income</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-household-income/">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:um_yOLJk-lYJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-household-incomes/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Household incomes information about your audience.</li>
  <li><strong>Audience Insights Purchase Behaviors</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-purchase-behaviors/">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:7GPqnSgOYVIJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-purchase-behaviors/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Purchase behaviors information for your audience.</li>
  <li><strong>Audience Insights Affinity</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:rhIkcFkbT7YJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-affinity/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Information about the affinity for a given page.</li>
</ul>

<p>These are the six API paths that you would use to scale and automate any information, or disinformation campaign. This is how you develop, evolve, and act upon your models when it comes to publishing Facebook Pages, buying advertising and spreading video, photos, news, and other (dis)information that you are targeting your users with. I see hints of these insight API going away on <a href="https://developers.facebook.com/docs/graph-api/changelog/version2.11">the most recent November 7th update</a>, but there are <a href="https://developers.facebook.com/docs/graph-api/changelog/version2.10#mapi-deprecate">no marketing API deprecations in the last one in July that changed how you are able share links via the API</a>–something that was a response to the election backlash. The last cache of the missing documentation pages was on November 9th, showing they’ve been actively working in November to clean things up, and by the looks of things they are still working on this.</p>

<p>Ok, many might say that this is a good thing. Facebook is removing the tools that allow you to automate these types of campaigns. Limiting who has access to them. Sure. However, it doesn’t stop them from still providing access to partners, and other folks behind the scenes, further reducing any observability into the process, after <a href="https://newsroom.fb.com/news/2017/10/update-on-our-advertising-transparency-and-authenticity-efforts/">they’ve promised to be more transparent about all of this</a>. Also, the sneaky nature of the API deprecation, which isn’t unusual for Facebook reveals their true motivation. The deprecation is only published in AdWeek, and clearly is something other outlets are either unaware of, or unwilling to talk about due to retribution by Facebook, which might limit your exposure on the network. Facebook has many news outlets by the balls when it comes to platform exposure these days, potentially limiting who will be critical of the platform.</p>

<p>The Facebook Audience Insights API represents the conundrum of APIs for me. If APIs don’t exist we can’t see into the algorithms that are increasingly governing our lives. If they do exist then people with ill intentions get access to them, and can use them for shady things like we’ve been seeing as part of the election. The answer? They should exist, but then provide access by auditors, regulators, researchers, and journalists to see what is possible via platforms. Then, EVERYONE who has access to the tools should be observable and accountable. Not just the APIs, but also the web interface. If you are developing models that target a demographic, that demographic should know about it, and auditors, researchers, and journalists should have API access to all of this, so that they can assess and report on what is going on. The watchers should also be accountable. This is why I do APIs, not because I believe they are always good, but because they provide us with secure, managed, accountable observability into how platforms and algorithms work (or don’t).</p>

<p>Ideally, tools like this do not exist in the first place. My feeling is that we burn it down. However, I know this isn’t a reality. My next recommendation is that ALL advertising platforms possess APIs for ALL aspects of operations, with access tiers for auditors, regulators, researchers, and journalists. Observability into how these platforms are operating is the only way we can move this conversation forward in a way that protects the end-users of platforms from harm. It is clear that Facebook is not interested in true observability, and are playing the usual transparency games by acting like they are self-regulating, but then just pulling the curtains on what they are up to. In coming years, we’ll see more APIs be deprecated because of this, as the platforms realizing what is possible, and just commence more secretive about what they do. The cat is out of the bag. The technology exists to give us visibility into what is going on, the trick is going to be all about keeping the APIs that exist operational, and delivering 100% coverage of platform operations, and regulating that APIs be introduced where they do not exist already. Sorry platforms, you had plenty of time to be straight up about this stuff, and you chose not to.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/04/facebook-quietly-deprecates-the-audience-insight-api-used-to-automate-targeting-during-the-election/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/01/the-conversational-interface-appetite-for-data-via-apis/">The Conversational Interface Appetite For Data Via APIs</a></h3>
        <span class="post-date">01 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/16_77_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px" /></p>
<p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p>I spend a lot of time studying what is going on around bots on Twitter, Facebook, and Slack, as well as voice enablement like we see with Alexa, Google, and Siri. I lump these all under a research category called conversational interfaces. Conversational interfaces represent the next generation of API clients, with AWS Alexa being the most sophisticated example at how it will all work(eventually). While there are some interesting examples of conversational interfaces in action, for the most part they are still pretty simple, silly, and not providing much value. I’d say that any of the bots or voice implementations I’ve come across which are useful, are also pretty corporate, demonstrating the amount of resources you need to invest when crafting conversational interfaces.</p>

<p>From my vantage point I’m seeing three main areas slowing the growth of true usability of conversational interfaces, 1) desire, and people not wanting or caring to engage, 2) availability of data via APIs in format that is usable, and 3) the performance of APIs that do have relevant data, and their ability to deliver it as an answer to a question in reasonable amount of time. You can put me squarely into the first category of not really wanting to use conversational interfaces, but I do understand that there are people who are into doing it, which gets me somewhat involved when it comes to thinking about the 2nd, and 3rd challenge. APIs are what delivers answers in conversational interfaces, and since APIs are my jam, I’m tuning in.</p>

<p>One of the biggest challenges the conversational interface space will face in coming years is having the access to the answers or data they need to function as expected. It’s not that the data isn’t out there, it is that it isn’t available in accessible, usable API interfaces that developers can quickly wire up via platforms like Slack and Alexa. There is a wealth of sports data out there, but to make it available via bots and voice platforms you have to be able to get at via APIs. There is a wealth of movie data out there, but you have to be able to get at it via simple APIs. I can go on and on about the types of data we need, and even point out where you can find it, the problem is that it isn’t available via a simple web API so that a developer can quickly build a conversational interface on top of it.</p>

<p>This is why you’ll find me doing more research into data, and database to API implementations, partnering with folks like <a href="https://www.slashdb.com/">SlashDB</a>, who help make deploying web APIs from databases dead simple. We need more APIs, not thousands more, but millions more. We need the APIs to be simple, and authentication standardized, so that developers can quickly get their hands on what they need to develop valuable conversational interfaces. We don’t need API providers publishing APIs trying to be the next Twilio or SendGrid. We need API providers making ALL their valuable data available via APIs, and removing the friction for conversational interface developers to find what they need, so they can wire up the answers demanded by bots, voice, and other applications. If you want your valuable data available in conversational interfaces you need to be exposing it via web APIs.</p>

<p>Personally, I do not get excited by bot or voice enabled applications. I enjoy automation, but I’m more of a fan of the intimacy between my brain, my fingers, and the keyboard. However, like most of the tech space I understand that conversational interfaces will keep evolving, and want to contribute where I can to make them more usable. Another aspect of why I am getting on board with conversational interfaces, as with all the other API driven applications, is when it comes to surveillance and privacy. I want to play a role in helping define the backend layers of conversational interfaces, make them usable, valuable, while also protecting the privacy, security, and data ownership of individuals who are putting them to work. This is why you’ll find me chiming in more on the subject, not because I’m pro-conversational interface, it is because they are happening, and I want to make sure it works as well as possible for everyone involved.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/01/the-conversational-interface-appetite-for-data-via-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/01/how-do-you-ask-questions-of-data-using-apis/">How Do You Ask Questions Of Data Using APIs?</a></h3>
        <span class="post-date">01 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/27_93_800_500_0_max_0_-5_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m preparing to publish a bunch of transit related data as APIs, for us across a number of applications from visualizations to conversation interfaces like bots and voice-enablement. As I’m learning about the data, publishing it as unsophisticated CRUD APIs, I’m thinking deeply about how I would enable others to ask questions of this data using web APIs. I’m thinking about the hard work of deriving visual meaning from specific questions, all the way to how would you respond to an Alexa query regarding transit data in less than a second. Going well beyond what CRUD gives us when we publish our APIs and taking things to the next level.</p>

<p>Knowing the technology sector, the first response I’ll get is machine learning! You take all your data, and you train up some machine learning models, put some natural language process to work, and voila, you have your answer to how you provide answers. I think this is a sensible approach to many data sets, and for organizations who have the machine learning skills and resources at their disposal. There are also a growing number of SaaS solutions for helping put machine learning work to answer complex questions that might be asked of large databases. Machine learning is definitely part of the equation for me, but I’m not convinced it is the answer in all situations, and it might not always yield the correct answers we are always looking for.</p>

<p>After machine learning, and first on my list of solutions to this challenge is API design. How can I enable a domain expert to pull out the meaningful questions that will be asked of data, and expose as simple API paths, allowing consumers to easily get at the answers to questions. I’m a big fan of this approach because I feel like the chance we will get right answers to questions will be greater, and the APIs will help consumers understand what questions they might want to be asking, even when they are not domain experts. This approach might be more labor intensive than the magic of machine learning, but I feel like it will produce much higher quality results, and better serve the objectives I have for making data available for querying. Plus, this is a lower impact solution, allowing more people to implement, who might not have the machine learning skills or resources at their disposal. API design using low-cost web technology, makes for very accessible solutions.</p>

<p>Whether you go the machine learning or artisanal domain expert API design route, there has to be a feedback loop in place to help improve the questions being asked, as well as the answers being given. If there is no feedback loop, the process will never be improved. This is what APIs excel at when you do them properly. The savvy API platform providers have established feedback loops for API consumers, and their users to correct answers when they are wrong, learn how to ask new types of questions, and improve upon the entire question and answer life cycle. I don’t care whether you are going the machine learning route, or the API design route, you have to have a feedback loop in place to make this work as expected. Otherwise it is a closed loop system, and unlikely to give the answers people are looking for.</p>

<p>For now, I’m leaning heavily on the API design route to allow for my consumers to ask questions of the data I’m publishing as APIs. I’m convinced of my ability to ask some sensible questions of the data, and expose as simple URLs that anyone can query, and then evolve forward and improve upon as time passes. I just don’t have the time and resources to invest in the machine learning route at this point. As the leading machine learning platforms evolve, or as I generate more revenue to be able to invest in these solutions I may change my tune. However, for now I’ll just keep publishing data as simple web APIs, and crafting meaningful paths that allow people to ask questions of some of the data I’m coming across locked up in zip files, spreadsheets, and databases.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/01/how-do-you-ask-questions-of-data-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/01/how-to-say-you-might-charge-for-api-access-in-the-future-without-being-a-dick/">How To Say You Might Charge For API Access In The Future Without Being A Jerk</a></h3>
        <span class="post-date">01 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/statue-face-open-mouth_copper_circuit.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I get it. It takes money to operate APIs. I’m a big advocate for making sure API providers, even public data API providers can sensibly charge for access to their valuable resources. I’m also painfully aware at how unrealistic a libertarian driven view of the web being open and free makes it very difficult to begin charging for data that has been historically free. However, I’m also a fan of helping API providers understand how they can communicate that they might / will be charging for access to data at some point in the future without being complete jerks about it.</p>

<p>I see API providers regularly make the statement that they will begin charging for API access at some point in the future, but this particular story is driven from hearing it out of the <a href="https://technical.ly/dc/2017/11/22/developers-upset-wmatas-new-data-terms-use/">Washington Metropolitan Area Transit Authority (WMATA) making changes to their terms of service</a>, where one of the bullet points was that they would begin charging for access at some point. Making the announcement that you intend to begin charging for something that has been free is challenging in any API ecosystem, but especially so within public data API ecosystems like WMATA. In any of these environments you can’t just shoot across your community’s bow with a statement like this, and expect a positive response. Doing so, just shows how out of touch with your community you are.</p>

<p>First rule of communicating around the business side of your road map is don’t just say you’ll be charging at some point and leave things there. Give details of what this means. Demonstrate your knowledge around how API management and service composition works. Will ALL developers be charged? Will it just be commercial developers? Will it be developers over a certain level of consumption? Do not leave it to the communities imagination regarding what will happen, because this is where the powers of Internet speculation will take hold, and begin working against your API efforts. This is where your entire community will begin talking about how these changes will impact their business, and begin to prepare for the worst, even if the changes won’t even impact them. Creating a ripple effect across your API platform, and potentially hurting business beyond what will actually be reality.</p>

<p>Next, share some thoughts behind the reasoning behind these changes. Craft a blog post. Hold some office hours. Talk to your API consumers about why you will need to start charging for access to ALL or some of your API resources. Back up the details you provied with some actual insight into what went into the decision making process. Prove to your API consumers that you have their best interest in mind, and aren’t just looking to screw everyone over. A lack of visibility into the decision making process will only push your API consumers to assume the worst. Ideally, this isn’t just a one time event, and you publish a series of blog posts sharing the story behind the process of needing to generate more revenue, to cover rising costs, or whatever else might be the reason behind the need to charge for access at some point in the future. Don’t make this just a sudden thing, build up to it, and ease your community into the concept that APIs will move from free to paid.</p>

<p>After providing details on the API monetization strategy and plan, and sharing the story behind this shift in platform operations, lean on your API feedback loop as part of your shift in strategy. You have a strong feedback loop in place directly with your strongest API consumers, and at scale across the rest of your API consumers, right? You actively understand what your strongest platform consumers are thinking, and how the introduction of fees might impact their operations, right? I’m guessing if you are making vague statements about charging for access in the future and just walking away, that there is NO feedback loop, or the feedback loop is pernicious to say the least. You don’t really have much interest in what your API consumers are thinking, and how the shifts in a fee structure and monetization strategy will impact them. Otherwise, you’d fully understand the impacts of making statements about charging for API consumption at some date down the road.</p>

<p>Being an API provider isn’t easy. Balancing your platform concerns with those of your API consumers isn’t easy. Time and time again I see providers enter into the game without having put much thought into a monetization strategy, and have no coherent plan in place. Making changes down the road painful for everyone. Do yourself a favor, and spend the time learning about modern API management practices, and how API service composition works. Visit the API portals of leading API providers to see how they have structured their plans, and composed their service access tiers. Talk to people like me who study this stuff for a living, before you ever go public with your API. However, once you do, know that communication is essential, and that you won’t get away with being a jerk on this stuff, and just randomly telling people that at some point in the future you will be charging for access doesn’t fly in API-land, things don’t work like that.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/01/how-to-say-you-might-charge-for-api-access-in-the-future-without-being-a-dick/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/30/sql-statement-pass-through-using-web-apis/">SQL Statement Pass-Through Using Web APIs</a></h3>
        <span class="post-date">30 Nov 2017</span>
        <p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p><a href="https://www.slashdb.com/how-it-works/#sql-pass-thru"><img src="https://s3.amazonaws.com/kinlane-productions/slashdb/slashdb-sql-pass-through-mode.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I’m closely following the approach of GraphQL when it comes to making data resources more accessible by API consumers when developing applications. I think there is some serious value introduced when it comes empowering front-end developers with the ability to get exactly the data they need using a variety of querying structures. I enjoy studying up on different approaches to making different dimensions of a database to consumers and end-users, and found a pretty scrappy one from my friends over at SlashDB, with <a href="https://www.slashdb.com/how-it-works/#sql-pass-thru">their SQL statement pass through</a>. It’s not the most formal approach to query a database, but I think it’s scrappy and simple enough, that it might work for a wide variety of technical, as well as non-technical users.</p>

<p>Using the SlashDB mode, an administrator, or an application backend developer can define arbitrary SQL queries which once defined, can be executed as a smple URL. The example query they provide returns customers from London: http://demo.slashdb.com/query/customers-in-city/city/London.html. It is something that will make RESTafarians pull their hair (dreads?) out, but for business users looking to get their hands on some data to populate a spreadsheet, or share with a partner when developing an application–it will be a lifesaver. As the GraphQL folks like trumpet, REST isn’t the only way to get things done, and while I think we should be thinking critical about the long term impact of our API design choices, getting business done efficiently is an important aspect of doing APIs as well.</p>

<p>What I like about the SlashDB approach is it makes for an intuitive URL. Something business users can understand. I could see crafting these in bulk, and some becoming permanent, while others maybe being more of a temporary thing. Depending on the application you may want to standardize how you publish your URLs, using common patterns, and making sure queries aren’t changing, if they are being baked into applications. I think that simple URLs that retrieve data from a database will always trump a more complex, technical solution that developers often want. Developers are always going to want more robust solutions that they can tweak and play with, but business users just want what they need, and are looking for the quickest way to solve their business problem–SQL statement pass-through is this.</p>

<p>I’ve worked at companies that have an HTML Textarea on the dashboard of the internal portal where you can hand type SQL statements, or use from a pre-configured set of statements. Allowing business users to quickly query a database and dump to spreadsheet, CSV, and import into other applications. I can see SQL pass-through being a quick and dirty solution that reflects these other approaches I’ve seen in the past. I could see bookmarks, quick links, and other scrappy ways of using the web to query backend databases like this. When you couple this with some sort of API key or other identifier, you can also begin to develop an awareness of who is making these types of queries, and what types of applications they are putting them to use in. Taking SQL query pass-through to the next level and going beyond just API deployment, and moving into the realms of API management.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/30/sql-statement-pass-through-using-web-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/30/the-average-person-will-never-care-about-apis-until-it-does-something-meaningful/">The Average Person Will Never Care About APIs Until It Does Something Meaningful</a></h3>
        <span class="post-date">30 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/17_88_800_500_0_max_0_-5_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am always looking for ways to introduce people to the concept of APIs, and that they are right below everything digital you do in your daily life. Even with my prolific writing, and sharing on social media, the number of new converts to API awareness are relatively low. I’m alright with what I do not scaling. I’m in this for the long haul, not to sell products or services. I’m looking to help turn on the API light for people not because I want them building the next API, I want to help enlighten folks so that they can take more control over their digital presence, and push back on the platforms and algorithms that are increasingly dominating our lives.</p>

<p>One thing I’ve learned about normal folks in my journey as the API Evangelist is that nobody will ever care about APIs until they do something meaningful in their lives. Technologists learn about APIs for other reasons, but normal people aren’t motivated in the same ways, and need to have some meaning before they’ll wade into this more technical world of unknown, unknowns. When talking to technologists about APIs I focus on the API lifecycle, and the agility that APIs bring. With normal folks I tend to focus on platforms they already use, and algorithms that directly impact their lives, or impact people they know. As an API storyteller it is important for me to develop meaningful stories, that make APIs accessible in everyday scenarios to average people I encounter.</p>

<p>If someone is a photographer I will tell stories of the Flickr or Instagram API. If someone is an accountant, I will work through how the Intuit API is rapidly being used by small businesses. If someone is a genealogist I will talk about how the Family Search API drives Ancestry.com. If someone is a music professional I will focus on Spotify, or maybe the Bandcamp API. This is why I play with as many APIs as I can, so that I’m familiar with them, and can tell meaningful stories around the impact they make (good or bad). I tell these stories, so that I can share them with average people who may not be aware that APIs exist right below the surface of their world. Once I show them in a meaningful way, they’ll almost always continue their journey on their own, poking, scratching, and learning about what APIs can do in their world.</p>

<p>Technologists often take for granted why people understand APIs. They don’t think about the why of it. You either are a technologist and know, or you aren’t in this class of tech wizards and have no business knowing. I do not see the world like that. I’m regularly thinking about how I can produce new converts, and open up the average person’s eyes to what APIs are. I do not believe in a technologist class, and that some people should know this, and others should not. I believe that EVERYONE should be aware of how their data, and other bits and bytes are moving around beneath the websites they use, the mobile applications they depend on, and the devices they are putting into their homes, automobiles, and other aspects of our lives. Even with this belief, I fully understand I will never convert normal folks into being an API aware individual until they do something meaningful in their life.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/30/the-average-person-will-never-care-about-apis-until-it-does-something-meaningful/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/30/licensing-over-dc-transit-data/">Troubling Terms of Service Changes From Washington Metropolitan Area Transit Authority (WMATA) Data APIs</a></h3>
        <span class="post-date">30 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/transit/wmata-transit-terms-of-service.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://technical.ly/dc/2017/11/22/developers-upset-wmatas-new-data-terms-use/">I was turned onto a developing problem within the Washington Metropolitan Area Transit Authority (WMATA) around a recent terms of service change made around the transit data API by Technically DC</a>. While the transit authority is saying the changes are business as usual and make sense for the platform, some of the developers, specifically one of the biggest API users <a href="https://dcmetrohero.com/">MetroHero</a> says the changes are targeting them specifically.</p>

<p>MetroHero presented what they feel are the unreasonable changes to the WMATA API terms of service in a WMATA Board Meeting recently, focusing on four main areas:</p>

<ul>
  <li>That no user or developer can mention “WMATA” in press releases without letting WMATA first review it.</li>
  <li>That WMATA can gain access to any user’s applications that use the data, can audit personnel information for anyone working on those applications, and WMATA can also create their own version at any time.</li>
  <li>That WMATA forbids users from claiming their data is accurate, complete or timely, or claiming it is more so than WMATA’s data.</li>
  <li>That the transit agency may now charge users in the future for using their data.</li>
</ul>

<p>These are all common changes I’ve seen made to API terms of service before, and are usually signs that a platform operator that is pretty out of touch with what it is like to be an API consumer, and with their own API community. It is a sign of a broken or pernicious feedback loop which leads to API providers making decisions that do lasting damage to their communities like this. These types of changes <a href="https://www.wired.com/2012/09/twitters-new-rules-of-the-road-means-some-apps-are-roadkill/">reflects the “rules of road” terms of service changes Twitter made to back in 2012</a>. Which didn’t fully kill off the Twitter API, but set such a bad tone in the community, the company is still working to dig out of it five years later. I know platform operators feel they need to assert this level of control, but in an API community you need to learn to let go a little, communicate, and work with your community, not against them.</p>

<p>I’m going to work through all of these bullet points as separate stories, and try to help other API providers, as well as the WMATA understand how they might be able to handle these types of requests better. Doing it in a way that doesn’t cause irreparable damage to their communities, and still achieves their objectives. However, I will emphasize that I think if WMATA spent more time actually communicating with their developers, especially the leading ones like MetroHero, you’d probably see how ridiculous your requests area in the first place. APIs are not just about you opening up access to your data, it is about opening up access to your processes, and feedback loops, and collaborating with your community. It isn’t about command and control, and broadcasting the rules of the road for you platform–Twitter showed us this does not work well in todays environment.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/30/licensing-over-dc-transit-data/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/29/sorry-your-api-effort-falls-a-little-short-of-the-apis-I-cover/">Sorry Your API Effort Falls A Little Short For The APIs I Cover</a></h3>
        <span class="post-date">29 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/supreme-court-judgement.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I get a lot of emails from companies asking me to look at their APIs. Too many for a one person operation like me to consider. I have to be picky about the APIs I’m taking a look at, and over time I’ve developed a set of criteria for determining how much energy I will invest in an API. Usually within about 2-3 minutes I can tell if it is an API I will be diving in deeper, or I will just be walking away and moving on with my work.</p>

<p>The first thing that turns me off of an API is that it just isn’t interesting. I’ll land on the page and I can tell what it does, but it just doesn’t interest me. It doesn’t offer any value, or it is in a category that I’m just not eager to be thinking about and showcasing in my work. If an API doesn’t deliver value, and stand out as being interesting beyond the hundreds of other APIs I see each week, I’m just not going to stop and take notice. Sorry, it might be to others–don’t just take my opinion.</p>

<p>The next thing that keeps me from going deeper is I can’t tell what an API does. I’m always amazed at how much head scratching, clicking and reading I will do before I ever figure out what an API does. I’m pretty hard headed, so sometimes its me, but other times I’m just stuck at figuring out what is going on under the hood. Usually after about 3-5 minutes of struggling to understand what is happening, I will just walk away. It is unlikely that other folks will be investing more time than that, and the API will not last long in my experience.</p>

<p>After that, the biggest crime I see companies and organizations make is that they just do not invest enough into a dedicated portal, and the other supporting resources for their API. If someone sends me a link to their API and it is in the help or knowledge base section of their website, I know that they don’t really care about it, and won’t be investing much more into it. APIs shouldn’t be a side project for companies in 2017, they should be front and center, in their own dedicated portal, with a prominent link off the website navigation.</p>

<p>I try to always respond to emails I get from folks letting them know their API efforts fall short of what I’m expecting to see. I feel bad raining on their parade, but the bar is pretty high in 2017. Your API needs to stand out, deliver value, and be something you are investing in. Maybe my response will light the fire under your API operations, and at least get you reading my blog some more, and learning about what other API providers are doing. Then you can take some of what you’ve learned back to your organization and get to work building a first class API operation.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/29/sorry-your-api-effort-falls-a-little-short-of-the-apis-I-cover/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/29/asyncapi-is-a-specification-format-for-message-driven-apis/">AsyncAPI Is A Specification Format For Message-Driven APIs</a></h3>
        <span class="post-date">29 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/asyncapi/asyncapi-editor-sample.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been learning about a new API definition format called AsyncAPI that allows you to define message-driven APIs in a machine-readable format. It is protocol-agnostic, which means you can use it for APIs that work over MQTT, AMQP, WebSockets, STOMP, and other real-time, and Internet of Things focused APIs. The specification format mirrors OpenAPI, making it pretty easy to get up to speed understanding what is going on.</p>

<p>There are two primary concepts at play with the AsyncAPI:</p>

<ol>
  <li>Messages - Consumer(s) communicate with your API via messages. A message is a piece of information two or more programs exchange. Most of the times to notify the other end(s) that, either an event has occurred or you want to trigger a command. Technically speaking the events and actions will always be sent in the same way. These are just messages, and their content can be anything. So when we talk about the difference between events and actions, this is only a semantic differentiation of message’s content. We do not enforce you to make any difference between them, although we encourage you to do it. A message can contain headers and a payload. However, both are optional. The specification allows you to define any header, to remain as much protocol-agnostic as possible.</li>
  <li>Topics -  Message-driven protocols usually contain something called topic (MQTT), routing key (AMQP), destination (STOMP), etc. To some extent, they can compare to URLs in HTTP APIs. So, when you send a message to your API, it will be routed depending on the topic you published on. This feature allows you to create APIs that subscribe to specific topics and publish to other ones.
There’s no standard way of naming topics, so we recommend you to have a look at our proposal here.</li>
</ol>

<p>I don’t have any APIs I can apply AsyncAPI to, so I have to just learn from the examples and any other work I come across. It makes me happy to see folks developing API specifications like this, going beyond what OpenAPI is doing, but also keeping so closely in alignment with the existing work out of the OAI. I’m always hearing folks say that the OpenAPI specification doesn’t do what they want it to do, yet they don’t invest in vendor extensions, or even augment the work that is going on with a complimentary set of specifications. Good to see people just make it happen!</p>

<p>I’m adding AsyncAPI to <a href="http://definitions.apievangelist.com/">my API definition research</a> so I can keep in tune with where it goes. I’m talking with some folks regarding how it should viewed by the OAI. In my opinion, the OAI is going to have to begin considering how it will embrace specs that go beyond what it can do, as well as <a href="http://apievangelist.com/2017/09/25/considering-the-future-of-the-openapi-initiative/">begin to adopt industry specific OAI implementations that may require acknwoledging some vendor extensions that may never get brought into the core specification</a>. Anyways, it’s good to see movement in this area. Nice work <a href="https://twitter.com/fmvilas">Fran</a>, <a href="https://twitter.com/bpedro">Bruno</a>, and <a href="https://twitter.com/PermittedSoc">Mike</a>–you guys are rocking it.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/29/asyncapi-is-a-specification-format-for-message-driven-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/29/api-deployment-is-about-publishing-them-wherever-they-are-needed/">API Deployment Is About Publishing Them Wherever They Are Needed</a></h3>
        <span class="post-date">29 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/slashdb/slashdb-database-deployment.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p>I spun out a separate research area for <a href="http://deployment.apievangelist.com">API deployment</a>, from my core <a href="http://deployment.apievangelist.com">API management research</a> back in 2012 when companies were regularly asking me which of the API management providers they should be using to publish new APIs. At the time, none of them would help you actually publish your APIs, and there just wasn’t enough conversations going on around the subject. When I give talks which include my section on API deployment, some people still scratch their heads thinking there really isn’t that many options on the table–they deploy APIs wherever they’ve been deploying their APIs. However, in a cloud-driven world, the opportunities for how and where we can deploy our APIs are increasing, and the savvy teams are getting more versatile in how they get things done.</p>

<p>Supporting multi-cloud is something all API service providers should be supporting. <a href="https://www.slashdb.com/pricing/">I was reviewing the approach to pricing from my friends and partners over at SlashDB</a>, and I noticed as part of their pricing tier that they have “deployment” as one of the options. Allowing for deployment of their database to API solution on Debian, RedHat, VMWare, VirtualBox, Docker, Vagrant, Amazon, Azure, as well as custom solutions at the enterprise tiers. Focusing on the needs of a diverse range of enterprise customers, while also paying attention to where the API deployment conversation has been shifting for some time with Amazon, Docker, and the other platforms that are dominating the IT landscape.</p>

<p>API service providers should be supporting multiple cloud platforms like SlashDB does, but API providers should also be looking at their own API deployment in the context of multi-cloud as well. You may have your primary way of doing APIs now, but I’m guessing that once you begin doing APIs at scale, your approaches to deploying APIs will begin to shift. When I talk with companies, organizations, institutions, and government agencies about their API deploy practices, it is increasingly common to see different groups using different platforms, as well as multiple API gateways in operation. This is usually not due to some grand plan in place, and has happened in a more organic, and often disorganized way. Sometimes there are good reasons for using different platforms, services, and tools, and other times there is not–it is up to your API governance, and leadership teams to decide which is which.</p>

<p>API deployment should be about publishing APIs wherever you need them. It could because different teams prefer different platforms, tools, and services, or maybe it is a project or partner requirement that you deploy an API somewhere out of the norm. Regardless of the reasons the most seasoned teams I come across are able to roll with the punches, deploy their APIs where they need them, while still keeping in sync with overall API governance and standards practices. In my opinion, both API providers, as well as the API service providers should be multi-platform, and multi-cloud prepared. You may not be fluent, but be ready for the possibility that at some point you may need to get out of your comfort zone. Even if you aren’t actively playing with alternate platforms, services, and tools, I recommend reading and staying in tune with other approaches.</p>

<p>I’ve been pretty content with my hand-crafted approach to deploying APIs using Linux, Apache, MySQL, and Slim PHP API framework. It’s standardized, clean, and something that many of my clients can support. However, I’m rapidly shifting my approach to support AWS API Gateway, as well as beginning to play with different flavors my APIs that are deployable on Google and Azure. I’m looking to keep my toolbox focused, with my primary ways of reliably deploying APIs in as little time as possible. However, I’m fully aware that API deployment has become about being able to publish them wherever they are needed, whether it is one of my clients requesting it, or maybe it is just because I’m looking to deploy a specific API prototype, and tell a specific story on a platform I may not be 100% fluent in–pushing my API deployment skills beyond where they are today.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/29/api-deployment-is-about-publishing-them-wherever-they-are-needed/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/28/making-your-api-pricing-page-accessible-to-everyone/">Making Your API Pricing Page Accessible To Everyone</a></h3>
        <span class="post-date">28 Nov 2017</span>
        <p>I’ve been talking with <a href="https://bitscoop.com/">the folks over at Bitscoop about their integration platform as a service (iPaaS) offering</a>. I would API mapping as a service, but that is another story. After talking with them, and going through their website, I wanted to focus on <a href="https://bitscoop.com/pricing/">Bitscoop’s pricing page</a>, which I feel reflects where API service pricing and plans are headed. There are three main areas of their pricing that I think are worth highlighting for accessing APIs at scale.</p>

<p>Bitscoop is really priced for EVERYONE, with a simple free tier to get started using the platform.</p>

<p align="center"><a href="https://bitscoop.com/pricing/"><img src="https://s3.amazonaws.com/kinlane-productions/bitscoop/bitscoop-pricing-free.png" align="center" width="40%" /></a></p>

<p>Next there are three tiers of access: developer, organization, and enterprise. It’s not as “ascendable” as I’d like it (smoother hop from tier to tier), but because Bitscoop clearly articulates how much additional calls are for each tier, the jump from tier to tier isn’t as painful.</p>

<p align="center"><a href="https://bitscoop.com/pricing/"><img src="https://s3.amazonaws.com/kinlane-productions/bitscoop/bitscoop-pricing-tiers.png" align="center" width="75%" /></a></p>

<p>Closing out the Bitscoop pricing page they have a custom solutions section letting you know they’ll deploy your API service to Google, Amazon, or Azure. Reflecting where API deployment, and API service deployment is headed.</p>

<p align="center"><a href="https://bitscoop.com/pricing/"><img src="https://s3.amazonaws.com/kinlane-productions/bitscoop/bitscoop-pricing-custom.png" align="center" width="75%" /></a></p>

<p>Thats it. That is the story. Make your services accessible. Don’t price people out. Make your solutions available to everyone, with the opportunity to grow. I’m always fascinated by how many differing opinions there are out there regarding how you craft your SaaS and API plans. I think Bitscoop pricing reflects the reality of when you are integrating with hundreds or thousands of APIs. To be able to compete at this scale you are going to have to be plug and play with your tech, as well as the business of your APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/28/making-your-api-pricing-page-accessible-to-everyone/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/28/the-openapi-driven-mock-api-server-from-stripe/">The OpenAPI-Powered Mock API Server From Stripe</a></h3>
        <span class="post-date">28 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/stripe/stripe-mock-api-server.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I showcased Stripe’s OpenAPI definition the other week, so I wanted to also highlight a side effect of Stripe deciding to be OpenAPI-Driven. <a href="https://github.com/stripe/stripe-mock#development">Stripe recently published an OpenAPI-powered mock server</a>, allowing Stripe API consumers to test drive, and play with the Stripe API in a simulated environment. “It operates statelessly (i.e. it won’t remember new resources that are created with it) and responds with sample data that’s generated using a similar scheme to the one found in the API reference.”</p>

<p>The Stripe Mock Server is written in Go, and <a href="https://github.com/stripe/stripe-mock">is available on Github</a>. You can rebuild the Stripe API mock server from an updated OpenAPI anytime. It is a pretty dead simple mock server that seems like should be standard practice for any API. Providing a simple, safe, and portable way to play with an API. I’m going to fork the Stripe Mock API and play with it, see what is possible with the tool.</p>

<p>I will be keeping an eye out for any other OpenAPI-powered tools out of Stripe, now they are actively working with it. Adoption of OpenAPI at this level of API provider is helpful to the rest of the community, by providing an example of how you can bake OpenAPI into your operations, but also the open source tooling these companies produce. It’s an important community effect that makes this whole API thing work so well.</p>

<p>Ideally, the leading API providers, with the most resources, could coordinate their efforts and deliver a suite of open source tooling. However, I’m patient, I’m just happy that big companies like Stripe, Slack, Box, New York Times are doing OpenAPI at all. I can wait for all the cool tooling to happen next. I’ll keep an eye on Stripe’s Github organization to see what pops up.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/28/the-openapi-driven-mock-api-server-from-stripe/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/28/getting-a-handle-on-our-database-schema-using-apis/">Getting A Handle On Our Database Schema Using APIs</a></h3>
        <span class="post-date">28 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/server-cloud1_internet_numbers.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p>When I take money from my partners, I am always looking for characteristics in their products and services that allow me to write honest stories about the solutions they provide. I can’t do this for all API companies that approach me, but the ones that are doing useful things, make it pretty easy for me. <a href="https://www.slashdb.com/">SlashDB</a> helps me out on this front because they aren’t the shiny new startup doing APIs–they are the real world business helping other companies, organizations, institutions, and government agencies get a handle on their databases using APIs. One huge benefit of this process in my opinion is how it helps us get a handle on the schema we use, by letting a little light in on the process.</p>

<p>One of the main reasons our databases are such a mess is because they are hidden away behind a dark technical or organizational curtain, and there really isn’t much accountability regarding how we define, name, organize, and store our data. Of course there are exceptions to this, but a messy, bloated, unwieldy database is a hallmark of about 75% of the organizations I’ve worked with over my 30 year career. Central databases are often a mashup of years, even decades of creating databases, tables, and adding columns, often times occurring over generations of database teams. The result is often an incoherent mess regarding how things are named, with layers of cryptic field names, and irrelevant table names, which might seem normal until you go and try to expose these data resources to 3rd party and partner developers.</p>

<p>Many of the data APIs I come across in my research lack any API design investment. Meaning they didn’t take any consideration when it came to exposing backend databases as coherent paths, parameters, and other elements. Many API providers just spit out the database as a web API, and called it good enough. This can be very frustrating for many Restafarians, and API designers. I agree, and I would love to see more efforts from API providers when it comes to making their APIs more intuitive, and doing the hard work of understand what resources they have, and how to best present their resources to their consumers. However, I feel like just exposing your database as endpoints can be an important first step in the API journey, and one that isn’t always 100% dialed in on day one–that is ok. Just publishing APIs, even if they reflect exactly the table and fields structures behind, is still an important first step for many companies. Not everybody is API design ready, and having APIs can prove to be more important than good design practices.</p>

<p>As I was looking through SlashDB’s site looking for potential story ideas, I thought their approach to exposing database and tables as paths, and helping take the first step of evolving any database towards being an API was worth telling a story about. I know this is the stuff that drives API obsessed folks crazy, and feel I shouldn’t be encouraging people, but I think it is more important that folks are doing APIs, and have embarked on their API journey, over doing things perfectly. API providers like SlashDB aren’t the bleeding edge of API design technology, they are the industrial grade API deployment solutions folks need to go from database to API. So go ahead and publish APIs that look exactly like your database structure. I’m not going to shame you. I think letting the sunlight in a bit is way more healthier than waiting until you have the perfect design, or worse, never doing it at all.</p>

<p>Tools like SlashDB allow us to begin the long process of unwinding our legacy database schema, and start being more consistent in the vocabulary we use. Even though the first version might not be as coherent, and plain language as we’d like, publishing a web API from your backend database like SlashDB provides, at least gets things out on the workbench–allowing you to begin having a conversation with external partners about what the future of your schema should look like. You are never going to learn API design by keeping everything behind closed doors, and even though you are going to have to support your first version out of the box for a significant amount of time–at least you are pushing your schema forward, making it more usable by external partners, and (hopefully) open to discussions about why your database schema might not work 100% at the moment.</p>

<p>Database to API is something ALL companies, organizations, institutions, and government agencies should be doing in 2017. ALL your databases should have web APIs available, even if you are still using ODBC/JDBC and other connectivity options. If you have the time and resources to inject some healthy API design practices into the mix you should, however don’t let it hold back your API deployments if you can’t. You should be eliminating any obstacles between your backend databases and the applications that need access to this data. Even if you did have the time to think through your API design, there is good chance you will need to shift the design of your API down the road based upon the feedback of consumers. So, just get your APIs published today, and begin the hard work of getting a handle on your database schema–it is too important to put off until you have everything just right.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/28/getting-a-handle-on-our-database-schema-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/27/multi-region-apis-using-aws-api-gateway/">Multi-Region APIs Using AWS API Gateway</a></h3>
        <span class="post-date">27 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/amazon-api-gateway-regions.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been deploying two project using AWS API Gateway, Lambda, and Amazon RDS lately. I’ve become so sold on this approach to deploying APIs as part of this work, that I am evolving my own internal API process to use the same approach. The technical aspect of serverless plus the gateway definitely convinced me of the potential, but it was also the usage of AWS IAM which sealed the deal for me. I’m all too aware of how much my API security lacks as a one person shop, something that I also see reflected in my client operations, and I’d rather be offloading security to AWS than ending up taking the hit on it down the road.</p>

<p>While deploying my project using AWS API Gateway, and Lambda, I was faced with the question regarding which zone I should be deploying the APIs in. It is the first time I’ve been faced with the opportunity to deploy my APIs into multiple zones. Sure, I could have deployed my servers into any AWS zone before, but for some reason now that I’m doing with AWS API Gateway, and Lambda, the opportunity seemed more of a possibility. I’ve pitched it to my client to consider an east as well as a west coast API deployment, so that we can give developers the choice in the documentation to choose which availability zone they’d like to use in their application. Before I make the proposal I’m going to deploy some prototypes, and do some benchmark testing, and see what the benefits are.</p>

<p>Even if I end up publishing APIs into separate regions, I still have the backend database to content with. Where do I put the database, and how to I replicate between zones. Amazon RDS gives me the tools to tackle this, but historically I would only do this just for backup, not for actual redundancy, as well as performance gains. Amazon zones have been a staple of the cloud since early days, but I still have many clients who only operate in the east coast region. I’m thinking I will begin to push for a multi-region approach, and see if I can’t convince some of them to start thinking bigger. Getting to know where their customers are, and delivering infrastructure closer where the resources are needed.</p>

<p>I’m rolling out some new APIs as part of my own infrastructure. Most of my APIs are retail, as well as wholesale APIs. Meaning you can use the APIs I’ve published, or I’ll deploy them specifically for you, in your own AWS account. I think I’m going to make east / west an option for my retail APIs, and ALL the AWS regions an option for the wholesale APIs. I’m seeing geographical region as another consideration when I’m thinking about how I should be breaking down my APIs, into smaller more bite-size chunks. I see geographical region as a variable in the host for my APIs, just like I’d add any other variable in the path. Opening up a whole new set of possibilities when it comes to not just API deployment, but also API reliability and performance.</p>

<p>As I continue to drink the AWS Kool-Aid I’m questioning my increased dependence on the platform. However, it gets harder to deny when they bring security to the table, and increased performance, availability, and reliability to my API stack, and the APIs I’m delivering for my clients. There is no way I could afford to do APIs at scale, across multiple regions without AWS. I think back to the day when I would secure my own T1s, colo-facility, and servers. Even with the threat of vendor lock-in, I do not want to ever go back to that world. I enjoy the benefits of AWS. I’m just going to try and keep things as well defined as simple, microservice APIs, so that I can migrate to Google, Azure, or if necessary my own infrastructure. As much as I’d like to reduce my dependencies on major cloud providers, in the current online environment I just can’t. Being able to operate in any region around the globe is a pretty significant benefit to doing business online.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/27/multi-region-apis-using-aws-api-gateway/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/27/hints-of-banking-api-regulations-from-cfpb-with-consumer-authorized-financial-data-sharing-and-aggregation-rules/">Hints of Banking API Regulations From CFPB With Consumer Authorized Financial Data Sharing And Aggregation Rules</a></h3>
        <span class="post-date">27 Nov 2017</span>
        <p><a href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-outlines-principles-consumer-authorized-financial-data-sharing-and-aggregation/"><img src="https://s3.amazonaws.com/kinlane-productions/cfpb/cfpb-outlines-principles-for-consumer-authorized-financial-data-sharing-and-aggregation.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p><a href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-outlines-principles-consumer-authorized-financial-data-sharing-and-aggregation/">The Consumer Finance Protection Bureau (CFPB) has started laying out some consumer-authorized data sharing and aggregation rules to begin moving forward the banking data scraping conversation in (hopefully) a more production way</a>. It is common knowledge that many financial focused (Fintech) companies regularly access consumers account data using their credentials, so that they scrape relevant account information from their bank, for use in a wide variety of 3rd party tools. This is a common practice that everyone in the industry knows about, understands is a potential security and privacy risk, but everyone looks the other way because it adds value to the consumer ecosystem.</p>

<p>In a perfect world each bank would have a public API portal where Fintech aggregators could come and sign up for application keys, and get the authorization of users via OAuth, and obtain access to their banking data in a secure, and accountable way. However, as we are well aware, we do not live in a perfect world, and banks are pretty resistant to change, so the scraping continues. At some point we are going to see the landscape begin to shift, and I’m guessing it will be at the regulatory level where we finally begin to see this behavior changed–making the CFPB’s rules announcement a reflection of what is coming down the pipes when it comes to banking API regulation.</p>

<p><a href="http://files.consumerfinance.gov/f/documents/cfpb_consumer-protection-principles_data-aggregation.pdf">The consumer protection principles for consumer-authorized financial data sharing and aggregation</a> announcement focuses on:</p>

<ul>
  <li><strong>Access</strong> - Consumers are able, upon request, to obtain information about their ownership or use of a financial product or service from their product or service provider. Such information is made available in a timely manner. Consumers are generally able to authorize trusted third parties to obtain such information from account providers to use on behalf of consumers, for consumer benefit, and in a safe manner. Financial account agreements and terms support safe, consumer-authorized access, promote consumer interests, and do not seek to deter consumers from accessing or granting access to their account information. Access does not require consumers to share
their account credentials with third parties.</li>
  <li><strong>Data Scope and Usability</strong> - Financial data subject to consumer and consumer-authorized access may include any transaction, series of transactions, or other aspect of consumer usage; the terms of any account, such as a fee schedule; realized consumer costs, such as fees or interest paid;
and realized consumer benefits, such as interest earned or rewards. Information is made available in forms that are readily usable by consumers and consumer-authorized third parties. Third parties with authorized access only access the data necessary to provide the product(s) or service(s) selected by the consumer and only maintain such data as long as necessary.</li>
  <li><strong>Control and Informed Consent</strong> - Consumers can enhance their financial lives when they control information regarding their accounts or use of financial services. Authorized terms of access, storage, use, and disposal are fully and effectively disclosed to the consumer, understood by the consumer, not overly broad, and consistent with the consumer’s reasonable expectations in light of the product(s) or service(s) selected by the consumer. Terms of data access include access frequency, data scope, and retention period. Consumers are not coerced into granting third-party access. Consumers understand data sharing revocation terms and can readily and simply revoke authorizations to access, use, or store data. Revocations are implemented by providers in a timely and effective manner, and at the discretion of the consumer, provide for third parties to delete personally identifiable information.</li>
  <li><strong>Authorizing Payments</strong> - Authorized data access, in and of itself, is not payment authorization. Product or service providers that access information and initiate payments obtain separate and distinct consumer authorizations for these separate activities. Providers that access information and initiate payments may reasonably require consumers to supply both forms of authorization to obtain services.</li>
  <li><strong>Security</strong> - Consumer data are accessed, stored, used, and distributed securely. Consumer data are maintained in a manner and in formats that deter and protect against security breaches and prevent harm to consumers. Access credentials are similarly secured. All parties that access, store, transmit, or dispose of data use strong protections and effective processes to mitigate the risks of, detect, promptly respond to, and resolve and remedy data breaches, transmission errors, unauthorized access, and fraud, and transmit data only to third parties that also have such protections and processes. Security practices adapt effectively to new threats.</li>
  <li><strong>Access Transparency</strong>  - Consumers are informed of, or can readily ascertain, which third parties that they have authorized are accessing or using information regarding the consumers’ accounts or other consumer use of financial services. The identity and security of each such party, the data they access, their use of such data, and the frequency at which they access the data is reasonably ascertainable to the consumer throughout the period that the data are accessed, used, or stored.</li>
  <li><strong>Accuracy</strong> - Consumers can expect the data they access or authorize others to access or use to be
accurate and current. Consumers have reasonable means to dispute and resolve data inaccuracies, regardless of how or where inaccuracies arise.</li>
  <li><strong>Ability to Dispute and Resolve Unauthorized Access</strong> - Consumers have reasonable and practical means to dispute and resolve instances of unauthorized access and data sharing, unauthorized payments conducted in connection with or as a result of either authorized or unauthorized data sharing access, and failures to comply with other obligations, including the terms of consumer authorizations. Consumers are not required to identify the party or parties who gained or enabled
unauthorized access to receive appropriate remediation. Parties responsible for unauthorized access are held accountable for the consequences of such access.</li>
  <li><strong>Efficient and Effective Accountability Mechanisms</strong> -  The goals and incentives of parties that grant access to, access, use, store, redistribute, and dispose of consumer data align to enable safe consumer access and deter misuse. Commercial participants are accountable for the risks, harms, and costs they introduce to consumers. Commercial participants are likewise incentivized and empowered effectively to prevent, detect, and resolve unauthorized access and data sharing, unauthorized payments conducted in connection with or as a result of either authorized or unauthorized data sharing access, data inaccuracies, insecurity of data, and failures to comply with other obligations, including the terms of consumer authorizations.</li>
</ul>

<p>Smells like a PSD2-esque set of API standards are on the horizon for the U.S. Ideally this is something the banks would see as an opportunity, rather than a regulatory thing, but I understand how hard-headed they are. I’m spending some time over the next month or two getting up to speed more on where we stand with the PSD2 rollout, as well as the GDPR rollout in the EU. Both of these efforts provide us with a blueprint to follow here in the US. Obviously it is a much different regulatory and banking environment here, but there are still plenty of lessons to consider, and think about as agencies like the CFPB get to work on this topic.</p>

<p>All nine aspects of this latest announcement from the CFPB reflect what APIs are all about. We have the blueprint for tackling this problem head on in use across the tech sector already. This isn’t a technology problem, this is a business and politics problem. It would make sense for a savvy bank (cough, cough Capital One) to get ahead of this one and be the Amazon Web Services of the banking space and set the standard for how data aggregation and sharing occurs. Define the open blueprint for how consumer data is accessed and put to work in the banking ecosystem, gain teh competitive advantage when it comes to Fintech tooling servicing the space, and make all the other banks play catch up. As usual, I’ll keep an eye on what the banks are up to (not much), and look out for more movement from the federal government on this issue, and report back anything I find.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/27/hints-of-banking-api-regulations-from-cfpb-with-consumer-authorized-financial-data-sharing-and-aggregation-rules/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/27/making-sure-you-operate-in-the-cloud-marketplaces-as-an-api-service-provider/">Making Sure You Operate In The Cloud Marketplaces As An API Service Provider</a></h3>
        <span class="post-date">27 Nov 2017</span>
        <p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/slashdb/slashdb-automatic-rest-api-for-databases-in-aws-marketplaces.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>As the cloud giants like AWS, Microsoft, and Google continue to assert their dominance of the digital world, one aspect of their operations I’m watching closely has to do with their marketplaces. Google’s marketplaces are still very Android focused, but Amazon and Microsoft have shifted their recent editions of their marketplaces to be more cloud oriented, and accommodating a wide variety of applications, machine learning models, as well as APIs and API-focused services. While these marketplaces are still growing, and asserting their role in the digital economy, they are something I advise API providers, and service providers to be keeping a close eye on, and begin considering how they will want to operate within these environments.</p>

<p>If you are an API service provider, and you are selling services to API providers anywhere along the API lifecycle, I recommend you follow the example of friends over at SlashDB, who have their database to API offerings in two of the leading marketplaces:</p>

<ul>
  <li><a href="https://aws.amazon.com/marketplace/pp/B01MU8W71L"><strong>AWS</strong></a> - Automatically constructing a REST API to databases for reading and writing on the AWS platform.</li>
  <li><a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps/vte.slashdb"><strong>Azure Marketplace</strong></a> - SlashDB enables you to do more with traditional databases and Microsoft Azure.</li>
</ul>

<p>As more companies, organizations, institutions, and government agencies move their databases into the cloud, SlashDB sees the opportunity to help them quickly turn databases and tables into web interfaces for querying data. Having your API service ready to go, in the environments where your potential customers are already operating is how much of this API stuff will go down in the future. Amazon has set the stage for how we’ll be delivering IT infrastructure over the last decade with the introduction of the cloud, and Google and Microsoft are quickly playing catch up. The savvy API service providers understand their role in this cloud evolution and make sure their services are available as retail solutions, but also as plug and play wholesale solutions in these cloud marketplaces.</p>

<p>SlashDB is clearly serving the deployment and management aspects of the API lifecycle, but I’m tracking on virtualization, testing, monitoring, security, and other aspects of doing business with APIs who are also deploying using these cloud marketplaces. I’m also seeing an uptick in the growth of machine learning models being made available via AWS, Azure, and Google, demonstrating that the algorithmic evolution of the API sector will occur in these environments. The algorithmic wave of APIs is just getting started, but publishing APIs from your databases on the leading cloud platforms is standard operating procedure for businesses of all shapes and sizes in 2017. Are your API services available in the AWS or Azure marketplaces?</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/27/making-sure-you-operate-in-the-cloud-marketplaces-as-an-api-service-provider/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/21/the-defensive-database-administrator-and-the-eager-blockchain-believer/">The Defensive Database Administrator And The Eager Blockchain Believer</a></h3>
        <span class="post-date">21 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/adam-smith_dali_three.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Think about the power that database administrators have in your organizations world? I’ve been working with databases since my first job in 1987. I’ve seen the power bestowed upon database administrators in organization after organization. They are fully aware of the power they control, and most other people in an organization are regularly reminded of this power. The defensive database administrator is always the biggest obstacle in the way of API teams who are often seen as a threat to the power and budgets that database groups command. This power is why databases are often centralized, scaled vertically, and are the backends to so many web, mobile, desktop, and server applications.</p>

<p>I spend a significant amount time thinking about the power that database administrators wield, and how we can work to find more constructive, secure, and sensible approaches to shifting legacy database behaviors. Lately, I also find myself thinking a lot more about Blockchain. Not because I’m a believer, but because so many believers are pushing it onto my radar. Blockchain will continue to be a thing, not because it is a thing, but because so many people believe it is a thing. Most blockchains will not withstand the test of time, they are vapor, but the blockchains that remain will because people have convinced other people to put something meaningful into their blockchain. Much like we have convinced so many companies, organizations, institutions, and government agencies to put data into databases. Yes we. I’m complicit.</p>

<p>A definition of the blockchain is, “a continuously growing list of records, called blocks, which are linked and secured using cryptography”. It’s a database, linked and secured using cryptography. The reason you hear about the blockchain so much, and how it can revolutionize almost every business sector, is the blockchain believers want to convince you to put your digital assets into their blockchain, which will eventually make it something real. I can setup a blockchain today, call it anything I want, but it is nothing more than an empty distributed database. It doesn’t become anything until there is something of value stored in it, which is why there are so many eager folks right now trying to convince that blockchain is something, so you’ll put your valuable things in there, and it will become something.</p>

<p>Think of blockchain believers as the frontend version of the defensive database administrator. After a blockchain has been up for 20 years, and has a bunch of valuable things stored in it, the blockchain believers will become more like the database administrators. They’ll grow beards (even the women), and become more defensive of their precious data stores from whatever the next threat to their power is, and do whatever it takes to defend their power. Blockchain believers are young energetic, and looking to build their empires, and database administrators are usually older and motivated to defend their empires. When you are down in the trenches trapped within the tractor beam of a database it is hard to see beyond it. When you are basking in glow of Internet technology, and everything is new and exciting, it can also be hard to see beyond it. With everything, give it 20 years, and things often times become whatever they’ve replaced.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/21/the-defensive-database-administrator-and-the-eager-blockchain-believer/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/21/when-you-believe-everything-in-tech-is-new-and-nothing-repeats-itself/">When You Believe Everything In Tech Is New And Nothing Repeats Itself</a></h3>
        <span class="post-date">21 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/beach-rocks-currents_blue_circuit_4.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I get regular waves of commenters and tweeters who like to point out the API patterns I’m covering in the API space, have all been done before. We tried discovery docs before they are called WSDL! That API discovery thing is called UDDI! RPC is nothing new! That isn’t new. We tried that before, and it didn’t work. I rarely ever engage with these folks, as this behavior is one pattern in behavior I actually do believe we SHOULDN’t be repeating and showcasing.</p>

<p>I’m fascinated by the reasons someone would feel so strongly they need to respond. That something happened in the past, and because it didn’t work we shouldn’t try again today. That somehow the world of compute isn’t built upon, and remixed upon previous ideas that worked, and many that didn’t work until just the right conditions existed. This kind of behavior is really fascinating for me in the world of APIs where reuse, aggregation, facades, and so many patterns of reworking what already exists is core to the entire concept. Where do folks get such strange believes in the past, and what can and cannot be re-interpreted in the future?</p>

<p>Hey you, electric car manufacturers, the electric car was done in early 20th century and it didn’t work! Hey musician, that baseline was originally present in the big band era and didn’t go over well, it won’t work now! Those pants were first tried in the 1950s and were a flop. Someone already wrote a book on Abraham Lincoln, why would you want to write another? Where do people get the idea that something that existed in the past shouldn’t be tried again, when it comes to the world of technology? Not only have the thought, but so many feel so strongly that they have to reach out and tell me what I’m writing about is dumb because it’s already been done?</p>

<p>What is it about web technology that makes people think something can’t be tried again? That conditions aren’t different now? That they need to condemn people? It’s a fascinating phenomena that surrounds Internet technology. It’s one of the reasons it whispers so strongly to young men, who usually do not have a strong understanding of the past, let alone how the future is built on the past. They feel so strongly in their beliefs, and in technology, that they feel compelled to tell people regularly how wrong they are. This behavior manifests itself in strange ways, it feels oddly like bot behavior, meaning that they operate within a certain set of Internet age rules, and do not understand the real world. They work kind of like antibody against anything that it feels questions its reality that everything is new, everything is awesome, and nothing from the past is important or should be used again.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/21/when-you-believe-everything-in-tech-is-new-and-nothing-repeats-itself/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/21/day-2638-apis-are-dumb/">Day 2,638: APIs Are Dumb</a></h3>
        <span class="post-date">21 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/christianity-under-construction_atari_asteroids.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>It is one of those weeks where writing API stories, and doing my API work is completely uninteresting, and my three year old self is throwing a temper tantrum when it comes to doing anything. APIs are dumb. Why the hell would I care about this aspect of technology? Most people don’t understand what the fuck I’m talking about, and people keep doing really dumb shit with them, instead of working on the problems that really matter. Why do I keep doing what I’m doing? Why don’t I just go get a real job, make some real money, and give a shit less? Great question!</p>

<p>Most weeks I can just turn the API Evangelist persona on, and with a notebook full of ideas, and inbox full of questions, I begin writing the API blah blah blah. It just flows. This week it all seems dumb, and I have to fabricate any ounce of caring about APIs. Beyond APIs and Internet technology in general feeling like a pretty bad idea, I feel complicit in helping bring about this technological beast that is wreaking havoc on our world right now. Why the hell should I continue doing API Evangelist, when so many of my ideas can be used for exploitation, and just keep making rich white people richer? It just seems like a bad idea, so why shouldn’t I just shut things down and go find a meaningful job (does that exist)?</p>

<p>First, I always start with the basic API Evangelist mission: helping non-techies understand what APIs are, and how they are right under the hood of everything we are using that is digital. What I do will never receive venture capital, be profitable, and return measurable ROI. Few other companies, let alone individual care about a digitally literate world, they just want consumers, and refuse to see the correlation. I’m the one showcasing API stories consistently regardless of the latest trends, and focus on understanding what is happening outside the current popular areas of investment. I’m the one person that isn’t changing my tune based upon what my investors are telling me, and my storytelling doesn’t reflect where I am at in my runway.</p>

<p>Second, I’m working on important projects. I pushing forward the human services data API (HSDA), and if I get the bandwidth I’ll help lend a hand on Open 311, and work to standardize how we also report issues in cities around the glob. I’m studying how city, state, and federal governments can use common API management practices to generate the next generation of tax base and revenue from the valuable data, and content resources they are stewards of. I’m thinking about how we take back  control from the big tech companies when it our personal data, and content. I’m also thinking about how Twitter, Facebook, and other API platforms are allowing their APIs to be abused by bots because it supports their bottom line, and strengthens their numbers–despite what it is doing to our democracy, our communities, and society. Who else is doing this?</p>

<p>Third, I just don’t want the greedy fucking people to win. I just want to keep being a monkey wrench in the works. I don’t think I’m going to win at this game. I don’t think I’m convince everyone of the right way of doing technology (is there one), but god dam I’m going to make it harder for the people with the money to always win. I’m going to make them spend more money. I’m going work to educate people about how the technology works, and show how we can all resist. On the days that I find it hard to care about educating people about APIs, or even the good APIs projects I’m on, being a wrench in the gears always brightens my day and puts a smile on my face. If nothing else, I’m going to just screw with your grand plans for world domination, and getting rich on our backs. On our data. On our personal lives. I’m going to make you work harder to exploit all of us.</p>

<p>Ok. I’m getting closer to being back on track. Just needed a little reminder of why I’m doing this. APIs are dumb. I’m pretty sure we shouldn’t be fucking doing them in the first place, however with all this technology, algorithms, and artificial stupidity in place, we need some way of making it all a little more observable, and APIs is the best we got. They are the best example I have of making black box algorithms a little more transparent, and how we can take back a little of that data exhaust we generate each day on our mobile phones and laptops. If nothing else, I just need to keep studying this API bullshit so I understand what they are doing, and how they are moving the bits and bytes around in this surveillance capitalism reality we’ve allowed to be constructed. So that when the time is right, I can throw myself against the machine and make it come to a halt, even for a brief moment.</p>

<p>Looking in the mirror: Ok, asshole. Go get em! You can do this. Stop being such a whiney bitch and keep writing stories and doing the research.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/21/day-2638-apis-are-dumb/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/20/generating-operational-revenue-from-public-data-access-using-api-management/">Generating Operational Revenue From Public Data Access Using API Management</a></h3>
        <span class="post-date">20 Nov 2017</span>
        <p><i>This is part of some research I'm doing with <a href="http://apis.how/streamdata">Streamdata.io</a>. We share a common interest around the accessibility of public data, so we thought it would be a good way for us to partner, and Streamdata.io to underwrite some of my work, while also getting the occasional lead from you, my reader. Thanks for supporting my work <a href="http://apis.how/streamdata">Streamdata.io</a>, and thanks for support them readers!</i></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/public-data-api-management/parks-prohibit-commercial-use.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>A concept I have been championing over the years involves helping government agencies and other non-profit organizations generate revenue from public data. It is a quickly charged topic whenever brought up, as many open data and internet activists feel public data should remain freely accessible. Something I don’t entirely disagree with, but this is a conversation, that when approached right can actually help achieve the vision of open data, while also generating much needed revenue to ensure the data remains available, and even has the opportunity to improve in quality and impact over time.</p>

<p><strong>Leveraging API Management</strong>
I’d like to argue that APIs, and specifically API management has been well established in the private sector, and increasingly in the public sector, for making valuable data and content available online in a secure and measurable way. Companies like Amazon, Google, and even Twitter are using APIs to make data freely available, but through API management are limiting how much any single consumer can access, and even charging per API call to generate revenue from 3rd party developers and partners. This proven technique for making data and content accessible online using low-cost web technology, requiring all consumers to sign up for a unique set of keys, then rate limiting access, and establishing different levels of access tiers to identify and organize different types of consumers, can and should be applied in government agencies and non-profit organizations to make data accessible, while also asserting more control over how it is used.</p>

<p><strong>Commercial Use of Public Data</strong>
While this concept can apply to almost any type of data, for the purposes of this example, I am going to focus on 211 data, or the organizations, locations, and services offered by municipalities and non-profit organizations to hep increase access and awareness of health and human services. With 211 data it is obvious that you want this information to be freely available, and accessible by those who need it. However, there are plenty of commercial interests who are interested in this same data, and are using it to sell advertising against, or enrich other datasets, and products or services. There is not reason why cash strapped cities, and non-profit organizations carry the load to maintain, and serve up data for free, when the consumers are using it for commercial purposes. We do not freely give away physical public resources to commercial interests (well, ok, sometimes), without expecting something in return, why would we behave differently with our virtual public resources?</p>

<p><strong>It Costs Money To Serve Public Data</strong>
Providing access to public data online costs money. It takes money to run the database, servers, bandwidth, and websites and applicatiosn being used to serve up data. It takes money to clean the data, validate phone numbers, email addresses, and ensure the data is of a certain quality and brings value to end-users. Yes this data should be made freely available to those who need it. However, the non-profit organizations and government agencies who are stewards of the data shouldn’t be carrying the financial burden of this data remaining freely available to commercial entities who are looking to enrich their products and services, or simply generate advertising revenue from public data. As modern API providers have learned there are always a variety of API consumers, and I’m recommending that public data stewards begin leverage APIs, and API management to better understand who is accessing their data, and begin to put them into separate buckets, and understand who should be sharing the financial burden of providing public data.</p>

<p><strong>Public Data Should Be Free To The Public</strong>
If it is public data, it should be freely available to the public. One the web, and through the API. The average citizen should be able to come use human service websites to find services, as well as us the API to help them in their efforts to help others find services. As soon as any application of the public data moves into the commercial realm, and the storage, server, and bandwidth costs increase, they shouldn’t be able to offload the risk and costs to the platform, and be forced to help carry load when it comes to covering platform costs. API management is a great way to measure each application consumption, and then meter and quantify their role and impact, and either allow them to remain freely accessing information, or be forced to pay a fee for API access and consumption.</p>

<p><strong>Ensuring Commercial Usage Helps Carry The Load</strong>
Commercial API usage will have a distinctly different usage fingerprint than the average citizen, or smaller non-commercial application. API consumers can be asked to declare they application upon signing up for API access, as well as be identified throughout their consumption and traffic patterns. API management excels at metering and analyzing API traffic to understand where it is being applied, either on the web or in mobile, as well as in system to system, and other machine learning or big data analysis scenarios. Public data stewards should be in the business of requiring ALL API consumers sign up for a key which they include with each call, allowing the platform to identify and measure consumption in real-time, and on recurring basis.</p>

<p><strong>API Plans &amp; Access Tiers For Public Data</strong>
Modern approaches to API management lean on the concept of plans or access tiers to segment out consumers of valuable resources. You see this present in software as a service (SaaS) offerings who often have starter, professional, and enterprise levels of access. Lower levels of the access plan might be free, or low cost, but as you ascend up the ladder, and engage with platforms at different levels, you pay different monthly, as well as usage costs. While also enjoying different levels of access, and loosened rate limits, depending on the plan you operate within. API plans allows platforms to target different types of consumers with different types of resources, and revenue levels. Something that should be adopted by public data stewards, helping establish common access levels that reflect their objectives, as well as is in alignment with a variety of API consumers.</p>

<p><strong>Quantifying, Invoicing, And Understanding Consumption</strong>
The private sector focuses on API management as a revenue generator. Each API call is identified and measured, grouping each API consumers usage by plan, and attaching a value to their access. It is common to charge API consumers for each API call they make, but there are a number of other ways to meter and charge for consumption. There is also the possibility of paying for usage on some APIs, where specific behavior is being encouraged. API calls, both reading and writing, can be operated like a credit system, accumulating credits, as well as the spending of credits, or translation of credits into currency, and back again. API management allows for the value generated, and extracted from public data resources is measured, quantified, and invoiced for even if money is never actually transacted. API management is often used to show the exchange of value between internal groups, partners, as well as with 3rd party public developers as we see commonly across the Internet today.</p>

<p><strong>Sponsoring, Grants, And Continued Investment in Public Data</strong>
Turning the open data conversation around using APIs, will open up direct revenue opportunities for agencies and organizations from charging for volume and commercial levels of access. It will also open up the discussion around other types of investment that can be made. Revenue generated from commercial use can go back into the platform itself, as well as funding different applications of the data–further benefitting the overall ecosystem. Platform partners can also be leveraged to join at specific sponsorship tiers where they aren’t necessarily metered for usage, but putting money on the table to fund access, research, and innovative uses of public data–going well beyond just “making money from public data”, as many open data advocates point out.</p>

<p><strong>Alternative Types of API Consumers</strong>
Discovering new applications, data sources, and partners is increasingly why companies, organizations, institutions, and government agencies are doing APIs in 2017. API portals are becoming external R&amp;D labs for research, innovation, and development on top of digital resources being made available via APIs. Think of social science research that occurs on Twitter or Facebook, or entrepreneurs developing new machine learning tools for healthcare, or finance. Once data is available, identified as quality source of data, it will often be picked up by commercial interests building interesting things, but also university researchers, other government agencies, and potentially data journalists and scientists. This type of consumption can contribute directly to new revenue opportunities for organization around their valuable public data, but it can also provide more insight, tooling, and other contributions to a cities or organizations overall operations.</p>

<p><strong>Helping Public Data Stewards Do What They Do Best</strong>
I’m not proposing that all public data should be generating revenue using API management. I’m proposing that there is a lot of value in these public data assets being available, and a lot of this value is being extracted by commercial entities who might not be as invested in public data stewards long term viability. In an age where many businesses of all shapes and sizes are realizing the value of data, we should be helping our government agencies, and the not for profit organizations that serve the public good realize this as well. We should be helping them properly manage their digital data assets using APIs, and develop an awareness of who is consuming these resources, then develop partnerships, and new revenue opportunities along the way. I’m not proposing this happens behind closed doors, and I’m interested in things following an open API approach to providing observable, transparent access to public resources.</p>

<p>I want to see public data stewards be successful in what they do. The availability, quality, and access of public data across many business sectors is important to how the economy and our society works (or doesn’t). I’m suggesting that we leverage APIs, and API management to work better for everyone involved, not just generate more money. I’m looking to help government agencies, and non-profit organizations who work with public data understand the potential of APIs when it comes to access to public data. I’m also looking to help them understand modern API management practices so they can get better at identifying public data consumers, understanding how they are putting their valuable data to work, and develop ways in which they can partner, and invest together in the road map of public data resources. This isn’t a new concept, it is just one that the public sector needs to become more aware of, and begin to establish more models for how this can work across government and the public sector.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/20/generating-operational-revenue-from-public-data-access-using-api-management/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/17/the-many-meanings-of-do-not-make-the-same-mistake-as-twitter-did-with-their-api/">The Many Meanings Of "Do Not Make The Same Mistake As Twitter Did With Their API"</a></h3>
        <span class="post-date">17 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/kinlane-white-board-twitter_copper_circuit.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I remember the first time I heard someone say that they didn’t want to make the same mistake as Twitter did with their API. It was from Pinterest. After that I heard the phrase uttered by many companies, with almost an entirely different meaning behind what the mistake was. Twitter is a darling of the API community when it comes to being the poster child for what not to do in the API space. I consider Twitter to be in the top 10 most important APIs out there, as well as being in the top ten APIs I wouldn’t want to be responsible for, and is a platform full of endless examples of how to do APIs right, and how to do them wrong.</p>

<p>When some companies say this phrase, they mean they don’t want to make the mistake Twitter did by having an API at all–usually heard from executives. Other times, it is said in response to anti competitive behavior in their API ecosystem, and treating startups badly. When you hear from developers, it is usually about their rate limits, and their rules of the road they published a few years back. It coming years I predict we’ll be saying it about automation, and using Twitter as case study for how not to assert control of bots on your API platform. You’ll find me leveraging this statement regularly to talk about making sure you have a real API monetization strategy, and don’t wait a decade to start offering premium access to your APIs that are accessible to EVERYONE.</p>

<p><a href="https://apievangelist.com/2012/06/29/twitter-continues-to-restrict-access-to-our-tweets/">I’ve been complaining about access to the Twitter API for over five years now</a>. API plans are the heart of every API I keep an eye on. They set the tone for ALL conversations that go on around an API. The lack of a coherent, equitable, API access plan at Twitter has set into motion almost every other illness on the platform from harassment to bots. Many of the reasons I would utter the phrase “you don’t want to make the same mistake as Twitter” all stem from the lack of a coherent API plan for the platform. Not having a dedicated page, with a coherent plan for access to your API is the number one mistake you can make operating your APIs in 2017.</p>

<p>When I say this, I am not making any assumptions around what you should be charging, or how you should be limiting access. I’m simply saying that you should have to have a plan, and your community needs a URI to be able to become aware of your plan before ever consuming an API. There are may ways you can make your plan too restrictive, and inject other problems into your API plan, but having one is a good start, and really helps distinguish the APIs who have their act together and those who do not. An API plan demonstrates that you, well, have a plan. Having it publicly available in your developer portal, demonstrates some transparency of your plan, and that you are somewhat willing to include your API community in this plan–always a good idea, but you’d be surprised who doesn’t quite understand this.</p>

<p>Twitter’s biggest crime in my book is not having an API plan over the years. There are many ways we can beat up on Twitter for their shortcomings over the years. I’ve done my fair share. However, I do try and understand the scope of their challenges, and showcase the good that comes from the ecosystem as well. I’m hopeful that their move in offering premium APIs is more about defining a coherent plan for the API, and not simply about chasing a new revenue stream. The release of the APIs, and the structure of the plans and pricing seem to reflect they’ve done some deep thinking from the consumer perspective, so I am optimistic they are moving towards having a plan over just squeezing more money out of our information.</p>

<p>The moral of today’s story kids, is that if you don’t want to make the same mistake as Twitter, do not wait a decade to have a plan in place for your API. Make sure all your APIs have a clear monetization strategy, with a coherent plan in place regaring how you’ll manage your APIs in a way that delivers on your monetization strategy, but also provides a shared plan that includes your partners, and regular API consumers. Without a solid plan in place for your API, your community will never truly be in sync with your organization, and you’ll be incentivizing the worst behavior amongst your consumers. Don’t be like Twitter, and have a plan in place from day one.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/17/the-many-meanings-of-do-not-make-the-same-mistake-as-twitter-did-with-their-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

	<table width="100%" border="1" style="background-color:#FFF; border: 0px #FFF;">
		<tr style="background-color:#FFF; border: 0px #FFF;">
			<td align="left">
				<a href="/blog/page7" class="button"><< Prev</a></li>
			</td>
			<td></td>
			<td align="right">
				<a href="/blog/page9" class="button">Next >></a>
			</td>
		</tr>
	</table>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/" target="_blank"><img src="https://apievangelist.com/images/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/" class="image"><img src="https://apievangelist.com/images/postman-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
