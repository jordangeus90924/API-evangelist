<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }
    
    .container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
    }
    .video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }    

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/25/adding-new-dimension-by-including-patents-in-my-dns-api-research/"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/company/logos/uspto.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/25/adding-new-dimension-by-including-patents-in-my-dns-api-research/">Adding New Dimension By Including Patents In My DNS API Research</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>I have been tracking on API related patents for some time. I regularly pull XML dumps from the US Patent Office, a process in which I am getting more refined, so that I am able to easily tag, and organize them alongside the rest of my research. I spent some time this last week diving into my DNS API research, and after updating the rest of the data behind, I added some DNS related patents. The patent information is already available in my API monitoring system, I just needed to be able to tag the patents, and write a script to publish the tagged patents&nbsp;to each of my Github projects. Now that I have this in place, it is pretty easy for me to spend an hour or two looking through the patents that come each week, and putting them into each area of the API space I study--which is why I have organized my API research the way that I have. The patent information provides shines another light on each layer of the space for me. Understanding the companies, tools, and individual API endpoints provide me with important insight on what is going on, but the patents being submitted are an extremely important indicator of what is actually going on behind the curtain within industries, and companies. With this addition, my research is finally reaching the levels I originally envisioned when I first began organizing my work in this way, going beyond what is available on the "features" page for each company, organization, institution, or government agency is doing with APIs. I can extract a lot of information from the product pages of the companies who are doing APIs, but these pages are designed to tell a specific story. I find that API definitions, press releases, and patent filings often tell a different story than the company's main product page. I would say that all these areas lie, but marketing, API definitions, press releases,...[<a href="/2016/10/25/adding-new-dimension-by-including-patents-in-my-dns-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/24/the-api-behind-every-feature-in-the-user-interface/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_23_at_11.14.58_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/24/the-api-behind-every-feature-in-the-user-interface/">The API Behind Every Feature In The User Interface</a></h3>
			<p><em>24 Oct 2016</em></p>
			<p>I have blogged about this topic in the last 60 days, but I predict it is an area you will hear from me about regularly until I see it baked into more software solutions. CloudFlare, one of my favorite DNS API providers&nbsp;has what I think is the best approach to linking to an API in the bottom corner of every UI element in their dashboard. If you look in the bottom right corner, next to the help icon you will see an API link. When I click on the API link I'm given the API path that corresponds to the UI element. In this scenario it allows me to purge the cache for my domain. I am also given a link to the full CloudFlare API documentation. I have always been an advocate for companies making sure to have an "API" or "Developer" link in the footer of their main website. If you can make sure and have it front and center in the global navigation--all the better. Now I"m going to be advocating for an inline approach like CloudFlare. If all software as a service (SaaS) providers provided API transparency and access in this way, it would be a much different landscape. The more people that know about APIs, the more people are putting them to work developing web and mobile applications, to assist us in moving our valuable bits and bytes around the cloud using iPaaS services like Zapier, and the increasing number of other applications APIs are being put to work in. When you make APIs accessible in this way, it transforms API solutions like Push by Zapier into some pretty empowering solutions for helping the average business user put APIs to work, whenever and wherever they need. Inline access to the API resources behind ALL UI elements is something I will be writing about regularly. Hopefully, I won't have to just keep talking about CloudFlare, and some other SaaS providers step...[<a href="/2016/10/24/the-api-behind-every-feature-in-the-user-interface/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/24/prototype-api-design-guide-builder-developed-on-top-of-api-stylebook/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_23_at_10.59.54_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/24/prototype-api-design-guide-builder-developed-on-top-of-api-stylebook/">Prototype API Design Guide Builder Developed On Top Of API Stylebook</a></h3>
			<p><em>24 Oct 2016</em></p>
			<p>I was pretty happy when my friend Arnaud Lauret&nbsp;(@arno_di_loreto) developed&nbsp;API Stylebook. I want to see his work expand and grow into someday containing hundreds or thousands of API design guides. To help contribute to his work I took the&nbsp;YAML core of the design topics&nbsp;he's aggregated and began developing an API design guide builder that runs 100% on Github, allowing anyone to fork, and use to build their own API Stylebook on top of Arnaud's work. Currently, I have two screens for API design guide builder: Editor&nbsp;- Allows for the editing of the YAML API Design Stylebook stored in the Github repository. View&nbsp;- Allows you to view the API Design Stylebook stored in the Github repository. I'm a big fan of this approach to developing little machine readable (YAML / JSON) micro tools that are simply HTML, CSS, and JavaScript, with a data core. In this scenario, Arnaud's design topics act's as the machine readable YAML core. I just developed the self-contained editor, and viewer, allowing anyone to fork and use to manage their own API design guide. This is just a prototype. I am just getting started. I am looking to add autocomplete suggestions based on the other API design guides that Arnaud has aggregated in his API Stylebook. I am adding this, and some other features to the&nbsp;Github issues for the project, if you have any feedback or suggestions feel free to submit an issue to the Github repository.&nbsp; My objective here is to allow anyone to quickly fork, and build their own corporate or project API design guide, built on top of the existing best practices out there. I envision a future where every company, organization, institution, and an agency has their own API design guide, building on the best approaches available. Arnaud can continue to aggregate and merge the best practices (using Github) out there, and API providers can keep forking, and building on top of the best practices of the APIs...[<a href="/2016/10/24/prototype-api-design-guide-builder-developed-on-top-of-api-stylebook/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/24/preserving-the-twitter-api-field-guide/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_24_at_3.54.55_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/24/preserving-the-twitter-api-field-guide/">Preserving The Twitter API Field Guide</a></h3>
			<p><em>24 Oct 2016</em></p>
			<p>I'm a fan of the human elements of this technological shift that is going on in our world. We tend to focus on the technology, and the dudes who do the technologies (the cyber is HUGE), but what will really matter in 50 or 100 years will be the more human aspects of how we did all of this. Trust me, it is hard to make this boring ass API shit human in any way, so I am always excited when there are people&nbsp;who soften the hard edges of the gears as they grind forward. One of the moments that has stood out for me in the last decade&nbsp;was the Twitter API Field Guide created by Taylor Singletary (@episod) during his time as an evangelist at Twitter. This field guide was removed by Twitter (not sure when as I just noticed today), but the original blog post remained. Thank the Internet gods for the Internet Archive (and Tyler pointing it out), because a copy of his work still lives on for us all to enjoy. I wanted to make sure a copy of it lives on beyond Twitter, and the Internet Archive, so I copied (cleaned up) and published on Github (ha). While this work may seem out of date, and irrelevant, it is art and will be something we will look back on fondly in the future--it shouldn't simply be deleted. This is one of the problems with the constant change we have embraced in the tech space--many things we care about will just be deleted and lost forever. For me, the Twitter Field Guide represents a specific time in the history of the web, and as I write this post I realize this was a dark time for API optimism in the Twitter API ecosystem, something that was having an effect on the wider API movement. I think this field guide was a very creative human response as we are struggling with not...[<a href="/2016/10/24/preserving-the-twitter-api-field-guide/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/24/amazon-groups-should-share-more-api-design-patterns/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/squirrelbin_arch.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/24/amazon-groups-should-share-more-api-design-patterns/">Amazon Groups Should Share More API Design Patterns</a></h3>
			<p><em>24 Oct 2016</em></p>
			<p>The sharing of common API design patterns is something we are really bad at in the API space. I'm not a believer that there is one API design pattern to rule them all, but I am a believer in learning from what works, consuming other people's APIs, and sharing design tips over the cubicle wall. I don't believe that everyone should be 100% REST-compliant in the crafting of their APIs, but you should be picking your head up from time to time, and learning from what the rest of the world is up to, especially across the other groups within your own company. I tend to shy away on critiquing companies on API design, and prescribing any specific approach, but I can't help but point out inconsistencies in any approach, when it is clear that they aren't tuning into some of the common patterns out there, especially between their own internal groups. An example of this can be found at one of the API gods, Amazon Web Services. Amazon isn't known for their RESTful APIs, which is something I can overlook, but&nbsp;when it comes to their lack consistency between their different APIs, I think there are lessons for all of us to learn from. I have not hacked against all of the Amazon APIs, but here are the four distinct patterns I've seen: S3&nbsp;- /?{method} EC2&nbsp;- /?Action={method} Route 53&nbsp;- /2013-04-01/{methodname} Route 53 Domains&nbsp;-&nbsp;route53domains.us-east-1.amazonaws.com/ header: x-amz-target:Route53Domains_v20140515.{method} There might be additional patterns employed over at other Amazon APIs, but these are the four that I'm exposed to in my own integrations. The presence of two separate patterns within the Route 53 team was what prompted me to write this post. While I'm not a fan of the action={method} approach, which is the most common AWS pattern I have seen used, the passing of method as part of custom header just seems even wackier to me.&nbsp; I do not get dogmatic about specific API design patterns, but I...[<a href="/2016/10/24/amazon-groups-should-share-more-api-design-patterns/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/21/icons-to-describe-each-of-your-api-resources-like-aws/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/parquetrename.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/21/icons-to-describe-each-of-your-api-resources-like-aws/">Icons To Describe Each Of Your API Resources Like AWS</a></h3>
			<p><em>21 Oct 2016</em></p>
			<p>Amazon Web Service teams sure have been rocking their architectural icons across their storytelling lately. They standardized a set of icons for each of their cloud services and published in a variety of formats as the&nbsp;AWS Simple Icons for Architecture Diagrams. I am a big fan of the Noun Project API for use across my storytelling&nbsp;and find that having a standardized library of meaningful images and icons to be extremely valuable in helping quickly convey meaning (or entertain).
I was reading&nbsp;optimizing Amazon S3 for high concurrency in distributed workloads from AWS, and the diagram they provided, daisy chaining each of the AWS services at play, really brought the concept home for me. I read a lot, but I also scan a lot, and meaningful diagrams like this can go a long way to help me get up to speed as quickly as possible.

I have been an advocate for establishing icon sets for common API technologies, and even service providers, and I think I will add individual API resources to the stack. It would be valuable to have icon sets for common API resources like images, video, storage, etc. Icons that spoke to what they did, but also clearly identifies them as an API resource&nbsp;and allowing them to possibly be daisy chained together like Amazon does.
API industry icons will have to wait until some benefactor steps up to invest in this crazy idea. I'm graphically challenged, I know what looks good, but I can't craft anything graphical to save my life--look at my logo. Even if we don't have an industry-wide set of icons, it might be something individual API providers would want to consider creating for their API resources, like Amazon does. It is something that could help make your storytelling more impactful, and provide a common set of icons for everyone to use when referencing specific API resources.
[<a href="/2016/10/21/icons-to-describe-each-of-your-api-resources-like-aws/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/21/does-your-business-model-reflect-where-your-api-deployment-is-going/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-site/blog/platform-nanoscale-io-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/21/does-your-business-model-reflect-where-your-api-deployment-is-going/">Does Your Business Model Reflect Where Your API Deployment Is Going</a></h3>
			<p><em>21 Oct 2016</em></p>
			<p>
I've been thinking about the concept of a wholesale API for some time. Going beyond how we technically deploy our APIs, and focusing more on how we can provide a wholesale version of the same API resources, with accompanying terms of services that go beyond just a retail level of API access in the cloud. Not all APIs fit into this category of API, but with the containerization of everything, and the evolving world of Internet of Things (IoT), there are many new ways in which API resources are being deployed.
You can see this evolution in how we are deploying APIs present in one of the latest API deployment platforms I added to my API deployment research, Nanoscale.io. This image is just a portion of their platform, but the separation of deployment concerns articulates the technical side of what I'm talking about, we just need to add in considerations for the business and political side of how this works.
We've seen API deployment move from on-premise and back again, and now we are seeing it move onto everyday objects like cameras, printers, routers, and other everyday objects. I'm watching service providers like Nanoscale.io emerge to help us deploy our APIs exactly where we need them. I'm guessing that the companies who have their business models in similar order, allowing for API service composition from the management layer to further slide down the stack to the deployment layer, will come out ahead.

[<a href="/2016/10/21/does-your-business-model-reflect-where-your-api-deployment-is-going/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/21/an-api-discovery-api-for-your-api-with-tyk/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_20_at_7.37.43_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/21/an-api-discovery-api-for-your-api-with-tyk/">An API Discovery API For Your API With Tyk</a></h3>
			<p><em>21 Oct 2016</em></p>
			<p>
If you are selling services to the API space you should have an API, it is just how this game works (if you are savvy). I was going through Tyk's&nbsp;API for their open source API management solution and came across their API definitions API, which gives you a list of APIs for each Tyk deployment--baking in API discovery into the open source API management solution by default.
The API API (I still enjoy saying that) gives you the authentication, paths, versioning, and other details about each API being managed. I'm writing about this because I think that an API API should be the default for all API service providers. If you are selling me API services you should have an API for all your services, especially one that allows me to discover&nbsp;and manage all the APIs I'm applying your service to.&nbsp;
I am expanding my definition of a minimum viable blueprint for API service providers, and adding an API API as one of the default APIs. I'm going to be adding the account, billing, and a handful of other essential APIs to my default definition. If I'm using your service to manage any part of my API operations, I need to be automating discovery, management, and billing in our relationship.
It seems obvious to me but I'm looking to provide a simple checklist that other API service providers can consider as they craft their strategy. My goal is to help make sure each stop along the lifecycle can be orchestrated in a programmatic way like Tyk.
Disclosure: Tyk is an API Evangelist partner.

[<a href="/2016/10/21/an-api-discovery-api-for-your-api-with-tyk/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/20/what-is-more-important-helping-new-users-be-aware-of-apis-or-pushing-concept-forward/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-forward.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/20/what-is-more-important-helping-new-users-be-aware-of-apis-or-pushing-concept-forward/">What Is More Important? Helping New Users Be Aware Of APIs Or Pushing Concept Forward?</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>
This is a&nbsp;topic that has come up in several discussions lately&nbsp;and is a topic I struggle with on a regular basis. What is more important, helping new users, both developer and non-developer be more aware of APIs, or is helping push forward the concept of APIs amongst those who are already tuned in? While I don't think there is a perfect answer, I think it is an important concept to explore and discuss.
I started API Evangelist on the premise of helping educate new users about the importance of APIs&nbsp;and is something I regularly strive to do, but if you read my blog I definitely do not always keep things simple and accessible&nbsp;to a wide audience. I keep coming back to this founding premise of API Evangelist&nbsp;and work hard to write about 101, and 201 level concepts, despite definitely being a card carrying member of the core API community.
There are a number of leading-edge topics in the space right now from hypermedia to GraphQL, as well as leading edge implementations like bot and voice enablement, but should we be careful to not focus too heavily on these leading issues, and make sure we all invest in new users education as well. I'm feeling like we should keep a core academic group focusing on the big issues, but also enlist armies of folks to help translate down the food chain.
I do not feel there is a right answer to this question. You ask me on separate days, I will answer differently. Sometimes the topics that push forward the API conversation seems most important, but then sometimes educating the developer masses, and the average business users about APIs seems like what will truly push forward the conversation. Even with my flip-flopping, which is why I will never run for president, I think this topic is something we should ask ourselves regularly, and try to understand where we stand as time marches forward.
[<a href="/2016/10/20/what-is-more-important-helping-new-users-be-aware-of-apis-or-pushing-concept-forward/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/20/what-are-your-intentions-with-my-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_19_at_4.41.44_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/20/what-are-your-intentions-with-my-api/">What Are Your Intentions With My API?</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>While it can be easy to bash on API providers for being tight with their API resources, it can be very difficult to be an API provider operating in today's online environment. Some developers are just badly behaved and hold some pretty unrealistic expectations when it comes to opening up access to valuable content, data, and algorithms. This is why I work to be supportive of providers when locking down their API resources, as long as they do it a constructive way. One positive approach I came across recently was from the Oxford Dictionaries API, who ask a lot of questions as part of their API signup form, but I felt it was a positive experience, and provided no barriers to me gaining access to their valuable API resources. They asked me details about what type of application I'm developing, as well as the platform and language I am employing--allowing me to provide a summary of what it was I would be doing. They provide me with a summary of the terms of service, and the ability to opt into a platform email list, and they also let me know that they will not be selling my personal details to third parties. I have never been a fan of lengthy API signup forms, as I tend to feel like I'm just a lead in their sales funnel, but this left me feeling the opposite. The Oxford Dictionary API signup form felt more like it was about making sure everyone was informed&nbsp;and in the service of protecting valuable API assets. I'm a supporter of API providers requesting more details on what API developers are up to, as long as they do it in the right way. If you don't make me feel like I'm a criminal, that I'm just one of many users in your sales funnel, and that you are just looking to extract value from me, I'm more than happy to share more information...[<a href="/2016/10/20/what-are-your-intentions-with-my-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/20/the-potential-of-the-openapi-spec-parameters-object/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_19_at_9.04.05_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/20/the-potential-of-the-openapi-spec-parameters-object/">The Potential Of The OpenAPI Spec Parameters Object</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>
I enjoy learning from the OpenAPI Specs of the API providers I track on. Just having an OpenAPI Spec present tells a lot about an API provider in my book, but the level of detail some providers put into their API definitions adds another level to this for me. While reviewing the OpenAPI Spec for the Oxford Dictionaries API, I noticed their robust usage of the OpenAPI Spec parameters definitions collection, which provides an interesting overview of the surface area of the API, augmenting the benefits brought to the table by the definitions collection of an APIs underlying data schema.
When you are defining each path for an API you can either define the parameters using each paths parameters, or you can add them to the overall parameters definition object, allowing them to be reused across all paths. This object provided me with a centralized place to learn about the parameters used when making calls to the Oxford Dictionary API, and I'm assuming it helped them be more organized in how they defined the surface area of their APIs.

I can see how the processing of defining each path's parameters, and centrally organizing them for reuse can be a healthy thing. The more you lift yourself out of the individual&nbsp;definition&nbsp;of each path&nbsp;and consider the parameter patterns that have been used for other paths, the chances you will have a better view of the landscape will increase. I am optimistic about this OpenAPI Spec object, and curious about how it can be evolved as part of other conversation around GraphQL--something I'll work to understand better in the future.
[<a href="/2016/10/20/the-potential-of-the-openapi-spec-parameters-object/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/20/the-open-guide-to-amazon-web-services/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_19_at_8.25.43_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/20/the-open-guide-to-amazon-web-services/">The Open Guide to Amazon Web Services</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>I keep an eye on things that are trending daily and weekly on Github&nbsp;because it is a great way to discover new companies and individuals doing interesting things with APIs. While looking at this earlier this week I came across the open guide to Amazon Web Services, a pretty robust, and well organized&nbsp;getting started guide to everything AWS. Here is their description of this resource out of the leading cloud computing platform: A lot of information on AWS is already written. Most people learn AWS by reading a blog or a &ldquo;getting started guide&rdquo; and referring to the&nbsp;standard AWS references. Nonetheless, trustworthy and practical information and recommendations aren&rsquo;t easy to come by. AWS&rsquo;s own documentation is a great but sprawling resource few have time to read fully, and it doesn&rsquo;t include anything but official facts, so omits experiences of engineers. The information in blogs or&nbsp;Stack Overflow&nbsp;is also not consistently up to date. This guide is by and for engineers who use AWS. It aims to be a useful, living reference that consolidates links, tips, gotchas, and best practices. It arose from discussion and editing over beers by&nbsp;several engineers&nbsp;who have used AWS extensively. I find it interesting when API providers invest in authoritative&nbsp;solutions like this, acknowledging the scattered and often out of date nature of blogs, QA sites, forums, and the wealth of other self-service resources available for APIs. Amazon is getting seriously organized with their approach to provider resources for developers--they have been doing this a while, and know where the pain points are.&nbsp; Amazon's organized approach, the breaking down by service, and the usage of Github are all interesting things I think are worth noting as part of my research. AWS is a tough API pioneer to showcase because they have way more resources than the average API provider, but as one of the early leaders in the API&nbsp;space they possess some serious&nbsp;wisdom&nbsp;and practices that are worth emulating. I'll keep going through their open...[<a href="/2016/10/20/the-open-guide-to-amazon-web-services/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/20/reducing-friction-for-api-developers-with-enums-in-api-definitions/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_19_at_7.24.09_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/20/reducing-friction-for-api-developers-with-enums-in-api-definitions/">Reducing Friction For API Developers With Enums In API Definitions</a></h3>
			<p><em>20 Oct 2016</em></p>
			<p>
I am going through the Oxford Dictionaries API, learning about this valuable resource. Their onboarding process for registration, and learning about what the API does using interactive documentation, is very smooth. One of the things that really cuts the rough edges off learning about each API are the enums that are available for each path.
The parameters required for making calls to many of the paths, like language and country, have their enum values populated as part of their API definition. I look at numerous OpenAPI Specs in the course of my work&nbsp;and they rarely have values present for enum, providing critical default values for developers to use--eliminating some often serious frustration.
Not having the right values available when making even the simplest of API calls can be a significant point of friction when trying to get up and running using an API. While it may seem like a small thing, the work the Oxford Dictionaries API team has put into this level of detail for their API definitions will go a long way towards making their API resources more accessible and usable.
[<a href="/2016/10/20/reducing-friction-for-api-developers-with-enums-in-api-definitions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/19/we-want-to-do-apis-you-already-are-it-is-just-not-in-an-organized-way/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-organized.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/19/we-want-to-do-apis-you-already-are-it-is-just-not-in-an-organized-way/">We Want To Do APIs -- You Already Are, It Is Just Not In An Organized Way</a></h3>
			<p><em>19 Oct 2016</em></p>
			<p>
I have a number of folks at companies, organizations, institutions, and government agencies come to me saying that they want to do APIs, and they need some help. In many of these discussions, the first task centers around addressing the motivations behind this declaration and helping folks realize they are already doing APIs, they are just not doing it in any sort of coherent and organized fashion.
Any business, organization, institution and government agency is moving machine readable data between internal, and with external systems in 2016, for use in a variety of applications. This is just done in a variety of often proprietary, ad-hoc, data dump, and other approaches that are often dictated without any coherent vision. API does not always mean 100% REST, it is about establishing interfaces between systems for programmatic usage across a variety of applications--the trick is to do all of this in a simplified, standardized, consistent, and low-cost way modeled on what has been working for the web.&nbsp;
You are already doing API. You need to identify all the different ways you are already exporting, importing, migrating, syncing, and putting data to work across operations and begin to establish a coherent roadmap and approach to communicating about all of this. Then get to work identifying where the opportunities for standardizing how everything is defined, how&nbsp;access is needed, and a baseline for communication around these critical integrations on an ongoing basis. It's not about whether we should do APIs or not, you are already there, it is how do start doing it in a more organized and coherent way.
[<a href="/2016/10/19/we-want-to-do-apis-you-already-are-it-is-just-not-in-an-organized-way/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/19/how-do-we-keep-the-fire-alive-in-api-space/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-flame.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/19/how-do-we-keep-the-fire-alive-in-api-space/">How Do We Keep The Fire Alive In API Space?</a></h3>
			<p><em>19 Oct 2016</em></p>
			<p>It is tough to keep a sustained fire burning in the world of technology, at the individual, organizational, and community level. I have been doing API Evangelist full time for six years, and it is no secret that I have had several moments where I've experienced a crisis of faith, and I do not doubt that there will be&nbsp;many more of these in my future--there is no perfect solution. It takes hard work, creativity, and a myriad of other considerations to keep going, stay energized, and keep other folks doing the same. I have spent a great deal of time this fall thinking about all of the factors that influence me, and contribute to the fire burning, or acting as a flame retardant to me and the API space. When exploring these contributing factors, it is only logical we start with the external forces right? Because this all sucks because of everything external, aka all of you! Couldn't possibly be me? So what are some of the external forces out there that contribute to the fire burning brightly, or possibly being dampened across API space are? People Aren't Always Nice - For some reason, the Internet has brought the worst out in many of us. I personally feel this is the nature of technology -- it isn't human, and the more time you spend with it, the less human we are, and less empathy we will have for other people. Everyone Takes A Little - Until you've spent a great deal of time in the spotlight writing, speaking, or otherwise you don't fully grasp this one. Every person you talk to, every hand you shake, takes a little bit from you -- making it super critical for people to give back -- it all takes a toll, whether we acknowledge it or not. Few Ever Give Back - The general tone set by startup culture and VC investment is to take, take, take, and rarely...[<a href="/2016/10/19/how-do-we-keep-the-fire-alive-in-api-space/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/18/the-twitter-branding-page-provides-minimum-bar-for-api-providers/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/twitter_logo_blue.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/18/the-twitter-branding-page-provides-minimum-bar-for-api-providers/">The Twitter Branding Page Provides Minimum Bar For API Providers</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>API branding is an area that I find to be contradictory in the space, with the loss of brand control being in the top concerns for companies when doing APIs, while simultaneously one of the most deficient areas of API operations, with most API providers have no branding guidance in their developer portal whatsoever. I think it is just one of those telling aspects of how dysfunctional many companies are, and how their concerns are out of alignment with reality, and where they are investing their resources. Every API should have some sort of branding page or area for their API operations--I even have a branding page. ;-) If you are looking for a healthy example to consider as a baseline of your branding page, take a look at Twitters branding page, which provides the essential building blocks you should be considering: Simple URL - Something easy to find, easily indexed by search engines, even a subdomain like Twitter does. Logos &amp; Assets - Provide us with at least a logo for your company, if not a wealth of assets&nbsp;to put to use. Branding Guidelines - Offer up some structure, helping guide us, and show you've put some thought into branding. Link to Embeddables - If you have any buttons, badges, and widgets, point us to your embeddable resources. Link to Terms of Service - Provide us with a quick link to the terms of service as it applies to branding. Contact Information - Give me an email, or another channel for asking a question if I get confused at any time. I do not agree with all of Twitter's branding enforcement, but I respect that they have set a bar, and provide the ecosystem with guidance. At the very least, it makes sure all of us are using the latest logo, and when done right it can help encourage consistency across every integration that is putting API resources to work. I find it hard...[<a href="/2016/10/18/the-twitter-branding-page-provides-minimum-bar-for-api-providers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/18/learning-about-apis-has-to-be-relevant-and-interesting/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_relevant.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/18/learning-about-apis-has-to-be-relevant-and-interesting/">Learning About APIs Has To Be Relevant And Interesting</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>I am working on a project with a 16-year-old young lady to extract and tell a story using the YouTube API. I'm pretty excited about the project because the young lady happens to be my daughter Kaia Lane. If you've ever seen my API Blah Blah Blah t-shirt, you've seen her work. Historically she could care less about APIs, until recently when pulling data about one of her favorite YouTube stars came up--suddenly she is interested in learning more about APIs. During regular chatting with my daughter, I shared a story on&nbsp;the entire history of Kickstarter projects broken down by a city. She is a little geeky and likes Kickstarter, so I figured I'd share the approach to telling stories with data, and said that if she ever wanted help telling a story like this using YouTube, Instagram, or another platform, that she should let me know. She came back to me a couple days later asking to learn more about how she could tell a story like this using data pulled from one of the YouTube stars she follows. &nbsp; Ok. I have to stop there for a moment. My 16-year-old daughter&nbsp;just asked me to learn more about APIs. :-) As an old goofy dad who happens to be the API Evangelist, I am beside myself. I'm not 100% sure where this project will go. Right now I'm just seeing what data I can pull on Dan &amp; Phil's video game YouTube channel, and from there we'll talk more about what type of story we want to tell about their followers&nbsp;and get to work pulling and organizing the data we need. I couldn't think of a tougher audience to be trying to get her interested in APIs. She isn't going to care about APIs, wants to learn about APIs, let alone become proficient with APIs unless they are relevant and interesting to her world.&nbsp; I do not think this lesson is exclusive to teaching...[<a href="/2016/10/18/learning-about-apis-has-to-be-relevant-and-interesting/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/18/are-api-docs-amp-definition-formats-a-single-thing-or-separate/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_splitter.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/18/are-api-docs-amp-definition-formats-a-single-thing-or-separate/">Are API Docs &amp; Definition Formats A Single Thing Or Separate?</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>I was reading a virtual panel: document and description formats for web APIs,&nbsp;and thought the conversation was very productive when it comes to helping bring the world of API documentation and definitions into better focus. I encounter daily reminders that folks do not see the many dimensions of API definitions, and the role they play in almost every stop along the life cycle. This virtual panel helps move this discussion forward for me, providing some clarification for when it comes to the separation between API definitions and API documentation. One of the questions asked of the panels was "Do you see API Documentation and Description formats as a single thing? Or multiple things?" Which I found&nbsp;Zdenek Nemec (@zdne) answer to be a great introduction for folks when it comes to understanding the importance of this separation: There are definitely two different things. But truth be told, the initial incentive for the use API description formats was definitely the vision of API documentation without much work. However, the tide is turning as more and more people are discovering the benefits of the upfront design, API contracts, and quick prototyping Many people still see machine readable definitions as purely something that drives API documentation. OpenAPI Specs are just for deploying Swagger UI, and API Blueprint is just for using Apiary. When in reality, the why and how you are doing API definitions is much, much deeper. As Z from Apiary points out, it is key to the API design and prototyping process, and critical to establishing the API contract. Realizing that crafting machine readable API definitions is not just about API documentation, and that it is essential to establishing a meaningful technical, business, and legal contract internally, with partners, and maybe the public, early on in this API life cycle is empowering. I would say that I didn't fully appreciate API design, and understanding the depth of it until I had OpenSpec providing me with a scaffolding...[<a href="/2016/10/18/are-api-docs-amp-definition-formats-a-single-thing-or-separate/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/18/all-the-right-channel-icons-in-support-of-your-api-platform/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/restlet_icons.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/18/all-the-right-channel-icons-in-support-of-your-api-platform/">All The Right Channel Icons In Support Of Your API Platform</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>I look at a lot of websites for companies who are providing APIs and selling services to the API space. When I find a new company, I can spend upwards of 10 minutes looking for all the relevant information I need to connect. Elements like where their Twitter and Github accounts are. These are all the key channels I am looking for so that I can better understand what a company does and stay in tune with any activity, but they are also the same channels that developers will be looking for so that they can stay in tune a platform as well. I spend a great deal of time looking for these channels, so I'm always happy when I find companies who provide a near complete set of icons for all the channels that matter. Restlet, the API design, deployment, management,and&nbsp;testing&nbsp;platform&nbsp;has a nice example of this in action, providing the following channels: Facebook Twitter Google+ LinkedIn Vimeo Slideshare Github Stack Overflow Email All of these channels are available as orderly icons in the footer of their site. Making my job easier, and I'm sure making it easier for other would be API developers. They also provide an email newsletter signup along with the set of icons. While this provides me with a nice set of channels to tune into, more than I usually find, I would still like to have a blog and atom feed icons, as well as maybe an AngelList or Crunchbase, so that i can peak behind the business curtain a little. I know. I know. I am demanding, and never happy. I am just trying to provide an easy checklist for companies looking to do interesting things APIs of the common channels they should consider offering. You should only offer up channels that you can keep active, but I recommend that you think about offering up as many of these as you possibly can manage. No matter which ones you...[<a href="/2016/10/18/all-the-right-channel-icons-in-support-of-your-api-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/18/a-more-honest-and-flexible-api-contract-using-hypermedia/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-contract.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/18/a-more-honest-and-flexible-api-contract-using-hypermedia/">A More Honest And Flexible API Contract Using Hypermedia</a></h3>
			<p><em>18 Oct 2016</em></p>
			<p>One of the reasons I write so much on API Evangelist is to refine how I tell stories about APIs and hopefully make a bigger impact by being more precise in what I'm saying. I feel like one of the reasons why hypermedia API concepts have to take longer than we anticipated to spread is because many (not all) of the hypermedia elite suck at telling stories. I am sorry, but you have done a shit job selling the importance of hypermedia, and often times were just confrontational and turning many folks off to the subject. I am working on playing around with telling different stories about hypermedia, hoping to soften some of the sharp edges of the hypermedia stories we tell. One of the core elements of hypermedia APIs is they provide us with links as part of each API response, emulating much of what works with the web, in the system to system, and application layers of the web. One of the benefits of these links is they help facilitate&nbsp;the evolution and change that is inevitable in our API infrastructure. The default argument&nbsp;for hypermedia folk is often about versioning&nbsp;and change-resistant API clients. I'm looking to help make these concepts more human, and that employing hypermedia&nbsp;as part of our API contracts is more about providing an honest and flexible view of the business relationship we are entering into. As API provider and consumer, we are acknowledging that there will be change and evolution in the resources that are being delivered, and being more honest that this exists, as we craft this API contract. As an API provider I am not just going to dump this JSON product response on you, and expect you to know what to do, refusing to have a discussion with my consumers that this product will change. it might be out of inventory, and might be replaced by another product, and any number of evolution changes that may occur in...[<a href="/2016/10/18/a-more-honest-and-flexible-api-contract-using-hypermedia/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/17/thinking-more-about-api-driven-conversational-interfaces/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-conversational-interfaces.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/17/thinking-more-about-api-driven-conversational-interfaces/">Thinking More About API Driven Conversational Interfaces</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>I am spending a lot of time thinking about conversational interfaces, and how APIs are driving the voice and bot layers of the space. While I am probably not as excited about Siri, Alexa and the waves of Slack bots being developed as everyone else, I am interested in the potential when it comes to some of the technology and business approaches behind them. When it comes to these "conversational interfaces", I think voice can be interesting, but not always practical for actually interacting with everyday systems--I just can't be talking to devices to get what I need done&nbsp;each day, but maybe that is just me. I'm also not very excited about the busy, chatty bots in my Slack channels, as I'm having trouble even dealing with all the humans in there, but then again maybe this is just me.&nbsp; I am interested in the interaction between these conversational interfaces and the growing number of API resources I track on, and how the voice and bot applications which are done thoughtfully, might be able to do some interesting things&nbsp;and enable some healthy interactions. I am also interested in how webhooks, iPaaS, and push approaches like we are seeing out of Zapier, can influence the conversation around conversational interfaces.&nbsp; Conceptually I can be optimistic about voice enablement, but I work in the living room across from my girlfriend, I'm just not going to be talking a lot to Siri, Alexa or anyone else...sorry. Even if I move back to our home office, I'm really not going to be having a conversation with Siri or Alex to get my work done, but then again maybe its just me. I'm also really aware of the damaging effects of too much messaging, chat, and push notification channels open, so the bot thing just doesn't really work for me, but then again maybe it's me.&nbsp; I am more of a fan of asynchronous conversations than I am of the synchronous...[<a href="/2016/10/17/thinking-more-about-api-driven-conversational-interfaces/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/17/people-do-not-know-what-your-api-does-if-you-do-not-showcase-it/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-spotlights.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/17/people-do-not-know-what-your-api-does-if-you-do-not-showcase-it/">People Do Not Know What Your API Does If You Do Not Showcase It</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>
With a lot of my storytelling, I feel like captain obvious, but I also recognize the importance of simple, and sometimes repetitive storytelling to help reach my audience of time and resource-strapped&nbsp;API providers. Sometimes API providers are just too busy to remember the small things, and this is where I come in to help you remember some of the most obvious aspects of providing APIs that can be essential to success.
This morning's reminder is that nobody will know the cool things your API does if you do not showcase what is being done with it. This is why I added my API showcase research, to highlight the approach of the successful API providers, and give me a regular reminder to write about the topic. I understand you are busy, but so are your API consumers, and there is a good chance they need a little help understanding what can be done with your super valuable API resource(s).
Showcasing your successful API integrations is where the rubber meets the road with API operations. Tell us all about the cool things people are doing with your APIs. It doesn't have to be a full blown&nbsp;case study (although that would be nice too), it can be 250 words, plus some bullets, and another 250 words, helping us understand the possibilities in 2 minutes or less. From my experience in the space, people eat up these simple, easy to read examples of APIs solving problems, and providing solutions.
Ideally, you are showcasing the cool things being done with your API on a regular basis via your blog, but if for some reason, you are too busy, make sure and at least share your thoughts with me via email. You never know, if it is worthy, I might take the time to share here on API Evangelist.
[<a href="/2016/10/17/people-do-not-know-what-your-api-does-if-you-do-not-showcase-it/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/17/discovering-new-apis-through-security-alerts/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/cf60890864f3b19978e7a9a6a9c84152_400x400.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/17/discovering-new-apis-through-security-alerts/">Discovering New APIs Through Security Alerts</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>
I tune into a number of different channels looking for signs of individuals, companies, organizations, institutions, and government agencies doing APIs. I find APIs using Google Alerts, monitoring Twitter and Github, using press releases and&nbsp;via patent filings. Another way I am learning to discover APIs is via alerts and notifications about security events.
An example of this can be found via the Industrial Control Systems Cyber Emergency Response Team out of the U.S. Department of Homeland Security (@icscert), with the recent issued advisory ICSA-16-287-01 OSIsoft PI Web API 2015 R2 Service Acct Permissions Vuln to ICS-CERT website, leading me to the OSIsoft website. They aren't very forthcoming with their API operations, but this is something I am used to, and in my experience, companies who aren't very public with their operations tend to also cultivate an environment where security issue go unnoticed.
I am looking to aggregate API related security events and vulnerabilities like the feed coming out of Homeland Security. This information needs to be shared more often, opening up further discussion around API security issues, and even possibly&nbsp;providing an API for sharing real-time updates and news. I wish more companies, organizations, institutions, and government agencies would be more public with their API operations&nbsp;and be more honest about the dangers of providing access to data, content, and algorithms via HTTP, but until this is the norm, I'll continue using API related security alerts and notifications to find new APIs operating online.
[<a href="/2016/10/17/discovering-new-apis-through-security-alerts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/17/defining-oauth-scope-inline-within-the-api-documentation/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_16_at_2.35.09_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/17/defining-oauth-scope-inline-within-the-api-documentation/">Defining OAuth Scope Inline Within The API Documentation</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>I am working on a project using the Youtube API, and came across their inline OAut 2.0 scopes, allowing you to explore what the API does as you are browsing the API docs. I am a huge fan of what interactive documentation like Swagger UI, and Apiary brought to the table, but I'm an even bigger fan of the creative ways people are evolving&nbsp;upon the concept, making learning about APIs a hands-on, interactive experience wherever possible.
To kick off my education of the YouTube API I started playing with the search endpoint for the Youtube Data API. As I was playing with I noticed the had an API explorer allowing me to call the search method and see the live data.

Once I clicked on the "Authorize requests using OAuth 2.0" slider I got a popup that gave me options for selecting OAuth 2.0s copes, that would be applied by the API explorer when I make API calls.

The inline OAuth is simple, intuitive, and what I needed to define my API consumption, in line within the Youtube API documentation. I didn't have to write any code&nbsp;or jump through a bunch of classic OAuth hoops. It gves me what I need for OAuth, right in the documentation--simple OAuth is something you don't see very often.
I'm a supporter of more API documentation being an attractive static HTML layout like this, with little interactive modules embedded throughout the API docs. I'm also interested in seeing more web literacy being thrown in at this layer as well, pulling common web concepts and specification details, and providing popups, tooltips, and other inline API design learning opportunities.
I'm adding YouTube's approach to OAuth to my list of approaches to a modular approach to delivering interactive API documentation, for use in future storytelling.
[<a href="/2016/10/17/defining-oauth-scope-inline-within-the-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/17/convert-openapi-spec-to-slate-shins-markdown-api-docs/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/widdershins_logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/17/convert-openapi-spec-to-slate-shins-markdown-api-docs/">Convert OpenAPI Spec to Slate / Shins Markdown API Docs</a></h3>
			<p><em>17 Oct 2016</em></p>
			<p>
Someone turned me on to an OpenAPI Spec to Slate / Shins compatible markdown converter on Github this last week. I have been an advocate for making sure we are still using machine readable API definitions for our API documentation, even if we are deploying the more attractive Slate. I've been encouraging folks to develop an attractive option for API documentation driven by OpenAPI Spec for some time, so I am happy to add this converter to my API documentation research and toolbox.
The OpenAPI Spec to markdown converter also introduced me to a version of Slate that is ported to JavaScript / Node.js called Shins. I'm going to add Shins to my API documentation research, and "widdershins" the OpenAPI Spec to markdown converter to my API definition research. The auto-generation of attractive API documentation like Slate and Shins seems like a valid approach to getting things done, and worth including in my research.
I am increasingly publishing&nbsp;YAML editions of my OpenAPI Specs which drive API documentation that operates on Jekyll, using Liquid. So I am all about having many different ways to skin the API documentation beast, allowing it to be easily deployed as part of any CI flow, and enabling the publishing of API docs for many different APIs, in many different developer portals or embedded on any device as part of IoT deployments. I think that a diverse range of approaches are optimal, as long as we do not lose our machine readable core.

[<a href="/2016/10/17/convert-openapi-spec-to-slate-shins-markdown-api-docs/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/14/transparency-in-police-access-to-social-platforms-using-oauth-and-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/facebook_instagram_and_twitter_provided_data_access_for_a_surveillance_product_marketed_to_target_activists_of_color__american_civil_liberties_union.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/14/transparency-in-police-access-to-social-platforms-using-oauth-and-apis/">Transparency In Police Access To Social Platforms Using OAuth And APIs</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>I was learning about&nbsp;Geofeedia providing law enforcement access to social media data from Twitter, Facebook, and Instagram via their API(s) this week. Geofeedia was making money by selling surveillance services to law enforcement build on top of these social APIs and is something that I guess Facebook and Instagram have cut-off access, but they could still have Twitter access through a reseller (Gnip?).&nbsp; This isn't something that will just go away. If law enforcement wants access to user's data on Facebook, Twitter, and Instagram, they are going to get it. I am guessing that the rules regarding what law enforcement can or can't do aren't clear (I will have to learn more), and something that is just left up to platforms to enforce via their terms of service. It is a problem that modern approaches to API authentication, management, and analytics are well designed to help make sense of--we just have to come up with a new layer defined specifically for law enforcement. Law enforcement should be able to fire up any standard, or customized solution they desire to search against social media data via APIs. However, they should be required to obtain an application key, and obtain the OAuth tokens that any other developer would need to. Rather than law enforcement being the customer of companies like Geofeedia, they should each get their own app id and keys, providing an identifying&nbsp;application that represents a specific law enforcement agency. They can still buy the software from providers, they just need the unique identifier when it comes to API consumption. Along with this access, we also need to begin to define an auditable or regulatory layer, where other government agencies or 3rd party auditors can get access to the access logs for all applications registered to law enforcement&nbsp;agencies. A kind of real time FOIA access to the API management layer, allowing for a window into how law enforcement agencies are searching and putting social media data...[<a href="/2016/10/14/transparency-in-police-access-to-social-platforms-using-oauth-and-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/14/the-monitoring-layer-of-the-devops-aggregation-api-platform/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-puzzle-four-pieces.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/14/the-monitoring-layer-of-the-devops-aggregation-api-platform/">The Monitoring Layer Of The DevOps Aggregation API Platform</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>
While spending some time going through my API monitoring research I found myself creating an OpenAPI spec and APIs.json&nbsp;index for the DataDog API, and had the realization that this is the beginning of what I'm looking for when I was talking about a DevOps aggregation API platform. DataDog is just the monitoring layer of this vision I have, but it has many of the other elements I'm looking for.
DataDog has all the monitoring elements present in their API platform, and they have all the platform integrations I'm envisioning in a DevOps aggregate API. We just need the same thing for design, deployment, virtualization, serverless, DNS, SDK, documentation, and the other critical stops along a modern API life cycle.
I'll keep profiling the APIs for the service providers in my life cycle research&nbsp;until I get more of this DevOps aggregate API definition mapped out. Hopefully, I will stumble across other providers like DataDog who are doing such an interesting job with the choreography, and orchestration that will be needed to work across so many platforms. I appreciate API aggregation service providers who 1) have an API, and 2) share so much of the definition behind their work.
The next thing I will work on is profiling the metrics that DataDog has defined across the platforms they integrate with. Take a look at the metrics they have defined for each integration, there are some valuable patterns available in their work. I'd love to see a common set of API monitoring metrics emerge from across providers, something that if we standardize and share in a machine readable way, others will emulate--making&nbsp;interoperability much smoother when it comes to monitoring.
I just wanted to keep beating my drum about the fact that APIs aren't just about building applications, they are also critical to the API life cycle, and making sure there are stable, scalable APIs to build applications on top of in the first place.
[<a href="/2016/10/14/the-monitoring-layer-of-the-devops-aggregation-api-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/14/taking-a-fresh-look-at-the-twitter-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/twitter_developers.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/14/taking-a-fresh-look-at-the-twitter-api/">Taking A Fresh Look At The Twitter API</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>I am working on profiling the Twitter API again, and I thought their stack of APIs have evolved significantly beyond what we tend to think of as the Twitter API, and was worth taking another look at. It is easy to think of Twitter API being about tweeting, friends, and following people, and #hashtags, but they have an interesting mix that I think tells its own story about Twitter's journey. Here is the current Twitter API stack: Public REST API - The public REST APIs provide programmatic access to read and write the Twitter data -- what we think of when we talk about the Twitter API. Media API - The APi for managing photo, videos or animated GIFs, that are used by other Twitter API endpoints when tweeting, direct messaging, and others. Collections API - The API for managing collections of tweets to tell specific stories, providing a single URL that represents each Twitter collection. The TON (Twitter Object Nest) API - Allowing implementers to upload media and various assets to Twitter, allowing for resumable, and single file uploads. Curator API - Provides broadcasters their curator-created streams for on-air graphics systems, or other digital displays.&nbsp; Streaming APIs - Deliver new responses to REST API queries over a long-lived HTTP connection, providing a regular stream of tweets from the platform. Ads API - The Ads API gives partners a way to integrate Twitter advertising management in their product. Selected partners have the ability create their own tools to manage Twitter Ad campaigns while easily integrating into existing, cross-channel advertising management solutions. Gnip -&nbsp;Gnip is Twitter&rsquo;s enterprise API platform, delivering real-time and historical Twitter firehose data for large use applications. It is interesting to think about Twitter's long API evolution that got them here. I hear people often reference Twitter as the most extreme example of a public API out there. Granted, it is definitely the original example and has a very public element to it,...[<a href="/2016/10/14/taking-a-fresh-look-at-the-twitter-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/14/slack-shares-their-view-on-bot-advertising/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/1_z2iwyx5ebaipra4gsrrljw.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/14/slack-shares-their-view-on-bot-advertising/">Slack Shares Their View On Bot Advertising</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>
I was reading the hard questions on bot ethics from Slack, and their thoughts on bot advertising grabbed my attention. Trying to understand how bots will be monetizing things has been something I'm learning about, so I found Slack's post rather timely, and relevant to this fast growing layer of the API world.&nbsp;
Here was Slack's view on advertising with bots by developers:
A bot should not serve ads unless it has a strong, expressed purpose that benefits the user in doing so, and even then only on B2C platforms. I would hate to see bots becoming the new tracking pixel. Bots should not be prompting users to click on things and buy things unless explicitly asked to do so.
They continue by adding that, "ads in apps are against the&nbsp;Slack API&nbsp;terms of service, and that makes me rather proud". I'm hoping that the Slack's business model is solid enough that they'll never need to consider bot advertising. I think it is an interesting constraint upon the community, and one that I'm curious how they'll work around when it comes to making money with bots. Could we be looking at a post-advertising world, when it comes to generating revenue?
From what I can tell, bot monetization will either be about mining data from users,&nbsp;paying for premium features, or a little of both. I'm still a few weeks off from having more examples of how the bots are generating revenue, but I wanted to make sure I recorded Slack's stance on the subject, for reference in future work.
[<a href="/2016/10/14/slack-shares-their-view-on-bot-advertising/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/14/including-the-twitter-object-nest-api-as-a-file-upload-api-example/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_13_at_9.17.46_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/14/including-the-twitter-object-nest-api-as-a-file-upload-api-example/">Including The Twitter Object Nest API As A File Upload API Example</a></h3>
			<p><em>14 Oct 2016</em></p>
			<p>One request I get from folks on a regular basis, is an example of file upload APIs. Each time I get one of these requests I regret that I do not have more file upload and storage APIs profiled, allowing me to share a list of examples. So file upload APIs are high on my list to keep an eye out for as I'm doing my regular monitoring and mapping of the API universe.&nbsp; An API I wanted to add to this list was the&nbsp;TON (Twitter Object Nest) API, which "allows implementers to upload media and various assets to Twitter".&nbsp;The TON API is an interesting model for me because it supports resumable, and non-resumable uploads--with all files over 64MB required to be resumable. I wanted to profile the API in a story, and add some of the key aspects to my research on file upload APIs, so that I could reference in future conversations. Some of the core features of how the TON API operates are: The Content-Type of requests cannot be application/x-www-form-urlencoded. The Content-Type of requests&nbsp;are&nbsp;a valid media type as defined by&nbsp;IANA. Chunks should be in integer multiples of X-TON-Min-Chunk-Size (except the last). The Location header after upload needs to be saved to be used in other Twitter API calls. Here is the basic makeup of the initial request to kick off a resumable upload: Authorization: See 3-legged authorization Content-Length: Set to 0 Content-Type: The Content-Type of the asset to be uploaded. X-TON-Content-Type: Identical to Content-Type X-TON-Content-Length: Total number of bytes of the asset to be uploaded The initialization response contains a Location which can then be used in other calls to the Twitter API. After you make the resumable upload initialization call, you can make each of the follow-up chunk uploads for the file--here is an example resumable video upload request: PUT /1.1/ton/bucket/{bucket}/SzFxGfAg_Zj.mp4?resumable=true&amp;resumeId=28401873 HTTP/1.1 Authorization: // oAuth1.0a (3-legged) details here Content-Type: video/mp4 Content-Length: {number of bytes transferred in this request} Content-Range: bytes {starting...[<a href="/2016/10/14/including-the-twitter-object-nest-api-as-a-file-upload-api-example/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/13/the-open-skills-api-from-dept-of-labor-amp-university-of-chicago/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_12_at_11.19.25_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/13/the-open-skills-api-from-dept-of-labor-amp-university-of-chicago/">The Open Skills API From Dept of Labor &amp; University of Chicago</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>
We like to talk about the API economy in this space. It is kind of the grand dream of API obsessed, that helps us articulate how big of a deal we think APIs are going to be. We know APIs are going to be big, but in reality, the impact most APIs make don't size up. This is one of the reasons I'm such a big support of APIs in the public sector, as the potential for a positive impact tends to be greater in my opinion.
One of the APIs that has the potential to contribute at the API economy scale is out of a partnership between the Department of Labor and the University of Chicago--the DataAtWork Open Skills API. Providing "a complete and standard data store for canonical and emerging skills, knowledge, abilities, tools, technologies, and how they relate to jobs". Opening up a pretty useful API from their collaborative work to "map the DNA or genome of the U.S. labor market."
The Open Skills API uses Swagger UI for the documentation, which always makes me happy because it means there is an OpenAPI Spec behind, for use in Postman, APIMATIC, and other API solutions. It's a simple, open API, that has some serious potential for use in web, and mobile apps, as well as visualizations, analysis, and other types of applications--making it worthy to add the APIs in my list that I think could actually make an impact at API economy scale.
[<a href="/2016/10/13/the-open-skills-api-from-dept-of-labor-amp-university-of-chicago/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/13/saving-and-versioning-api-definitions-in-editor-using-github-gists/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_12_at_6.28.48_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/13/saving-and-versioning-api-definitions-in-editor-using-github-gists/">Saving and Versioning API Definitions In Editor Using Github Gists</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>
My friend Jordan Walsh (@jordwalsh) just released a new take on the Swagger editor, that inches closer to my vision of a dream API sketchbook and portfolio. His&nbsp;swagger-gist.io tool allows you to open and save your API definitions to Github Gists, allowing you to use the snippet sharing solution to manage your API definitions, and their evolution.
While it isn't my entire vision for an API sketchbook and portfolio, swagger-gist.io's usage of Github Gists is a move in the right direction. This is just the first draft of his&nbsp;tool, and it looks like he plans on building in more of the API definition management features I am looking for--leveraging Github Gists as the book, in my sketchbook definition. #Creative
I like this model, especially when it comes to collaboration and storytelling around the API design process. I could see offering more sharing features for API definitions within the editor, enabling you to email, Slack, and share throughout an API's life cycle. I can also see more copy and paste opportunities, embedding API definitions using Github Gists in blog, knowledge base, and forum posts--grabbing the embed code from within the editor.
I'm curious to see where Jordan takes it. I have lots of ideas, but will just keep an eye on his work. My only critique at the moment is to not couple the functionality too tightly with the word "Swagger", as that is a trademarked product. I recommend relying on "OpenAPI Spec" or even better, some other way of identifying Gists that contain an OpenAPI Spec definition. ;-)
Cool stuff Jordan, keep up the good work.
[<a href="/2016/10/13/saving-and-versioning-api-definitions-in-editor-using-github-gists/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/13/preserving-the-sunlight-on-github/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/ok_800.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/13/preserving-the-sunlight-on-github/">Preserving The Sunlight On Github</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>
I'm following along as the Sunlight Foundation winds down their operations and gathering any lessons along the way, that can help&nbsp;us open data and transparency folks can learn from as we do our work. I wrote earlier that we should be learning from the Sunlight Foundation situation and that we are making sure we bake transparency into our projects&nbsp;and wanted to continue to extract wisdom we can reuse as they turn out the lights.
The Sunlight Foundation shared that they are working with the Internet Archive and Github to preserve their projects, and that they are "trying to ensure the open source community can understand and use our projects in the future" by:

adding documentation
standardizing licenses&nbsp;
scrubbing sensitive info

This is the benefits of being transparent and open by default is that you tend&nbsp;to do all of this in real-time. If you are in the business of opening up data, making it accessible with open APIs, you should be using Github, documenting, and telling the story as you go along. Then you do not have to do it all when you are walking away--everything is open by default.
This is why I work out in the open on Github each day, it allows people to take my tools like the CSV Converter, and the API Stack, and put them to work, even as I'm still evolving them. When I step away from a project, it can continue to live on with all the code, definitions, schema, data, licensing, and the story behind in a nice forkable package--no extra work necessary.
Anyways, just a couple of nuggets to consider as we are working on open data and API project across the space.

[<a href="/2016/10/13/preserving-the-sunlight-on-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/13/keeping-api-communications-in-shape-with-workbench-blogging/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-stretching.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/13/keeping-api-communications-in-shape-with-workbench-blogging/">Keeping API Communications In Shape With Workbench Blogging</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>I consider about 75% of the content I create on my network of sites to be workbench blogging--where I tell the story of what I am working on each day. You can see this approach in action with my friend&nbsp;Guillaume Laforge (@glaforge) over at&nbsp;Google, with his post on&nbsp;a day in the life of a Developer Advocate for Google Cloud Platform. Guillaume is workbench blogging, pulling back the curtain on API operations a little bit, while also keeping API communications flowing. This type of blogging isn't about any specific API, feature, or products and services. Workbench blogging really isn't about people learning any particular thing, they are more about pulling back the curtain, humanizing API operations, generating a little SEO, while also keeping the communication pipes open. The more you write, the easier it is to craft valuable stories. Not everyone will be reading these stories, but they are great for collecting your thoughts&nbsp;and even communicate internally with other stakeholders. You know how many times I've used my blog to recall what I worked on last week, last month, and last year? I know this type of storytelling isn't for everyone, but if you want to be able to create quality content you need practice. Writing regularly is much easier when you write regularly. It helps to have several different areas to write in, allowing you to avoid writer's block by shifting gears to a new topic or just blogging about what was accomplished that day. People always ask how I am able to generate so much content for the blog(s), and staying in shape with workbench blogging is how I do it.I consider about 75% of the content I create on my network of sites to be workbench blogging--where I tell the story of what I am working on each day. You can see this approach in action with my friend&nbsp;Guillaume Laforge (@glaforge) over at&nbsp;Google, with his post on a day in the life of...[<a href="/2016/10/13/keeping-api-communications-in-shape-with-workbench-blogging/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/13/drones-and-other-devices-having-their-own-software-defined-networks/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-drone-signal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/13/drones-and-other-devices-having-their-own-software-defined-networks/">Drones And Other Devices Having Their Own Software Defined Networks</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>I was learning about&nbsp;Verizon starting to sell wireless data plans for drones in the Wall Street Journal, as part of my research on what could be a drone API stack. As an Internet of Things (IoT)&nbsp;concept, I find drones fascinating because they can have so many dimensions of APIs at play. The drone, its camera, battery, and other components can have APIs. They can publish video to Youtube, Facebook via APIs. They can receive real-time updates about weather, fire, infrastructure, and other changing events via APIs. This post is about thinking through about network API layer when it comes to drones. If we are going to be rolling out networks for specific devices like drones, automobiles, and other IoT devices, it seems like every object should have an API, including the underlying network itself. As I wade through the different companies in my network API research and learn about the myriad of ways web APIs are being exposed at the network level, I am left thinking about the programmability of the network in service of specific devices like this, and what this will mean for net neutrality. Let's take that to another level. What happens when drones can be programmed to fly around and either be the network, or program the network? This is what captivates&nbsp;me about drones and APIs. It all breaks me free from thinking about APIs from just a provider and consumer dimensions&nbsp;and blurs all the lines. I know that Verizon is just reselling its existing network to a new type of device ($$$), but I think the potential with rolling out networks for IoT devices will be in making sure it is also programmable, along with every object that is connected. All of this opens up some really interesting security concerns&nbsp;when you have so many dimensions of the physical and virtual world possessing APIs. Drones with APIs, flying around providing and consuming APIs, redefining its own network with an API--it is...[<a href="/2016/10/13/drones-and-other-devices-having-their-own-software-defined-networks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/12/we-understimated-the-time-it-would-take-for-hypermedia-to-be-absorbed/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-hourglass.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/12/we-understimated-the-time-it-would-take-for-hypermedia-to-be-absorbed/">We Understimated The Time It Would Take For Hypermedia To Be Absorbed</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>
While I still see a steady uptick in the number of hypermedia APIs out there in the wild, as well as conversations around the different media types that are available, I think we severely underestimated the time it would take for the average API developer to absorb and accept the concept. When you are immersed in any of the leading formats, from HAL to Siren, and you have the aha moment about why hypermedia makes sense, it can be easy to think everyone will see the future like we do. When in reality, I just don't think people are always seeking the wisdom in the same way, they are often just looking to get the job done.
It takes a lot of work to become hypermedia literate. An investment, not everyone can afford. While I am seeing more APIs employ hypermedia I have not seen an increase in the tooling and definitions we need to help developers speed up their understanding--providing examples they can reverse engineer. Siren is my hypermedia format of choice&nbsp;and I found that the&nbsp;TV and streaming video API platform Wurl&nbsp;gave me a strong example to reverse engineer&nbsp;and learn from in my own journey.
Maybe not everyone learns like I do, but I can't help but feel like people need more common examples to learn from. I will spend some time going through the hypermedia APIs I've included in my research, and generalize some of the design patterns that are present and publish them as simple examples on Github. I need to refresh my own hypermedia skills, as I dive back into my subway map API design, which uses Siren as an enabler of the journey. It will also give me some good stories here on the blog--hypermedia is always an evergreen driver of users, as they Google for things in their hypermedia journey.
[<a href="/2016/10/12/we-understimated-the-time-it-would-take-for-hypermedia-to-be-absorbed/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/12/the-white-house-wants-our-thoughts-on-data-portability/"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/tool/white-house-seal.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/12/the-white-house-wants-our-thoughts-on-data-portability/">The White House Wants Our Thoughts On Data Portability</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>
The White House is looking for our thoughts on data portability. While it is the U.S. federal government asking for our thoughts, something that could apply to our tax returns, veterans records, or student loan information. They seem to be most interested in what data portability means to us as consumers, or via many online services today--as the product.
Here is what they are looking for:
The Office of Science and Technology Policy (OSTP) is interested in understanding the benefits and drawbacks of increased data portability as well as potential policy avenues to achieve greater data portability. The views of the American people, including stakeholders such as consumers, academic and industry researchers, and private companies, are important to inform an understanding of these questions.
They want our input in 5000 words or less by&nbsp;November 23, 2016. I am going to gather my own thoughts on data portability, and publish to API Evangelist, as well as submit via their RFI form. This is an extremely important topic&nbsp;and one I'm glad to see the White House picking up, and fingers crossed, will be moving forward in a consumer-centered way.
While I feel government open data is a critical piece of the puzzle, I think data literacy amongst consumer is just as critical. Data is what is fueling the tech sector, and in most scenarios, it is our personal data. The more control we have over this data, the better off we will be. The more opportunities we have to store locally, and migrate where we need to, the more data literate we will all be.
Anyways, I'll save my thoughts for my own response for the White House data portability RFI. I'd love to hear your thoughts on the subject, if you want to post to your blog, or share with me via email.
[<a href="/2016/10/12/the-white-house-wants-our-thoughts-on-data-portability/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/12/some-key-iot-security-considerations/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/acti_e77_10mp_od_dome_with_1027471.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/12/some-key-iot-security-considerations/">Some Key IoT Security Considerations</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>I am continuing to learn from folks studying the recent DDOS attack on Krebs on Security. While not a straightforward API story, it overlaps with the API world in several ways, from the technical aspects of how the IoT devices were hacked and enlisted in the bot army, to how the hack has been analyzed online, and the sharing of machine-readable details of the attack. There were some interesting nuggets from the attack analysis I wanted to include in my wider Internet of Things (IoT) research. When it came to the security lapses in the surveillance cameras, printers, and other devices used as part of the DDOS, there were several key areas in play: Username / Password - Default passwords for devices aren't changed, or the settings of unique passwords are not enforced. This makes it pretty easy to get right into the most common devices, after learning the default configuration. Universal Plug and Play (UPnP) - Increased usage of UPnP for the discovery and default opening up of networks to support device communication and monitoring. Often doing so without much consideration for security, and the bigger picture, serving the specific needs of a company and a single device. Telnet / SSH - Even if one changes the password on the device&rsquo;s Web interface, the same default credentials may still allow remote users to log in to the device using telnet and/or SSH--providing a separate doorway for manufacturers, developers, and hackers to gain control over a device. Firmware Upgrades - A device may have had a patch released, but the actual firmware for a device has not actually been updated. The ease of installation, discover and access to the Internet, allows devices to be more easily installed by average folks unaware of wider security concerns, and the fact that devices&nbsp;will need to be updated regularly. While none of these areas speak directly to APIs, they definitely speak to some of the similar ways in which...[<a href="/2016/10/12/some-key-iot-security-considerations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/12/do-you-have-agencies-ready-to-develop-with-your-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/google_developer_agency_program.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/12/do-you-have-agencies-ready-to-develop-with-your-api/">Do You Have Agencies Ready To Develop With Your API?</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>
I was doing some research on how API providers are providing certification of their developers. I want to better understand how leading API providers are developing curriculum for certifying that developers have the skills needed to integrate with their API, and how API providers are also showcasing these certified developers.
As I was looking through various API programs I came across Google's Developers Agency Program, where they certify companies as competent to work with Google's APIs when developing Android applications. Google provides an eBook to help provide agencies with the resources they need to get certified, the details on how to get certified, as well as a page showcasing the agencies that have been certified. I came across the program through a press release from one of the agencies, tooting their horn about being a Google certified agency.

Google's agency certification got me thinking about how this can apply to the average API provider. While API providers might not have the resources to provide an agency certification program like Google does, it might make sense to at least establish relationships with a handful of agencies, and showcase them as part of API operations. Having some pre-certified, ready to go, agency level development resources that can tackle API integrations seems like a good idea for any size API provider.
I've had conversations with agencies before, discussing which APIs they put to use, and they tend to be either commerce related, or social integrations with Facebook, Twitter, and Instagram. This is something I'd like to explore further. I would also like to continue looking for examples of how API providers certify and showcase development resources, whether they are individual, or agency focused like Google is doing. I'm thinking that investment in API expertise at the agency level is one of the areas that will need a lot more attention as the world of APIs continues to expand and evolve.
[<a href="/2016/10/12/do-you-have-agencies-ready-to-develop-with-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/12/apis-driving-augmented-reality-for-drones/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/base_drone_screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/12/apis-driving-augmented-reality-for-drones/">APIs Driving Augmented Reality For Drones</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>Now that I have API Evangelist fired back up I am spending more time with my drones, working to understand the role APIs can play in the booming industry. I have been studying how companies like Airmap are working to be API brokers for the drone industry, and how government open data and APIs are being injected into the personal and commercial drone experience. I want to continue this exploration, and learn more about how companies are injecting data, content, and other algorithmic resources into this experience, and brainstorm some other approaches to augmenting the world of drones with API resources. As part of my regular monitoring of the space, I am tuned into many of the examples of how drones are being used. Many of them are pretty unrealistic, but some of them have real world usage, like&nbsp;disaster relief. Think about the potential when you being to feed in vital data about local infrastructure pulled from city open data, or maybe household data from census data. This is where augmenting the drone experience begins to get interesting for me, going beyond just telling me where I can or cannot fly. I fly a DJI Phantom 3 Professional drone (I am eyeballing that Mavic), and my drone experience is primarily through the DJI Go iPad application, making API integration a pretty straightforward concept. My flight plan, and the latitude, longitude, elevation, and direction is shared via APIs in real-time (if there is an Internet connection), and I can share live-stream of the video to Youtube and Facebook via APIs. I also get regular updates about the flight condition, warnings of airports, forest fires, and other activity via the app--this is just the beginning.&nbsp; I want to explore what is next for APIs, based on what I am seeing emerge across the drone sector. I'm taking a series of screenshots of the DJI Go app and turn it into a transparent augmented reality template that I...[<a href="/2016/10/12/apis-driving-augmented-reality-for-drones/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/11/some-api-embeddable-best-practices-out-of-yelp/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_10_at_8.08.25_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/11/some-api-embeddable-best-practices-out-of-yelp/">Some API Embeddable Best Practices Out Of Yelp</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>Yelp has shared some of the wisdom behind how they design, deploy, and operate their embeddable reviews. I like it when leading API providers share the story behind their tooling like this. This type of storytelling generates SEO for their API, educates their API consumers, and provides educational resources for other API providers (and content for analysts like me). So, what makes for a good embeddable widget, according to Yelp? a minimal amount of HTML code&nbsp; a consistent &amp; responsive design stay up-to-date with contextual information load fast &amp; gracefully handle traffic spikes record accurate analytics Yelp shares a little bit about the technical approach to achieving their definition of a good embeddable widget, which "are served as Yelp pages within iframes, adhere to the Yelp Styleguide, and show the most up-to-date review": iframe embeds allow for a simpler widgets.js iframe embeds make development easier iframe embeds can take advantage of HTML caching I've heard about lightening the load for your JavaScript, and the ease of embeddability before, from other providers. I hadn't thought about the cache-ability of using iframes before. It makes sense, allowing the user's browser to carry more of the load. They close up the post with some more interesting insight into the architectural decisions behind their embeddable(s): The embed HTML snippet consists of unstyled and empty elements so that the HTML snippet is minimal and durable. We use a controller loaded via script tag to create and load iframes. The controller consists of pure JavaScript and doesn&rsquo;t use any libraries. The controller communicates with the iframes using postMessage. We use a Google Analytics iframe served by Yelp to handle sending events for multiple review widget iframes on a single page. Resources served by Yelp, such as the controller and the iframes are either cached or served via CDN. The use of Google Analytics is another interesting aspect that I hadn't considered. It gets me thinking about what other examples are there...[<a href="/2016/10/11/some-api-embeddable-best-practices-out-of-yelp/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/11/ipaas-in-your-browser-with-push-by-zapier/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/zapier_push_icon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/11/ipaas-in-your-browser-with-push-by-zapier/">iPaaS In Your Browser With Push By Zapier</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>Zapier is up to more good things with the launch of Push by Zapier, allowing you to trigger API driven events from your browser. The new Chrome browser extension lets anyone, even non-developers to trigger the functionality of over 700 apps from the browser toolbar--further expanding the definition of how APIs can be put to work. Allowing users to trigger API functionality from the browser adds an empowering dimension to the API conversation for non-developers. It allows the average user to access the features of any SaaS platform with an API, in their default environment--the browser. It allows the user to define, and queue up the API driven events that matter to them, where they operate the most. Zapier gave non-developers access to orchestrate the integration between the platforms they depend on, and Push by Zapier gives them even more granular level control over this world.&nbsp; Some of the API driven events Zapier highlights in their release are: Add Tasks to Your To-Do List Send an Email or SMS to a Specific Person Crunch Numbers and Calculate Payments Copy Data from an Unsupported Site or App and Add It to a Zapier Workflow Create Invoices with a Click Send Documents to be Signed Create a New Project or Folder Impersonate a Slack Bot Look Up Data and Use It in a Workflow Get Details from Your CRM Delivered Anywhere Translate Text Send Sales and Onboarding Emails to Potential or New Customers Set a Reminder for Yourself Create a Templated Document Push a Button to Build a Report or Pull Statistics All very business-centric, API-driven functionality. That is what I like about what Zapier enables. They provide meaningful API-driven functionality for the average business user, and Push just gives them another way to define what this functionality is, and when it gets executed. This transcends&nbsp;just the features and functionality that each API platform offers, it also is about end-users being API literate, and aware that they...[<a href="/2016/10/11/ipaas-in-your-browser-with-push-by-zapier/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/11/i-am-stuck-on-the-datadog-integration-page/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_10_at_10.23.45_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/11/i-am-stuck-on-the-datadog-integration-page/">I Am Stuck On The Datadog Integration Page</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>
I wrote about having an integrations page for your API service the other day, and as I'm continuing to study the approach of other providers I find myself stuck on&nbsp;b DataDog's integration page.&nbsp;Datadog provides the monitoring layer across many of the top&nbsp;service providers in the space, making for a pretty stellar list of what solutions are being put to use across the sector.&nbsp;
The Datadog integration page has been open in my browser for the last week, as I make way through each provider. Some of them I'm very familiar with, but others are entirely new to me. Integration pages like this show me what is possible with a service provider like Datadog, but also provides me an opportunity to learn about new services that I can put to use in my own operations, and what the cool kids are using.
When you click on the detail for each of the potential integrations you get more information about what is possible, as well as a configuration file in YAML, and what metrics are made available when monitoring is activated. Datadog even groups their integrations by a tag, something they don't expose via the user interface very well, but is something I'll include in my suggestions for crafting an integration page for any other API.&nbsp;
An integration page is definitely a building block I will be suggesting to other API providers, and I am also a big fan of sharing configuration, and other integration details like Datadog does. There are infinite learning opportunities available on this type of API integration pages, for analysts like me, for API providers, as well as service providers who sell to API providers.&nbsp;These are the types of common API building blocks that I feel contribute in a positive way to the tech sector, reflecting&nbsp;what APIs do best--enabling API integration, and API literacy all in the same motion.
[<a href="/2016/10/11/i-am-stuck-on-the-datadog-integration-page/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/11/expanding-on-the-3rd-party-analysis-of-security-threats/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-cybersecurity-2.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/11/expanding-on-the-3rd-party-analysis-of-security-threats/">Expanding On The 3rd Party Analysis Of Security Threats</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>I was learning from the Splunk's analysis of the Mirai Botnet, which was behind the massive attack against Krebs on Security, implemented via common Internet of Things devices like security cameras, and printers. I've been reading several of these types of security event analysis, which is something I think is extremely important in helping the industry deal with the increasing number of security events that are occurring across the online landscape.&nbsp; The sharing of log files from compromised systems in this way is super important. We need as many eyes as we can get on these attacks, helping analyze what happened, and maybe possibly who was behind it. Of course, there&nbsp;are some scenarios where you might want to be cautious in opening up this data to the general public, but using common approaches to API deployment and management, this can be managed sensibly--while also adding another logging layer to the conversation, keeping track of who joins participates in the analysis.&nbsp; At a minimum, the DNS, application, and server logs should be made available via Github, leveraging it's Git core, as well as the Github API as part of the evaluation and analysis of the attack information. Ideally, key aspects of the data, attack vector, and other elements should also be added to some sort of shared API infrastructure for continued community security threat analysis. In addition to the growing number of attacks, and analysis by leading analysts like Splunk, I'm also seeing increased discussion around the sharing of threat data in a standardized way--APIs can act as a distributed engine for this operation. I learn a lot from the analysis&nbsp;that occurs on security events like this. I know that other security analysts learn from this as well. With digital security being such a critical issue, right along with environmental events like hurricanes, or health care concerns like the Zika virus, I'm suggesting that APIs be employed in a standardized way. We should have a common...[<a href="/2016/10/11/expanding-on-the-3rd-party-analysis-of-security-threats/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/11/api-embeddables-with-skills-and-intent/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-embeddable.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/11/api-embeddables-with-skills-and-intent/">API Embeddables With Skills and Intent</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>I am seeing some renewed interest and discussion around API driven embeddable(s)--an area of my API research that has been going on for years, focusing on buttons, badges, and widgets, but is something that I'm seeing continued investment in from API providers lately. To help fuel the innovation that is already occurring, I figured I'd contribute with my API thoughts extracted from across the bot and voice API landscape. As I monitor the bot community growing out of the Slack platform, the voice API integration emerging from the Alexa development community, and read news about Google's latest push into the space, I'm thinking about how APIs are being used to define the intents, skills, and actions that are&nbsp;driving these bot and voice implementations. I am also processing this intersection with the latest release of Push by Zapier. All of this about delivering the meaningful API responses, to where the end users desire--in their browser, their chat, or voice enablement in the business and home. While processing the wisdom shared by Yelp about their deployment of embeddable reviews, I'm thinking about how these embeddable JavaScript widgets can be used to further allow users to quantify the intent, discover the skills, and achieve the action they are looking for. How can API providers, and the savvy API developers make valuable API resources accessible to users on their terms, and in the client they desire. For example, I might need to know my availability next Thursday while talking to my Amazon Echo, engaging in a Slack conversation, or possibly filling out a form on my corporate network--in all these scenarios I will need API access to the calendar(s) that I depend on, in the way that is required in each unique situation. I am not always the biggest fan of voice and bot enabled scenarios, but I do think they provide us with some interesting constraints&nbsp;on API design. I'm hoping that some of these constraints can further be...[<a href="/2016/10/11/api-embeddables-with-skills-and-intent/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/10/the-internet-of-things-shows-us-how-regulatory-beasts-are-created/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-frankenstein-beast.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/10/the-internet-of-things-shows-us-how-regulatory-beasts-are-created/">The Internet of Things Shows Us How Regulatory Beasts Are Created</a></h3>
			<p><em>10 Oct 2016</em></p>
			<p>I am watching the world of Internet of Things (IoT) unfold, not because I'm a big fan of it, but more because I'm concerned that it is happening, and often worried that much of it is happening without any focus on security, and privacy. As I look at this week's stories in my API IoT bucket I can't help but think that IoT is a live demonstration of how the regulatory beasts, that we love to hate on in America, are created. It starts with a bunch of fast moving, greedy, corner cutting capitalists who are innovating and all that shit. These are not always the first wave of movers in a space, but usually the second and third waves of opportunists with one thing in mind--making some money. These are the companies that are so focused on revenue and profits they ignore things like security, and they see the data generated being key to their success, and concepts around privacy often do not even exist--it's the new oil motha fuckkers! As the number of security and privacy events increase, things like the unprecedented attack on Krebs on Security, the calls for a fix will only&nbsp;grow. Eventually, these calls for help are heard by the government, if they are negatively impacting enough well to do white folk, and the government steps up to figure out what to do. Often times, these investigative forces aren't fully up to speed on the area they are investigating, but with the resources they have, they'll usually inflict some regulatory and legal response. If there are any existing companies with a strong lobbying presence, the immediate response will be significantly watered down, making it more of a nuisance than anything else. This is when the market voice begins its complaining about the government overstepping its responsibilities, and stepping in to throw a wet blanket on business. The government is bad. Regulation is bad. Then we repeat, rinse, and go about...[<a href="/2016/10/10/the-internet-of-things-shows-us-how-regulatory-beasts-are-created/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/10/opportunity-for-someone-to-help-organize-auto-industry-data/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-car-mechanic.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/10/opportunity-for-someone-to-help-organize-auto-industry-data/">Opportunity For Someone To Help Organize Auto Industry Data</a></h3>
			<p><em>10 Oct 2016</em></p>
			<p>
There is a lot of data coming out of the automobile industry. I was just reading about Udacity open sources an additional 183GB of driving data and the global public registry of electric vehicle charging locations with 42K+ listings, providing us with two examples from the wild. I'm seeing an increasing number of these stories about institutions, government agencies, and the private sector making automobile related data available in this way--pointing to a pretty big opportunity when it comes to aggregating this valuable "data exhaust" (pun intended) in a coherent&nbsp;way.
Whether its self-driving, electric, car share, rental or otherwise, the modern automobile is generating a lot of data. There are some significant ways in which the automobile industry is being expanded upon, and the need to understand, and become more aware using data are immense. Making API access for this public, and private data will be increasingly important.&nbsp;
There is a number of different interests producing this data, and they aren't always immediately thinking about sharing, and reuse of the data, let alone making sure there is standardized APIs and data schema at play. Opening up a pretty big opportunity for someone to focus on aggregating all these emerging automobile datasets, make available via a unified API, and helping define a common set of API definitions and schema for accessing this valuable data exhaust from the industry.
I do not think automobile industry data aggregation is the next VC fundable idea, but I to think that with some hard work, and the slow build of some expertise in this fast moving area, an individual, or small group of folks could do very well. I know from dabbling in this area that the auto industry, the department of transportation, and the aftermarket product and service providers don't always see eye to eye, and a neutral, 3rd party aggregator&nbsp;and evangelist has the potential to make a significant impact.
[<a href="/2016/10/10/opportunity-for-someone-to-help-organize-auto-industry-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/10/google-shares-insight-on-how-to-improve-upon-the-api-experience/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/google_improve_your_api_experience.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/10/google-shares-insight-on-how-to-improve-upon-the-api-experience/">Google Shares Insight On How To Improve Upon The API Experience</a></h3>
			<p><em>10 Oct 2016</em></p>
			<p>We all like it when the API providers we depend on make using their APIs easier to put to work. I also like it when API providers also share the story behind how they are making their APIs easier to use because it gives me material for a story, but more importantly it provides examples that other API providers can consider as part of their own operations. Google recently shared some of the improvements they have made to help make our API experience better--here are some of the key takeaways: Faster, more flexible key generation - Making this step simpler, by reducing the old multi-step process with a single click. Streamlined getting started flow -&nbsp;Introduced an in-flow credential set up procedure directly embedded within the developer documentation. An API Dashboard - To easily view usage and quotas, so you can view all the APIs you&rsquo;re using along with usage, error and latency data. If you spend any time-consuming APIs you know that these areas represent the common friction many of us API developers experience regularly. It is nice to see Google addressing these areas of friction, as well as sharing their story with the rest of us, providing us all a reminder of how we can cut off these sharp corners in our own operations. These areas represent what I'd say are the two biggest pain points with getting up and going using an API, and the API dashboard represents the biggest pain point we face once we are up and running--where do we stand with our API consumption, within the rate limits provided by the platform. If you use a modern API management platform you probably have a dashboard solution in place, but for API providers who have hand-rolled their own solution, this continues to be a big problem area. While some of the historical Google API experiences have left us API consumers desiring more (Google Translate, Google+, Web Search), they have over 100...[<a href="/2016/10/10/google-shares-insight-on-how-to-improve-upon-the-api-experience/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/10/embrace-extend-and-exterminate-in-the-world-of-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-danger-shaky.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/10/embrace-extend-and-exterminate-in-the-world-of-apis/">Embrace, Extend, and Exterminate In The World Of APIs</a></h3>
			<p><em>10 Oct 2016</em></p>
			<p>I am regularly reminded in my world as the API Evangelist that things are rarely ever what they seem on the surface. Meaning that what a company actually does, and what a company says it does are rarely in sync. This is one of the reasons I like APIs, is they often give a more honest look at what a company does (or does not do), potentially cutting through the bullshit of marketing. It would be nice if companies were straight up about their intentions, and relied upon building better products, offering more valuable services, but many companies prefer being aggressive, misleading their customers, and in some cases an entire industry. I'm reminded of this fact once again while reading a post on software backward compatibility, undocumented APIs and importance of history, which provided a stark example of it in action from the past: &ldquo;Embrace, extend, and extinguish&ldquo;,[1]&nbsp;also known as &ldquo;Embrace, extend, and exterminate&ldquo;,[2]&nbsp;is a phrase that the&nbsp;U.S. Department of Justice&nbsp;found[3]&nbsp;that was used internally by&nbsp;Microsoft[4]&nbsp;to describe its strategy for entering product categories involving widely used standards, extending those standards with&nbsp;proprietary&nbsp;capabilities, and then using those differences to disadvantage its competitors. This behavior is one of the contributing factors to why the most recent generation(s) of developers are so adverse to standards&nbsp;and is behavior that exists within current open API and open source efforts. From experience, I would emphasize that the more a company feels the need to say they are open source, or open API, the more likely they are indulging in this type of behavior. It is like,&nbsp;some sort of subconscious response, like the dishonest person needing to state that they are being honest, or that you need to believe them--we are open, trust us. I am not writing this post as some attempt to remind us that Microsoft is bad--this isn't at all about Microsoft. It is simply to remind us that this behavior has existed in the past, and it exists right now....[<a href="/2016/10/10/embrace-extend-and-exterminate-in-the-world-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/07/the-anatomy-of-api-call-failure/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/ct3y_zqviaaggk5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/07/the-anatomy-of-api-call-failure/">The Anatomy Of API Call Failure</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>I have been&nbsp;spending time thinking about how we can build in fault tolerance, and change resiliency into our API SDKs, and client code. I want to better understand what is necessary to develop the best possible integrations as possible. While doing my regular monitoring this week I came across a Tweet from @Runscope, with a pretty interesting image on this subject crafted by @realm, a mobile platform for sync.

There is a wealth of building blocks here to apply at the client and SDK level, helping us achieve more fault tolerance, and make our applications, systems, and device integrations more change resilient. I wanted to break them out, providing a bulleted list I could include in my research:


Is the API Online?
Did the server receive the request?
Was URL request successful?
Did the request timeout?
Was there a server error?
Was JSON receive successfully?
Was JSON malformed?
Was there an unexpected response?
Were we&nbsp;able to map to JSON successfully?
Is the JSON valid?
Does local model match server model?

There are some valuable nuggets present in this diagram. It should be crafted into some sort of algorithmic template that developers can apply when developing their API integrations, as well as for API providers when developing the SDK and client solutions they make available to their API communities. I'm taking note so that next time I spend some cycles on my API SDK research I can help solidify my own definition.
This is a very micro look at fault-tolerance when it comes to API integration, and I'm continuing to look for other examples of change resiliency at this layer. Meaning, is there a plan B for the API call? Is there revenue ceiling considerations? Or other more non-technical, business and political considerations that should be baked into the code as well. Helping us all think more deeply around how we encourage change resiliency across&nbsp;the API community.
[<a href="/2016/10/07/the-anatomy-of-api-call-failure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/07/regulatory-api-monitoring-for-validating-algorithmic-assertions/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-check-black-round.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/07/regulatory-api-monitoring-for-validating-algorithmic-assertions/">Regulatory API Monitoring For Validating Algorithmic Assertions</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>
As I was learning about behavior driven development (BDD) and test driven development (TDD) this week, I quickly found myself applying this way of thought to my existing API regulation, and algorithmic transparency research. BDD and TDD are both used by API developers to ensure APIs are doing what they are supposed to, in development, QA, and production environments. There is no reason that this line of thought can't be elevated beyond just development groups to other business units, up to a wider industry level, or possibly employed by regulators to validate data or algorithmic solutions.
I am not a huge fan of government regulation, but I am a fan of algorithms doing what is being promised, and APIs plus BDD and TDD testing is one way that we can accomplish this. Similar to how the federal government is working together to define OAuth scopes which help&nbsp;sets the bar for how a user&nbsp;data is accessed, BDD assertion templates can be defined, shared, and validated within regulated industries.
Right now we are just focused at the very local level when it comes to API assertions. With time I'm hoping an API assertion template format will emerge (maybe already something out there), and I'm hoping that we evolve ways for allowing the average business user to be part of defining and validating API assertions. I know my friends over at Restlet are working towards this with their DHC client solution, which provides testing solutions.&nbsp;
BDD, TDD, and API assertions still very much exist in the technical environments where APIs are born&nbsp;and managed. I'm hoping to help define the space, identify opportunities for establishing common patterns&nbsp;while encouraging more reuse of leading patterns. Like other layers of the API economy, I am hoping that API assertions will expand beyond just the technical, and enjoy use amongst business groups, including industry leaders, and government regulators when it applies.
[<a href="/2016/10/07/regulatory-api-monitoring-for-validating-algorithmic-assertions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/07/harmonizing-api-definitions-across-government-with-the-u-s-data-federation/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/usdf_logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/07/harmonizing-api-definitions-across-government-with-the-u-s-data-federation/">Harmonizing API Definitions Across Government With The U.S. Data Federation</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>Sharing of API definitions is critical to any industry or public sector where APIs are being put to work. If the API sector is going to scale effectively, it needs to be reusing common patterns, something that many API and open data providers have not been that great at historically. While this is critical in any business sector, there is no single area where this needs to happen more urgently than within the public sector. I have spent years trying wade through the volumes of open data that comes out of government, and even spent a period of time doing this in DC for the White House. The lack of open API definition formats like OpenAPISpec, API Blueprint, APIs.json, and JSON Schema across government is a passion of mine, so I'm very pleased to the&nbsp;new US Data Federation project coming out of the General Services Administration (GSA). "The U.S. Data Federation supports data interoperability and harmonization across Federal, state, and local government agencies by highlighting common data formats, API specifications, and metadata vocabularies." The&nbsp;U.S. Data Federation has focused in on some of the existing patterns that exist in service of the public sector, including seven existing initiatives: Building &amp; Land Development Specification National Information Exchange Model Open Referral Open311 Project Open Data Schema.org The Voting Information Project I am a big supporter of Open Referral, Open311, Project Open data, and Schema.org. I will step up and get more familiar&nbsp;with the building &amp; land development specification, national information exchange model, and the voting information projects. The US Data Federal project echoes the work I've been doing with Environmental Protection Agency (EPA) Envirofacts Data Service API, Department of Labor APIs, FAFSA API, and my general Adopta.Agency efforts. Defining the current inventory of government APIs and open data using OpenAPI Spec, and indexing the with APIs.json&nbsp;is how we do the hard work of identifying the common patterns that are already in place&nbsp;and being used by agencies on the...[<a href="/2016/10/07/harmonizing-api-definitions-across-government-with-the-u-s-data-federation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/07/hacking-on-amazon-alexa-with-aws-lambda-and-apis-at-apistrat/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_06_at_12.30.23_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/07/hacking-on-amazon-alexa-with-aws-lambda-and-apis-at-apistrat/">Hacking on Amazon Alexa with AWS Lambda and APIs At @APIStrat</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>
I'm neck deep in studying how Amazon is operating their Alexa platform, so I'm pretty excited about the chance to listen&nbsp;and learn from the Alexa team at APIStrat in Boston. Even if you aren't building voice-enabled applications, the approach to developing, managing, and evangelizing the Alexa platform provides a wealth of best practices that we should all strive to emulate in our own operations.
Rob McCauley (@RobMcCauley) from the Amazon Alexa team is doing a workshop, as well as a keynote at @APIStrat in Boston next month. This is relevant to what is going on in the wider space because voice-enablement is a fast-moving layer when it comes to delivering API resources, helping define what is being dubbed as the conversational interface movement, while also providing the best practices for a modern API strategy that I mentioned above.
There are a number of things that the Alexa team does which have captured my attention, including their approach to developing skills, their investment ($$) into their developers, and their overall communication strategy. I'm working on profiling all of this as part of what I call a blueprint reports, where I map out the approach of the Alexa team in a way that other API providers can put to work in their own operations.
I'm thinking I will have to wait until after @APIStrat to finish my blueprint report, as I'd like to attend the Alexa workshop, hear his keynote, and possibly even talk to him personally about their approach, in the hallway. I hope to see you there, and hear you share your story, even if you aren't on the stage at APIStrat, the hallways tend to be a great place to listen to the story of leaders from across the space,&nbsp;as well as share your own--no matter how big or small you might be.
Make sure you get registered for APIStrat before it is sold out, and I'll see you there!
[<a href="/2016/10/07/hacking-on-amazon-alexa-with-aws-lambda-and-apis-at-apistrat/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/07/amazon-launches-their-own-qa-solution-called-aws-answers/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/aws_answers_splat_1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/07/amazon-launches-their-own-qa-solution-called-aws-answers/">Amazon Launches Their Own QA Solution Called AWS Answers</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>
Amazon launched their own questions and answers site called simply called AWS Answers. Amazon is definitely in a class of their own, but I thought the move reflects illnesses in the wider QA space&nbsp;and an approach that smaller API providers might want to consider for their operations.
Quora doesn't have an API, so why would we use as a QA solution for the API space? I don't care how much network they have. While Stack Overflow is a wealth of API related questions and answers, the environment has been found to be pretty toxic for some API providers. Making hand rolling your own QA site a more interesting option.
AWS answers is&nbsp;a pretty basic implementation&nbsp;but also has a wealth of valuable content. it wouldn't take much to handroll your own FAQ or wider answers solution&nbsp;within your API developer portal. I can understand why AWS would do their own, to help ensure their users are able to find the answers they need, without leaving the AWS platform. It depends on the type of platform you are operating, but keeping QA local might make more sense than using 3rd party solutions--allowing for more precise control over the answers your customers receive.
As I work to expand my API portal definition beyond just the minimum version, I'm adding a FAQ solution to the stack, and now I'm going to consider adding a separate answers solution modeled after AWS Answers. While I think platforms like Stack Overflow and Quora will continue to do well, I'm more interested in supporting API providers to roll their own solution, maybe even provide an API, and allow for more interoperability, and control over their own resources.
[<a href="/2016/10/07/amazon-launches-their-own-qa-solution-called-aws-answers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/06/your-southwest-airlines-flight-has-an-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/14500451_10157473672265368_6769931295752399398_o.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/06/your-southwest-airlines-flight-has-an-api/">Your Southwest Airlines Flight Has An API</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>A friend of mine messaged me this photo of the Southwest Airlines flight API on Facebook the other day. After doing a little homework I found that every flight has this available on the planes local network. There is a pretty interesting write up on it from Roger Parks&nbsp;if you care to learn more. Looking through the response it has all the information you need for your flight update screen. It might seem scary for folks like us poking around the network on airplanes looking for things like this, but this is just the nature of the Internet and something any network operator should consider as normal. The API is available at&nbsp;getconnected.southwestwifi.com/current.json when you are on the planes local network, and I'd consult Roger's post if you want more details about how to sniff it out using your browser. Anytime I am on a guest network on a plane or in a hotel, I enjoy turning on my Charles Proxy to log a list of all the domains and IP address in use. This is a good way to learn about how people are architecting their networks, and delivering their resources to web, mobile, and device users. The problem with this activity is that sometimes you can discover things that you shouldn't. A line that I worry about a lot. I feel pretty strongly that if companies are using public DNS, or opening up their private network to the public, they should be aware that this is going to happen. I hope that someday this type of behavior is embraced by companies, institutions, and government agencies. Not everyone will have good intentions like I do, but network operators should know this will happen, and make the those of us where white hats welcome, so that we will report insecure infrstructure,&nbsp;and help&nbsp;keep things locked down--before the bad guys get in. Thanks to my friend Jason for pinging me with this. From reading up on it, it...[<a href="/2016/10/06/your-southwest-airlines-flight-has-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/06/providing-inline-api-documentation-within-your-saas-user-interface/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/cloudflare_dns_api_inline_1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/06/providing-inline-api-documentation-within-your-saas-user-interface/">Providing Inline API Documentation Within Your SaaS User Interface</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>The common approach to discovering that a SaaS provider has an API is through a single, external link in the footer of a website, simply labeled API or developers. Whenever I can I'm on the lookout for evolutionary approaches to making users aware of an API, and I just found a good one over at CloudFlare. When you are logged into CloudFlare managing your DNS, right below the area for adding, editing, and deleting DNS records you are given some extra options, including expandable access to your API--down in the right-hand corner, between Advanced and Help. Once you click on the API option, you are given a listing of DNS record related API endpoints, allowing me to bake the same functionality available to me in the CloudFlare UI, into my own systems and application. A summary, path, and verb is provided for each relevant API, with a link to the full API documentation. I really like this approach. It is a great way to make APIs more accessible to the muggles (thanks @CaseySoftware). It is also a great way to think about connecting UI functionality to the (hopefully) API behind. Imagine if every UI element had an API link in the corner to see the API behind, and a link to its documentation . You could even display the request and response bodies for the API call made by the UI, allowing people to easily reverse engineer what an API does.&nbsp; I have suggested this approach at several events, and to other API technologists who felt it was a bad idea, as the user doesn't want to be bothered by the details of why something does what it does, they just want it to be done. I disagree. I strongly believe that this is an extension of old school beliefs by the IT wizards, that the muggles aren't smart enough, and IT should have all the power (one ring and all that). Seriously, though. There...[<a href="/2016/10/06/providing-inline-api-documentation-within-your-saas-user-interface/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/06/an-auditing-api-for-checking-in-on-api-client-activity/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/google_auditing.gif" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/06/an-auditing-api-for-checking-in-on-api-client-activity/">An Auditing API For Checking In On API Client Activity</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>Google just released a mobile audit solution for their Google Apps Unlimited users&nbsp;looking to monitor activity across iOS and Android devices. At first look, the concept didn't strike me as anything I should write about, but once I got to thinking about how the concept applies beyond mobile to IoT, and the potentially for external 3rd party auditing of API and endpoint consumption--it stood out as a pattern I'd like to have in the filing cabinet for future reference. Using the Google Admin SDK Reports&nbsp;API you can access mobile audit information by users, device, or by auditing&nbsp;event. API responses include details about the device including model, serial numbers, user emails, and any other element that included as part of device inventory. This model seems like it could easily be adapted to IoT devices, bot and voice clients. One aspect that stood out for me as a pattern I'd like to see emulated elsewhere, is the ability to verify that all of your deployed devices are running the latest security updates. After the recent IoT launched DDOS attack on Krebs on Security, I would suggest that the security camera industry needs to consider implementing an audit API, with the ability to check for camera device security updates. Another area that caught my attention was their mention that "mobile administrators have been asking for is a way to take proactive actions on devices without requiring manual intervention." Meaning you could automate certain events, turning off, or limiting access to specific API resources. When you open this up to IoT devices, I can envision many benefits depending on the type of device in play. There are two dimensions of this story for me. That you can have these audit events apply to potentially any client that is consuming API resources, as well as the fact that you can access this data in real time, or on a scheduled basis via an API. With a little webhook action involved,...[<a href="/2016/10/06/an-auditing-api-for-checking-in-on-api-client-activity/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/06/adding-behaviordriven-development-assertions-to-my-api-research/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-check3.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/06/adding-behaviordriven-development-assertions-to-my-api-research/">Adding Behavior-Driven Development Assertions To My API Research</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>I was going through Chai, a behavior, and test driven assertion library, and spending some time learning about behavior driven development, or BDD, as it applies to APIs today. This is one of the topics I've read about and listened to talks from people I look up to, but just haven't had the time to invest too many cycles in learning more. As I do with other interesting, and applicable areas, I'm going to add as a research area, which will force me to bump it up in priority. In short, BDD is how you test to make sure an API is doing what is expected of it. It is how the smart API providers are testing their APIs, during development, and production to make sure they are delivering on their contract. Doing what I do, I started going through the leading approaches to BDD with APIs, and came up with these solutions: Chai -&nbsp;A BDD / TDD assertion library for&nbsp;node&nbsp;and the browser that can be delightfully paired with any javascript testing framework. Jasmine - A&nbsp;behavior-driven development framework for testing JavaScript code. It does not depend on any other JavaScript frameworks.&nbsp; Mocha -&nbsp;Mocha is a feature-rich JavaScript test framework running on&nbsp;Node.js&nbsp;and in the browser, making asynchronous testing&nbsp;simple&nbsp;and&nbsp;fun. Nightwatch.js -&nbsp;Nightwatch.js is an easy to use Node.js based End-to-End (E2E) testing solution for browser based apps and websites.&nbsp; Fluent Assertions -&nbsp;Fluent Assertions is a set of .NET extension methods that allow you to more naturally specify the expected outcome of a TDD or BDD-style test. Vows -&nbsp;Asynchronous behaviour driven development for Node. Unexpectd -&nbsp;The extensible BDD assertion toolkit If you know of any that I'm missing, please let me know. I will establish a research project, add them to it, and get to work monitoring what they are up to, and better track on the finer aspects of BDD. As I was searching on the topic I also came across these references that I think are worth...[<a href="/2016/10/06/adding-behaviordriven-development-assertions-to-my-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/06/a-machine-readable-jekyll-jig-for-each-area-of-my-api-research/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-puzzle-piece-gear.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/06/a-machine-readable-jekyll-jig-for-each-area-of-my-api-research/">A Machine Readable Jekyll Jig For Each Area Of My API Research</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>I have over 70 areas of research occurring right now as part of my API lifecycle work--these are areas that I feel directly impact how APIs are provided and consumed today. Each of these areas lives as a Github repository, using Github Pages as the front-end of the research.&nbsp; I use Github for managing my research because of its capabilities for managing not just code, but also machine readable data formats like JSON, CSV, and YAML. I'm not just trying to understand each area of the API lifecycle, I am working to actually map it out in a machine readable way.&nbsp; This process takes a lot of effort, and is always work in progress. To help me manage the workload I rely on Github, the Github API, and Github Pages. On top of this&nbsp;Github base, I leverage the data and content capabilities of Jekyll when you run it on Github Pages (or any other Jekyll enabled server or cloud service).&nbsp; Each of my research areas begins with me curating news from across space, then I profile companies and individuals who are doing interesting things with APIs, and the services, tooling, and APIs they are developing. I process all of this information on a weekly basis and publish to each of my research projects as its YAML core.&nbsp; An example of this can be seen with my API monitoring research (the most up to date) with the following machine-readable&nbsp;components: Master APIs.json&nbsp; - Each project has a YAML APIs.json&nbsp;(I know, I know) core, with a dynamically generated JSON version in the root of site, providing an index of all the companies and APIs included in this research.&nbsp; Individual APIs.json - In addition to the central project APIs.JSON file, there are individual APIs.JSON files for each company included in my research, which includes OpenAPI Specs for all APIs I have indexed. Blog Atom - There is an atom feed for the blog, showing any posts I write on...[<a href="/2016/10/06/a-machine-readable-jekyll-jig-for-each-area-of-my-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/05/where-is-the-wordpress-for-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-wordpress.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/05/where-is-the-wordpress-for-apis/">Where Is The WordPress For APIs?</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>
I feel like I have said this before, but probably is something that is worth refreshing--where is the WordPress for APIs? First, I know WordPress has an API, that isn't what I'm talking about. Second, I know WordPress is not our best foot forward when it comes to the web. What I am talking about is a ready to go API deployment solutions in a variety of areas, that are as easy to deploy and manage as WordPress.
There is a reason WordPress is as popular as it is. I do not run WordPress for any of my infrastructure, but I do help others setup&nbsp;and operate their own WordPress installs from time to time. I get why people like it. I personally think its a nightmare in there, when you start having to make it do things as a programmer, but I fully grasp why others dig it, and willing to support that whenever I can.
I want the same type of enabling solution for APIs. If you want a link API -- here you go. If you want a product API -- download over here. There should be a wealth of open source solutions that you can just download, unzip, upload, and go through the wizard. You get the API&nbsp;and a simple management interface. I would get to work building one in PHP / MySQL just to piss all the real programmers off, but I have too many projects on my plate already.
If you want to develop the WordPress of APIs for the community and make it push-button deployment via Heroku, AWS, Google, or Azure, please let me know and I'm happy to help amplify. ;-)
[<a href="/2016/10/05/where-is-the-wordpress-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/05/the-web-evolved-under-different-environment-than-web-apis-are/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-api-evolution.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/05/the-web-evolved-under-different-environment-than-web-apis-are/">The Web Evolved Under Different Environment Than Web APIs Are</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>I get the argument from hypermedia and linked data practitioners that we need to model our web API behavior on the web. It makes sense, and I agree that we need to be baking hypermedia into our API design practices. What I have trouble with is the fact that the web is a cornerstone that we should be modeling&nbsp;it after. I do not know what web y'all use every day, but the one I use, and harvest regularly is quite often is a pretty broken thing. It just feels like we overlooking so much to support this one story. I'm not saying that hypermedia principles don't apply because the web is shit, I'm just saying maybe it isn't as convincing of an anchor to build a story that currently web APIs are shit. I understand that you want to sell your case, and trust me...I want you to sell your case, but using this argument just does not pencil out for me. There is another aspect of this that I find difficult. That the web was developed and took root in a very different environment than web APIs are. We had more time and space to be more thoughtful about the web, and I do not think we have that luxury with web APIs. The stakes are higher, the competition is greater, and the incentives for doing it thoughtfully really do not exist in the startup environment that has taken hold. We can't be condemning API designers and architects for serving their current master (or can we?).&nbsp; While I will keep using core web concepts and specs to help guide my views on designing, defining, and deploying my web APIs, I'm going to explore other ways to articulate why we should be putting them to use. I'm going to also be considering the success or failure of these elements based on the shortcomings of the web, and web APIs,&nbsp;while I work to better polish the...[<a href="/2016/10/05/the-web-evolved-under-different-environment-than-web-apis-are/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/05/github-as-the-api-life-cycle-engine/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/undefined/bw-api-engine.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/05/github-as-the-api-life-cycle-engine/">Github As The API Life Cycle Engine</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>
I am playing around with some new features from the SDK generation as a service provider APIMATIC, including the ability to deploy my SDKs to Github. This is just many of the ways Github, and more importantly Git is being used as what I'd consider as an engine in the API economy. Deploying your SDKs is nothing new, but when your autogenerating SDKs from API definitions, deploying to Github and then using that to drive deployment, virtualization, containers, serverless, documentation, testing, and other stops along the API life cycle--it is pretty significant.
Increasingly we are publishing API definitions to Github, the server side code that serves up an API, the Docker image for deploying and scaling our APIs, the documentation that tell us what an API does, the tests that validate our continuous integration, as well as the clients and SDKs. I've been long advocating for use of Github as part of API operations, but with the growth in the number of APIs we are designing, deploying, and managing--Github definitely seems like the progressive way forward for API operations.
I will keep tracking on which service providers allow for importing from Github, as well as publishing to Github--whether its definitions, server images, configuration, or code. As these features continue to become available in these companies APIs I predict we will see the pace of continuous integration and API orchestration dramatically pick up. As we are more easily able to automate the importing and exporting of essential definitions, configurations, and the code that makes our businesses and organizations function.
[<a href="/2016/10/05/github-as-the-api-life-cycle-engine/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/05/evolving-the-api-sdk-with-apimatic-dx-kits/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/apimatic_dx_kits.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/05/evolving-the-api-sdk-with-apimatic-dx-kits/">Evolving The API SDK With APIMATIC DX Kits</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>I've been a big supporter of APIMATIC since they started, so I'm happy to see them continuing to evolve their approach to delivering SDKs&nbsp;using machine readable API definitions. I got a walkthrough of their new DX Kits the other day, something that feels like an evolutionary step for SDKs, and contributing to API providers making onboarding and integration as frictionless as possible for developers. Let's walk through what APIMATIC already does, then I'll talk more about some of the evolutionary steps they are taking when auto-generating SDKs. It helps to see the big picture of where APIMATIC fits into the larger API lifecycle to assist you in getting beyond any notion of them simply being just an SDK generation service. API DefinitionsWhat makes APIMATIC such an important service, in my opinion, is that they just don't speak using modern API definition formats, they speak in all of the API definition formats, allowing anyone to generate SDKs from the specification of your choice:&nbsp; API Blueprint Swagger 1.0 - 1.2 Swagger 2.0 JSON Swagger 2.0 YAML WADL - W3C 2009 Google Discovery RAML 0.8 I/O Docs - Mashery HAR 1.2 Postman Collection APIMATIC Format As any serious API service provider should do be doing, APIMATIC then opened up their API definition transformation solution as a standalone service and API. This allows this type ofAPI &nbsp;transformations to occur&nbsp;and be baked in, at every stop along a modern API lifecycle,&nbsp;by anyone. API DesignBeing so API definition driven focused, APIMATIC needed a practical way to manage API definitions, and allow their customers to add, edit, delete, and manipulate the definitions that would be driving the SDK auto generation process. APIMATIC provides one of the best API design interfaces I've found across the API service providers that I monitor, allowing customers to manage: Endpoints Models Test Cases Errors Because APIMATIC is so heavily invested in having a complete API definition, one that it will result in a successful SDK, they've had...[<a href="/2016/10/05/evolving-the-api-sdk-with-apimatic-dx-kits/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/05/considering-a-web-api-ecosystem-through-featurebased-reuse/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_04_at_11.24.39_am.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/05/considering-a-web-api-ecosystem-through-featurebased-reuse/">Considering A Web API Ecosystem Through Feature-Based Reuse</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>I recently carved out some time to read&nbsp;A Web API ecosystem through feature-based reuse by Ruben Verborgh (@RubenVerborgh) and Michel Dumontier. It is a lengthy, very academic proposal on how we can address the fact that "the current Web API landscape does not scale well: every API requires its own hardcoded clients in an unusually short-lived, tightly coupled relationship of highly subjective quality." I highly recommend reading their proposal, as there are a lot of very useful patterns and suggestions in there that you can put to use in your operations. The paper centers around the notion that the web has succeeded because we were able to better consider interface reuse, and were able to identify the most effective patterns using analytics, and pointing out that there really is no equivalent to web analytics for measuring an APIs effectiveness.&nbsp; In order to evolve Web API design from an art into a discipline with measurable outcomes, we propose an ecosystem of reusable interaction patterns similar to those on the human Web, and a task-driven method of measuring those. To help address these challenges in the world of web APIs, Verborgh and Dumontier propose that we work to build web interfaces, similar to what we do with the web, employing a bottom-up to composing reusable features such as full-text search, auto-complete, file uploads,&nbsp;etc.--in order to unlock the benefits of bottom-up interfaces, they propose 5 interface design principles: Web APIs consist of features that implement a common interface Web APIs partition their interface to maximize feature reuse. Web API responses advertise the presence of each relevant feature Each feature describes its own invocation and functionality. The impact a feature on a Web API should be measured across implementations. They provide us with a pretty well thought out vision involving implementations and frameworks, and the sharing of documentation, while universally applying metrics for being able to identify the successful patterns. It provides us with a compelling, "feature-based method to...[<a href="/2016/10/05/considering-a-web-api-ecosystem-through-featurebased-reuse/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/04/please-share-your-openapi-specs-so-i-can-use-across-the-api-life-cycle/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_01_at_12.43.12_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/04/please-share-your-openapi-specs-so-i-can-use-across-the-api-life-cycle/">Please Share Your OpenAPI Specs So I Can Use Across The API Life Cycle</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>I was profiling the New Relic API, and while I was pleased to find OpenAPI Specs behind their explorer, I was less than pleased to have to reverse engineer their docs to get at their API definitions. It is pretty easy to open up my Google Chrome Developer Tools and grab the URLs for each OpenAPI Spec, but you know what would be easier? If you just provided me a link to them in your documentation!

Your API definitions aren't just driving the API documentation on your website. They are being used across the API life cycle. I am using them fire up and playing with your API in Postman, generating SDKs using APIMATIC, or creating a development sandbox so I do not have to develop against your&nbsp;live environment. Please do not hide your API definitions, bring them out of the shadow of your API documentation and give me a link I can click on--one click access to a machine-readable definition of the value your API delivers.
I'm sure my regular readers are getting sick of hearing about this, but the reality of my readers is that they are a diverse, and busy group of folks and will most likely not read every post on this important subject. If you have read a previous post on this subject from me, and are reading this latest one, and still do not have API definitions&nbsp;or prominent links--then shame on you for not making your API more accessible and usable...because isn't that what this is all about?
[<a href="/2016/10/04/please-share-your-openapi-specs-so-i-can-use-across-the-api-life-cycle/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/04/making-data-serve-humans-through-api-design/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-waiter.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/04/making-data-serve-humans-through-api-design/">Making Data Serve Humans Through API Design</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>APIs can help make technology better serve us humans&nbsp;when you execute them thoughtfully. This is one of the main reasons I kicked off API Evangelist in 2010. I know that many of my technologist friends like to dismiss me in this area, but this is more about their refusal to give up the power they possess&nbsp;than it is ever about APIs. I have been working professionally with databases since the 1980s, and have seen the many ways in which data and power go together, and how technology is used as smoke and mirrors as opposed to serving human beings. One of the ways people keep data for themselves is to make it seem big, complicated, and only something a specific group of people (white men with beards (wizards)) can possibly&nbsp;make work. There is a great excerpt from a story by Sara M. Watson (@smwat), called Data is the New &ldquo;___&rdquo;&nbsp;that sums up this for me: The dominant industrial metaphors for data do not privilege the position of the individual. Instead, they take power away from the person to which the data refers and give it to those who have the tools to analyze and interpret data. Data then becomes obscured, specialized, and distanced. We need a new framing of a personal, embodied relationship to data. Embodied metaphors have the potential to bring big data back down to a human scale and ground data in lived experience, which in turn, will help to advance the public&rsquo;s investment, interpretation, and understanding of our relationship to our data. DATA IS A MIRROR portrays data as something to reflect on and as a technology for seeing ourselves as others see us. But, like mirrors, data can be distorted, and can drive dysmorphic thought. This is API for me. The desire to invest, interpret, and understand our relationship to our data is API design. This is why I believe in the potential of APIs, even if the reality of it...[<a href="/2016/10/04/making-data-serve-humans-through-api-design/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/04/increased-analytics-at-the-api-client-and-sdk-level/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/aws_coding_analytics.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/04/increased-analytics-at-the-api-client-and-sdk-level/">Increased Analytics At The API Client And SDK Level</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>I am seeing more examples of analytics at the API client and SDK level, providing more access to what is going on at this layer of the API stack. I'm seeing API providers build them into the analytics they provider for API consumers, and more analytic services from providers for the web, mobile, and device endpoints. Many companies are selling these features in the name of awareness, but in most cases, I'm guessing it is about adding another point of data generation which can then be monetized (IoT is a gold rush!). As I do, I wanted to step back from this movement&nbsp;and look at it from many different dimensions, broken down into two distinct buckets: Positive(s) More information - More data than can be analyzed More awareness - We will have visibility across integrations. Real-time insights - Data can be gathered on real time basis. More revenue - There will be more revenue opportunities&nbsp;here. More personalization - We can personalize the experience for each client. Fault Tolerance - There are opportunities for building in API fault tolerance. Negative(s) More information - If it isn't used it can become a liability. More latency - This layer slows down the primary objective. More code complexity - Introduces added complexity for devs. More security consideration - We just created a new exploit opportunity. More privacy concerns - There are new privacy concerns facing end-users. More regulatory concerns - In some industries, it will be under scrutiny. I can understand why we want to increase the analysis and awareness at this level of the API stack. I'm a big fan of building in resiliency in our clients &amp; SDKs, but I think we have to weigh the positive and negatives before jumping in. Sometimes I think we are too willing to introduce unnecessary&nbsp;code, data gathering, and potentially opening up security and privacy holes chasing new ways we can make money. I'm guessing it will come down to each...[<a href="/2016/10/04/increased-analytics-at-the-api-client-and-sdk-level/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/04/an-integrations-page-for-your-api-solution/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_03_at_10.44.34_am.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/04/an-integrations-page-for-your-api-solution/">An Integrations Page For Your API Solution</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>
A new way that I am discovering the new tech services that the cool kids are using is from the dedicated integrations pages of API service providers I track on. Showcasing the services your platform integrates with is a great way of educating consumers about what the possibilities are when it comes to your tools and services. It is also a great way for analysts like me to connect the dots around which services are most important to the average user.
API service providers like DataDog, OpsClarity,&nbsp;and Pingometer&nbsp;are providing dedicated integration pages showcasing the other 3rd party platforms they integrate with. Alpha API dogs like Runscope also have integration APIs, allowing you to get a list of integrations your team depends on (perfect for another story). I'm just getting going tracking on tracking the existence of these integration pages, but each time I have come across one lately I find myself stopping and looking through each of the services included.
Directly, API integrations provide a great way to inform customers about which of the other services they use can be integrated with this platform, potentially adding to the number of reasons why they might choose to go with a service. Indirectly, API integration pages provide a great way to inform the sector about which API driven platforms are important to service providers, and their customers. After I get a number of these integration pages bookmarked as part of my research, I will work on other stories showcasing the various approaches I find.

[<a href="/2016/10/04/an-integrations-page-for-your-api-solution/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/04/amazon-alexa-as-an-example-when-it-comes-to-api-communications/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/amazon_alexa_echo_dot_tap_4011.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/04/amazon-alexa-as-an-example-when-it-comes-to-api-communications/">Amazon Alexa As An Example When It Comes To API Communications</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>
I'm always looking for specific API providers to showcase as examples we can follow when crafting different portions of our API strategies. The Amazon Alexa team is doing a pretty kick ass job at blogging, and owning the conversation when it comes to developing conversational interfaces, so I thought I'd highlight them as an example to follow when planning the communications portion of your strategy.
Take a look at the #Alexa tag for the AWS blog. They have a regular stream of storytelling coming out of the platform. Its a mix of talking about the tech of the platform, and showcasing what it can do. What really captured my attention for this story is there regular showcasing of the interesting solutions developers are building on top of the platform. Many platform blogs I read are a one trick pony, just talking about their service, and I think the AWS Alexa team has found a compelling blend.
Ok, AWS probably has just a few more resources than your API team, but trust me, one person can do a lot when they are really engaged. I produce at least five posts a day (ok, they are ranty and weird), and always work to keep thing as diverse as possible, and not about my products or services (fact I don't have any probably helps as well). I do not recommend you using API Evangelist as a model for your platform blogging. I do recommend you use Amazon Alex as a model for how you can create a compelling API platform communication experience.
[<a href="/2016/10/04/amazon-alexa-as-an-example-when-it-comes-to-api-communications/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/03/the-different-reasons-behind-why-we-craft-api-definitions/"><img src="https://s3.amazonaws.com/kinlane-productions2/whitepapers/definitions/api-evangelist-api-definitions-guide-may-2016-cover.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/03/the-different-reasons-behind-why-we-craft-api-definitions/">The Different Reasons Behind Why We Craft API Definitions</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
I wrote a post about the emails I get from folks telling me the API definitions contained within my API stack research, something that has helped me better see why it is I do API definitions. I go through APIs and craft OpenAPI Specs for them because it helps me understand the value each company offers, while also helping me discover interesting APIs&nbsp;and the healthy practices behind them.
The reason I create API definitions and organize them into collections is all about discovery. While some of the APIs I will be putting to use, most of them just help me&nbsp;better understand the world of APIs&nbsp;and the value and the intent behind the companies who are doing the most interesting things in the space.
I would love it if all my API definitions were 100% certified, and included complete information about the request, response, and security models, but just having the surface area defined makes me happy. My intention is to try and provide as complete of a definition as possible, but the primary stop along the API lifecycle I'm looking to serve is discovery, with other ones like design, mocking, deployment, testing, SDKs, and others following after that.
Maybe if we can all better understand the different reasons behind why&nbsp;we all craft and maintain API definitions we can better leverage Github to help make more of them complete. For now, I'll keep working on my definitions, and if you want to contribute head over to the Github repo for my work, and share any of your own definitions, or submit an issue about which APIs you'd like to see included.
[<a href="/2016/10/03/the-different-reasons-behind-why-we-craft-api-definitions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/03/running-synthetic-data-and-content-through-your-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_synthetic_api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/03/running-synthetic-data-and-content-through-your-apis/">Running Synthetic Data And Content Through Your APIs</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
I was profiling the New Relic API and came across their Synthetics service,which is a testing and monitoring solution that lets you&nbsp;"send calls to your APIs to make sure each output and system response are successfully returned from multiple locations around the world"--pretty straight forward monitoring stuff. The name is what caught my attention, and got me thinking the data and content that we run through our APIs.
Virtualization feels like it defines the levers and gears our API-driven systems, and synthetics feels like it speaks to the data and content that flows through&nbsp;flows through these systems. It feels like everything in the API stack should be able to be virtualized, and sandboxes, including the data and content, which is the lifeblood--allowing us to test&nbsp;and monitor everything.
It also seems like another reason we'd want to share our data schemas, as well as employ common ones like schema.org, so that others can create synthetic data and content sets for variety of scenarios--then API providers could put these sets to work in testing and monitoring their operations. A sort of synthetic data and content marketplace for the growing world of API testing and monitoring.
I see that New Relic has the name Synthetics trademarked, so I'll have to play around with variations to describe the data and the content portion of my API virtualization research. I'll use virtualization to describe gears of the engine, and something along the lines of synthetic data and content to describe everything that we run through it. I am just looking for ways to better describe the different approaches I am seeing, and tell more stories about API virtualization, and sandboxing in ways that resonate with folks.
[<a href="/2016/10/03/running-synthetic-data-and-content-through-your-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/03/apis-can-give-an-honest-view-of-what-a-company-does/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_02_at_11.04.26_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/03/apis-can-give-an-honest-view-of-what-a-company-does/">APIs Can Give An Honest View Of What A Company Does</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
One of the reasons I enjoy profiling APIs is that they give an honest view of what a company does, absent of all the marketing fluff, and the promises that I see from each wave of startups. If designed right, APIs can provide a very functional, distilled down representation of data, content, and algorithmic resources of any company. Some APIs can be very fluffy and verbose, but the good ones are simple, concise, and straight to the point.
As I'm profiling the APIs for the companies included in my API monitoring research, what&nbsp;API Science, Apica, API Metrics, BMC Software, DataDog, New Relic, and Runscope offer quickly become pretty clear. A simple list of valuable resources you can put to use when monitoring your APIs. Crafting an OpenAPI Spec allows me to define each of these companies APIs, and easily articulate what it is that they do--minus all the bullshit that often comes with the businesses side of all of this.&nbsp;
I feel like the detail I include for each company in an APIs.json&nbsp;file provides a nice view of the intent behind an API, while the details I put into the OpenAPI Spec provide insight into whether or not a company actually has any value behind this intent. It can be frustrating to wade through the amount of information some providers feel they need to publish as API documentation, but it all becomes worth it once I have the distilled down OpenAPI Spec, giving an honest view of what each company does.

[<a href="/2016/10/03/apis-can-give-an-honest-view-of-what-a-company-does/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/03/a-service-level-agreement-api-for-api-service-providers/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-service-level-agreements.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/03/a-service-level-agreement-api-for-api-service-providers/">A Service Level Agreement API For API Service Providers</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
I am spending some time profiling the companies who are part of my API monitoring research, specifically learning about the APIs they offer as part of their solutions. I do this work so that I can better understand what API monitoring service providers are offering, but also for the discoveries I make along the way--this is how I keep API Evangelist populated with stories.&nbsp;
An interesting API I came across during this work was from the Site24X7 monitoring service, specifically their service level agreement (SLA) API. An API for adding, managing, and reporting against SLA's that you establish as part of the monitoring of your APIs. Providing a pretty interesting API pattern that seems like it should be part of the default API management stack for all API providers.
This would allow API providers to manage SLA's for their operations, but also potentially expose this layer for each consumer of the API, letting them understand SLA"s that are in place, and whether or not they have been met--in a way that could be seamlessly integrated with existing systems. An API for SLA management for API providers seems like it could also be a standalone operation as well, helping broker this layer of the API economy, and provide a rating system for how well API providers are holding up their end of the API contract.
[<a href="/2016/10/03/a-service-level-agreement-api-for-api-service-providers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/03/a-dedicated-security-page-for-your-api-portal/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/security__datadog.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/03/a-dedicated-security-page-for-your-api-portal/">A Dedicated Security Page For Your API Portal</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
One area I am keeping an eye on while profiling APIs, and API service providers, are any security-related practices that I can add to my research. While looking through DataDog I came across their pretty thorough security page, providing some interesting building blocks that I will add to my API security research. This is all I do as the API Evangelist--aggregate the best practices of existing providers, and shine a light on what they are up to.&nbsp;
On their security page, DataDog provides details on physical and corporate security, information about data in transit, at rest, as well as retention, including personally identifiable information (PII), and details surrounding customer data access. They also provide details of their monitoring agent and how it operates, as well as how they patch, employ SSO, and require their staff to undergo security awareness training. The important part of this is that they encourage you to disclose any security issues you find--critical for providers to encourage this.
Transparency when it comes to security practice is an important tool in our API security toolbox. It is important that API providers share their security practices like DataDog does, helping build trust, and demonstrate competency when it comes to operations. I'm working on an API security page template for my default API portal, and DataDog's approach provides me with some good elements I can add to my template.
[<a href="/2016/10/03/a-dedicated-security-page-for-your-api-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/30/you-can039t-say-ai-benefits-outweigh-risk-without-some-algorithmic-transparency/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-algorithmic-transparency-2.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/30/you-can039t-say-ai-benefits-outweigh-risk-without-some-algorithmic-transparency/">You Can&#039;t Say AI Benefits Outweigh Risk Without Some Algorithmic Transparency</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>
I am increasingly hearing the phrase, "the benefits outweigh&nbsp;the risks" applied when talking about AI, machine learning, and the increasing number of algorithmic decisions that are being made in all parts of our digital world. This seems&nbsp;to be the new default of AI and machine learning advocates looking to tip the scales in favor of their technology, over the human side of the discussion.
This can be found used in discussions about AI used in self-driving cars, all the way to policing algorithms making decisions on the street or in a court of law. I'm not opposed to this argument if it is truly the case, but it seems something you can claim without providing the data behind this decision, and simply relying on your lack of faith in humans being able to consistently making decisions.
This is why I wrote about the important of data sharing in industries where algorithms are making an impact, and I am an advocate for providing API access for journalists, analysts, and regulators to actually follow-up with claims that are being made. Allowing 3rd parties to actual weight the pros and cons, and make a collective, more fair and balanced determination of whether or not the benefits truly do outweigh&nbsp;the risk.
I'm not saying that folks who make these claims are being dishonest, but in my experience, in the API space most folks blindly believe in tech and their algorithms, and seem to have almost no faith in humans, and are more than happy to make false claims in the service of the algorithm. This is why I have to say that you can't ever tell me the benefits outweigh the risk without some algorithmic transparency involved--it just won't mean anything to me.
[<a href="/2016/09/30/you-can039t-say-ai-benefits-outweigh-risk-without-some-algorithmic-transparency/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/30/if-you-have-an-api-for-your-platform-you-are-a-stage-for-cybersecurity-theater/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-theatre.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/30/if-you-have-an-api-for-your-platform-you-are-a-stage-for-cybersecurity-theater/">If You Have An API For Your Platform You Are A Stage For Cybersecurity.Theater</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>
Adding to the many reasons you would want, or not want APIs these days, is the escalating cyber war playing out on the web around the world. APIs aren't playing a role in the cyber security realm in the way you'd think, allowing the bad guys, or even the good guys to get into systems, but they are how these actors are spreading information&nbsp;or disinformation about their cyber activities.&nbsp;
Increasingly Facebook, Twitter, Instagram, Reddit, and other API driven platforms are being used to broadcast, engage, and study the fast growing world of&nbsp;cyber security. Whether it is the Israeli Defense Force, U.S. Cyber Command, or a 15-year-old hacker in your basement, they are using these API driven channels to broadcast their message, as well as monitor the message of their adversaries, with us analysts&nbsp;following up behind trying to make sense of it all--using the same channels.&nbsp;
Moving forward if you have a platform with an API, you will have a stage for the Cybersecurity.Theater to play out. Actors will use you to tell their story, to communicate, syndicate images, publish their videos, and make their payments. This will scare the shit out of many of you, but for others, it will be an opportunity to sell popcorn&nbsp;and other concession items.&nbsp;
Since "securing cyberspace is a 24/7 responsibility&nbsp;(United States army Cyber Command and Second Army), it will need a 24/7, API driven theater to perform in.
[<a href="/2016/09/30/if-you-have-an-api-for-your-platform-you-are-a-stage-for-cybersecurity-theater/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/30/defining-a-conversational-layer-on-top-of-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-conversational-interfaces.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/30/defining-a-conversational-layer-on-top-of-apis/">Defining A Conversational Layer On Top Of APIs</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>As I am exploring, and writing about&nbsp;Meya's&nbsp;Bot Flow Markup Language (BFML), &nbsp;I came across the announcement from Google about their acquisition of API.AI, titled "Making Conversational Interfaces Easier to Build". I feel like this description reflects what I was writing about "Beyond Mobile: API Ready For iPaaS, Voice, and Bots", and sounds better to me than saying voice, bot, or integration workflow. Whether its skills for voice enablement, intents and flows for bot interactions, or triggers, actions, and integrations with iPaaS, I'm guessing we are going to need a way to define, and convey meaning through this growing conversation we'll be having using API resources. With OpenAPI Spec and API Blueprint we finally have adequate ways to describe where our data, content, and algorithmic resources reside, and a little bit about what they do, but it feels like we need a similar way of defining the conversational layer on top. I see the beginning of this present in&nbsp;Meya's&nbsp;Bot Flow Markup Language (BFML), which is a YAML definition description a flow, made of components that can each make an API call, all in the services of what they consider "intent". &nbsp;I"ll have to see how other bot providers are defining this layer, as well as learn more about how Alexa is defining the conversational layer for their skills. All of this smells like we need some Hydra injected into the conversation, but I need to do more research before I start evangelizing anything. The whole Slackbot thing is interesting to me from a technical point of view--not so much from a business side of things. Twitter bots I find intriguing because they are public, and can wreak havoc, or be very creative. Alexa is interesting from both a technical&nbsp;and business perspective for me. But, helping define a conversational layer on top of the world of APIs is intriguing to me, mostly because it continues building on top of what I consider to be one of the...[<a href="/2016/09/30/defining-a-conversational-layer-on-top-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/30/api-sdks-getting-more-specialized/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_sdk_expanding.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/30/api-sdks-getting-more-specialized/">API SDKs Getting More Specialized</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>I have been doing a lot of thinking about the client and SDK areas of my research lately, considering how these areas overlap with the world of bots, as well as with voice, and iPaaS. I'm thinking about the hand-crafted, autogenerated, and even API client as a service like Postman, and Paw. I'm thinking about how APIs are being put to work, across not just web and mobile, but also systems to system integration, and&nbsp;the skills in voice platforms like Alexa, and the intents in bot platforms like Meya. I'm considering how APIs can deliver the skills needed for the next generation of apps beyond just a mobile phone. I kicked off my SDK research over a year ago, where I track on the approaches of leading platforms who are offering up code samples, libraries, and SDKs in a variety of programming languages. While conducting my research, I've been seeing the definition of what is an SDK slowly expand and get more specialized, with most of the expansion in these areas: Mobile Development Kit - Providing code resources to help developers integrate their iPhone, Android, Windows, and other mobile&nbsp;applications with APIs. Platform Development Kits - Provide code resources for using APIs in other specific platforms like WordPress, SalesForce, and others. In addition to mobile, and specific platform solutions, I am seeing API providers stepping up and providing iPaaS options, like ClearBit is doing with their Zapier solutions. As part of this brainstorm exercise, I feel like I should also add a layer dedicated to delivering via iPaaS: Integration Platform as a Service Development Kits - Delivering code resources for use in iPaaS services like Zapier, allowing for simpler system to system integration across many different platforms, with some having a specific industry focus. Next, if I scroll down the home page of API Evangelist, I can easily spot 11 other areas of my research that stand out as either areas I'm seeing SDK movement&nbsp;or an...[<a href="/2016/09/30/api-sdks-getting-more-specialized/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/30/a-plan-b-api-switch/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-plan-b.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/30/a-plan-b-api-switch/">A Plan B API Switch</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>I've had an idea for a bot-related&nbsp;service I call "plan b", which would act as a secondary action for any sort of bot request / response to an API. When developers are providing common bot responses like looking up a business address, sports statistic or stock quote, it could be exposed to suggestions for a "plan b". When a request is made, it can travel via its regular path, but it would also be included in a queue&nbsp;where other 3rd party developers could provide plan b suggestions, either free or paid. When a user is engaging with the bot and didn't like the primary response, they could click on the "plan b" option, opening up alternative responses. In theory, the user could cycle through each "plan b" suggestion until they find a suitable response.&nbsp; Since I don't have any startup aspirations&nbsp;I enjoy&nbsp;working through these ideas on my blog as part of my wider research, I found myself thinking my Plan B bot&nbsp;idea as I was learning about Meya's&nbsp;Bot Flow Markup Language, and in the context of how we can build in resiliency into API client code. The concept of a plan B seems extremely relevant to this discussion, and worth consideration beyond just bots, into voice, iPaaS, and other clients being put to work on top of APIs. In the context of fault and change resistance, it seems like we'd have a "plan b" layer in our SDKs to deal with when an API goes away temporarily, or even permanently. I know I do not have ANY plan b in place for any of my API integrations, either directly in the SDK, or in my business strategy--I am guessing this is the case for most API integrations. It seems like responding to status codes etc could be considered fault-tolerance (micro), where a plan b option would be in the change resistance category (macro). I had pictured "plan b" being some sort of hypermedia layer that...[<a href="/2016/09/30/a-plan-b-api-switch/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/29/schemahub039s-usage-of-github-to-launch-their-api-service-is-a-nice-approach/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/schemahub_thanks.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/29/schemahub039s-usage-of-github-to-launch-their-api-service-is-a-nice-approach/">SchemaHub&#039;s Usage Of Github To Launch Their API Service Is A Nice Approach</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>I'm looking through a new API definition focused service provider called SchemaHub today, and I found their approach to using Github as a base of operations was interesting and provided a nice blueprint for other API server providers to follow. I'm continually amazed at the myriad of ways that Github can be put to use in the world of APIs, which is one of the things I love about it. As a base for SchemaHub, they created a Github Org, and made their first repository the website for the service, hosted on Github Pages. In my opinion, this is how all API services should begin, as a repo, under an organization on Github--leveraging the social coding platform as a base for their operations. SchemaHub is taking advantage of Github for hosting their API definition focused project--free, version controlled, static website hosting for schemahub.io.&nbsp; As I was looking through their site, learning about what they are doing I noticed a subscription button at the bottom of the page, asking me to subscribe, and they'll notify me when things are ready. Once I clicked on the button, I was taken for a Github OAuth dance, which now makes SchemaHub not just a Github repo for the site, it is an actual Github Application that I've authenticated with using my Github account. They only have access to my profile and email, but is the types of provider to developer connection I like to see in the API world. Once I authorize and connect I am taken to a thank you page back on their website, letting me know I will be contacted shortly with any updates about the service. Oh, and I'm offered a Twitter account as well, allowing me to stay in tune with what they are up to--providing a pretty complete picture for how new API services can operate.&nbsp; SchemaHub's approach reflects what I'm talking about when I say that Github should offer an Oauth service,...[<a href="/2016/09/29/schemahub039s-usage-of-github-to-launch-their-api-service-is-a-nice-approach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/29/flow-abstraction-and-intent-layer-on-top-of-apis-to-feed-the-bots/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/1_lfoybsgdnspy0i24b3dubg.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/29/flow-abstraction-and-intent-layer-on-top-of-apis-to-feed-the-bots/">Flow Abstraction And Intent Layer On Top Of APIs To Feed The Bots</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>I was reading an interesting post on developing bots from Meya, a bot platform provider, which I think describes the abstraction layer between what we are calling&nbsp;bots, and what we know as APIs. I have been trying to come up with a simple way of quantifying the point where bots and APIs work together,&nbsp;and Meya's approach to flow and intent provides me with a nice scaffolding. The flow step of their bot design rationale&nbsp;provides a nice way to think about how bots will work, breaking out each step of the bot interaction in plain English.&nbsp;They use a YAML format they call Bot Flow Markup lLnguage, or BFML, to describe the flows, comparing BFML to HTML, with this definition: HTML is spatial, and BFML is temporal. HTML determines&nbsp;where&nbsp;UI exists, and BFML determines&nbsp;when&nbsp;UI exists. The second part of their bot design rationale involves&nbsp;Intents, providing this additional definition: If BFML is like HTML, then intents are like URLs. According to Meya, "intents can be keywords, regular expressions, and natural language models as you get more sophisticated". This seems to be where the&nbsp;more human aspect of what is getting done here is defined, mapping each intent to a specific flow, which can execute one or many steps to potentially satisfy the intent. The third step is components, which is where the direct API connection comes clear. If you look at their example, in the component they are simply making a call to the Chuck Norris joke API, returning the results as part of the flow. Each part of the flow calls its targeted component, and each component can make a GET, POST, PUT, PATCH, or DELETE to an API that provides the data, content, or algorithm behind the component. This provides me with a beginning scaffolding to think about how bot platforms are constructing the API abstraction layer behind bot activity. I will be going through other bot platforms to understand each individual napproach. Bots to me are just...[<a href="/2016/09/29/flow-abstraction-and-intent-layer-on-top-of-apis-to-feed-the-bots/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/29/code-resiliency-lessons-in-how-twitter-deploys-their-embeddables/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/twitter_widgets_js_depoly_arch_v2.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/29/code-resiliency-lessons-in-how-twitter-deploys-their-embeddables/">Code Resiliency Lessons In How Twitter Deploys Their Embeddables</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>I am learning about how Twitter deploys their widgets. Extracting some insight for my research around how we can build change resiliency into our client code. As I'm doing my regular monitoring of the API space I am trying to keep an eye out for any examples from leading providers of how there are investing in client code being more change resilient. This Twitter blog post provides me with three concepts I wanted to&nbsp;add to my research: Reversibility:&nbsp;&lsquo;Rollback first, debug later&rsquo; is our motto. Rollback should be fast, easy, and simple. Ideally, it&rsquo;s a giant red button that can get our heart rates down. Incremental release:&nbsp;All code has bugs and deploys have an uncanny way of surfacing them. That&rsquo;s why we wanted the ability to release new code in phases. Visibility:&nbsp;We need to have graphs to show how both versions of widgets.js are doing at all times. We also need the ability to drill down by country, browser type, and widget type. These graphs should be real time so we can quickly tell how a deploy is going and take action as necessary. These are change elements that seem like they need consideration as we craft our web, mobile, device, visualization, bot, voice, and other types of API clients. These three elements should be present in the code, anywhere I'm making an API call. Being able to reverse how I'm interacting with an API, the incremental release of new API paths or changes to existing APIs, and having an analytics&nbsp;layer can contribute to helping us deal with change. I think I am going to get started with an analytics layer for my own client code. Start thinking about logging the calls I'm making to any API I depend on. I have this in place for the server side of the APIs that I manage&nbsp;but do not have any sort of logging at the client level. Not only do I not have any plan for change...[<a href="/2016/09/29/code-resiliency-lessons-in-how-twitter-deploys-their-embeddables/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/29/beyond-mobile-api-ready-for-ipaas-voice-and-bots/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/announcing_the_clearbit__zapier_integration.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/29/beyond-mobile-api-ready-for-ipaas-voice-and-bots/">Beyond Mobile: API Ready For iPaaS, Voice, and Bots</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>I enjoy being able to switch gears between all the different areas of my API research. It helps me find the interesting areas of&nbsp;overlap and potentially synchronicity&nbsp;in how APIs are being put to work. After thinking about the API abstraction layer present in Meya's&nbsp;bot platform, I was reading about Clearbit's&nbsp;iPaaS integration layer with Zapier. Zaps are just like the components employed by Meya, and Clearbit walks us through delivering intended workflows with the valuable APIs they provide, executed Zapier's iPaaS service. Whether its skills for voice, intents for bots, or triggers for iPaaS, an API is delivering the data, content, or algorithmic response required for these interactions. I've been pushing for API providers to be iPaaS ready, working with providers like Zapier for some time. I predict you'll hear find me showcasing examples of API providers sharing their voice and bot integration solutions, just like with Clearbit has with their iPaaS solutions, in the future. I would say that even before API providers think about the Internet of Things, they should be thinking more deeply about iPaaS, voice, and bots. Not that all these areas will be relevant, or valuable to your API operations, but they should be considered. If you have the resources, they might provide you with some interesting ways to make your API more accessible to non-developers--as Clearbit opens their blog post&nbsp;opening. When it comes to skills, intents, and iPaaS workflows, I am thinking we are going to have to&nbsp;be more willing to share our definitions (broken record), &nbsp;like we see Meya&nbsp;doing with their Bot&nbsp;Flow&nbsp;Markup&nbsp;Language (BFML) in YAML. I will have to do some more digging to see how Amazon is working to make Alexa Skills more shareable and reusable, as well as take another look edition of the Zapier API to understand what is possible--I took a look at it back in the spring, but will need a refresher.&nbsp; While the world of voice and bots API integration seems to be...[<a href="/2016/09/29/beyond-mobile-api-ready-for-ipaas-voice-and-bots/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/29/an-opportunity-for-a-restful-api-layer-on-top-of-new-tensorflow-models/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/tensorflow.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/29/an-opportunity-for-a-restful-api-layer-on-top-of-new-tensorflow-models/">An Opportunity For A RESTful API Layer On Top Of New TensorFlow Models</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>
I was looking the open source models available for execution via the machine learning&nbsp;platform TensorFlow, and couldn't help but think there is a pretty big opportunity for a web API layer on top of it. After a little Googling, I see there is someone asking on Stack Overflow, Google Groups, and a student project to tackle the need. Maybe there are some other projects out there already in the works, but I couldn't find anything with 10 minutes of Googling (mad skills).
Google has twelve pretty compelling machine learning models available on Github:

autoencoder&nbsp;-- various autoencoders
inception&nbsp;-- deep convolutional networks for computer vision
namignizer&nbsp;-- recognize and generate names
neural_gpu&nbsp;-- highly parallel neural computer
privacy&nbsp;-- privacy-preserving student models from multiple teachers
resnet&nbsp;-- deep and wide residual networks
slim&nbsp;-- image classification models in TF-Slim
swivel&nbsp;-- the Swivel algorithm for generating word embeddings
syntaxnet&nbsp;-- neural models of natural language syntax
textsum&nbsp;-- sequence-to-sequence with attention model for text summarization.
transformer&nbsp;-- spatial transformer network, which allows the spatial manipulation of data within the network
im2txt&nbsp;-- image-to-text neural network for image captioning.

That would make a pretty stellar machine learning API stack, with a simple, intuitive, RESTl wrapper. Once done it seems like there would also be a pretty big opportunity for containerized deployment of these machine learning APIs, on a wholesale basis. I'm still not sure how the whole open source code to commercial API implementation model will work, but I'm sure there is some money to made in there somewhere--at least when it comes to implementation and support.
I will add to the list of open source software I'd like to see have an accompanying web API, as well as containerized, or even serverless implementation. It makes me happy that Google is helping commoditize&nbsp;machine learning by open sourcing their tools, but I'd also like to see them further simplified and polished for consumption by a wider developer, or even non-developer audience, using web APIs.
[<a href="/2016/09/29/an-opportunity-for-a-restful-api-layer-on-top-of-new-tensorflow-models/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/28/we-focus-on-interacting-with-the-api-developer-community-where-they-live/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-localhost.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/28/we-focus-on-interacting-with-the-api-developer-community-where-they-live/">We Focus On Interacting With The API Developer Community Where They Live</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>Another story I harvested fro&nbsp;a story&nbsp;by Gordon Wintrob (@gwintrob) about how Twilio's distributed team solves developer&nbsp;evangelism, was about how they invest in having a distributed team, providing an on the ground presence in the top cities they are looking to reach. I know this isn't something all API providers can afford, but I still think it was still an important approach worth noting. Like with many other aspects of Twilio's approach, they are pretty genuine about why they invest in a distributed API evangelism team: We also focus on interacting with the developer community where we actually live. We don&rsquo;t think it&rsquo;s valuable to parachute into a tech community, do an event, and then leave. We need to participate in that community and make a real impact.&nbsp; I wish there was a way that smaller API providers could deliver like this. I wish we all had the resources of Twilio, but in reality, most API providers won't even be able to "parachute into a tech community", let alone have a dedicated presence there. I've seen several attempts like this fail before, so I am hesitant to say it, but I can't help but think there is an opportunity for evangelists in certain cities. There isn't any startup potential here (let me make that clear), but I think there is an opportunity for developer advocates, evangelists, and would-be evangelists to band together, network, and offer up services to API providers. All you'd have to do is take the page from the Twilio&nbsp;playbook&nbsp;and execute in a decentralized way--where multiple evangelists could work together as a co-op. The trick is to bring together evangelists who actually give a shit about the space--something that would be very difficult to accomplish. Anyways, just some more thoughts from my API notebook, inspired by Gordon's post. If nothing else, Twilio's approach should help guide other larger API providers, showing how important it is to invest in developers, in-person at the local level....[<a href="/2016/09/28/we-focus-on-interacting-with-the-api-developer-community-where-they-live/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/28/thinking-about-how-i-can-build-change-resilience-into-my-api-integrations/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-change-2.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/28/thinking-about-how-i-can-build-change-resilience-into-my-api-integrations/">Thinking About How I Can Build Change Resilience Into My API Integrations</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>After I wrote a piece on guidance from the USGS around writing fault-resistant code when putting their API to use, my friend Darrel Miller expanding on this by suggesting I include "change resilience" as part of the definition.&nbsp; @kinlane I would like to see that guidance expanded to include writing change resilient client code. &mdash; Darrel Miller (@darrel_miller) September 9, 2016 It is something that has sat in my notebook for a couple weeks, and keeps floating up as a concept I'd like to explore further. I have some initial thoughts on what this means&nbsp;but is something that I need to write about before I grasp better. Hopefully, it will bring more suggestions about what change resilient code means to other people. Ok, so off the top of my head, what elements would I consider when thinking about producing change resilient client code: Status Codes - Making sure clients read, and pay attention to HTTP status codes used by API providers. Hypermedia - Links are fragile, and avoiding baking them into clients makes a whole lotta sense.&nbsp; Plan B API - Have a backup API identified, that can be used when the A API provider goes away. Circuit Breaker - Build in a circuit breaker into code that responds to specific status codes and events. Now that I'm exploring, I have to ask, who's responsibility is it to build change resilience into the clients? Provider or consumer? Seems like there is a healthy responsibility on both parties? IDK. I guess we should just all be honest about how fragile the API space is, and providers should be honest with consumers when it comes to thinking about change resiliency, but ultimately API consumers have to begin to thinking more deeply&nbsp;and investing more when it comes planning for change--not just freaking out when it happens. I have to admit that the code I have written as part of my API monitoring system, which integrates with over 30...[<a href="/2016/09/28/thinking-about-how-i-can-build-change-resilience-into-my-api-integrations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/28/the-bot-platform-that-operates-like-alexa-will-win/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/amazon_alexa_blue.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/28/the-bot-platform-that-operates-like-alexa-will-win/">The Bot Platform That Operates Like Alexa Will Win</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>
I'm going through Amazon's approach to their Alexa voice services, and it is making me think how bot platforms out there should be following their lead when it comes crafting their own playbook. I see voice and bots in the same way that I see web and mobile--they are just one possible integration channel for APIs. They each have their own nuances of course, but as I'm going through Amazon's approach, there are quite a few lessons on how to do it correctly here--that apply to bots.&nbsp;
Amazon's approach to investment in developers on the Alexa platform and their approach to skills development&nbsp;should be replicated across the bot space. I know Slack has an investment fund, but I don't see the skills development portion present in their ecosystem. Maybe it's in there, but it's not as prominent as Amazon's approach. Someday, I envision galleries of specific voice and bot skills like we have application galleries today--the usefulness and modularity of these skills will be central to each provider's success (or failure).
I had profiled Slack's approach before I left for the summer, something I will need to update as it stands today. I will keep working on profiling Amazon's approach to Alexa, and put together both as potential playbook(s). I would like to eventually be able to lay them side by side and craft a common definition that could be applied in both vthe oice API, as well the bot API sector. I need to spend more time looking at the bot landscape, but currently I'm feeling like any bot platform that can emulate Amazon's approach is going to win at this game--like Amazon is doing with voice.
[<a href="/2016/09/28/the-bot-platform-that-operates-like-alexa-will-win/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/28/learning-about-opc-the-interoperability-standard-for-industrial-automation/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/plcopen_opcfoundation_opc_ua_diagram.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/28/learning-about-opc-the-interoperability-standard-for-industrial-automation/">Learning About OPC, The Interoperability Standard For Industrial Automation</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>I am spending a portion of my time each week learning about how APIs are being applied at the industrial level. An example of this can be found over at Opto 22, with their approach to using REST across their Programmable Automation Controllers (PAC). As I do with other industries I spend my time looking through the approaches&nbsp;of API pioneers in the space, which leads me to other contributing factors to why web APIs are being used to change how things are done in any industry. For now, my industrial API research is a pretty big umbrella, encompassing&nbsp;oil &amp; gas, manufacturing, and often moving into other areas I'm already tracking agriculture and energy. This approach allows me to identify companies who are leading the charge (like Opto 22), as well as specifications, tools, and other elements that are contributing to the evolution of APIs in each area--in this case, its broadly industrial usage of web APIs. In my researching of industrial APIs I have come across the&nbsp;OPC format which was originally known as the Object Linking and Embedding for Process Control, which is defined as: OPC is the interoperability standard for the secure and reliable exchange of data in the industrial automation space and in other industries&nbsp;The OPC standard is a series of specifications developed by industry vendors, end-users and software developers. These specifications define the interface between Clients and Servers, as well as Servers and Servers, including access to real-time data, monitoring of alarms and events, access to historical data and other applications. I'm still getting going with the world of industrial automation, but I am looking through the OPC Unified Architecture to see where I can find any common definitions and schemas that could apply to industrial API design. I don't have any sense of how open these standards bodies are with their specifications, and I don't want to end up like Carl Malumud, but I do want to help identify and encourage...[<a href="/2016/09/28/learning-about-opc-the-interoperability-standard-for-industrial-automation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/28/every-government-agency-should-have-an-faq-api-like-the-dol/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_09_27_at_9.27.34_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/28/every-government-agency-should-have-an-faq-api-like-the-dol/">Every Government Agency Should Have An FAQ API Like The DOL</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>
I wrote about my feelings that all government agencies should have a forms API like the Department of Labor (DOL), and I wanted to separately showcase their FAQ API, and say same thing--ALL government agencies should have a frequently asked question (FAQ) API. Think about the websites and mobile applications that would benefit from ALL government agencies at the federal, state, and local level having frequently asked questions available in this way--it would be huge.&nbsp;
In a perfect world, like any good API provider, government agencies should also use their FAQ API to run their website(s), mobile, and internal systems--this way the results are always fresh, up to date, and answering the relevant questions (hopefully). I get folks in government questioning the opening up of sensitive information via APIs, but making FAQs available in a machine readable way, via the web, just makes sense in a digital world.
Like the forms API, I will be looking across other government agencies for any FAQ APIs. I will be crafting an OpenAPI Spec for the DOL FAQ API (man that is a lot of acronyms). I will take any other FAQ APIs that I find and consider any additional parameters, and definitions I might want to include in a common FAQ API definition for government agencies. This is another area that should have not just a common open API and underlying schemas defined, but also a wealth of server and client side code--so any government agency can immediately put it to work in any environment.
[<a href="/2016/09/28/every-government-agency-should-have-an-faq-api-like-the-dol/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/27/thanks-for-reaching-out-about-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-email.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/27/thanks-for-reaching-out-about-your-api/">Thanks For Reaching Out About Your API</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>I get a number of folks emailing me about their API and API-focused&nbsp;services. When I have the bandwidth I spend time in my inbox and respond to these emails. To help me do this a little more efficiently (I'm not always very quick about it), I'm formalizing some snippets I can use in my response(s). I want to thank them for reaching out, while also helping them understand my approach to successfully operating API Evangelist. Here is one basic email I crafted today, in response to a pretty slick API provider that I will be writing about shortly: Hi There, I received your email. Thanks for the kind words.&nbsp;Appreciate you introducing me to your [API / API related service]. I'm going to have to pass on the posting of the [guest post, infographic, white paper, case study, etc] to apievangelist.com, but I'm happy to keep an eye on what you are up to as part of my regular work. I visited your site and see that you have a blog (with feed), Twitter, and a Github account. These are the channels I&rsquo;ll be keeping an eye on, and when you post a&nbsp;blog post or press release, Tweet something out, or I see a Github repo or commit of interest, I'll definitely include in my research, and craft a story for the blog. I have also added your company, blog, feed, twitter, and Github accounts to my monitoring system. Keep on doing interesting things with APIs and I'll make sure it becomes part of my storytelling in the space. So far, and reviewing your web site and developer, your API efforts [looked pretty polished / could use some work / is not very modern] I&rsquo;ll keep digging around and publish anything interesting that I find. Thanks! Kin Lane@kinlane This is a basic template I will use moving forward. I'll tweak it some for each response, but ultimately I am trying to keep thing consistent with folks...[<a href="/2016/09/27/thanks-for-reaching-out-about-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/27/github-needs-client-oauth-proxy-for-more-complete-clientside-apps-on-pages/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/new_personal_access_token.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/27/github-needs-client-oauth-proxy-for-more-complete-clientside-apps-on-pages/">Github Needs Client OAuth Proxy For More Complete Client-Side Apps On Pages</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>I'm building what I am calling "micro tools", that run 100% on Github. To push my work forward I developed a base template I can use for deploying apps that run 100% on Github, using Github Pages, the Github API, and Github OAuth as the engine. As a next step I wanted to develop a simple YAML editor that run on Github, allowing me to edit the YAML core of each tool, that is stored in the _data folder for each Jekyll site I host on Github Pages. The key to all of this working securely is Github personal access tokens, which every Github user has in their accounts under settings. I have employed this approach to running apps on Github Pages before using OAuth.io as the broker, something that works very well, and I highly recommend it. I have also run using my own Github OAuth proxy, where I had server side code that would do the OAuth dance for me, when authenticating via these apps. The problem is I want them to run 100% on Github, and be forkable by anyone, leaving personal access tokens as my only option. What would really rock, is if Github provide us with a solution to client-side authentication via the Github API. We can already accomplish the hole thing, we just need Github to offer the same functionality that OAuth.io -- heck I recommend you just buy them and implement. An increasing number of API providers are managing their API operations on Github. From API portal, to documentation and SDKS--they are using Github and Github Pages to take care of business. So having Github OAuth, plus authentication via other providers would be a huge benefit. Additionally, it would open up Github Pages to be more than just static project pages--they could become little mini apps, or micro tools as I call them. Forking one of my micro tools, then finding your personal access tokens is not that...[<a href="/2016/09/27/github-needs-client-oauth-proxy-for-more-complete-clientside-apps-on-pages/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/27/every-government-agency-should-have-a-forms-api-like-dol-does/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_09_26_at_4.59.29_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/27/every-government-agency-should-have-a-forms-api-like-dol-does/">Every Government Agency Should Have A Forms API Like DOL Does</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>
I was taking another look at the API efforts out of the Department of Labor (DOL), to help refresh my awareness of what they are serving up, and I came across the DOL Forms API. The API does what it says, providing access on " the most frequently requested Department of Labor forms", which seems like to me should be the&nbsp;default for ALL government agencies.
The API returns some valuable details about each agency from including OMB number, URL, file extension, file size, and other meta information like a description, tags, and revision. I know that many in the API community would like all forms to be APIs, but I would be happy if we just started by making the concept of a forms API default across all government agencies first.
Before I dig into this individual API, I'm thinking that I will craft an OpenAPI Spec for the DOL Forms API, and see if there are any other form APIs available across US federal agencies that I should be considering. With a little work maybe I can merge them into a single open API definition that any government agency can follow, when thinking about which APIs they should be making available.
[<a href="/2016/09/27/every-government-agency-should-have-a-forms-api-like-dol-does/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/27/api-design-is-not-requirement-for-all-devs-but-a-little-empathy-should-be/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-design-empathy.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/27/api-design-is-not-requirement-for-all-devs-but-a-little-empathy-should-be/">API Design Is Not Requirement For All Devs But A Little Empathy Should Be</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>My friend Matthew Reinbold wrote a great post on his blog asking "what if developers aren't meant to do API design"? I think he is touching on an important aspect of why DevOps might not work everywhere in the same expected ways. We all have our strengths, and we all have our weaknesses, and I agree with him that maybe we are asking too much of our developers--API design might not be their strength.&nbsp; As the owner of a small business operated by one person (me), DevOps is hard. I cannot do everything myself, and require a variety of services to help me out, but I still hit areas where I'm deficient like graphic design and editing. I'm getting better at editing, but my graphic design skills never seem to evolve at all. I would like to think I can do everything, but I can't, and if I had a job at a large organization, and was expected to learn every piece of a modern stack--I'm not sure I could do it. Bringing it back to the API design, though, the question of who does API design still needs to be answered. It can't just be left behind because a developer of an API doesn't have the chops. Ideally, a company could afford to hire an API designer and architect to come in and work their magic, but I know in&nbsp;reality this isn't going to happen within many organizations--so what can be done? API Design In Our Tools &amp; Services - Developers use language and platform specific&nbsp;IDE functionality as a crutch, we need the same enablement for API design in leading API design tools and services. This is why projects like the API design stylebook are so important&nbsp;because they begin to provide us with the definitions that are needed to drive the advancements. Sharing Of Common API Patterns - I feel like such a nag on this subject, but we need to share the common...[<a href="/2016/09/27/api-design-is-not-requirement-for-all-devs-but-a-little-empathy-should-be/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/27/api-access-to-your-account-by-default-but-requires-permission-to-see-others/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-access-cloud.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/27/api-access-to-your-account-by-default-but-requires-permission-to-see-others/">API Access To Your Account By Default But Requires Permission To See Others</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>I wrote about SoundCloud beginning to require approval before developers get access to any API resources&nbsp;yesterday, a concept that I want to keep exploring. I'm going to be going through the APIs track on, looking for different variations of this, but before I did this I wanted to explore a couple of approaches I already had rattling around in my head. What if, when you first sign up for API access you only get access to your own data, and content? You couldn't get access to any other users until you were approved. It seems like something that would incentivize developers to publish data and content, build their profiles out, which is good for the platform right? It will also protect other end-users from malicious activity by random developers who are just looking to wreak havoc in support of their own objectives and do not care about the platform--like we saw with Soundcloud. A good example of how this could be applied is evident in&nbsp;the post yesterday by Kris Shaffer on Medium, who was looking to get his content out of the platform. I use the Medium API to syndicate blog posts to Medium&nbsp;(POSSEE), but there is no read API allowing me to pull my content out--I agree with Kris, this is a problem. What if Medium opened up API access, allowing us platform users to get at our own content, but then required approval of any app before there ever is access to other users content? Some food for thought. I hear a lot of platforms say they don't do APIs because they don't want to end up with the same problems as Twitter. I think this is the result of some legacy views about public APIs that should just go away. Not all APIs are created equal, and I feel that APIs shouldn't always be just about applications, and often times are just a lifeline for platform users, helping us end-users better manage...[<a href="/2016/09/27/api-access-to-your-account-by-default-but-requires-permission-to-see-others/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/26/taking-another-look-at-the-department-of-labor-api-efforts/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/developer_dol_gov__united_states_department_of_labor_developer_portal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/26/taking-another-look-at-the-department-of-labor-api-efforts/">Taking Another Look At The Department Of Labor API Efforts</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>Someone asked me about the current state of the Department of Labors (DOL) API efforts the other day, and since I hadn't actually taken a look in a few months I wanted to spend some time in there seeing what they have going on. There is no better way to get a feel for what a government agency is up to than going through their API efforts--the DOL is pretty ahead of the game in this area. The vibe when you land on developer.dol.gov (which is a great subdomain) is nice. It is clean&nbsp;and has all the links that I am looking for, providing access to their APIs, as well as supporting code, while allowing you to ask questions and report bugs. One thing I think is interesting in their approach is that they efficiently use Github in support of their code, apply Stack Exchange in support of asking questions, and employ Github issues for reporting bugs. I understand that government agencies do not always have the resources necessary to support their API efforts, so this approach seems sensible to me when providing the minimum viable support for an API. The Department of Labor provides a number of APIs which provide access to some key economic data, broken down into four separate categories Health &amp; Safety&nbsp; Injuries And Illness - Rate and number of work-related injuries, illnesses, and fatal injuries. Gulf Oil Spill - Sample results for chemicals, noise, and heat stress index measurements from monitoring workers engaged in the oil spill cleanup in the Gulf of Mexico for exposure to hazardous chemicals and conditions. DOL OSHA Compliance - Fatalities and catastrophes resulting in the hospitalization of three or more workers. MSHA Employment Production - Annual summation of employee hours and coal production reported by mine operators. Mine Violation - Violations issued from 1/1/1978 to 1/1/2000 as a result of MSHA inspections. Fatal Occupational Injuries - Nonfatal and fatal data for the nation and for...[<a href="/2016/09/26/taking-another-look-at-the-department-of-labor-api-efforts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/26/my-api-definitions-are-incomplete-but-you-do-not-want-to-contribute/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-share-stack.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/26/my-api-definitions-are-incomplete-but-you-do-not-want-to-contribute/">My API Definitions Are Incomplete But You Do Not Want To Contribute</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>I am perpetually working to publish all of my API definitions my API Stack Github repository, with the front available as the API Stack. I regularly push the latest copies of all of my OpenAPI Specs to these Github repos when I have time, but my OpenAPI Specs are far from complete, and are&nbsp;something I'm always working to make as complete as a I can, and certify when possible. My primary objective around defining APIs using OpenAPI Spec is all about API discovery, and understanding the request and response surface area for many of the publicly available APIs out there today. Secondarily I'm interested in actually putting these API definitions to use across the API lifecycle in design, deployment, management, testing, virtualization, and client tools and services. It takes a lot of work to certify an API definition as complete--a process best executed by the API owner. In the absence of this, I am trying to fill in the gap(s) where I can, but as a one-person operation, I only&nbsp;have so much bandwidth. I have had people ask me if I'd focus on specific APIs they are interested in having, and even have some folks pay me for this work, but what really surprises me is the number of emails, DMs, and Github requests I get from folks telling me about how incomplete they are, and the low number of folks who ever contribute back by sharing their definitions with me. People really enjoy emailing me to tell me my API definitions are incomplete, but they rarely&nbsp;fork&nbsp;and contribute back any API definitions to my work--even though I regularly ask people if they could (at the very least submit an issue with URL to where it exists on your server). I feel like this is the general tone of the space that has been set by VC investment and unhealthy views of intellectual property defined by companies like Oracle and Google. People are perfectly happy building...[<a href="/2016/09/26/my-api-definitions-are-incomplete-but-you-do-not-want-to-contribute/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/26/helping-validate-data-and-algorithmic-sovereignty-at-the-api-layer/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_algorithmic_sovereignty.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/26/helping-validate-data-and-algorithmic-sovereignty-at-the-api-layer/">Helping Validate Data And Algorithmic Sovereignty At The API Layer</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>
I have showcased&nbsp;examples of API providers allowing you to deploy your API into various regions around the world like Algolia does, but it is a topic that I think will keep gaining traction as data, content, or algorithmic sovereignty continues to be a privacy, security, or regulatory concern. As the Internet continues to evolve, people and companies are only going to continue being concerned&nbsp;and dare I say become nationalistic about where they digital worlds exist and operate.
When I Google "data sovereignty" I get:
Data sovereignty is the concept that information which has been converted and stored in binary digital form is subject to the laws of the country in which it is located.
Where data is stored tends to dominate conversations historically, but I think that increasingly algorithmic sovereignty will become a concern as well. With more platforms employing algorithms at almost every level of operation, at some point where they are operating is going to come into play (ie. the Facebook news feed algorithm might have to have different considerations in the US than it does in EU).
The DNS layer is the first place to start when validating where data is being stored, or an algorithm is being executed, and which rules apply. After that I'd guess that we will need the API layer to help us also broker this, ensuring that web, mobile, and device based clients can articulate&nbsp;where data should be stored, and which algorithms&nbsp;or variations of algorithms can be applied.&nbsp;
With the growth in regional specific cloud storage and compute solutions this doesn't seem like it will be too difficult to implement, but I'm sure will take a significant amount of time to standardize how API providers actually get it done. I'll keep looking for examples of it in the wild, and contribute to how we can better validate and ensure data and algorithmic sovereignty at the API layer.
[<a href="/2016/09/26/helping-validate-data-and-algorithmic-sovereignty-at-the-api-layer/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/26/doing-away-with-selfservice-api-access-without-approval-like-soundcloud/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/backstage_blog__api_sign_up_changes__soundcloud_developers.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/26/doing-away-with-selfservice-api-access-without-approval-like-soundcloud/">Doing Away With Self-Service API Access Without Approval Like SoundCloud</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>
SoundCloud recently made changes to the signup process for their API and are now requiring approval before any 3rd party developer can get an API key and access the&nbsp;API. While I encourage API providers to be as open and transparent with their API portal, documentation, and other resources, I honestly can't criticize&nbsp;API providers for locking down APIs and requiring approval--especially when 3rd party developers can be so badly behaved.&nbsp;
Modern API management solutions allow for API providers to decide how open they want to be with their APIs, and while there are many benefits for having an open presence for an API portal, documentation, and other resources, I predict that many API providers will require approval before you get full access to resources in the future. Especially if it impacts the end user experience in a negative way like it was on SoundCloud.
I support SoundCloud's decision because&nbsp;they kept their overall operations public and because&nbsp;they shared the decision so transparently on their blog. This type of transparent, communicative approach is important to helping set the tone for the API community&nbsp;and prevent any backlash from developers. There really is no reason API providers have to be 100% open with the data, content, and algorithms &nbsp;thatthey are providing access to, and I think SoundCloud's approach provides a basic model that other providers can consider when they are thinking about exactly how "open" they want to be.
[<a href="/2016/09/26/doing-away-with-selfservice-api-access-without-approval-like-soundcloud/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/26/api-service-providers-please-have-a-logos-page-like-dreamfactory/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/dreamfactory_logo__google_search.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/26/api-service-providers-please-have-a-logos-page-like-dreamfactory/">API Service Providers Please Have A Logos Page Like dreamfactory</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>
I was adding dreamfactory as one of my sponsors&nbsp;today. I have them in my in my API monitoring system already, so I have a logo for them, but whenever there is a significant event involving one of the companies I keep an eye on in the API space, I tend to make sure I have an update to date version of their logo. In the case of adding them as a sponsor I definitely want the latest logo--so I did what I always do, I Googled "dreamfactory logo".
A while back&nbsp;I wrote about how companies who ask me to update their logo on my site(s) almost never have a dedicated logo page--which might have helped make sure I have had the right logo in the first place. &nbsp;So when I Googled the&nbsp;dreamfactory&nbsp;logo", I was pleased to find tthe dreamfactory logo page as the first result, with a wealth of logos for me to select from.
I can't imagine why logo and branding pages are default for all companies, but I'll focus on making sure API service providers, as well as API providers, invest in this area (by writing about whenever I can). When you open up your data, content, and algorithms to 3rd party developers it just makes sense to have logo assets easy available, allowing them to provide attribution. It also makes sense to have a logo and branding so that journalists, bloggers, and analysts like me can get the most up to date assets as possible, for&nbsp;use in our storytelling.
[<a href="/2016/09/26/api-service-providers-please-have-a-logos-page-like-dreamfactory/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/09/23/the-sharing-of-data-via-apis-will-be-key-to-viability-of-every-industry/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-information-sharing.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/09/23/the-sharing-of-data-via-apis-will-be-key-to-viability-of-every-industry/">The Sharing Of Data Via APIs Will Be Key To Viability Of Every Industry</a></h3>
			<p><em>23 Sep 2016</em></p>
			<p>
As I'm processing some guidelines around the importance of sharing data in the cybersecurity.theater a story on NPR came on the radio about the importance of data sharing when it comes to the emerging self-driving car marketing. I do API Evangelist, not to encourage everyone to do APIs and be the next Twitter, it is to help everyone understand the important of sharing machine-readable&nbsp;information in a secure and accessible way, to make all industries healthier, secure, and more viable environments for digital transformation (man I hate that phrase).
APIs are about sharing information in machine readable formats like YAML, JSON, and XML. Modern approaches to API deployment and management makes this information available in a self-service, secure way, ensuring those who should have access do, in a 24/7, always-on environment. Sharing cybersecurity threat information, and sharing of real-world and laboratory data around self-driving cars fall into this area, and APIs should be a&nbsp;default for anyone playing in these industries.
We are seeing API efforts emerge for expediting Zika virus research, by sharing machine-readable data that other researchers can put to work in their own efforts, without reinventing the wheel and duplicating work within many separate silos--we need more of this in other industries. If you need a primer&nbsp;on how APIs can be put to work in your industry feel free to reach out to me. APIs are not the next vendor solution, and if someone is telling you this, you should run the other direction. APIs are about sensibly, and securely sharing critical information by default across your organization, and industry, using low-cost web technology--not buying the next product or service.
[<a href="/2016/09/23/the-sharing-of-data-via-apis-will-be-key-to-viability-of-every-industry/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page14" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page16" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home Page</a></li>
    <li><a href="/blog/">The Blog</a></li>
    <li><a href="https://101.apievangelist.com/">API 101</a></li>
    <li><a href="http://history.apievangelist.com">History of APIs</a></li>
    <li><a href="https://women-in-tech.apievangelist.com/">Women in Technology</a></li>    
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
