<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/27/multi-region-apis-using-aws-api-gateway/">Multi-Region APIs Using AWS API Gateway</a></h3>
        <span class="post-date">27 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/amazon-api-gateway-regions.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been deploying two project using AWS API Gateway, Lambda, and Amazon RDS lately. I’ve become so sold on this approach to deploying APIs as part of this work, that I am evolving my own internal API process to use the same approach. The technical aspect of serverless plus the gateway definitely convinced me of the potential, but it was also the usage of AWS IAM which sealed the deal for me. I’m all too aware of how much my API security lacks as a one person shop, something that I also see reflected in my client operations, and I’d rather be offloading security to AWS than ending up taking the hit on it down the road.</p>

<p>While deploying my project using AWS API Gateway, and Lambda, I was faced with the question regarding which zone I should be deploying the APIs in. It is the first time I’ve been faced with the opportunity to deploy my APIs into multiple zones. Sure, I could have deployed my servers into any AWS zone before, but for some reason now that I’m doing with AWS API Gateway, and Lambda, the opportunity seemed more of a possibility. I’ve pitched it to my client to consider an east as well as a west coast API deployment, so that we can give developers the choice in the documentation to choose which availability zone they’d like to use in their application. Before I make the proposal I’m going to deploy some prototypes, and do some benchmark testing, and see what the benefits are.</p>

<p>Even if I end up publishing APIs into separate regions, I still have the backend database to content with. Where do I put the database, and how to I replicate between zones. Amazon RDS gives me the tools to tackle this, but historically I would only do this just for backup, not for actual redundancy, as well as performance gains. Amazon zones have been a staple of the cloud since early days, but I still have many clients who only operate in the east coast region. I’m thinking I will begin to push for a multi-region approach, and see if I can’t convince some of them to start thinking bigger. Getting to know where their customers are, and delivering infrastructure closer where the resources are needed.</p>

<p>I’m rolling out some new APIs as part of my own infrastructure. Most of my APIs are retail, as well as wholesale APIs. Meaning you can use the APIs I’ve published, or I’ll deploy them specifically for you, in your own AWS account. I think I’m going to make east / west an option for my retail APIs, and ALL the AWS regions an option for the wholesale APIs. I’m seeing geographical region as another consideration when I’m thinking about how I should be breaking down my APIs, into smaller more bite-size chunks. I see geographical region as a variable in the host for my APIs, just like I’d add any other variable in the path. Opening up a whole new set of possibilities when it comes to not just API deployment, but also API reliability and performance.</p>

<p>As I continue to drink the AWS Kool-Aid I’m questioning my increased dependence on the platform. However, it gets harder to deny when they bring security to the table, and increased performance, availability, and reliability to my API stack, and the APIs I’m delivering for my clients. There is no way I could afford to do APIs at scale, across multiple regions without AWS. I think back to the day when I would secure my own T1s, colo-facility, and servers. Even with the threat of vendor lock-in, I do not want to ever go back to that world. I enjoy the benefits of AWS. I’m just going to try and keep things as well defined as simple, microservice APIs, so that I can migrate to Google, Azure, or if necessary my own infrastructure. As much as I’d like to reduce my dependencies on major cloud providers, in the current online environment I just can’t. Being able to operate in any region around the globe is a pretty significant benefit to doing business online.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/27/multi-region-apis-using-aws-api-gateway/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/27/hints-of-banking-api-regulations-from-cfpb-with-consumer-authorized-financial-data-sharing-and-aggregation-rules/">Hints of Banking API Regulations From CFPB With Consumer Authorized Financial Data Sharing And Aggregation Rules</a></h3>
        <span class="post-date">27 Nov 2017</span>
        <p><a href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-outlines-principles-consumer-authorized-financial-data-sharing-and-aggregation/"><img src="https://s3.amazonaws.com/kinlane-productions/cfpb/cfpb-outlines-principles-for-consumer-authorized-financial-data-sharing-and-aggregation.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p><a href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-outlines-principles-consumer-authorized-financial-data-sharing-and-aggregation/">The Consumer Finance Protection Bureau (CFPB) has started laying out some consumer-authorized data sharing and aggregation rules to begin moving forward the banking data scraping conversation in (hopefully) a more production way</a>. It is common knowledge that many financial focused (Fintech) companies regularly access consumers account data using their credentials, so that they scrape relevant account information from their bank, for use in a wide variety of 3rd party tools. This is a common practice that everyone in the industry knows about, understands is a potential security and privacy risk, but everyone looks the other way because it adds value to the consumer ecosystem.</p>

<p>In a perfect world each bank would have a public API portal where Fintech aggregators could come and sign up for application keys, and get the authorization of users via OAuth, and obtain access to their banking data in a secure, and accountable way. However, as we are well aware, we do not live in a perfect world, and banks are pretty resistant to change, so the scraping continues. At some point we are going to see the landscape begin to shift, and I’m guessing it will be at the regulatory level where we finally begin to see this behavior changed–making the CFPB’s rules announcement a reflection of what is coming down the pipes when it comes to banking API regulation.</p>

<p><a href="http://files.consumerfinance.gov/f/documents/cfpb_consumer-protection-principles_data-aggregation.pdf">The consumer protection principles for consumer-authorized financial data sharing and aggregation</a> announcement focuses on:</p>

<ul>
  <li><strong>Access</strong> - Consumers are able, upon request, to obtain information about their ownership or use of a financial product or service from their product or service provider. Such information is made available in a timely manner. Consumers are generally able to authorize trusted third parties to obtain such information from account providers to use on behalf of consumers, for consumer benefit, and in a safe manner. Financial account agreements and terms support safe, consumer-authorized access, promote consumer interests, and do not seek to deter consumers from accessing or granting access to their account information. Access does not require consumers to share
their account credentials with third parties.</li>
  <li><strong>Data Scope and Usability</strong> - Financial data subject to consumer and consumer-authorized access may include any transaction, series of transactions, or other aspect of consumer usage; the terms of any account, such as a fee schedule; realized consumer costs, such as fees or interest paid;
and realized consumer benefits, such as interest earned or rewards. Information is made available in forms that are readily usable by consumers and consumer-authorized third parties. Third parties with authorized access only access the data necessary to provide the product(s) or service(s) selected by the consumer and only maintain such data as long as necessary.</li>
  <li><strong>Control and Informed Consent</strong> - Consumers can enhance their financial lives when they control information regarding their accounts or use of financial services. Authorized terms of access, storage, use, and disposal are fully and effectively disclosed to the consumer, understood by the consumer, not overly broad, and consistent with the consumer’s reasonable expectations in light of the product(s) or service(s) selected by the consumer. Terms of data access include access frequency, data scope, and retention period. Consumers are not coerced into granting third-party access. Consumers understand data sharing revocation terms and can readily and simply revoke authorizations to access, use, or store data. Revocations are implemented by providers in a timely and effective manner, and at the discretion of the consumer, provide for third parties to delete personally identifiable information.</li>
  <li><strong>Authorizing Payments</strong> - Authorized data access, in and of itself, is not payment authorization. Product or service providers that access information and initiate payments obtain separate and distinct consumer authorizations for these separate activities. Providers that access information and initiate payments may reasonably require consumers to supply both forms of authorization to obtain services.</li>
  <li><strong>Security</strong> - Consumer data are accessed, stored, used, and distributed securely. Consumer data are maintained in a manner and in formats that deter and protect against security breaches and prevent harm to consumers. Access credentials are similarly secured. All parties that access, store, transmit, or dispose of data use strong protections and effective processes to mitigate the risks of, detect, promptly respond to, and resolve and remedy data breaches, transmission errors, unauthorized access, and fraud, and transmit data only to third parties that also have such protections and processes. Security practices adapt effectively to new threats.</li>
  <li><strong>Access Transparency</strong>  - Consumers are informed of, or can readily ascertain, which third parties that they have authorized are accessing or using information regarding the consumers’ accounts or other consumer use of financial services. The identity and security of each such party, the data they access, their use of such data, and the frequency at which they access the data is reasonably ascertainable to the consumer throughout the period that the data are accessed, used, or stored.</li>
  <li><strong>Accuracy</strong> - Consumers can expect the data they access or authorize others to access or use to be
accurate and current. Consumers have reasonable means to dispute and resolve data inaccuracies, regardless of how or where inaccuracies arise.</li>
  <li><strong>Ability to Dispute and Resolve Unauthorized Access</strong> - Consumers have reasonable and practical means to dispute and resolve instances of unauthorized access and data sharing, unauthorized payments conducted in connection with or as a result of either authorized or unauthorized data sharing access, and failures to comply with other obligations, including the terms of consumer authorizations. Consumers are not required to identify the party or parties who gained or enabled
unauthorized access to receive appropriate remediation. Parties responsible for unauthorized access are held accountable for the consequences of such access.</li>
  <li><strong>Efficient and Effective Accountability Mechanisms</strong> -  The goals and incentives of parties that grant access to, access, use, store, redistribute, and dispose of consumer data align to enable safe consumer access and deter misuse. Commercial participants are accountable for the risks, harms, and costs they introduce to consumers. Commercial participants are likewise incentivized and empowered effectively to prevent, detect, and resolve unauthorized access and data sharing, unauthorized payments conducted in connection with or as a result of either authorized or unauthorized data sharing access, data inaccuracies, insecurity of data, and failures to comply with other obligations, including the terms of consumer authorizations.</li>
</ul>

<p>Smells like a PSD2-esque set of API standards are on the horizon for the U.S. Ideally this is something the banks would see as an opportunity, rather than a regulatory thing, but I understand how hard-headed they are. I’m spending some time over the next month or two getting up to speed more on where we stand with the PSD2 rollout, as well as the GDPR rollout in the EU. Both of these efforts provide us with a blueprint to follow here in the US. Obviously it is a much different regulatory and banking environment here, but there are still plenty of lessons to consider, and think about as agencies like the CFPB get to work on this topic.</p>

<p>All nine aspects of this latest announcement from the CFPB reflect what APIs are all about. We have the blueprint for tackling this problem head on in use across the tech sector already. This isn’t a technology problem, this is a business and politics problem. It would make sense for a savvy bank (cough, cough Capital One) to get ahead of this one and be the Amazon Web Services of the banking space and set the standard for how data aggregation and sharing occurs. Define the open blueprint for how consumer data is accessed and put to work in the banking ecosystem, gain teh competitive advantage when it comes to Fintech tooling servicing the space, and make all the other banks play catch up. As usual, I’ll keep an eye on what the banks are up to (not much), and look out for more movement from the federal government on this issue, and report back anything I find.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/27/hints-of-banking-api-regulations-from-cfpb-with-consumer-authorized-financial-data-sharing-and-aggregation-rules/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/27/making-sure-you-operate-in-the-cloud-marketplaces-as-an-api-service-provider/">Making Sure You Operate In The Cloud Marketplaces As An API Service Provider</a></h3>
        <span class="post-date">27 Nov 2017</span>
        <p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/slashdb/slashdb-automatic-rest-api-for-databases-in-aws-marketplaces.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>As the cloud giants like AWS, Microsoft, and Google continue to assert their dominance of the digital world, one aspect of their operations I’m watching closely has to do with their marketplaces. Google’s marketplaces are still very Android focused, but Amazon and Microsoft have shifted their recent editions of their marketplaces to be more cloud oriented, and accommodating a wide variety of applications, machine learning models, as well as APIs and API-focused services. While these marketplaces are still growing, and asserting their role in the digital economy, they are something I advise API providers, and service providers to be keeping a close eye on, and begin considering how they will want to operate within these environments.</p>

<p>If you are an API service provider, and you are selling services to API providers anywhere along the API lifecycle, I recommend you follow the example of friends over at SlashDB, who have their database to API offerings in two of the leading marketplaces:</p>

<ul>
  <li><a href="https://aws.amazon.com/marketplace/pp/B01MU8W71L"><strong>AWS</strong></a> - Automatically constructing a REST API to databases for reading and writing on the AWS platform.</li>
  <li><a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps/vte.slashdb"><strong>Azure Marketplace</strong></a> - SlashDB enables you to do more with traditional databases and Microsoft Azure.</li>
</ul>

<p>As more companies, organizations, institutions, and government agencies move their databases into the cloud, SlashDB sees the opportunity to help them quickly turn databases and tables into web interfaces for querying data. Having your API service ready to go, in the environments where your potential customers are already operating is how much of this API stuff will go down in the future. Amazon has set the stage for how we’ll be delivering IT infrastructure over the last decade with the introduction of the cloud, and Google and Microsoft are quickly playing catch up. The savvy API service providers understand their role in this cloud evolution and make sure their services are available as retail solutions, but also as plug and play wholesale solutions in these cloud marketplaces.</p>

<p>SlashDB is clearly serving the deployment and management aspects of the API lifecycle, but I’m tracking on virtualization, testing, monitoring, security, and other aspects of doing business with APIs who are also deploying using these cloud marketplaces. I’m also seeing an uptick in the growth of machine learning models being made available via AWS, Azure, and Google, demonstrating that the algorithmic evolution of the API sector will occur in these environments. The algorithmic wave of APIs is just getting started, but publishing APIs from your databases on the leading cloud platforms is standard operating procedure for businesses of all shapes and sizes in 2017. Are your API services available in the AWS or Azure marketplaces?</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/27/making-sure-you-operate-in-the-cloud-marketplaces-as-an-api-service-provider/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/21/the-defensive-database-administrator-and-the-eager-blockchain-believer/">The Defensive Database Administrator And The Eager Blockchain Believer</a></h3>
        <span class="post-date">21 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/adam-smith_dali_three.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Think about the power that database administrators have in your organizations world? I’ve been working with databases since my first job in 1987. I’ve seen the power bestowed upon database administrators in organization after organization. They are fully aware of the power they control, and most other people in an organization are regularly reminded of this power. The defensive database administrator is always the biggest obstacle in the way of API teams who are often seen as a threat to the power and budgets that database groups command. This power is why databases are often centralized, scaled vertically, and are the backends to so many web, mobile, desktop, and server applications.</p>

<p>I spend a significant amount time thinking about the power that database administrators wield, and how we can work to find more constructive, secure, and sensible approaches to shifting legacy database behaviors. Lately, I also find myself thinking a lot more about Blockchain. Not because I’m a believer, but because so many believers are pushing it onto my radar. Blockchain will continue to be a thing, not because it is a thing, but because so many people believe it is a thing. Most blockchains will not withstand the test of time, they are vapor, but the blockchains that remain will because people have convinced other people to put something meaningful into their blockchain. Much like we have convinced so many companies, organizations, institutions, and government agencies to put data into databases. Yes we. I’m complicit.</p>

<p>A definition of the blockchain is, “a continuously growing list of records, called blocks, which are linked and secured using cryptography”. It’s a database, linked and secured using cryptography. The reason you hear about the blockchain so much, and how it can revolutionize almost every business sector, is the blockchain believers want to convince you to put your digital assets into their blockchain, which will eventually make it something real. I can setup a blockchain today, call it anything I want, but it is nothing more than an empty distributed database. It doesn’t become anything until there is something of value stored in it, which is why there are so many eager folks right now trying to convince that blockchain is something, so you’ll put your valuable things in there, and it will become something.</p>

<p>Think of blockchain believers as the frontend version of the defensive database administrator. After a blockchain has been up for 20 years, and has a bunch of valuable things stored in it, the blockchain believers will become more like the database administrators. They’ll grow beards (even the women), and become more defensive of their precious data stores from whatever the next threat to their power is, and do whatever it takes to defend their power. Blockchain believers are young energetic, and looking to build their empires, and database administrators are usually older and motivated to defend their empires. When you are down in the trenches trapped within the tractor beam of a database it is hard to see beyond it. When you are basking in glow of Internet technology, and everything is new and exciting, it can also be hard to see beyond it. With everything, give it 20 years, and things often times become whatever they’ve replaced.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/21/the-defensive-database-administrator-and-the-eager-blockchain-believer/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/21/when-you-believe-everything-in-tech-is-new-and-nothing-repeats-itself/">When You Believe Everything In Tech Is New And Nothing Repeats Itself</a></h3>
        <span class="post-date">21 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/beach-rocks-currents_blue_circuit_4.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I get regular waves of commenters and tweeters who like to point out the API patterns I’m covering in the API space, have all been done before. We tried discovery docs before they are called WSDL! That API discovery thing is called UDDI! RPC is nothing new! That isn’t new. We tried that before, and it didn’t work. I rarely ever engage with these folks, as this behavior is one pattern in behavior I actually do believe we SHOULDN’t be repeating and showcasing.</p>

<p>I’m fascinated by the reasons someone would feel so strongly they need to respond. That something happened in the past, and because it didn’t work we shouldn’t try again today. That somehow the world of compute isn’t built upon, and remixed upon previous ideas that worked, and many that didn’t work until just the right conditions existed. This kind of behavior is really fascinating for me in the world of APIs where reuse, aggregation, facades, and so many patterns of reworking what already exists is core to the entire concept. Where do folks get such strange believes in the past, and what can and cannot be re-interpreted in the future?</p>

<p>Hey you, electric car manufacturers, the electric car was done in early 20th century and it didn’t work! Hey musician, that baseline was originally present in the big band era and didn’t go over well, it won’t work now! Those pants were first tried in the 1950s and were a flop. Someone already wrote a book on Abraham Lincoln, why would you want to write another? Where do people get the idea that something that existed in the past shouldn’t be tried again, when it comes to the world of technology? Not only have the thought, but so many feel so strongly that they have to reach out and tell me what I’m writing about is dumb because it’s already been done?</p>

<p>What is it about web technology that makes people think something can’t be tried again? That conditions aren’t different now? That they need to condemn people? It’s a fascinating phenomena that surrounds Internet technology. It’s one of the reasons it whispers so strongly to young men, who usually do not have a strong understanding of the past, let alone how the future is built on the past. They feel so strongly in their beliefs, and in technology, that they feel compelled to tell people regularly how wrong they are. This behavior manifests itself in strange ways, it feels oddly like bot behavior, meaning that they operate within a certain set of Internet age rules, and do not understand the real world. They work kind of like antibody against anything that it feels questions its reality that everything is new, everything is awesome, and nothing from the past is important or should be used again.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/21/when-you-believe-everything-in-tech-is-new-and-nothing-repeats-itself/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/21/day-2638-apis-are-dumb/">Day 2,638: APIs Are Dumb</a></h3>
        <span class="post-date">21 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/christianity-under-construction_atari_asteroids.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>It is one of those weeks where writing API stories, and doing my API work is completely uninteresting, and my three year old self is throwing a temper tantrum when it comes to doing anything. APIs are dumb. Why the hell would I care about this aspect of technology? Most people don’t understand what the fuck I’m talking about, and people keep doing really dumb shit with them, instead of working on the problems that really matter. Why do I keep doing what I’m doing? Why don’t I just go get a real job, make some real money, and give a shit less? Great question!</p>

<p>Most weeks I can just turn the API Evangelist persona on, and with a notebook full of ideas, and inbox full of questions, I begin writing the API blah blah blah. It just flows. This week it all seems dumb, and I have to fabricate any ounce of caring about APIs. Beyond APIs and Internet technology in general feeling like a pretty bad idea, I feel complicit in helping bring about this technological beast that is wreaking havoc on our world right now. Why the hell should I continue doing API Evangelist, when so many of my ideas can be used for exploitation, and just keep making rich white people richer? It just seems like a bad idea, so why shouldn’t I just shut things down and go find a meaningful job (does that exist)?</p>

<p>First, I always start with the basic API Evangelist mission: helping non-techies understand what APIs are, and how they are right under the hood of everything we are using that is digital. What I do will never receive venture capital, be profitable, and return measurable ROI. Few other companies, let alone individual care about a digitally literate world, they just want consumers, and refuse to see the correlation. I’m the one showcasing API stories consistently regardless of the latest trends, and focus on understanding what is happening outside the current popular areas of investment. I’m the one person that isn’t changing my tune based upon what my investors are telling me, and my storytelling doesn’t reflect where I am at in my runway.</p>

<p>Second, I’m working on important projects. I pushing forward the human services data API (HSDA), and if I get the bandwidth I’ll help lend a hand on Open 311, and work to standardize how we also report issues in cities around the glob. I’m studying how city, state, and federal governments can use common API management practices to generate the next generation of tax base and revenue from the valuable data, and content resources they are stewards of. I’m thinking about how we take back  control from the big tech companies when it our personal data, and content. I’m also thinking about how Twitter, Facebook, and other API platforms are allowing their APIs to be abused by bots because it supports their bottom line, and strengthens their numbers–despite what it is doing to our democracy, our communities, and society. Who else is doing this?</p>

<p>Third, I just don’t want the greedy fucking people to win. I just want to keep being a monkey wrench in the works. I don’t think I’m going to win at this game. I don’t think I’m convince everyone of the right way of doing technology (is there one), but god dam I’m going to make it harder for the people with the money to always win. I’m going to make them spend more money. I’m going work to educate people about how the technology works, and show how we can all resist. On the days that I find it hard to care about educating people about APIs, or even the good APIs projects I’m on, being a wrench in the gears always brightens my day and puts a smile on my face. If nothing else, I’m going to just screw with your grand plans for world domination, and getting rich on our backs. On our data. On our personal lives. I’m going to make you work harder to exploit all of us.</p>

<p>Ok. I’m getting closer to being back on track. Just needed a little reminder of why I’m doing this. APIs are dumb. I’m pretty sure we shouldn’t be fucking doing them in the first place, however with all this technology, algorithms, and artificial stupidity in place, we need some way of making it all a little more observable, and APIs is the best we got. They are the best example I have of making black box algorithms a little more transparent, and how we can take back a little of that data exhaust we generate each day on our mobile phones and laptops. If nothing else, I just need to keep studying this API bullshit so I understand what they are doing, and how they are moving the bits and bytes around in this surveillance capitalism reality we’ve allowed to be constructed. So that when the time is right, I can throw myself against the machine and make it come to a halt, even for a brief moment.</p>

<p>Looking in the mirror: Ok, asshole. Go get em! You can do this. Stop being such a whiney bitch and keep writing stories and doing the research.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/21/day-2638-apis-are-dumb/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/20/generating-operational-revenue-from-public-data-access-using-api-management/">Generating Operational Revenue From Public Data Access Using API Management</a></h3>
        <span class="post-date">20 Nov 2017</span>
        <p><i>This is part of some research I'm doing with <a href="http://apis.how/streamdata">Streamdata.io</a>. We share a common interest around the accessibility of public data, so we thought it would be a good way for us to partner, and Streamdata.io to underwrite some of my work, while also getting the occasional lead from you, my reader. Thanks for supporting my work <a href="http://apis.how/streamdata">Streamdata.io</a>, and thanks for support them readers!</i></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/public-data-api-management/parks-prohibit-commercial-use.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>A concept I have been championing over the years involves helping government agencies and other non-profit organizations generate revenue from public data. It is a quickly charged topic whenever brought up, as many open data and internet activists feel public data should remain freely accessible. Something I don’t entirely disagree with, but this is a conversation, that when approached right can actually help achieve the vision of open data, while also generating much needed revenue to ensure the data remains available, and even has the opportunity to improve in quality and impact over time.</p>

<p><strong>Leveraging API Management</strong>
I’d like to argue that APIs, and specifically API management has been well established in the private sector, and increasingly in the public sector, for making valuable data and content available online in a secure and measurable way. Companies like Amazon, Google, and even Twitter are using APIs to make data freely available, but through API management are limiting how much any single consumer can access, and even charging per API call to generate revenue from 3rd party developers and partners. This proven technique for making data and content accessible online using low-cost web technology, requiring all consumers to sign up for a unique set of keys, then rate limiting access, and establishing different levels of access tiers to identify and organize different types of consumers, can and should be applied in government agencies and non-profit organizations to make data accessible, while also asserting more control over how it is used.</p>

<p><strong>Commercial Use of Public Data</strong>
While this concept can apply to almost any type of data, for the purposes of this example, I am going to focus on 211 data, or the organizations, locations, and services offered by municipalities and non-profit organizations to hep increase access and awareness of health and human services. With 211 data it is obvious that you want this information to be freely available, and accessible by those who need it. However, there are plenty of commercial interests who are interested in this same data, and are using it to sell advertising against, or enrich other datasets, and products or services. There is not reason why cash strapped cities, and non-profit organizations carry the load to maintain, and serve up data for free, when the consumers are using it for commercial purposes. We do not freely give away physical public resources to commercial interests (well, ok, sometimes), without expecting something in return, why would we behave differently with our virtual public resources?</p>

<p><strong>It Costs Money To Serve Public Data</strong>
Providing access to public data online costs money. It takes money to run the database, servers, bandwidth, and websites and applicatiosn being used to serve up data. It takes money to clean the data, validate phone numbers, email addresses, and ensure the data is of a certain quality and brings value to end-users. Yes this data should be made freely available to those who need it. However, the non-profit organizations and government agencies who are stewards of the data shouldn’t be carrying the financial burden of this data remaining freely available to commercial entities who are looking to enrich their products and services, or simply generate advertising revenue from public data. As modern API providers have learned there are always a variety of API consumers, and I’m recommending that public data stewards begin leverage APIs, and API management to better understand who is accessing their data, and begin to put them into separate buckets, and understand who should be sharing the financial burden of providing public data.</p>

<p><strong>Public Data Should Be Free To The Public</strong>
If it is public data, it should be freely available to the public. One the web, and through the API. The average citizen should be able to come use human service websites to find services, as well as us the API to help them in their efforts to help others find services. As soon as any application of the public data moves into the commercial realm, and the storage, server, and bandwidth costs increase, they shouldn’t be able to offload the risk and costs to the platform, and be forced to help carry load when it comes to covering platform costs. API management is a great way to measure each application consumption, and then meter and quantify their role and impact, and either allow them to remain freely accessing information, or be forced to pay a fee for API access and consumption.</p>

<p><strong>Ensuring Commercial Usage Helps Carry The Load</strong>
Commercial API usage will have a distinctly different usage fingerprint than the average citizen, or smaller non-commercial application. API consumers can be asked to declare they application upon signing up for API access, as well as be identified throughout their consumption and traffic patterns. API management excels at metering and analyzing API traffic to understand where it is being applied, either on the web or in mobile, as well as in system to system, and other machine learning or big data analysis scenarios. Public data stewards should be in the business of requiring ALL API consumers sign up for a key which they include with each call, allowing the platform to identify and measure consumption in real-time, and on recurring basis.</p>

<p><strong>API Plans &amp; Access Tiers For Public Data</strong>
Modern approaches to API management lean on the concept of plans or access tiers to segment out consumers of valuable resources. You see this present in software as a service (SaaS) offerings who often have starter, professional, and enterprise levels of access. Lower levels of the access plan might be free, or low cost, but as you ascend up the ladder, and engage with platforms at different levels, you pay different monthly, as well as usage costs. While also enjoying different levels of access, and loosened rate limits, depending on the plan you operate within. API plans allows platforms to target different types of consumers with different types of resources, and revenue levels. Something that should be adopted by public data stewards, helping establish common access levels that reflect their objectives, as well as is in alignment with a variety of API consumers.</p>

<p><strong>Quantifying, Invoicing, And Understanding Consumption</strong>
The private sector focuses on API management as a revenue generator. Each API call is identified and measured, grouping each API consumers usage by plan, and attaching a value to their access. It is common to charge API consumers for each API call they make, but there are a number of other ways to meter and charge for consumption. There is also the possibility of paying for usage on some APIs, where specific behavior is being encouraged. API calls, both reading and writing, can be operated like a credit system, accumulating credits, as well as the spending of credits, or translation of credits into currency, and back again. API management allows for the value generated, and extracted from public data resources is measured, quantified, and invoiced for even if money is never actually transacted. API management is often used to show the exchange of value between internal groups, partners, as well as with 3rd party public developers as we see commonly across the Internet today.</p>

<p><strong>Sponsoring, Grants, And Continued Investment in Public Data</strong>
Turning the open data conversation around using APIs, will open up direct revenue opportunities for agencies and organizations from charging for volume and commercial levels of access. It will also open up the discussion around other types of investment that can be made. Revenue generated from commercial use can go back into the platform itself, as well as funding different applications of the data–further benefitting the overall ecosystem. Platform partners can also be leveraged to join at specific sponsorship tiers where they aren’t necessarily metered for usage, but putting money on the table to fund access, research, and innovative uses of public data–going well beyond just “making money from public data”, as many open data advocates point out.</p>

<p><strong>Alternative Types of API Consumers</strong>
Discovering new applications, data sources, and partners is increasingly why companies, organizations, institutions, and government agencies are doing APIs in 2017. API portals are becoming external R&amp;D labs for research, innovation, and development on top of digital resources being made available via APIs. Think of social science research that occurs on Twitter or Facebook, or entrepreneurs developing new machine learning tools for healthcare, or finance. Once data is available, identified as quality source of data, it will often be picked up by commercial interests building interesting things, but also university researchers, other government agencies, and potentially data journalists and scientists. This type of consumption can contribute directly to new revenue opportunities for organization around their valuable public data, but it can also provide more insight, tooling, and other contributions to a cities or organizations overall operations.</p>

<p><strong>Helping Public Data Stewards Do What They Do Best</strong>
I’m not proposing that all public data should be generating revenue using API management. I’m proposing that there is a lot of value in these public data assets being available, and a lot of this value is being extracted by commercial entities who might not be as invested in public data stewards long term viability. In an age where many businesses of all shapes and sizes are realizing the value of data, we should be helping our government agencies, and the not for profit organizations that serve the public good realize this as well. We should be helping them properly manage their digital data assets using APIs, and develop an awareness of who is consuming these resources, then develop partnerships, and new revenue opportunities along the way. I’m not proposing this happens behind closed doors, and I’m interested in things following an open API approach to providing observable, transparent access to public resources.</p>

<p>I want to see public data stewards be successful in what they do. The availability, quality, and access of public data across many business sectors is important to how the economy and our society works (or doesn’t). I’m suggesting that we leverage APIs, and API management to work better for everyone involved, not just generate more money. I’m looking to help government agencies, and non-profit organizations who work with public data understand the potential of APIs when it comes to access to public data. I’m also looking to help them understand modern API management practices so they can get better at identifying public data consumers, understanding how they are putting their valuable data to work, and develop ways in which they can partner, and invest together in the road map of public data resources. This isn’t a new concept, it is just one that the public sector needs to become more aware of, and begin to establish more models for how this can work across government and the public sector.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/20/generating-operational-revenue-from-public-data-access-using-api-management/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/17/the-many-meanings-of-do-not-make-the-same-mistake-as-twitter-did-with-their-api/">The Many Meanings Of "Do Not Make The Same Mistake As Twitter Did With Their API"</a></h3>
        <span class="post-date">17 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/kinlane-white-board-twitter_copper_circuit.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I remember the first time I heard someone say that they didn’t want to make the same mistake as Twitter did with their API. It was from Pinterest. After that I heard the phrase uttered by many companies, with almost an entirely different meaning behind what the mistake was. Twitter is a darling of the API community when it comes to being the poster child for what not to do in the API space. I consider Twitter to be in the top 10 most important APIs out there, as well as being in the top ten APIs I wouldn’t want to be responsible for, and is a platform full of endless examples of how to do APIs right, and how to do them wrong.</p>

<p>When some companies say this phrase, they mean they don’t want to make the mistake Twitter did by having an API at all–usually heard from executives. Other times, it is said in response to anti competitive behavior in their API ecosystem, and treating startups badly. When you hear from developers, it is usually about their rate limits, and their rules of the road they published a few years back. It coming years I predict we’ll be saying it about automation, and using Twitter as case study for how not to assert control of bots on your API platform. You’ll find me leveraging this statement regularly to talk about making sure you have a real API monetization strategy, and don’t wait a decade to start offering premium access to your APIs that are accessible to EVERYONE.</p>

<p><a href="https://apievangelist.com/2012/06/29/twitter-continues-to-restrict-access-to-our-tweets/">I’ve been complaining about access to the Twitter API for over five years now</a>. API plans are the heart of every API I keep an eye on. They set the tone for ALL conversations that go on around an API. The lack of a coherent, equitable, API access plan at Twitter has set into motion almost every other illness on the platform from harassment to bots. Many of the reasons I would utter the phrase “you don’t want to make the same mistake as Twitter” all stem from the lack of a coherent API plan for the platform. Not having a dedicated page, with a coherent plan for access to your API is the number one mistake you can make operating your APIs in 2017.</p>

<p>When I say this, I am not making any assumptions around what you should be charging, or how you should be limiting access. I’m simply saying that you should have to have a plan, and your community needs a URI to be able to become aware of your plan before ever consuming an API. There are may ways you can make your plan too restrictive, and inject other problems into your API plan, but having one is a good start, and really helps distinguish the APIs who have their act together and those who do not. An API plan demonstrates that you, well, have a plan. Having it publicly available in your developer portal, demonstrates some transparency of your plan, and that you are somewhat willing to include your API community in this plan–always a good idea, but you’d be surprised who doesn’t quite understand this.</p>

<p>Twitter’s biggest crime in my book is not having an API plan over the years. There are many ways we can beat up on Twitter for their shortcomings over the years. I’ve done my fair share. However, I do try and understand the scope of their challenges, and showcase the good that comes from the ecosystem as well. I’m hopeful that their move in offering premium APIs is more about defining a coherent plan for the API, and not simply about chasing a new revenue stream. The release of the APIs, and the structure of the plans and pricing seem to reflect they’ve done some deep thinking from the consumer perspective, so I am optimistic they are moving towards having a plan over just squeezing more money out of our information.</p>

<p>The moral of today’s story kids, is that if you don’t want to make the same mistake as Twitter, do not wait a decade to have a plan in place for your API. Make sure all your APIs have a clear monetization strategy, with a coherent plan in place regaring how you’ll manage your APIs in a way that delivers on your monetization strategy, but also provides a shared plan that includes your partners, and regular API consumers. Without a solid plan in place for your API, your community will never truly be in sync with your organization, and you’ll be incentivizing the worst behavior amongst your consumers. Don’t be like Twitter, and have a plan in place from day one.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/17/the-many-meanings-of-do-not-make-the-same-mistake-as-twitter-did-with-their-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/17/api-management-is-about-awareness-and-control-over-our-digital-resources/">API Management Is About Awareness And Control Over Our Digital Resources</a></h3>
        <span class="post-date">17 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/adam-smith_blue_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been diving into the fundamentals of API management as part of several projects I am working on. I am setting up API management for a single API project, as well as thinking through API management practices across many API implementations in a single industry. I also just had lunch with a friend at an API startup I work with who is looking to invest in me doing some further research and storytelling when it comes to API management. All of this is providing me with a great opportunity to step back and think about API management from the small detailed moving parts, all the way up to the industry, regulatory, and macro levels of managing digital resources online.</p>

<p><a href="http://management.apievangelist.com/">API management</a> is the oldest aspect of my research, and one I still think is one of the most critical aspects of doing APIs in my opinion. While there are many features modern API management brings to the table, the core of it is all about allowing consumers to sign up to access some data, content, media, or algorithm. Each consumer receives a set of keys that will identify and allow for their access to be measured, which then allows the owners or stewards of digital resources to develop awareness around who is accessing a resource, and what they are doing with it. Some call it security, others analytics, but I see it about developing an awareness and asserting control over our digital resources.</p>

<p>If you are focused on monetization, API management is about generating revenue. If you are worried about who has access to your digital assets, API management is about security. If you are doing API management right you realize it is about being aware of the digital resources you have, working to make sure they are well defined, and are tuned into your API management dashboard to understand how they are being used (or not used). I feel like this has been one of the shortcomings of an VC led world of API management, is that it became heavily focused on restricting and controlling access, and fixated on generating revenue, leaving a significant amount of opportunity on the table for making sense of the digital resources we all depend on, and maximizing their access, usage, and yes, revenue.</p>

<p>I see more investment in APIs when it comes to startups getting access to resources. I also see heavy investment when it comes to APIs generating new data points (home, auto, wearables, sensors, etc.) However, when it comes to understanding and quantifying the data, content, and algorithms already in use, there just isn’t much investment. Not there isn’t value there. There just isn’t enough value there to attract VC level interest. I feel like the tech sector wants APIs for all the wrong reasons. The reasons that benefit them. The pervasiveness nature of this way of thinking has stagnated companies, organizations, institutions, and government agencies from establishing control over their vital digital resources, developing awareness, and establishing control, using API management.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/17/api-management-is-about-awareness-and-control-over-our-digital-resources/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/17/we-love-what-you-do-in-the-api-space-now-do-it-our-way/">We Love What You Do In The API Space But Could You Do It Our Way</a></h3>
        <span class="post-date">17 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist-logos/api-evangelist-butterfly-vertical.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I hear it daily in my inbox, on Twitter, and via LinkedIn. We love what you do! We’ve followed your work for a while, and love your unique voice, and the way you tell stories on your blog. I’m not very good at accepting praise on my work, especially when I know that much of it isn’t sincere and genuine. Saying it casually to me is weird, and I am not sure why people feel like they should be saying it, but it is the folk who go the distance to say it, but then also try to change the way I am, after acknowledging over and over, that they like what I do.</p>

<p>From running a major conference, to my everyday storytelling, I get waves of people who like what I’ve done historically, want to support and be part of it, but once engaged actively try to change the conversation, and change the tone of what I do. The community has really seemed to rally around your conference, and clearly you’ve built a loyal group by making your event about ideas–we’d like to sponsor, but we really need a main stage talk where we can talk about our products. We love the tone of your storytelling on the blog and how you educated people people about the real world aspects of doing APIs–we’d love to sponsor, but we need you to talk about our products, and shift the focus to what we are doing. There are so many ways people acknowledge the value of what I do, but then want me to do the same old tired thing they’ve been doing.</p>

<p>I get why you do it. It is easy. It is going from zero to what you want in as little time as possible. However, you seem to be all too willing to completely ignore why my thing is working and why your old tired thing isn’t, and why you are even attracted to my thing in the first place. It is because your approach isn’t creative. It is’t genuine. Nobody cares. It takes work to actually care about something, and find the way to share it in a way that folks will actually care about it. You can’t just do this with any technology, product, or service. If I just do your thing, then I’d be just like you, and people like you wouldn’t even notice me. I wouldn’t have any audience, or people who trust me. Maybe that is just want you want though? Maybe I make you look bad, and it would be easier if I just went away.</p>

<p>Honestly, I just can’t get into the heads of why folks are attracted to what I do, approach me, then want to change what I do. Why 1000 lb gorillas are so used to getting their way sponsoring conferences, and getting the tech blogosphere to be their mouthpiece? I guess the ROI on it is still greater than the work of having to do anything meaningful. They can use up small bloggers like me, and move along without any meaningful consequences. I guess many haven’t really stopped to evaluate why I’ve managed to build and maintain an audience of 7 years of doing this, they just hear people talking about what I do, and think “I need some of that!”. Well, I guess you’ll keep doing your version, and I’ll keep doing mine, and we’ll keep meeting like oil and water until one of us gets our way. I’m guessing your type of approach will ultimately win out, but as long as I’m alive I’m going to keep doing my way, just so I can be a monkey wrench in your way of doing things.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/17/we-love-what-you-do-in-the-api-space-now-do-it-our-way/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/17/my-base-yaml-for-starter-api-plans/">My Basic YAML For Starter API Plans</a></h3>
        <span class="post-date">17 Nov 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/api_plans_pricing_tiers.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://plans.apievangelist.com/">I started developing a machine readable format for describing the API plans and pricing for leading API providers a few years back</a>. Eventually I’d like to see the format live alongside OpenAPI, Postman, and other machine readable API specifications within a single APIs.json index. I am looking to adequately describe the plans and pricing for APIs, which are often just as important as the technical details, in the same way we’ve describe the technical surface area of an API using OpenAPI for some years now. People love to tell me that I will never be able to do it, which only makes me want to do it more.</p>

<p>I’m revisiting my work as part of work I’m doing on a clients project, which I’m also using to push forward my API portal and management toolbox. The project I’m working on has two API plans:</p>

<p>1) Starter - The free level of access everyone gets when signing up for access to an API.
2) Verified - A verified level of pay as you go usage once you have credit card on file.</p>

<p>I’ve taken the common elements across these plans and described them in a YAML format which allows me to remix the elements into the two plans I currently have, while also allowing me to reuse them for possible future plans, helping keep my approach consistent.</p>

<script src="https://gist.github.com/kinlane/e1bbbabe8f24c0aced4d41b45f2295d8.js"></script>

<p>I’m using the plan elements in this YAML file to generate the plans and pricing page for each API. Generating two separate plan boxes, with the details, and elements of each plan. I keep all the moving parts of each plan defined as separate fields and collections so that I can reuse in any new plans. I also make use of the individual elements in comparison charts, and other pricing and plan related resources through an APIs portal. The specification isn’t perfect, but it provides me a starting point for considering how I make my API plans and pricing machine readable, and indexed as part of the <a href="http://apisjson.org/">APIs.json</a> for each of my projects.</p>

<p>Next, I am taking API plan templates and auto-generating plans using AWS API Gateway. I’m going to play around with recreating some of the common plans we see for leading API providers out there using an existing gateway solution. Similar to generating the technical surface area of an API using AWS API Gateway, I’m looking to generate the business surface area of each API using the API Gateway in the same way. Definitely still a lot of work ahead of me when it comes to polishing my API plan specification format, but I feel like it is pretty good start, and after publishing a version for leading API providers, as well as some of the custom projects I’m working on, I think it will be a little more ready for prime time.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/17/my-base-yaml-for-starter-api-plans/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/16/the-three-stripe-openapi-vendor-extensions/">Three Stripe OpenAPI Vendor Extensions</a></h3>
        <span class="post-date">16 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/stripe/stripes-openapi-vendor-extension.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>As part of my work on <a href="http://openapi.toolbox.apievangelist.com/">my OpenAPI toolbox</a> I am keeping an eye out for how leading API providers are using OpenAPI. One layer of this part of my research is understanding how teams are extending the OpenAPI specification, while also encouraging other companies to understand that they can extend the specification in the first place. I’m always surprised how many people I come across that say they do not use the specification because it doesn’t do everything they need. I alternatively feel like it is my responsibility to understand what the spec can do, and then bend it to do what I need it to using vendor extensions.</p>

<p>I have been studying how <a href="https://github.com/stripe/openapi">payment provider Stripe has been crafting their OpenAPI</a> throughout the week, while also understanding how they are applying it across their platform operations. As part of their Github repository for managing the Stripe OpenAPI they share three vendor extensions they are using to evolve what is possible with OpenAPI:</p>

<ul>
  <li><strong>x-expandableFields</strong> - Resources include an x-expandableFields that contains a list of fields that are expandable by making an API request with an expand parameter. See expanding objects.</li>
  <li><strong>x-polymorphicResources</strong> - Some API responses are “polymorphic” in that they might return multiple types of resources which is a case that OpenAPI can’t handle. In these cases the spec will reference a “synthetic” resource which is an aggregate of the properties common to all the possible resources. It will also include the field x-polymorphicResources which references those resources more precisely.</li>
  <li><strong>x-resourceId</strong> - Resources include x-resourceId which is a canonical name for each resource. It can be used in conjunction with openapi/fixtures{2,3}.{json,yaml} to look up a sample representation (otherwise known as a “fixture”) of the resource.</li>
</ul>

<p>Some interesting extensions. The expandable fields, and resource id is pretty straight forward, but the polymorphic resources opens up some interesting questions when it comes to API design. It makes me want to learn more about the how and why Stripe does this with their API. Maybe it is just me, but I find OpenAPI a very useful tool for quantifying the design decisions that go into an API. I’m eager to learn more about how consistent providers are, as well as understanding where they deviated, and I find vendor extension are useful in revealing clues behind the decisions to deviate from common API design patterns.</p>

<p>I am going to be spending a lot of time studying Stripe’s usage of OpenAPI. It is significant that top tier providers to share their OpenAPI on Github like Stripe does. It helps us learn more about the Stripe API, and the design and documentation decisions that have gone into the payment API. I wish more API providers would share their OpenAPI definition(s) via Github, and share any vendor extensions they have defined. A machine readable API definition on Github, with easy to find vendor extensions, across many API providers sounds like the beginning of a new generation of API discovery. One that can help drive the future of the OpenAPI Initiative (OAI) through real world usage, and shaping the OpenAPI specification road map through actively defining and sharing vendor extensions.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/16/the-three-stripe-openapi-vendor-extensions/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/16/the-information-you-get-when-allowing-developers-to-sing-up-for-api-using-github/">The Information You Get When Allowing Developers To Sign Up For An API Using Github</a></h3>
        <span class="post-date">16 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-authentication-screenshot.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m a big Github user. I depend on Github for managing all my projects, and Github Pages for the presentation layer around all my research. When anything requires authentication, whether for accessing an API, or gaining access to any of my micro apps, I depend on Github authentication. <a href="https://gist.github.com/kinlane/00db3d871b615c8b1c43dbc60ae41f86">I have a basic script that I deploy regularly after setting up a Github OAuth application</a>, which I use to enable authentication for my API portals and applications, handling the OAuth dance, and returning me the information I need for my system.</p>

<p>After a user authenticates I am left with access to the following fields: id, avatar_url, gravatar_id, url, html_url, followers_url, following_url, gists_url, starred_url, subscriptions_url, organizations_url, repos_url, events_url, received_events_url, type, site_admin, name, company, blog, location, email, hireable, bio, public_repos, public_gists, followers, following, created_at, updated_at, private_gists, total_private_repos, owned_private_repos, disk_usage, collaborators, two_factor_authentication, and plan. Not all these fields are filled out, and honestly I don’t care about most of them for my purposes, but it does provide an interesting look at what you get from Github, over a basic email and password approach to authentication.</p>

<p>I’m just looking for any baseline information to validate someone is a human being when signing up. Usually a valid email is this baseline. However, I prefer some sort of active profile for a human being, and have chosen Github as the baseline. When anyone signs up I also quickly calculate some other considerations regarding how long they’ve had a Github account, how active it is, and some numbers regarding this history and activity. I don’t expect everyone to have a full blown public Github profile like I do, but if you are looking to use on of my APIs, or API-driven micro tools I’m looking for something more than just a valid email–I want some sign of life. I will be evolving this algorithm, and enforcing it in different ways at different times.</p>

<p>I always hesitate using Github as the default login for my API portals and applications, but honestly I think it is a pretty low bar to expect folks to have a Github account. I feel like we should be raising the bar a little when it comes to who is accessing our resources online. The APIs and tooling I’m making available are mine, and I just want to make sure you are human, and are verifiable on some level, and I find that the links available as part of your Github profile provide me with more reliable and verifiable aspects of being human in the tech space. Making the fields returned as part of Github authentication pretty valuable for verifying humans in my self-service, and increasingly automated world.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/16/the-information-you-get-when-allowing-developers-to-sing-up-for-api-using-github/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/16/you-thinking-i-mean-rest-when-say-apis-is-about-your-limited-views-not-mine/">You Thinking I Mean REST When I Say API Is About Your Limited Views, Not Mine</a></h3>
        <span class="post-date">16 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/kin-chesapeake-sun_light_dali.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m fascinated by the baggage people bring to the table when engaging in discussions around technology with me. A common opener for many conversations with season technologists centers around REST not penciling out as everyone thought, failing to be the catch-all solution, and will quickly move to how I feel about some new technology (GraphQL, gRPC, Kafka, other) making my work irrelevant. I wish I had some quick phrase to help folks understand how this line of questioning demonstrates their extremely limiting views of the tech sector, as well as my work with APIs, but alas I find silence usually does the job in these situations–allowing everyone to quickly move.</p>

<p>For me, application programming interface, or API, is all about finding the right interface for programming against for a specific application. I’d say the closest things that anchors my belief system to REST, is that I tend to focus on leveraging the web when it comes to defining the web because it is low coast, usually well known, and avoids reinventing the wheel. I’m not a RESTafarian, and you will not find me online arguing the finer details of REST over other approaches. It just isn’t my style, and I leave it to ya’ll to work out these finer details, and share the stories about what is working, and what is not working in your operations.</p>

<p>Your assumptions around what APIs means to me demonstrates your limited views, only partially because of the technological underpinnings. The technical details of API is only 1/3 of the equation for me, and the majority of my research and storytelling focuses on the business and politics of doing APIs, but I’m guessing you aren’t aware of this. I find that the technology definitely sets the tone for API implementations and conversations, but the ones that actually make a significant impact always transcend the technology, and help acknowledge, and understand the other aspects of operating online which is making doing APIs a good or bad thing. From your opening statements, I’m guessing our conversation won’t be transcending anything.</p>

<p>I appreciate you taking a moment to share your limited view of APIs and what I do. You’ll have to excuse me for not having much to say, but after doing this for seven years I know that I will have little effect to shifting your limited views of what is API, and what it is that I do as the API Evangelist. I know that you feel pretty strongly REST APIs didn’t deliver, but I’m pretty busy helping folks understand how they can effectively manage their digital resources on the web, and securely share and provide access to their data, content, and algorithms using web technology. I don’t see the need for managing and moving our these digital bits going away anytime soon, and I find my time is better spent avoiding the political eddies that swirl at the edges of the API mainstream.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/16/you-thinking-i-mean-rest-when-say-apis-is-about-your-limited-views-not-mine/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/16/stripe-elements-and-how-we-organize-our-api-embeddables/">Stripe Elements And How We Organize Our API Embeddables</a></h3>
        <span class="post-date">16 Nov 2017</span>
        <p><a href="https://stripe.github.io/elements-examples/"><img src="https://s3.amazonaws.com/kinlane-productions/stripe/stripe-elements-grey-embeddable.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I am setting up Stripe for a client, and I found myself browsing through Stripe Elements, and <a href="https://stripe.github.io/elements-examples/">the examples they have published to Github</a>. If you aren’t familiar, “Stripe Elements are pre-built rich UI components that help you build your own pixel-perfect checkout flows across desktop and mobile.” I put Stripe Elements into my bucket of <a href="http://embeddable.apievangelist.com/">API embeddables</a>, which overlaps with <a href="http://sdk.apievangelist.com/">my API SDK research</a>, but because they are JavaScript open up a whole new world of possibilities for developers and non-developers, I keep separate.</p>

<p><a href="https://stripe.com/docs/stripe-js">Stripe.js and supporting elements</a> provides a robust set of solutions for integrating the Stripe API into your website, web or mobile application. You can choose the pre-made element, customize as you see fit, or custom build your own using the Stripe.js SDK. It provides a great place to start when learning about Stripe, reverse engineering some existing solutions, and figuring out what integration will ultimately look like. In my scenario, the default Stripe element in their documentation works just fine for me, but I couldn’t help but playing with some of the others just to see what is possible.</p>

<p>You can tell Stripe has invest A LOT into their Sripe.js SDK, and the overall user experience around it. It provides a great example of how far you can go with embeddable API solutions. I like that they have the Stripe Elements published to Github, and available in six different languages. As I was learning and Googling, I came across other examples of Stripe.js deployment on other 3rd party sites, making me think it would be nice if Stripe had a user generated elements gallery as part of their offering, accepting pull requests from developers in the Stripe community. It wouldn’t be that hard to come up with a template markdown page that developers could fill out and submit, sharing their unique approach to publishing Stripe Elements.</p>

<p>Having a Github repository to display example embeddable API tooling makes sense, and is something I’ll add to my embeddable research. While not all SDKs warrant having their own Github repository, I’d say that embeddable JavaScript SDKs rise to the occasion, and when they are as robust as Stripe Elements might benefit from their own landing page, and forkable, continuously integratable elements. Actually, on second thought, in a CI/CD world I’m feeling like ALL API SDKs should have their own repository, opening up the possibility for them to have their own landing page, issues, and code in a separate repo, for easier integration. I’m going to do a round-up of Stripe’s embeddable efforts, as well as the other SDKs they support, and see what other examples I can extract for other API providers to consider as they pull together their approach to supporting the intersection of embeddable and SDK.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/16/stripe-elements-and-how-we-organize-our-api-embeddables/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/15/form-posts-as-gateway-for-showing-people-they-can-program-the-web-using-apis/">Form Posts As Gateway For Showing People They Can Program The Web Using APIs</a></h3>
        <span class="post-date">15 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/forms/contact-form.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am always looking for new avenues to help on-board folks with APIs. <a href="http://apievangelist.com/2017/11/10/are-people-ready-for-an-online-api-driven-world-that-is-progammable/">I’m concerned that folks aren’t quite ready for the responsibility that comes with a programmable web</a>, and I’m looking for ways to help show them how the web is already programmable, and that APIs can help them take more control over their data and content online. A significant portion of <a href="https://apievangelist.com/2016/04/13/formalizing-my-approach-to-identifying-the-low-hanging-api-fruit/">my low hanging fruit API work</a> centers around the forms already in use across websites, and how these forms are a doorway for data, and content that should also be available via an API. If information is already available on your website, and being gathered or displayed in response to a form on your website, it is a great place to start a conversation around providing an API that delivers the same functionality.</p>

<p>Sometimes forms drive website searches, and act as a way to gather some data before presenting results–providing a good example of a GET API. In other situations forms act as an input for data, such as a contact form, or survey response. In these scenarios, forms provide a good example of a writable, or POST API path–allowing data and content to be added into a system. I’m always pointing out that if data is displayed in tables, or accessible through a form submission on a website, this is where you should start with readable APIs. The same holds true for form submissions that gather data, being where folks should bet starting with writable APIs.</p>

<p>I feel like contact, messaging, and survey forms are all good examples of how companies, organizations, institutions, and government agencies can begin with write APIs. Business users get the concept of a form, and even its submission via a POST on the web. It is a great place to start the conversation with folks about having APIs that don’t just deliver information, but also accept new information using the web. I’m thinking about how I can use Google Forms in conjunction with the work I’ve been doing around managing data using Google Sheets, and exposing it publicly using the Google Sheets API. Demonstrating how simple it is to make data reusable across many applications using Google Sheets, but then also open up access to submit data using Google Forms.</p>

<p>Forms are nothing new in my work as the API Evangelist. I see regular waves of starts emerge to try and crack open the intersection of forms and APIs. I feel like it is one of those areas where we need a lot more training and educational materials, as well as simple prototypes and open source tooling to help folks understand how forms and APIs work together before the next waves of form based startups can get the traction they desire. I feel like there is a significant opportunity to open up the mind of the average business user regarding the programmatically of the web using APIs, but it isn’t just a business opportunity, its an educational opportunity. Which the API space isn’t always good at delivering in because we tend to be so distracted with the building and selling of startups. Hopefully, someone comes along and can use forms as a way to onboard the next wave of API savvy folks, which eventually will pencil out in the success of one or more form centered API startups.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/15/form-posts-as-gateway-for-showing-people-they-can-program-the-web-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/15/deploy-low-hanging-fruit-rogue-api-portals-for-those-who-are-behind-the-curve/">Deploy Low Hanging Fruit Rogue API Portals For Those Who Are Behind The Curve</a></h3>
        <span class="post-date">15 Nov 2017</span>
        <p>&lt;p<img src="https://s3.amazonaws.com/kinlane-productions/low-hanging-fruit/api-evangelist-low-hanging-fruit-story-screenshot.png" align="right" width="40%" style="padding: 15px;" />&lt;/p&gt;The concept of rogue APIs isn’t anything new. Instagram started out as a rogue API, and many leading platforms who are less than open with their platforms have rogue APIs. They are usually APIs that have been reverse engineered from mobile applications, and published to Github for other developers to use. I’m looking to marry this concept with <a href="https://apievangelist.com/2016/04/13/formalizing-my-approach-to-identifying-the-low-hanging-api-fruit/">my low hanging fruit API work</a>, where I help organizes start their API journey using data and content that is already on their website. Meaning, if it is already available on the web as table, form, or as CSV, spreadsheet, or other machine readable fie, it should be available via an API. As APIs are just the next step in the evolution, this is the logical place for the API journey to begin for many companies, organizations, institutions, and government agencies.</p>

<p>I’ve spidered the entire web site of organizations to extract lists of data sources they should be turning into APIs. I’ve done this at the request of the website owner, as well as without the permission. Honestly, it provides a pretty compelling look at the digital presence for an organization when you harvest raw data like this and publish to a Github repository. It isn’t a view that every organization is ready for, or has thought about. Making it an even more important place for organizations to start their API journey. APIs aren’t just about providing access to your data and content for your partners and 3rd party developers, it is about getting a handle on your digital assets, and how you present and provide access to this digital representation of your organization–something many suck at profoundly.</p>

<p>I’d like to invest more cycles into <a href="https://apievangelist.com/2016/04/13/formalizing-my-approach-to-identifying-the-low-hanging-api-fruit/">my low hanging fruit API research</a>. I’d love to take some government agencies and not just identify the low hanging fruit, but actually deploy a rogue API portal, and hang some of the APIs there. I’d like to do this to a couple of companies, institutions, as well as government agencies. I know that I’d get in trouble doing this with some companies, and even other entities, but I think it is a good way to instigate the API conversation, and I am willing to take the chance. I<a href="http://university-of-oklahoma-api.apievangelist.com/">had the University of Oklahoma contact me after I scraped their web site</a>, and I think I could recreate the effect with other groups. The trick is doing it in a transparent and observable way, with everything on Github, and communicated in a clear way. So, that someone knows who is behind it, and can reach out to do things in a more formal way–moving from a rogue API, to an official API.</p>

<p>To move this forward I am going to target a single government agency, scrape their website, and any other open daa I can find, and then public an official rogue API portal, and begin hanging some of the APIs there. I’m even going to open up read and write capabilities via the API for any developer who wants to register, and pay for access to the API. I’ll make sure things are clearly marked as being a unofficial rogue API, and provide contact information for anyone looking to communicate with me. I see low hanging fruit rogue APIs as being a way I can get the attention of companies, organizations, institutions, and government agencies when it comes to APIs. Even begin to build awarness and critical mass within a community around the digital assets shared on the website, and now via an API portal. A kind of activist API deployment, and beginning the public API journey.</p>

<p>This goes well beyond the concept of scraping for me. Which I’ve seen a number of startups come and go trying to accomplish. This is about helping show organizations the importance having a website as well as APIs to help counter scraping efforts, and get a better handle on their digital presence. It is meant to start the conversation with some very entrenched folks around the digital resources they are making public, and how APIs can help them better quantify their digital presence, and take control over that presence beyond just their website. If there is an agency, institution, or organization you’d like to see target, or even would be willing to invest some money in deploying a low hanging fruit rogue API portal for, feel free to let me know. I’ll be investing some cycles into this area of my research, just to make sure my content is fresh, while also seeing what new conversations I might be able jumpstart, so its a good time to get involved and fund what I’m doing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/15/deploy-low-hanging-fruit-rogue-api-portals-for-those-who-are-behind-the-curve/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/15/headless-cms-and-api-evolution-beyond-wordpress/">Headless CMS And The API Evolution Beyond WordPress</a></h3>
        <span class="post-date">15 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/headless/headless-cms-brackets.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am a fan of what WordPress has done for the online world. I feel like it has enabled a lot of folks to take some control over their web presence, and in some situations even made programmers out of business people who never thought that is what they’d end up doing. Even with all the positive benefits of WordPress, it has had some significant negative side effects which I think warrant us to begin looking beyond the existing ecosystem–something I’m hoping the headless CMS, and static website movement can help fuel. I’m not anti-WordPress, but I think the movement has run its course, and we can do better when it comes to helping folks take control over their web presence, as well as avoid much of the security challenges we experience as a result of WordPress.</p>

<p>If you aren’t familiar with the concept of headless, it is just about doing APIs, but centered around the end deliverable–the application. Headless focuses on decoupling content for use in apps, websites, or any other data-driven projects, allowing content to be created and managed independently from where it’s used. To us API-aware folks this is how All applications should behave, but I feel like the headless CMS concept is an important API gateway for business users who have drank the WordPress kool-aid, and are looking to do more with their CMS, and break free of some of the challenges of operating exclusively in a WordPress state of mind.</p>

<p>The most damaging aspect of WordPress I have felt is it’s emphasis on the blog. Everything is centered around the blog with WordPress installs, which isn’t the reason many folks should be doing a website in the first place, but because of their platform they feel compelled to. Headless CMS can be about managing ANY content, and crafting an API backend, that can deliver exactly the content you need in any website, web or mobile backend, bypassing the concept of the blog entirely. Which may not seem like much, but I’ve seen the blog become a pretty big obstacle for some individuals and companies looking to get a handle on their digital content.</p>

<p>The second most challenging aspect of operating WordPress is security. Managing a dynamically driven CMS that is so ubiquitous, and ultimately a huge target by bad actors is daunting for any seasoned IT people, but can be damaging for any unaware individual just trying to manage their website. I stopped running API Evangelist on WordPress because I couldn’t keep up with the security concerns, and operating my public presence as a series of statically published websites has done wonders to the security of my platform. I just do not feel that WordPress is sustainable as a CMS for individuals and companies who do not have the resources to properly manage and secure. There are many flavors of headless CMS, I’m looking to push the more static flavor I’be been seeing on the landscape.</p>

<p>I’m hopeful that the concept of headless coupled with a static CMS can be a proper gateway for individuals, companies, organizations, institutions, and government agencies who want to get a handle on a specific layer of managing their data and content, to learn about APIs. I find that most people aren’t interested in learning about APIs, they are interested in delivering API-driven solutions. Once they get a taste of this, they want to learn more. I feel like the WordPress API is going to be a complicated introduction to the world of APIs for many, but a simple, static, website implementation with a robust API backend provides a much more approachable view of what APIs can do. I’m going to keep an eye on everything headless, and keep scratching out stories to tel my audience about what they can do. I feel like they are key to helping us evolve beyond the Web 2.0 world WordPress set into motion, and begins to give us more control over our backends using APIs, but in a way that help us manage many different front-end applications, with CMS being the first stop for many individuals.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/15/headless-cms-and-api-evolution-beyond-wordpress/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/15/twitter-finally-begins-to-monetize-their-apis/">Twitter Finally Begins To Monetize Their APIs</a></h3>
        <span class="post-date">15 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/twitter/TwitterPremiumAPIs.gif" align="right" width="45%" style="padding: 15px;" /></p>
<p>It has been a long time coming, but T<a href="https://blog.twitter.com/developer/en_us/topics/tools/2017/introducing-twitter-premium-apis.html">witter has finally started charging for premium access to their APIs</a>. Until now, you could only access data via the free Twitter API with limitations, or pay to use Gnip at the enterprise level–nothing in between. <a href="http://apievangelist.com/2012/06/29/twitter-continues-to-restrict-access-to-our-tweets/">I’ve long complained about how Twitter restricts access to our tweets</a>, as well as <a href="http://apievangelist.com/2015/10/22/if-twitter-can-deliver-transparency-around-api-access-and-business-model-they-might-be-able-to-find-their-way-again/">the lack of transparency and honesty around their business model</a>. I’ve complained so much about it, I eventually stopped writing about it, and I never thought I’d see the day where Twitter starts charging for access to their platform.</p>

<p>While I have concerns about Twitter further limiting access to our data by charging for API access, their initial release has some positive signs that give me hope that they are monetizing things in a sensible way. They seem to be focused on monetizing some of the key paint points around Twitter API consumption, like being able to get more access to historical data, offer more Tweets per request, higher rate limits, a counts endpoint that returns time-series counts of Tweets, more complex queries and metadata enrichments, such as expanded URLs and improved profile geo information. Twitter seems to be thinking about the primary pain we all experience at the lower rungs of Twitter access, and focusing on making the platform more scalable for companies of all shapes and sizes–which has been core to my complaints.</p>

<p>Twitter even provides a quote from a client which highlights what I’ve been complaining about for some time about the inequity in Twitter API access:</p>

<blockquote>
  <p>I wish these premium APIs were available during our first few years. As we grew, we quickly ran into data limitations that prevented expansion. Ultimately, we raised a round of funding in order to accelerate our growth with the enterprise APIs. With the premium APIs, we could have bootstrapped our business longer and scaled more efficiently. - Madeline Parra, CEO and Co-Founder, Twizoo (now part of Skyscanner) @TwizooSocial</p>
</blockquote>

<p><img src="https://s3.amazonaws.com/kinlane-productions/twitter/twitter-plans.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>This demonstrates for me how venture capital, and the way the belief systems around it railroad folks down a specific path, and is blind to those of us who do not choose the path of venture capital. Twitter’s new pricing page starts things off at $149.00 / month, after the current free public tier of access, and climbs up five tiers to $2,499.00 / month. Giving you more access with each access tier you climb. While not a perfect spacing of pricing tiers, something that sill might be difficult for some startups to climb, it is much better than what was there before, or should I say, what wasn’t there. At least you can scale your access now, in a sensible, above the board vertical way, and not horizontally with separate accounts. Incentivizing the more positive behavior Twitter should want to see via their API.</p>

<p>Twitter should have started doing this back in 2008 and 2009 to help throttle bad behavior on their platform. The platform would look very different today if there had been a level playing field in the API ecosystem, and developers weren’t forced to scale horizontally. API monetization and planning isn’t just about generating revenue to keep a platform up and running serving everyone. Sensible API monetization is about being aware of, and intimately understanding platform behavior and charging to restrict, shape, and incentivize the behavior you want to see on your platform. Twitter has missed out on a huge opportunity to truly understand the API community at this level, as well as generate much needed revenue over the years. Shutting out many good actors, incentivizing bad actors, and really only recognizing trusted partners, while being blind to everything else.</p>

<p>After a decade of complaining about Twitter’s practices in their ecosystem, and their clear lack of a business model around their API. I am happy to see movement in this area. While there is a HUGE amount of work ahead of them, I feel like monetization of the API ecosystem, and crafting of a sensible API plan framework is essential to the health and viability of the platform. It is how they will begin to get a hold on the automation that plagues the platform, and begin de-platforming the illness that has become synonymous with Twitter during the 2016 election, while also leveling the playing field for many of us bootstrapped startups who are trying to do interesting things with the Twitter API. I’ll keep an eye on the Twitter premium APIs, and see where things head, but for now I’m cautiously optimistic.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/15/twitter-finally-begins-to-monetize-their-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/14/the-seo-benefits-of-publishing-your-api-operations-to-github/">The SEO Benefits Of Publishing Your API Operations To Github</a></h3>
        <span class="post-date">14 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-api-evangelist.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://kinlane.com/2013/01/02/all-side-projects-are-now-hosted-on-github/">I’ve been operating 100% of my public presence for API Evangelist on Github for almost five years now</a>. I really like the public layer of my world being static, but I also like the modularity that using Github repos for my projects have injected into my workflow. API Evangelist runs as almost 100 separate Github repositories, all using a common Jekyll template for the UI, making it look like you are always on the same API Evangelist website. Any website, application, data, or API begins as a Github repository in my world, and grows from there depending on how much energy I give a project during my daily work.</p>

<p>When I first started doing all of this, I worried a little bit about the search engine optimization of my public websites. From what I could tell in 2013, my sites ranked lower after the switch, but since I’m not in this for the numbers game, I shrugged it off. However, in 2017 the numbers look different, and some of the projects I’ve been cultivating on Github actually rank pretty high, even with minimal optimization on my part. This isn’t just the web front-end for my projects–I am also seeing the Github repositories themselves showing up pretty prominently in search engine results.</p>

<p>All of this is anecdotal. I haven’t done any official measurements or testing on the topic, I just don’t care enough to invest that amount of work in it all. I just have to note that in the last year I am seeing significant benefit for my SEO by running things on Github. When you bundle this with the search and discovery opportunities via Github, the benefits to running an API project on Github as much as possible makes sense. It is something I encourage all of my clients who are operating public APIs consider as part of their overall marketing, communications, and evangelism strategy.</p>

<p>I’ve been profiling all the possible ways that an API provider can use Github as part of operations for a number of years, but I think I will be reassessing all of this in light of the SEO benefits. Exploring the ways that you can increase the exposure of your public API operations using Github. I don’t think my usage of Github is the only reason behind my SEO domination when it comes to the world of APIs, but I am beginning to think it is playing a bigger role than I had expected. I’m publishing a new API for a client right now, where they have given me full control over publishing to the Github ecosystem, which is a perfect opportunity to rethink all of this, and begin to think a little more constructively about the SEO benefits of using Github for API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/14/the-seo-benefits-of-publishing-your-api-operations-to-github/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/14/glitch-is-where-you-will-learn-the-essential-human-side-of-operating-your-api/">Glitch Is Where You Will Learn The Essential Human Side Of Operating Your API</a></h3>
        <span class="post-date">14 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/glitch/1_-GNpo5PEhPm-1Ns4F76t9w.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>The biggest deficiency I see in the world of APIs is an ability to understand the human side of what we are all doing. The space is dominated by men, and people who have an understanding of, and deep belief in technology, over that of humans. The biggest problems APIs face across their life cycle is humans, and increasingly one of the biggest threats to humans is an API (ie. Twitter API automation &amp; harassment, IoT device exploitation, Facebook advertising, etc.) APIs encounter human friction because their creators didn’t anticipate the human portion of the equation, and APIs often get used against humans because their creators again didn’t anticipate human nature, and how people might use their technology for doing harmful things.</p>

<p>I rarely see folks in the API sector focusing on the human side of the equation, but I am pleasantly surprised to see a constant drumbeat coming out of <a href="https://glitch.com">Glitch</a>, “the friendly community where you’ll build the app of your dreams.” Glitch is a platform where API consumers can remix apps that use APIs, and API providers can engage with API consumers who are building and remixing interesting things. Glitch has been on my list to write about more, and is something I’ll be using, and focusing more time on in future posts, but I wanted to just highlight how much focus is spent on the human side of the API world over at Glitch.</p>

<p>Take a look at the articles coming out of the Glitch blog, <a href="https://medium.com/glitch/dev-rel-success-requires-an-ongoing-connection-to-a-community-of-peers-ed660b40b62">Dev Rel success requires an ongoing connection to a community of peers</a>, and <a href="https://medium.com/glitch/dev-rel-must-be-supported-with-ongoing-investment-in-professional-development-19ba90326b7a">Dev Rel must be supported with ongoing investment in professional development</a>–all part of the ongoing stories around <a href="https://medium.com/glitch/a-developer-relations-bill-of-rights-21381920e273">a Developer Bill of Rights</a>, which Glitch has been very vocal about, emphasizing the importance of the human aspects of doing APIs and building applications. Which is the first startup I’ve seen come along that is investing so much energy into discussing what really makes all of this actually work.</p>

<p>The core of Glitch is all about building apps. Which is the same core objective of API providers. However, as you begin to spend time there, you begin to learn a lot more about developer relations (dev rel), and the focus on applications just becomes part of the conversation. They do a great job to identify the human elements of building applications, and delivering meaningful things for not just humans, but humans at large organizations. There is talk of working with marketing and sales, and helping developers and API providers not forget about the little meaningful details that can make or break your API efforts. I’m going to spend some time building an app on Glitch, and remix using some of what is already there. I found I’ve learned a lot on their blog, and I am interested in learning more about what they are bringing to the community.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/14/glitch-is-where-you-will-learn-the-essential-human-side-of-operating-your-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/14/provide-me-with-api-discovery-using-an-openapi-diff/">Could I Please Get An API Discovery Tool That Evaluates An OpenAPI Diff</a></h3>
        <span class="post-date">14 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/openapi-logo.png" align="right" width="30%" style="padding: 25px;" /></p>
<p>I am increasingly tracking on OpenAPI definitions published to Github by leading API providers I track on. Platforms like <a href="http://apievangelist.com/2017/11/12/stripes-openapi-is-available-on-github-in-version-30/">Stripe</a>, <a href="http://apievangelist.com/2017/05/22/box-goes-all-in-on-openapi/">Box</a>, <a href="http://apievangelist.com/2017/03/01/new-york-times-manages-their-openapi-using-github/">New York Times</a> are actively managing their OpenAPI definitions using Github, making them well suited for integration into their platform operations, API consumer scenarios, and even within analyst systems like what I have going on as the API Evangelist.</p>

<p>Once I have an authoritative source of an OpenAPI, meaning a public URI for an OpenAPI that is actively being maintained by the API provider, I have a pretty valuable feed into the roadmap, as well as change log for an API. I feel like we are getting to the point where there are enough authoritative OpenAPIs that we can start using as a machine readable notification and narrative tool for helping us stay in tune with one or many APIs across the landscape. Helping us stay in tune with APIs in real-time, and giving APIs an effective tool for communicating out changes to the platform–we just need more OpenAPIs, and some new tooling to emerge.</p>

<p>I’m envisioning an OpenAPI client that regularly polls OpenAPIs and caches them. Anytime there is a change it does a diff, and isolated anything new. Think of an RSS reader, but for OpenAPIs, and going well beyond new entries, and actually creates a narrative based upon the additions and changes. Tell me about the new paths added, and any new headers, parameters, or maybe how the schema has grown. Provide me insights on what has changed, and possibly what has been removed, or will be removed in future editions. As an API analyst, I’d like to be able to have an OpenAPI-enabled approach to receiving push notifications when an API changes, with a short, concise summary about what has change in my inbox, via Twitter, or Github notification.</p>

<p>OpenAPI already provides API discovery features through the documentation it generates, and I’m increasingly using Github to find new APIs after they publish their OpenAPIs to Github, but this type of API discovery and notification at the granular level would be something new. If there was such tooling out there, it would provide yet another incentive for API provides to publish and maintain an active, up to date OpenAPI definition. This is a concept I’d like to also see expanded to the API operational level using <a href="http://apisjson.org/">APIs.json</a>, where we can receive notifications about changes to documentation, pricing, SDKs, and other critical aspects of API integration, beyond just the surface area of the API. All of this stuff will take many years to unfold, as it has taken over five years for us to reach a critical mass of OpenAPI definitions to emerge, I suspect it will take another five to ten years for robust tooling to emerge at this level, which also depends on many API definitions to be available.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/14/provide-me-with-api-discovery-using-an-openapi-diff/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/14/added-a-simple-bulk-api-for-my-human-services-data-api/">I Added A Simple Bulk API For My Human Services Data API</a></h3>
        <span class="post-date">14 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/human-services-data-bulk-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>The core Human Services Data API allows for adding of organizations, locations, services, and contacts one by one using a single POST on the core API paths for each available resource. However, if you want to add thousands, or even hundreds of records, it can quickly become cumbersome to submit each of the calls, so I wanted to introduce a simple Human Services Bulk API for helping handle the adding of large quantity of data, on a one-time, or recurring basis. I know there job queuing solutions available out there, but my goal with this project is to focus on the API definition, as well as the backend system(s). For this round, I just want to get a simple baseline definition in place, with a simple API backend for orchestrating. I’ll update to support AWS, and other queuing solutions as part of the road-map–further hammering out a consistent <a href="http://developer.open.referral.adopta.agency/#HSDA Bulk">HSDA Bulk API</a>.</p>

<p>The first dimension of this new HSDA Bulk API focuses on providing paths for POSTing large quantities of data across the core human service resources:</p>

<ul>
  <li>organizations/ - POST complete organizations JSON records as array.</li>
  <li>locations/ - POST complete locations JSON records as array.</li>
  <li>services/ - POST complete services JSON records as array.</li>
  <li>contacts/ - POST complete contacts JSON records as array.</li>
</ul>

<p>You can submit as many records to each of these paths (well, within reason), including the sub-resources for each object like physical address, phones, etc. When POSTed each record doesn’t immediately go into the main HSDA database. Each entry is entered into a jobs system, which can be run on a schedule, based upon events, or maybe just wait until the middle of the night. The goal is to offload the bulk insert to a job system, which can spread things out over time, and minimize negative impact on resources strapped human services database. HSDA Bulk API runs as a separate microservice which can be run side by side with the core HSDA implementation, or possibly scaled on separate infrastructure to allow for handling of expected loads.</p>

<p>The next dimension of this new HSDA Bulk API allows for importing of <a href="https://openreferral.readthedocs.io/en/latest/hsds/reference/">HSDS datapackage.json files</a>, bringing things back to basics with the Human Services Data Specification(HSDS). <a href="https://s3.amazonaws.com/kinlane-productions/open-referral/sample-datapackage/datapackage.json">The JSON file contains a list of paths to individual HSDS CSV files</a>, which are then processed as individual resources, with each record inserted as a job, for running on schedule, event, or other approach. Adding a more comprehensive approach to loading up large datasets into any human services system using an HSDA API, while also continuing to smooth out the impact of the core system using jobs.</p>

<p>With both of these dimension, you can perform bulk uploads of data using the individual organizations, locations, services, and contacts page, as well as a complete datapackage.json file. Next I will be hammering on the demo API I have a bunch more, to harden the code, and see if I’m missing any details. After that I’ll started looking at switching out the custom backend I have with an AWS API Gateway managed, and possibly AWS SQS or other jobs solution. I’m trying to keep <a href="https://github.com/human-services/portal/blob/master/_data/api-commons/openapi-hsda-bulk.yaml">the HSDA Bulk API definition</a> simple, but also allow for scaling up with other more robust backend system. For now, I’d call this edition of <a href="http://developer.open.referral.adopta.agency/#HSDA Bulk">the HSDA Bulk API</a> to be a decent v1.0 start for this new HDSA microservice.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/14/added-a-simple-bulk-api-for-my-human-services-data-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/13/i-added-a-taxonomy-api-to-support-the-human-services-data-api-hsda/">I Added A Taxonomy API To Support The Human Services Data API (HSDA)</a></h3>
        <span class="post-date">13 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/open-referral/human-services-taxonomy-api.png" align="right" width="45%" style="padding: 15px" /></p>
<p>I have been organizing <a href="http://org.open.referral.adopta.agency/">my Human Services Data API (HSDA) specification work</a> into separate microservices as part of version 1.0 for the API definition that cities and other organizations running 211 operations can pick and choose which aspects they want to run. One service I carved off of the move from version 1.0 to 1.1 of the specification was taxonomy, and how the human services are categories and organized. I saw there was more research to be done around 211 taxonomy, and I felt it had the potential to be a separate but supporting service to augment what Open Referral is already trying to do with <a href="http://org.open.referral.adopta.agency/#Specification">the Human Services Data API (HSDA) specification</a>.</p>

<p><a href="http://developer.open.referral.adopta.agency/#HSDA Taxonomy">The HSDA Taxonomy API specification provides a handful of API paths for creating, reading, updating, and deleting taxonomy used in any HSDA implementation</a>. I have populated my demo API with <a href="https://github.com/auntbertha/openeligibility">the Open Elegibility taxonomy</a> to help jumpstart folks, but any HSDA provider can populate with their own custom taxonomy, or other existing format. Then you can apply any taxonomy to any of the services stored within an HSDA database, and there is an API path for querying services by taxonomy. Next I’ll make sure you can search by taxonomy, and see the taxonomy as part of the response body for all services returned across HSDA, HSDA Search, and HSDA Taxonomy.</p>

<p>HSDA Taxonomy API is a simple service, but it is one that I want to get up and running, and maturing quickly. I feel like there is a lot of opportunity around 211 taxonomy aggregation, an further standardizing and evolving on top of Open Eligibility, or establishing a separate Open Referral taxonomy that can be open source. The current 211 taxonomy dominating the landscape is the AIRS 211 format, which is a proprietary taxonomy, something that I’ll address in a separate post. Have a shared vocabulary around human services is almost as important as the data itself–if you can’t find meaningful services, in a consistent way across providers, the data itself becomes much less valuable.</p>

<p><a href="http://developer.open.referral.adopta.agency/#HSDA Taxonomy">I have launched a demo copy of the API at my Human Services Demo API</a>, and next I am working on a handful of UI elements for managing, browsing, and searching for services using the HSDA Taxonomy API. My goal is to get v1.0 of the HSDA Taxonomy specification being discussed as part of the overall HSDA governance process, while I’m also hardening the API, and UI tooling via a couple of HSDA implementations I’m working on. Then I’ll circle back in a couple months and see where things are, learn more about some other taxonomies, and hopefully pull together more of a strategy around how to get people sharing 211 taxonomies, and speaking a common language about how they categorize human services, as well as store and provide access to human service organizations, locations, and services.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/13/i-added-a-taxonomy-api-to-support-the-human-services-data-api-hsda/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/12/stripes-openapi-is-available-on-github-in-version-30/">Stripes OpenAPI Is Available On Github In Version 3.0</a></h3>
        <span class="post-date">12 Nov 2017</span>
        <p><a href="https://github.com/stripe/openapi"><img src="https://s3.amazonaws.com/kinlane-productions/stripe/stripes-openapi-specification-on-github.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I can’t write about every API provider who publishes their OpenAPI to Github, there are just too many. But, I can write about the rockstar API providers who do though, and showcase what they are doing, so I can help influence the API providers who have not started publishing their OpenAPIs in this way. If you are looking for a solid example of a leading API provider publishing their OpenAPI to Github, <a href="https://github.com/stripe/openapi">I recommend taking a look at the payment provider Stripe</a>.</p>

<p><a href="https://github.com/stripe/openapi">Their repository contains OpenAPI specifications for Stripe’s API</a>, with multiple files available in the in the openapi/ directory:</p>

<ul>
  <li><strong>spec3.{json,yaml}</strong> - OpenAPI 3.0 spec.</li>
  <li><strong>spec2.{json,yaml}</strong> - OpenAPI 2.0 spec. We’re continuing to generate this for now, but it will be deprecated in favor of spec3.</li>
  <li><strong>fixtures3.{json,yaml}</strong> - Test fixtures for resources in spec3. See below for more information.</li>
  <li><strong>fixtures2.{json,yaml}</strong> - Test fixtures for resources in spec2.</li>
</ul>

<p>It is pretty exciting to see them already embracing version 3.0. They even provide a listing of the OpenAPI vendor extensions they are using, which are specific to their API. <a href="http://openapi.toolbox.apievangelist.com/">I’ll be adding these to my OpenAPI toolbox</a> when I have the time, adding to the number of vendor extensions I have indexed. Stripe provides another pretty solid example of an API provider taking ownership of their OpenAPI spec, publishing to Github for their consumers to put tow rok, but clearly they are also using as part of their own internal workflows as well.</p>

<p>Every API provider should have a Github repository with an up to date OpenAPI <a href="https://github.com/stripe/openapi">like Stripe does</a>. I know many API architects envision a hypermedia API discovery landscape, where APIs are defined and discoverable by default, but I think an OpenAPI on Github is the best we can hope for at this stage in the evolution of the space. With the momentum I’m seeing in the number API providers publishing their OpenAPIs to Github, I’m feeling like Github is going to become the continuous integration, API discovery engine we’ve all been looking for over the last decade. Allowing us to discover, integrate and orchestrate with our APIs across the API life cycle–we just need everyone to follow Stripe’s lead. ;-)</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/12/stripes-openapi-is-available-on-github-in-version-30/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/12/locking-up-any-taxonomy-is-short-sighted-in-todays-online-environment/">Locking Up Any Open Data Taxonomy Is Short Sighted In Todays Online Environment</a></h3>
        <span class="post-date">12 Nov 2017</span>
        <p><a href="https://211taxonomy.org/"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/open-referral/211tax.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I published a taxonomy API as part of my Human Services Data API (HSDA) work recently, and as part of the work I wanted it to support a handful of the human services taxonomies available currently. The most supported taxonomy available out there is <a href="https://211taxonomy.org/">the AIRS/211 LA County Taxonomy</a>. It is a taxonomy in use by 211 of LA County, as well as owned and licensed by them. From what I gather, it is the most common format in use, and <a href="http://211wny.org/index.php/airs-license">you can find licensing pages for it from other municipal 211 providers</a>. Before you can download a copy of the taxonomy you have to agree to the license I’ve posted at the bottom of this post, something I was unwilling to do.</p>

<p>Taxonomies shouldn’t be locked up this way. Let alone taxonomies for use in open data, helping citizens at the municipal level. I understand that 211 LA will argue that they’ve put a bunch of work into the schema, and therefore they want to protect what they view their intellectual property, but in 2017 this is wrong. This isn’t the way things should be done, sorry. The AIRS taxonomy should be openly available, and reusable in a machine readable format, and evolved by an open governance process. There is no reason for this valuable taxonomy, that has the potential to make our cities better, should be locked up like this–it needs to be widely used, and adopted without any legal friction along the way.</p>

<p>I understand that it takes work, and resources to keep a taxonomy meaningful, and usable, but we should not stand in the way of people finding human services, and restricting 211 providers from using the same vocabulary. There are other was to generate revenue, and evolve forward a taxonomy in an online, collaborative environment, much like we are currently doing with open source software. This kind of stuff drives me nuts, and the licensing around this important technology is something I’ll keep an eye on, and contributing whatever I can to help stimulate the discussion in favor of open sourcing. In the absence of AIRS, I have adopted <a href="https://github.com/auntbertha/openeligibility">an open source 211 taxonomy called Open Eligibility</a>, but alas it seems like an effort that has gone dormant. <a href="https://github.com/human-services/openeligibility">I have forked the specification</a>, added more simpler JSON, CSV, and JSON formats which I will be working with it alongside the rest of <a href="http://developer.open.referral.adopta.agency/#HSDA Taxonomy">my Human Services Data API (HSDA) taxonomy work</a>.</p>

<p>Taxonomy licensing is another area of consideration I’ll add to <a href="http://licensing.apievangelist.com/">my API licensing research</a>, as well as for guidance around my HSDA work. I wish this type of stuff didn’t still happen in 2017. It is a relic of another time, and in a digital age, taxonomies for any aspect of public infrastructure should be openly licensed, and reusable by everyone. I would like to see <a href="http://openreferral.org/">Open Referral</a> expand its portfolio to push forward one or more taxonomies for not just human services, but also organizations, locations, and potentially other relevant schema we are pushing forward. I see Open Referral as an incubator for schema, OpenAPI definitions, as well as datasets like 211 taxonomy, helping provide a commons where 211 organizations can find what they need.</p>

<hr />

<p>TAXONOMY SUBSCRIPTION AGREEMENT</p>

<p>CAREFULLY READ THIS TAXONOMY SUBSCRIPTION AGREEMENT BEFORE DOWNLOADING, INSTALLING OR USING THE TAXONOMY (DEFINED BELOW). TAKING ANY STEP TO DOWNLOAD, INSTALL OR USE THE TAXONOMY IN ANY WAY CONSTITUTES YOUR ASSENT TO AND ACCEPTANCE OF THIS AGREEMENT AND IS A REPRESENTATION BY YOU THAT YOU HAVE THE AUTHORITY TO ASSENT TO AND ACCEPT THIS AGREEMENT. IF YOU DO NOT AGREE WITH THE TERMS AND CONDITIONS OF THIS AGREEMENT, YOU MUST NOT DOWNLOAD, INSTALL OR USE THE TAXONOMY AND YOU MUST IMMEDIATELY RETURN THE TAXONOMY (AND NOT KEEP ANY COPIES) TO 211 LA AND SO NOTIFY 211 LA OF SUCH FAILURE TO AGREE.</p>

<p>Taxonomy Subscription Agreement (“Agreement”) contains the terms and conditions by which Information and Referral Federation of Los Angeles, Inc., doing business as 211 of LA County (“211 LA”) provides a subscription license to use the Taxonomy. This Agreement is a binding legal contract between you (both the individual downloading, installing and/or using the Taxonomy and, if applicable, the legal entity on behalf of which such individual is acting) (“Subscriber”) and 211 LA.</p>

<p>1.Definitions</p>

<p>Subscriber Database means a database of health and human services information and resources that is created and maintained by Subscriber.</p>

<p>Subscriber Directory means a printed directory or electronic read-only directory of health and human services that is created and maintained by Subscriber using a Subscriber Database. As used in this definition, “read-only” means an electronic version of a directory in which the information in such directory, including the Taxonomy, cannot be modified or altered in any way.</p>

<p>Subscription Order Form means the form(s) used by 211 LA for allowing subscribers to purchase or renew subscription licenses of the Taxonomy.</p>

<p>Taxonomy means the Taxonomy of Human Services maintained and made generally available by 211 LA for purposes of indexing health and human services information and resources, including its terms, definitions, codes and references and any and all updates, upgrades, enhancements or other modifications that may be made available by 211 LA to Subscriber from time to time.</p>

<p>Taxonomy Website means a website hosted by 211 LA through which subscription licenses for the Taxonomy can be purchased and the Taxonomy can be downloaded.</p>

<p>2.Taxonomy License.</p>

<p>Limited License. Subject to Subscriber’s compliance with the terms and conditions of this Agreement (including, without limitation, Sections 2.2, 2.3, 3 and 4), 211 LA hereby grants to Subscriber a limited, non-exclusive, non-transferable, non-sublicensable license to:</p>

<p>access portions of the Taxonomy Website designated by 211 LA for subscribers and download the Taxonomy as made generally available thereon;</p>

<p>use the Taxonomy, including its codes, terms, definitions, and references, as a classification structure for indexing health and human services information and resources in a Subscriber Database;</p>

<p>include Taxonomy terms in a survey instrument prepared by Subscriber as reasonably necessary to collect information from third party organizations regarding health and human services, provided that such survey instrument may only include the Taxonomy terms that Subscriber has used to index health and human services information and resources in the Subscriber Database;</p>

<p>include Taxonomy terms as an index in a Subscriber Directory distributed or otherwise made available to third parties (including over the Internet), provided that (i) the proceeds of any monetary or other consideration provided in connection with such distribution are provided to a non-profit, charitable organization and (ii) any such Subscriber Directory may only include the Taxonomy terms that Subscriber has used to index health and human services information and resources in the Subscriber Database, along with any higher level terms on the same branch that are needed to display the hierarchical structure; and</p>

<p>include the Taxonomy definitions in the Subscriber Directory referenced in Section 2.1(d) above, provided that such Subscriber Directory may include only those definitions for terms that Subscriber has used to index health and human services information and resources in the Subscriber Database.</p>

<p>License Restrictions. Nothing contained in this Agreement will be construed as conferring upon Subscriber, by implication, operation of law or otherwise, any license or other rights except as expressly set forth in Section 2.1. Subscriber shall not, and shall not allow any third party to: (i) copy, display or otherwise use all or any portion of the Taxonomy except as incorporated into a Subscriber Directory as permitted in Section 2.1; (ii)  transmit or otherwise distribute the Taxonomy (or any portion of the Taxonomy) as a separate product, module or material to any end user or other third party, or make the Taxonomy (or any portion of the Taxonomy) available to any end user or other third party as a downloadable file separate from a Subscriber Directory; (iii) transmit or otherwise distribute any portion of the Taxonomy for commercial purposes or otherwise for profit or other monetary gain; (iv) incorporate the Taxonomy (or any portion of the Taxonomy) into any database or software through which the Taxonomy (or any portion of the Taxonomy) can be printed, downloaded or modified; or (v) loan, lease, resell, sell, offer for sale, sublicense, adapt, translate, create derivative works of or otherwise modify or alter all or any part of the Taxonomy. Further, Subscriber acknowledges and agrees that use of the Taxonomy Website is also subject to 211 LA’s Terms of Use, as made available thereon and updated from time to time.</p>

<p>Notice.</p>

<p>If the Taxonomy, or any potion of the Taxonomy (including its terms, codes, definitions or references), are utilized in a Subscriber Directory that is published, transmitted, distributed or otherwise made available to third parties, by any means or medium, the Subscriber shall prominently display the following notice in such directory:</p>

<p>Copyright © 1983-2007, Information and Referral Federal of Los Angeles County, Inc. All rights reserved. The index, codes and definitions of the terms contained herein are the intellectual property of Information and Referral Federal of Los Angeles, Inc. and are protected by copyright and other intellectual property laws. No part of this listing of human services terms and definitions may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electrical, mechanical, photocopying, recording or otherwise without the prior written permission of the Information and Referral Federal of Los Angeles County, Inc.</p>

<p>If the Taxonomy is displayed or otherwise made available in a Subscriber Directory via the Internet, Subscriber shall prominently include a link to a copyright acknowledgement statement that 211 LA maintains online, currently located at: http://www.211la.org/Content.asp?Content=Taxonomy&amp;SubContent=Copyright.</p>

<p>3.Intellectual Property Ownership.</p>

<p>Ownership of the Taxonomy. As between 211 LA and Subscriber, 211 LA retains and shall own all right, title and interest in and to the Taxonomy and any derivative works or other modifications thereof, including, without limitation, all copyright, trademark, trade secret and other intellectual rights, subject only to the limited license set forth herein. Subscriber does not acquire any other rights, express or implied, in the Taxonomy. Subscriber hereby assigns, and agrees to assign, to 211 LA all right, title and interest (including all intellectual property rights) throughout the world that Subscriber has or may have in the Taxonomy (including with respect to any modifications suggested by, or other contributions made by, Subscriber), which assignment shall be deemed effective as to any future modifications or contributions immediately upon the creation thereof. Subscriber further irrevocably waives any “moral rights” or other rights with respect to attribution of authorship or integrity of any modifications suggested by, or other contributions made by, Subscriber under any applicable law under any legal theory.</p>

<p>Access and Security.</p>

<p>Subscriber is solely responsible for providing, installing and maintaining at Subscriber’s own expense all equipment, facilities and services necessary to access and use the Taxonomy, including, without limitation, all computer hardware and software, modems, telephone service and Internet access.</p>

<p>Subscriber may be issued or otherwise assigned a user identification and/or password (collectively, “User Identifications “) to access the Taxonomy and/or Taxonomy Website as permitted hereunder. Subscriber is solely responsible for tracking all use of the User Identifications and for ensuring the security and confidentiality of all User Identifications. Subscriber acknowledges that Subscriber is fully responsible for all liabilities incurred through the use of any User Identification and that any download, transmission or transaction under a User Identification will be deemed to have been performed by Subscriber.</p>

<p>Subscriber shall ensure that each of its employees complies with this Agreement, including, without limitation, the license restrictions in Section 2.2 and shall protect the Taxonomy from any use that is not permitted under this Agreement. Subscriber shall promptly notify 211 LA of any unauthorized copying, display, modification, transmission, distribution, or use of the Taxonomy of which it becomes aware.</p>

<p>211 LA reserves the right at any time and without prior notice to Subscriber to change the hours of operation of the Taxonomy Website or to limit Subscriber’s access to the Taxonomy (i) in order to perform repairs or to make updates, upgrades, enhancements or other modifications or (ii) in response to unforeseen circumstances or circumstances beyond 211 LA’s reasonable control. 211 LA may add or withdraw elements of to or from the Taxonomy and/or Taxonomy Website from time to time in its sole discretion, although Subscriber acknowledges and agrees that 211 LA has no obligation to maintain or provide any updates, upgrades, enhancements, or other modifications to the Taxonomy or Taxonomy Website.</p>

<p>Verification. 211 LA may, during the term of this Agreement and with seven (7) days prior notice, request and gain access to Subscriber’s premises for the limited purpose of conducting an inspection to determine and verify that Subscriber is in compliance with the terms and conditions hereof. Subscriber shall promptly grant such access and cooperate with 211 LA in the inspection; provided, however, the inspection shall be conducted in a manner not intended to disrupt unreasonably Subscriber’s business and shall be restricted in scope, manner and duration to that reasonably necessary to achieve its purpose.</p>

<p>4.Payment</p>

<p>Payment. In consideration for the subscription license granted under Section 2.1, Subscriber shall pay the applicable fees as set forth in the Subscription Order Form and/or Taxonomy Website. All fees are nonrefundable. Any information that you may provide in connection with obtaining the subscription (including any nonprofit and/or AIRS membership information) may be verified and your subscription license may be placed on hold or terminated in the event of inaccuracies or discrepancies.</p>

<p>Taxes. In addition to all applicable fees, Subscriber shall pay all sales, use, personal property and other taxes resulting from this Agreement or any activities under this Agreement, excluding taxes based on 211 LA’s net income, unless Subscriber furnishes proof of exemption from payment of such taxes in a form reasonably acceptable to 211 LA. Further, If Subscriber is required by law to deduct or withhold any taxes, levies, imposts, fees, assessments, deductions or charges from or in respect of any amounts payable hereunder, (a) Subscriber shall pay the relevant taxation authority the minimum amounts necessary to comply with the applicable law, (b) Subscriber shall make such payment prior to the date on which interest or penalty is attached thereto, and (c) the amounts payable hereunder shall be increased as may be necessary so that after Subscriber makes all required deductions or withholdings, 211 LA shall receive amounts equal to the amounts it would have received had no such deductions or withholdings been required.</p>

<p>Discounts. From time to time, and in 211 LA’s sole discretion, 211 LA may offer discounts to particular organizations (such as nonprofits, government agencies and members of the Alliance of Information and Referral Systems (AIRS)). If such a discount is offered to Subscriber, Subscriber may be required to submit proof of nonprofit status (such as a federal EIN number), a valid AIRS membership number and/or additional information in order to receive such discounts.</p>

<p>Delivery. The Taxonomy is only made available electronically via download from the Taxonomy Website and, unless otherwise agreed by 211 LA in writing on a case-by-case basis, will not be delivered in any other form or via in any other method.</p>

<p>5.Term and Termination</p>

<p>Term. Subscriber’s rights with respect to the Taxonomy will commence on the date full payment of license fees are received and approved by 211 LA and will continue for an initial period of one (1) year, at which point Subscriber’s rights and this Agreement shall expire. If available, Subscriber may renew its subscription license via the Taxonomy Website, which renewal will be subject to and governed by 211 LA’s then-current fees and then-current terms and conditions (which may include a new or updated Taxonomy Subscription Agreement).</p>

<p>Termination of Agreement. Subscriber may terminate this Agreement at any time by sending an email message addressed to taxonomy@infoline la.org, with the subject “Subscription Cancellation.” Further, if Subscriber commits any breach of any provision of this Agreement, 211 LA will have the right to terminate this Agreement (including the rights granted to Subscriber under 2.1) by written notice, unless Subscriber remedies such breach to 211 LA’s reasonable satisfaction within thirty (30) calendar days after receiving written notice from 211 LA.</p>

<p>Effect of Termination.</p>

<p>Upon termination or expiration, and except as expressly provided in Section 5.3(b), the rights and license granted to Subscriber hereunder shall immediately cease and Subscriber will immediately cease all use of the Taxonomy, will destroy all copies of the Taxonomy and will promptly certify such action to 211 LA in writing. Without limitation of the foregoing, 211 LA may immediately terminate Subscriber’s account and ability to access the Taxonomy via the Taxonomy Website upon any expiration or termination of this Agreement. Expiration or termination of this Agreement will not limit either party from pursuing other remedies available to it, including injunctive relief.</p>

<p>The parties’ rights and obligations under Sections 2.2, 3, 4, 5.3, 6, and 7 will survive expiration or termination of this Agreement. Further, unless this Agreement has been terminated by 211 LA for breach, the rights granted to Subscriber under Sections 2.1(b) through (e), as well as the rights and obligations set forth in Section 2.3, shall survive and continue following any expiration or termination of this Agreement, but only with respect to the most recent version of the Taxonomy downloaded by Subscriber from the Taxonomy Website as of the date of expiration or termination and provided that Subscriber’s rights shall continue to be subject to termination by 211 LA under Section 5.2.</p>

<p>6.Disclaimer of Warranty and Limitation of Liability.</p>

<p>Disclaimer of Warranty. 211 LA does not represent that the Taxonomy will meet any expectations or specifications of Subscriber. THE TAXONOMY AND ANY OTHER INFORMATION, PRODUCTS OR SERVICES PROVIDED BY 211 LA TO SUBSCRIBER ARE PROVIDED “AS IS,” WITHOUT WARRANTY OF ANY KIND. 211 LA HEREBY DISCLAIMS ANY AND ALL WARRANTIES OF ANY KIND, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, WITHOUT LIMITATION, THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, SATISFACTORY QUALITY, ACCURACY, TITLE AND NONINFRINGEMENT, AND ALL WARRANTIES THAT MAY ARISE FROM COURSE OF PERFORMANCE, COURSE OF DEALING OR USAGE OF TRADE.</p>

<p>Limitation of Liability. TO THE EXTENT PERMITTED BY APPLICABLE LAW: (I) IN NO EVENT WILL 211 LA BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL OR PUNITIVE DAMAGES, OR DAMAGES FOR LOSS OF PROFITS, REVENUE, BUSINESS, SAVINGS, DATA, USE OR COST OF SUBSTITUTE PROCUREMENT, INCURRED BY SUBSCRIBER OR ANY THIRD PARTY, WHETHER IN AN ACTION IN CONTRACT OR TORT, EVEN IF THE SUBSCRIBER OR THE OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES OR SUCH DAMAGES ARE FORESEEABLE AND (II) 211 LA’S LIABILITY FOR DAMAGES HEREUNDER WILL IN NO EVENT EXCEED THE FEES RECEIVED BY 211 LA HEREUNDER. SUBSCRIBER ACKNOWLEDGES THAT THE LIMITATIONS OF LIABILITY IN THIS SECTION 6.2 AND IN THE OTHER PROVISIONS OF THIS AGREEMENT, AND THE ALLOCATION OF RISK HEREIN, ARE AN ESSENTIAL ELEMENT OF THE BARGAIN BETWEEN THE PARTIES, WITHOUT WHICH 211 LA WOULD NOT ENTER INTO THIS AGREEMENT.</p>

<p>7.Miscellaneous Provisions</p>

<p>No Assignment. Subscriber may not assign, sell, transfer, delegate or otherwise dispose of, whether voluntarily or involuntarily, by operation of law or otherwise, this Agreement, or any rights or obligations under this Agreement. Any purported assignment, transfer, or delegation by Subscriber will be null and void. Subject to the foregoing, this Agreement will be binding on the parties and their respective successors and assigns.</p>

<p>Relationship Between the Parties. The parties shall at all times be and remain independent contractors. Nothing in this Agreement creates a partnership, joint venture or agency relationship between the parties.</p>

<p>Governing Law. This Agreement is to be construed in accordance with and governed by the internal laws of the State of California (as permitted by Section 1646.5 of the California Civil Code (or any similar successor provision)) without giving effect to any choice of law rule that would cause the application of the laws of any jurisdiction other than the internal laws of the State of California to the rights and duties of the parties. In the event of any controversy, claim or dispute between the parties arising out of or relating to this agreement, such controversy, claim or dispute may be tried solely in a state or federal court located within the County of Los Angeles, California and the parties hereby irrevocably consent to the jurisdiction and venue of such courts.</p>

<p>Severability and Waiver. If any provision of this Agreement is held to be illegal, invalid, or otherwise unenforceable, such provision will be enforced to the extent possible consistent with the stated intention of the parties, or, if incapable of such enforcement, will be deemed to be severed and deleted from this Agreement, while the remainder of this Agreement will continue in full force and effect. The waiver by either party of any default or breach of this Agreement will not constitute a waiver of any other or subsequent default or breach.</p>

<p>Headings. The headings used in this Agreement are for convenience only and shall not be considered in construing or interpreting this Agreement.</p>

<p>No Third Party Beneficiaries. This Agreement is made and entered into for the sole protection and benefit of the parties hereto, and is not intended to convey any rights or benefits to any third Party, nor will this Agreement be interpreted to convey any rights or benefits to any person except the parties hereto.</p>

<p>Entire Agreement. This Agreement, along with the Terms of Use made available on the Taxonomy Website, constitutes the complete agreement between the parties and supersedes any prior or contemporaneous agreements or representations, whether written or oral, concerning the subject matter of this Agreement. This Agreement may be changed by 211LA from time to time immediately upon notice to Subscriber (which may be achieved by posting an updated copy of this Agreement on the Taxonomy Website) or by written agreement of the parties. Continued use of the subscriber portions of the Taxonomy Website following any change constitutes acceptance of the change.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/12/locking-up-any-taxonomy-is-short-sighted-in-todays-online-environment/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/12/i-finally-have-a-weekly-email-newsletter-roundup-of-api-evangelist-posts/">I Finally Have A Weekly Email Newsletter Roundup Of API Evangelist Posts</a></h3>
        <span class="post-date">12 Nov 2017</span>
        <p><a href="http://apievangelist.com/#Newsletter"><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/api-evangelist-newsletter.png" align="right" width="35%" style="padding: 15px;" /></a></p>
<p>I’ve had people asking me for an email newsletter containing everything I’ve done over the week for quite a while now, and I finally got around to do doing it. I’m now using MailChimp to pull in the last 20 API Evangelist blog posts and send out as a digest each Monday morning. Providing a summary of everything I wrote for the previous week.</p>

<p>I’m thankful for services like MailChimp which help me get up and running with things like a newsletter quickly. Then  can scale it over time, and even use their API if I want. Without service providers like this I’d never have things like a newsletter. Here is the email newsletter sign up form from MailChimp, and you can always find <a href="http://apievangelist.com/#Newsletter">on the bottom of the home page for API Evangelist</a>:</p>

<!-- Begin MailChimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css" />

<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
</style>

<div id="mc_embed_signup">
<form action="https://apievangelist.us17.list-manage.com/subscribe/post?u=36a583d2f353a9d31387227ea&amp;id=0593412b68" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
    <div id="mc_embed_signup_scroll">
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required="" />
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_36a583d2f353a9d31387227ea_0593412b68" tabindex="-1" value="" /></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button" /></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->

<p>I am not a big email person, as many of you know. However, I understand that many of you are, and I’m seeing a resurgence of readership via email newsletter over using RSS. While I still love me some RSS, I understand that many folks have moved on, often filling the void with their inboxes. After some high profile folks asked me for an email digest, because they were too busy to always remember to tune in, I had to make it happen.</p>

<p>Thanks for tuning in. If there is anything else you’d like to see in the weekly round up–let me know. I’m trying to keep basic for now, but I am thinking about adding some additional thoughts, or other aspects of my API industry research in there–like links to my guides, tools, and services. Right now, I’m giving a shout out to all my sponsors in the footer, but as the list grows it might be another place I entertain sponsorship of this crazy train I call API Evangelist. Thanks for all your support.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/12/i-finally-have-a-weekly-email-newsletter-roundup-of-api-evangelist-posts/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/10/i-can-keep-evangelizing-the-same-api-stories-for-the-next-decade/">I Can Keep Evangelizing The Same API Stories For The Next Decade In Government</a></h3>
        <span class="post-date">10 Nov 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/34_33_700_500_0_max_0_1_1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://www.fedscoop.com/events/redhatgov/2017/agenda/">I spoke on a panel at the Red Hat, Fed Scoop Government Symposium in Washington D.C. yesterday</a>. I had some great conversations with technology vendors, as well as government agencies about everything API. I enjoy being outside the Silicon Valley echo chamber when it comes to technology because I enjoy helping folks understand the basics of what is going on with the basics of APIs, over getting too excited over the latest wave of new technology, and a constant need to be moving forward before ever getting a handle on the problems on the table.</p>

<p>It can be hard to to repeat some of the same stories I’ve been telling for the last seven years while in these circles, but honestly the process helps me refine what I’m saying, and continue to actively think through the sustained relevancy of the stories I’ve been telling. After this round of discussions in D.C. I feel there are a some themes in my work, I can keep refining, and crafting stories for sharing in the government space.</p>

<ul>
  <li><strong>Open</strong> - I know its a tired term, but learning to be more open with other agencies, partners, and the public is an essential component of doing APIs in the federal government.</li>
  <li><strong>Documentation</strong> - Do not reinvent the wheel with documentation, and leverage OpenAPI to help you keep documentation usable, up to date, and valuable to developers using existing open source API documentation solution.</li>
  <li><strong>Support</strong> - Provide email, office hours, Twitter, ticketing, Github issues, and other common support building blocks for API consumers, making sure people know they can get help when they need.</li>
  <li><strong>Communication</strong> - Talk to your consumers. Have a blog, Twitter account, and other social channels for communicating internally, with partners, and publicly with API consumers.</li>
  <li><strong>Experiment</strong> - See your APIs as an R&amp;D extension of an agency, and allow for experimentation with APIs, as well as the consumption of the APIs. Think about sandboxes, data virtualization, and other ways of minimizing agency risk.</li>
  <li><strong>Education</strong> - Make sure you are reaching out, educating, and providing training for all API stakeholders, ensuring that everyone is up to speed, and making no assumptions about what people know, or do not know.</li>
</ul>

<p>None of this is technical. This is all basic API knowledge that any business or technical API stakeholder can take ownership of. These are all things that I see kill API efforts in the public, as well as private sector. These are all things that IT and developer folks scoff at and feel are unnecessary, and business users do not always see as an essential part of technical implementations. <a href="http://apievangelist.com/2017/07/27/state-of-apis-in-the-federal-government/">These are all deficient elements present across the 100 developer portals, and the 500 APIs I keep an eye on across the federal government</a>. They are common building blocks of API operations that I’ll keep beating a drum about on my blog, and in person at events I attend in D.C.</p>

<p>The API environment in D.C. would frustrate your average API developer, architect, and evangelist. I get frustrated at the speed of things, and having to say the same things over and over. However, I also understand the scope of what the federal government does, and the number of people we have to get up to speed on things. The number of APIs available is actively growing, but the consistency, usability, and effectiveness of APIs isn’t keeping pace. To keep things in balance we are going to need even more evangelism around operating government in an online environment, and how APIs can help provide access to data, content, and even algorithms across all branches of government in a safe and secure ways.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/10/i-can-keep-evangelizing-the-same-api-stories-for-the-next-decade/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/10/admitting-there-is-so-much-i-do-not-know-makes-me-better-at-apis/">Admitting There Is So Much I Do Not Understand Makes Be Better At APIs</a></h3>
        <span class="post-date">10 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/server-racks-clouds_copper_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>One of the reasons I’m so good at APIs is because I embrace how little I know. This rolling realization keeps my appetite wet when it comes to learning to things, and working hard to discover, and realize sensible API practices. I am comfortable with the fact that I do not know something. I enjoy coming up against things I do not understand, eager to learn more. However, I think there is one big difference in the way I approach technology from other developers, is that I’m not confident that I will ever be able to fully understand a particular domain, let alone think that technology, or specifically APIs are a solution to a specific set of problems within every domain.</p>

<p>Many developers are overly confident in what they know. They are also overly confident in their ability to learn new things. They are also overly confident that they can hammer out a technological solution that will solve all problems within a domain. I feel like many technologists aren’t in the game to learn, they are in the game to prove they have the chops to solve problems, and when they can’t they just walk away. When you approach APIs in this way you are leaving a lot of opportunity for learning and growth on the table. APIs shouldn’t be seen as simply a solution. APIs are just a tool (like the web) in a business toolbox, that should be applied when appropriate, and not applied when it doesn’t make sense.</p>

<p>Beyond developers, I feel like many business users are scared off by the uncertainty in the world of APIs. They don’t thrive in an environment where there are so many possibilities, configurations, and ways to do things right and wrong. APIs give you more control over your data, content, and algorithms, allowing you to provide access to them in many ways, and reach across many client channels like the web, mobile, and other device or network implementations. I feel like many business users want this amount of control, but aren’t willing to invest the time to be able to make decisions in this environment, and own the responsibility surrounding so much uncertainty. Unlike developers, they may have the domain expertise, but aren’t willing to experiment, play around with, and bang their head on the different ways APIs can help deliver solutions.</p>

<p>I feel like developers tend to suck at admitting they don’t understand things, and have to much belief in our technological toolbox when it comes to filling in the gaps. I feel like business users just aren’t confident enough with technology to thrive in environments where you perpetually do not know everything. I feel like my ability to admit I do not know, and maybe never fully understand a specific domain–coupled with my insatiable appetite for learning, puts me in a good position to be studying APIs. I can dive into new domains without thinking I’m going to save or disrupt the world. I can develop API prototypes, and help define API specifications, fully aware that I may not get it right on the first release. I know this is ok. I know that APIs are a journey, and not a destination. I know that APIs shouldn’t always be done. I know that sometimes they are a bad idea. I respect that there are domain experts in specific industries that I should be learning from. Admitting there is so much I do not know, helps me be better at what I do with APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/10/admitting-there-is-so-much-i-do-not-know-makes-me-better-at-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/10/are-people-ready-for-an-online-api-driven-world-that-is-progammable/">Are People Ready For An Online API-Driven World That Is Programmable?</a></h3>
        <span class="post-date">10 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/van-gogh-starry-night-container-bridge-2.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am struggling with helping some folks get beyond their API being just readable, and helping them understand the potential of having POST, PUT, and other writable aspects to their resources, making things much more programmable. My client has a firm grasp on the ability to GET data from their API and publish on websites. They also have the concept that they can GET other data from other 3rd party APIs, and display on their website alongside their data. Where they are struggling is that they can also add new data to their API, and update existing data they are making available via their API, and ultimately their website as well.</p>

<p>This hurdle isn’t limited to any single project I’m working on. I find a number of people who seem to have a decent grasp on APIs in general, struggling with or completely avoiding conversations around making the data writeable. They are able to make the transition from web to API when it comes to retrieving data, but making the same jump when it comes to adding and updating data is proving to be more difficult. I think there will always be a cognitive load with jumping from read to write, as you have to think more about security, data quality, and other common concerns. However, beyond that, I’m trying to explore what might be the challenges people are facing. Many of the folks I’m working with are a bit shaky on their grasp of APIs, and aren’t too confident in sharing what they don’t understand.</p>

<p>As I do, I’ll put it out to the universe and ask my audience what they’ve seen. On the surface, I’d say that adding or updating data into a database online is tougher to wrap your head around without some context, and some of the affordances we enjoy in the browser. Adding a Tweet through mobile application or website? No problem. POST a tweet through API, is a little tougher to envision. Updating a contact record in your administrative system? No problem. PUT via API to the /contact/ path doesn’t compute as quickly. API developers can quickly see these interfaces as programmable, but for the average business user, there is more dependence on the affordances provided in the browser, and us developers are taking the importance of these affordances for granted.</p>

<p>Beyond that, I’m guessing there is a perceived lack of control. Anyone can add or update? How do we address quality control? IDK, just spitball’n here. The majority of APIs I come across are GET only, so I have to believe there are issues around control, and beliefs around ownership. I can’t believe all of these API providers don’t grasp the technical aspects of writeable APIs. It is the same in the federal government. ALL the web APIs are read only. We’ve started to see this ice break a little in recent years, but federal agencies are rightfully wary of the responsibility of letting the public write data to their databases. I almost feel like the Facebook and Twitters of the world are almost too open to allowing folks to write, without being thoughtful around privacy, security, and data ownership or stewardship–just to get their hands on the data.</p>

<p>I wondering if POST, PUT, and DELETE should be 101 concepts that I’m teaching to folks. Should I be starting out with just the safer, more straightforward reading of data, content, and algorithmic resources. Then down the road introduce the ability to add, update, and delete information. I know the concept of DELETE freaks people out pretty quickly. IDK. I’m just doing my job, and questioning things at all levels, and wondering if people are even ready for a programmable online world. I think SaaS has delivered a programmable world with a wealth of affordances that help onboard people with all of this, and us API people are failing at translating the significance of an API-driven world that is programmable to our business users.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/10/are-people-ready-for-an-online-api-driven-world-that-is-progammable/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/10/you-can-lead-a-horse-to-water-but-you-cannot-make-them-drink-the-api-edition/">You Can Lead A Horse To Water But You Cannot Make Them Drink--The API Edition</a></h3>
        <span class="post-date">10 Nov 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/bluelake/clean_view/file-00_00_38_67.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I have seven years of API research available at apievangelist.com. I regularly publish short form, and long form versions of this information on my blogs on a weekly basis. I publish prototypes, demo websites and portals, and develop API training curriculum for use across a wide variety of industries. I regularly take versions of my API research, and rework, rebrand, and dial in to speak to a specific company, organizations, institution, agency, or industry. In many cases I make this information freely available, helping make sure it is available to those who need it. Despite all this work, many folks who are already doing APIs refuse to read, listen, and learn from what is already going on in the API space, and doomed to repeat the mistakes many of us have already made and learned from in our API journeys.</p>

<p>Many folks don’t really understand my motivations and think I have some sort of agenda to sell them something, disrupt their current reality, or other uninformed perspective. Ultimately, not trusting what I’m putting out there. I guess viewing that the water is poised in some way. Others don’t feel they need it, either because they feel like they have all the answers, or the problems haven’t become a reality in their worlds yet, so my solutions seem irrelevant. I find it tough to argue with someone about preventative care when it comes to their API operations, when they spend their days triaging bugs, problems, and legacy technology challenges. They are fire fighters, water isn’t for drinking!</p>

<p>A long standing example of this can be found in the hypermedia realm. No matter how much some very smart people, with a wealth of experience deploying and managing APIs warn about challenges with maintaining API SDKs and clients, some folks will never see it as a problem until they actually face it themselves. I can showcase endless numbers of healthy practices extract from companies like AWS, Twitter, and Twilio for people to learn from, but many folks will never see their relevance until they directly experience the problem. Most people have trouble looking forward, as well as stepping outside their own API operations and looking at them side by side with other leading API pioneers. They are different. Special. Often times, people never even engage in these thought exercises at all. There just isn’t the room in their operations for thinking proactively.</p>

<p>I regularly get frustrated when my clients, or people I’m asked to speak with about healthy API practices actively ignore, criticize, and dismiss what I’m sharing. I shouldn’t. I can’t force people to trust me. I can’t force people to drink from the water I’m providing. All I can do is plant the seed in their minds that there is water over here when you get thirsty. It seems to be a chronic condition across many industries, that folks only drink when they are super thirsty, after they get dehydrated, rather than proactively drinking water regularly throughout the day. In the end, my energy is better spend doing what I do best–researching, then openly sharing what I’m learning across the API space from other people doing APIs well. My job will often involves leading a horse to water, but does not ever involve forcing anyone to drink. I considered creating a blog called API Water Boarding, but it just didn’t seem like a good idea. ;-)</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/10/you-can-lead-a-horse-to-water-but-you-cannot-make-them-drink-the-api-edition/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/09/the-impact-of-api-management-on-api-security/">The Impact Of API Management On API Security</a></h3>
        <span class="post-date">09 Nov 2017</span>
        <p><a href="http://apis.how/security/"><img src="https://s3.amazonaws.com/kinlane-productions/guides/security/api-security-guide-api-management.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p><em>This is a story from my latest <a href="http://security.apievangelist.com/#Guide">API Evangelist API security industry guide</a>. My partner <a href="http://apis.how/elasticbeam">ElasticBeam</a> has underwritten my API security research, allowing me to publish a formal PDF of my guide, providing business and technical users with a walk-through of the moving parts, tools, and companies doing interesting things with API security. When I publish each guide, I publish each story here on the blog, helping build awareness around my research–this is a short one on API management.</em></p>

<p>API management has done an amazing job in helping companies, organizations, institutions, and government agencies make their digital resources more available on-line in a secure way. Allowing API providers to require developers to sign up, obtain keys, and tokens which need to accompany all API calls. This, along with encryption by default has gone a long way towards making data, content, and algorithms more accessible, while also being secure. However, many API providers have stopped here, and think their resources are secure, when in reality there is so much more work to be done.</p>

<p>Requiring all developers obtain keys to access resources, and encryption data in transit is an important part of API security, but it is just one tool in the API security toolbox. Out of API management you also receive an enhanced set of logging, analysis, and reporting tools for how developers are putting API resources to work. When done well, this pushes the API security conversation forward, allowing API providers to balance access with security, and be proactive when it comes to limiting access, or even shutting off access when their is abuse. The problem is not all API providers are investing here, let alone going beyond what API management providers offer.</p>

<p>The awareness brought to the table my API management is valuable, but there are so many aspects of API operations at the web server, DNS, and other levels that are often left out of the API management conversation. I’ll be pushing API providers to look beyond just the API management layer, and expanding API security awareness to every other stop along the API life beyond just management.</p>

<p><em>You can <a href="http://security.apievangelist.com/#Guide">download or purchase my API Evangelist API security industry guide over at my API security research</a>, and if you want to point out any corrections, and share your thoughts on what is missing, feel free to submit a Github issue on the research project’s Github repository. I appreciate your support of my work, and depend on folks like you, and <a href="http://apis.how/elasticbeam">ElasticBeam</a> to make this all work.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/09/the-impact-of-api-management-on-api-security/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/09/apis-that-use-apis-to-manage-my-apis/">Using APIs To Manage My APIs</a></h3>
        <span class="post-date">09 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/cargo-ship-on-sea_blue_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m going further down the AWS rabbit hole lately with my APIs. Historically my APIs ran on an AWS EC2 instance with leveraged Linux for the OS, Apache for the web server, and Slim for the RESTful framework of my APIs–all with an RDS MySQL backend. I’ve now evolved the EC2 instance to be spread across numerous AWS Lambda scripts, tied together into various stacks of APIs using AWS API Gateway. At first, I was hesitant to go further down the AWS rabbit hole, but <a href="http://apievangelist.com/2017/11/06/api-security-beginning-to-outweigh-my-vendor-lock-in-concerns/">the security benefits of AWS-driven solutions</a>, as well as the API-driven aspects of operating my APIs, is slowly shifting my view of how I need to be managing my APIs.</p>

<p>AWS RDS, Lambda, and API Gateway all have APIs. I’ve been spending the week developing Lambda scripts that help me manage my APIs, using the AWS APIs behind these three services, leveraging them to setup, configure, deploy, manage, and test my APIs. I enjoy how APis push me to think about my digital resources, and when my digital resources are APIs, the benefts begin to feel like API inception. I’m increasingly having APIs that do one thing and do it well, when it comes to API operations, allowing me to distill down the building blocks of my API operations, into a very workable world of API functionality.</p>

<p>I am now deploying, backing up, migrating, and working with the database behind my APIs using APIs. I primarily use AWS RDS MySQL instances behind my APIs, but when I’m using AWS DynamoDB, I leverage AWS APIs even more, as DynamoDB lets you do adds, updates, queries, and deletes using the API, elevating beyond SQL to manage the contents of each data store. Whether it is RDS or DynamoDB, I’m using APIs manage the operational side of each database behind the API, and I’m beginning to explore how to make my database more ephemeral, and scalable using AWS APIs, giving me more control over my database budgets.</p>

<p>Moving up the API stack, I’m deploying, optimizing, testing, and working with all my AWS Lambda scripts using the API. I’m working to organize, index, and manage all my Lambda scripts using AWS S3, bringing some order to the serverless madness I’m unleashing. This approach to using APIs to manage the code layer for my API operations is entirely new. I’ve never had API access to the code I deployed in the Slim framework, on my Linux servers. While there is significant overhead in architecting and setting up this new approach, the benefits down the road for deploying, updating, and managing the code that drives my APIs will be significant.</p>

<p>Lastly, I’m crafting a whole suite of API Gateway scripts, allowing me to setup plans, issue and revoke keys, quantify API consumption, and a whole host of other API gateway driven functionality across all my APIs. I’m even to the point where I’m launching new APIs in the gateway using OpenAPI definitions, and now I am figuring out how I can wire each API resource and method to their appropriate Lambda function behind. I’m still figuring this part out. This is where I find myself in my API driven journey to automate my API lifecycle using APIs. I’ve long had APIs for my API management, but only recently was I able to deploy new APIs that help me manage my APIs, beyond the base stack given to me by my API management provider.</p>

<p>Using APIs to manage APIs quickly becomes API inception in my brain. I quickly find myself looping infinitely through what is possible, but discovering new ways to orchestrate my world, and save myself work. I thoroughly enjoy the concept of using my cloud provider APIs to manage my APIs, but I’m also finding it rewarding to design, deploy, and operate new APIs that contribute to the management of my APis. This is where I think the whole API game is going to shift. When I’m not just deploying all my data, content, and algorithms using APIs, but also using APIs to manage these APIs, a new found freedom emerges to program the digital world around me.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/09/apis-that-use-apis-to-manage-my-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/09/learning-to-play-nicely-with-others-using-apis/">Learning To Play Nicely With Others Using APIs</a></h3>
        <span class="post-date">09 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/dark-clouds-la-buildings.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>This is a topic I talk about often, write about rarely, but experience on a regular basis doing APIs. It has to do with encounters I have with people in companies who do not know how to share and play nicely with other companies and people, and want to do APIs. For a variety of reasons these folks approach me to learn more about APIs, but are completely unaware of what it takes, and how it involves working with external actors. Not all of these APIs are public, but many of them involve engaging with individuals outside the corporate firewall, possess a heavy focus on the technical, and business of doing APIs, but rarely ever consider the human and more political aspects of engagements with APIs.</p>

<p>I find that people tend to have wildly unrealistic expectations of technology, and believe that APIs will magically connect them to some unlimited pool of developers, or seamlessly connect disparate organizations across vast distances. People come to me hoping I will be able to explain it all to them in a single conversation, or provide in a short white paper, which is a state of being that also makes individual very susceptible to vendor promises. People want to believe that APIs will fix the problems they have introduced into their operations via technology, and effortlessly connect them with outside revenue streams and partnerships, unencumbered by the same human challenges they face within their firewalls.</p>

<p>Shortly after getting to know people it often becomes apparent that they possess a lot of baggage, legacy processes and beliefs, that they are often unaware of. Or, I guess more appropriately they are unaware that these legacy beliefs are not normal, and are something everybody faces. Where they are usually pretty uniquely dysfunctional to a specific organization. People usually have hints that these problems aren’t normal, but just aren’t equipped to operate outside their regular sphere of influence, and within days or weeks of engagement, their baggage becomes more visible. They start expecting you to jump through the same hoops they do. Respond to constraints that exist in their world. See data, content, and algorithms through the same lens as their organization. At this point folks are usually unaware they’ve opened the API door on their world, and the light is shining in on their usually insulated environment–revealing quite a mess.</p>

<p>This is why you’ll hear me talk of “dating” folks when it comes to API engagements. Until you start opening, and leaving the door open on an organization for multiple days at a time, you really can’t be sure what is happening behind those closed doors. I am also not always confident that someone can keep internal dysfunction from finding its way out. I don’t expect that all organization be dysfunction free, I’m just looking for folks who are aware of their dysfunction, and are looking to actively control, rate limit, and minimize the external impacts. As well as being willing to work through some of this dysfunction, as they engage with external actors. I have to also acknowledge that this game is a two way street, and I also encounter API consumers who are unable to leave their baggage at home, and bring their seemingly natural internal dysfunction to the table when looking to put external resources to work across their operations. In general, folks aren’t always equipped to play nicely with others by default, it takes some learning, and evolving, but this is why we do APIs. Right?</p>

<p>This is why I suggest people become API consumers, before they become API providers, so that they can truly understand what it is like to be on both sides of the equation. API providers who haven’t experienced what its like to be on the receiving end of an external API, rarely make for empathetic API providers. Our companies, organizations, institutions, and government agencies often insulate us from the outside world, which works in many positive ways, but as you are aware, it also works in some negative, and very damaging political ways. The trick with doing APIs, is to slowly change this behavior, and being able to let in outside ideas, and resources, without too much disruption, internally or externally. Then over time, learn to play nicely with others, and build relationships with other groups, and individuals who might benefit our organizations, and we might also help bring value to their way of doing things as well.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/09/learning-to-play-nicely-with-others-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/09/the-open-web-application-security-project-owasp-and-api-security/">The Open Web Application Security Project (OWASP) And API Security</a></h3>
        <span class="post-date">09 Nov 2017</span>
        <p><a href="http://apis.how/security/"><img src="https://s3.amazonaws.com/kinlane-productions/guides/security/open-web-application-security-project-owasp.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p><em>This is a story from my latest <a href="http://security.apievangelist.com/#Guide">API Evangelist API security industry guide</a>. My partner <a href="http://apis.how/elasticbeam">ElasticBeam</a> has underwritten my API security research, allowing me to publish a formal PDF of my guide, providing business and technical users with a walk-through of the moving parts, tools, and companies doing interesting things with API security. When I publish each guide, I publish each story here on the blog, helping build awareness around my research–this is a short one on OWASP.</em></p>

<p>The Open Web Application Security Project (OWASP) is a 501(c)(3) worldwide not-for-profit charitable organization focused on improving the security of software, with a mission to make software security visible, so that individuals and organizations are able to make informed decisions. OWASP is looking to provide impartial, practical information about application security (AppSec) to individuals, corporations, universities, government agencies and other organizations worldwide. Operating as a community of like-minded professionals, OWASP issues software tools and knowledge-based documentation on application security.</p>

<p>As the web API space has expanded OWASP has expanded its focus to include the most common threats to APIs. OWASP has acknowledged the overlap between web applications, and web APIs, and quickly becoming a valuable source for API specific security knowledge, expanding beyond its web application roots. Providing one of the best resources to find security related information, and tooling you can apply throughout your API operations.</p>

<p>OWASP doesn’t endorse commercial services, and is a member driven organization, so you will find all the information they provide to be vendor neutral, and focused on the task at hand. You will find me regularly anchoring my API security work in what the OWASP community is doing, as security should always be a team effort. API security isn’t my primary focus as API Evangelist, but helping guide you to where you can find the latest information is what this guide is about.</p>

<p>OWASP is your source for unbiased API security information!</p>

<p><em>You can <a href="http://security.apievangelist.com/#Guide">download or purchase my API Evangelist API security industry guide over at my API security research</a>, and if you want to point out any corrections, and share your thoughts on what is missing, feel free to submit a Github issue on the research project’s Github repository. I appreciate your support of my work, and depend on folks like you, and <a href="http://apis.how/elasticbeam">ElasticBeam</a> to make this all work.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/09/the-open-web-application-security-project-owasp-and-api-security/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/08/apis-and-other-ways-to-service-up-machine-learning-models/">APIs And Other Ways Of Serving Up Machine Learning Models</a></h3>
        <span class="post-date">08 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algorotoscope/stories/crypto-machine-bletchley_copper_circuit.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>As with most areas of the tech sector, behind the hype there are real world things going on, and machine learning is one area I’ve been studying, learning, and playing withd what is actually possible when it comes to APIs. I’ve been studying the approach across each of the major cloud platforms, including AWS, Azure, and Google t push forward my understanding of the ML landscape. Recently the Google Tensorflow team released an interesting overview of how they are serving up Tensorflow models, making machine learning accessible across a wide variety of use cases. Not all of these are API specific, but I do think they are should be considered equally as part of the wider machine learning (ML) application programming interface (API) delivery approach.</p>

<p>Over the past year and half, with the help of our users and partners inside and outside of Google, TensorFlow Serving has advanced performance, best practices, and standards for ML delivery:</p>

<ul>
  <li><strong>Out-of-the-box Optimized Serving and Customizability</strong> - We now offer a pre-built canonical serving binary, optimized for modern CPUs with AVX, so developers don’t need to assemble their own binary from our libraries unless they have exotic needs. At the same time, we added a registry-based framework, allowing our libraries to be used for custom (or even non-TensorFlow) serving scenarios.</li>
  <li><strong>Multi-model Serving</strong> - Going from one model to multiple concurrently-served models presents several performance obstacles. We serve multiple models smoothly by (1) loading in isolated thread pools to avoid incurring latency spikes on other models taking traffic; (2) accelerating initial loading of all models in parallel upon server start-up; (3) multi-model batch interleaving to multiplex hardware accelerators (GPUs/TPUs).</li>
  <li><strong>Standardized Model Format</strong> - We added SavedModel to TensorFlow 1.0, giving the community a single standard model format that works across training and serving.</li>
  <li><strong>Easy-to-use Inference APIs</strong> - We released easy-to-use APIs for common inference tasks (classification, regression) that we know work for a wide swathe of our applications. To support more advanced use-cases we support a lower-level tensor-based API (predict) and a new multi-inference API that enables multi-task modeling.</li>
</ul>

<p>I like that easy-to-use APIs are on the list. I predict that ease of use, and a sensible business model will provide to be just as important as the ML model itself. Beyond these core approaches, the Tensorflow team is also exploring two experimental areas of ML delivery:</p>

<ul>
  <li><strong>Granular Batching</strong> - A key technique we employ to achieve high throughput on specialized hardware (GPUs and TPUs) is “batching”: processing multiple examples jointly for efficiency. We are developing technology and best practices to improve batching to: (a) enable batching to target just the GPU/TPU portion of the computation, for maximum efficiency; (b) enable batching within recursive neural networks, used to process sequence data e.g. text and event sequences. We are experimenting with batching arbitrary sub-graphs using the Batch/Unbatch op pair.</li>
  <li><strong>Distributed Model Serving</strong> - We are looking at model sharding techniques as a means of handling models that are too large to fit on one server node or sharing sub-models in a memory-efficient way. We recently launched a 1TB+ model in production with good results, and hope to open-source this capability soon.</li>
</ul>

<p>I’m working to develop my own ML models, and actively using a variety of them available over at <a href="https://algorithmia.com/">Algorithmia</a>. I’m pushing forward a project to help me quantify and categorize ML solutions that I come across, and when they are delivered using an API, I am generating an OpenAPI, and applying a set of common tags to help me quantify, categorize, and index all the ML APIs. While I am focused on the technology, business, and politics of ML APIs, I’m interested in understanding how ML models are being delivered, so that I can help keep pace with exactly how APIs can augment these evolving practices, making ML more accessible, as well as observable.</p>

<p>We are in the middle of the algorithmic evolution of the API space, where we move beyond just data and content APIs, and where ML, AI, and other algorithmic voodoo is becoming more mainstream. I feel pretty strongly that APIs are an import piece of this puzzle, helping us quantify and understand what ML algorithms do, or don’t do. No matter how the delivery model for ML evolves, batches, distributes, or any other mutation, there should be an API layer for accessing, auditing, and shining a light on what is going on. This is why I regularly keep an eye on what teams like Tensorflow are up to, so that I can speak intelligently to what is happening on this evolving ML front.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/08/apis-and-other-ways-to-service-up-machine-learning-models/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/08/the-api-evangelist-api-security-industry-guide/">The API Evangelist API Security Industry Guide</a></h3>
        <span class="post-date">08 Nov 2017</span>
        <p><em>This edition of my API security industry guide has been underwritten by <a href="http://apis.how/elasticbeam">ElasticBeam</a>, who provides next generation API security, leveraging machine learning, and behavorial analysis that works with the existing web and API management solutions you already have in place across your API operations.</em></p>

<p><a href="https://gum.co/UOBZd"><img src="https://s3.amazonaws.com/kinlane-productions/guides/security/api-evangelist-api-security-industry-guide-screenshot-2017-11-08.png" align="right" width="40%" style="padding: 15px;" /></a></p>

<p>I have been working on this resulting guide from <a href="http://security.apievangelist.com/">my API security research</a> for over a year now. Thanks to <a href="http://apis.how/elasticbeam">ElasticBeam</a> I’ve finally gotten it out the door. As with all my industry guides, it is a work in progress, and something that will never be finished. I’ll keep taking what I’ve learned, and publishing in as a PDF every couple months, and receive the edits, and feedback from my readers and the wider community, then publish again. I’m feeling like I’m finally finding my groove again with these guides, and there is no better time to be back on game, especially when it comes to API security.</p>

<p>Security is the number one concern of companies, organizations, institutions, and government agencies considering investing more resources into their API infrastructure, as well as companies who are ramping up their existing efforts. At the same time it is also the most deficient area when it comes to investment in API infrastructure by existing API providers. Many groups are rushing along their API journey, and deploying web, mobile, device, and other applications, but rarely stopping to properly secure things with each step along the way.</p>

<p>In 2016 I began investing more into the topic of API security. I have been ramping up my research into how APIs were being secured, and how they weren’t being secured. I’ve been tracking on breaches, vulnerabilities, as well as the companies who are offering products and services that help API providers secure their APIs, as well as some of the open source tooling that is available. As I do with my approach to researching everything APIs, along the way I’m keeping notes on the common building blocks, and other patterns that are contributing to the wider API conversation–in this case it is all about securing our APIs.</p>

<p>From my research into where things are at with API security in 2016, it is clear that one of the reasons things were deficient in the area of API security was that API management had sucked much of the oxygen out of the conversation. Numerous API providers I talked with about API security thought it was all about making sure APIs were keyed up, applying OAuth, using encryption, and rate limiting their APIs. With that, API security was taken care of. Very few were actively scanning, testing, and looking through web server, DNS and other logs for signs of security threats and incidents. While a major contributing factors to API security deficiency is that API providers are short on resources, which means API security is often under-invested in by a company, but beyond this, I think API management has been the biggest reason API security still lags behind in 2017.</p>

<p>Another thing I have noticed in my research, is that many of the APIs being operated were in service of mobile applications, and many API providers were investing in mobile application security, and considered their APIs behind secure as a result. APIs in service of mobile applications were living in the shadows, despite being easily reverse engineered using off the shelf proxy tools. This perception of what API security is has added on a whole other dimension to why investment in this area is so far behind. Many companies feel like they don’t have public APIs when they are developing them in the service of mobile phones, despite using HTTP as the transport, and leverage public DNS.</p>

<p>Even with all of the deficiencies in API security, I’m beginning to see forward motion in the API conversation in 2017. I’m seeing new service providers emerge to help secure web APIs, addressing the specific threats API providers face. I’m seeing machine learning, behavioral analysis, and other new approaches to analyzing log files, and studying the surface area of our API infrastructure. There is still much work ahead, but there are signs of renewed conversations on the subject. The biggest challenge is going to be helping companies, institutions, organizations, and agencies understand that they should be investing significantly more resources into API security. That is the objective of this API security industry guide, to provide a simple walk through of the space, and introduce the services, tools, and common building blocks that are in use by successful providers.</p>

<p><em>If you find any mistakes in the guide, or would like to make suggestions, <a href="https://github.com/api-evangelist/security/issues">feel free to head over to the Github repository for my API security research and submit an issue</a>. I depend on my audience ot help me refine, polish, and keep these guides updated. Hopefully this is just the first of many updates to the API security industry guide, allowing me to keep in sync with the ever-changing security landscape. Thank you for your support!</em></p>

<ul class="actions" style="text-align: center;">
	<li><a href="http://apis.how/security" class="button big" target="_blank">Download Guide</a></li>
	<li><a href="https://gum.co/UOBZd" class="button big" target="_blank">Purchase Guide</a></li>
</ul>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/08/the-api-evangelist-api-security-industry-guide/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/08/i-appreciate-this-api-walk-through-from-fannie-mae-but-give-me-the-api/">I Appreciate This API Walk Through From Fannie Mae But Just Give Me The API!</a></h3>
        <span class="post-date">08 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/fannie-mae/fannie-mae-d-messages-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I came across the new Desktop Underwriter (DU) API from Fannie Mae which provides lenders a comprehensive credit risk assessment data that determines whether a loan meets Fannie Mae’s eligibility requirements. <a href="http://www.buildingoncertainty.com">They have a slick new website for the project</a>, with the tag line “building on certainty”, and <a href="http://fanniemae2.articulate-online.com/p/5070324196/story_html5.html">a smooth HTML story to walk you through what the new DU API</a> can do. While the API seems very exciting, and valuable, the whole production is missing one thing–the API!</p>

<p>I am sure you have to be a partner to get access to the API, but you can tell the whole things is being led by people who have never actually used an API. Otherwise you would give us an API to actually use, and allow us to kick the tires. A hallmark of modern APIs is that you get to play with it. Marketing materials, and a sharp single page application website isn’t enough. We need the documentation, and be able to actually see what the request and response structure is, so that we can better understand the value being generated, and how we will be integrating with it. Without this, there isn’t any value. Of course, you don’t have to make the real API 100% public, you can always create API access tiers, and even deploy a sandboxed or virtualized version of the API and data for new users, protecting your valuable resources–just do not hide the API away from us, and make us consumers beg for access.</p>

<p>When you hide your APIs, you leave first impressions like you did with me. Wouldn’t it be better if my first impression was all about writing a story on how cool your API was, and how all my readers should be using it? Instead, I’m using you as a case of how to not do APIs. There is no reason the Fannie Mae Desktop Underwriter (DU) API can’t be publicly available, allowing us analysts and developers to read the documentation, and kick the tires. Another thing I want to push on back on, is the use of acronyms. I had to Google what DU meant. I know this is ironic, because of API and all, but I work overtime to spell out application programming interface (API) for my (new) users on a regular basis. Please don’t assume all your API consumers will immediately know what DU is. Please help unpack your acronyms, using a glossary, or expanding them inline as you explain what you do. It will make on-boarding much easier for the edges of your target audience.</p>

<p>Anyways, I look forward to some day testing driving the API (someday). I looked for a good five minutes for how to onboard with the API, and eventually gave up. Maybe someone involved in the project will read this, and email me with more information about how I could test drive, so that I can do a proper write up on what Fannie Mae is up to with aPIs. I’m happy to see such large entities working to make their valuable resources available via APIs, but I’m feeling like you might not have done all your homework on what API is, but luckily I’m here as the API Evangelist to help you understand the essence of why APIs work, beyond just the technology and acronym. I look forward to helping you along in your API journey Fannie Mae.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/08/i-appreciate-this-api-walk-through-from-fannie-mae-but-give-me-the-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/08/additional-call-pricing-info-are-the-press-relief-valves-for-api-plans/">Additional Call Pricing Info Are The Pressure Relief Valves For API Plans</a></h3>
        <span class="post-date">08 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bitscoop/bitscoop-pricing-plans-additional-calls.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve complained about unfair API pricing tiers several times over the last couple years, even declaring API access tiers irrelevant in a mult-API consumer world. Every time I write about this subject I get friends who push back on me that this is a requirement for them to generate revenue as a struggling startup. With no acknowledgement that their API consumers might also be struggling startups trying to scale consumption within these plans, only to reach a rung in the ladder they might not be able to actually reach. My goal in this storytelling isn’t to condemn API providers, but make them aware of what things look like from the other side, and that their argument essentially pulls up the ladder after they’ve gotten theirs–leaving the rest of us at the bottom.</p>

<p>My complaint isn’t with startups crafting pricing tiers, and trying to make their revenue projects more predictable. My complaint is when the plans are priced too far apart and I can’t afford to move from one plan to the next. More importantly, my complaint is when the tier I can’t moved from is rate limited with a cap on usage, and I can’t burst beyond my plans limits without scaling to the next access tier which I cannot afford to reach. I understand putting hard caps on public or free tier plans, but when you are squarely in a paid access tier, you shouldn’t be shut down when you reach the ceiling. Sure, I might pay a premium for each additional call, but I shouldn’t be shut down and forced to move to the next higher access tier–which might be out of my monthly price range. I just can’t go from $49.95 to $499.95 in monthly payments as a small business, sorry.</p>

<p>The key element that needs to be present for me, even in situations where I cannot afford to jump to the next tier, is the ability to go beyond my plans usage, with clear pricing regarding what I will pay. I may not be able to jump from $49.95 to $499.95 as monthly payments, but I might be able to burst, and absorb the costs as I need. If my plan is capped, and I cannot burst, and know what I will be charged for my usage (even if at a premium), it is a deal breaker for me. While I would prefer API providers do away with access tiers, and go with straight pay for what you use model (like Amazon), I accept the reality that API access plans help startups better predict and communicate their revenue. As long as they have this relief valve for each plan, allowing me to stay in my plan, but consume resources beyond what my tier allows.</p>

<p>Having relief valves for plans won’t hurt your revenue forecasting. I would argue it will actually help your revenue (not forecasting) with bursts in revenue that you wouldn’t see with just a capped plan approach. If you have trouble seeing API access in this way, I would argue that you are primarily an API provider, and building business exclusively focused on an exit. If you can empathize, I would say you are focused on delivering a set of services that people need, and you are probably an active consumer of other services, broadening your empathy spectrum beyond just a startup exit objectives. I honestly don’t want to mess with startups ability to generate revenue with this storytelling, I’m just trying to make the case for us startups on the API consumption side of the coin. Ok, I lied, I kind of do want to mess up the revenue strategy for startups who are exclusively focused on an exit, because when there isn’t a relief valve, you won’t find me signing up for one of your predictable plans in the first place.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/08/additional-call-pricing-info-are-the-press-relief-valves-for-api-plans/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/07/when-we-are-told-that-api-security-investments-will-affect-profitability/">When We Are Told That API Security Investments Will Affect Profitability</a></h3>
        <span class="post-date">07 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/castle_walls_cannon_satan_red.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was listening to Mark Zuckerberg talk about how security investments will affect the platforms profitability on the Facebook earnings call this last week. This line of thinking sounds pretty consistent with what I’m hearing from other folks when it comes to why they haven’t been investing more into their API security. My challenge for this line of thought is about shutting down proactive security investments, and does not speak of responsive security investments–meaning after you’ve had a breach, or when there is other security investment. From a leadership perspective this view of security just doesn’t do it for me, and I’d push back, and require it consider what profitability will look like if we do not invest properly in security.</p>

<p>Viewing security in this way is common. It is also a short-sighted view of security, in the name of profits today, over health of a platform down the road. It demonstrates that leadership is more focused on profits, than whatever the platform focus actually doing. I would add that I think this line of thinking reflects a perspective of leadership that is out of sync with the technical details of operating a platform, and the current threat landscape. I get that a company has to be profitable, and that it is the job of the CEO is to represent the investors, but after Equifax, and the many other breaches, as well as what I’m seeing on the ground at companies I’m talking to, it is pretty clear that things are out of whack when it comes to overall security investment.</p>

<p>I work with a lot of folks who want to invest in API security more, but they just don’t have the resources. I’ve been in leadership roles where I’ve had my hands tied when it came to decisions around infrastructure to deliver on PCI, and other compliance, as well as being able to hire security focused talent. This type of thought regarding security practices tends to make investors and other leadership happy, but is corrosive to the actual health of operations. This stuff shouldn’t be about profits or security, it should be about doing what is needed for security, then making assessments regarding how that impacts the bottom line. Security shouldn’t be polarized like this, and it should reflect proactive, as well as responsive costs, as well as practices.</p>

<p>This isn’t a technology of API security story, this is a politics of API security story. This type of response and tone from leadership is something that the majority of my readers will experience when trying to grow their API security efforts. Investment in API security will continue to be a challenge for most companies, organizations, institutions, and government agencies in coming years. As I do with other stops along the API life cycle, I’m going to spend more time developing stories to push back on leadership telling stories about investments in security. My goal is to have a toolbox of examples to help educate the people making security investment decisions that investment in API security now, will pay off later, and cost a lot less than investment in API security after the fact.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/07/when-we-are-told-that-api-security-investments-will-affect-profitability/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/07/hiding-apis-in-plain-sight/">Hiding APIs In Plain Sight</a></h3>
        <span class="post-date">07 Nov 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/methuselah/clean_view/file-00_00_20_99.jpg" align="right" width="45%" style="padding: 15px" /></p>
<p>I’m always surprised by how secretive folks are. I know that it is hard for many folks to be as transparent as I am with my work, but if you are doing public APIs, I have a basic level of expectation that you are going to be willing to talk and share stories publicly. I regularly have conversations with enterprise folks who are unwilling to talk about what they are doing on the record, or allow me to share stories about their PUBLIC API EFFORTS!!! I get the super secret internal stuff. I’ve worked for the government. I don’t have a problem keeping things private when they should be, but the secretive nature of companies around public API efforts continues to keep me shaking my head.</p>

<p>People within enterprise groups almost seem paranoid when it comes to people keeping an eye on what they are up to. I don’t doubt their competitors keep an eye on what they are doing, but thinking that people are watching every move, everything that is published, and will be able to understand what is going on, and be able to connect the dots is borderline schizophrenic. I publish almost everything I do public by default on Github repositories, and my readers, clients, and other folks still have trouble finding what I am doing. You can Google API Evangelist + any API topic and find what I’m working on each day, or you can use the Github search to look across my repositories, and I still have an inbox and social messaging full of requests for information.</p>

<p>My public by default stance has done amazing things for my search engine and social media presence. I don’t even have to optimize things, and I come up for almost every search. I get regular waves of connections from folks on topics ranging from student art to the government, because of my work. The schema, API definitions, documentation, tests, and stories for any API I am working on is public from day one of the life cycle. Until I bring it together into a single landing page, or public URL, the chances someone will stumble across it, and leverage my work before I’m ready for it to be public is next to zero. The upside is that when I’m ready to go public with a project, by the time I hit publish, and make available via the primary channels for my network, things are well indexed, and easily found via the usual online channels.</p>

<p>I feel like much of the competitive edge that enterprise groups enjoy is simply because they are secretive. There really isn’t a thing there. No secret sauce. Just secret. I find that secret tends to hide nothing, or at least is hiding incompetency, or shady behavior. I was talking with a big data group the other day that was looking for a contractor that was skilled in their specific approach to doing APIs. I asked for a link to their public APIs, so I could assess this specific approach, and they declined, stating that things private. Ok, so you want me to help find someone who knows about your API data thing, is well versed in your API data thing, but I can’t find this API data thing publicly, and neither can anyone else? I’m sorry, this just doesn’t make sense. How can your API data thing ever really be a thing, if nobody knows about it? It just all seems silly to me.</p>

<p>Your API, it’s documentation, and other resources can be public, without the access to your API being public. Even if someone can mimic your interface, they still don’t have all your data, content, and algorithmic solutions. You can vet every consumer of your API, and monitor what they are doing, this is API management 101. You can still protect your valuable digital assets while making them available for discovery, and consideration by potential consumers. You can make controlled sandbox environments available for talent acquisition, building of prototypes, crafting visualizations, doing analysis, and other ways businesses benefit from doing APIs. My advice to companies, institutions, organizations, and government agencies looking to be successful with APIs is stop being so secretive, and start hiding everything you are doing with your public APIs out in plain sight.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/07/hiding-apis-in-plain-sight/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/07/postman-as-a-live-coding-environment-in-presentations-at-apistrat/">Postman As A Live Coding Environment In Presentations At APIStrat</a></h3>
        <span class="post-date">07 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/postman/postman-working-screenshot.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>We just wrapped up the 8th edition of APIStrat in Portland, Oregon this last week. I’ll be working through my notes, and memory of the event in future posts, but thing that stood out for me was the presence of Postman at the event. No, I’m not talking about their booth, and army of evangelists and company reps on site–although this was the first time I’ve seen them out in such force. I’m talking about the usage of the API development environment by presenters, as a live coding environment in their talks, replacing the command line and browser for how you demonstrate the magic of APIs to your audience.</p>

<p>On the first day of the conference I attended two separate workshops where Postman was the anchor for the talk. As they worked their way through their slides they kept switching back to the Postman application to show some sort of real results, from an actual, or mocked API. It is the new live coding environment for API evangelist, architects, designers, developers, and security folks. It is the quickest way to go from API concept, to demonstrating API solutions in any presentation. What I also really like is that it transcends any single programming language. In the past, I’ve always hated when someone would bust out some .NET code to show an API call, or something very language or platform specific. Postman reflects a more API way of doing things, that is elevated above the dogma of any single programming language community.</p>

<p>I am beginning to use Postman and Restlet client in my API training and curriculum more. Directing my users to actually try something out in the API client before moving on to the next step. It is kind of becoming the new interactive API documentation, but something that is linkable from any story, training materials, or incorporated directly into a live talk. As an evangelist it is yet another reason to maintain OpenAPI definitions and Postman Collections of all your most common API use cases. So that you can find, and include all your relevant API calls directly into your storytelling. I’m going to start exploring the viability of doing this with non-developers, and folks who aren’t familiar with Postman yet. I have a feeling that within the echo chamber there is a sort of familiarity taking hold when it comes to Postman, but I want to make sure the environment isn’t a shock for newcomers, and is something that can help users go from zero to API response in 60 seconds or less.</p>

<p>It is interesting to watch the API space evolve, and what tools become common place like Postman has become. For a while the Apigee API Explorer was common place, but has quickly fallen into the background. Swagger UI has enjoyed dominance when it comes to interactive API documentation, but is something I see beginning to shift. I’m hoping that <a href="http://client.apievangelist.com/">API clients and development environment</a> like <a href="http://www.getpostman.com/">Postman</a>, <a href="https://restlet.com/modules/client/">Restlet</a>, <a href="http://luckymarmot.com/paw">PAW</a>, and <a href="http://insomnia.rest/">Insomnia</a> continue to develop mindshare. These tools are all good for helping make API integration easier, but as I saw at APIStrat, they can also help us communicate with our readers, and audience about what is possible with APIs. Speaking to as wide of a group as possible, elevating above any single programming language, and just speaking API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/07/postman-as-a-live-coding-environment-in-presentations-at-apistrat/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/07/developing-your-talent-pool-in-your-api-platform-community/">Developing A Talent Pool Within Your API Community</a></h3>
        <span class="post-date">07 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/art-museum/art-museum_blue_circuit_3.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>There are many reasons for having an API. The direct reason is to provide your partners and 3rd party developers access to your data, content, and algorithmic resources using the web. However, there are many indirect, and less obvious reasons for having an active API program at your company, organization, institution, or government agency. Things that you probably haven’t thought of, but the groups who are already doing APIs have known about these benefits for a while. One of these benefits is in the area of talent acquisition, and building relationships with, and identifying folks with the skills you are looking for.</p>

<p>I remember the first time I heard the executives at Paypal say that they often hire out of their development community. After hearing that I began asking more API providers about the talent acquisition opportunities within their API developer ecosystem, and about 50% of the people I talked said they had hired someone building an application on their API, or had shown up to a hackathon to participate and build interesting things. It is easy to think of our API platforms as simply a place for developing applications, but along with each integration and application, there is one or many humans behind it, who have an understanding of your API, and possibly the industry you are targeting. Once you are introduced to the concept, it makes complete sense to be using an API as a talent acquisition vehicle, allowing developers to flex their skills in a place you are already paying attention in.</p>

<p>This is my new response to people who are either looking for talent. Have you tried your API community? Many of the people I’ve tested this out on do not have a public API program. They aren’t at this place in their journey where they have a public presence in this way. Either because they don’t have any public resources to make available, or they just haven’t evolved to the point where they are ready to make things publicly available. Talent acquisition is another motivator in this journey. Maybe your not ready for making your data, content, and algorithms public, but there has to be something that is already available on your website, or some sort of sandbox environment you could make public to begin attracting talented folks. Its another motivation for being public around your digital assets. Even if you aren’t entertaining direct public access to all your resources, you should be casting an SEO net in your industry, publishing relevant (sample) data sets, content, or algorithms that someone might find Googling, or some other discovery process.</p>

<p>A public API program can be so many different things. Those who have embarked on this journey are the ones who are pushing the boundaries, and are discovering the unexpected benefits like talent acquisition. Those who aren’t, will struggle to keep up, and secure the talent they need. API portals aren’t just for building the next Twitter. They are your externally facing R&amp;D lab, where you explore new ideas, and learn to play nicely with others. It is where you develop new products, integrations, applications, and when done right, new talent. Your APIs don’t have to be production quality, they might just be a sampling, or sandboxed version of the real thing. Just enough to attract the interest of just the right people in your industry. Don’t let your lack of imagination about what an API is hold you back from experimenting and playing around, and realizing these benefits.</p>

<p>If you do not have your API platform available yet, this is all probably news to you, because you haven’t been in the game. You haven’t been building relationships with other companies, and individuals. You haven’t been publishing documentation, white papers, and telling other stories that will be attracting the interesting folks you are looking to hire. You are probably still relying on job sites, and social network sites to find the talent you need, where you are competing with everyone else in your industry. Why not get started on your API program, publish some safe data, and content that is already on your web site. Or, maybe get creative and publish some sandbox APIs that allow folks to play, explore, understand, while you are studying who the most interesting consumers are , and using your API as a talent acquisition funnel for your organization.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/07/developing-your-talent-pool-in-your-api-platform-community/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/06/a-simple-api-using-aws-rds-lambda-and-api-gateway/">A Simple API Using AWS RDS, Lambda, and API Gateway</a></h3>
        <span class="post-date">06 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/api-gateway/aws-rds-lambda-api-gateway.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2017/10/23/a-simple-api-with-aws-dynamodb-lambda-and-api-gateway/">I wrote about a simple API with AWS DynamoDB, Lambda, and API Gateway last week</a>. I like this approach because of the simple nature of AWS DynamoDB. One benefit of going this route is that you can even bypass Lambda, as the AWS API Gateway can work directly with AWS DynamoDB API. I’m just playing around with different configurations and pushing forward my understanding of what is possible, and this week I switched out the database in this with AWS RDS, which opens up the ability to use MySQL or Postgres as the backend for any API.</p>

<p>For this example, I’m using a simple items database, which you can build with this SQL script after you fire up an RDS instance (I’m using MySQL):</p>

<script src="https://gist.github.com/kinlane/f428a30c1a6a59718657cafd8e52f615.js"></script>

<p>Next I wanted to have the basic CRUD operations for my API. I opted to use Node.js running in Lambda for the code layer of this API, starting with the ability to get all records from the database:</p>

<script src="https://gist.github.com/kinlane/4afdde612d31d50b0fed2658ba73df29.js"></script>

<p>After that I want to be able to insert new records:</p>

<script src="https://gist.github.com/kinlane/36eb527dbd5e843f9f3a3c954302245c.js"></script>

<p>Then of course be able to get a single record:</p>

<script src="https://gist.github.com/kinlane/f1fc319aefa7dbde63b2f914f998d7e6.js"></script>

<p>Then be able to update a single record:</p>

<script src="https://gist.github.com/kinlane/17dd4cfb0c1bef1fefc132a5b4d20c7d.js"></script>

<p>And of course I want to be able to delete records:</p>

<script src="https://gist.github.com/kinlane/80b6d50b16097be5b5ae91b183c041b7.js"></script>

<p>Now that I have the business logic setup in AWS Lambda for reading, and writing data to my relational database I want an API front-end for this backend setup. I am using AWS API Gateway as the API layer, and to setup I’m just importing an OpenAPI definition to jumpstart things:</p>

<script src="https://gist.github.com/kinlane/73bda5467abe7428e9de93e47e774849.js"></script>

<p>This gives me the skeleton framework for my API, with the paths and methods I need to accomplish the basics of reading and writing data. Now, I just need to wire up each API method to its accompanying Lambda function, something API Gateway makes easy.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/api-gateway/aws-api-gateway-lambda.png" /></p>

<p>Now I have an API for my basic backend. There is one thing you have to do to make each method work properly with the Lambda function. You have to setup a body mapping to the item_id when passed in the path for the PUT, GET, and DELETE functions. If you don’t the item_id won’t be passed on to the Lambda function–it took me a while to get this one. There are other things you have to do, like setting up a usage plan, turning on API key access for each API, and setting up custom domain if you want, but hopefully this simple gets the point across. I will work on other parts in future posts. Hopefully it provides a basic example of an API using RDS, Lambda, and API Gateway, which is something I have wanted to have in my toolbox for some time.</p>

<p>The process has opened my eyes up wider to the serverless world, as well as playing more with Node.js–which has been on my list for some time now. It provides a pretty solid, scalable, manageable way to deploy an API using AWS. I have all the code on Github, and will be evolving as I push it forward. If you apply the Lambda scripts, make sure you upload individually as zipped files, so that the mysql dependencies are there, otherwise the script won’t connect to the database. It should provide a base template you can use to seed any basic data API. This is why I’ve added it to my API Evangelist toolbox, giving me a simple, forkable set of scripts I can use as a seed for any new API. I will add more scripts and templates to it over time, rounding off the functionality as I evolve in my understanding of deploying API using AWS RDS, Lambda, and API Gateway.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/06/a-simple-api-using-aws-rds-lambda-and-api-gateway/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/06/api-security-beginning-to-outweigh-my-vendor-lock-in-concerns/">API Security Beginning To Outweigh My Vendor Lock-In Concerns</a></h3>
        <span class="post-date">06 Nov 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/iam/aws-iam-api-gateway.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been on the AWS train since day one. I’ve been integrating Amazon S3 and EC2 into my business(es) since they first launched a decade ago. While the platform has faithfully provided my storage and compute for over a decade I’ve always been wary of vendor lock-in. After a decade long ride on Microsoft (1998-2008), I felt pretty burned. Then recently after a similar decade long ride on Google (2005-2015), I felt burned, but in a different way. After a decade on AWS I’m nervous, but I don’t feel as burned, however I’d say there is one aspect of doing business online that is making me put aside some of my concerns regarding vendor lock-in on AWS, and even on Google, and Azure–SECURITY. I’m just not convinced I can do this alone, and I need the help of the platforms I operate on to help make sure my operations are secure.</p>

<p>I spent the weekend setting up a set of APIs using AWS RDS as the backend database, AWS Lambda as the code layer, and AWS API Gateway as the API front-end. As part of this work I established an identity and access management (IAM) role with policies tailored for brokering the exchanges between RDS, Lambda, and the API Gateway. I’m leaning on AWS for two key API security pieces here: 1) API key management with AWS API Gateway, and 2) backend security using AWS IAM. The key management stuff is pretty straightforward and something I can easily replicate, but the AWS IAM stuff is a little more involved, and I’m grateful for how easy AWS IAM makes it for me to setup roles, cherry pick from common policies, and lock down the infrastructure I a using to drive the backend of my APIs and other applications.</p>

<p>As I moved another API project into the home stretch this weekend, I migrated an RDS, and EC2 instance into a separate account for a client. I had been defining, designing, and developing an API for them over the last coupe of months, and as we approached completion, I wanted everything in their own account where they could be accountable for the AWS bill, and take control over operations, as well as security of their own API operations. Being able to configure IAM for each project, and pass it off to clients using a separate AWS account is super beneficial to me, as well as my clients. I’m not confident my clients can tackle API security any better than I can in the long run, and being able to configure roles, policies, and stand-up a working set of APIs that are secure, is a critical aspect of doing business online using APIs for both me and my clients.</p>

<p>While I still have vendor lock-in concerns with AWS, especially when it comes to adopting a serverless approach with Lambda, the ability to define API and backend security in this way is softening these concerns. In the volatile online environment we find ourselves in I am just not confident that I have the time, skills, and resources to do API security properly. I’m also not confident my clients can either. I’m thankful for AWS IAM, and the other security solutions they bring to the table. I’m feeling like we are going to need our cloud platform providers to have our backs like this in the future, as well as the services of a suite of other API security providers to help us do all of this properly. I wish it was a utopian online world where we could hand roll our infrastructure and remain safe online, but in 2017 I’m not interested in doing any of this alone. There are just too many threats out there.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/06/api-security-beginning-to-outweigh-my-vendor-lock-in-concerns/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/06/an-example-of-how-every-api-provider-should-be-using-openapi-out-of-the-slack-platform/">An Example Of How Every API Provider Should Be Using OpenAPI Out Of The Slack Platform</a></h3>
        <span class="post-date">06 Nov 2017</span>
        <p><a href="https://medium.com/slack-developer-blog/standard-practice-slack-web-openapi-spec-daaad18c7f8"><img src="https://s3.amazonaws.com/kinlane-productions/slack/slack-standard-practice.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p><a href="https://medium.com/slack-developer-blog/standard-practice-slack-web-openapi-spec-daaad18c7f8">The Slack team has published the most robust and honest story about using OpenAPI, providing a blueprint that other API providers should be following</a>. What I like most about approach by Slack to develop, publish, and share their OpenAPI, is the honesty behind why their are doing it to help standardize around a single definition. <a href="https://github.com/slackapi/slack-api-specs">They publish and share the OpenAPI to Github</a>, which other API providers are doing, and I think should be standard operating procedure for all API providers, but they also go into the realities regarding the messy history of their API documentation–an honesty that I feel ALL API providers should be embracing.</p>

<p>My favorite part of the story from Slack is the opening paragraph that honestly portrays how they’ve got here: <em>“The Slack Web API’s catalog of methods and operations now numbers nearly 150 reads, writes, rights, and wrongs. Its earliest documentation, much still preserved on api.slack.com today, often originated as hastily written notes left from one Slack engineer to another, in a kind of institutional shorthand. Still, it was enough to get by for a small team and a growing number of engaged developers.”</em> Even though we all wish we could do APIs correctly, and supporting API document perfectly from day one, this is never the reality of API operations, and something OpenAPI will not be a silver bullet for fixing all of this, but can go a long way in helping standardize what is going on across teams, and within an API community.</p>

<p>Slack focuses on SDK development, Postman client usage, alternative forms of documentation, and mock servers as the primary reasons for publishing the OpenAPI for their API. They also share some of the back story regarding how they crafted the spec, and their decision making process behind why they chose OpenAPI over other specifications. They also share a bit of their road map regarding the API definition, and that they will be adopting v3.0 of OpenAPI v3.0, providing <em>“more expressive JSON schema response structures and superior authentication descriptors, specifications for incoming webhooks, interactive messages, slash commands, and the Events API tighter specification presentation within api.slack.com documentation, and example spec implementation in Slack’s own SDKs and tools”</em>.</p>

<p>I’ve been covering leading API providers move towards OpenAPI adoption for some time. Writing about <a href="https://apievangelist.com/2017/03/01/new-york-times-manages-their-openapi-using-github/">the New York Times publishing of their OpenAPI definition to Github</a>, and <a href="https://apievangelist.com/2017/05/22/box-goes-all-in-on-openapi/">Box doing the same, but providing even more detail behind the how and why of doing OpenAPI</a>. Slack continues this trend, but showcases more of the benefits it brings to the platform, as well as the community. All API providers should be publishing and up to date OpenAPI definition to Github by default like Slack does. They should also be standardizing their documentation, mock and virtualized implementations, generating SDKs, and driving continuous integration and testing using this OpenAPI, just like Slack does. They should be this vocal about it too, encouraging the community to embrace, and ingest the OpenAPI across the on-boarding and integration process. I know some folks are still skeptical about what OpenAPI brings to the table, but increasingly the benefits are outweighing the skepticism–making it hard to ignore OpenAPI.</p>

<p>Another thing I want to highlight in this story, is that Taylor Singletary (<a href="https://twitter.com/episod">@episod</a>), reality technician, documentation &amp; developer relations at Slack, brings an honest voice to this OpenAPI tale, which is something that is often missing from the platforms I cover. This is how you make boring ass stories about mundane technical aspects of API operations like API specifications something that people will want to read. You tell an honest story, that helps folks understand the value being delivered. You make sure that you don’t sugar coat things, and you talk about the good, as well as some of the gotchas like Taylor has, and connect with your readers. It isn’t rocket science, it is just about caring about what you are doing, and the human beings your platform impacts. When done right you can move things forward in a more meaningful way, beyond what the technology is capable of doing all by itself.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/06/an-example-of-how-every-api-provider-should-be-using-openapi-out-of-the-slack-platform/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/02/i-like-the-scope-of-the-aws-sdk-for-javascript/">I Like The Scope Of The AWS SDK for JavaScript</a></h3>
        <span class="post-date">02 Nov 2017</span>
        <p><a href="http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/"><img src="https://s3.amazonaws.com/kinlane-productions/amazon/SDK/aws-sdk-for-javascript.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I’ve tried picking up Node.js as a server side approach to delivering APIs a couple of times in the past few years. Both times express, and a handful of other issues ran me off from using it as my default backend programming language for APIs. I use a lot of JavaScript in the client, but it just didn’t feel like it was what I needed to drive my backend solutions. Over the last week I’ve been architecting a handful of APIs using Lambda and Node.js scripts, and I have to admit I’m pretty sold on Node.js as a backend solution now. This conversion is partly due to Lambda and a serverless way of doing things, but is also due to <a href="http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/">the AWS SDK for JavaScript</a>, which is a pretty robust example of an API SDK.</p>

<p><a href="http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/">The AWS SDK for JavaScript</a> covers 106 separate AWS services, and I haven’t actually calculated how many methods are available–a lot. I’ve been using it recently to manage AWS RDS, DynamoDB, and API Gateway, and the SDK is a delight to use, is well documented, and enjoys excellent coverage of AWS services. There were two elements at play here that made this work for me, 1) simplicity of the AWS SDK in Node, and 2) the simplicity of implementing in the serverless environment that is Lambda. I found Node.js scaffolding to be overly complex for APIs in previous experiences, something that melted away with a serverless implementation. I am well versed in JavaScript, and with the AWS SDK being so simple enough, I was able to get up and running with a variety of functions on Lambda pretty quickly. Something that becomes really powerful when exposed as an API resource using AWS API Gateway.</p>

<p>This is going to date me, and I know that only a handful of people will understand, but the AWS SDK for JavaScript, combined with Lambda and API Gateway reminds me of VBScript on Windows server, circa 2000. Back in the day, I had my own server farm, on my own dedicated T1, on an entire C block of IP addresses, running Windows 2000. I was able to orchestrated some pretty cool cross-server voodoo using VBScript when it came to managing the server, database, web server, DNS (Active Directory), and other moving parts of my world. I have way more resources at my disposal with AWS, and a much more robust vocabulary with the AWS SDK for Javascript than I did back in the day, but the feeling I have is very similar. I am able to orchestrate pretty effectively across my operations, using a variety of AWS, 3rd party, as well as my own home grown APIs. I have the control over my infrastructure that I am looking to have, allowing me to get what I need done.</p>

<p>I like the scope of the AWS SDK for JavaScript. It gives me access to what I need, while also introducing me to other AWS APIs at the same time. I set out to use the SDK for DynamoDB, then RDS, and API Gateway, and now I’m exploring how I can use it for managing EC2 ad S3, evolving the current PHP SDK I’ve had in place for the last five years. I was’t looking to evolve some of my legacy API infrastructure using this new approach which I’ve evolving for a client, but the ease of use, scalability, and benefits brought to the table are proving to be worth the extra work. I’m rolling out a suite of Lambda functions for managing RDS, EC2, S3, and API gateway, as well as expanding to work with Github, Stripe, and other external APIs I depend on. This ease of use, plus the security benefits introduced by AWS IAM is pushing me further into the AWS universe, without the vendor lock-in regret I was expecting to have.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/02/i-like-the-scope-of-the-aws-sdk-for-javascript/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/26/my-response-on-the-department-of-veterans-affairs-rfi-for-the-lighthouse-api-management-platform/">My Response On The Department Of Veterans Affairs (VA) RFI For The Lighthouse API Management Platform</a></h3>
        <span class="post-date">26 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/veterans-affairs/va-logo.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am working with my partners in the government API space (<a href="https://skylight.digital/">Skylight</a>, <a href="https://540.co/">540</a>, <a href="https://agile6.com/">Agile Six</a>) to respond to a request for information (RFI) out of the Department of Veterans Affairs (VA), for what they call the <a href="https://www.fbo.gov/index?s=opportunity&amp;mode=form&amp;tab=core&amp;id=b5a3e0c29fa78545f0556dac972bac69">Lighthouse API Management platform</a>. The RFI provides a pretty interesting look into the way the government agency which supports our vets is thinking about how they should be delivering government resource using APIs, but also how they play a role in the wider healthcare ecosystem. My team is meeting today to finalize our response to the RFI, and in preparation I wanted to prepare my thoughts, and in my style of doing things, involves publishing them here on API Evangelist.</p>

<p>You can read <a href="https://www.fbo.gov/index?s=opportunity&amp;mode=form&amp;tab=core&amp;id=b5a3e0c29fa78545f0556dac972bac69">the whole RFI</a>, but I’ll provide the heart of it, to help set the table for my response.</p>

<p><strong>Introduction:</strong>
To accelerate better and more responsive service to the Veteran, the Department of Veterans Affairs (VA) is making a deliberate shift towards becoming an Application Programming Interface (API) driven digital enterprise. A cornerstone of this effort is the setup of a strategic Open API Program that is adopting an outside-in, value-to-business driven approach to create APIs that are managed as products to be consumed by developers within and outside of VA.</p>

<p><strong>Objectives:</strong>
VA has started the process of establishing an API Management Platform, named Lighthouse.  The purpose of Lighthouse is to establish the Next Generation Open Digital Platform for Veterans, accelerating the transformation in core domains of VA, such as Health, Benefits, Burial and Memorials. This platform will be a system for designing, developing, publishing, operating, monitoring, analyzing, iterating and optimizing VA’s API ecosystem. These APIs will allow VA to leverage its investment in various digital assets, support application rationalization, and allow it to decouple outdated systems and replace them with new, commercial, off the shelf, Software as a Service (SaaS) solutions. It will enable creation of new, high value experiences for our Veterans, VA’s provider partners, and allow VA’s employees to provide better service to Veterans.</p>

<p><strong>With some insight into how they will achieve those objectives:</strong></p>

<ul>
  <li>Integrate more effectively with the community care providers by connecting members of a Veteran’s “Care Team” from within and outside the Veterans Health Administration (VHA); and, generate greater opportunities for collaboration across the care continuum with private sector providers,</li>
  <li>Effectively shift technology development to commercial electronic health record (EHR) and administrative systems vendors that can integrate modular components into the enterprise through open APIs, thus allowing VA to leverage these capabilities to adopt more efficient and effective care management processes,</li>
  <li>Foster an interoperable, active, innovation ecosystem of solutions and services through its Open API Framework that contributes to the next generation of healthy living and care models that are more precise, personalized, outcome-based, evidence-based, tiered and connected across the continuum of care regardless of where and how care is delivered,</li>
  <li>Create an open, accessible platform that can be used not only for Veterans care but also for advanced knowledge sharing, clinical decision support, technical expertise, and process interoperability with organizations through the U.S. care delivery system. By simplifying access to the largest data set of clinical data anywhere, albeit de-identified, it will accelerate the discovery and development of new clinical pathways for the benefit of the Veterans and the community at large.</li>
</ul>

<p>They include some bullets for what they see as the road map for the Lighthouse API management platform:</p>

<ul>
  <li>The API Gateway, through creating the facades for common, standard Health APIs will allow multiple EHRs to freely and predictably interoperate with each other as VA deploys the COTS EHR through the enterprise and finds itself with multiple EHR’s during the transition and stabilization phases of the implementation.</li>
  <li>Establish the Single Point of Interoperability with health exchanges and EHR systems of Community Care Providers.</li>
  <li>The development of APIs across the enterprise spanning additional domains following a federated development approach that can see an exponential growth in the number APIs offered by the platform.</li>
  <li>Operate at scale to support the entire population of Veterans, VA Employees and Community Providers that provide a variety of services to the Veterans</li>
</ul>

<p>This is all music to my ears. These are objectives, and a road map that I can get behind. It is an RFI that reflects where all federal agencies should be going, but it also is also extra meaningful for me to see this coming out of the VA. Definitely making it something I want to support however I can. The window for responding is short, so I wanted to be able to stop what I’m doing, and give the RFI a proper response, regardless of who ends up running the platform. I’ve done type of RFI storytelling several times in the past, with <a href="http://ed-data.github.io/fafsa-api/">the FAFSA API for Department of Education</a>, the <a href="http://apievangelist.com/2014/10/16/i-need-help-to-make-sure-the-dept-of-agriculture-leads-with-apis-in-their-parks-and-recreation-rfp/">Recreational Information Database (RIDB) out of Department of Interior</a>, when <a href="https://apievangelist.com/2014/07/15/chief-data-officer-needs-to-make-the-department-of-commerce-developer-portal-the-center-of-api-economy/">Commerce hired a new Chief Data Officer</a>, and <a href="https://apievangelist.com/2014/07/15/chief-data-officer-needs-to-make-the-department-of-commerce-developer-portal-the-center-of-api-economy/">just in general support of the White House API strategy</a>. This time is different, only because I have a team of professionals who can help me deliver beyond just the RFI.</p>

<p><strong>Some Background On Me And The VA</strong>
Before I get to the questions included as part of the RFI, I wanted to give some background on my relationship to the VA, and supporting veterans. First, my biological father was career military, and the father that raise me was a two tour Vietnam veteran, exposing me to the veterans administration and hospitals at an early age. He passed away in the 1990s from cancer he developed as a result of his Agent Orange exposure. All this feeds into my passion for applying APIs in this area of our society. Additionally, I used to work for the Department of Veterans affairs, as a Presidential Innovation Fellow in 2013. I didn’t stay for the entire fellowship, exiting during the government shutdown, but I have continued to work on opening up data sets, supporting VA related API conversations, and trying to keep in tune with anything that is going on within the agency. All contributing to this RFI making me pretty happy.</p>

<p><strong>My Answers To The RFI Questions</strong>
The VA provides 20 separate questions as part of their RFI. Shining a light on some of the thinking that is occurring within the agency. I’ve bolded their questions, and provide some of my thought inline, sharing my official responses. My team will work to combine everyones feedback, and publish a formal response to the VA’s RFI.</p>

<p><strong>1. Drawing on your experience with API platforms, how do you see it being leveraged further within the healthcare industry in such a manner as described above? What strengths and opportunities exist with such an approach in healthcare?</strong></p>

<p>An API first platform as proposed by the VA reflects what is already occurring in the wider healthcare space. With veteran healthcare spending being such a significant portion of healthcare spending in this country, the presence of an API platform like this would not just benefit the VA, veterans, veteran hospitals, and veteran service organizations (VSO), it would benefit the larger economy. A VA API platform would be the seed for a federated approach to not just consuming valuable government resources, but also deploying APIs that will benefit veterans, the VA, as well as the larger healthcare ecosystem.</p>

<p>Some of the strengths of this type of approach to supporting veterans via an open data and API platform, with a centralized API operations strategy would be:</p>

<ul>
  <li><strong>Consistency</strong> - Delivering APIs as part of a central vision, guided by a central platform, but echoed by the federated ecosystem can help provide a more consistent platform of APIs.</li>
  <li><strong>Agility</strong> - An API-first approach across all legacy, as well as modern VA systems will help break the agency, and its partners into much smaller, bite-size, reusable components.</li>
  <li><strong>Scope</strong> - API excel at decouple legacy systems that are often larger and more complex, delivering much smaller, reusable components that can be developed, delivered and deprecated in smaller chunks, that work at scale.</li>
  <li><strong>Observability</strong> - A modern approach to delivering an API platform for government with a consistent model for API definitions, design, deployment, management, monitoring, testing, and other steps of the API lifecycle, combined with a comprehensive approach to measuring and analysis brings much needed observability to all stops along the lifecycle. Letting in the sunlight required for success at scale.</li>
  <li><strong>Feedback Loops</strong> - An API platform at this scale, which the appropriate communication and support channels open up feedback loops for all stops along the lifecycle, benefitting from community, and partner input from design to deprecation. Making all VA systems</li>
</ul>

<p>These platform strengths set the stage for some interesting and beneficial opportunities to emerge from within the platform community, but also the wider healthcare ecosystem that already exists, and is evolving, which sill step up and participate and engage with available VA resources. Here are some examples of how this can evolve based upon existing API ecosystem within and outside the healthcare ecosystem:</p>

<ul>
  <li><strong>Practitioner Participation</strong> - An open API platform always starts with engaging backend domain experts, allowing for the delivery of APIs that deliver access to critical resources. Second, modern API platforms help open up, and attract external domain experts, and in the case of a VA platform, this would mean more engagement with health care practitioners, further rounding off what goes into the delivery of critical veteran services.</li>
  <li><strong>Healthcare Vendor Integration</strong> - Beyond practitioners, a public API platform for the VA would attract existing healthcare vendors, providing EHR, and other critical services. Vendors would be able to define, design, deploy, and integrate their API driven solutions outside the tractor beam of internal VA operations.</li>
  <li><strong>Partner Programs</strong> - The establishment of formal partner program accompany the most mature API platforms available, allowing for more trusted engagement across the public and private sector. Partners will bring needed resources to the ecosystem, while benefitting from deeper access to VA resources.</li>
  <li><strong>Certification</strong> - Another benefit of a mature API platform is the ability to certify partners, applications, and developers, establishing a much more trusted pool of vendors, and developers that the VA can benefit from, as well as rest of the ecosystem.</li>
  <li><strong>Standardization</strong> - A centralized API platform helps aggregates common schema, API definitions, models, and blueprints that can be used across the federated public and private sector. Helping establish consistency across VA API resources, but also in the tooling, services, and other applications built on top of VA APIs.</li>
  <li><strong>Data Ecosystems</strong> - A significant number of VA APIs available on the platform will be driven by valuable data sets. The more APIs, and the consistency of APIs across many internal data stewards, as well as trusted partner sources will establish a data ecosystem that will benefit veterans, the VA, as well as the rest of the ecosystem.</li>
  <li><strong>Community Participation</strong> - Open API platform bring in a wide variety of developers, data scientists, healthcare practitioners, and folks who are passionate about the domain. When it comes to supporting veterans, it is essential that there is a public platform and forum for the community to engage around VA resources. Helping share the load with the community, when it comes to serving veterans.</li>
  <li><strong>Marketplace</strong> - With an API platform as proposed by this RFI, a natural opportunity is with a marketplace. An established process and platform for internal groups to publish their APIs, as well as supporting SDKs, and other applications. This marketplace could also be made available to trusted partners, and certified developers, allowing them to showcased their verified applications, and be listed as a certified developer or agency. The marketplace opportunity will bring the most benefit to the VA, when it comes to delivering across many internal groups.</li>
</ul>

<p>The benefits of an API platform such as the one proposed as part of this RFI is that they make the delivery of critical services a team effort. Domains within the VA can do what they do best, and benefit from having a central platform, and team to help them deliver consistent, reliable, scalable API access to their resources, for use across web, mobile, device, spreadsheet, analysis, and other applications. Externally, healthcare vendors, hospitals, practitioners, veterans, and the people that support them can all work together to put valuable API resources to work, helping make sense of data, deliver more modular, meaningful applications that all focus on the veteran.</p>

<p>The fact that this RFI exists shows that the VA is looking to keep pace with where the wider healthcare sector is headed. Four our of five of the leading EHR providers have existing API platforms. Demonstrating where the healthcare sector is headed, and something that reflects the vision established in this RFI. The strength and the opportunity with such an approach to delivering healthcare services for veterans will be fully realized when the VA becomes interoperable, and plug and play with the wider healthcare sector.</p>

<p><strong>2. Describe your experience with different deployment architecture supported by MuleSoft (e.g. SaaS only, On Premise Only, Private cloud, Hybrid – Mix of SaaS and Private Cloud) and in what industry or business process it was used? Please include whether your involvement was as a prime or subcontractor, and whether the work was in the commercial or government sector.</strong></p>

<p>My exposure to the Mulesoft in a production environment has been limited in recent years, especially within the last couple of years. However, during my time at the Veterans Affairs, and working in government, I was actively involved with the company, regularly engaging with their product and sales team in several areas:</p>

<ul>
  <li><strong>Industry Research</strong> - I was paid to conduct API industry research back in 2012 and 201 by Mulesoft, providing regular onsite presentations regarding my work, contributing to their road map.</li>
  <li><strong>Messaging Partner</strong> - I worked with Mulesoft on the creation of short form and long form content around the API industry, and as part of my API industry conference.</li>
  <li><strong>Sales Process</strong> - I have sat in on several sales meetings for Mulesoft with government agencies, enterprise organizations, and higher educaitonal institutions, and are familiar with what they offer.</li>
  <li><strong>RAML Governance</strong> - I was part of the early discussion, formation, and feedback on the RAML API specification form, but left shortly after I left the government.</li>
</ul>

<p>Based upon my experience working with the Mulesoft team, and through my regular monitoring of their services, tooling, and engaging with some of their clients I am confident in my ability to tailor a platform strategy that would work with their API gateway, IAM, definitions, discovery, and other solutions. I have been one of the analysts in the API sector who studies Mulesoft, and the other API management providers, and understand the strengths, and weaknesses of each of the leading vendors, as well maintain and understanding of how API management is being commoditized, standardized, and applied across all the players. I’m confident this knowledge will transfer to delivering an effective vision for the VA, involving Mulesoft solutions.</p>

<p><strong>3. Describe any alternative API Management Platforms that are offered as SaaS offerings, On Premise, Private Cloud, or Hybrid.  Please detail how these solutions can scale to VA’s needs managing approximately 56,000 transactions per second through connecting to VistA and multiple commercial and open source EHRs (conforming to US Core Profiles of the HL7 FHIR standards), multiple commercial Enterprise Resource Planning (ERP) systems, various home grown systems including Veterans Benefit Management Service (VBMS), Corporate Data Warehouse, and VA Time &amp; Attendance System (VATAS),  and commercial real-time analytics software packages, and open source tools enabling rapid web and mobile application development.</strong></p>

<p>Monitoring and understanding API management platforms is something I’ve done since 2010 in a full time capacity. I’ve studied the evolution of the enterprise API gateway from its service oriented architecture days, into the cloud as a commodity with AWS, Google, and Azure, as well as the deployment on-premise, in hybrid scenarios, and even on-device. To support my partners, clients, and the readers of my blog, I work regularly to test drive, and understand leading API management solutions available on the market today.</p>

<p>While I study the proprietary and open source API gateway and management solutions out there, I also work to understand where API providers will be operating these solutions, which means having a deep understanding of how native and installed API management occurs in the cloud, hybrid, on-premise, on-device, and anywhere it needs to be in 2017.</p>

<ul>
  <li><strong>Multi-Vendor</strong> - API gateways have become a commodity and are baked into every one of the major cloud providers. AWS has the lead in this space, with Google, and Azure following up. We stay in tune with a multi-vendor gateway approach to be able to support, and deliver solutions within the context of how our customers and clients are already operating. To support the number of transactions the VA is currently seeing, as well as with future API implementation, a variety of approaches will be required to support the requirements of many different internal groups, as well as trusted partners.</li>
  <li><strong>Multi-Cloud</strong> - As I already mentioned, always look to support multiple gateways across multiple datacenter, and cloud providers. Our goal is to understand the native opportunities on each cloud platform, the environment to deliver hybrid networks and environments, as well as how each of the leading open source and proprietary API management providers operate within a cloud environment. Making it easier to integrate with, and deliver API facades for any backend resources available within any cloud environment.</li>
  <li><strong>Reusable Models</strong> - One key to successful API management in any environment is the establishment of reusable models, and the reuse of definitions, schema, and templates which can be applied consistently across groups. Any API gateway, and management solution should have an accompanying database, machine, or container image, and be driven using a machine readable API definition using OpenAPI, API Blueprint, or RAML. All data should possess an accompanying definition using JSON Schema, and leverage standards like JSON Path for mapping relationships, and establishing mappings between resources. Everything should be defined for implementation, reuse, and continuous deployment as well as integration.</li>
  <li><strong>API-Driven</strong> - Every mature API management platform automates using APIs. Mulesoft, Apigee, 3Scale, AWS Gateway, all have APIs for managing the management of APIs. This allows for the seamless integration across all systems, even disparate backend legacy systems. Extending identity, service compositions, plans, rate limits, logging, analysis, and other exhaust from API management into any system it is needed. Lighthouse should be an API driven platform for defining, designing, deploying, and managing APIs the serve the VA.</li>
</ul>

<p>In full disclosure, I’ve worked with almost every API management provider out there in some capacity. Similar to providing industry analysis to Mulesoft, I helped WSO2 define and evolve their open source API management solution. Engaged and partnered with 3Scale (now Red Hat), Apigee (Now Google), Mashery (now Tibco), and others. I’ve also engaged with NREL and their development of API Umbrella, as well as the GSA when it came to implementing in support of api.data.gov. I’m currently taking money from Tyk, 3Scale, Runscope, and Restlet–all servicing the space. It is my job to understand the API management playing field, as well as the nuts and bolts of all the leading providers.</p>

<p>While it is important to be able to dive deep and support specific solutions for specific project when it comes to API management, for a platform to handle the scale and scope of the Lighthouse API management platform it will have to be able to provide support for connecting to a robust set of known, and unknown backend systems. While many organizations we’ve worked with strive for a single gateway or API management solution, in reality many often operating multiple solutions, across many vendors, and multiple cloud or on-premise environments. The key to a robust, scalable platform is the ability to easily define, configure, and repeat regularly across environment, providing a consistent API surface area across implementations, groups, backend, and gateway infrastructure.</p>

<p><strong>4. Describe your company’s specific experience and the strategies that you have employed for ensuring the highest level of availability and responsiveness of the platform (include information about configuration based capabilities such as multi-region deployments, dynamic routing, point of presence, flexible/multi-level caching, flexible scaling, traffic throttling etc.).</strong></p>

<p>Our approach to delivering API infrastructure involves assessing scalability at every level of API management within our control. When it comes to API deployment and management we aren’t always in control over every layer of the stack, but we always work to configure, optimize, and scale whenever we possibly can. Every API will be different, and a core team will enjoy a different amount of control over backends, but we always consider the full architectural stack when ensuring availability and responsiveness across API resources, looking at eight common layers:</p>

<ul>
  <li><strong>Network</strong> - When possible we work to configure the network to allow for prioritization of traffic, and isolation of resources at the network level.</li>
  <li><strong>Sytem</strong> - We will work with backend system owners to understand what their strengths and weaknesses are, and understand how systems are currently optimize and assess what new ways are possible as part of Lighthouse API operations.</li>
  <li><strong>Database</strong> - When there is access to the database level will will assess the scalability of instances and tables in service of API deployment. If possible we will work with database groups to deploy slave implementations that can be dedicated to supporting API implementations.</li>
  <li><strong>Serverless</strong> - Increasingly we are using serverless technology to help carry the load for backend systems, adding another layer for optimization behind the gateway. In some situations a serverless layer can act as a proxy, cache, and redundancy for backend systems. Opening up new approaches to availability.</li>
  <li><strong>Gateway</strong> - At the gateway level there are opportunities for scaling, and delivering performance, as well as enabling caching, and rate limiting to optimize specific types of behaviors amongst consumers. Each API will have its own plan for optimization and reliability, tailored for its precise configuration and time to live (TTL).</li>
  <li><strong>DNS</strong> - DNS is another layer which will play a significant role in the reliability and performance of API operations. There are numerous opportunities for routing, caching, and multi-deployment optimizations at the DNS to support API operations.</li>
  <li><strong>Caching</strong> - There are multiple levels to think about caching in API infrastructure, from the backend up to DNS. The entire stack should be considered on an API by API basis, with a robust approach to knowing when to use, and where not to use.</li>
  <li><strong>Regional</strong> - One of the benefits of being multi-vendor, and multi-cloud, and on-premise, is the ability to deliver API infrastructure where it is needed. Delivering in multiple geographic regions is an increasingly common reason for using the cloud, as well as allowing for flexibility in the deployment, management, and testing of APIs in any US region.</li>
  <li><strong>Monitoring</strong> - One aspect of availability and reliability is monitoring infrastructure 24x7, keeping an eye on availability and performance. Sustained monitoring, across providers, and regions is essential to understanding how to ensure APIs are available and dependable.</li>
  <li><strong>Analysis</strong> - Extracted from monitoring, and logging, the analysis across API operations feeds into the overall availability and reliability of APIs. Real-time analysis of API availability and performance over time is critical for dependable infrastructure.</li>
  <li><strong>Partners</strong> - I’ve experience the lack of dependability of VA APIs first hand. During my time at the agency I was forced to shut down APIs I had worked on during the government shutdown. Introducing me to another dimension of API reliability, where I feel external partners can help contribute to the redundancy and availability of APIs beyond what the agency can deliver on its own.</li>
</ul>

<p>API stability isn’t purely a technical game. There are plenty of tools at our disposal for scaling, optimizing, and injecting efficiencies into the API life cycle. However, the most important contributor to reliability is experience, and making sure you measure, analyze, and understand the needs of each API. This is where modern approaches to API management come into play, and understand how API consumers are putting resources to work, and optimizing each API to support this at which every layer possible.</p>

<p><strong>5. The experiences you have and the strategies that you have employed to ensure the highest level of security for the platform. Please address policy / procedure level capabilities, capabilities that ensure security of data in transit (e.g. endpoint as well as payload), proactive threat detection etc.</strong></p>

<p>API security is another dimension of the API sector I’ve been monitoring and studying on a regular basis. I have just finished a comprehensive guide to the world of API security, which will be the first guide of its kind when publish next week. I’ve been monitoring general approaches to securing infrastructure, as well as how that impacts API security. I’ve been taking what I’ve found in my research, as well as work with clients, and aggregated into 15 separate areas to consider as we develop a strategy to deliver the high levels of security.</p>

<ul>
  <li><strong>Encryption</strong> - Encryption in transport, on the disk, in storage, and in database.</li>
  <li><strong>API Application Keys</strong> - Requiring keys for ALL API access, no matter whether internal or external.</li>
  <li><strong>Identity &amp; Access Management (IAM)</strong> - Establish roles, groups, users, policies and other IAM building blocks, healthy practices and guidance across API implementations and teams.</li>
  <li><strong>Service Composition</strong> - Provide common definitions for how to organize APIs, establish and monitor API plans and usage, and use to secure and measure access to ALL API resources.</li>
  <li><strong>Common Threats</strong> - Be vigilant for the most common threat from SQL injection, to DDoS, making sure all APIs don’t fall victim to the usual security suspects.</li>
  <li><strong>Incident Response</strong> - Establish an incident response team, and protocol, making sure when a security incident occurs there is a standard response.</li>
  <li><strong>Governance</strong> - Bake security into API governance, taking beyond just API design, and actually connecting API security to the service level agreements governing platform operations.</li>
  <li><strong>DNS</strong> - Leverage the front line of API security and identifying threats at the DNS layer, and establish, and maintain a frontline of defense for API security.</li>
  <li><strong>Firewall</strong> - Building on top of existing web security practices, and deploying, and maintaining a comprehensive set of rules at a firewall level.</li>
  <li><strong>Automation</strong> - Implement security scanning, fuzzing, and automated approaches to crawling networks and infrastructure looking for security vulnerabilities.</li>
  <li><strong>Testing &amp; Monitoring</strong> - Dovetail API security with API testing and monitoring, identifying malformed API requests, responses, and other illnesses that can plague API operations.</li>
  <li><strong>Isolation</strong> - Leveraging virtualization, serverless, and other API design and infrastructure patterns to keep API resources isolated, and minimizing any damage that can occur in a breach.</li>
  <li><strong>Orchestration</strong> - Acknowledging the security risks that come with modern approaches to orchestrating API operations, continuously deploying, and integrating across the platform.</li>
  <li><strong>Client Automation</strong> - Develop a strategy for managing API consumption, and identifying, understanding, and properly managing bots, and other types of client automation.</li>
  <li><strong>Machine Learning</strong> - Leveraging the development and evolution of machine learning models that help look through log files, and othe threat information sources, and using artificial intelligence to respond and address evolving security concerns.</li>
  <li><strong>Threat Information</strong> - Subscribing to external sources of threat information, and working with external organizations to open up the learning opportunities across platforms regarding the threats they face.</li>
  <li><strong>Metrics &amp; Analysis</strong> - Tap into the metrics and analysis that comes with API management, and build upon the awareness introduced when you consistently manage API consumption. Translating this awareness into a responsive and agile approach to delivering API security across groups, and externally with partners.</li>
  <li><strong>Bug Bounties</strong> - Defining and executing public and private bug bounties to help identify security vulnerabilities early on.</li>
  <li><strong>Communications</strong> - Establish a healthy approach to communicating around platform security. Openly discussing healthy practices, speaking at conferences, and participating in external working groups. While have a regular cadence to communication in normal times, as well as protocol for communication during security incidents.</li>
  <li><strong>Education</strong> - Develop curriculum and other training materials around platform security, and make sure API consumers, partners, and internal groups are all getting regular doses of API security education.</li>
</ul>

<p>Our API security experience comes from past projects, working with vendors, and studying the best practices of API leaders. Increasingly API security is a group effort, with a growing number of opportunities to work with security organizations, and major tech platforms who see the majority of threats present today. Increasing the volume of information available to integrate directly into platform security operations.</p>

<p><strong>6. Please describe your experience with all the capabilities that the platform offers and the way you have employed them to leverage existing enterprise digital assets (e.g. other integration service buses, REST APIs, SOAP services, databases, libraries)</strong></p>

<p>I have been working with databases for 30 years. I began working with web services in 2000, and have worked exclusively with web APIs in a full time capacity since 2010. I’ve worked hard to keep myself in tune with the protocols, and design patterns in use across the public and private API sectors. Here are the capabilities I’m tuned into currently.</p>

<ul>
  <li><strong>SOAP</strong> - Making sure we are able to support web services, and established approaches to accessing data and other resources.</li>
  <li><strong>REST</strong> - Web APIs that follow RESTful patterns is ur primary focus, leveraging low-cost web infrastructure to securely making resources available.</li>
  <li><strong>Hypermedia</strong> - Understanding the benefits delivered by adopting common hypermedia media types, delivering more experience focused APIs that emphasize relationships between resources.</li>
  <li><strong>GraphQL</strong> - Understanding when the use of GraphQL makes sense for some data focused APIs, delivering resources to single page adn mobile applications.</li>
  <li><strong>gRPC</strong> - Staying in tune with being able to deliver higher performance APIs using gRPC, augmenting the the features brought by web APIs, for use internally, and with trusted partners.</li>
  <li><strong>Websockets</strong> - Leverage websockets for streaming of data in some situations, providing more real time integration for applications.</li>
  <li><strong>Webhooks</strong> - Developing webhook infrastructure that makes APIs a two-way street, pushing notifications and data externally based upon events or on a schedule.</li>
  <li><strong>Event Sourcing</strong> - Developing a sophisticated view of the API landscape based upon events, and identifying, orchestrating and incentivizing event-based behavior.</li>
</ul>

<p>This reflects the core capabilities present across the API landscape. While SOAP and REST make up much of the landscape, hypermedia, GraphQL, event sourcing, and other approaches are seeing more adoption. I emphasize every platform make REST, or web APIs the central focus of the platform, keeping the bar low for API consumers, leverage web technology to reach the widest audience possible.</p>

<p><strong>7. Please describe your experience and strategies that you have employed at enterprises to create Experience Services from mashup/aggregation/combination of other API’s, services, database calls etc.</strong></p>

<p>I’ve been tracking on API aggregation as a discipline for over five years now, having worked with several groups to aggregate common APIs. It is an under developed layer to the web API sector, but one that has a number of proprietary, as well as open source solutions available.</p>

<p>I’ve personally worked on a number of API aggregation project, spanning a handful of business sectors:</p>

<ul>
  <li><strong>Social</strong> - Developed an open source , as well as SaaS solution for aggregating Facebook, LinkedIn, Twitter, and other social network.</li>
  <li><strong>Images</strong> - Extended a systems to work across image APIs, working with Flickr, Facebook, Instagram, and other image sharing solutions.</li>
  <li><strong>Wearables</strong> - Developed a strategy for aggregating of health data across a handful of the leading wearable device providers.</li>
  <li><strong>Documents</strong> - Partnered with a company who provides document aggregation across Box, Google Drive, and other leading document sharing platforms.</li>
  <li><strong>Real Estate</strong> - I founded a startup that did MLS data aggregation across APIs, FTP locations, and scraped targets, providing regional, and specific zip code API solutions.</li>
  <li><strong>Advertising</strong> - I’ve worked with a company that aggregates advertising APIs bringing Google, Facebook, Twitter, and other platforms together into a single interface.</li>
</ul>

<p>These are all the reasons why I was working on API aggregation, and spending time researching the subject. Here are some of the technology, and approaches I have been using to deliver on API aggregation.</p>

<ul>
  <li><strong>OpenAPI</strong> - I heavily use the OpenAPI specification to aggregate API definitions, and leverage JSON Schema to make connections and map API and data resources together.</li>
  <li><strong>APIs.json</strong> - After working on the data.json project with the White House, inventorying open data inventory across federal agencies, I developed an open specification for indexing APIs, and building collections, and other aggregate definitions for processing at discovery or runtime.</li>
  <li><strong>JSON Schema</strong> - All databases and data used as part of API operations possesses a JSON schema definition, that accompanies the OpenAPI that defines access to an API. JSON Schema provides the ability to define references across and between individual schema.</li>
  <li><strong>JSON Path</strong> - XPath for JSON, enabling the ability to analyze, transform and selectively extract data from API requests and responses.</li>
</ul>

<p>In an API utopia, everyone would use common API definitions, media types, and schema. In the real world, everyone does things their own way. API aggregation, and tools like OpenAPI, APIs.json, JSON Schema, and JSON Path allow us to standardize through machine readable definitions that connect the dots for us. API aggregation is still very much a manual process, or something offered by an existing SaaS provider, without much innovation–the tools are out there, we just need more examples we can emulate, and tooling to support.</p>

<p><strong>8. Please describe your experience and strategies for Lifecycle Management of APIs for increasing productivity and enabling VA to deliver high value APIs rapidly, on-boarding app developers and commercial off the shelf applications.</strong></p>

<p>I feel like this overlaps with question number 14, but is more focused on on-boarding, and client perspective of the API lifecycle. Question 14 feels like lifecycle optimization from an API provider perspective, and this is about efficiencies regarding consumption. Maybe I’m wrong, but I am a big believer in the ability of a central API portal, as well as federated portals, to increase API production, because they are delivering the resources consumers need, in a way that allows them to onboard and consume across many developers and commercial software platforms.</p>

<p>My experiences in fulfilling an API lifecycle from the consumer perspective always begins with a central portal, but one that posses all the required API management building blocks to not just deliver aPIs rapidly, incentivize consumption, and feedback in a way that evolves them throughout their life cycle:</p>

<ul>
  <li><strong>Portal</strong> - Have a central location to discover and work with all API resources, even if there is a federated network of portals that work together in concert across groups.</li>
  <li><strong>Getting Started</strong> - Providing clear, frictionless on-boarding for all API consumers in a consistent way across all API resources.</li>
  <li><strong>Authentication</strong> - Simple, pragmatic authentication with APIs, that are part of a larger IAM scaffolding.</li>
  <li><strong>Documentation</strong> - Up to date, API definition driven, interactive API documentation that demonstrates what an API does.</li>
  <li><strong>Code Resources</strong> - Code samples, libraries, and SDKs that make integration painless, and frictionless, allow applications to quickly move from development to production.</li>
  <li><strong>Support</strong> - Providing direct and indirect support services that funnel back through feedback loops into the product roadmap.</li>
  <li><strong>Communications</strong> - A regular drumbeat around API Platform communications, helping API consumers navigate discovery to integration, and keeps them informed regarding what is happening across the platform.</li>
  <li><strong>Road Map</strong> - A clear roadmap, as we as resulting change log that keeps API consumers in tune with whats next, reducing the gap that can exist between API provider and consumer.</li>
  <li><strong>Applications</strong> - A robust directory of certified applications, browser, platform, and 3rd party plugins and connectors, demonstrating what is possible via the platform.</li>
  <li><strong>Analysis</strong> - Logging, measuring, and analyzing API platform traffic, sign ups, logins, and other key data points, developing an awarness of what is working when it comes to API consumption, and quickly identifying where the friction is, and eliminating with future releases.</li>
</ul>

<p>Developer Experience (DX) is something that significantly speeds up the overall API lifecycle. Backend teams can efficiently define, deliver, and evolve their APIs without API consumers on-boarding, integrating, and providing feedback on what works, and what doesn’t work. A central API portal strategy for Liqhthouse API management is key to facilitating movement along the API lifecycle, reducing friction, eliminating road blocks, and reducing the chance an API will never full be realized in production.</p>

<p>Lighthouse API management is a central API platform portal, but could also be a forkable DX experience that can be emulated by other internal groups, adding federated edges to the Lighthouse API management platform. Providing a network of consistent API portals, across many different groups, which share a common on0boarding, authentication, documentation, support, communication, and analysis approach. Shifting the lifecycle beyond just a linear start to finish thing, and making something that works as a network of API nodes that all work together as a single platform.</p>

<p><strong>9. Please describe your experience and strategies for establishing effective governance of the APIs.</strong></p>

<p>I’ve been working with a variety of organizations around the topic of API governance for 2-3 years now, and with some recent advances I’m starting to see more API governance strategies mature, and become something that goes well beyond just API design.</p>

<p>API governance is something i’ve tuned into across a variety of enterprise, higher educational institutions, and government agencies. These are the areas I’m researching currently as part of my work as the API Evangelist.</p>

<ul>
  <li><strong>Design Guide</strong> - Establishing API design guide(s) for use across organizations.</li>
  <li><strong>Deployment</strong> - Ensuring there are machine readable definitions for every deployment pattern, and they are widely shared and implemented.</li>
  <li><strong>Management</strong> - Quantify what API management services, tooling, and best practices groups should be following, and putting to work in their API operations.</li>
  <li><strong>Testing</strong> - Defining the API testing surface area, and provide machine readable definitions for all test patterns, while leverage OpenAPI and other definitions as a map of the landscape.</li>
  <li><strong>Communication</strong> - Plan out the communication strategy as part of the API governance strategy.</li>
  <li><strong>Support</strong> - What support mechanisms are in place for governance, as well as definitions for best practices for supporting indvidiaul API implementations.</li>
  <li><strong>Deprecation</strong> - Define what deprecation looks like even before an API is defined or ever delivered, establishing best practices for deprecation of all resources.</li>
  <li><strong>Analysis</strong> - Measuring, analyzing, and reporting on API governance. Identifying where it is being applied effectively, and where the uncharted territory lies.</li>
  <li><strong>Coaches</strong> - Train and deploy API coaches who work with every group on establishing, evolving, incentivizing, and enforcing API governance across groups.</li>
  <li><strong>Rating</strong> - Establishing a set of rating system for quantifying how compliant APIs are when it comes to API governance, providing an easy to understand how close an API is, or isn’t.</li>
  <li><strong>Training</strong> - The development and execution of training around API governance, working with all external groups to help define API governance, and take active role in its implementation.</li>
</ul>

<p>Sadly, API governance isn’t something I’ve seen consistently applied across the enterprise, at institutions and government agencies. There is no standard for API governance. There are few case studies when it comes to API governance to learn from. Slowly I am seeing larger enterprises share their strategies, and seeing some universities publish papers on the topic. Providing some common building blocks we can organize into a coherent API governance strategy.</p>

<p><strong>10. Please describe your experience and methodologies for DevOps/RunOps processes across deployments. Highlight strategies for policy testing, versioning, debugging etc.</strong></p>

<p>API management is a living entity, and we focus on delivering API operations with flat teams who have access to the entire stack, from backend to understanding application and consumer needs. All aspects of the API life cycle embraces a microservices, continuous evolution pace, with Github playing a central from from define to deprecation.</p>

<ul>
  <li><strong>CI/CD</strong> - Shared repositories across all definition, code, documentation, and other modules that make up the lego pieces of API operations.</li>
  <li><strong>Testing &amp; Monitoring</strong> - Ensuring every building block has a machine readable set of assertions, tests, and other artificats that can be used to verify operational integrity.</li>
  <li><strong>Microservices</strong> - Distilling down everything to a service that will meet a platform objective, and benefit an API consumer in as small as package as possible.</li>
  <li><strong>Serverless</strong> - Leverage functions that reflect a microservices frame of mind, allowing much smaller unit of compute to be delivered by developers.</li>
  <li><strong>Versioning</strong> - Following a semantic versioning with version numbers reflecting a MAJOR.MINOR.PATCH, and kept in sync across backend, API, and client applications.</li>
  <li><strong>Dependencies</strong> - Scanning and assessing for vulnerabilities in libraries, 3rd party APIs, and other ways dependencies are established across architecture.</li>
  <li><strong>Security</strong> - Scanning and assessing security risk introduced by dependencies, and across code, ensuring that testing and monitoring reflects wider API security practices.</li>
</ul>

<p>A DevOps focus is maintained across as many stops along an API life cycle, and reflected in API governance practices. However, it is also recognized that a DevOps will not always be compatible with existing legacy practices, and custom approaches might be necessary to maintain specific backend resources, until their practices can be evolved, and brought in alignment with wider practices.</p>

<p><strong>11. Please describe your experience and strategies employed for analytics related to runtime management, performance monitoring, usage tracking, trend analysis.</strong></p>

<p>Logging and anaysis is a fundamental component of API management, feeding an overall awareness of how API are being consumer, which contributes to the product road map, security, and overall platform reliability. The entire API stack should be analyzed from the backend, to the furthest application endpoints, whenever possible.</p>

<ul>
  <li><strong>Network</strong> - Logging, and anaysis at the packet and network level, understanding where the network bottlenecks are.</li>
  <li><strong>Instance</strong> - Monitoring and measuring any server instance, whether it is bare metal, virtual, or containerized, providing a complete picture of how each instance in the API chain is performing.</li>
  <li><strong>Database</strong> - When access to the database layer is present, adding the logging, and other diagnostic information to the analysis stack, understand how backend databases are doing their job.</li>
  <li><strong>Serverless</strong> - Understanding the performance and usage of each function in the API supply chain, making sure each lego building block is accounted for.</li>
  <li><strong>Regional</strong> - Understanding how API resources deploy in various regions are performing, but also adding the extra dimension of measuring and monitoring APIs from a variety of geographic regions.</li>
  <li><strong>Management</strong> - Leveraging API plans at the API gateway, and understanding API consumption at the user, application, and individual resource level. Painting a picture of how APIs are performing across access tiers.</li>
  <li><strong>DNS</strong> - Logging, measuring, and runtime responsiveness at the DNS layer, understanding any setting, configuration, or other detail that may be creating friction at the DNS layer.</li>
  <li><strong>Client</strong> - Introducing logging, analysis, and tracking of errors at the SDK level, developing usage and performance reports from the production client vantage point.</li>
</ul>

<p>API management has matured over the last decade to give us a standard approach to managing API access at consumption time, optimizing usage, limiting bad behavior, and incentivizing healthy behavior. API monitoring and testing practices have evolved this perspective of the health, availability, and how APIs are being used from the client perspective. All of this information gets funneled into the road map, refining API gateway plans, rate limits, and other run time systems to adjust and support desired API usage.</p>

<p><strong>12. Please describe your experience with integrating MuleSoft with Enterprise Identity Management, Directory Services and implementing Role Based access.</strong></p>

<p>I do not have any experience in this area with Mulesoft specifically, but have worked in general with IAM and directory solutions in other platforms, and heavy usage on AWS, for securing the API gateway’s interaction with existing backend system.</p>

<p><strong>13. Please describe your experience with generating and documenting API’s and the type of standards they follow. Describe approaches taken for hosting these documentations and keeping them evergreen.</strong></p>

<p>API documentation is always a default aspect of the API platform, and delivered just like code as part of DevOps and CI/CD workflows using Github. With almost every stop along the API life cycle, all documentation is API definition driven, keeping things interactive, with the following elements always in play:</p>

<ul>
  <li><strong>Definitions</strong> - All documentation is driven using OpenAPI, API Blueprint, or RAML, acting as the central machine readable truth of what an API does, and what schema it uses. API definition driven API documentation contributes to them always being up to date, and reflect the freshest view of any API.</li>
  <li><strong>Interactive</strong> - All API documentation is API driven, allowing for the documentation to be interactive, acting as an explorer, and dynamic dashboard for playing with and understanding what an API does.</li>
  <li><strong>Modular</strong> - Following a microservices approach, all APIs should have small, modular, API definitions, that drive modular, meaningful, and useful API documentation.</li>
  <li><strong>Visualizations</strong> - Evolving on top of interactive documentation features, we are beginning to weave in more visual, dashboard, and reporting features directly into API document.</li>
  <li><strong>Github</strong> - Github plays a central role in the platform life cycle, with all aPI definitions and documentation running within Github repository, and integrated as part of CI/CD workflows, and DevOps frame of mind.</li>
</ul>

<p>All platform documentation is a living, breathing, element of the ecosystem. If should be versioned, evolved, and deployed along with other supporting microservice artifacts. Mulesoft has documentation to support this approach, as well as there being a suite of open source solutions we can consider to support a variety of different types of APIs.</p>

<p><strong>14. Please describe your proposed complete process lifecycle for publishing high quality, high value APIs with highest speed to market.</strong></p>

<p>Evolving beyond question number 8, and addressing the API provider side of the coin, I wanted to share a complete (enough) view of the life cycle from the API provider perspective. Addressing the needs of backend API teams, as well as the core API team when it comes to delivering usable, reliable, APIs that are active, and enjoy health release cycles. While not entirely a linear approach, here as many of the stops along our proposed API lifecycle, as applied to individual APIs, but applied consistently across the entire platform.</p>

<ul>
  <li><strong>Definitions</strong> - Every API starts as an API definition, which follows the API throughout its life, and drives every other stop along the way.</li>
  <li><strong>Design</strong> - Planning, applying, evolving, and measuring how API design is applied, taking the definition, and delivering as a consistent API that complies with API governance.</li>
  <li><strong>Virtualization</strong> - Delivery of mock, sandbox, and other virtualized instances of an API allowing for prototyping, testing, and playing around with an API in early stages, or throughout its life</li>
  <li><strong>Deployment</strong> - Deploying APIs using a common set of deployment patterns that drives API gateway behavior, connectivity with backend systems, IAM, and other consistent elements of how APIs will be delivered.</li>
  <li><strong>Management</strong> - The employment of common approaches to API management, some of which will be delivered and enforced by the API gateway, but also through other training, and sharing of best practices made available via API governance.</li>
  <li><strong>Testing &amp; Monitoring</strong> - Following DevOps, CI/CD workflows when it comes to developing tests, and other artifacts that can be used to monitor, test, and assert that APIs are meeting service level agreements, and in compliance with governance.</li>
  <li><strong>Security</strong> - Considering security at every stop along the API life cycle, scanning, testing, and baking in security best practices all along the way.</li>
  <li><strong>Client &amp; SDK</strong> - Establish common approaches to generating, delivering, and deploying client solutions that help API consumer get up and running using an API.</li>
  <li><strong>Discovery</strong> - Extending machine readable definitions to ensure an API is indexed and discoverable via service directories, documentation, and other catalogs of platform resources.</li>
  <li><strong>Support</strong> - Establishing POC, and support channels that are available as part of any single APIs operations, ensuring it has required support that meets the minimum bar for service operations.</li>
  <li><strong>Communications</strong> - Identifying any additions to an overall communication strategy around an APIs life cycle, announcing it exists on the blog, changes and releases are on the road map, and showcasing integrations via the platform Twitter account.</li>
  <li><strong>Road Map</strong> - Making sure there are road map, and change log entries for an API showing where it is going, and where it has been.</li>
  <li><strong>Deprecation</strong> - Lay out the deprecation strategy for an API as soon as it is born, setting expectations regarding when an API will eventually go away.</li>
</ul>

<p>This life cycle will play out over and over for each API published on the Lighthouse API platform. It will independently be executed by API teams for each API they produce, and replayed with each major and minor release of an API. A well defined, and well traveled API life cycle helps ensure consistency across teams, helping enforce compliance, familiarity, and reusability across APIs, no matter which team is behind the facade.</p>

<p><strong>15. Please describe your experience and approach towards establishing a 24x7 technical support team for the users, developers and other stakeholders of the platform.</strong></p>

<p>Support is an essential building block of any successful API platform, as well as a default aspect of every single APIs individual life cycle. We break API platform support into two separate distinct categories.</p>

<p>API support should begin with self-service support, encouraging self-sufficiency of API consumers, and minimizing the amount of platform resources needed to support operations:</p>

<ul>
  <li><strong>FAQ</strong> - Publishing a list of frequently asked questions as part of getting started and API documentation, keeping it an easy to access, up to date list of the most common questions API consumers face.</li>
  <li><strong>Knowledge Base</strong> - Publishing of a help directory or knowledge base of content that can help API consumers understand a platform, providing access to high level concepts, as well as API specific functionality, errors, and other aspects of integration.</li>
  <li><strong>Forum</strong> - Operate an active, moderated, and inclusive forum to assist with support, providing self-service answers to API consumers, which also allows them to asynchronously get access to answers to their questions.</li>
</ul>

<p>Beyond self-service support all API platforms should have multiple direct support channels available to API consumers:</p>

<ul>
  <li><strong>Monitoring</strong> - Providing a live, 3rd party status page that shows monitoring status across the platform, and individual APIs, including historical data.</li>
  <li><strong>Email</strong> - Allow API consumers to email someone and receive support.</li>
  <li><strong>Tickets</strong> - Leveraging a ticketing system, such as Zendesk, or other SaaS or possibly open source solution, allowing API consumers to submit private tickets, and move each ticket through a support process.</li>
  <li><strong>Issues</strong> - Leveraging Github issues for supporting each component of the API lifecycle, from API definition, to API code, documentation, and every other stop. Providing relevant issues and conversation going on around each of the moving parts.</li>
  <li><strong>SMS</strong> - SMS notifications regarding platform events, support tickets, platform monitoring, and other relevant areas of platform operations.</li>
  <li><strong>Twitter</strong> - Providing public support via a Twitter account, responding to all comments, and routing them to the proper support channel for further engagement.</li>
</ul>

<p>A combination of self-service and direct support channels allows for a resource starved core API team, as well as individual backend teams to manage many developers and applications, across many API resources in an efficient manner. Ensuring API consumers get the answers they are looking for, while ensuring relevant feedback and comments end up back in the product road map, with each appropriate product team.</p>

<p><strong>16. Please describe your experience in establishing metrics and measures for tracking the overall value and performance of the platform.</strong></p>

<p>This is somewhat overlapped with question #11, but I’d say focuses in on the heart of metrics and analysis at the API management level. Understand performance and reliability through the entire stack is critical to platform reliability, but the API management core is all about developing an awareness of how APIs are being consumed, and the value generated as part of this consumption. While performance definitely impacts value, we focus on API management to help us measure and understand consumption of each resource, and understand and measure the value delivered in consumption.</p>

<p>I’ve been studying how API management establishes the metrics needed, and measures and tracks API consumption, trying to understand value generation using the following elements:</p>

<ul>
  <li><strong>Application Keys</strong> - Every API call possess a unique key identifying the application and user behind consumption. This key is essential to understanding how value is being generated in the context of each API resources, as well as across groups of resources.</li>
  <li><strong>Active Users</strong> - New, no name, automated API users are easy to attack. It is the active, identifiable, communicative users we are shooting for, so quantifying and measuring what an active user is, and how many exist on the platform is essential to understanding value generation.</li>
  <li><strong>Active APIs</strong> - Which APIs are the APIs that consumers are integrating with, and using on a regular basis? Not all APIs will be a hit, and not all APIs will be transformative, but there can be utility, and functional APIs that make a significant impact. API management is key to understanding how APIs are being used, as well as how they are not being used, painting a complete picture of what is valuable, and what is not.</li>
  <li><strong>Consumption</strong> - Measuring the nuance of how API consumers are consuming APIs, beyond just the number of calls being made. What is being consumed? What is frequency of consumption? What are the limitations, constraints, and metrics for consumption, and incentives for increasing consumption?</li>
  <li><strong>Plans</strong> - Establishing many different access tiers and plans, containing many different APIs, allowing for API service composition to evolve create different API packages, or products that deliver across many different web, mobile, and device channels.</li>
  <li><strong>Design Patterns</strong> - Which design patterns are being used, and driving consumption and value creation. Are certain types of APIs favored, or specific types of parameters favored and backed up by API consumption numbers. Measuring successful API design, deployment, management, and testing patterns, and identifying the not so successful patterns, then incorporating these findings into the road map as well as platform governance.</li>
  <li><strong>Applications</strong> - Considering how metrics can be applied at the client, and SDK level, providing visibility and insight into how APIs are being consumed, from the perspective of each type of actual application being used.</li>
</ul>

<p>API management is the future of public data, content, and other resource management. Understanding API consumption, measuring, analysing, and reporting upon API consumption is required to being able to evolve healthy API resources forward, and develop viable applications, that are putting API resources to use. API management will continue to become the tax system, the royalty system, and value generation capture mechanism across government agencies and public resources in coming years.</p>

<p><strong>17. Please describe your experience and the approach you would take as the API Program Core Team to deploy an effective strategy that will allow VA to distribute the development of API’s across multiple teams and multiple contractor groups while reducing friction, risk and time to market.</strong></p>

<p>A microservice, CI/CID, DevOps approach to providing and consuming APIs has begun to shift the landscape for how API platforms are supporting the API life cycle across many disparate teams, trusted external partners, and even 3rd party application and system developers. Distilling all elements of the API supply chain to modular, well defined components, helps establish a platform that can be centralized, but encouraging a consistent way to approach the federated delivery of APIs and supporting resources.</p>

<p>I focus on making API operations more reusable, agile, and effective in a handful of areas:</p>

<ul>
  <li><strong>Github</strong> - Everything starts, lives, and is branched and committed to via Github, either as a public or private repository. Definitions, code, documentation, training, and all other resources exist as repositories allowing them to be versioned, forked, collaborated around, and delivered wherever they are needed.</li>
  <li><strong>Governance</strong> - Establishing API design, deployment, management, testing, monitoring, and other governance practices, then making them available across teams as living, collaborative documents and practices, and allow teams to remain independent, but working together in concert at scale, using a common set of health practices.</li>
  <li><strong>Training</strong> - Establishing a training aspect to platform operations, making sure materials is readily developed to support all stops along the API lifecycle, and the curriculum is circulated, evolved, and made available across teams. Ensuring that all training remains relevant, up to date, and versioned to keep in sync with all platform changes.</li>
  <li><strong>Coaching</strong> - Cultivate a team of API coaches who are embedded across teams, but also report back and worth with the central API team, helping train and educate around platform governance, but help work with and improve a teams ability to deliver within this framework. API coaches make API compliance a two way street, feeding resources downstream to teams, but then also working with teams to ensure their needs are being met, and reducing any friction that exists with delivering in a consistent way that meets the platform objectives.</li>
  <li><strong>Modular</strong> - Keeping everything small, modular, and limiting the time and budget needed to define, design, deploy, manage, version, and deprecate, allowing the entire platform to move forward in concert, using smaller release cycles.</li>
  <li><strong>Continuous</strong> - API governance, training, coaching, and all other operational level support across teams is also continuously being deployed, integrated, and evolved, ensuring that teams can work independently, but also be in sync when possible. Reducing bottlenecks, eliminating road blocks, and minimizing friction between teams, and across APIs that originate from many different systems.</li>
</ul>

<p>API operations needs to work much like each individual API. It needs to be small, modular, well defined, doing one thing well, but in a way that can be harnessed at scale to deliver on platform objectives. The API lifecycle needs to be well documented, with disparate teams well educated regarding what healthy API design, deployment, management, and testing looks like. We make operations reusable, and forkable across teams by establishing machine readable models and supporting tooling that help teams be successful independently, but in alignment with the greater platform good.</p>

<p><strong>18. Please describe the roles and skill sets that you would be assembling to establish the API Program core team.</strong></p>

<p>Our suggested team for the Lighthouse Core API management implementation reflects how a federated, yet centralized API platform team should work. There are a handful of disparate groups, each brining a different set of skills to the table. We are always working to augment and grow this to meet each projects we work on, and open to working with 3rd party developers. Bringing forward some skills we already have on staff, while also introducing some new skills to bring a well rounded set of voices to the table to support the Lighthouse API platform.</p>

<ul>
  <li><strong>Architects</strong> - Backend, platform, and distributed system architects.</li>
  <li><strong>Database</strong> - Variety of database administration, analysts, and scientists.</li>
  <li><strong>Veterans</strong> - Making sure the veteran perspective is representative on the platform team.</li>
  <li><strong>Healthcare</strong> - Opening the door to some healthcare practitioner advisor roles, making sure a diverse group of end users are represented.</li>
  <li><strong>Designers</strong> - Bringing forward the required API design talent required to deliver consistent APIs at web scale.</li>
  <li><strong>Developers</strong> - Provide a diverse set of developer skills supporting a variety of programming languages and platforms.</li>
  <li><strong>Security</strong> - Ensure there is necessary security operations skills to pay attention to the bigger security picture.</li>
  <li><strong>Testing</strong> - Provide dedicated testing and quality assurance talent to round off the set of skills on the table.</li>
  <li><strong>Evangelists</strong> - Make sure there is an appropriate amount of outreach, communication, and storytelling occurring.</li>
</ul>

<p>As the number of APIs grow, and an awareness of what consumers are desiring additional team members will be sought to address specific platform needs around emerging areas like machine learning, streaming and real time, or other aspects we haven’t addressed in this RFI.</p>

<p><strong>19. How would you recommend the Government manage performance through Service Level Agreements?  Do you have commercial SLAs established that can be leveraged?</strong></p>

<p>Each stop along the provider dimensions, as well as the consumer perspective will be quantified, logged, measured, and analyzed as part of API management practices. As part of this process, each individual area will have it’s set of metrics and questions that support API governance objectives. These are some of the areas that will be quantified, and measured, for inclusion in service level agreements.</p>

<ul>
  <li><strong>Database</strong> - What are the benchmarks for the database, and what are the acceptable ranges for operation. Log, measure, and evaluate historically, establishing a common scoring for delivery of database resources.</li>
  <li><strong>Server</strong> - Similar to the database layer. What are the units of compute available for API deployment and management, and what are the benchmarks for measuring the performance, availability, and reliability of each instance, no matter how big or small.</li>
  <li><strong>Serverless</strong> - Establishing benchmarks for individual functions, and defining appropriate scaling, service levels for each script beyond each API.</li>
  <li><strong>Gateway</strong> - Through gateway logging, measuring, and understanding performance and availability at the gateway level. Factoring in caching, rat limits, as well as backend dependencies, then establishing a level of service that can / should be met.</li>
  <li><strong>APIs</strong> - Through monitoring and testing each API should have its tests, assertions, and overall level of service defined. Is the API directly meeting its service level agreement? Is an API meeting API governance requirements, as well as meeting its SLA? What is the relationship between performance and compliance?</li>
</ul>

<p>Each stop along the API lifecycle should have its guidance when it comes to governance, and possess a set of data points for measuring monitoring, testing, and performance. All of this can be used to establish a comprehensive service level agreement definition that goes beyond just uptime and availability, ensuring that APIs are meeting the SLA, but are doing so through API governance practices that are working.</p>

<p><strong>20. If the Government required offerors to build APIs as part of a technical evaluation, what test environments and tools would be required?</strong></p>

<p>We’d be willing to build APIs in support of a technical evaluation. We are willing to do this using an existing API gateway solution, via any of the existing cloud platforms. We would need access to deploy mock and virtualized APIs using a gateway environment, as well as configure and work with IAM, and some sort of DNS layer, to access addressing and routing for all prototypes deployed.</p>

<p>Our team is capable of working in any environment, and suggest test environments and tool reflect existing operations, supporting what is already in motion. We can make recommendations, and bring our open cloud, SaaS, and open tooling to the table, and build a case for why another tool might be needed, and make more sense as part of the technical evaluation.</p>

<p>To facilitate the defining, and evolution of a technical evaluation we recommend starting a private Github repository, either initiated by the VA, or off error, where a set of requirements, and definitions can be established as a seed for the technical evaluation, as well as quickly become many seeds for actual platform implementation.</p>

<p><strong>In Conclusion</strong><br />
That was a title more work than I anticipated. I had scheduled about six hours to think through this, and go through my research to find relevant answers. When I started this response, I was planning on adding a whole other section for suggestions beyond the questions that were being asked, but I think they are able to get at more of the proposed strategy than I anticipated. There were some redundant areas of this RFI, as well as some foreshadowing regarding what would be possible with such an API, but I think the questions were able to get at what is needed.</p>

<p>I’m not sure how my responses will size up with my partners, or alongside the other responders to the RFI. My responses reflect what I see working across the wider public API space, and are based upon what I’ve learned opening up data, and trying to move forward other API projects across other federal agencies, all the way down to the municipal level. I’m guessing my approach will be heavier on the business and politics of API platform operations, and lighter on the reliance on the technical elements, addressing in my opinion the biggest obstacle to API success–culture, silos, and delivering consistently across disparate groups working under a common vision.</p>

<p>Next, I will publish this and share with my partners for inclusion in the final response for the group. I’ll make it available on API Evangelist, so that others can learn from the response to the RFI. I’m not sure where this will go next, but I can’t ever miss out on an opportunity to respond to inquiries about API operations at this level. This RFI was the most sophisticated API-focused vision to come out of VA in my experience. Regardless, of what happens next, it demonstrates to me that there are groups within the VA that understand the API potential, and what it can do for veterans, and the organizations, hospitals, and communities that support them.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/26/my-response-on-the-department-of-veterans-affairs-rfi-for-the-lighthouse-api-management-platform/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/25/we-are-all-using-apis/">We Are All Using APIs</a></h3>
        <span class="post-date">25 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist-logos/apis-are-all-around-us.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>When I talk to ordinary people about what I do as the API Evangelist, they tend to think APIs don’t have much of anything to do with their world. APIs exist in a realm of startups, technology, and make believe that doesn’t have much to do with their ordinary lives. When trying to make the connection with folks on airplanes, in the hotel lobby, and at the coffee shop, I always resort to the most common API-driven thing in all of our lives–the smart phone. Pulling out my iPhone is the quickest way I can go from zero to API understanding, with almost anyone.</p>

<p>When people ask what an API is, or how it has anything to do with them, I always pull out my iPhone, and say that all of the applications on the home page of your mobile phone use APIs to communicate. When you post something to your Facebook wall, you are using the Facebook API. When you publish an image to Instagram, you are using the Instagram API. When you check the balance on your bank account, you are using your banks API. APIs are everywhere. We are all using APIs. We are all impacted by good APIs, and bad APIs. Most of the time we just don’t know it, and are completely unaware of what is going on behind the curtain that is our smart phones.</p>

<p>I started paying attention to APIs in 2010 when I saw the impact mobile phones were beginning to have in our lives, and the role APIs were playing behind this new technological curtain. In 2017, I’m watching APIs expand to our homes via our thermostats, and other appliances. I’m seeing APIs in our cars. Applied to security cameras, sensors, signage, and other common objects throughout public spaces. APIs aren’t something everyone should be doing, however I feel they are something that everyone should be aware of. I usually compare it to the financial and banking system. Not everyone should understand how credit and banking systems work, but they should dam sure understanding who has access to their bank account, how much money they have, and other functional aspects of the financial system.</p>

<p>When it comes to API awareness I don’t expect you to be able to write code, or understand how OAuth works. However, you should know whether or not an online service you are using has an API or not, and you should understand whether or not OAuth is in use. Have you used OAuth? I’m pretty sure you have as part of your Facebook or Google logins, when you have authenticated with 3rd party applications, giving them access to your Facebook or Google data. You probably just didn’t know what you were using was OAuth, and that it was providing API access, as you clicked next, next, through the process. I’m not expecting you to understand the technical details, I am just injecting a little more awareness around the API-driven things you are already doing.</p>

<p>We are all using APIs. We are all being impacted by APIs existing, or not existing. We are being impacted by unsecured APIs (ie. Equifax). We are all being influenced, manipulated, and manipulated by bots who are using Twitter, Facebook, and other APIs to bombard us with information. On a daily basis we are being targeted with advertising that is personalizing, and surveilling us using APIs. We should all be aware that these things are happening, and have some ownership in understanding what is going on. I don’t expect you to become an API expert, or learn to hack, I’m just asking that you assume a little bit more accountability when it comes to understanding what goes on behind the digital curtain for the production of our online world.</p>

<p>I recently had the pleasure of engaging with my friend Bryan Mathers (<a href="https://twitter.com/BryanMMathers">@bryanmathers</a>) of <a href="https://visualthinkery.com/">Visual Thinkery</a>, and as I was telling this story, he doodled the image you see here. Giving me a digital representation of how I use my mobile phone to help people understand APIs. It gives me a visual anchor for telling this story over and over here on my blog. My readers who have heard it before can tune it out. These stories are for my new readers, and for me to link to when helping share this story with folks who are curious about APIs, and what I do. I feel it is important to help evangelize APIs, not because everybody should be doing them, it is because everyone should be aware that others are doing APIs. I’m stoked to have visual elements that I can use in this ongoing storytelling, that help connect my readers and listeners with APIs, using objects that are relevant and meaningful to their world. Thanks Bryan!!</p>

<p><strong>Photo Credit:</strong> Bryan Mathers (<a href="https://twitter.com/BryanMMathers">@bryanmathers</a>), of <a href="https://visualthinkery.com/">Visual Thinkery</a>.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/25/we-are-all-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/25/i-am-talking-about-jekyll-as-a-hypermedia-client-at-apistrat-in-portland-next-week/">I Am Talking About Jekyll As A Hypermedia Client At APIStrat in Portland OR Next Week</a></h3>
        <span class="post-date">25 Oct 2017</span>
        <p><a href="https://jekyllrb.com/"><img src="https://s3.amazonaws.com/kinlane-productions/jekyll/jekyll-logo.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>Static website, and headless CMS approaches to providing API driven solutions have grown in popularity in recent years. Jekyll has been leading the charge when it comes to static website deployment, partly due to Github being behind the project, and their adoption for <a href="https://pages.github.com/">Github Pages</a>. I’ve been pushing forward a new approach to <a href="http://apievangelist.com/2017/09/20/jekyll-as-a-hypermedia-client/">using Jekyll as a hypermedia client</a> to help deliver some of my API training and curriculum, and as part of this work I’m <a href="http://events.linuxfoundation.org/events/apistrat">giving a talk at APISTrat next week on the concept</a>. APIStrat is a great forum for this kind of project, helping me think through things in the form of a talk, the opportunity to share with an audience, and get immediate feedback on its viability, which I can then use to push forward my thinking on this aspect of my API work.</p>

<p>If you aren’t familiar with <a href="https://jekyllrb.com/">Jekyll</a>, I recommend spending some time reading about it, and even implementing a simple site using Github Pages. I have multiple non-developers in my life who I’ve introduced to Jekyll, and it has made a significant impact on the way they do their work. Even if you do know about Jekyll, additionally I recommend spending time learning more about the underscore data folder, and the <a href="https://jekyllrb.com/docs/collections/">concept of Jekyll collections</a>. Every Jekyll site has its default data collection, and the opportunity to create any additional collection, using any name you desire. Within these folders you can publish static CSV, JSON, and YAML data files, using any schema you desire. All of these data collections then become available for referencing through the static Jekyll website or application.</p>

<p>All of this functionality is native Jekyll. Nothing revelatory from me. What I’m looking to push things forward around is what happens when the data collections are using a hypermedia media type. I’m using <a href="https://github.com/kevinswiber/siren">Siren</a>, which allows me to publish structured data collections, complete with links that define a specific experience across the data and content stored within collections. Next piece of the puzzle involves leveraging Jekyll and its use of Liquid as a hypermedia client, allowing me to build resilient websites and applications that are data driven, but can also evolve and change without breaking. All I do is update the static data behind, with minor revisions to the client–all using Github’s API or Git infrastructure.</p>

<p>I’ve found Jekyll’s data features to be infinitely beneficial, and is something I use to run my entire network of sites on. Adding the hypermedia layer is allowing me to push forward the storytelling experience around my API research and curriculum. The first dimension I find interesting, is that I can publish updates to the data for each project without breaking the client–classic hypermedia benefits. However, additionally, I can publish data and changes using Github workflows, and because every application runs as independent Github Pages driven website, it can stay in sync with updates via commits, or stay completely independent and never receiving any changes, which will also introduce another dimension of client stability, which is also so hypermedia.</p>

<p>I am still in the beta phase of evolving some of my API projects to use this approach. I’m pioneering this work using my API design curriculum. I’ll be publishing an instance as part of<a href="http://events.linuxfoundation.org/events/apistrat">my session at APIStrat in Portland, OR next week</a>. I’ll make sure I cover some of the Jekyll and Github basics for people in the audience who aren’t familiar with these areas. Then I’ll walk through how I’m using the Jekyll collections, includes, and the Liquid syntax to behave as a hypermedia client for my Siren data collections. All my talks live on Github, so if you can’t make it you can still follow along with my work. <a href="http://events.linuxfoundation.org/events/apistrat">However, if you are going to be in Portland next week, make sure you come by and give a listen to my talk</a>–I’d love to get your feedback on the concept.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/25/i-am-talking-about-jekyll-as-a-hypermedia-client-at-apistrat-in-portland-next-week/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/25/helping-business-users-get-over-perceived-technical-gaps-when-it-comes-to-api-design/">Helping Business Users Get Over Perceived Technical Gaps When It Comes To API Design</a></h3>
        <span class="post-date">25 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/photos/cactus-flower.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Every single API project I’m working on currently has one or more business users involved, or specifically leading the work. With every business user, no matter how fearless they are, there is always a pretty heavy perception that some things are over their head. I see this over and over when it comes to API design, and the usage of OpenAPI to define an API. I’ve known a handful of folks who aren’t programmers, and have learned OpenAPI fluently, but for the most part, all business users tend to put up a barrier when it comes to learning OpenAPI–it lives in the realm of code, and exists beyond what they are capable of.</p>

<p>I get that folks are turned off by being exposed to code. Learning to read code takes a significant amount of time, and with the more framework, libraries, and other layers, you can find yourself pretty lost, pretty quickly. However, with OpenAPI, everything I do tends to be YAML, making it much more readable to humans. While there are rules and structure to things, I don’t feel it is out of the realm of the average user to study, learn, and eventually bring into focus. Along with the OpenAPI rules, there are a good deal of HTTP literacy required to fully understand what is going on, but I feel like the API design process is a much more forgiving environment to learn these things for both developers and business users.</p>

<p>I put the ownership of everything technical being complicated squarely on the shoulders of IT and developer folks. We’ve gone out of our way to make this stuff difficult to access, and out of reach of the average person. We’ve spent decades keeping people we didn’t see fit from having a seat at the table. However, increasingly I also feel like there is a significant amount of ownership to be given to business users who are willing to put up walls when there really is not reason. I think the API design layer is one place aspect of doing business with APIs where the average business user throws up unnecessary bluster around technical complexity, where it doesn’t actually exist. The definition of an API is simply a contract, no more complicated than a robust word document or spreadsheet, completely within the realm of understanding for any engaged user.</p>

<p>There is a pretty big opportunity to narrow the gap that exists between technical and business users with APIs. The challenge will be that many technologists want to keep things closed off to business users so they can charge for, and generate revenue when it comes to bridging the gap. We also have to figure out how to help business users better understand where the technical line is, and the importance that they become OpenAPI fluent, and have a stake in the API design conversation. Once I get back to my API training work, I’m going to think about a version of my API design curriculum, specifically for non-developer users. My goal is to figure out what it takes to onboard more business users to the world of APIs design, and make sure we balance out who is at the table when we are defining API solutions.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/25/helping-business-users-get-over-perceived-technical-gaps-when-it-comes-to-api-design/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/25/a-suite-of-human-services-api-definitions-using-openapi/">A Suite Of Human Services API Definitions Using OpenAPI</a></h3>
        <span class="post-date">25 Oct 2017</span>
        <p><img src="http://org.open.referral.adopta.agency/images/openreferral-expanded.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m needing to quantify some of the work that has occurred around <a href="http://org.open.referral.adopta.agency/">my Human Services Data Specification work</a> as part of a grant we received from Stanford. The grant shas helped us push forward almost three separate revisions of the API definition for working with human services data, and one of the things I’m needing to do is quantify the work that has occurred specifically around the OpenAPI definitions. At this point the specification is pretty verbose, and is now spanning multiple documents, making it difficult to quantify and share within an executive summary. To help support I wanted to craft some language that could help introduce the value that has been created to a non-technical audience.</p>

<p>The Human Services Data Specification (HSDS) provides the common schema for accessing, storing, and sharing of human data, providing a machine readable definition that human service practitioners can follow in their implementations. The Human Services Data API (HSDA) takes this schema, and provides an API definition for accessing, as well as adding, updating, and deleting data using a web API. While there are a growing number of code projects emerging that support HSDS/A, the center of the project is a set of OpenAPI definitions that outline the finer details of working with human services data via a web API.</p>

<p>With the assistance of the grant from Stanford, Open Referral was able to move forward the HSDA specification from version 1.0, to 1.1, 1.2, and now we are looking at delivering version 1.3 of the specification before end of 2017. The core OpenAPI definition for HSDA provides guidance for the design of human services APIs, with a focus on the core set of resources needed to operate a human services project. There are the handle of core resources defined as part of what is called HSDA core:</p>

<ul>
  <li><strong>Organizations</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda.yaml">OpenAPI Definition</a>) - Providing a framework for adding, updating, reading, and deleting organizational data. Describing the paths, parameters, and HSDS schema required to effectively communicate around organizations delivering human services.</li>
  <li><strong>Locations</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda.yaml">OpenAPI Definition</a>) - Providing a framework for adding, updating, reading, and deleting location data. Describing the paths, parameters, and HSDS schema required to effectively communicate around specific locations that provide human services.</li>
  <li><strong>Services</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda.yaml">OpenAPI Definition</a>) - Providing a framework for adding, updating, reading, and deleting services data. Describing the paths, parameters, and HSDS schema required to effectively communicate around specific human services.</li>
  <li><strong>Contacts</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda.yaml">OpenAPI Definition</a>) - Providing a framework for adding, updating, reading, and deleting contact data. Describing the paths, parameters, and HSDS schema required to effectively communicate around contacts associated with organizations, locations, and services delivering human services.</li>
</ul>

<p>This is considered to be the HSDA core specification, focusing on exactly what is needed to manage organizations, locations, services, and contacts involved with human services, and nothing more. As of version 1.2 of the specification we have spun off several additional specifications, which support the HSDA core specification, but are meant to reduce the complexity of the core specification. Here are seven additional projects we’ve managed to accomplish as part of our recent work:</p>

<ul>
  <li><strong>HSDA Search</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda-search.yaml">OpenAPI Definition</a>) - A machine readable API definition for searching across organizations, location, services, and contacts, as well as their sub-resources.</li>
  <li><strong>HSDA Bulk</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda-bulk.yaml">OpenAPI Definition</a>) - A machine readable API definition dictating how bulk operations around organizations, location, services, and contacts can occur, minimizing the impact of core systems.</li>
  <li><strong>HSDA Meta</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda-meta.yaml">OpenAPI Definition</a>) - A machine readable API definition describing all the meta data in use as part of any HSDA implementation, providing a history of everything that has occurred.</li>
  <li><strong>HSDA Taxonomy</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda-taxonomy.yaml">OpenAPI Definition</a>) - A machine readable API definition for managing one or more taxonomies in use, and how these taxonomies are applied to core organizations, locations, services, and contacts.</li>
  <li><strong>HSDA Management</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda-management.yaml">OpenAPI Definition</a>) - A machine readable API definition for the management level details of operating an HSDA implementation, providing guidance for user access, application management, service usage, and authentication and logging for the HSDA meta system.</li>
  <li><strong>HSDA Orchestration</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda-orchestration.yaml">OpenAPI Definition</a>) - A machine readable API definition for orchestration data exchanges between HSDA implementations, allowing for schedule or event based integrations to be executed, involving one or many HSDA implementations.</li>
  <li><strong>HSDA Validation</strong> (<a href="https://github.com/openreferral/api-specification/blob/master/_data/api-commons/openapi-hsda-utility.yaml">OpenAPI Definition</a>) - A machine readable API definition for validating an HSDA implementation follows the API and data schema, providing a validation API that can be used keep any platform compliant.</li>
</ul>

<p>The Stanford grant has allowed us to move forward HSDA three separate versions, but has also given us the time to properly break down HSDA into sensible, pragmatic services that do one thing, and does it well. The OpenAPI specifications provide a machine readable blueprint that can be used by any city, county, or other organization to define, implement, and manage their human services implementation, keeping it in sync with Open Referrals guidance. Each of the OpenAPI definitions provide a distilled down version of Open Referral guidance that anyone can follow within individual operations–designed for both business and technical users to adopt.</p>

<p>The HSDA specification is defined using <a href="https://www.openapis.org/">OpenAPI</a>, which is an open source specification format for defining web APIs, which is now part of the Linux Foundation. OpenAPI allows us to define HSDA using JSON Schema, but then also allows us to define how that schema is used as part of API requests and responses. It provides a YAML or JSON definition of the HSDA core, as well as each individual project. These definitions are then used to design, deploy, and manage individual API implementations, including providing documentation, code libraries, testing, and other essential aspects of operating an API that supports web, mobile, device, voice, spreadsheet, and other types of applications.</p>

<p>Hopefully this breaks down how the HSDA OpenAPI specifications are central to what Open Referral is doing. It provides the guidance that other human service providers can follow, going beyond just the data schema, and actually helping ensure access across many different implementations can work in concert. The HSDA OpenAPIs act as a set of rules that can guide individual implementations, while also ensuring their interoperability. When each human services provider adopts one of the HSDA specifications they are establishing a contract with their API consumers and application developers, promising they’ll deliver against this set of rules, ensuring their API will speak the same language as other human service APIs that also speak HSDA. This is the objective of Open Referral, and why we use OpenAPI as the central definition for everything we do.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/25/a-suite-of-human-services-api-definitions-using-openapi/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/24/some-new-api-evangelist-art/">Some New API Evangelist Art</a></h3>
        <span class="post-date">24 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist-logos/api-evangelist-butterfly-vertical.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>When I first started API Evangelist I spent two days trying to create a logo. I then spent another couple days trying to find a service to create something. Unhappy with everything I produced, I resorted to what I considered a temporary logo, where I just typed a JSON representation of the logo, mimicking what a JSON response for calling a logo through an API might look like. Seven years later, the logo has stuck, resulting in me never actually invested any more energy into my logo.</p>

<p>The API Evangelist imagery is long overdue for an overhaul. I stopped wearing the logo on my signature black t-shirts a couple years back, and I do not want to reach the 10 year mark before I actually do anything new. My logo, and the other artwork I’ve accumulated over the last several years played their role, but I’m stoked to finally begin the process of evolving some artwork for API Evangelist. To help me move the needle I’ve began working with my friend Bryan Mathers (<a href="https://twitter.com/BryanMMathers">@BryanMMathers</a>), where I have experienced <a href="https://visualthinkery.com/">his Visual Thinkery induced journey</a>, where he generates image ideas by engaging in a series of conversations. Producing some whimsical, colorful, and entertaining images for anything he can imagine out of our discussions. It is something I’ve experienced as part of our parent company <a href="http://contrafabulists.com/">Contrafabulists</a>, and stoked to experience as part of my API Evangelist work.</p>

<p>Bryan doodled the butterfly image while we were talking, and didn’t anticipate it would be the one I’d choose to be the logo. Honestly, at first I didn’t think it was logo quality, but after spending time looking and thinking about, I feel it suits what I’m doing very well. It still has the technical elements in the brackets in the wing outline, and the digital or pixel nature of the color in the wings, but is moving beyond the tech and represents the natural, more human side of things I would like to emphasize. The logo represents my API Evangelist ideas, and what I want them to be when I put them out there on my blog, and in my other work.</p>

<p>At first, a butterfly didn’t feel like Kin Lane, but I’m not creating a Kin Lane logo. I am creating an API Evangelist logo. When you think about the caterpillar to a butterfly evolution, and even the butterfly effect, it isn’t a stretch to apply all of this to what I work to do as the API Evangelist. I’m just looking to plant ideas in people’s heads with my work. I want my ideas to grow, expand, and take flight on their own. Making an impact on the world. I enjoy seeing my ideas fluttering around, adding color, adding motion, and presence around the API community. At the moment where I couldn’t imagine any image to represent API Evangelist, Bryan was able to extract a single image that I think couldn’t better represent what it is I do. #VisualThinkery</p>

<p>Bryan has created a horizontal, and vertical logo for me, using the butterfly. He’s also created a handful of stamps, and supporting images for my work. I will be sharing these other elements out as part of my storytelling, and see if I can find places for them to live more permanently somewhere on my network of site(s). Luckily, my website is pretty minimal, and the change of logo works well with it. I’m pretty happy with the change, and the introduction of color. Thanks Bryan for the work. I’m a little sad to retire my other logo, but it has run its course. I’ll still use it as a backdrop / background in some of my social profiles, and other work, as I think it reflects the history of API Evangelist. However, I am pretty stoked to finally have some new art for API Evangelist, that adds some new color to my work.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/24/some-new-api-evangelist-art/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/24/the-api-portal-outline-for-a-project-i-am-working-on/">The API Portal Outline For A Project I Am Working On</a></h3>
        <span class="post-date">24 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/portal/api-portal-forkable.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working through a project for a client, helping them deliver a portal for their API. As I do with any of my recommendations with my clients, I take my existing API research, and refine it to help craft a strategy to meets their specific needs. Each time I do this it gives me a chance to rethink some of my recommendations I’ve already gathered, as well as learn from new types of projects. I’ve taken the building blocks from my <a href="http://portal.apievangelist.com">API portal</a>, as well as my <a href="http://portal.apievangelist.com">API management</a> research, and have taken a crack at organizing them into an outline that I can use to guide my current project.</p>

<p>Here is a walk through of the outline I’m recommending as part of a basic API portal implementation, to support a simple public API:</p>

<ul>
  <li><strong>Overview</strong> - / - Everything starts with a landing page, with a simple overview of what an API does.</li>
</ul>

<p>Then you need some basics to help make on-boarding as frictionless as possible, providing everything an API consumer needs to get going:</p>

<ul>
  <li><strong>Getting Started</strong> - /getting-started/ - Handful of steps with exactly what is needed to get started.</li>
  <li><strong>Authentication</strong> - /authentication/ - An overview of what type of authentication is used.</li>
  <li><strong>Documentation</strong> - /documentation/ - Documentation for the APIs that are available.</li>
  <li><strong>FAQ</strong> - /faq/ - Answer the most common questions.</li>
  <li><strong>Code</strong> - /code/ - Provide code samples, libraries, and SDKs to get going.</li>
</ul>

<p>Then get API consumers signed up, or able to login and get at their API keys as quickly as you possibly can:</p>

<ul>
  <li><strong>Sign Up</strong> - /developer/ - Provide a simple sign up for a developer account.</li>
  <li><strong>Login</strong> - /developer/ - Allow users to quickly log back in after they have account.</li>
</ul>

<p>Next, provide a wealth of communication and support mechanisms, helping make sure developers are aware of what is going on:</p>

<ul>
  <li><strong>Blog</strong> - /blog/ - A simple blog dedicated to sharing stories about the API.</li>
  <li><strong>Support</strong> - /support/ - Offer up a handful of support channels like email and tickets.</li>
  <li><strong>Road Map</strong> - /road-map/ - Provide a road map of what the future will hold for the API.</li>
  <li><strong>Issues</strong> - /issues/ - Share what the known issues are for the API platform.</li>
  <li><strong>Change Log</strong> - /change-log/ - Publish a log of everything that has changed with the platform.</li>
  <li><strong>Status</strong> - /status/ - Provide a status page showing the availability of API services.</li>
  <li><strong>Security</strong> - /security/ - Publish an overview of what security practices are in place.</li>
</ul>

<p>Make sure your consumers know how to get involved at the right level, making plans, pricing, and partnership opportunities easy to find:</p>

<ul>
  <li><strong>Plans</strong> - /plans/ - Offer a single page with API access plans and pricing information.</li>
  <li><strong>Partners</strong> - /partners/ - Share what the partnership opportunities are, as well as existing partners.</li>
</ul>

<p>Then let’s take care of the legal side of things, making sure API consumers are fully aware of the TOS, and other aspects of operations:</p>

<ul>
  <li><strong>Terms of Service</strong> - /terms-of-service/ - Make the terms of service easy to find.</li>
  <li><strong>Privacy</strong> - /Privacy/ - Publish a privacy statement for all API consumers.</li>
  <li><strong>Licensing</strong> - /Licensing/ - Share the licensing for API, code, data, and other assets.</li>
</ul>

<p>I also wanted to make sure I took care of the basics for the developer accounts, flushing out the common building blocks developers will need to be successful:</p>

<ul>
  <li><strong>Dashboard</strong> - /developer/dashboard/ - Provide a simple, comprehensive dashboard for developers.</li>
  <li><strong>Account</strong> - /developer/account/ - Allow API consumers to change their account information.</li>
  <li><strong>Applications</strong> - /developer/applications/ - Allow API consumers to add multiple applications, and receive API keys for each application.</li>
  <li><strong>Plans</strong> - /developer/plans/ - If there are multiple plans, allow developers to change plans.</li>
  <li><strong>Usage</strong> - /developer/usage/ - Show history of all API usage for API consumers.</li>
  <li><strong>Billing</strong> - /developer/billing/ - Allow API consumers to add, update, and remove billing information.</li>
  <li><strong>Invoices</strong> - /developer/invoices/ - Share a listing, as well as detail of all invoices for use.</li>
</ul>

<p>Then I had a handful of other looser items that I wanted to make sure were here. Some of these will be for the second phase of developoment, but I want to make sure is on the list:</p>

<ul>
  <li><strong>Branding</strong> - /branding/ - Providing a logo, branding guidelines, and requirements.</li>
  <li><strong>Embeddable</strong> - /embeddable/ - Share a handful of butts and widgets for use by consumers.</li>
  <li><strong>Webhooks</strong> - /Webhooks/ - Publish details on how to setup and manage webhook notifications.</li>
  <li><strong>iPaaS</strong> - /ipaas/ - Information on Zapier, and other iPaaS integration solutions that are available.</li>
  <li><strong>Spreadsheets</strong> - /spreadsheets/ - Share any spreadsheet plugins or connectors that are available for integration.</li>
</ul>

<p>That concludes what I’d consider to be the essential building blocks for an API. Sure, there are some more minor APIs that can operate on a bare bones version of this list, but for any API looking to conduct business at scale, I’d recommend considering everything on this list. It reflects what I find across the leading providers like Stripe and Twilio, and reflect what I like to see from an API provider when I am getting up and running using any API.</p>

<p>I have Jekyll templates for each of these building blocks, which use the Bootstrap UI for the layout. I am updating it for this project, then I will publish again as a set of forkable tools that anyone can implement. I’m going to publish a new API portal that runs as an interactive outline of essential building blocks, then creates new Github repository for a user, and publishes each of the selected components to the repo. Providing a buffet of API developer portal tools anyone can put to work for their API without much work.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/24/the-api-portal-outline-for-a-project-i-am-working-on/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/24/aws-api-gateway-export-in-openapi-and-postman/">AWS API Gateway Export In OpenAPI and Postman Formats</a></h3>
        <span class="post-date">24 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/aws-api-gateway-export.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2017/10/19/importing-openapi-definition-to-create-api-with-aws-gateway/">I wrote about being able to import an OpenAPI into the AWS API Gateway to jumpstart your API the other day</a>. OpenAPI definitions are increasingly used for every stop along the API life cycle, and being able to import an OpenAPI to start a new API, or update an existing in your API gateway is a pretty important feature for streamlining API operations. OpenAPI is great for defining the surface area of deploying and managing your API, as well as potentially generate client SDKs, and interactive API documentation for your API developer portal.</p>

<p>Another important aspect of this API lifecycle is being able to get your definitions out in a machine readable format as well. All service providers should be making API definitions a two-way street, just like Amazon does with the AWS API Gateway. Using the AWS Console you can export an OpenAPI definition for any API. What makes things interesting is you can also export an OpenAPI complete with all the OpenAPI extensions they use to customize each API within the API Gateway. Also, they provide an export to OpenAPI, but with Postman specific extensions, allowing you to use use in the desktop client tooling when developing as well as integrating with any API.</p>

<p>I’ve said it before, and I’ll say it again. Every API service provider should allow for the importing and exporting of common API definition formats like OpenAPI and Postman. If you are selling services and tooling to API designers, developers, architects, and providers, you should ALWAYS provide a way for them to generate a static definition of what is going on–bonus, if you allow them to publish API definitions to Github. I know some API service providers like to allow for API import, but worry about customers abandoning their service if there is an export. Too bad, offer better services, and affordable pricing models, and people will stick around.</p>

<p>Beyond selling services to folks with day jobs, having the ability to import and export an OpenAPI allows me to play aorund with tools more. Using the AWS API Gateway I am setting up a number of APIs, then exporting them, and saving the blueprints for when I need to tell as part of a story, or use in a clients project. Having the ability to import and export an OpenAPI helps me better deliver actual APIs, but it also helps me tell better stories around what is possible with an API service or tool. If I can’t get in there and actually test drive, play with and see how things work, it is unlikely I will be telling stories about it, or have in my toolbox when I come across a project where I’ll need to apply a specific service or tool.</p>

<p>If you are having trouble making your API service or tool speak  OpenAPI, Postman, or other formats, make sure and check out <a href="https://apimatic.io/transformer">API Transformer</a>, which provides an API you can use to convert between the format. So if you can support one format, you can support them all. They even have the ability to take a .har file and generate an OpenAPI. I recommend natively supporting the importing and exporting of OpenAPI, then using API Transformer you can also allow for the importing and exporting in other formats. When you handle the import, you’ll always just make sure it speaks OpenAPI before you ingest. When a service supports OpenAPI import and export, it is much more likely I’m going to play with more, as well as increases the chance I’m going to be baking it into the life cycle for one of the projects I’m working on for a client. I’m sure other folks are thinking along the same lines, and are grateful when they can get their API definitions in and out of your platform.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/24/aws-api-gateway-export-in-openapi-and-postman/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/24/your-api-road-map-helps-others-tell-their-story-about-your-api/">Your API Road Map Helps Others Tell Stories About Your API</a></h3>
        <span class="post-date">24 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/los-angeles-downtow-freeway_blue_circuit_5.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>There are many reasons you want to have a road map for your API. It helps you communicate with your API community where you are going with your API. It also helps you have a plan in place for the future, which increases the chances you will be moving things forward in a predictable and stable way. When I’m reviewing and API I don’t see a public API road map available, I tend to give them a ding on the reliability and communication for their operations. One of the reasons we do APIs is to help us focus externally with our digital resources, which communication plays an important role, and when API providers aren’t communicating effectively with their community, there are almost always other issues right behind the scenes.</p>

<p>A road map for your API helps you plan, and think through how and what you will be releasing for the foreseeable future. Communicating this plan externally helps force you think about your road map in context of your consumers. Having a road map, and successfully communicating about it via a blog, on Twitter, and other channels helps keep your API consumers in tune with what you doing. In my opinion, an API road map is an essential building block for all API providers, because it has direct value on the health of API operations, but because it also provides an external sign of the overall health of a platform.</p>

<p>Beyond the direct value of having an API road map, there are other reasons for having one that will go beyond just your developer community. <a href="https://searchengineland.com/google-posts-can-now-automated-new-api-support-284842">In a story in Search Engine Land about Google Posts</a>, the author directly references the road map as part of their storytelling. <em>“In version 4.0 of the API, Google <a href="https://developers.google.com/my-business/content/change-log">noted</a> that “you can now create Posts on Google directly through the API.” The <a href="https://developers.google.com/my-business/content/change-log">changelog</a> include a bunch of other features, but the Google Posts is the most notable.”</em> Adding another significant dimension to the road map conversation, and helping out with SEO, and other API marketing efforts.</p>

<p>As you craft your road map you might not be thinking about the fact that people might be referencing it as part of stories about your platform. I wouldn’t sweat it too much, but you should at least make sure you are having a conversation about it with your team, and maybe add an editorial cycle to your road map publishing process. Making sure what you publish will speak to your API consumers, but also make for quotable nuggets that other folks can use when referencing what you are up to. This is all just a thought I am having as I’m monitoring the space, and reading about what other APIs are are up to. I find road maps to be a critical piece of the API communication, and support, and I depend on them to do what I do as the API Evangelist. I just wanted to let you know how important your API road map is, so you don’t forget to give it some energy on a regular basis.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/24/your-api-road-map-helps-others-tell-their-story-about-your-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/23/apis-reduce-everything-to-a-transaction/">APIs Reduce Everything To A Transaction</a></h3>
        <span class="post-date">23 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist-logos/reduce+everything+to+a+transaction_tr.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>My partner in crime Audrey Watters crafted a phrase that I use regularly, that “APIs reduce everything to a transaction”. She first said it jokingly a few years back, but is something I regularly repeat, and think about regularly, as I feel it profoundly describes the world I study. I like the phrase because of its dual meaning to me. If I say it with a straight face, in different company, I will get different responses. Some will be positive, and others will be negative. Which I think represents the world of APIs in a way that show how APIs are neither good, bad, or neutral.</p>

<p>If you are an API believer, when I describe how APIs reduce everything to a transaction, you probably see this as a positive. You are enabling something. You are distilling down aspects of our personal and business worlds down into a small enough representation, so that it can be transmitted via an API. Enabling payments, messages, likes, shares, and other aspects of our digital economy. Your work as an API believer is all about reducing things down to a transaction, so that you can make people’s lives better, and enable some kind of functionality that will deliver value online, and via our mobile phones. API transactions are enablers, and by using APIs, you are working to make the world a better place.</p>

<p>If you aren’t an API believer, when I describe how APIs reduce everything to a transaction, you are probably troubled, and left asking why I would want to do this. Not everything can or should be distilled down into a single transaction. Shifting something to be a transaction opens it up for being bought and sold. This is the nature of transactions. Even if you are delivering value to end-users by reducing a piece of their world to a transaction, now that it is a transaction, it is vulnerable to other market forces. It is these unintended consequences, and side effects of technology that many of us geeks do not anticipate, but market forces see as an opportunity, and benefit greatly from mindless technologists like us doing the hard work to transform the world into transactions that they can get their greedy little hands on.</p>

<p>Our desire to purchase a product is reduced to a transaction. Nothing bad here, right? Our conversations, images we take, videos we watch, and our location becomes a transaction. Getting a little scarier. Then our healthcare, education, and personal thoughts are turned into transactions. They are being bought and sold online, and via our mobile phones. What we buy, photos of our children, health records, and our most personal thoughts are all being reduced to a transaction so they can be transmitted via web and mobile applications. Secondarily, because it has been reduced to a transaction, it can be bought and sold on data markets, and become a transaction in the surveillance economy. Fulfilling the darker side of what we know as reducing everything to a transaction.</p>

<p>This is one of the reasons I’m fascinated with APIs. My technologist brain is attracted to the process of reducing things to a transaction, so they can be transmitted via APIs for use in web, mobile, and device applications. I’m obsessed with reducing everything to a transaction. Along this journey I’m also fascinated by our collective lack of awareness regarding how these transactions can be used for harm. I’m intrigued by our inability to think through this as technologists, and even become annoyed when it is pointed out. I’m fascinated by our lack of accountability as technologists when the transactions we are responsible for are used to harm people, exploit individuals, and are bought and sold on the open market like we are cattle. We seem unable to stop or slow what we have set in motion, and must keep reducing everything to a transaction at all cost.</p>

<p>Why do we need photos of our children to be transactions? Why do we need our DNA to be transactions? Why do we need every moment, every thought, every impulse in our day to be a digital transaction? Once anything is reduced to a transaction in our world, why do we then feel compelled to measure it? The steps we take? The calories we consume? The hours we sleep? These are all aspects of our lives that have been reduced to transactions. Why? For our benefit? To improve our life? Or is it just so that someone can sell us something, or even worse, just sell this little piece of us. This is why everything is being reduced to a transaction. The reasoning behind is always about making our lives better, and improving the quality of our daily life, but the real reason is always about distilling down a piece of who we are into something that can be bought and sold–transacted.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/23/apis-reduce-everything-to-a-transaction/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/23/budget-api-management-using-github/">Budget API Management Using Github</a></h3>
        <span class="post-date">23 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-oauth-api-management.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am always looking for the cheapest, easiest ways to get things done in the world of APIs. As a small business owner I’m always on the hunt for hacks to get done what I need, and hopefully make things easier for my users, while keeping things free, or at least minimally priced for my business. When it comes to my simplest of APIs, where I’m not looking to fully manage, but I do want anyone using them to authenticate, and pass in API keys, so that I can track on their use. In some cases I’m going to bill against this usage, but for the most part I just want to secure, and quantify their consumption.</p>

<p>The quickest and dirtiest way I will enable authentication for any API is using Github. First thing you do is create a Github OAuth application, which is available under settings for your Github user or organization. Then I add a JavaScript icon and login link, and then paste a PHP script at another location, where it will be handling the login. All you have to do is update the URLs in both scripts, and when someone clicks on icon, they’ll be authenticated and then dropped back on the original page with username, and valid OAuth token–which then at this point you have a validated user, and valid token.</p>

<p>In some situations I just require that the appid and appkey be passed in with each API call. I do not rate limit, or bill against usage. I am just looking to identify consumers. Other projects I’m actively billing against consumption, but I’m doing this by processing the web server logs for the API. Again, bare bones operations. For the next level up, in the login PHP script I’m actually adding a user to a specific plan I’ve setup for an API using the AWS API Gateway, and adding their key to the gateway. Now a user has access to APIs, and the platform has (limited) access to their Github account, and a way to identify all their API usage.</p>

<p>I use this approach to account authentication for reading and writing to the underlying Github repository I’m using for API developer portals, and other applications. I’m also using these accounts to access API resources I’m serving up across a variety of projects. The combination allows for some pretty easy, but powerful engagement with API consumers via Github. The best part is I can rely on Github security and accounts for this layer of my API management. Next, I like that I can extend the features of their account using the AWS API Gateway. Extending the API management capabilities for specific projects when I have higher needs.</p>

<p><a href="http://github.oauth.apievangelist.com/">You can find the scripts for this on Github</a>. If you don’t want to host the server side portion of this, I recommend checking out OAuth.io, as they have a simple SaaS version of this that will work beyond just Github. However, if you are looking for quick and dirty solution, this one should work just fine.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/23/budget-api-management-using-github/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/23/a-simple-api-with-aws-dynamodb-lambda-and-api-gateway/">A Simple API With AWS DynamoDB, Lambda, and API Gateway</a></h3>
        <span class="post-date">23 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/aws-dynamodb-lambda-api-gateway.png" align="right" width="45%" /></p>
<p>I’ve setup a few Lambda scripts from time to time, but haven’t had any dedicated project time to push forward API serverless concepts. Over the weekend I had a chance to deploy a couple of APIs using AWS DynamoDB, Lambda, and API Gateway, lighting up some of the serverless API possibilities in my brain. Like most areas of the tech sector, I think the term is dumb, and there is too much hype, but I think underneath there is some interesting possibilities, at least enough to keep me playing around with things.</p>

<p>Right now my primary API setup is Amazon Aurora (MysQL) backend, with API deployed on EC2, using Slim API framework in PHP. It is clean, simple, and gets the job done. I use 3Scale, or Github for the API management layer. This new approach simplifies some things for me, but definitely goes further down the AWS rabbit hole with the adoption of API Gateway and Lamdba, but also introduces some interesting enough benefits, that has me considering for use on some specific projects.</p>

<p><strong>Identity and Access Management (IAM) Role</strong>
The first thing you need to do to make the whole AWS thing work is setup a role using AWS IAM. I created a role just for this project, and added CloudWatchFullAccess, AmazonDynamoDBFullAccess, and AWSLambdaDynamoDBExecutionRole. I need this role to handle a bunch of management level things with the database, and logging. IAM is one of the missing aspects of hand crafting my APIs, and is why I am considering adopting on behalf of my customers, to help them get a handle on security.</p>

<p><strong>Simple API Database Backends Using AWS DynamoDB</strong>
I am a big fan of relational databases, mostly out of habit and experience. A client of mine is fluent in AWS DynamoDB, which is a simple NoSQL solution, so I felt compelled to ensure the backend database for their APIs spoke DynamoDB. It’s a pretty simple database, so I got to work creating an account table, and <a href="https://gist.github.com/kinlane/77d72d775dd423fa096193f836e21e4f">added a simple JSON object that contained 4 or 5 fields</a>, and fired up an index for the simple accounts database. The databases I’m creating are meant to track aspects of API management, so the tables won’t end up being too large, or have high performance requirements, regardless, DyanamoDB is a perfect backend for APIs, leaving me unsure why I don’t use the platform more often.</p>

<p><strong>Using Lambda Functions Behind The API</strong>
Instead of firing up an Amazon EC2 and hand crafting my API framework, I crafted a handful of serverless scripts in Node.js that will run as independent Lambda functions. I’m going to eventually need a whole bunch of functions, but to get me going with this new API I crafted four separate Lambda functions that I can use to drive the API:</p>

<ul>
  <li><a href="https://gist.github.com/kinlane/de34bdeaaddce336957db071773f6c1c"><strong>searchAccounts</strong></a> - Using the DynamoDB API scan method to query the table.</li>
  <li><a href="https://gist.github.com/kinlane/56880c9282d9e8537a9a5826535c1d79"><strong>addAccount</strong></a> - Using the DynamoDB API putItem method to add a record to the table.</li>
  <li><a href="https://gist.github.com/kinlane/d00a6698b637708d215a96d74b4ea070"><strong>updateAccount</strong></a> - Using the DynamoDB API udpateItem method to add a record to the table.</li>
  <li><a href="https://gist.github.com/kinlane/96685141e5616f3ce0d2ea5a6eac7ea9"><strong>deleteAccount</strong></a> - Using the DynamoDB API deleteItem method to add a record to the table.</li>
</ul>

<p>Using the AWS SDK, I’m simply making calls to the DynamoDB API to get all the work done. I’m fluent in JavaScript, but not well versed in using Node.js, but it doesn’t take much energy to understand what is going on. The serverless functions are pretty utilitarian, and all that is unique is the DynamoDB method to call, and the JSON that is being sent with each call. It is something that is pretty straightforward, and easily replicated for other APIs. I will keep developing functions for my API, but now I can at least handle the basic CRUD functionality around my new database.</p>

<p><strong>Publish An API Using AWS API Gateway</strong>
The last piece of the puzzle for this story, is the API. Each Lambda function accepts and returns JSON, which is technically an API, but there is no management layer, or RESTful infrastructure present. The AWS API Gateway gives me the ability to craft API paths, with accompanying GET, POST, PUT, DELETE, and other methods. For each method I add, I’m given four options for connecting to my backend, either via HTTP call, create just a mock API, leverage other AWS service, or connect to a Lambda function. I quickly wire up a GET, POST, PUT, and DELETE to each of my functions, and add my API to an AWS API Gateway plan, requiring API keys, and limiting who can access what.</p>

<p>I now have an accounts API which allows me to add, update, delete, and search for accounts using an API. My data is stored in DynamoDB, and served up via Lambda functions, through the API Gateway. It is secured. It is scalable. I can easily quantify what my database, functions, and gateway resource usage and costs will end up being. I get why folks are interested in serverless. It’s clean. It’s modular. It scales. It is very manageable. I don’t feel like it will be the answer for every API I need to deploy, but it does make sense for quickly deploying APIs for customers who are open to AWS, and need things to be secure, highly performant, and scalable.</p>

<p>A serverless approach definitely takes the sysadmin load off a little bit. Especially when you depend on DynamoDB for the backend. DynamoDB, Lambda, and API Gateway offer a pretty nice stack that can auto tune and scale itself. I’m going to fire up five separate APIs using this new approach, and setup some monitoring and testing to see how it delivers, and maybe get a handle on the costs associated with operating an API like this. I still need to attach a custom domain, get a handle on logging with AWS CloudWatch, and some of the other aspects of API management using AWS API Gateway. However, it provides me with a nice look into the serverless world, and how I can use it to deploy, and manage APIs, but also use APIs to manage a serverless approach by publishing functions using the Lambda API, keeping things in tune with my API definitions stored on Github.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/23/a-simple-api-with-aws-dynamodb-lambda-and-api-gateway/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/23/api-monetization-framework-introduced-by-aws-marketplace/">API Monetization Framework As Introduced By AWS Marketplace</a></h3>
        <span class="post-date">23 Oct 2017</span>
        <p><a href="https://s3.amazonaws.com/awsmp-loadforms/SaaS+Seller+Integration+Guide.pdf"><img src="https://s3.amazonaws.com/kinlane-productions/amazon/aws-marketplace-aws-saas-seller-integration-guide.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I am learning about <a href="https://aws.amazon.com/marketplace">the AWS Marketplace</a> through the lens of selling your API there, adding a new dimension to my <a href="http://monetization.apievangelist.com/">API monetization</a> and <a href="http://plans.apievangelist.com/">API plan</a> research. I’ve invested a significant amount of energy to try and standardize what I learn from studying the pricing and plans for the API operations of the leading API providers. As I do this work I regularly hear from folks who like to tell me how I’ll never be able to standardize and normalize this, and that it is too big of a challenge to distill down. I agree that things seem too big to tame at the current moment, but with API pioneers like AWS, who have been doing this stuff for a while, you can begin to see the future of how this stuff will play itself out.</p>

<p>Amazon set into motion a significant portion of how we think about monetizing our API resources. The pay for what you use model has been popularized by Amazon, and continue to dominate conversations around how we generate revenue around our valuable digital assets. AWS has some of the most sophisticated pricing structure around their API services, as well as very mature pricing calculators, and have created markets around their resources (ie. spot instances for compute). You can see these concepts playing out in the guidance they offer software developers in their <a href="https://s3.amazonaws.com/awsmp-loadforms/SaaS+Seller+Integration+Guide.pdf">AWS Marketplace Seller Guide</a>, which helps sellers modify their SaaS products to sell them through AWS Marketplace using two models: 1) metering, or 2) contract. When you list or application in the AWS Marketplace you must choose between one of these models, but both involve thinking critically about your monetization strategy, which includes your hard costs, as well as where the value will lie with your customers–striking the balance necessary to operate a viable API business.</p>

<p>According to the <a href="https://s3.amazonaws.com/awsmp-loadforms/SaaS+Seller+Integration+Guide.pdf">AWS Marketplace Seller Guide</a>, each SaaS application owner listing through AWS Marketplace has two options for listing and billing your software:</p>

<ul>
  <li>You can use the <strong>AWS Marketplace Metering Service</strong> to bill customers for SaaS application use.
AWS Marketplace Metering Service provides a consumption monetization model in which customers are
charged only for the number of resources they use in your application. The consumption model is similar to that for most AWS services. Customers pay as they go.</li>
  <li>You can use the <strong>AWS Contract Service</strong> to bill customers in advance for the use of your software.
AWS Marketplace Contract Service provides an entitlement monetization model in which customers pay
in advance for a certain amount of usage of your software. For example, you might sell your customer a certain amount of storage per month for a year, or a certain amount of end-user licenses for some amount of time.</li>
</ul>

<p>No matter which plan you choose to deliver your API resources within, you will have to have the nuts and bolts of your API operations defined as part of your overall API monetization strategy. Each plan you offer needs to be derived from the hard costs involved with operations, but reflect the needs of your consumers. AWS gives you a handful of common dimensions for thinking through which type of plan you go with, and quantifying how you will be monetizing your API solution, in these nine areas:</p>

<ul>
  <li><strong>Users</strong> – One AWS customer can represent an organization with many internal users. Your SaaS
application can meter for the number of users signed in or provisioned at a given hour. This category is appropriate for software in which a customer’s users connect to the software directly (for example, with customer-relationship management or business intelligence reporting).</li>
  <li><strong>Hosts</strong> – Any server, node, instance, endpoint, or other part of a computing system. This category is appropriate for software that monitors or scans many customer-owned instances (for example, with performance or security monitoring). Your application can meter for the number of hosts scanned or provisioned in a given hour.</li>
  <li><strong>Data</strong> – Storage or information, measured in MB, GB, or TB. This category is appropriate for software that manages stored data or processes data in batches. Your application can meter for the amount of data processed in a given hour or how much data is stored in a given hour.</li>
  <li><strong>Bandwidth</strong> – Your application can bill customers for an allocation of bandwidth that your application provides, measured in Mbps or Gbps. This category is appropriate for content distribution or network interfaces. Your application can meter for the amount of bandwidth provisioned for a given hour or the highest amount of bandwidth consumed in a given hour.</li>
  <li><strong>Request</strong> – Your application can bill customers for the number of requests they make. This category is appropriate for query-based or API-based solutions. Your application can meter for the number of requests made in a given hour.</li>
  <li><strong>Tiers</strong> – Your application can bill customers for a bundle of features or for providing a suite of dimensions below a certain threshold. This is sometimes referred to as a feature pack. For example, you can bundle multiple features into a single tier of service, such as up to 30 days of data retention, 100 GB of storage, and 50 users. Any usage below this threshold is assigned a lower price as the standard tier. Any usage above this threshold is charged a higher price as the professional tier. Tier is always represented as an amount of time within the tier. This category is appropriate for products with multiple dimensions or support components. Your application should meter for the current quantity of usage in the given tier. This could be a single metering record (1) for the currently selected tier or feature pack.</li>
  <li><strong>Units</strong> – Whereas each of the above is designed to be specific, the dimension of Unit is intended to be generic to permit greater flexibility in how you price your software. For example, an IoT product which integrates with device sensors can interpret dimension “Units” as “sensors”. Your application can also use units to make multiple dimensions available in a single product. For example, you could price by data and by hosts using Units as your dimension. With dimensions, any software product priced through the use of the Metering Service must specify either a single dimension or define up to eight dimensions, each with their own price.</li>
</ul>

<p>These dimensions reflect the majority of software services being sold out there today. Make sure you not get stuck thinking about one way of thinking, like just paying per API call. Think about how your different API plans might have one or more dimensions, beyond any single use case.</p>

<ul>
  <li><strong>Single Dimension</strong> - This is the simplest pricing option. Customers pay a single price per resource unit per hour, regardless of size or volume (for example, $0.014 per user per hour, or $0.070 per host per hour).</li>
  <li><strong>Multiple Dimensions</strong> – Use this pricing option for resources that vary by size or capacity. For example, for host monitoring, a different price could be set depending on the size of the host. Or, for user-based pricing, a different price could be set based on the type of user (admin, power user, and read-only user). Your service can be priced on up to eight dimensions. If you are using tier-based pricing, you should use one dimension for each tier.</li>
</ul>

<p>Alll of these dimensions reflect <a href="http://plans.apievangelist.com/#BuildingBlocks">the common building blocks of API plans and pricing</a> which I’ve been tracking on for a number of years. It’s based upon Amazon selling their own APIs, as well as watching their customers price and sell their resources. Their pricing guide goes well beyond just APIs, and consider how you can generate revenue from any type of SaaS, but the dimensions they provide a place to start for ALL API providers, whether you are looking to sell them in the AWS Marketplace or not. You can find <a href="http://plans.apievangelist.com/#BuildingBlocks">even more dimensions on my API plan research</a>, but what Amazon provides will work for about 75% of the use cases out there today, and I’m looking to get you thinking critically your API monetization and plans, not provide you with too many options.</p>

<p>There just aren’t too many examples like this available, when it comes to thinking through how to price your APIs. My friends over at <a href="https://algorithmia.com/">Algorithmia</a> have pushed the conversation forward some with their algorithmic API marketplace, but you just don’t see this level of maturity with the pricing of resources over at Azure, Google, or others yet. Amazon is the furthest along in this journey. They have the most experience, and the most data regarding what digital resources are worth, and how you can measure and meter access. I think it will take a number of years to mature, but I think by 2020 we will see more standardization in how we structure the pricing for the most common digital resources available online–even if it is just the APIs we are selling on Amazon.</p>

<p>There will always be an infinite number of ways to charge for your APIs, but for many of the digital commodities that have become staples, we’ll see one or two common approaches stick. We’ll see less innovation in how we price the most used APIs, because those with market share will dictate the model that gets adopted, and others will emulate just so they can get a piece of the pie. As other API resources continue to mature, becoming digital commodities, we’ll see their pricing structure stabilize, and standardize to fit into market frameworks like we see emerging on the AWS platform. It will take time, but we’ll begin to see machine readable templates governing API pricing and plans, allowing cross platform markets to flourish, as API consumers figure out they can make API usage more predictable, budget-able, and orchestrate-able. We aren’t there yet, but you can see hints of this API economy over at AWS within their marketplace approach.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/23/api-monetization-framework-introduced-by-aws-marketplace/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/20/api-management-dashboard-the-provider-view/">API Management Dashboard: The Provider View</a></h3>
        <span class="post-date">20 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/cargo-ship-on-sea_atari_missle.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m helping some clients think through their approach to API management. These projects have different needs, as well as different resources available to them, so I’m looking to distill things down to the essential components needed to get the job done. I’ve taken a look at the API consumer account basics as well as their usage, and next I want to consider the view of all of this from the API provider vantage point. For both of my current projects, I’m needing to think about the UI elements that deliver on API management elements from the API provider perspective.</p>

<p>To help me think though the UI elements needed for helping manage the essential elements of managing APIs I wanted to create a simple list of each screen that will be needed to get the job done. So far, I have the following X UI elements as part of my API management base:</p>

<ul>
  <li><strong>Creation</strong> - The landing page for account creation. Ideally, these are using OpenID / OAuth for major providers, eliminating the need for passwords.</li>
  <li><strong>Login</strong> - The page for logging back in after a user has registered. Ideally, these are using OpenID / OAuth for major providers, eliminating the need for passwords.</li>
  <li><strong>Dashboard</strong> - The landing page once an API provider is logged in – providing access to all aspects of API management.</li>
  <li><strong>APIs</strong> - A list of APIs, with detail pages for managing each individual API definition.</li>
  <li><strong>Plans</strong> - A list of API plans, with detail pages for managing each individual API plan definition.</li>
  <li><strong>Accounts</strong> - A list of API consumer accounts, with detail pages for managing each individual API consumer.</li>
  <li><strong>Usage</strong> -  - A list of API calls from logs, with tools for breaking down by API, plan, or consumer.</li>
  <li><strong>Invoices</strong> - A list of all invoices that have been generated for a specific time period, across specific consumers. With a detail page for seeing individual invoice details.</li>
</ul>

<p>There may be more of these added to my current projects. Things like forgot password, and other aspects, but this gives me this visibility into the API consumer account, and usage aspects of the API management I’m trying deliver. These are the basic features any API management provider delivers, and represent the default areas of control an API provider should have over API consumption. It doesn’t have to be pretty, and have all the bells and whistles, but it should give enough to help API providers understand what is going on in real-time, and over a variety of time periods.</p>

<p>My clients I refer to 3Scale, Tyk, Restlet and the other API management services that are available won’t have to reinvent the wheel here. It is my clients who are using AWS API Gateway, and developing seamless custom integrations I’m having to specify things at this level. My advice is to make use of existing providers whenever you can, but for this round of projects I’m putting AWS API Gateway to work, so I’m going to need to do a little more custom work on the API management front. When I come out of this I should have a suite of API definitions that can be imported into the AWS API Gateway, as well as some Jekyll page templates for delivering the UI functionality listed above. Augmenting the AWS API Gateway with essential API management features that should be present within any API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/20/api-management-dashboard-the-provider-view/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/20/sell-your-api-gateway-api-through-the-aws-marketplace/">Selling Your AWS API Gateway Driven API Through The AWS Marketplace</a></h3>
        <span class="post-date">20 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/cargo-ship-on-sea_diego_rivera1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am getting intimate with AWS API Gateway. Learning about what it does, and what it doesn’t do. The gateway brings a number of essential API management elements to the table, like issuing keys, establishing plans, and enforcing rate limits. However, it also lacks many of the other active elements of API management like billing for usage, which is an important aspect of managing API consumption for API providers. With AWS, things tend to work like legos, meaning many of their services work together to deliver a larger picture, and I’ve been learning more about how AWS API Gateway works with the AWS Marketplace to deliver some of the business of API features I’m looking for.</p>

<p><a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/sell-api-as-saas-on-aws-marketplace.html">Here is the blurb from the AWS API Gateway documentation regarding how you can setup AWS API Gateway to work with AWS Marketplace</a>, making your API available for sale as a SaaS service:</p>

<blockquote>
  <p>After you build, test, and deploy your API, you can package it in an API Gateway usage plan and sell the plan as a Software as a Service (SaaS) product through AWS Marketplace. API buyers subscribing to your product offering are billed by AWS Marketplace based on the number of requests made to the usage plan.<br /><br />
To sell your API on AWS Marketplace, you must set up the sales channel to integrate AWS Marketplace with API Gateway. Generally speaking, this involves listing your product on AWS Marketplace, setting up an IAM role with appropriate policies to allow API Gateway to send usage metrics to AWS Marketplace, associating an AWS Marketplace product with an API Gateway usage plan, and associating an AWS Marketplace buyer with an API Gateway API key. Details are discussed in the following sections.<br /><br />
To enable your customers to buy your product on AWS Marketplace, you must register your developer portal (an external application) with AWS Marketplace. The developer portal must handle the subscription requests that are redirected from the AWS Marketplace console.</p>
</blockquote>

<p>While it is not exactly the billing model I’m looking for in my API management layer currently, it provides a compelling look at selling your APIs in a marketplace setting. <a href="http://apievangelist.com/2017/09/08/cloud-marketplace-becoming-the-new-wholesale-api-discovery-platform/">There are a growing number of APIs available in the AWS marketplace</a>, but still less than a hundred from what I can tell. Smells like an opportunity for API providers to step up and publish their APIs. The linkage between AWS API Gateway usage plan and AWS Marketplace product makes a lot of sense when you think about having an APIs as a product focus for your organization. I will be writing more about this relationship in the future, as I think it reflects how we should be thinking about our API service composition, and how we craft our API usage plans.</p>

<p>I’m going to setup one of my simple, more utility style APIs, like screen capture, or proper noun search, and deploy using AWS API Gateway, then publish as an AWS Marketplace product. I want to have a clear view of what it takes, and what the on-boarding process looks like for a consumer. Another thing I’m interested in doing, is how can I also offer a more wholesale version of these APIs. Meaning, I’m not metering their usage with AWS API Gateway and billing via AWS Marketplace. It will be a separate product that gets deployed in their AWS infrastructure as an EC2, RDS, and maybe AWS API Gateway, where they aren’t billed by usage, and billed by the implementation. This is a model I’ve been watching, considering, and thinking about for some time, but only now getting to where the public cloud infrastructure is able to support this type of API deployment and management. It will be interesting to see what I can make work.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/20/sell-your-api-gateway-api-through-the-aws-marketplace/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/20/api-manageent-dashboard-the-consumer-view/">API Management Dashboard: The Consumer View</a></h3>
        <span class="post-date">20 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/cargo-ship-on-sea_constitution.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m helping some clients think through their approach to API management. These projects have different needs, as well as different resources available to them, so I’m looking to distill things down to the essential components needed to get the job done. I’ve taken a look at the API consumer account basics as well as their usage, and next I want to consider the view of all of this from the API provider vantage point. For both of my current projects, I’m needing to think about the UI elements that deliver on API management elements from both the API provider and consumer levels. I’ve already tackled the API provider view, next up is the API consumer view.</p>

<p>To help me think though the UI elements needed for helping manage the essential elements of managing API consumption for developers I wanted to create a simple list of each screen that will be needed to get the job done. So far, I have the following X UI elements as part of my API management base:</p>

<ul>
  <li><strong>Creation</strong> - The landing page for developer account creation. Ideally, these are using OpenID / OAuth for major providers, eliminating the need for passwords.</li>
  <li><strong>Login</strong> - The page for logging back in after a developer has registered. Ideally, these are using OpenID / OAuth for major providers, eliminating the need for passwords.</li>
  <li><strong>Dashboard</strong> - The landing page once an API consumer is logged in – providing access to all aspects of their API access.</li>
  <li><strong>Account</strong> - The ability to update developer account information.</li>
  <li><strong>Key(s)</strong> - The ability to get the master set, or multiple copies of API keys.</li>
  <li><strong>Plans</strong> - Viewing all available plans, with the ability to see which plan a consumer is part of and switch plans if relevant.</li>
  <li><strong>Usage</strong> - See history of all API consumption by API and time period.</li>
  <li><strong>Credit Card</strong> - The addition and updating of account credit card.</li>
  <li><strong>Billing</strong> - See history of all invoices for API consumption.</li>
</ul>

<p>There may be more of these added to my current projects. Things like forgot password, and other aspects, but this gives me this visibility into the API consumer account, and usage aspects of the API management I’m trying deliver. These are the basic features any API management provider delivers, and represent the default areas of control an API provider should have over API consumption. It doesn’t have to be pretty, and have all the bells and whistles, but it should give each API consumer what they need to manage their access, consumption, and understand what they’ve consumed and been billed for.</p>

<p>My clients I refer to 3Scale, Tyk, Restlet, and the other API management services that are available won’t have to reinvent the wheel here. It is my clients who are using AWS API Gateway, and developing seamless custom integrations which I’m having to specify things at this level. My advice is to make use of existing providers whenever you can, but for this round of projects I’m putting AWS API Gateway to work, so I’m going to need to do a little more custom work on the API management front. When I come out of this I should have a suite of API definitions that can be imported into the AWS API Gateway, as well as some Jekyll page templates for delivering the UI functionality listed above. Augmenting the AWS API Gateway with essential API management features that should be present within any coherent API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/20/api-manageent-dashboard-the-consumer-view/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/20/api-developer-account-usage-basics/">API Developer Account Usage Basics</a></h3>
        <span class="post-date">20 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/cargo-ship-on-sea_copper_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m helping some clients think through their approach to API management. These projects have different needs, as well as different resources available to them, so I’m looking to distill things down to the essential components needed to get the job done. <a href="http://apievangelist.com/2017/10/20/api-developer-account-basics/">I spent some time thinking through the developer account basics</a>, and now I want to break down the aspects of API consumption and usage around these APIs and developer accounts. I want to to think about the moving parts of how we measure, quantify, communicate, and invoice as part of the API management process.</p>

<p><strong>Having A Plan</strong>
We have developers signed up, with API keys that they’ll be required to pass in with each API call they make. The next portion of API management I want to map out for my clients is the understanding and management of how API consumers are using resources. One important concept that I find many API providers, and would be API providers don’t fully grasp, is service composition. Something that requires the presence of a variety of access tiers, or API plans, which define the business contract we are making with each API providers. API plans usually have these basic elements:</p>

<ul>
  <li><strong>plan id</strong> - A unique id for the plan.</li>
  <li><strong>plan name</strong> - A name for the plan.</li>
  <li><strong>plan description</strong> - A description for the plan.</li>
  <li><strong>plan limits</strong> - Details about limits of the plan.</li>
  <li><strong>plan timeframe</strong> - The timeframe for limits applied.</li>
</ul>

<p>There can be more than one plan, and each plan can provide different types of access to different APIs. There might be plans for new users versus verified ones, as well as possibly partners. The why and how of each plan will vary from API provider to provider, but their are all essential to managing API usage. Something that needs to be well defined, in place, with APIs and consumers organized into their respective tiers. Once this is done, we can begin thinking about the next layer, logging.</p>

<p><strong>Everything Is Logged</strong>
Each call made to the API contains API key which identify the consumer, who should always be operating within a specific plan. Each API call is being logged as part of the web server, and ideally the API management layer, providing timestamped entries for every drop of APIs consumed. These logs are used across API management operations, providing a history of all usage that will be evaluated on a per API, as well as per API consumer level. If a request and response isn’t available in the API logs, then it didn’t happen.</p>

<p><strong>Quantifying API Usage</strong>
Every log entry recorded will have the keys for a specific API consumer, as well as the path of the API they are consuming. When this usage is viewed through the lens of the API plan an API consumer is operating within, you have the ability to quantify usage, and place a value on overall consumption by any time frame. This is the primary objective of API management, quantifying who is accessing API resources, and how much they are consuming. This is how valuable API resources are being secured, and in many situations monetized, using simple web APIs.</p>

<p>API usage begins with quantifying what consumers have used, but then there are other dimensions that should be considered as well. For example, usage across API users for a single path, or group of paths. Also possibly usage across APIs and consumers within a particular plan. Then you can begin to look across time periods, regions, and other relevant information, providing a more complete picture of how APIs are being put to use. This awareness is why API providers are employing API management techniques, beyond what can be extracted from database logs, or just website or application analytics–providing a more wholesale view of how APIs are consumed.</p>

<p><strong>Invoicing For Consumption</strong>
Now that we have all API usage defined, and available through the lens of API plans, we have the option of invoicing for consumption. We know each call API consumers have made, and we have a rate specific for each API as part of the API plans each consumer is subscribed to. All we have to do is the math, and generate an invoice for the designated time period (ie. daily, weekly, monthly, quarterly). Invoicing doesn’t have to always be settled with cash, as consumption may be internally, with partners, and with a variety of public consumers. I’d also warn against thinking of consumption always costing the consumer, and sometimes it might be relevant to pay API some consumers for some of their usage–incentivizing a particular type of behavior that benefits a platform.</p>

<p>Measuring, quantifying, and accounting for API consumption is the primary reason companies, organizations, institutions, and government agencies are implementing API management layers. It is how Amazon generates revenue from their Amazon Web Services. It is how Stripe, Twilio, Sendgrid, and other rockstar API providers generate the revenue behind their operations. It is how government agencies are understanding how the public is putting valuable public resources to work. API management is something that is baked into cloud platforms, with a variety of service providers available as well, providing a wealth of solutions for API providers to choose from.</p>

<p>Next I will be looking at the API account and usage layers of API management from the view of API provider, as well as the API consumer via their developer area. Ideally, API management solutions are providing the dashboards needed for both sides of this equation, but in some of the projects I’m working on this isn’t available. There is no ready to go dashboard for API providers or consumers to look at when standing up an AWS API Gateway in front of an API, falling short when it comes to going the distance as an API management solution. You can define accounts, issue keys, establish plans and limit manage API consumption, but we need AWS CloudFront, and other services to deliver on API logging, authentication, and other common aspect of management–with API management dashboards being a custom thing when employing AWS for management. One consideration for why you might go with a more robust API management solution, beyond what an API gateway offers.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/20/api-developer-account-usage-basics/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/20/api-developer-account-basics/">API Developer Account Basics</a></h3>
        <span class="post-date">20 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/cargo-ship-on-sea_blue_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m helping some clients think through their approach to API management. These projects have different needs, as well as different resources available to them, so I’m looking to distill things down to the essential components needed to get the job done. The first element you need to manage API access is the ability for API consumers to be able to sign up for an account, that will be used to identify, measure usage, and engage with each API consumer.</p>

<p><strong>Starts With An Account</strong>
While each company may have more details associated with each account, each account will have these basics:</p>

<ul>
  <li><strong>account id</strong> - A unique identifier for each API account.</li>
  <li><strong>name</strong> - A first name and last name, or organization name.</li>
  <li><strong>email</strong> - A valid email address to communicate with each user.</li>
</ul>

<p>Depending on how we enable account creation and login, there might also be a password. However, if we use existing OpenID or OAuth implementations, like Github, Twitter, Google, or Facebook, this won’t be needed. We are relying on these authentication formats as the security layer, eliminating the need for yet another password. However, we still may need to store some sort of token identifying the user, adding these two possible elements:</p>

<ul>
  <li><strong>password</strong> - A password or phrase that is unique to each user.</li>
  <li><strong>token</strong> - An OAuth or other token issued by 3rd party provider.</li>
</ul>

<p>That provides us with the basics of each developer API developer account. It really isn’t anything different than a regular account for any online service. Where things start to shift a little specifically for APIs, is that we need some sort of keys for each account that is signing up for API access. The standard approach is to provide some sort of API key, and possibly a secondary secret to compliment it:</p>

<ul>
  <li><strong>api key</strong> - A token that can be passed with each API call.</li>
  <li><strong>api secret</strong> - A second token, that can be passed with each API call.</li>
</ul>

<p>Many API providers just automatically issue these API keys when each account is created, allowing consumers to possibly reset and regenerate at any point. These keys are more about identification than they are about security, as each key is passed along with each API call, and provide identification via API logging, which I’ll cover in a separate post. The only security keys deliver is that API calls are rejected if keys aren’t present.</p>

<p><strong>Allow For Multiple Applications</strong>
Other API providers will go beyond this base level of API account functionality, allowing API consumers to possibly generate multiple sets of keys, sometimes associated with one or many applications. This opens up the question of whether these keys should be associated with the account, or with one or many registered applications:</p>

<ul>
  <li><strong>app id</strong> - A unique id for the application.</li>
  <li><strong>app name</strong> - A name for the application.</li>
  <li><strong>app description</strong> - A description of the application.</li>
  <li><strong>api key</strong> - A token that can be passed with each API call.</li>
  <li><strong>api secret</strong> - A second token, that can be passed with each API call.</li>
</ul>

<p>Allowing for multiple applications isn’t something every API provider will need, but does reduce the need for API consumers to signup for multiple accounts, when they need an additional API key for a separate application down the road. This is something that API providers should be considering early on, reducing the need to make additional changes down the road when needed. If it isn’t a problem, there is no reason to introduce the complexity into the API management process.</p>

<p>This post doesn’t even touch on logging and usage. It is purely about establishing developer accounts, and providing what they’ll need to authenticate and identify themselves when consuming API resources. Next I’ll explore the usage, consumption, and billing side of the equation. I’m trying to keep things decoupled as much as I possibly can, as not every situation will need every element of API management. It helps me articulate all the moving parts of API management for my readers, and customers, and allows me to help them make sensible decisions, that they can afford.</p>

<p>Ideally, developer accounts are not a separate thing from any other website, web or mobile application accounts. If I had my way, API developer accounts would be baked into the account management tools for all applications by default. However, I do think things should be distilled down, standardized, and kept simple and modular for API providers to consider carefully, and think about deeply when pulling together their API strategy, separate from the rest of their operations. I’m taking the thoughts from this post and applying in one project I’m deploying on AWS, with another that will delivered by custom deploying a solution that leverages Github, and basic Apache web server logging. Keeping the approach standardized, but something I can do with a variety of different services, tools, and platforms.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/20/api-developer-account-basics/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/19/the-tractor-beam-of-the-database-in-an-api-world/">The Tractor Beam Of The Database In An API World</a></h3>
        <span class="post-date">19 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/dragon-shadows-black-white-outline.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m an old database person. I’ve been working with databases since my first job in 1987. Cobol. FoxPro. SQL Server. MySQL. I have had a production database in my charge accessible via the web since 1998. I understand how databases are the center of gravity when it comes to data. Something that hasn’t changed in an API driven world. This is something that will make microservices in a containerized landscape much harder than some developers will want to admit. The tractor beam of the database will not give up control to data so easily, either because of technical limitations, business constraints, or political gravity.</p>

<p>Databases are all about the storage and access to data. APIs are about access to data. Storage, and the control that surrounds it is what creates the tractor beam. Most of the reasons for control over the storage of data are not looking to do harm. Security. Privacy. Value. Quality. Availability. There are many reasons stewards of data want to control who can access data, and what they can do with it. However, once control over data is established, I find it often morphs and evolves in many ways, that can eventually become harmful to meaningful and beneficial access to data. Which is usually the goal behind doing APIs, but is often seen as a threat to the mission of data stewards, and results in a tractor beam that API related projects will find themselves caught up in, and difficult to ever break free of.</p>

<p>The most obvious representation of this tractor beam is that all data retrieved via an API usually comes from a central database. Also, all data generated or posted via an API, also ends up within a database. The central database always has an appetite for more data, whether scaled horizontally or vertically. Next, it is always difficult to break off subsets of data into separate API-driven project, or prevent newly established ones from being pulled in, and made part of existing database operations. Whether due to technical, business, or political reasons, many projects born outside this tractor beam will eventually be pulled into the orbit of legacy data operations. Keeping projects decoupled will always be difficult when your central databases has so much pull when it comes to how data is stored and accessed. This isn’t just a technical decoupling, this is a cultural one, that will be much more difficult to break from.</p>

<p>Honestly, if your database is over 2-3 years old, and enjoys any amount of complexity, budget scope, and dependency across your organization, I doubt you’ll ever be able to decouple it. I see folks creating these new data lakes, which act as reservoirs for any and all types of data gathered and generated across operations. These lakes provide valuable opportunities for API innovators to potentially develop new and interesting ways of putting data to work, if they possess an API layer. However, I still think the massive data warehouse and database will look to consume and integrated anything structured and meaningful that evolves on the shores. Industrial grade data operations will just industrialize any smaller utilities that emerge along the fringes of large organizations. Power structures have long developed around central data stores, and no amount of decoupling, decentralizing, or blockchaining will change this any time soon. You can see this with the cloud, which was meant to disrupt this, when it just moved it from your data center to the someone else’s, and allowed it to grow at a faster rate.</p>

<p>I feel like us API folks have been granted ODBC and JDBC leases for our API plantations, but rarely will we ever decouple ourselves from the mother ship. No matter what the technology whispers in our ears about what is possible, the business value, and political control over established databases will always dictate what is possible and what is not possible. I feel like this is one reason all the big database platforms have waited so long to provide native API features, and <a href="http://apievangelist.com/2017/10/05/big-data-is-not-abut-access-using-web-apis/">why next generation data streaming solutions rarely have simple, intuitive API layers</a>. I think we will continue to see the tractor beam of database culture continue to be aggressive, as well as passive aggressive to anything API, trumping access possibilities brought to the table by APIs, with outdated power and control beliefs rooted in how we store and control our data. These folks rarely understand they can be just as controlling and greedy with APIs, but they seem to be unable to get over the promises of access APIs afford, and refuse to play along at all, when it comes to turning down the volume on the tractor beam so anything can flourish.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/19/the-tractor-beam-of-the-database-in-an-api-world/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/19/adding-ping-events-to-my-webhooks-and-api-research/">Adding Ping Events To My Webhooks And API Research</a></h3>
        <span class="post-date">19 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-webhook-ping-events.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am adding another building block to my webhooks research <a href="https://developer.github.com/webhooks/">out of Github</a>. As I continue this work, it is clear that Gthub will continue to play a significant role in my <a href="https://webhooks.apievangelist.com/">webhook research</a> and storytelling, because they seem to be the most advanced when it comes to orchestration via API and webhooks. I’m guessing this is a by-product of continuous integration (CI) and continuous deployment (CD), which Github is at the heart of. The API platforms that have embraced automation and orchestration as part of what they do, always have the most advanced webhook implementations, and provide the best examples of webhooks in action, which we can all consider as part of our operations.</p>

<p><a href="https://developer.github.com/webhooks/#ping-event">Today’s webhook building block is the ping event</a>. “When you create a new webhook, we’ll send you a simple ping event to let you know you’ve set up the webhook correctly. This event isn’t stored so it isn’t retrievable via the Events API. You can trigger a ping again by calling the ping endpoint.” A pretty simple, but handy features when it comes to getting up and going with webhooks, making sure everything is working properly out of the gate–something that clearly comes from experience, and listening to the problems your consumers are encountering.</p>

<p>These types of subtle webhook features are exactly the types of building blocks I’m looking to aggregate as part of my research. As I do with other areas of my research, is at some point I will publish all of these into a single, (hopefully) coherent guide to webhooks. After going through the webhook implementations across the leading providers like Github, I should have a wealth of common patterns in use. Since webhooks aren’t any formal standard, it is yet another aspect of doing business with APIs we have to learn from the health practices already in use across the space. It helps to emulate providers like Github, because developers are pretty familiar with how Github works, when your webhooks behave in similar ways it reduces the cognitive load API consumers face when they are getting started.</p>

<p>One other thing to note in this story–<a href="https://developer.github.com/webhooks/#ping-event">my link to Github’s documentation goes directly to the section on webhook ping events</a>, because they use anchors for all titles and subtitles. This is something that makes storytelling around API operations soooooooooo much easier, and more precise. Please, please, please emulate this in your API operations. If I can directly link to something interesting within your API documentation, the chances are much greater I will tell a story, and publish a blog post about it. If I have to make a user search for whatever I’m talking about, I’m probably just gonna pass on it. One more trick for your toolbox, when it comes to getting me to tell more stories about what you are up to.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/19/adding-ping-events-to-my-webhooks-and-api-research/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/19/using-apis-to-enrich-the-data-you-have-in-spreadsheets/">Using APIs To Enrich The Data You Have In Spreadsheets</a></h3>
        <span class="post-date">19 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/blockspring/blockspring-bulk-connectors.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>As my friend John Sheehan over at <a href="http://apis.how/8nlsropidv">Runscope</a> says, “the spreadsheet is the most underrated API client”. The spreadsheet is where a significant amount of business gets done each day in the business world, so it make sense that we should be integrating APIs at this level whenever we possibly can. The best tool for doing this today is with <a href="https://www.blockspring.com/">Blockspring</a>, which provides non-developers (and developers) with the tools they need to integrate APIs into spreadsheets, either Microsoft Excel or Google Sheets–putting the power of APIs directly into the hands of average business folk.</p>

<p>Blockspring has over 100 APIs available for integration into your spreadsheets, but I wanted to highlight their recent release of <a href="https://open.blockspring.com/browse">bulk connectors</a>, which currently <a href="https://open.blockspring.com/browse">provides 10 separate data enrichment features from a handful of API providers</a>:</p>

<ul>
  <li><strong>Bing</strong> - Company Domain Lookups</li>
  <li><strong>Bing</strong> - Search Query Lookups</li>
  <li><strong>FullContact</strong> - Email Lookups</li>
  <li><strong>FullContact</strong> - Company Lookups</li>
  <li><strong>FullContact</strong> - Twitter Lookups</li>
  <li><strong>FullContact</strong> - Phone Lookups</li>
  <li><strong>Mailgun</strong> - Validate Emails</li>
  <li><strong>Clarifai</strong> - Deep Learning Image Tagging</li>
  <li><strong>Google</strong> - Shorten URLs</li>
  <li><strong>Google</strong> - Expand URLs</li>
</ul>

<p>These bulk connectors are meant to help you work with bulk data you have stored in spreadsheets and CSV files by enriching your data using these valuable API services. These connectors are all features I’ve custom developed as part of my internal systems, to help me monitor the world of APIs. They are simple, useful data management features, but instead of having to custom integrate with each API as I had to, anyone can use Blockspring to deliver these features within their own spreadsheets. Making the spreadsheet act as an API client, but for any average business user, not just for developers who are API savvy, and have the skills to deliver custom integration.</p>

<p>I’m a big advocate of companies publishing APIs. I also do a lot of pushing for API providers to make sure <a href="http://apievangelist.com/2017/09/25/embeddable-api-integrations-for-non-developers-with-zapier/">their APIs are available to non-developers using Zapier</a>. I consider Blockspring to be on this list of essential API services you should be working with as an API provider. This approach to consuming APIs will increasingly be how business gets done. As much as many of us developers would love for spreadsheets to go away, they ain’t going anywhere. Most normal folk in the world of business live and breathe within their spreadsheets, and if we want to deliver our API services to them, it will have to be through brokers like Blockspring. Just pause for a moment and think about the potential for delivering machine learning models via APIs within the spreadsheet like this, and you’ll begin to realize what an underserved aspect of API consumption spreadsheets are.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/19/using-apis-to-enrich-the-data-you-have-in-spreadsheets/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/19/importing-openapi-definition-to-create-api-with-aws-gateway/">Importing OpenAPI Definition To Create An API With AWS API Gateway</a></h3>
        <span class="post-date">19 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/api-gateway/aws-api-gateway-create-new-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been learning more about AWS API Gateway, and wanted to share some of what I’m learning with my readers. The AWS API Gateway is a robust way to deploy and manage an API on the AWS platform. The concept of an API gateway has been around for years, but the AWS approach reflects the commoditization of API deployment and management, making it a worthwhile cloud API service to understand in more depth. With the acquisition or all the original API management providers in recent years, as well as Apigee’s IPO, API management is now a default element of major cloud providers. Since AWS is the leading cloud provider, AWS API Gateway will play a significant role into the deployment and management of a growing number of APIs we see.</p>

<p>Using AWS API Gateway you can deploy a new API, or you can use it to manage an existing API–demonstrating the power of a gateway. What really makes AWS API Gateway reflect where things are going in the space, is the ability to import and define your API using OpenAPI. When you first begin with the new API wizard, you can upload or copy / paste your OpenAPI, defining the surface area of the API, no matter how you are wiring up the backend. OpenAPI is primarily associated with publishing API documentation because of the success of Swagger UI, and secondarily associated with generating SDKs and code samples. However, increasingly the OpenAPI specification is also being used to deploy and define aspects of API management, which is in alignment with the AWS API Gateway approach.</p>

<p>I have server side code that will take an OpenAPI and generate the server side code needed to work with the database, and handle requests and responses using the Slim API framework. I’ll keep doing this for many of my APIs, but for some of them I’m going to be adopting an AWS API Gateway approach to help standardize API deployment and management across the APIs I deliver. I have multiple clients right now who I am deploying, and helping them manage their API operations using AWS, so adopting AWS API Gateway makes sense. One client is already operating using AWS which dictated that I keep things on AWS, but the other client is brand new, looking for a host, which also makes AWS a good option for setting up, and then passing over control of their account and operations to their on-site manager.</p>

<p>Both of my current projects are using OpenAPI as the central definition for all stops along the API lifecycle. One API was new, and the other is about proxying an existing API. Importing the central OpenAPI definition for each project to AWS API Gateway worked well to get both projects going. Next, I am exploring the staging features of AWS gateway, and the ability to overwrite or merge the next iteration of the OpenAPI with an existing API, allowing me to evolve each API forward in coming months, as the central definition gets added to. Historically, I haven’t been a fan of API gateways, but I have to admit that the AWS API Gateway is somewhat changing my tune. Keep the service doing one thing, and doing it well, with OpenAPI as the definition, really fits with <a href="http://apievangelist.com/2017/10/16/the-basics-of-api-management/">my definition of where API management is headed as the API sector matures</a>.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/19/importing-openapi-definition-to-create-api-with-aws-gateway/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/18/most-api-developers-will-not-care-as-much-as-you-do/">Most API Developers Will Not Care As Much As You Do</a></h3>
        <span class="post-date">18 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/40_45_800_500_0_max_0_-5_-5.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I believe in the potential of what APIs can do, and care about learning how we can do things right. Part of it is my job, but part of it is me wanting to do things well. Master my approach to delivering APIs, using my well-rounded API toolbox. Reading the approach of other leading API providers, and honing my understanding of healthy, and not so healthy practices. I thoroughly enjoy studying what is going on and then applying it across what I do. However I am reminded regularly that most people are not interested in knowing, and doing things right–they just want things done.</p>

<p>As many of us discuss the finer details of API design, and the benefits of one approach over the other, other folks would rather us just point them to the solution that will work for them. They really don’t care about the details, or mastering the approach, they just want it to work in their situation. Whether it is the individual, the project or organization they are working in, the environment is just not conducive to learning, understanding, and growth. They are just interested in services and tools that can deliver the desired solution for as cheap as possible–free, and open source whenever available.</p>

<p>You can see this reality playing out across the space. An example is OpenAPI (fka Swagger). It has largely been successful because of Swagger UI. Most people think OpenAPI is all about documentation, with their understanding reflecting the solution they delivered, not the full benefits brought to the table as part of the process of implementing the specification. This is just one example of how folks across the API space are interested in solutions, rather than the journey. This is why many API programs will stagnate and fail, because folks do them thinking they’ll achieve some easier way of doing things, easy integrations, effortless innovation, or some other myth around API Valhalla.</p>

<p>I feel like much of this reality is set into motion through our education system. We don’t always teach people to learn, we tend to teach them job skills that we (companies) perceive are relevant today. Technology training tends to become about software solutions, brands, and platforms, and rarely concepts. Then I feel like this is baked into company operations because of the escalated pace introduced by startup funding, all the way up to the constant quest for profits of publicly traded companies. An incentive model that encourages folks to just do, not necessary do right. Achieve short term goals, quarterly and annual metrics, and ignore the technical debt we created–that will be someone elses problem.</p>

<p>Helping developers learn about APIs feels like our healthcare system sometimes to me. Developers are focused on fixing problems, treating symptoms, and living with good enough outcomes. Nothing preventative. No healthy diet or exercise. Just solving the problems that are in front of us. Looking for solutions to these immediate needs. I need API documentation. I need some sort of automated testing, or security scanner. I just need that API to be performant. I’m not interested in learning about the web, standards, or other things that would help me in my work. I just want this to work, so I can move on to the next problem. It’s a pretty big problem in the API space which I really don’t have any solution for. I’ll just keep learning, educating, and sharing with folks, but regularly reminding myself that not everyone will care about this stuff as much as I do, and that is just the way things are.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/18/most-api-developers-will-not-care-as-much-as-you-do/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/16/api-design-maturity-at-capital-one/">API Design Maturity At Capital One</a></h3>
        <span class="post-date">16 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-api-maturing-pyramid.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>API design is something that many have tried to quantify and measure, but very few ever establish any meaningful way of doing so properly in my experience. <a href="https://matthewreinbold.com/2017/10/03/API-Governance-Blueprint/">I’ve been learning about the approach to API governance from the Capital One DevExchange team</a>, and found their approach to defining API design maturity pretty interesting. I’m mostly interested in their approach because it speaks to actual business objectives, and aren’t about the common technical aspects we see API design being quantified across the community each day.</p>

<p>Capital One breaks things down into five distinctive layers that offer value to any organization doing APIs. Starting at the bottom of their maturity period, here are the levels of maturity they are measuring things by:</p>

<ul>
  <li><strong>Functional</strong> - Doing the basics, providing some low-level functionality, and nothing more.</li>
  <li><strong>Reliable</strong> - An API that is reliable, and scalable, beyond just basic functionality.</li>
  <li><strong>Intuitive</strong> - Where an investment in developer experience is made, further standardizing and streamlining what an API does.</li>
  <li><strong>Empowering</strong> - Where an API really begins to deliver value to an organization by being function, reliable, and intuitive, which all contributes significantly to operations.</li>
  <li><strong>Transformative</strong> - APIs that are game changer. Few APIs ever rise to this level, but all should aspire to this level of maturity.</li>
</ul>

<p>It provides a whole other lens for looking at API design through. Moving beyond just, is it RESTful? Hypermedia, GraphQL, gRPC, or other emerging approach. It also understands that not all APIs will be equal, and that we should be standardizing the value the design of our APIs deliver to our business operations. Forcing us to ask some pretty simple questions about the API design patterns we are using, and the actual value they bring to the table. Moving beyond API design being about technical details, and considering the business, and other political aspects of doing APIs.</p>

<p>The Capital One API design maturity definition also demonstrates another important aspect of all of this. That it takes work, time, and experience to properly design an API that will rise an empowering or transformative level. It can be easy to deliver functional APIs, but making them become reliable, and intuitively get the job done will take investment. I’m adding this definition to <a href="http://design.apievangelist.com">my API design research</a>, as well as my upcoming API governance research. I’m looking to evolve my definition of what API design looks like beyond just REST, and I’m wanting to move API governance beyond some IT led concept, and something that is in sync with business objectives, like I’m seeing from the Capital One team. It is easy to think of API design as mature once it enters production, but I’m thinking there is a lot more we should be considering before we consider our approach to API design mature.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/16/api-design-maturity-at-capital-one/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/16/adwords-api-release-and-sunset-schedule-for-2018/">AdWords API Release and Sunset Schedule For 2018</a></h3>
        <span class="post-date">16 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/beach-rocks-currents_kand_two.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>APIs are not forever, and eventually will go away. The trick with API deprecation is to communicate clearly, and regularly with API consumers, making sure they are prepared for the future. I’ve been tracking on the healthy, and not so healthy practices when it comes to <a href="http://deprecation.apievangelist.com/">API deprecation</a> for some time now, but felt like Google had some more examples I wanted to add to our toolbox. Their approach to setting expectations around API deprecation is worthy of emulating, and making common practice across industries.</p>

<p><a href="http://googleadsdeveloper.blogspot.com/2017/10/adwords-api-release-and-sunset-schedule.html">The Google Adwords API team is changing their release schedule</a>, which in turns impacts the number of APIs they’ll support, and how quickly they will be deprecating their APIs. They will be releasing new versions of the API three times a year, in February, June and September. They will also be only supporting two releases concurrently at all times, and three releases for a brief period of four weeks, pushing the pace of API deprecation alongside each release. I think that Google’s approach provides a nice blueprint that other API provides might consider adopting.</p>

<p>Adopting an API release and sunset schedule helps communicate the changes on the horizon, but it also provides a regular rhythm that API consumers can learn to depend on. You just know that there will be three releases a year, and you have a quantified amount of time to invest in evolving integration before any API is deprecated. It’s not just the communication around the roadmap, it is about establishing the schedule, and establishing an API release and sunset cadence that API consumers can be in sync with. Something that can go a lot further than just publishing a road map, and tweeting things out.</p>

<p>I’ll add this example to my API deprecation research.  Unfortunately the topic is one that is widely communicated around in the API space, but Google has long a strong player when it comes to finding healthy API deprecation examples to follow. I’m hoping to get to the point soon where I can publish a simple guide to API deprecation. Something API providers can follow when they are defining and deploying their APIs, and establish a regular API release and deprecation approach that API developers can depend on. It can be easy to get excited about launching a new API, and forget all about it’s release and deprecation cycles, so a little guidance goes a long way to helping API providers think about the bigger picture.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/16/adwords-api-release-and-sunset-schedule-for-2018/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/16/operating-your-api-portal-using-github/">Operating Your API Portal Using Github</a></h3>
        <span class="post-date">16 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/jekyll/jekyllrb.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>Operating on Github is natural for me, but I am regularly reminded what a foreign concept it is for some of the API providers I’m speaking with. Github is the cheapest, easiest way to launch a public or private developer portal for your API. With the introduction of <a href="https://pages.github.com/">Github Pages</a>, each Github repository is turned into a place to host any API related project. In my opinion, every API should begin with Github, providing a place to put your API definition, portal, and other elements of your API operations.</p>

<p>If you are just getting going with understand how Github can be used to support your API operations, I wanted to provide a simple checklist of the concepts at play, that will lead you being able to publish your API portal to Github.</p>

<ul>
  <li><strong>Github Account</strong> - You will need an account to be able to use Github. Anything you do on Github that is public will be free. You can do private portals on Github, but this story is about using it for a public API portal.</li>
  <li><strong>Github Organization</strong> - I recommend starting an organization for your API operations, instead of under just a single users account. Then you can make the definition for the API the first repository, and possibly the portal your second repository you create.</li>
  <li><strong>Github Repo</strong> - A Github repository is basically a folder on the platform which you can start the code, pages, and other content used as part of API operations.</li>
  <li><a href="https://pages.github.com/"><strong>Github Pages</strong></a> - Each Github repository has the ability to turn on a public project site, which can be used as a hosting location for a developer portal.</li>
  <li><a href="https://jekyllrb.com/"><strong>Jekyll</strong></a> - Github Pages allows any Github repository to become a website hosting location which you can access via your Github user account, or even provide an address using your own domain.</li>
</ul>

<p>I recommend every API provider think about hosting their API portal on Github. The learning curve isn’t that significant to get up and running, and if your portal is public, it is free. You can version control, and leverage other key aspects of Github for evolving and managing your API portal. There are a growing number of examples of <a href="https://apievangelist.com/2017/06/14/gsa-api-standards-with-working-prototype-api-and-portal/">forkable API portals like from the GSA</a>, or <a href="http://portal.apievangelist.com/2017/09/05/a-new-minimumviable-documentation-jekyll-template-for-apis/">an interesting minimum viable API documentation template</a> from my friend James Higginbotham (<a href="https://twitter.com/launchany">@launchany</a>). Demonstrating that the practice is growing, with the number of healthy examples to build on diversifying.</p>

<p>If you need help understanding how to use Github for hosting your API developer portal, feel free to reach out. I am happy to see where I can help. Another thing to note is that this approach to running a Jekyll static website isn’t limited to Github. You can always start the project there, and move off to any Jekyll enabled hosting provider. I run my entire network of websites and API project this way, leveraging Github as my plan A, and AWS as my plan B, with a server image ready to go when I need. Github just provides a number of bells and whistles that make it much more usable, as well as something others can collaborate around, enjoying the network effects that come with using the platform.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/16/operating-your-api-portal-using-github/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/16/the-basics-of-api-management/">The Basics Of API Management</a></h3>
        <span class="post-date">16 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/42_16_600_400_0_max_1_1_1-5.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am developing a basic API management strategy for one of my client’s API. With each area of their API strategy I am taking what I’ve learned monitoring the API sector, but pausing for a moment to think about again, and then applying to their operations. Over the years I have separated out many aspects of API management, distilling it down to a core set of elements that reflect the evolution of API management as its evolved into a digital commodity. It helps me to think through these aspects of API operations in general, but also applying to a specific API I am working on, helping me further refine my API strategy advice.</p>

<p>API management is the oldest area of my research. It has spawned every other area of the lifecycle I track on, but also is the most mature aspect of the API economy. This project I am working on gives me an opportunity to think about what is API management, and what should be spun off into separate areas of concern. I am looking to distill API management down to:</p>

<ul>
  <li><strong>Portal</strong> - A single URL to find out everything about an API, and get up and running working the resources that are available.</li>
  <li><strong>On-Boarding</strong> - Think just about how you get a new developer to from landing on the home page of the portal to making their first API call, and then an application in production.</li>
  <li><strong>Accounts</strong> - Allowing API consumers to sign up for an account, either for individual, or business access to API resources.</li>
  <li><strong>Applications</strong> - Enable each account holder to register one or many applications which will be putting API resources to use.</li>
  <li><strong>Authentication</strong> - Providing one, or multiple ways for API consumers to authenticate and get access to API resources.</li>
  <li><strong>Services</strong> - Defining which services are available across one or many API paths providing HTTP access to a variety of business services.</li>
  <li><strong>Logging</strong> - Every call to the API is logged via the API management layer, as well as the DNS, web server, file system, and database levels.</li>
  <li><strong>Analysis</strong> - Understanding how APIs are being consumed, and how applications are putting API resources to use, identifying patterns across all API consumption.</li>
  <li><strong>Usage</strong> - Quantifying usage across all accounts, and their applications, then reporting, billing, and reconciling usage with all API consumers.</li>
  <li><strong>APIs</strong> - API access to accounts, authentication, services, logging, analysis, and usage of API resources.</li>
</ul>

<p>There are other common elements bundled with API management, but this reflects the core of what API management is about–the business of APIs. Keeping track of who has access to what, and how much they are using. There are a number of other aspects of API management that many will consider under the API management umbrella, but I’ve elevated to being their own stops along the API lifecycle. Some areas are:</p>

<ul>
  <li><strong>Documentation</strong> - Static or interactive documentation for all available API paths, parameters, headers, and other details of the request and response surface area of the API.</li>
  <li><strong>Support</strong> - Self-service, or direct support channels that API consumers put to use to get help along the way.</li>
  <li><strong>SDKs</strong> - The SDKs, samples, libraries, and other supporting code elements for web, mobile, or other types of applications.</li>
  <li><strong>Road Map</strong> - Communicating what the future holds when it comes to the API.</li>
  <li><strong>Issues</strong>  - Notification of any open issues with the available of the API.</li>
  <li><strong>Change Log</strong> - A history of what has happened when it comes to changes to the API.</li>
</ul>

<p>These areas compliment API management, but should be approached beyond the day to day management aspects of API operations. I’d also consider authentication, logging, and analysis to be bigger than just about API management, as all three areas should cover more than just the API, but they are still very coupled to the core aspects of API management. In my definition, API management is very much about managing the consumption of resources, and not always the other aspects of API operations. This isn’t just my definition, it is what I’m seeing with the commodization of API management like we see over at Amazon Web Services.</p>

<p>AWS API Gateway is really about accounts, applications, authentication, and services. The logging, analysis are delivered by AWS CloudWatch. For this particular project I’m using Github and Jekyll as the portal, and custom delivering on-boarding, usage, and supporting APIs separately. Further narrowing down my definition of just what is API management. I’d say that AWS represents this evolution in API management well, with the decoupling of concerns between AWS API Gateway and AWS CloudWatch. If you apply <a href="https://aws.amazon.com/cognito/">AWS Cognito</a> to authentication, you can separate out another. I do not see any viable solution to handling usage, billing, and what used to be the business and monetization side of API management.</p>

<p>This API project I am working on is already using AWS for the backend of their operations, so I’m investing cycles into better understanding the moving parts of API management in context of the AWS platform. It makes sense to think some more about the decoupling of API management in context of AWS, since they are a major player in the commodization, and maturing of the concept. Once I’m done with AWS, I’m going to take another look at Google, then Azure, who are the other major players who are defining the future of API management.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/16/the-basics-of-api-management/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/12/bots-voice-and-conversational-apis-are-your-next-generation-of-api-clients/">Bots, Voice, And Conversational APIs Are Your Next Generation Of API Clients</a></h3>
        <span class="post-date">12 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-conversational-interfaces.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>Around 2010, the world of APIs began picking up speed with the introduction of the iPhone, and then Android mobile platforms. Web APIs had been used for delivering data and content to websites for almost a decade at that point, but their potential for delivering resources to mobile phones is what pushed APIs into the spotlight. The API management providers pushed the notion of being multi-channel, and being able to deliver to web and mobile clients, using a common stack of APIs. Seven years later, web and mobile are still the dominant clients for API resources, but we are seeing a next generation of clients begin to get more traction, which includes voice, bot, and other conversational interfaces.</p>

<p>If you deliver data and content to your customers via your website and mobile applications, the chance that you will also be delivering it to conversational interfaces, and the bots and assistants emerging via Alexa and Google Home, as well as on Slack, Facebook, Twitter, and other messaging platforms, is increasing. I’m not selling that everything will be done with virtual assistants, and voice commands in the near future, but as a client we will continue to see mainstream user adoption, and voice be used in automobiles, and other Internet connected devices emerging in our world. I am not a big fan of talking to devices, but I know many people who are.</p>

<p>I don’t think Siri, Alexa, and Google Home will live up to the hype, but there is enough resources being invested into these platforms, and the devices that they are enabling, that some of it will stick. In the cracks, interesting things will happen, and some conversational interfaces will evolve and become useful. In other cases, as a consumer, you won’t be able to avoid the conversational interfaces, and be required to engage with bots, and use voice enabled devices. This will push the need to have conversationally literate APIs that can deliver data to people in bite-size chunks. Sensors, cameras, drones, and other Internet-connected devices will increasingly be using APIs to do what they do, but voice, and other types of conversational interfaces will continue to evolve to become a common API client.</p>

<p>I am hoping at this point we begin to stop counting the different channels we deliver API data and content to. Despite many of the Alexa skills, and Slack bots I encounter being pretty yawn-worthy, I’m still keeping an eye on how APIs are being used by these platforms. Even if I don’t agree with all the uses of APIs, I still find the technical, business, and politics beyond them evolving worth tuning into. I tend to not emphasize to my clients that they work on voice or bot applications if they aren’t too far along their API journey, but I do make sure they understand one of the reasons they are doing APIs is to support a wide and evolving range of clients, and that at some point they’ll have to begin studying how voice, bots, and other conversational approaches will be a client they have to consider a little more in their overall strategy.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/12/bots-voice-and-conversational-apis-are-your-next-generation-of-api-clients/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/12/air-an-asthma-api/">Air, An Asthma API</a></h3>
        <span class="post-date">12 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/propeller/air-asthma-api-text-phone-aa3.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>You don’t find me showcasing specific APIs often. I’m usually talking about an API because of their approach to the technology, business, or politics of how they do APIs. It just isn’t my style to highlight APIs, unless I think they are interesting, and delivering value that is worth talking about, or possibly reflecting a meaningful trend that is going on. In this case it is a useful API that I think brings value, but also provides an example of an API I can showcase to non-developer folks as a meaningful example of an API.</p>

<p>The API I’m talking about today, is <a href="https://www.propellerhealth.com/air-by-propeller/">the Air API</a>, an asthma API from Propeller, which provides a set of free tools to help people understand current asthma conditions in their neighborhoods. The project is led by the Propeller data scientists and clinical researchers, looking to leverage Air API to help predict how asthma may be affected by local conditions, including a series of tools that share local asthma conditions, ranging from an email or text subscription, to an embeddable Air Widget for other websites.</p>

<p>The Air API provides an easy to explain example of what is possible with APIs. Environmental APIs will continue to be an important aspect of doing APIs. Aggregating sensor and other data to help us understand the air, water, weather, and other critical environmental factors that impact our lives each day. I like the idea of these APIs being open and available to 3rd party developers to build tools on top of them, while the platforms using them as a marketing vehicle for their other products and services, while making sure to keep the valuable data accessible to everyone.</p>

<p>I’ll put the Air API into my toolbox of APIs I use to help onboard folks with APIs. If they are impacted by asthma, or know someone who is, it helps make the personal connection, which can be important when on-boarding folks with the abstract concepts surrounding APIs. People tend to not care about technology until it makes an impact on their world. Which is one reason I think healthcare and environment APIs are going to play an important role in the sector for years to come. They provide a rich world of data, content, and algorithms, that can be exposed via APIs, and be applied in meaningful ways in people’s lives. Leaving a (hopefully) positive impression on folks about what APIs can do.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/12/air-an-asthma-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/12/everything-is-headless-in-an-api-world/">Everything Is Headless In A Decoupled API World</a></h3>
        <span class="post-date">12 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/catacombs_copper_circuit_2.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>It is always funny how long some concepts take to fully capture my attention. Sometimes I understand a concept on the surface, but never really invest the time into thinking deeply about how it actually fits into the big picture of my API research. One of these concepts is “headless”. Most commonly applied to the “headless CMS”. The wikipedia entry for headless CMS proclaims, “<a href="https://en.wikipedia.org/wiki/Headless_CMS">a Headless CMS is a back-end only content management system (CMS) built from the ground up as a content repository that makes content accessible via a RESTful API for display on any device.</a>”</p>

<p>A headless CMS is basically API-first, but instead of API being the first focus, the entire CMS, and administrative system for managing the content gets top billing, with API playing a critical supporting role. It’s what I am always prescribing as a way for API providers to consider the role between their applications, and their backend API resources. Decoupling your apps, from the backend resources. The administration interface for the content management system is one application, and each of your web, mobile, or other applications act as independent solutions. I see headless as just a business view of doing APIs, which makes sense when selling the concept to normals.</p>

<p>I’m adding a research area for headless that augments <a href="http://deployment.apievangelist.com/">my API deployment research</a>. Some of the open source implementations I’ve come across like <a href="https://getdirectus.com/">Directus</a> are pretty slick. It’s a pretty quick way to go from API to something that immediately benefits the average user in as short of time as possible. Headless is just another way to frame the the API conversation in the context of delivering an internal content management system, over getting 3rd party developers to build applications on top. It is something that is still possible because their is an API behind, but it is not the primary objective in any headless implementation.</p>

<p>While the main definition of headless is about having an API behind everything, I am also finding examples where Github is the backend, instead of just a RESTful API. <a href="http://prose.io/">Prose.io</a> which is actually the CMS I use for API Evangelist is considered a headless CMS, but Github acts my backend. Some of the headless CMS solutions I’ve used that depend on Github use Git as the connection, while others depend on the Github API for reading and writing data and content. This reflects how I use Github across my projects, beyond just Prose.io, except I am depending on web APIs, Github, as well as Google Sheets for the backend of Jekyll driven websites and applications.</p>

<p>I have been tracking on headless CMS stories for almost two years now, but after diving a little deeper this last week, it finally clicked for me. I feel headless is a good way to help the world move past WordPress, and embrace a more decoupled way of delivering websites, mobile, and other types of applications. I’m going to deploy Directus so that I can better understand headless as an approach to deploying APIs, and see about deploying basic demo implementations that I can point to. I’m hoping to use it as a way to introduce folks to APIs where public APIs is not the first objective, allowing them to get their feet wet with a simple API and website implementation. Helping deliver value without all the immediate risk, allowing folks to learn about how APIs can drive one or many applications in a more controlled internal environment.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/12/everything-is-headless-in-an-api-world/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/12/obfuscating-the-evolving-code-behind-my-api/">Obfuscating The Evolving Code Behind My API</a></h3>
        <span class="post-date">12 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/christianity-under-construction_copper_circuit.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m dialing in a set of machine learning APIs that I use to obfuscate and distort the images I use across my storytelling. The code is getting a little more hardened, but there is still so much work ahead when it comes to making sure it does exactly what I needed it to do, with only dials and controls I need–nothing more. I’m the only consumer of my APIs, which I use them daily, with updates made to the code along the way, evolving the request and response structures until they meet my needs. Eventually the APIs will be done (enough), and I’ll stop messing with them, but that will take a couple months more of pushing forward the code.</p>

<p>While the code for these APIs are far from finished, I find the API helps obfuscate and diffuse the unfinished nature of things. The API maintains a single set of paths, and I might still evolve  the number of parameters it accepts, and the fields it outputs, the overall will keep a pretty tight surface area despite the perpetually unfinished backend. I like this aspect of operating APIs, and how they can be used as a facade, allowing you to maintain one narrative on the front-end, even with another occurring behind behind the scenes. I feel like API facades really fit with my style of coding. I’m not really an A grade programmer, more a B- level one, but I know how to get things to work–if I have a polished facade, things look good.</p>

<p>Honestly, I’m pretty embarrassed by my wrappers for TensorFlow. I’m still figuring everything out, and finding new ways of manipulating and applying Tensor Flow models, so my code is rapidly changing, maturing, and not always complete. When it comes to the API interface I am focused on not introducing breaking changes, and maintaining a coherent request and response structure for the image manipulation APIs. Even though the code will at some point be stabilized, with updates becoming less frequent, the code will probably not see the light of day on Github, but the API might eventually be shared beyond just me, providing a simple, usable interface someone else can use.</p>

<p>I wouldn’t develop APIs like this outside a controlled group of consumers, but I find being both API provider and consumer pushes me to walk a line of unfinished and evolving backend, with a stable, simple API, without any major breaking changes. The fewer fixes I have to do to my API clients, the more successful I’ve been pushing forward the backend, while keeping the API in forward motion without the friction. I guess I just feel like the API makes for a nice wrapper for code, acting as shock absorbers for ups and downs of evolving code, and getting it to do what you need. Providing a nice facade that obfuscates the evolving code behind my API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/12/obfuscating-the-evolving-code-behind-my-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/11/provide-an-open-source-database-and-api-and-then-sell-the-data/">Provide An Open Source Threat Information Database And API Then Sell Premium Data Subscriptions</a></h3>
        <span class="post-date">11 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/old-door-lock_copper_circuit.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was doing some <a href="http://security.apievangelist.com">API security research</a> and stumbled across <a href="https://github.com/toolswatch/vFeed">vFeed</a>, a “<a href="https://github.com/toolswatch/vFeed">Correlated Vulnerability and Threat Intelligence Database Wrapper</a>”, providing a JSON API of vulnerabilities from the vFeed database. The approach is a Python API, and not a web API, but I think provides an interesting blueprint for open source APIs. What I found interesting (somewhat) from the vFeed approach was the fact they provide an open source API, and database, but if you want a production version of the database with all the threat intelligence you have to pay for it.</p>

<p>I would say their technical and business approach needs a significant amount of work, but I think there is a workable version of it in there. First, I would create a Python, PHP, Node.js, Java, Go, Ruby version of the API, making sure it is a web API. Next, remove the production restriction on the database, allowing anyone to deploy a working edition, just minus all the threat data. There is a lot of value in there being an open source set of threat intelligence sharing databases and API. Then after that, get smarter about having a variety different free and paid data subscriptions, not just a single database–leverage the API presence.</p>

<p>You could also get smarter about how the database and API enables companies to share their threat data, plugging it into a larger network, making some of it free, and some of it paid–with revenue share all around. There should be a suite of open source threat information sharing databases and APIs, and a federated network of API implementations. Complete with a wealth of open data for folks to tap into and learn from, but also with some revenue generating opportunities throughout the long tail, helping companies fund aspects of their API security operations. Budget shortfalls are a big contributor to security incidents, and some revenue generating activity would be positive.</p>

<p>So, not a perfect model, but enough food for thought to warrant a half-assed blog post like this. Smells like an opportunity for someone out there. Threat information sharing is just one dimension of my API security research where I’m looking to evolve the narrative around how APIs can contribute to security in general. However, there is also an opportunity for enabling the sharing of API related security information, using APIs. Maybe also generating of revenue along the way, helping feed the development of tooling like this, maybe funding individual implementations and threat information nodes, or possibly even fund more storytelling around the concept of API security as well. ;-)</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/11/provide-an-open-source-database-and-api-and-then-sell-the-data/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/11/their-security-practices-are-questionable-but-their-communication-is-unacceptable/">Their Security Practices Are Questionable But Their Communication Is Unacceptable</a></h3>
        <span class="post-date">11 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/status-berlin_propaganda_leaflets.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I study the API universe every day of the week, looking for common patterns in the way people are using technology. <a href="http://apievangelist.com/api-lifecycle/">I study almost 100 stops along the API lifecycle</a>, looking for healthy practices that companies, organizations, institutions, and government agencies can follow when dialing in their API operations. Along the way I am also looking for patterns that aren’t so healthy, which are contributing to many of the problems we see across the API sector, but more importantly the applications and devices that they are delivering valuable data, content, media, and algorithms to.</p>

<p><a href="http://security.apievangelist.com/">One layer of my research is centered around studying API security</a>, which also includes keeping up with <a href="http://vulnerabilities.apievangelist.com/">vulnerabilities</a> and <a href="http://breaches.apievangelist.com/">breaches</a>. I also pay attention to <a href="http://cybersecurity.apievangelist.com/">cybersecurity</a>, which is a more theatrical version of regular security, with more drama, hype, and storytelling. I’ve been reading everything I can on the Equifax, Accenture, and other scary breaches, and like the other areas of the industry I track on, I’m beginning to see some common patterns emerge. It is something that starts with the way we use (or don’t use) technology, but then is significantly amplified by the human side of things.</p>

<p>There are a number of common patterns that contribute to these breaches on the technical side, such as not enough monitoring, logging, and redundancy in security practices. However, there are also many common patterns emerging from the business approach by leadership during security incidents, and breaches. These companies security practices are questionable, but I’d say the thing that is the most unacceptable about all of these is the communication around these security events. I feel like they demonstrate just how dysfunctional things are behind the scenes at these companies, but also demonstrate their complete lack of respect and concern for individuals who are impacted by these incidents.</p>

<p>I am pretty shocked by seeing how little some companies are investing in API security. The lack of conversation from API providers about their security practices, or lack of, demonstrates how much work we still have to do in the API space. It is something that leaves me concerned, but willing to work with folks to help find the best path forward. However, when I see companies do all of this, and then do not tell people for months, or years after a security breach, and obfuscate, and bungle the response to an incident, I find it difficult to muster up any compassion for the situations these companies have put themselves in. Their security practices are questionable, but their communication around security breaches is unacceptable.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/11/their-security-practices-are-questionable-but-their-communication-is-unacceptable/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/11/a-grpc-meetup-kit/">The gRPC Meetup Kit</a></h3>
        <span class="post-date">11 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/grpc/grpc-logo.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2017/09/29/tyk-is-conducting-api-surgery-meetups/">I wrote about Tyk’s API surgery meetups last week</a>, and adding a new approach to our API event and workshop toolbox, and next I wanted to highlight <a href="https://github.com/grpc-ecosystem/meetup-kit">the gRPC Meetup Kit</a>, a resource for creating your own gRPC event. gRPC is an approach out of Google for designing, delivering, and operating high performance APIs. If you look at the latest wave of APIs out of Google you’ll see they are all REST and/or gRPC. Most of them are dual speed, providing both REST and gRPC. gRPC is an open source initiative, but very much a Google led effort that we’ve seen picking up momentum in 2017.</p>

<p>While I am keeping an eye on gRPC itself, this particular story is about the concept of providing a Meetup kit for your API related service or tooling, providing an “In a Box” solution that anyone can use to hold a Meetup. The gRPC team provides three groups of resources:</p>

<h3 id="grpc-101-presentation">gRPC 101 Presentation</h3>
<ul>
  <li><a href="https://www.youtube.com/watch?v=UVsIfSfS6I4">Talk</a> - A 15 minute course introduction video.</li>
  <li><a href="https://docs.google.com/presentation/d/1dgI09a-_4dwBMLyqfwchvS6iXtbcISQPLAXL6gSYOcc/edit?usp=sharing">Slides</a> - Slides that go along with the talk.</li>
  <li><a href="https://codelabs.developers.google.com/codelabs/cloud-grpc/index.html">Codelab</a> - A 45m codelab that attendees can complete using <a href="https://cloud.google.com/shell/">Cloud Shell</a>.</li>
</ul>

<h2 id="resources-and-community">Resources and community</h2>
<ul>
  <li><a href="http://www.grpc.io/"><strong>gRPC Website</strong></a></li>
  <li><strong>Codelab</strong>
    <ul>
      <li><a href="https://codelabs.developers.google.com/codelabs/cloud-grpc/index.html">Building a gRPC service with Node.js</a></li>
      <li><a href="https://codelabs.developers.google.com/codelabs/cloud-grpc-csharp/index.html">Building a gRPC service with C#</a></li>
    </ul>
  </li>
  <li><a href="https://github.com/grpc"><strong>GitHub Source</strong></a>
    <ul>
      <li><a href="https://github.com/grpc-ecosystem">Extended gRPC Ecosystem</a></li>
      <li><a href="http://www.grpc.io/blog/">Blog</a></li>
      <li><a href="https://www.youtube.com/channel/UCrnk1HWelWnYtF78YZX80fg">Youtube Channel</a></li>
    </ul>
  </li>
  <li><strong>Ask Questions</strong>
    <ul>
      <li><a href="https://gitter.im/grpc/grpc">Gitter Chat</a></li>
      <li><a href="https://groups.google.com/forum/#!forum/grpc-io">Google Group</a></li>
      <li><a href="http://stackoverflow.com/tags/grpc/">Stack Overflow</a></li>
    </ul>
  </li>
  <li><strong>Keep in Touch</strong>
    <ul>
      <li><a href="https://twitter.com/grpcio">Twitter</a></li>
    </ul>
  </li>
</ul>

<h2 id="request-support-for-your-event">Request Support for Your Event</h2>
<ul>
  <li><a href="https://goo.gl/forms/C3TCtFdobz4ippty2">gRPC Stickers</a></li>
  <li><a href="https://goo.gl/forms/pvxNwWExr5ApbNst2">Sign up for office hours with gRPC team</a></li>
</ul>

<p>It provides a nice blueprint for what is needed when crafting your own Meetup Kit as well as some material you could weave into any other type of Meetup, or workshop that might contain gRPC. Maybe an API design and protocol workshop, where you cover all of the existing approaches out there today like REST, Hypermedia, gRPC, GraphQL, and others. If nothing else the gRPC Meetup Kit provides a nice forkable project, that you could use as scaffolding for your own kit.</p>

<p>As I mentioned in my piece about Tyk, I don’t think Meetups are going anywhere as a tool for API providers, and API service providers to reach a developer audience. However, I think we are going to have to get more creative about how we organize, produce, and incentivize others to put them on. They are a great vehicle for brining together folks in a community to learn about technology, but we have to make sure they are delivering value for people who show up. I am guessing that a little planning, and evolving a toolkit using Github is a good way to approach putting on Meetups, workshops, and other small events around your products, services, and tooling.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/11/a-grpc-meetup-kit/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/11/the-api-coaches-at-capital-one/">The API Coaches At Capital One</a></h3>
        <span class="post-date">11 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/21586757_10155715320589813_1876210064026688571_o.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>API evangelism and even advocacy at many organizations has always been a challenge to introduce, because many groups aren’t really well versed in the discipline, and often times it tends to take on a more marketing or even sales like approach, which can hurt its impact. I’ve worked with groups to rebrand, and change how they evangelize APIs internally, with partners, and the public, trying to ensure the efforts are more effective. While I still bundle all of this under <a href="http://evangelism.apievangelist.com/">my API evangelism research</a>, I am always looking for new approaches that push the boundaries, and evolve what we know as API evangelism, advocacy, outreach, and other variations.</p>

<p>I was introduced to a new variation of the internal API evangelism concept a few weeks back while at Capital One talking with my friend Matthew Reinbold(<a href="https://twitter.com/libel_vox">@libel_vox</a>) about their approach to API governance. His team at the Capital One API Center of Excellence has the concept of the API coach, and I think Matt’s description from <a href="https://matthewreinbold.com/2017/10/03/API-Governance-Blueprint/">his recent API governance blueprint story</a> sums it up well:</p>

<blockquote>
  <p><em>At minimum, the standards must be a journey, not a destination. A key component to “selective standardization” is knowing what to select. It is one thing for us in our ivory tower to throw darts at market forces and team needs. It is entirely another to repeatedly engage with those doing the work.<br /><br />
Our coaching effort identifies those passionate practitioners throughout our lines of business who have raised their hands and said, “getting this right is important to my teams and me”. Coaches not only receive additional training that they then apply to their teams. They also earn access to evolving our standards.<br /><br />
In this way, standards aren’t something that are dictated to teams. Teams drive the standards. These aren’t alien requirements from another planet. They see their own needs and concerns reflected back at them. That is an incredibly powerful motivator toward acceptance and buy-in.</em></p>
</blockquote>

<p>A significant difference here between internal API evangelism and API coaching is you aren’t just pushing the concept of APIs (evangelizing), you are going the extra mile to focus on healthy practices, standards, and API governance. Evangelism is often seen as an API provider to API consumer effort, which doesn’t always translate to API governance internally across organizations who are developing, deploying, and managing APIs. API coaches aren’t just developing API awareness across organizations, they are cultivating a standardized, bottom up, as well as top down awareness around providing and consuming APIs. Providing a much more advanced look at what is needed across larger organizations, when it comes to outreach and communication.</p>

<p>Another interesting aspect of Capital One’s approach to API coaching, is that this isn’t just about top down governance, it has a bottom up, team-centered, and very organic approach to API governance. It is about standardizing, and evolving culture across many organizations, but in a way that allows team to have a voice, and not just be mandated what the rules are, and required to comply. The absence of this type of mindset is the biggest contributor to a lack of API governance we see across the API community today. The is what I consider the politics of APIs, something that often trumps the technology of all of this.</p>

<p>API coaching augments <a href="http://evangelism.apievangelist.com/">my API evangelism research</a> in a new and interesting way. It also dovetails with my <a href="http://design.apievangelist.com/">API design research</a>, as well as begins rounding off a new area I’ve wanted to add for some time, but just have not see enough activity in to warrant doing so–API governance. I’m not a big fan of the top down governance that was given to us by our SOA grandfathers, and the API space has largely been doing alright without the presence of API governance, but I feel like it is approaching the phase where a lack of governance will begin to do more harm than good. It’s a drum I will start beating, with the help of Matt and his teams work at Capital One. I’m going to reach out to some of the other folks I’ve talked with about API governance in the past, and see if I can produce enough research to get the ball rolling.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/11/the-api-coaches-at-capital-one/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/10/explore-download-api-and-share-data/">Explore, Download, API, And Share Data</a></h3>
        <span class="post-date">10 Oct 2017</span>
        <p><a href="https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9"><img src="https://s3.amazonaws.com/kinlane-productions/nyc-open-data/nyc-opendata-explore-download-api.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m regularly looking through API providers, service providers, and open data platforms looking for interesting ways in which folks are exposing APIs. I have written about <a href="http://apievangelist.com/2017/10/05/show-that-api-call-behind-that-dashboard-visualization/">Kentik exposing the API call behind each dashboard visualization for their networking solution</a>, as well as <a href="https://apievangelist.com/2016/10/24/the-api-behind-every-feature-in-the-user-interface/">CloudFlare providing an API link for each DNS tool available via their platform</a>. All demonstrating healthy way we can show how APIs are right behind everything we do, and <a href="https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9">today’s example of how to provide API access is out of New York Open Data, providing access to 311 service requests made available via the Socrata platform</a>.</p>

<p>The page I’m showcasing provides access 311 service requests from 2010 to present, with all the columns and meta data for the dataset, complete with a handy navigation toolbar that lets you view data in Carto or Plot.ly, download the full dataset, access via API, or simply share via Twitter, Facebook, or email. It is a pretty simple example of offering up multiple paths for data consumers to get what they want from a dataset. Not everyone is going to want the API. Depending on who you are you might go straight for the download, or opt to access via one of the visualization and charting tools. Depending on who you are targeting with your data, the list of tools might vary, but the NYC OpenData example via Socrata provides a nice example to build upon. With the most important message being do not provide only the options you would choose–get to know your consumers, and deliver solutions they will also need.</p>

<p>It provides a different approach to making APIs behind available to users than the Kentik or CloudFlare approaches do, but it adds to the number of examples I have to show people how APIs and API enabled integration can be exposed through the UI, helping educate the massess about what is possible. I could see standardized buttons, drop downs, and other embeddable tooling emerge for helping deliver solutions like this for providers. Something like we are seeing with <a href="https://apievangelist.com/2017/06/05/extending-your-apps-using-embeddable-serverless-webhooks/">the serverless webhooks out Auth0 Extensions</a>. Some sort of API-enabled goodness that triggers something, and can be easily embedded directly into any existing web or mobile application, or possibly a browser toolbar–opening up API enabled solutions to the average user.</p>

<p>One of the reasons I keep showcasing examples like this is that I want to keep pushing back on the notion that APIs are just for developers. Simple, useful, and relevant APIs are not beyond what the average web application user can grasp. They should be present behind every action, visualization, and dataset made available online. When you provide useful integration and interoperability examples that make sense to the average user, and give them easy to engage buttons, drop downs, and workflows for implementing, more folks will experience the API potential in their world. The reasons us developers and IT folk keep things complex, and outside the realm of the normal folk is more about us, our power plays, as well as our inability to simplify things so that they are accessible beyond those in the club.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/10/explore-download-api-and-share-data/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/10/connecting-service-level-agreement-to-api-monitoring/">Connecting Service Level Agreements To API Monitoring</a></h3>
        <span class="post-date">10 Oct 2017</span>
        <p><a href="https://apimetrics.io"><img src="https://s3.amazonaws.com/kinlane-productions/apimetrics/api-metrics-latency-sla.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>Monitoring your API availability should be standard practice for internal and external APIs. If you have the resources to custom build API monitoring, testing, and performance infrastructure, I am guessing you already have some pretty cool stuff in place. If you don’t, then you should not be reinventing the wheel out there, and you should be leveraging one of the existing API monitoring services out there on the market. When you are getting started with monitoring your APIs I recommend you begin with uptime and downtime, and once you deliver successfully on that front, I recommend you work on API performance, and the responsiveness of your APIs.</p>

<p>You should begin with making sure you are delivering the service level agreement you have in place with your API consumers. What, you don’t have a service level agreement? No better time to start than now. If you don’t already have an explicitly stated SLA in place, I recommend creating one internally, and see what you can do to live up to your API SLA, then once you ensure things are operating at acceptable levels, you share with your API consumers. I am guessing they will be pretty pleased to hear that you are taking the initiative to offer an SLA, and are committed enough to your API to work towards such a high bar for API operations.</p>

<p>To help you manage defining, and then ultimately monitoring and living up to your API SLA, I recommend taking a look at <a href="https://apimetrics.io">APIMetrics</a>, who is obsessively focused on API quality, performance, and reliability. They spend a lot of time monitoring public APIs, and have <a href="https://apievangelist.com/2017/04/24/a-ranking-score-to-determine-if-your-api-was-sla-compliant/">developed a pretty sophisticated approach to ranking and scoring your API to ensure you meet your SLA</a>. As you can see in the picture for this story, the APIMetrics administrative dashboard provides a pretty robust way for you to measure any API you want, and establish metrics and triggers that let you know if you’ve met or failed to meet your SLA requirements. As I said, you could start out by monitoring internally if you are nervous about the results, but once you are ready to go prime time you have the tools to help you regularly reporting internally, as well as externally to your API consumers.</p>

<p>I wish that every stop along the life cycle had a common definition for defining a specific aspect of service level agreements, and was something that multiple API providers could measure and report upon similar to what APIMetrics does for monitoring and performance. I’d like to see API design begin to have a baseline definition, that was verifiable through a common set of machine readable API assertions. I’d love for API plans, pricing, and even terms of service measurable, reportable, in a similar way. These are all things that should be observable through existing outputs, and reflected as part of service level agreements. I’d love to see the concept of the SLA evolve to cover all aspects of the quality of service beyond just availability. APIMetrics provides a good look at how the services we use to manage our APIs can be used to define the level of service we provide, something that we can be emulating more across our API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/10/connecting-service-level-agreement-to-api-monitoring/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/10/algorithmic-observability-should-work-like-machine-readable-food-labels/">Algorithmic Observability Should Work Like Machine Readable Food Labels</a></h3>
        <span class="post-date">10 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algorithms/new-food-labels.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve been doing a lot of thinking about <a href="https://apievangelist.com/2016/08/04/pushing-for-more-algorithmic-transparency-using-apis/">algorithmic transparency</a>, as well as a more evolved version of it I’ve labeled as <a href="https://apievangelist.com/2017/06/28/algorithmic-transparency-in-policing/">algorithmic observability</a>. Many algorithmic developers feel their algorithms should remain black boxes, usually due to intellectual property concerns, but in reality the reasons will vary. My stance is that algorithms should be open source, or at the very least have some mechanisms for auditing, assessing, and verifying that algorithms are doing what they promise, and that algorithms aren’t doing harm behind the scenes.</p>

<p>This is a concept I know algorithm owners and creators will resist, but algorithms observability should work like food labels, but work in a more machine readable way, allowing them to be validated by other external (or internal) systems. Similar to food you buy in the store, you shouldn’t have to give away the whole recipe and secret sauce behind your algorithm, but there should be all the relevant data points, inputs, outputs, and other “ingredients” or “nutrients” that go into the resulting algorithm. <a href="http://apievangelist.com/2016/01/04/api-definition-origin-validation-and-attribution/">I talked about algorithm attribution before</a>, and I think there should be some sort of algorithmic observability manifest, which provides the “label” for an algorithm in a machine readable format. It should give all the relevant sources, attribution, as well as input and outputs for an algorithm–with different schema for different industries.</p>

<p>In addition to there being an algorithmic observability “label” available for all algorithms, there should be live, or at least virtualized, sandboxed instances of the algorithm for verification, and auditing of what is provided on the label. As we saw with the Volkswagen emissions scandal, algorithm owners could cheat, but it would provide an important next step for helping us understand the performance, or lack of performance when it comes to the algorithms we are depending on. Why I call this algorithmic observability, instead of algorithmic transparency, is each algorithm should be observable using it’s existing inputs and outputs (API), and not just be a “window” you can look through. It should be machine readable, and audit-able by other systems in real time, and at scale. Going beyond just being able to see into the black box, but also be able to assess, and audit what is occurring in real time.</p>

<p>Algorithmic observability regulations would work similar to what we see with food and drugs, where if you make claims about your algorithms, they should have to stand up to scrutiny. Meaning there should be standardized algorithmic observability controls for government regulators, industry analysts, and even the media to step up and assess whether or not an algorithm lives up to the hype, or is doing some shady things behind the scenes. Ideally this would be something that technology companies would do on their own, but based upon my current understanding of the landscape, I’m guessing that is highly unlikely, and will be something that has to get mandated by government in a variety of critical industries incrementally. If algorithms are going to impacting our financial markets, elections, legal systems, education, healthcare, and other essential industries, we are going to have to begin the hard work of establishing some sort of framework to ensure they are doing what is being sold, and not hurting or exploiting people along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/10/algorithmic-observability-should-work-like-machine-readable-food-labels/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/10/offering-a-guest-blogger-program-to-create-unique-content/">A Guest Blogger Program To Create Unique Content For Your API</a></h3>
        <span class="post-date">10 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/runscope/runscope-featured-guest-series.png" align="right" width="40%" style="padding: 15p;" /></p>
<p>Creating regular content for your blog is essential to maintaining a presence. If you don’t publish regularly, and refresh your content, you will find your SEO, and wider presence quickly becoming irrelevant. I understand that unlike me, many of you have jobs, and responsibilities when it comes to operating your APIs, and carving out the time to craft regular blog posts can be difficult. To help you in your storytelling journey I am always looking for other stories to help alleviate your pain, while helping keep your blog active, and ensure folks will continue stumbling across your API, or API service, while Google, or on social media.</p>

<p>Another interesting example of how to keep your blog fresh came from my partners over at Runscope, <a href="https://blog.runscope.com/writing-for-runscope">who conducted a featured guest blog post series</a>, where they were paying API community leaders to help “create an incredible resource of blog posts about APIs, microservices, DevOps, and QA.” Which has produced a handful of interesting posts:</p>

<ul>
  <li><a href="https://blog.runscope.com/?offset=1493310540201">Monolith to Microservices: Transforming a web-scale, real-world e-commerce platform using the Strangler Pattern</a></li>
  <li><a href="https://blog.runscope.com/posts/you-might-not-need-graphql">You Might Not Need GraphQL</a></li>
  <li><a href="https://blog.runscope.com/posts/3-easy-steps-to-cloud-operational-excellence-devops-pagerduty-statuspage">3 Easy Steps to Cloud Operational Excellence</a></li>
  <li><a href="https://blog.runscope.com/posts/building-steam-powered-iot-api-thingsboard-raspberry-pi">Building a Steam Powered IoT API with Thingsboard</a></li>
</ul>

<p>One thing to note is that Runscope paid $500.00 per post to help raise the bar when it comes to the type of author that will step up for such an effort. I’ve seen companies try to do this before, offering gift cards, swag, and even nothing in return, with varying grades of success and failure. I’m not saying a guest author program for your blog will always yield the results you are looking for, but it is a good way to help build relationships with your community, and help augment your existing workload, with some regular storytelling on the blog.</p>

<p>A guest blogger program is a tool I will be adding to <a href="http://communications.apievangelist.com/">my API communications research</a>, expanding on the tools API operators have in their toolbox to keep their communication strategies active. An active blog does more than just educate your community, and boost your SEO. An active blog, that is informative, and relevant shows that there is someone home behind an API, and that they are investing in the platform. While there are exceptions, the clearest sign that an API will soon be deprecated, or does not have the resources to support consumers properly is when the blog hasn’t been updated in the last six months. While I’m reviewing, indexing, and learning about different APIs, when I come across an inactive blog, or Twitter account for an API, I’ll almost always keep moving, feeling like there really isn’t much worthwhile there, as it will soon be gone.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/10/offering-a-guest-blogger-program-to-create-unique-content/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/09/treating-your-apis-like-they-are-infrastructure/">Treating Your APIs Like They Are Infrastructure</a></h3>
        <span class="post-date">09 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/stripe/stripe-api-versioning.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>We all (well most of us) strive to deliver as stable of an API presence as we possibly can. It is something that is easier said than done. It is something that takes caring, as well as the right resources, experience, team, management, and budget to do APIs just right. It is something the API idols our there make look easy, when they really have invested a lot of time and energy into developing a agile, yet scalable approach to ensuring APIs stay up and running. Something that you might able to achieve with a single API, but can easily be lost between each API version, as we steer the ship forward.</p>

<p>I spend a lot of time at the developer portals of these leading API providers looking for interesting insight into how they are operating, and I though <a href="https://stripe.com/blog/api-versioning">Stripe’s vision around versioning their API is worth highlighting</a>. Specifically their quote about treating your API like they are real life physical infrastructure.</p>

<p><em>“Like a connected power grid or water supply, after hooking it up, an API should run without interruption for as long as possible.Our mission at Stripe is to provide the economic infrastructure for the internet. Just like a power company shouldn’t change its voltage every two years, we believe that our users should be able to trust that a web API will be as stable as possible.”</em></p>

<p>This is possible. This is how I view Amazon S3, and Pinboard. These are two APIs I depend on to make my business work. Storage and bookmarking are two essential resources in my world, and both these APIs have consistently delivered stable API infrastructure, that I know I can depend on. I think it is also interesting to note that one is a tech giant, while the other is a viable small business (not startup). Demonstrating for me that there isn’t a single path to being a reliable, stable, API provider, despite what some folks might believe.</p>

<p>I am spending a lot of time lately thinking of API infrastructure along the lines of our energy grid, and transit system. The analogies are not perfect, but I do feel like as time moves on, some of our API infrastructure will continue to become commoditized, deemed essential, and something we depend on just as much as power, gas, and other utilities. These are the APIs that will be sticking around, the ones that can prove their usefulness, and deliver reliable integrations that do not change with each funding season, or technology trend. I’m looking forward to getting beyond the wild west days of APIs, and moving into the stage where APIs are treated like they are infrastructure, not just some toy, or latest fad, and we can truly depend on them and build our businesses around.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/09/treating-your-apis-like-they-are-infrastructure/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/09/api-governance-in-the-capital-one-devexchange/">Learning About API Governance From Capital One DevExchange</a></h3>
        <span class="post-date">09 Oct 2017</span>
        <p><a href="https://medium.com/capital-one-developers/api-lifecycle-governance-best-practices-56a1ebbb4f1b"><img src="https://cdn-images-1.medium.com/max/800/0*DOAJXcR7VcCN3iOh.jpg" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I am still working through my notes from a recent visit to Capital One, where I spent time talking with Matthew Reinbold (<a href="https://twitter.com/libel_vox">@libel_vox</a>) about their API governance strategy. I was given a walk through their approach to defining API standards across groups, as well as how they incentivize, encourage, and even measure what is happening. I’m still processing my notes from our talk, and waiting to see Matt publish more on his work, before I publish too many details, but I think it is worth looking at from a high level view, setting the bar for other API governance conversations I am engaging in.</p>

<p>First, what is API governance. I personally know that many of my readers have a lot of misconceptions about what it is, and what it isn’t. I’m not interesting in defining a single definition of API governance. I am hoping to help define it so that you can find it a version of it that you can apply across your API operations. API governance is at its simplest form, about ensuring consistency in how you do API across your development groups, and a more robust definition might be about having an individual or team dedicated to establishing organization-wide API standards, helping train, educate, enforce, and in the case of capital one, measure their success.</p>

<p>Before you can begin thinking about API governance, you need to start establishing what your API standards are. In my experience this usually begins with API design, but should also quickly also be about consistent, API deployment, management, monitoring, testing, SDKs, clients, and every other stop along the API lifecycle. Without well-defined, and properly socialized API standards, you won’t be able to establish any sort of API governance that has any sort of impact. I know this sounds simple, but I know more API providers who do not have any kind of API design, or other guide for their operations, than I know API providers who have consistent guides to design, and other stops along their API lifecycle.</p>

<p>Many API providers are still learning about what consistent API design, deployment, and management looks like. In the API industry we need to figure out how to help folks begin establishing organizational-wide API design guides, and get them on the road towards being able to establish an API governance program–it is something we suck at currently. Once API design, then deployment and management practices get defined we can begin to realize some standard approaches to monitoring, testing, and measuring how effective API operations are. This is where organizations will begin to see the benefits of doing API governance, and it not just being a pipe dream. Something you can’t ever realize if you don’t start with the basics like establishing an API design guide for your group. Do you have an API design guide for your group?</p>

<p>While talking with Matt about their approach at Capital One, he asked if it was comparable to what else I’ve seen out there. I had to be honest. I’ve never come across someone who had established API design, deployment, and management practices. Were actively educating and training their staff. Then actually measuring the impact and performance of APIs, and the teams behind them. I know there are companies who are doing this, but since I tend to talk to more companies who are just getting started on their API journey, I’m not seeing anyone organization who is this advanced. Most companies I know do not even have an API design guide, let alone measuring the success of their API governance program. It is something I know a handful of companies would like to strive towards, but at the moment API governance is more talk than it is ever a reality.</p>

<p>If you are talking API governance at your organization, I’d love to learn more about what you are up to. No matter where you are at in your journey. I’m going to be mapping out what I’ve learned from Matt, and compare with I’ve learned from other organizations. I will be publishing it all as stories here on API Evangelist, but will also look to publish a guide and white papers on the subject, as I learn more. I’ve worked with some universities, government agencies, as well as companies on their API governance strategies. API governance is something that I know many API providers are working on, but Capital One was definitely the furthest along in their journey that I have come across to date. <a href="https://medium.com/capital-one-developers/api-lifecycle-governance-best-practices-56a1ebbb4f1b">I’m stoked that they are willing to share their story</a>, and don’t see it as their secret sauce, as it is something that doesn’t just need sharing, it is something we need leaders to step up and show everyone else how it can be done.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/09/api-governance-in-the-capital-one-devexchange/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/09/publishing-your-api-road-map-using-trello/">Publishing Your API Road Map Using Trello</a></h3>
        <span class="post-date">09 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/tyk/tyk-road-map.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I consider a road map for any API to be an essential building block, whether it is a public API or not. You should be in the business of planning the next steps for your API in an organized way, and you should be sharing that with your API consumers so that they can stay up to speed on what is right around the corner. If you want to really go the extra mile I recommend following what Tyk is up to, with <a href="https://trello.com/b/59d5kAZ5/tyk-api-platform-roadmap">their public road map using Trello</a>.</p>

<p>With the API management platform Tyk, you don’t just see a listing of <a href="https://trello.com/b/59d5kAZ5/tyk-api-platform-roadmap">their API road map</a>, you see all the work and conversation behind the road ma using the visual collaboration platform Trello. Using their road map you can see proposed features, which is great to see if something you want has already been suggested, and you can get at a list of what the next minor releases will contain. Plus using the menu bar you can get at a history of the changes the Tyk team has made to the platform, going back for the entire history of the Trello board.</p>

<p>Using Trello you can subscribe to, or vote up any of the message boards. If you want to submit something you need to sign-up and post something to <a href="https://community.tyk.io/">the Tyk community</a>. Then they’ll consider adding it to the proposed road map features. It is a pretty low cost, easy to make public, approach to delivering a road map. Sometimes this stuff doesn’t need a complex solution, just one that provides some transparency, and help your customers understand what is next. Tyk provides a nice way to provide a road map that any other API provider, or service provider can follow.</p>

<p>Another interesting approach to delivering an API road map that I can add to my research.  I’m a big fan of having many different ways of delivering the essential building blocks of API operations, using a variety of simple free or paid SaaS tools. You’d be surprised at how useful an open road map can be for your API. Even if you aren’t adding too many new features, or have a huge number of people participating, it provides an easy reference showing what is next for an API. It also shows someone is home behind the scenes, and that an API is actually active, alive, and something you should be using.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/09/publishing-your-api-road-map-using-trello/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/09/communicating-sections-of-your-api-documentation/">Communication Strategy Filler Using Sections Of Your API Documentation</a></h3>
        <span class="post-date">09 Oct 2017</span>
        <p><a href="https://twitter.com/AWSSecurityInfo/status/868059146280550400"><img src="https://s3.amazonaws.com/kinlane-productions/aws/aws-tweet-documentation-links.png" align="right" width="35%" style="padding: 15px;" /></a>&lt;/a&gt;</p>
<p>Coming up with things creative things to write about regularly on the blog, and on Twitter when you are operating an API is hard. It has taken a lot of discipline to keep posts going up on API Evangelist regularly for the last seven years–totaling almost 3K total stories told so far. I don’t expect every API provider to have the same obsessive compulsive disorder that I do, so I’m always looking for innovative things that they can do to communicate with their API communities–something that Amazon Web Services is always good at providing healthy examples that I feel I can showcase.</p>

<p>One thing the AWS team does on a regular basis is tweeting out links to specific areas of their documentation, that helps users accomplish specific things with AWS APIs. The AWS security team is great at doing this, with recent examples focusing on securing things with <a href="https://twitter.com/AWSSecurityInfo/status/868059146280550400">the AWS Directory Service</a>, and <a href="https://twitter.com/AWSSecurityInfo/status/867711869900935169">API Organizations</a>. Each contains a useful description, attractive looking image, and a link to a specific page in the documentation that helps you learn more about what is possible.</p>

<p>I have been pushing myself to make sure all headers, and sub headers in my API documentation have anchors, so that I can not just link to a specific page, but I can link to a specific section, path, or other relevant item within my API documentation. This helps me in my storytelling when I’m looking to reference specific topics, and would help when it comes to tweeting out regular elements across my documentation in tweets. I’m slowly going to push out some of the lower grade tweets of curated news that I push out, and replace with relevant work I do in specific areas of my research–using my own work to fill the cracks over less than exciting things I may come across in the API space.</p>

<p>Tweeting out what is possible with your API, with links to specific sections of your API documentation is something I’m going to add to my <a href="http://communications.apievangelist.com/">API communication research</a>. Providing a common building block that other API providers, or even API service providers can consider when looking for things to fill the cracks in their platform communication strategy. It is simple, useful to API consumers, and is an easy way to keep the Tweet stream regularly flowing, and helping developers understand what is possible. I feel it is also something that should impact how we craft our documentation, as well as our communication strategies, making sure we are publishing with appropriate titles and anchors so we can easily reference and cite the valuable information we are making available across our platforms.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/09/communicating-sections-of-your-api-documentation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/06/publish-share-monetize-machine-learning-apis/">Publish, Share, Monetize Machine Learning APIs</a></h3>
        <span class="post-date">06 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/azure/azure-publish-share-monetize-ml.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve been playing with Tensor Flow for over a year now, specifically when it comes to working with images and video, but it has been something that has helped me understand what things looks like behind the algorithmic curtain that seems to be part of a growing number of tech marketing strategies right now. Part of this learning is exploring beyond Google’s approach, who is behind Tensor Flow, and understand what is going on at AWS, as well as Azure. I’m stil getting my feet wet learning about what Microsoft is up to with their platform, but I did notice one aspect of the <a href="https://azure.microsoft.com/en-us/services/machine-learning/">Azure Machine Learning Studio</a> emphasized developers to, <em>“publish, share, monetize”</em> their ML models. While I’m sure there will be a lot of useless vapor ware being sold within this realm, I’m simply seeing it as the next step in API monetization, and specifically the algorithmic evolution of being an API provider.</p>

<p>As the label says in the three ML models for sale in the picture, this is all experimental. Nobody knows what will actually work, or even what the market will bear. However, this is something APIs, and the business of APIs excel at. Making a digital resource available to consumers in a retail, or even wholesale way via marketplaces like Azure and AWS, then playing around with features, pricing, and other elements, until you find the sweet spot. This is how Amazon figured out the whole cloud computing game, and became the leader. It is how Twilio, Stipe and other API as a product companies figured out what developers needed, and what these markets would bear. This will play out in marketplaces like Azure and Google, as well as startup players like <a href="https://algorithmia.com/">Algorithmia</a>–which is where I’ve been cutting my teeth, and learning about ML.</p>

<p>The challenge for ML API entrepreneurs will be helping consumers understand what their models do, or do not do. I see it as an opportunity, because there will be endless amounts of vapor ware, ML voodoo, and smoke and mirrors trying to trick consumers into buying something, as well as endless traps when it comes to keeping them locked in. If you are actually doing something interesting with ML, and it actually provides value in the business world, and you provide clear, concise, no BS language about what it does–you are going to do well. The challenge for you will be getting found in the mountains of crap that is emerging, and differentiating yourself from the smoke and mirrors that we are already seeing so much of. Another challenge you’ll face is navigating the vendor platform course set up by AWS, Google, and Azure as they battle it out for dominance–a game that many of us little guys will have very little power to change or steer.</p>

<p>It is a game that I will keep a close eye on. I’m even pondering publishing a handful of image manipulation models I’ve been working on. IDK. I do not think they are quite ready, and I’m not even entirely sure they are something I want widely used. I’m kind of enjoying using them in my own work, providing me with images I can use in my storytelling. I don’t think the ROI is there yet in the ML API game, and I’ll probably just keep being a bystander, and analyst on the sideline until I see just the right opportunity, or develop just the right model I think will stand out. After seven years of doing API Evangelist I’m pretty good at seeing through the BS, and I’m thinking this experience is going to come in handy in this algorithmic evolution of the API universe, where the magic of AI and ML put so many people under their spell.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/06/publish-share-monetize-machine-learning-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/06/thinking-about-why-we-rate-limit-our-apis/">Thinking About Why We Rate Limit Our APIs</a></h3>
        <span class="post-date">06 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/power-lines-empty-space_sunday.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am helping a client think through their API management solution at the moment, so I’m working through all the moving parts of how, and why of API management solutions. The API management landscape has shifted since the last time I helped a small company navigate the process of getting up and running, so I wanted to work through each aspect and think critically before I make any recommendations. My client has a content API, which isn’t very complex, but possesses some pretty valuable data they’ve aggregated, curated, and are looking to make available via a simple web API. It is pretty clear that all developers will need a key to be access the API, but I wanted to pause for a moment and think more about API rate limiting.</p>

<p>Why do we rate limit? The primary reason is to help manage the compute resources available for all API consumers. You don’t want any single user hitting the server too hard, and taking things down for everyone else. I’d say after that, the next major reason is to enforce API access tiers, and ensure API consumers are only consuming what they should be. Which both seem like pretty dated concepts, that might need re-evaluation in general, but also in the context of this particular project. There is no free access to this API. I believe there will be a public account for test driving (making very limited # of calls), and some that drive their embeddable strategy, but for access to the majority of content, developers will have to register for a key, and provide a credit card to pay for their consumption. Which leaves me with the question, should we be rate limiting at all?</p>

<p>If users are paying for whatever they consume, and there is a credit card on file, do we want to rate limit? Why are we so worried about server capacity in a cloud world? It seems like rate limiting is a legacy constraint, that has continue to live on unquestioned, and even propped up by accounting and business decisions over simple technical ones. API access tiers with varying rate limits are sometime imposed as part of identity and access control, limiting what new users have access to, but often times they are used to corral and route users into specific, and measurable account plans, that help startups predict and articulate revenue to investors. I know many of my friends disagree with my thoughts on this, but I feel this accounting decision behind rate limiting are hurting the bottom line, more than they are helping. If you are focused on your API being the product it is hurting it, if you are focused on your API consumers being your product, then you are helping it.</p>

<p>My client in question is looking to build an actual business that sells a product to customers, without an exit strategy, so I want to do my best to help them understand how they can reduce technical and business complexity, while maximizing revenue around the API services they are offering. If we have the API properly resourced with scalable compute, load-balancing, monitoring, checks and balances. Then we also have a verified credit card on file for each API key holder. Why do we want to rate limit? It seems like it is an unnecessary complexity for API consumers to have to wrestle with. Let’s just allow them to register, make API calls, measure, and bill accordingly. Amazon provides a clear precedent for how this works, and from my experience I tend to spend more on my AWS bill then I do with services I use which keep me in tiered access plans. I’m not saying tiered access plans don’t have their place, I’m saying we should be questioning their value each time we are constructing them, and not just assuming they should be done my default.</p>

<p>A by-product of noticing how the API management landscape recently is helping me reassess each of the common building blocks of API management, and think more critically about the how and why behind their existence. There is a significant difference between rate limiting and metering API calls, and I don’t think we always have to do both. We still need the ability to turn off keys, and block specific user agents and IP addresses, but in some cases I think rate limiting shouldn’t be part of the API management operations. We have the compute, storage, and databases resources at our disposal to scale as we need to meet demand, and we have the credit card verified, and on file to bill against, let’s just get out of API consumers way. In the case of this particular project I’m working, I think this will be my recommendation. Focusing on reducing the amount of API management overhead, and simplifying the load for API consumers along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/06/thinking-about-why-we-rate-limit-our-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/06/the-api-management-landscape-looks-different-than-i-anticipated/">The API Management Landscape Has Shifted More Than I Anticipated</a></h3>
        <span class="post-date">06 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/yellow-tree-in-the-rocks.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>It is interesting to take a fresh look at the API management landscape these days. It has been a while since I’ve looked through all the providers to see where their pricing is at, and what they offer. I’d say the space has definitely shifted from what things looked like 2012 through 2015. There are still a number of open source offerings, which there weren’t in 2012, but the old guard has solidly turned their attention to the enterprise. There are the cloud solutions like <a href="https://restlet.com/">Restlet</a>, ad <a href="http://www.slashdb.com/">SlashDB</a> which really help you get up and running from existing data sources in the cloud, but for this particular project I am looking for a simple proxy and connector approach to deploying on any infrastructure, and they don’t quite fit the requirements.</p>

<p>Apigee, and the other more enterprise offerings have always been out of my league, and 3Scale’s entry level package is up to $750, which is a little out of my reach, but I do know they are open sourcing their offering, now that they are part of Red Hat. There is <a href="http://nrel.github.io/api-umbrella/">API Umbrella</a>, <a href="http://www.apiman.io/">APIMan</a>, <a href="http://www.fusio-project.org/">Fusio</a>, <a href="http://www.monarchapis.com/">Monarch</a>, and handful of other solutions that will work, but they take certain platform, or specific language commitment that doesn’t work for this project. Everything else is of the enterprise caliber, nothing really that I would recommend to my customers who are just getting started on their API journey. I’m really left with the cloud giants, which I guess is one of the main reasons we are at this junction in the evolution of API management. API management becoming a commodity has really shifted the landscape, making it more difficult to be a strong player like <a href="https://s3.amazonaws.com/kinlane-productions/partners/tyk-logo.png">Tyk</a> and Kong are managing to pull off.</p>

<p>If my customer was looking to launch a data API from an existing database I’d point them to SlashDb or Restlet. If they are an enterprise customer I’d point them to 3Scale. Tyk is pretty much my goto person for the lower end of the market, with Kong as the alternate. If my customer is already running on Google, Azure, or AWS, then I’m pretty much telling them to stay put, and use the tooling that is available to them. Another thing I’m noticing has dropped out of prominence is the billing for API usage aspect of API management. It’s in Tyk, and 3Scale, but really wasn’t a component of many of the other solutions I’ve been looking at. Overall things seem scattered out there after the last round of API management acquisitions, the VC funding shifted, and the cloud giants stepped into their game with their solutions. I’m guessing that is the just the circle of life and all.</p>

<p>In this new landscape I am going to have to spend more time playing with the low-end solutions that are available. It is essential that there are solutions that are accessible to folks who are just starting on their API journey. It is something that requires a climb-able ladder. Meaning you need to be able to afford the reach to the next rung of the ladder, otherwise it can become quite a blocker. I was a big advocate for this in the early days, but stopped pushing on because there were so many options out there. It will take some playing around to get a better feeling about where we are, before I feel good about making recommendations to new players again. A process I should probably be repeating each year, because things seem to be shifting a little more than I anticipated.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/06/the-api-management-landscape-looks-different-than-i-anticipated/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/">A Couple More Questions For The Equifax CEO About Their Breach</a></h3>
        <span class="post-date">06 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/digital-bits-capital-dc-flag-side-view.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="https://www.theverge.com/2017/10/3/16410806/equifax-ceo-blame-breach-patch-congress-testimony">Speaking to the House Energy and Commerce Committee, former Equifax CEO Richard Smith pointed the finger at a single developer who failed to patch the Apache Struts vulnerability</a>. Saying that protocol was followed, and a single developer was responsible, shifting the blame away from leadership. It sounds like a good answer, but when you operate in the space you understand that this was a systemic failure, and you shouldn’t be relying on a single individual, or even a single piece of scanning software to verify the patch was applied. You really should have many layers in place to help prevent breaches like we saw with Equifax.</p>

<p>If I was interviewing the CEO, I’d have a few other questions for him, getting at some of the other systemic and process failures based upon his lack of leadership, and awareness:</p>

<ul>
  <li><strong>API Monitoring &amp; Testing</strong> - You say the scanner for the Apache Struts vulnerability failed, but what about other monitoring and testing. The plugin in questions was a REST plugin, that allowed for API communication with your systems. Due to the vulnerability, extra junk information was allowed to get through. Where were your added API request and response integrity testing and monitoring process? Sure you were scanning for the vulnerability, but are you keeping an eye on the details of the data being passed back and forth? API monitoring &amp; testing has been around for many years, and service providers like <a href="http://apis.how/8nlsropidv">Runscope</a> do this for a living. What other layers of monitoring and testing were in place?</li>
  <li><strong>API Management</strong> - When you expose APIs like you did from Apache Struts, what does the standardized management approach look like? What sort of metering, logging, rate limiting, and analysis occurs on each endpoint, and verification occurs, ensuring that only required clients should have access? API management has been standard procedure for over a decade now for exposing APIs like this both internally and externally. Why didn’t your API management process stop this sort of breach after only a couple hundred record went out? API management is about awareness regarding access to all your resources. You should have a dashboard, or at least some reports that you view as a CEO on this aspect of operations.</li>
</ul>

<p>These are just two examples of systems and processes that should have been in place. You should not be depending on a single person, or a single tool to catch this type of security incident There should be many layers in place, with security triggers, and notifications in place. Your CTO should be in tune with all of these layers, and you as the CEO should be getting briefed on how they work, and have a hand in making sure they are in place. I’m guessing that your company is doing APIs, but is dramatically behind the times when it comes to commonplace API management practices. This is your fault as the CEO. This is not the fault of a single employee, or any group of employees.</p>

<p>I am guessing that as a CEO you are more concerned with the selling of this data, than you are of securing it in storage, or transit. I’m guessing you are intimately aware of the layers that enable you to generate revenue, but you are barely investing in the technology and processes to do this securely, while respecting the privacy of your users. They are just livestock to you. They are just products on a shelf. It shows your lack of leadership to point the finger at a single person, or single piece of technology. There should have been many layers in place to catch this type of breach beyond a single vulnerability. It demonstrates your lack of knowledge regarding modern trends in how we secure and provide access to data, and you should never have been put in charge of such a large data brokerage company.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/">Teaching My Client Three Approaches To Modular UI Design Using Their APIs</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/holmes-county/holmes-county-listing.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am working with a client to develop a simple user interface on top of a Human Services Data API (HSDA) I launched for them. They want a basic website for searching, browsing, and navigating the organizations, locations, and services available in their API. A part of this work is helping them understand how modular and configurable their web site is, with each page, or portion of a page being a simple API call. It is taking a while for them to fully understand what they have, and the potential of evolving a web application in this way, but I feel like they are beginning to understand, and are taking the reigns a little more when it comes to dictate what they want within this new world.</p>

<p>When I first published a basic listing of human services they were disappointed. They had envisioned a map of the listings, allowing users to navigate in a more visual way. I got to work helping them see the basic API call(s) behind the listing, and how we could use the JSON response in any way we wanted. I am looking to provide three main ways in which I can put the API data to work in a variety of web applications:</p>

<ul>
  <li><strong>Server-Side</strong> - A pretty standard PHP call to the API, taking the results and rendering to the page using HTML.</li>
  <li><strong>Client-Side</strong> - Leveraging JavaScript in the browser to call the API and then render to the page using Jquery.</li>
  <li><strong>Static Push</strong> - Calling the APPI using PHP, then publishing as YAML or JSON to a Jekyll site and rendering with Liquid and HTML.</li>
</ul>

<p>What the resulting HTML, CSS, and JavaScript looks like in all these scenarios is up to the individual who is in charge of dictating the user experience. In this case, it is my client. They just aren’t used to having this much control over dictating the overal user experience. Which path they choose depends on a few things like whether they want the content to be easily indexed by search engines, or if they want things to be more JavaScript enabled magic (ie. maps, transitions, flows). All the API does is gives them full access to the entire surface area of the human services schema they have stored in their database.</p>

<p>After moving past the public website, I’m beginning to show them the power of not just GETs via their API, I’m showing them the power of POST, PUT, and DELETE. I find it is easy to show them the potential in an administrative system using a basic form. I find people get forms. Once you show them that in order to POST they have to have a special token or key, otherwise the API will reject, they feel a whole lot better about the process. I find the form tends to put things into context for them, beyond displaying data and content, and allowing them to actually manage all this data and content. I find the modularity of an API really lends itself to giving business users more control over the user interface. They may not be able to do everything themselves, but they tend to be more invested in the process, and enjoy more ownership over the process–which is a good thing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/how-api-evangelist-works/">How API Evangelist Works</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/crypto-machine-bletchley_copper_circuit.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve covered this topic several times before, but I figured I’d share again for folks who might have just become readers int he last year. Providing an overview of how API Evangelist works, to help eliminate confusion as you are navigating around my site, as well as to help you find what you are looking for. First, API Evangelist was started in the summer of 2010 as a research site to help me better understand what is going on in the world of APIs. In 2017, it is still a research site, but it has grown and expanded pretty dramatically into a network of websites, driven by a data and a content core.</p>

<p>The most import thing to remember is that <a href="https://github.com/api-evangelist">all my sites run on Github</a>, which is my workbench in the the API Evangelist workshop. apievangelist.com is the front door of the workshop, with each area of my research existing as its own Github repository, at its own subdomain with the apievangelist domain. An example of this can be found in my API design research, where you will find at <a href="http://design.apievangelist.com/">design.apievangelist.com</a>. As I do my work each day, I publish my research to each of my domains, in the form of YAML data for one of these areas:</p>

<ul>
  <li><strong>Organizatons</strong> - Companies, organizations, institutions, programs, and government agencies doing anything interesting with APIs.</li>
  <li><strong>Individuals</strong> - The individual people at organizations, or independently doing anything interesting with APIs.</li>
  <li><strong>News</strong> - The interesting API related, or other news I curate and tag daily in my feed reader or as I browse the web.</li>
  <li><strong>Tools</strong> - The open source tooling I come across that I think is relevant to the API space in some way.</li>
  <li><strong>Building Blocks</strong> - The common building blocks I find across the organizations, and tooling I’m studying, showing the good and the bad of doing APIs.</li>
  <li><strong>Patents</strong> - The API related patents I harvest from the US Patent Office, showing how IP is impacting the world of APIs.</li>
</ul>

<p>You can find the data for each of my research areas in the _ data folder for each repository. Which is then rendered as HTML for each subdomain using Liquid via each Jekyll CMS driven website. All of this is research. It isn’t meant to be perfect, or a comprehensive directory for others to use. If you find value in it–great!! However, it is just my research laying on my workbench. It will change, evolve, and be remixed and reworked as I see fit, to support my view of the API sector. You are always welcome to learn from this research, or even fork and reuse it in your own work. You are also welcome to submit pull requests to add or update content that you come across about your organization or open source tool.</p>

<p>The thing to remember about API Evangelist is it exist primarily for me. It is about me learning. I take what I learn and publish as blog posts to API Evangelist. This is how I work through what I’m discovering as part of my research each day, and use as a vehicle to move my understanding of APIs forward. This is where it starts getting real. After seven years of doing this I am reaching 4K to 7K page views per day, and clearly other folks find value in reading, and sifting through my research. Because of this I have four partners, <a href="http://apis.how/ake3nxbapm">3Scale</a>, <a href="http://apis.how/5ytnitnakm">Restlet</a>, <a href="http://apis.how/8nlsropidv">Runscope</a>, and <a href="http://apis.how/zflfesymzk">Tyk</a> who pay me money to have their logo on the home page, in the navigation, and via a footer toolbar. Runscope also pays me to place a re-marketing tag on my site so they can show advertising to my users on other websites, and Facebook. This is how I pay my rent, an how I eat each month.</p>

<p>Beyond this base, I take my research and create API Evangelist industry guides. <a href="http://definitions.apievangelist.com/#Guide">API Definitions</a>, and <a href="http://design.apievangelist.com/#Guide">API Design</a> are the two most recent editions. I’m currently working on one for data, database, as well as deployment, and management. These guides are sometimes underwritten by my partners, but mostly they are just the end result of my API research. I also spend time and energy taking what I know and craft API strategy and white papers for clients, and occasionally I actually create APIs for people–mostly in the realm of human services, or other social good efforts. I’m not really interested in building APIs for startups, or in service of the Silicon Valley machine. Even tough I enjoy watching, studying, and learning from this world, because there are endless lessons regarding how we can use technology in this community, as well as how we should not be using technology.</p>

<p>That is a pretty basic walk through of API Evangelist works. It is important to remember I am doing this research for myself. To learn, and to make a living. API Evangelist is a production, a persona I created to help me wade through the technology, business, and politics of APIs. It reflects who I am, but honestly is increasingly more bullshit than it is reality, kind of like the API space. I hope you enjoy this work. I enjoy hearing from my readers, and hearing how my research impacts your world. It keeps me learning each day, and from ever having to go get a real job. It is always a work in progress and never done. Which I know frustrates some, but I find endlessly interesting, and is something that reflects the API journey, something you have to get used to if you are going to be successful doing APIs in this crazy world.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/how-api-evangelist-works/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/show-that-api-call-behind-that-dashboard-visualization/">Show The API Call Behind Each Dashboard Visualization</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><a href="https://www.kentik.com/kentik-apis-enable-multi-solution-integration/"><img src="https://s3.amazonaws.com/kinlane-productions/kentik/kentik_API_menu-300w.png" align="right" width="35%" style="padding: 15px;" /></a></p>
<p>I am a big fan of user interfaces that bring APIs out of the shadows. Historically, APIs are often a footnote in the software as a service (SaaS) world, available as a link way down at the bottom of the page, in the settings, or help areas. Rarely, are APIs made a first class citizen in the operations of a web application, which really just perpetuates the myth that APIs aren’t for everybody, and the “normals” shouldn’t worry their little heads about it. When in reality, EVERYBODY should know about APIs, and have the opportunity to put them to work, so we should stop burying the links to our APIs, and our developer areas. If your API is too technical for a business user to understand what is going on, then you should probably get to work simplifying it, not burying it and keeping it in developer and IT realm.</p>

<p>I have written before about <a href="https://apievangelist.com/2016/10/24/the-api-behind-every-feature-in-the-user-interface/">how DNS provider CloudFlare provides an API behind every feature in their user interface</a>, and I’ve found <a href="https://www.kentik.com/kentik-apis-enable-multi-solution-integration/">another great example of this over at the network API provider Kentik</a>. In their network dashboard visualization tooling they provide a robust set of tooling for accessing the data behind the visuals, allowing you to export, view SQL, show API call, and enter share view. In their post, they proceed to instruction about how you can get your API key as part of your account, as well as providing a pretty robust introduction into why APIs are important. This is how ALL dashboards should work in my opinion. Any user should be introduced to APIs, and have the ability to get at the data behind, and export it, or directly make an API call in their browser or at the command line.</p>

<p>Developers like to think this stuff should be out of reach of the average user, but that is more about our own insecurities, and power trips, than it is about the average users ability to grasp this stuff. There is no reason why ALL user interfaces can’t be developed on top of APIs, with native functionality for getting at the API call, as well as the data, content, or algorithms behind the user interface feature. It makes for more powerful user interfaces, as well as more educated, literate, and efficient power users of our applications. If all web applications operated this way, we’d see a much more API literate business world, where users would be asking more questions, curious about how things work, and experimenting with ways they can be more successful in what they do. While I do see positive examples like Kentik out there, I also find that many web application developers are further retreating from APIs being front and center, preferring to keep them in the shadows of web and mobile applications, out of the reach of the average user. Something we need to reverse.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/show-that-api-call-behind-that-dashboard-visualization/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/big-data-is-not-abut-access-using-web-apis/">Big Data Is Not About Access Using Web APIs</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/dragon_close-up_yellow_collage.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m neck deep in research around data and APIs right now, and after <a href="http://apievangelist.com/2017/10/03/looking-at-the-37-apached-data-projects/">looking at 37 of the Apache data projects</a> it is pretty clear that web APIs are not a priority in this world. There are some of the projects that have web APIs, and there a couple projects that look to bridge several of the projects with an aggregate or gateway API, but you can tell that the engineers behind the majority of these open source projects are not concerned with access at this level. Many engineers will counter this point by saying that web APIs can’t handle the volume, and it shows that the concept isn’t applicable in all scenarios. I’m not saying web APIs should be used for the core functionality at scale, I’m saying that web APIs should be present to provide access to the result state of the core features for each of these platform, whatever that is, which something that web APIs excel at.</p>

<p>From my vantage point the lack of web APIs isn’t a technical one, it is a business and political motivation. When it comes to big data the objectives are always about access, and it definitely isn’t about the wide audience access that comes when you use HTTP, and the web for API access. The objective is to aggregate, move around, and work with as much data as you possibly can amongst a core group of knowledgable developers. Then you distribute awareness, access, and usage to designated parties via distilled analysis, visualizations, or in some cases to other systems where the result can be accessed and put to use. Wide access to this data is not the primary objective, paying forward much of the power and control we currently see around database to API efforts. Big data isn’t about democratization. Big Data is about aggregating as much as you can and selling the distilled down wisdom from analysis, or derived as part of machine learning efforts.</p>

<p>I am not saying there is some grand conspiracy here. It just isn’t the objective of big data folks. They have their marching orders, and the technology they develop reflect these marching orders. It reflects the influence money and investment has on the technology. The ideology that drives how the tech is engineered, and the algorithms handle specific inputs, and provide intended outputs. Big data is often sold as data liberation, democratization, and access to your data, building on much of what APIs have done in recent years. However, in the last couple of years the investment model has shifted, the clients who are purchasing and implementing big data have evolved, and they aren’t your API access type of people. They don’t see wide access to data as a priority. You are either in the club, and know how to use the Apache X technology, or you are sanctioned one of the dashboard analysis visualization machine learning wisdom drips from the big data. Reaching a wide audience is not necessary.</p>

<p>For me, this isn’t some amazing revelation. It is just watching power do what power does in the technology space. Us engineers like to think we have control over where technology goes, yet we are just cogs in the larger business wheel. We program the technology to do exactly what we are paid to do. We don’t craft liberating technology, or the best performing technology. We assume engineer roles, with paychecks, and bosses who tell us what we should be building. This is how web APIs will fail. This is how web APIs will be rendered yesterdays technology. Not because they fail technically, it is because the ideology of the hedge funds, enterprise groups, and surveillance capitalism organizations that are selling to law enforcement and the government will stop funding data systems that require wide access. The engineers will go along with it because it will be real time, evented, complex, and satisfying to engineer in our <a href="http://kinlane.com/2017/10/04/isolated-development-environments/">isolated development environments (IDE</a>). I’ve been doing data since the 1980s, and in my experience this is how data works. Data is widely seen as power, and all the technical elements, and many of the human elements involved often magically align themselves in service of this power, whether they realize they are doing it or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/big-data-is-not-abut-access-using-web-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/apis-used-to-give-access-to-resources-that-were-out-of-our-reach/">APIs Used To Give Us Access To Resources That Were Out Of Our Reach</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/mountainlake/clean_view/file-00_00_58_86.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I remember when almost all the APIs out there gave us developers access to things we couldn’t ever possibly get on our own. Some of it was about the network effect with the early Amazon and eBay marketplaces, or Flickr and Delicious, and then Twitter and Facebook. Then what really brought it home was going beyond the network effect, and delivering resources that were completely out of our reach like maps of the world around us, (seemingly) infinitely scalable compute and storage, SMS, and credit card payments. In the early days it really seemed like APIs were all about giving us access to something that was out of our reach as startups, or individuals.</p>

<p>While this still does exist, it seems like many APIs have flipped the table and it is all about giving them access to our personal and business data in ways that used to be out of their reach. Machine learning APIs are using parlour tricks to get access to our internal systems and databases. Voice enablement, entertainment, and cameras are gaining access to our homes, what we watch and listen to, and are able to look into the dark corners of our personal lives. Tinder, Facebook, and other platforms know our deep dark secrets, our personal thoughts, and have access to our email and intimate conversations. The API promise seems to have changed along the way, and stopped being about giving us access, and is now about giving them access.</p>

<p>I know it has always been about money, but the early vision of APIs seemed more honest. It seemed more about selling a product or service that people needed, and was more straight up. Now it just seems like APIs are invasive. Being used to infiltrate our professional and business worlds through our mobile phones. It feels like people just want access to us, purely so they can mine us and make more money. You just don’t see many Flickrs, Google Maps, or Amazon EC2s anymore. The new features in mobile devices we carry around, and the ones we install in our home don’t really benefit us in new and amazing ways. They seem to offer just enough to get us to adopt them, and install in our life, so they can get access to yet another data point. Maybe it is just because everything has been done, or maybe it is because it has all been taken over by the money people, looking for the next big thing (for them).</p>

<p>Oh no! Kin is ranting again. No, I’m not. I’m actually feeling pretty grounded in my writing lately, I’m just finding it takes a lot more work to find interesting APIs. I have to sift through many more emails from folks telling me about their exploitative API, before I come across something interesting. I go through 30 vulnerabilities posts in my feeds, before I come across one creative story about something platform is doing. There are 55 posts about ICOs, before I find an interesting investment in a startup doing something that matters. I’m willing to admit that I’m a grumpy API Evangelist most of the time, but I feel really happy, content, and enjoying my research overall. I just feel like the space has lost its way with this big data thing, and are using APIs to become more about infiltrating and extraction, that it is about delivering something that actually gives developers access to something meaningful. I just think we can do better. Something has to give, or this won’t continue to be sustainable much longer.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/apis-used-to-give-access-to-resources-that-were-out-of-our-reach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/">API Providers Should Provide Observability Into Government Developer Accounts</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve talked about this before, but after reading several articles recently about various federal government agencies collecting, and using social media accounts for surveillance lately, it is a drum I will be beating a lot more regularly. <a href="http://transparency.apievangelist.com/2013/02/26/api-transparency-report-as-essential-building-block/">Along with the transparency reports we are beginning to see emerge from the largest platform providers</a>, I’d like to start seeing more observability regarding which accounts, both user and developer are out of government agencies. Some platforms are good at highlighting how government of all shapes and sizes are using their platform, and some government agencies are good at showcasing their social media usage, but I’d like to understand this from purely an API developer account perspective.</p>

<p>I’d like to see more observability into which government agencies are requesting API keys. Maybe not specific agencies ad groups, and account details, although that would be a good idea as well down the road. I am just looking for some breakdown of how many developer accounts on a platform are government and law enforcement. What does their API consumption look like? If there is Oauth via a platform, is there any bypassing of the usual authentication flows to get at data, any differently than regular developers would be accessing, or requiring user approval? From what I am hearing, I’m guessing that there are more government accounts out there than platforms either realize, or are willing to admit. It seems like now is a good time to start asking these questions.</p>

<p>I would add on another layer to this. If an application developer is developing applications on behalf of law enforcement, or as part of a project for a government agency, there should be some sort of disclosure at this level as well. I know I’m asking a lot, and a number of people will call me crazy, but with everything going on these days, I’m feeling like we need a little more disclosure regarding how government(s) are using our platforms, as well as their contractors. The transparency disclosure that platforms have been engaging is a good start to the legal side of this conversation, but I’m looking for the darker, more lower level surveillance that I know is going on behind the scenes. The data gathering on U.S. citizens that doesn’t necessarily violate any particular law, because this is such new territory, and the platform terms of service might sanction it in some loopholy kind of way.</p>

<p>This isn’t just a U.S. government type of thing. I want this to be standard practice for all forms of government on the platforms we use. A sort of UN level, General Data Protection Regulation (GDPR). Which reminds me. I am woefully behind on what GDPR outlines, and how the rolling out of it is going. Ok, I’ll quick ranting now, and get back to work. Overall, we are going to have to open up some serious observability into how the online platforms we are depending are being accessed and use by the government, both on the legal side of things, as well as just general usage. Seems like the default on the general usage should always be full disclosure, but I’m guessing it isn’t a conversation anyone is having yet, which is why I bring up. Now we are having it. Thanks.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

<p align="center"><a href="http://apievangelist.com/archive/"><strong>View Previous Posts Via Archives</strong></a></p>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/" target="_blank"><img src="https://apievangelist.com/images/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/" class="image"><img src="https://apievangelist.com/images/postman-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
