<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>API Evangelist</title>
  <updated>2019-07-18T02:00:00Z</updated>
  <link rel="self" href="http://localhost:4000/atom.xml"/>
  <author><name>Kin Lane</name></author>
  <id>http://localhost:4000/atom.xml</id>
	<entry>
    <title>What Is An Application?</title>
    <link href="http://apievangelist.com/2019/07/18/what-is-an-appication/"/>
    <updated>2019-07-18T02:00:00Z</updated>
    <content><![CDATA[
I have struggled asking this question in many discussions I’ve had around the world, at technology conferences, on consulting projects, and in the back rooms of dimly lit bars. What is an application? You get ten different answers if you ask this question to ten different people. I’d say the most common response is to reference the applications on a mobile device. These are the relevant. Most accessible. The most active and visible form of application in our modern life. Older programmers see them as desktop applications, where younger programmers see them as web applications, with varying grades of server applications in between. If you operate at the network layer, you’ve undoubtedly bastardized the term to mean several different things. Personally, I’ve always tried to avoid these obvious and tangible answers to this question, looking beyond the technology.

My view of what an application is stems from a decade of studying the least visible, and least tangible aspect of an application, its programming interface. When talking to people about applications, the first question I ask folks is usually, “do you know what an API is”? If someone is API savvy I will move to asking, “when it comes to application programming interface (API), who or what is being programmed? Is it the platform? The application? Or, is it the end-user of the applications?” I’ve spent a decade thinking about this question, playing it over and over in my end, never quite being satisfied with what I find. Honestly, the more I scratch, the more concerned I get, and the more I’m unsure of exactly what an “application” is, and precisely who are what is actually being programmed. Let’s further this line of thinking by looking at the definitions of “application”:


  noun - The act of applying.
  noun - The act of putting something to a special use or purpose.
  noun - A specific use to which something is put.
  noun - The capacity of being usable; relevance.
  noun - Close attention; diligence.
  noun - A request, as for assistance, employment, or admission to a school.
  noun - The form or document on which such a request is made.
  noun - Computers A computer program designed for a specific task or use.
  adjective - Of or being a computer program designed for a specific task or use.


This list comes from my friends over at Wordnik (https://www.wordnik.com/words/application), who I adore, have an API (https://developer.wordnik.com/), and who have contributed to this discussion by playing a leading role in introducing the API sector to the OpenAPI (fka Swagger) specification to the API community. That is whole other layer of significance when it comes to the semantics and meaning behind the word application, which I will have to write more about in a future post. In short, words matter. Really, words are everything. The meaning behind them. The intent. The wider understanding and belief. This is why APIs, and web applications like Wording are important. Helping us make sense of the world around us. Anyways, back to what is an application?

I like the first entry on the list — the act of applying. But, each of these definitions resonate with my view of the landscape. Yet, I’d say that the most common answer to this question hover towards the second half of this list, not the first half of it. Most application developers would say they are programming the application—the interface is for them. If you are an API developer you believe that are the one programming the application, where higher up on the platform decision making chain, they are the ones programming the application, and applying their vision of the world. Something that isn’t always visible at the lower levels, by API developers delivering APIs, the application developers consuming them, or the end-users of the tangible applications being delivered. I see the end desktop, web, or mobile as an application. I see the API as an application. I see the network connecting the two as an application. I also see the wider ideology being applied across all these layers, even when it is out of view to the outer layers.

One of the biggest imbalances in my belief system around technology, a result of be operating at the lower levels of business, institutional, or government, is that I am the one “applying” and “programming”. I was developing and delivering the application. These interfaces served me. After studying the machine closer. Tracking on the cycles. Documenting the results over the course of many cycles. I began to realize that there is more to this “application” thing than what I”m seeing. Maybe I was too close to the gears and the noise of the machine to see the bigger picture. I’m in the role of application developer not because I’m good at what I do. I’m there because I conveniently think I’m in control of this supply chain. As I worked my way up the supply chain, and became an API developer I continue to believe that I was the one “applying” and “programming”. However after over a decade of doing that, I’m realizing that I am not the one calling the shots. I’m applying something for someone else, and that applications were much more than just an iPhone or Android application, or even a TCP, FTP, STMP, HTTP, HTTP/2, or other protocol application. How you answer this questions depends on where you operate within the machine.

After climbing my way up through the layers of the machine, and finding my way to hatch at the top, finally getting some air in my lungs—-I still do not find myself in a better position. With more knowledge, just comes more concerns. Sometimes I wish I could climb back down to the lower levels, and enjoy the warmth and comfort of the inner workings of the machine. However, no I can’t find comfort in the toil that used to comfort me online late into the night. I can’t ignore what we are applying when we work in the service of the application. We are doing the hard work to mine, develop, integrate, and extract value. We are doing the dirty work of applying the vision of others. We are doing the hard work of laying the digital railroad tracks for the Internet tycoons. We are imposing their vision on the world. The special use or purpose of APIs do not always serve us. The usability and relevance is only minimally focused on us. The close attention and diligence is centered on maximum extraction and value generation for the platform. All wrapped in a computer program, designed for a specific task or use. Obfuscating the true application, with digital eye candy that keeps us always connected, and always open to something new being applied or directed in our life.
]]></content>
    <id>http://apievangelist.com/2019/07/18/what-is-an-appication/</id>
  </entry><entry>
    <title>What Makes You Think Your GraphQL Consumers Will Want To Do The Work</title>
    <link href="http://apievangelist.com/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/"/>
    <updated>2019-07-18T02:00:00Z</updated>
    <content><![CDATA[
Data work is grueling work. I’ve been working with databases since my first job developing student information databases in 1988 (don’t tell my wife). I’ve worked with Cobol, Foxpro, SQL Server, Filemaker, Access, MySQL, PostGres, and now Aurora databases over the last 30 years. I like data. I can even trick myself into doing massive data and schema refinement tasks on a regular basis. It is still grueling work that I rarely look forward to doing. Every company I’ve worked for has a big data problem. Data is not easy work, and the more data you accumulate, the more this work grows out of control. Getting teams of people to agree upon what needs to happen when it comes to schema and data storage, and actually execute upon the work in a timely, cost effective, and efficient way is almost always an impossible task. Which leaves me questioning (again), why GraphQL believers think they are going to be successfully in offshoring the cognitive and tangible work load to understand what data delivers, and then successfully apply it to a meaningful and monetizable business outcome.

Don’t get me wrong. I get the use cases where GraphQL makes sense. Where you have an almost rabid base of known consumers, who have a grasp on the data in play, and possesses an awareness of the schema behind. I’m have made the case for GraphQL as a key architectural component within a platform before. The difference in my approach over the majority of GraphQL believers, is that I’m acknowledging there is a specific group of savvy folks who need access to data, and understand the schema. I’m also being honest about who ends up doing the heavy data lifting here—-making sure this group wants it. However, I have an entirely separate group of users (the majority) who do not understand the schema, and have zero interest in doing the hard work to understand the schema, navigate relationships, and develop queries—-they just want access. Now. They don’t want to have to think about the big picture, they want one single bit of data, or a series of bits.

I’ve worked hard to engage in debate with GraphQL believers, and try to help provide them with advice on their approach. They aren’t interested. They operate within the echo chamber. They see a narrow slice of the API pie, and passionately believe their tool is the one solution. Like many API tooling peddlers who originate from within the echo chamber, they are hyper focused on the technology of managing our data. They think all data wranglers are like them. They think all data-driven companies are like theirs. They do not see the diverse types of API solutions that I see working across companies, institutions, organizations, and government agencies. They are not always aware of the business and political barriers that lie in between a belief in an API tool, and achieving a sustained implementation across the enterprise. They have that aggressive, tenacious startup way of penetrating operations, something that works well within the echo chamber, but it is an approach that will lose strength, and even begin to work against you outside the eco chamber within mainstream business operations.

I definitely see some interesting and useful tooling coming out of the GraphQL community. GraphQL is a tool in my API toolbox right along with REST, Hypermedia, Siren, HAL, JSON API, Alps, JSON Schema, Schema.org, OpenAPI, Postman Collections, Async API, Webhooks, Kafka, NATS, NGINX, Docker, and others. It has a purpose. It isn’t the silver bullet for me getting a handle on large amounts of data. It is one thing I consider when I’m trying to make sense of large data sets, and depending on the platform, the resources I have on staff, and the consumers, or the industry I am operating in–I MAY apply GraphQL. However, the times I will be able to successfully get it in the door, past legal, approved by leadership, accepted by internal developers, and then accepted and applied successfully by external developers, will be much fewer because of the aggressive echo chamber, investor-driven approach, which does not help sell the tool to my enterprise customers who have been investing in evolving their schema, and developing a suite of internal and external APIs over the last 20+ years. You might help me sell to a Silicon Valley savvy company, but you aren’t helping me in the mainstream enterprise.

This post will get the usual lineup of critical Tweets and comments. It’s fine. Once again the fact that I’m trying to help will be lost on believers. You will be more successful if you are honest about how the cognitive and  tangible workload is being shifted here. You are refusing to do the hard work to properly define and organize your schema, and provide meaningful imperative API capabilities that any developer can easily use, over providing a single declarative interface (and some tooling) to empower consumers to make sense of the schema, and access platform capabilities on their own. This works well with folks who are willing to take on the cognitive load of knowing the schema, knowing GraphQL, then are willing to accept the real work of crafting queries to get at what they desire. There are a whole lot of assumptions there that do not apply in all situations. I’d say about 12% of people I’ve worked with in my career would sign up for this. The rest of them, just want to get their task accomplished—get out of their way and give them an interface that is capable of accomplishing it for them.

Nobody wants to do data shit work. If you do, get help. It is a job that is only growing because our ability to generate, harvest, and store data has grown. There is a belief that data provides answers, without being really honest about what it takes to actually clean, refine, and organize the data, let alone any truthfulness regarding whether or not the answers are ever even there after we do invest in doing the dirty data work. Everyone is drowning in data. Everyone is chasing more data. Few are managing it well. This type of environment makes new data tooling very sexy. However, sexy tools rarely change the actual behavior on the ground within the enterprise. People still aren’t going to change behaviors overnight. GraphQL tooling will not solve all of our data problems. Ideally, we would see more interoperability of tooling between GraphQL and other API design, deployment, management, testing, monitoring, mocking, and client tooling. Like we are seeing with IBM Loopback, Postman, and others. Ideally, we would see less rhetoric around GraphQL being the one solution to rule them all. Ideally, we’d see less investor-driven rhetoric, and more real world solutions for applying through the mainstream business world. But, I’m guessing I’ll see the same response I’ve been getting on these posts since 2016. ;-(
]]></content>
    <id>http://apievangelist.com/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/</id>
  </entry><entry>
    <title>The Many Ways In Which APIs Are Taken Away</title>
    <link href="http://apievangelist.com/2019/07/17/the-many-ways-in-which-apis-are-taken-away/"/>
    <updated>2019-07-17T02:00:00Z</updated>
    <content><![CDATA[
APIs are notorious for going away. There are so many APIs that disappear I really stopped tracking on it as a data point. I used keep track of APIs that were shuttered so that I could play a role in the waves of disgruntled pitchfork mobs rising up in their wake–it used to be a great way to build up your Hacker News points! But, after riding the wave a couple hundred waves of APIs shuttering, you begin to not really not give a shit anymore—-growing numb to it all. API deprecation grew so frequent, I wondered why anyone would make the claim that once you start an API you have to maintain it forever. Nope, you can shut down anytime. Clearly.

In the real world, APIs going away is a fact of life, but rarely a boolean value, or black and white. There are many high profile API disappearances and uprising, but there are also numerous ways in which some API providers giveth, and then taketh away from API consumers.:


  Deprecation - APIs disappear regularly both communicated, and not so communicated, leaving consumers scratching their heads.
  Disappear - Companies regularly disappear specific API endpoints acting like they were never there in the first place.
  Acquisition - This is probably one of the most common ways in which high profile, successful APIs disappear.
  Rate Limits - You can always rate limit away users make APIs inaccessible, or barely usable for users, essentially making it go away.
  Error Rates - Inject elevated error rates either intentionally or unintentionally can make an API unusable to everyone or select audience.
  Pricing Tiers - You can easily be priced out of access to an API making it something that acts just like deprecating for a specific group.
  Versions - With rapid versioning, usually comes rapid deprecation of earlier versions, moving faster than some consumers can handle.
  Enterprise - APIs moving from free or paid tier, into the magical enterprise, “call us” tier is a common ways in which APIs go away.
  Dumb - The API should not have existed in the first place and some people just take a while to realize it, and then shut down the API.


I’d say Facebook, Twitter, and Google shutting down, or playing games in any of these areas have been some of the highest profile. The sneaky shuttering of the Facebook Audience Insight API was one example, but didn’t get much attention. I’d say that Parse is one that Facebook handled pretty well. Google did it with Google+ and Google Translate, but then brought back it with a paid tier. LinkedIn regularly locks down and disappears its APIs. Twitter has also received a lot of flack for limiting, restricting, and playing games with their APIs. In the end, many other APIs shutter after waves of acquisitions, leaving us with as my ex-wife says–“nothing nice”!

Face.com, Netflix, 23andMe, Google Search, ESPN, and others have provided us with lots of good API deprecation stories, but in reality, most APIs go aware without much fanfare. You are more likely to get squeezed out by rate limits, errors, pricing, and other ways of consciously making an API unusable. If you really want to understand the scope of API deprecation visit the leading deprecated API directory ProgrammableWeb—-they have thousands of APIs listed that do not exist anymore. In the end it is very difficult to successfully operate an API, and most API providers really aren’t in it for the long haul. The reasons why APIs stay in existence are rarely a direct result of them being directly financially viable. Developers squawk pretty loudly when the free API they’ve been mooching off of disappears, but there are many, many, many other APIs that go away and nobody notices, or nobody talks about. In the world of APIs there are very few things you can count on being around for very long, and you should always have a plan B and C for every API you depend on.
]]></content>
    <id>http://apievangelist.com/2019/07/17/the-many-ways-in-which-apis-are-taken-away/</id>
  </entry><entry>
    <title>Paying for API Access</title>
    <link href="http://apievangelist.com/2019/07/17/paying-for-api-access/"/>
    <updated>2019-07-17T02:00:00Z</updated>
    <content><![CDATA[
APIs that I can’t pay for more access grinds my gears. I am looking at you GitHub, Twitter, Facebook, and a few others. I spend $250.00 to $1500.00 a month on my Amazon bill, depending on what I’m looking to get done. I know I’m not the target audience for all of these platforms, but I’m guessing there is a lot more money on the table than is being acknowledged. I’m guessing that the reason companies don’t cater to this, is that there are larger buckets of money involved in what they are chasing behind the scenes. Regardless, there isn’t enough money coming my way to keep my mouth shut, so I will keep bitching about this one alongside the inaccessible pricing tiers startups like to employ as well. I’m going to keep kvetching about API pricing until we are all plugged into the matrix—-unless the right PayPal payment lands in my account, then I’ll shut up. ;-)

I know. I know. I’m not playing in all your reindeer startup games, and I don’t understand the masterful business strategy you’ve all crafted to get rich. I’m just trying to do something simple like publish data to GitHub, or do some basic research on an industry using Twitter. I know there are plenty of bad actors out there who want also access to your data, but it is all something that could be remedied with a little pay as you go pricing, applied to some base unit of cost applied to your resources. If I could pay for more Twitter and GitHub requests without having to be in the enterprise club, I’d be very appreciative. I know that Twitter has begun expanding into this area, but it is something that is priced out of my reach, and not the simple pay as you go pricing I prefer with AWS, Bing, and other APIs I happily spend money on.

If you can’t apply a unit of value to your API resources, and make them available to the masses in a straightforward way—-I immediately assume you are up to shady tings. Your business model is much more loftier than a mere mortal like me can grasp, let alone afford. I am just an insignificant raw material in your supply chain—-just be quiet! However, this isn’t rocket science. I can’t pay for Google Searches, but I can pay for Bing searches. I can’t pay for my GitHub API calls. I’m guessing at some point I’ll see Bing pricing go out of reach as Microsoft continues to realize the importance of investing at scale in the surveillance economy—-it is how you play in the big leagues. Or maybe they’ll be like Amazon, and realize they can still lead in the surveillance game while also selling things to the us lower level doozers who are actually building things. You can still mine data on what we are doing and establish your behavioral models for use in your road map, while still generating revenue by selling us lower level services.

The problem here ultimately isn’t these platforms. It is me. Why the hell do I keep insisting on using these platforms. I can always extricate myself from them. I just have to do it. I’d much rather just pay for my API calls, and still give up my behavioral data, over straight extraction and not getting what I need to run my business each day. I feel like the free model, with no access to pay for more API calls is a sign of a rookie operation. If you really want to operate at scale, you should be obfuscating your true intentions with a real paid service. If you are still hiding behind the free model, you are just getting going. The grownups are already selling services for a fair price as a front, while still exploiting us at levels we can’t see, so ultimately they can compete with all of us down the road. Ok, in all seriousness, why can’t we get on a common model for defining API access, and setting pricing. I’m really tired of all the games. It would really simplify my life if I could pay for what I use of any resource. I’m happy to pay a premium for this model, just make sure it is within my reach. Thanks.
]]></content>
    <id>http://apievangelist.com/2019/07/17/paying-for-api-access/</id>
  </entry><entry>
    <title>Imperative, Declarative, and Workflow APIs</title>
    <link href="http://apievangelist.com/2019/07/16/imperative-declarative-and-workflow-apis/"/>
    <updated>2019-07-16T02:00:00Z</updated>
    <content><![CDATA[
At every turn in my API work I come across folks who claim that declarative APIs solutions are superior to imperative ones. They want comprehensive, single implementation, do it all their way approaches, over granular, multiple implementation API calls that are well defined by the platform. Declarative calls allow you to define a single JSON or YAML declaration that can then be consumed to accomplish many things, abstracting away the complexity of doing those many things, and just getting it done. Imperative API interfaces require many individual API calls to tweak each and every knob or dial on the system, but is something that is often viewed as more cumbersome from a seasoned integrator, but for newer, and lower level integrators a well designed imperative API can be an important lifeline.

Declarative APIs are almost always positioned against imperative APIs. Savvier, more experienced developers almost always want declarations. Where newer developers and those without a full view of the landscape, often prefer well designed imperative APIs that do one thing well. From my experience, I always try to frame the debate as imperative and declarative where the most vocal developers on the subject prefer to frame it as declarative vs imperative. I regularly have seasoned API developers “declare” that I am crazy for defining every knob and dial of an API resource, without any regard for use cases beyond what they see. They know the landscape, don’t want to be burdened them with having to pull every knob and dial, just give them one interface to declare everything they desire. A single endpoint with massive JSON or YAML post or abstract it all away with an SDK, Ansible, GraphQL, or a Terraform solution. Demanding that a platform meet their needs, without ever considering how more advanced solutions are delivered and executed, or the lower level folks who are on boarding with a platform, and may not have the same view of what is required to operate at that scale.

I am all for providing declarative API solutions for advanced users, however, not at the expense of the new developers, or the lower level ones who spend their day wiring together each individual knob or dial, so it can be used individually, or abstracted away as part of more declarative solution. I find myself regularly being the voice for these people, even though I myself, prefer a declarative solution. I see the need for both imperative and declarative, and understand the importance of good imperative API design to drive and orchestrate quality declarative API implementations, and even more flexible workflow solutions, which in my experience are often what processes, breaks down, and executes most declarations that are fed into a system. The challenge in these discussions are that the people in the know, who want the declarative solutions, are always the loudest and usually higher up on the food chain when it comes to getting things done. They can usually rock the boat, command the room, and dictate how things will get delivered, rarely ever taking into consideration the full scope of the landscape, especially what lower level people encounter in their daily work.

For me, the quality of an API always starts with the imperative design, and how well you think through the developer experience around every dial and knob, which will ultimately work to serve (or not), a more declarative and workflow approach. They all work together. If you dismiss the imperative, and bury it within an SDK, Ansible, or Terraform solution, you will end up with an inferior end solution. They all have to work in concert, and at some point you will have to be down in the weeds turning knobs and dials, understanding what is possible to orchestrate the overall solution we want. It is all about ensuring there is observability and awareness in how our API solutions work, while providing very granular approaches to putting them to work, while also ensuring there are simple, useable comprehensive approaches to moving mountains with hundreds or thousands of APIs. To do this properly, you can’t be dismissing the importance of imperative API design, in the service of your declarative—-if you do this, you will cannibalize the developer experience at the lower levels, and eventually your declarations will become incomplete, clunky, and do not deliver the correct “big picture” vision you will need of the landscape.

When talking API strategy with folks I can always tell how isolated someone is based upon whether they see it as declarative vs imperative, or declarative and imperative. If it is the former, they aren’t very concerned with with others needs. Something that will undoubtedly introduce friction as the API solutions being delivered, because they aren’t concerned with the finer details of API design, or the perspectives of the more junior level, or newer developers to the conversation. They see these workers in the same way they see imperative APIs, something that should be abstracted away, and is of no concern for them. Just make it work. Something that will get us to the same place our earlier command and control, waterfall, monolith software development practices have left us. With massive, immovable, monolith solutions that are comprehensive and known by a few, but ultimately cannot pivot, change, evolve, or be used in new ways because you have declared one or two ways in with the platform should be used. Rather than making things more modular, flexible, and orchestrate-able, where anyone can craft (and declare) a new way of stitching things together, painting an entirely new picture of the landscape with the same knobs and dials used before, but done so in a different way than was ever conceived by previous API architects.
]]></content>
    <id>http://apievangelist.com/2019/07/16/imperative-declarative-and-workflow-apis/</id>
  </entry><entry>
    <title>Hoping For More Investment In API Design Tooling</title>
    <link href="http://apievangelist.com/2019/07/16/hoping-for-more-investment-in-api-design-tooling/"/>
    <updated>2019-07-16T02:00:00Z</updated>
    <content><![CDATA[
I was conducting an accounting of my API design toolbox, and realized it hasn’t changed much lately. It is still a very manual suite of tooling, and sometimes services, that help me craft my APIs. There are some areas I am actively investing in when it comes to API design, but honestly there really isn’t that much new out there to use. To help me remember how we got here, I wanted to take a quick walk through the history of API design, and check in on what is currently available when it comes to investing in your API design process.

API design has historically meant REST. Many folks still see it this way. While there has been plenty of books and articles on API design for over a decade, I attribute the birth of API design to Jakub and Z at Apiary (https://apiary.io). I believe they first cracked open the seed of API design, and the concept API design first. Which is one of the reasons I was so distraught when Oracle bought them. But we won’t go there. The scars run deep, and where has it got us? Hmm? Hmm?? Anyways, they set into motion an API design renaissance which has brought us a lot of interesting thinking on API design, a handful of tooling and services, some expansion on what API design means, but ultimately not a significant amount of movement overall.

Take a look at what AP+I Design (https://www.apidesign.com/) and API Plus (https://apiplus.com/) have done to architecture, API has done for the oil and gas industry (https://www.api.org/), and API4Design has done for the packaging industry (http://api4design.com/)—I am kidding. Don’t get me wrong, good things have happened. I am just saying we can do more. The brightest spot that represents the future for me is over at:


  Stoplight.io - They are not just moving forward API design, they are also investing in the full API lifecycle, including governance. I rarely plug startups, unless they are doing meaningful things, and Stoplight.io is.


After Stoplight.io, I’d say some of the open source tooling that still exists, and has been developed over the last five years, gives me the most hope that I will be able to efficiently design APIs at scale across teams:


  Swagger Editor - I still find myself using this tool for most of my quick API design work.
  API Stylebook - What Arnaud has done reflects what should be standard across all industries.
  OpenAPI GUI - A very useful and progressive OpenAPI GUI editor.
  Apicurio - Another useful GUI OpenAPI editor which shouldn’t be abandoned (cough cough).
  Web Concepts - The building blocks of every API design effort is located here.


I use these tools in my daily work, and think they reflect what I like to see when it comes to API design tooling investment. I would be neglectful if I didn’t give a shout out to a handful of companies doing good work in this area:


  Postman - While not coming from a pure API design vantage point, you can do some serious design work within Postman.
  APIMATIC - It takes good API design to deliver usable SDKs, and APIMATIC provides a nice set of design tooling for their services.
  Reprezen - They have invested heavily in their overall API design workflow and are a key player in the OpenAPI conversation.
  Restlet - My friends over at Restlet are still up to good things even thought they are part of Talend.


While I am not quite ready to showcase and highlight these companies because they don’t always reflect the positive API community influence I’d like to see, I don’t want to leave them out for what they bring to the table:


  SwaggerHub - They are doing interesting things, even if I’m still bummed over the whole Swagger -&amp;gt; OpenAPI bullshit, which I will never forget!
  Mulesoft - Their AnyPoint Designer is worth noting in this discussion. I will leave it there.


While writing this, and taking a fresh look at the search results for API design, editors, and tooling, and looking through my archives, I came across a couple players I have never seen before, either because I haven’t been tuned in, or because they are new:


  OpenAPI Designer - An interesting new player to the OpenAPI editor game.
  Visual Paradigm - Another interesting approach to delivering APIs – we may have to test drive.


I do not want to stop there. Maybe there are other API design toolbox forces at play, influencing, shifting, or directing the API design conversation in other ways, using different protocols and approaches:


  GraphQL - Maybe the GraphQL believers are right? Maybe it is the true solution to designing our APIs? What if? OMG
  Kafka - I think an event-driven approach has shifted the conversation for many, moving classic API design into the background.
  gRPC - Maybe we are moving towards a more HTTP/2 RPC way of delivering APIs and API design is becoming irrelevant.


It is possible that these solutions are siphoning off the conversation in new directions. Maybe the lack of investment is due to other influences that go well beyond the technology, or a specific approach to defining and designing the API problems. What might be some out of the box reasons the API design conversation hasn’t moved forward:


  Investment -  Most of the movement I’ve seen has occurred as well as stagnated because of the direction of venture capital.
  Hard - Maybe it is because it is hard, and nobody wants to do it, making it something that is difficult too monetize.
  Expertise - We need more training and the development of API design expertise to help lead the way, and show us how it is done.
  Bullshit - Maybe API design is bullshit, and we are delusional to think anything will ever become of API design in the first place.
  My Vision - Maybe my vision of API design tooling and services is too high of a bar, or unrealistic in some way.


Ultimately, I think API design is difficult. I think we need more investment in small open source API design tooling that do one thing well. I think other API paradigms will continue to distract us, but also potentially enrich us. I think my vision of API design is obtainable, but out of view of the current investment crowd. I think API design vision is either technical, or it is business. There is very little in between. This is why I highlight Stoplight.io. I feel they are critically thinking about not just API design, but also the rest of the lifecycle. I’d throw Postman into this mix, but they are more API lifecycle than pure design, but I do think they reflect more of the type of services and tooling I’d like to see.

I do not think resource centered web API design is going anywhere. From what I”m seeing, it is going mainstream. It is simple. Low cost. It gets the job done with minimal investment. I think we should invest more into open source API design solutions. I think we need continued investment in API design services like Stoplight.io, Postman, Reprezen, Restlet, and others. I think we need to shift the conversation to also include GraphQL, Kafka, and gRPC. I think investors can do a better job, but I”m not going to hold my breathe there. In the end, I go back to my hand-crafted, artisanal, API design workbench out back where I have cobbled together a few open source tools, on top of a Git foundation. Honestly, it is all I can afford, but I’ll keep playing with other tools I have access to, to see if something will shift my approach.
]]></content>
    <id>http://apievangelist.com/2019/07/16/hoping-for-more-investment-in-api-design-tooling/</id>
  </entry><entry>
    <title>What Is An API Contract?</title>
    <link href="http://apievangelist.com/2019/07/15/what-is-an-api-contract/"/>
    <updated>2019-07-15T02:00:00Z</updated>
    <content><![CDATA[
I am big on regularly interrogating what I mean when I use certain phrases. I’ve caught myself repeating and reusing many hollow, empty, and meaningless phrases over my decade as the API Evangelist. One of these phrases is, “an API contract”. I use it a lot. I hear it a lot. What exactly do we mean by it? What is an API contract, and how is it different or similar to our beliefs and understanding around other types of contracts? Is it truth, or is just a way to convince people that what we are doing is just as legitimate as what came before? Maybe it is even more legitimate, like in a blockchain kind of way? It is an irreversible, unbreakable, digital contract think bro!

If I was to break down what I mean when I say API contract, I’d start with being able to establish to a shared articulation of what an API does. We have an OpenAPI definition which describes the surface area of the request and response of each individual API method being offered. It is available in a machine and human readable format for both of us to agree upon. It is something that both API provider and API consumer can agree upon, and get to work developing and delivering, and then integrating and consuming. An API contract is a shared understanding of what the capabilities of a digital interface are, allowing for applications to be programmed on top of.

After an API contract establishes a shared understanding, I’d say that an API contract helps mitigate change, or at leasts communicates it—-again, in a human and machine readable way. It is common practice to semantically version your API contracts, ensuring that you won’t remove or rename anything within a minor or patch release, committing to only changing things in a big way with each major release. Providing an OpenAPI of each version ahead of time, allowing consumers to review that new version of an API contract before they ever commit to integrating and moving to the next version. Helping reduce the amount of uncertainty that inevitably exists when an API changes, and consumers will have to respond with changes in their client API integrations.

Then I’d say an API contract moves into service level agreement (SLA) territory, and helps provide some guarantees around API reliability and stability. Moving beyond any single API, and also speaking to wider operations. An API contract represents a commitment to offering a reliable and stable service that is secure, observability, and the provider has consumers best interest in mind. A contract should reflect a balance between the provider and the consumer interests, and provide a machine and human readable agreement that reflects the shared understanding of what an API delivers—for an agreed upon price. Any API contract reflects the technical and business details of us doing business in this digital world.

Sadly, an API contract is often wielded in the name of all of these things, but there really is very little accountability or enforcement when it comes to API contracts. It is 100% up to the API provider to follow through and live up to the contract, with very little an API consumer can do if the contract isn’t met. Resulting in many badly behaved API providers, as well as monstrous API consumers. Right now, API contract is thrown around by executives, evangelists, analysts and pundits, more than they are ever actually used to govern what happens on the ground of API operations. Only time will tell if API contracts are just another buzzword that comes and goes, or if they become common place when it comes to doing business online in a digital world.
]]></content>
    <id>http://apievangelist.com/2019/07/15/what-is-an-api-contract/</id>
  </entry><entry>
    <title>My Primary API Search Engines</title>
    <link href="http://apievangelist.com/2019/07/12/my-primary-api-search-engines/"/>
    <updated>2019-07-12T02:00:00Z</updated>
    <content><![CDATA[
I am building out several prototypes for the moving parts of an API search engine I want to build, pushing my usage of APIs.json and OpenAPI, but also trying to improve how I define, store, index, and retrieve valuable data about thousands of APIs through a simple search interface. I’m breaking out the actual indexing and search into their own areas, with rating system being another separate dimension, but even before I get there I have to actually develop the primary engines for my search prototypes, feeding the indexes with fresh signals of where APIs exist across the online landscape. There isn’t an adequate search engine out there, so I’m determined to jumpstart the conversation with an API search engine of my own. Something that is different from what web search engines do, and tailored to the unique mess we’ve created within the API industry.

My index of APIs.json and OpenAPI definitions, even with a slick search interface is just a catalog, directory, or static index of a small piece of the APIs that are out there. I see a true API search engine as three parts


  The Humans Searching for APIs - Providing humans with web application to search for new and familiar APIs.
  The Search Engine Searching For APIs - Ensuring that the search engine is regularly searching for new APIs.
  Other Systems Searching For APIs - Providing an API for other systems to search for new and familiar APIs.


Without the ability for the search engine to actually seek out new APIs, it isn’t a search engine in my opinion—-it is a search application. Without an API for searching for APIs, in my opinion, it isn’t an API search engine. It takes all three of these areas to make an application a true API search engine, otherwise we just have another catalog, directory, marketplace, or whatever you want to call it.

To help me put the engine into my API search engine, I’m starting with a handful of sources I’ve cultivated over the last five years studying the API industry. Providing me with some seriously rich sources of information when it comes to identifying new APIs:


  GitHub Code Search API - I have a vocabulary I use for uncovering artifacts that provide clues to where APIs exist. I can also expand this search to topics, repos, and other dimensions of GitHub search, but I’m going to make sure I’m exhausting and optimizing core search for all I can before I move on.  GitHub provides me with a handful of nuggets when it comes to finding APIs:
    
      Swagger - Machine readable JSON and YAML API definitions.
      OpenAPI - Machine readable JSON and YAML API definitions.
      Postman - Machine readable JSON API definitions.
      API Blueprint - Machine readable markdown definitions.
      RAML - Machine readable YAML definitions.
      HAR - Machine readable traffic snapshots.
      Domains - Domains doing interesting things with APIs.
      People - People doing interesting things with APIs.
    
  
  Bing Web Search API - I use my vocabulary to uncover domains and artifacts that provide clues to where APIs exist. Unlike GitHub, I have to pay for these API calls, so I’m being much more careful about how I spider, and coherently defining the vocabulary I use to uncover this landscape.  Bing provides me with a handful of nuggets when it comes to finding APIs:
    
      Domains - A look into many different domains who are talking APIs in specific verticals.
      GitHub - I find that Bing has some interesting indexes of GitHub — wondering how this will evolve.
    
  
  Twitter - I use my vocabulary to identify new domains where people are talking about APIs, and additional signs of APIs.
    
      Domains - A look into many different domains who are talking APIs in specific verticals.
      People - People doing interesting things with APIs.
    
  
  Domain - I have an exhaustive list of domains to spider for API artifacts, autogenerating OpenAPI index along the way.
    
      URLs - I look for a handful of valuable URLs for use as part of my index.
        
          Twitter - Their Twitter accounts.
          GitHub - Their GitHub accounts.
          LinkedIn - Their LinkedIn accounts.
          Feeds - Their Atom and RSS feeds.
          Definitions - Any API definitions I can find.
          Documentation - Where their documentation is.
          Other Links - I have a long list of other links I look for
        
      
    
  


These four areas represent the primary engines for my API search engine. I currently have these engines running on AWS, processing GitHub and Bing searches, and I’m currently refining my existing Twitter, and relevant domain harvesting engines. While Bing and GitHub are harvesting API signals, and indexing API artifacts like OpenAPI, Postman, and others, I’m going to overhaul my Twitter and domain approaches. Twitter has always been a treasure trove of API signals, but like on GitHub, it is getting harder to obtain these API signals at scale—-as their value increases, things are getting tighter with API access. Also, running a proper domain harvesting campaign across thousands of domains isn’t easy, and will require some refactoring to do at the scale I need for this type of effort.

While I have two of these up and running, indexing new APIs, I still have a significant amount of work to invest in each engine. What I have now is purely of prototype. It will take several cycles until I get each engine performing as desired, and then I’m expecting ongoing tweaks, adjustments, and refinements to be made daily, weekly, and monthly to get the results I’m looking for. There are many areas of deficiency in the API sector that bother me, but not having a simple way to search for new and existing APIs is one are I cannot tolerate any longer. I am happy that ProgrammableWeb has been around all these years, but they haven’t moved the needle in the right way. I also get why people do API marketplaces, but I’m afraid they aren’t moving the needle in a positive direction either. I’d say that APIs.Guru (https://apis.guru/openapi-directory/) is the most progressive vision when it comes to API search in the last decade–with all the innovation supposedly going on, that is just sad. I am guessing that venture capital does not always equal meaningful things we need will get built.
]]></content>
    <id>http://apievangelist.com/2019/07/12/my-primary-api-search-engines/</id>
  </entry><entry>
    <title>Taking A Fresh Look At The Nuance Of API Search</title>
    <link href="http://apievangelist.com/2019/07/11/taking-a-fresh-look-at-the-nuance-of-api-search/"/>
    <updated>2019-07-11T02:00:00Z</updated>
    <content><![CDATA[
I have a mess of APIs.json and OpenAPI definitions I need to make sense of. Something that I could easily fire up an ElasticSearch instance, point at my API “data lake”, and begin defining facets and angles for making sense of what is in there. I’ve done this with other datasets, but I think this round I’m going to go a more manual route. Take my time to actually understand the nuance of API search over other types of search, take a fresh look at how I define and store API definitions, but also how I search across a large volume of data to find exactly the API I am looking for. I may end up going back to a more industrial grade solution in the end, but I am guessing I will at least learn a lot along the way.

I am using API standards as the core of my API index—APIs.json and OpenAPI. I’m importing other formats like API Blueprint, Postman, RAML, HAR, and others, but the core of my index will be APIs.json and OpenAPI. This is where I feel solutions like ElasticSearch might overlook some of the semantics of each standard, and I may not immediately be able to dial-in on the preciseness of the APIs.json schema when it comes to searching API operations, and OpenAPI schema when it comes to searching the granular details of what each individual API method delivers. While this process may not get me to my end solution, I feel like it will allow me to more intimately understand each data point within my API index in a way that helps me dial-in exactly the type of search I envision.

The first dimensions are of my API search index are derived from APIs.json schema properties I use to define every entity within my API search index:


  Name - The name of a company, organization, institution, or government agency.
  Description - The details of what a particular entity brings to the table.
  Tags - Specific tags applied to an entity, or even a collection of entities.
  Kin Rank - What Kin thinks of the entity being indexed with APIs.json.
  Alexa Rank - What Alex thinks of the entity being indexed with APIs.json.
  Common Properties - Using common properties like blog, Twitter, and GitHub.
  Included - Other related APIs that are included within the index.
  Maintainers - Details about who is the maintainer of the API definition.
  API Name - The name of specific API program or project that an entity possesses.
  API Description - The details of a specific API program or project that an entity possesses.
  API Tags - How the individual API program or project is tagged for organization.
  API Properties - The details of specific properties of an API like documentation, pricing, etc.


After indexing the 100K view with APIs.json, providing references to the different layers of API operations, I’m indexing the following OpenAPI schema properties:


  Title - The title of an individual API program or project that an entity possesses.
  Description - The description of an individual API program or project that an entity possesses.
  Domain - The subdomain, or top level domain that an API operates within.
  Version - The version of each individual API.
  Tags - The tags that are applied to a specific API program or project that an entity possesses.
  API Path - The actual path of each API.
  API Method Summary - The summary for an individual API method.
  API Method Description - The description for an individual API method.
  API Method Operation ID - The operation id for an individual API method.
  API Method Query Parameters - The query parameters for an individual API method.
  API Method Headers  - The headers for an individual API method.
  API Method Body - The body of an individual API method.
  API Method Tags - The tags applied to each individual API methods.
  Schema Object Name - The name of each of the schema objects.
  Schema Object Description - The description of each of the schema objects.
  Schema Properties - The properties of each of the schema objects.
  Schema Tags - The tags of each of the schema objects.


These details provide to be by the OpenAPI definition for each API provides me with the long tail of my search, going beyond just the names and description of each API, allowing me to turn on or turn off different facets of the OpenAPI specification when indexing, and delivering search results.  My biggest challenges in building this index center around:


  Completeness - I struggle with being able to invest the resources to properly complete the profile for each API.
  Inconsistency - Navigating the inconsistency of APIs, trying to nail down a single definition across thousands of the is hard.
  Performance - The performance of basic JavaScript search against such a large set of YAML / JSON documents isn’t optimal.
  Accuracy - The accuracy of API methods is difficult to ascertain without actually getting a key and firing up Postman, or other script.
  Up to Date - Understanding when information has become out of date, obsolete, or deprecated is a huge challenge with search.


Right now I have about 2K APIs defined with APIs.json, with a variety of OpenAPI artifacts to support. With more coming in each day through my search engine spiders, trolling GitHub and the open web for signs of API life. I’m working to refine my current index of APIs, making sure they are complete-enough for making available publicly. Then I want to be able to provide a basic keyword search tool, then slowly add each of these individual data points to some sort of advanced filter setting for this search tool. I’m not convinced I’ll end up with a usable solution in the end, but I convinced that I will flesh out more of the valuable data points that exist within an APIs.json and OpenAPI index.

This prototype will at least give me something to play with when it comes to crafting a JavaScript interface for the YAML API index I am publishing to GitHub. I feel like these API search knobs will help me better define my search index, and craft cleaner OpenAPI definitions for use in this API search index. As the index grows I can dial in the search filters, and look for the truly interesting patterns that exist across the API landscape. Then I’m hoping to add an API ratings layer to further help me cut through the noise, and identify the truly interesting APIs amidst the chaos and trash. Not all APIs are created equal and I will need a way to better index, rank, and then ultimately search for the APIs I need. While also helping me more easily discover entirely new types of APIs that I may not notice in my insanely busy world.
]]></content>
    <id>http://apievangelist.com/2019/07/11/taking-a-fresh-look-at-the-nuance-of-api-search/</id>
  </entry><entry>
    <title>Navigating API Rate Limit Differences Between Platforms</title>
    <link href="http://apievangelist.com/2019/07/10/navigating-api-rate-limit-diffs-between-platforms/"/>
    <updated>2019-07-10T02:00:00Z</updated>
    <content><![CDATA[
I always find an API providers business model to be very telling about the company’s overall strategy when it comes to APIs. I’m currently navigating the difference between two big API providers, trying to balance my needs spread across very different approaches to offering up API resources. I’m working to evolve and refine my API search algorithms and I find myself having to do a significant amount of work due to the differences between GitHub and Microsoft Search. Ironically, they are both owned by the same company, but we all know their business models are seeking alignment as we speak, and I suspect my challenges with GitHub API is probably a result of this alignment.

The challenges with integrating with GitHub and Microsoft APIs are pretty straightforward, and something I find myself battling regularly when integrating with many different APIs. My use of each platform is pretty simple. I am looking for APIs. The solutions are pretty simple, and robust. I can search for code using the GitHub Search API, and I can search for websites using the Bing Search API. Both produce different types of results, but what both produce is of value to me. The challenge comes in when I can pay for each API call with Bing, and I do not have that same option with GitHub. I am also noticing much tighter restriction on how many calls I can make to the GitHub APIs. With Bing I can burst, depending on how much money I want to spend, but with GitHub I have no relief value—I can only make X calls a minute, per IP, per user.

This is a common disconnect in the world of APIs, and something I’ve written a lot about. GitHub (Microsoft) has a more “elevated” business model, with the APIs being just an enabler of that business model. Where Bing (Microsoft) is going with a much more straightforward API monetization strategy—pay for what you use. In this comparison my needs are pretty straightforward—-both providers have data I want, and I’m willing to pay for it. However, there is an additional challenge. I’m also using GitHub to manage the underlying application for my project. Meaning after I pull search results from GitHub and Bing, and run them through my super top secret, magical, and proprietary refinement algorithm, I publish the refined results to a GitHub repository, and manage the application in real time using Git, and GitHub APIs—which counts against my API usage.

I used to manage all my static sites and applications 100% on GitHub, using the APIs to orchestrate the data behind each Jekyll-driven site. For the last five years I’ve run API Evangelist, and waves of simple data-driven static applications on GitHub like this. It has been a good ride. A free ride. One I fear is coming to a close. I can no longer deploy static data-driven Jekyll apps on the platform, and confidently manage using the GitHub API anymore. It is something I do not expect to continue getting for free. I’d be happy to pay for my account on a per organization, per repo, and per API call basis. In the end, I’ll probably begin just relying on Git for bulk builds of each application I run on GitHub, and eventually begin migrating them to my own servers, running Jekyll on my own, and custom developing an API for managing the more granular changes across hundreds of micro applications that run on Jekyll using YAML data. It would be nice for GitHub to notice this type of application development as part of their business model, but I’m guessing it isn’t mainstream enough for folks to adopt, and GitHub to cater to.

Getting back to the search portion of this post. I am finding myself writing a scheduling algorithm that spread out my API calls across a 24 hour period. I guess I can also leverage the different GitHub accounts I have access to and maybe spread the harvesting across a couple EC2 instance, but I’d rather just do what Bing offers me, and put in my credit card. I am sure there are other ways I can find to circumvent the GitHub API rate limits, but why? I would rather just be above board and put in my credit card to be able to scale how I’m using the platform. One of the biggest challenges to API integration at scale in the future will be API providers who do not offer relief valves for their consumers. Significantly increasing the investment required to integrate with an API in a meaningful way, making it much more difficult to seamless use just a handful of APIs, let alone hundreds or thousands of them. This challenge is nothing new, and just one example of how the business of APIs can get in the way of the technology of APIs—-slowing things down along the way.
]]></content>
    <id>http://apievangelist.com/2019/07/10/navigating-api-rate-limit-diffs-between-platforms/</id>
  </entry><entry>
    <title>The JSON Schema Tooling In My Life</title>
    <link href="http://apievangelist.com/2019/07/10/the-json-schema-tooling-in-my-life/"/>
    <updated>2019-07-10T02:00:00Z</updated>
    <content><![CDATA[
I am always pushing for more schema order in my life. I spend way too much time talking about APIs, when a significant portion of the API foundation is schema. I don’t have as many tools to help me make sense of my schema, and to improve them as definitions of meaningful objects. I don’t have the ability to properly manage and contain the growing number of schema objects that pop up in my world on a daily basis, and this is a problem. There is no reason I should be making schema objects available to other consumers if I do not have a full handle on what schema objects exist, let alone a full awareness of everything that has been defined when it comes to the role that each schema object plays in my operations.

To help me better understand the landscape when it comes to JSON Schema tooling, I wanted to take a moment and inventory the tools I have bookmarked and regularly use as part of my daily work with JSON Schema:


  JSON Schema Editor - https://json-schema-editor.tangramjs.com/ - An editor for JSON Schema.
  JSON Schema Generator - https://github.com/jackwootton/json-schema - Generates JSON Schema from JSON
  JSON Editor - https://json-editor.github.io/json-editor/ - Generates form and JSON from JSON Schema.
  JSON Editor Online -https://github.com/josdejong/jsoneditor/ - Allows me to work with JSON in a web interface.
  Another JSON Schema Validator (AJV) - https://github.com/epoberezkin/ajv - Validates my JSON using JSON Schema.


I am going to spend some time consolidating these tools into a single interface. They are all open source, and there is no reason I shouldn’t be localizing their operation, and maybe even evolving and contributing back. This helps me understand some of my existing needs and behavior when it comes to working with JSON Schema, which I’m going to use to seed a list of my JSON Schema needs, as drive a road map for things I’d like to see developed. Getting a little more structure regarding how I work with JSON Schema.


  Visual Editor - Being able to visual render and edit JSON Schema in browser.
  YAML / JSON Editor - Being able to edit JSON Schema in YAML or JSON.
  YAML to JSON Converter - Converting my YAML JSON Schema into JSON.
  JSON to YAML Converter - Converting my JSON JSON Schema into YAML.
  JSON to JSON Schema Generator - Generate JSON Schema from JSON object.
  JSON Schema to JSON Generator - Generate a JSON object from JSON Schema.
  JSON Validation Using JSON Schema - Validate my JSON using JSON Schema.
  Enumerators - Help me manage enumerators used across many objects.
  Search - Help me search across my JSON Schema objects, wherever they are.
  Guidance - Help me create better JSON Schema objects with standard guidelines.


This is a good start. If I can bring some clarity and coherence to these areas, I’m going to be able to step up my API design and development game. If I can’t, I’m afraid I’m going to be laying a poor foundation for any API I’m designing in this environment. I mean, how can I consciously provide access to any schema object that I don’t have properly defined, indexed, versioned, and managed? If I don’t fully grasp my schema objects, my API design is going to be off kilter, and most likely be causing friction with my consumers. Granted, I could be offloading the responsibility for making sense of my schema to my consumers using a GraphQL solution, but I’m more in the business of doing the heavy lifting in this area, as it pertains to my business—-I’m the one who should know what is going on with each and every object that passes through my business servers.

I wish there was a schema tool out there to help me do everything that I need. Unfortunately I haven’t seen it. The tooling that has rose up around the OpenAPI specification helps us better invest in schema objects when they are in the service of our API contracts, but nothing just for the sake of schema management. I will keep taking inventory of what tooling is available, as well as what I am needing when it comes to JSON Schema management. Who knows, something might pop up out there on the landscape. Or, more realistically I’m hoping little individual open source solutions keep popping up, allowing me to stitch them together and create the experience I’m looking for. I’m a big fan of this approach, rather than one service provider swooping in and providing the one tool to rule them all, only to get acquired and then be shut down–breaking my heart all over again.
]]></content>
    <id>http://apievangelist.com/2019/07/10/the-json-schema-tooling-in-my-life/</id>
  </entry><entry>
    <title>The Details Of My API Rating Formula</title>
    <link href="http://apievangelist.com/2019/07/09/The-details-of-my-api-rating-formula/"/>
    <updated>2019-07-09T02:00:00Z</updated>
    <content><![CDATA[
Last week I put some thoughts down about the basics of my API rating system. This week I want to go through each of those basics, and try to flesh out the details of how I would gather the actual data needed to rank API providers. This is a task I’ve been through with several different companies, only to be abandoned, and then operated on my own for about three years, only to abandon once I ran low on resources. I’m working to invest more cycles into actually defining my API rating in a transparent and organic way, then applying it in a way that allows me to continue evolving, while also using to make sense of the APIs I am rapidly indexing.

First, I want to look at the API-centric elements I will be considering when looking at a company, organization, institution, government agency, or other entity, and trying to establish some sort of simple rating for how well they are doing APIs. I’ll be the first to admit that ratings systems are kind of bullshit, and are definitely biased and hold all kinds of opportunity for going, but I need something. I need a way to articulate in real time how good of an API citizen an API provider is. I need a way to rank the searches for the growing number of APIs in my API search index. I need a list of questions I an ask about an API in both a manual, or hopefully automated way:


  **Active / Inactive **- APIs that have no sign of life need a lower rating.
    
      HTTP Status Code - Do I get a positive HTTP status code back when I ping their URL(s)?
      Active Blog - Does their blog have regular activity on it, with relevant and engaging content?
      Active Twitter - Is there a GitHub account designated for the API, and is it playing an active role in its operations?
      Active GitHub - Is there a GitHub account designated for the API, and is it playing an active role in its operations?
      Manual Visit - There will always be a need for a regular visit to an API to make sure someone is still home.
    
  
  Free / Paid - What something costs impacts our decision to use or not.
    
      Manual Visit - There is no automated way to understand API pricing.
    
  
  Openness - Is an API available to everyone, or is a private thing.
    
      Manual Review - This will always be somewhat derived from a manual visit by an analyst to the API.
      Sentiment Analysis  - Some sentiment about the openness could be established from analyzing Twitter, Blogs, and Stack Exchange.
    
  
  Reliability - Can you depend on the API being up and available.
    
      Manual Review - Regularly check in on an API to see what the state of things are.
      Sentiment Analysis - Some sentiment about the reliability of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
      Monitoring Feed / Data - For some APIs, monitoring could be setup, but will cost resources to be able to do accurately.
    
  
  Fast Changing - Does the API change a lot, or remain relatively stable.
    
      Manual Review - Regularly check in on an API to see how often things have changed.
      Change Log Feed - Tune into a change log feed to see how often changes are Ade.
      Sentiment Analysis   - Some sentiment about the changes to an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
    
  
  Social Good - Does the API benefit a local, regional, or wider community.
    
      Manual Review - It will take the eye of an analyst to truly understand the social impact of an API.
    
  
  **Exploitative - Does the API exploit its users data, or allow others to do so.
    
      Manual Review - It will take a regular analyst review to understand whether an API has become exploitative.
      Sentiment Analysis - Some sentiment about the exploitative nature of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
    
  
  Secure - Does an API adequately secure its resources and those who use it.
    
      Manual Review - Regularly check in on an API to see how secure things are.
      Sentiment Analysis - Some sentiment about the security of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
      Monitoring Feed / Data - For some APIs, monitoring could be setup, but will cost resources to be able to do accurately, unless provided by provider.
    
  
  Privacy - Does an API respect privacy, and have a strategy for platform privacy.
    
      Manual Review - Regularly check in on an API to see how privacy is addressed, and what steps the platform has been taking to address.
      Sentiment Analysis - Some sentiment about the privacy of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
    
  
  Monitoring - Does a platform actively monitor its platform and allow others as well.
    
      **Manual Review **- Regularly check in on an API to see how secure things are.
      Sentiment Analysis - Some sentiment about the security of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
      Monitoring Feed / Data - For some APIs, monitoring could be setup, but will cost resources to be able to do accurately, unless provided by provider.
    
  
  Observability - Is there visibility into API platform operations, and its processes.
    
      Manual Review - It will always take an analyst to understand observability until there are feeds regarding every aspect of operations.
    
  
  Environment - What is the environment footprint or impact of API operations.
    
      Manual Review - This would take a significant amount of research into where APIs are hosted, and disclosure regarding the environment impact of data centers, and the regions they operate in.
    
  
  Popular - Is an API popular, and something that gets a large amount of attention.
    
      Manual Review - Analysts can easily provide a review of an API to better understand an APIs popularity.
      Sentiment Analysis - Some sentiment about the presence of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
      Twitter Followers** - The number of Twitter followers for an account dedicated to an API provides some data.
      Twitter Mentions - Similarly the number of mentions of an API providers Twitter account provides additional data.
      GitHub Followers - The number of GitHub followers provides another dimension regarding how popular an API is.
      Stack Exchange Mentions - The question and answer site always provides some interesting insight into which APIs are being used.
      Blog Mentions - The number of blog posts on top tech blogs, as well as independent blogs provide some insight into popularity.
    
  
  Value - What value does an API bring to the table in generalized terms.
    
      Manual Review **- The only way to understand the value an API brings to the table is for an analyst to evaluate the resources made available. Maybe some day we’ll be able to do this with more precision, but currently we do not have the vocabulary for describing.
    
  


I am developing a manual questionnaire I can execute against while profiling every API. I have already done this for many APIs, but I’m looking to refine for 2019. I will also be automating wherever I can, leverage other APIs, feeds, and some machine learning to help me augment my heuristic analyst rank with some data driven elements. Some of these will only change when I, or hopefully another analyst reviews them, but some of this will be more dependent on data gathered each month. It will take some time for a ranking system based upon these elements to come into focus, but I’m guessing along the way I”m going to learn a lot, and this list will look very different in twelve months.

Next, I wanted to look at the elements of the rating system itself which I think are essential to the success of an API ranking system based upon the elements above. I’ve seen a number of efforts fail when it comes to indexing and ranking APIs. It is not easy. It is a whole lot of work, without an easy path to monetization like Google established with advertising. Many folks have tried and failed, and I feel like some of these elements will help keep things grounded, and provide more opportunity for success, if not at least sustainability.


  YAML Core - I would define the rating system in YAML.
    
      Rating Formula - The rating formula is machine readable and available as YAML, taking everything listed above and automating the application of it across APIs using a standard YAML definition.
      Rating Results - Publishing a YAML dump of the results of rating for each API provider, also providing a machine readable template for understanding how each API provider is being ranked.
    
  
  GitHub Base - Everything would be in a series of repositories.
    
      GitHub Repo - A GitHub repository is the unit of compute and storage for the rating.
      Git Management - I am using GitHub to apply the rating system across all APIs in my search index.
      GitHub API Management - I am automating the granular editing of the YAML core using the GitHub API.
    
  
  Observable - The entire algorithm, process, and results are open.
    
      Search Transparency - I will be tracking keyword searches, minus IP and user agent, then publishing the results to GitHub as YAML.
      Minimal Tracking - There will be minimal tracking of end-users searching and applying the ranking, with tracking being provider focused.
    
  
  Evolvable - It would be essential to evolve and adapt over time.
    
      Semantic Versioned - The search engine will be semantically versioned, providing a way of understanding it as it evolves.
      YAML - Everything is defined as YAML which is semantically versioned, so nothing is removed or changed until major releases.
    
  
  Weighted - Anyone can weight the questions that matters to them.
    
      Data Points - All data points will have a weight applied as a default, but ultimately will allow end-users to define the weights they desire.
      Slider Interface - Providing end-users with a sliding interface for defining the importance of each data point to them, and apply to the search.
    
  
  Completeness - Not all the profiles of APIs will be as complete as others.
    
      Data Points - The continual addition and evolution of data points, until we find optimal levels of ranking across industries, for sustained periods of time.
    
  
  Ephemeral - Understanding that nothing lasts forever in this world.
    
      Inactive - Making sure things that are inactive reflect this state.
      Deprecation - Always flag something as deprecated, reducing in rank.
      Archiving - Archive everything that has gone away, keeping indexes pure.
    
  
  Community - It should be a collaboration between key entities and individuals.
    
      GitHub - Operate the rating system out in the open on GitHub, leveraging the community for evolving.
      Merge Request - Allow for merge requests on the search index, as well as the ratings being applied.
      Forks - Allow for the workability of the API search, leveraging ranking as a key dimensions for how things can be forked.
      Contribution - Allow for community contribution to the index, and the ranking system, establishing partnerships along the way.
    
  
  Machine Readable - Able for machines to engage with seamlessly.
    
      YAML - Everything is published as YAML to keep things simple and machine readable.
      APIs.json - Follow a standard for indexing API operations and making them available.
      OpenAPI - Follow a standard for indexes the APIs, and making them available.
    
  
  Human Readable - Kept accessible to anyone wanting to understand.
    
      HTML - Provide a simple HTML application for end-users.
      CSS - Apply a minimalist approach to using CSS.
      JavaScript - Drive the search and engagement with client-side JavaScript, powered by APIs.
    
  


This provides me with my starter list of elements I think will set the tone for how this API search engine will perform. Ultimately there will be a commercial layer to how the API search and ranking works, but the goal is to be as transparent, observable, and collaborative around how it all works. A kind of observability that does not exist in web search, and definitely doesn’t in anything API search related. I’ll give it to DuckDuckGo, for being the good guys of web search, which I think provides an ethical model to follow, but I want to also be open with the rating system behind, to avoid some of the illness that commonly exists within rating agencies of any kind.

Next stop, will be about turning the rating elements into a YAML questionnaire that I can begin systematically applying to the almost 2,000 APIs I have in my index. With most of it being a manual process, I need to get the base rating details in place, begin asking them, and then version the questionnaire schema as I work my way through all of the APIs. I have enough experience with profiling APIs to know that what questions I ask, how I ask them, and what data I can gather about API will rapidly evolve once I begin trying to satisfy questions again real world APIs. How fast I can apply my API rating system to the APIs I have indexed, as well as quickly turn around and refresh over time will depend on how much time and resources I am able to manifest for this project. Something that will come and go, as this is just a side project for me, to keep me producing fresh content and awareness of the API space.
]]></content>
    <id>http://apievangelist.com/2019/07/09/The-details-of-my-api-rating-formula/</id>
  </entry><entry>
    <title>Thinking Differently When Approaching OpenAPI Diffs And Considering How To Layer Each Potential Change</title>
    <link href="http://apievangelist.com/2019/07/08/thinking-differently-when-approaching-openapi-diffs-and-considering-how-to-layer-each-potential-change/"/>
    <updated>2019-07-08T02:00:00Z</updated>
    <content><![CDATA[
I have a lot of OpenAPI definitions, covering about 2,000 separate entities. For each entity, I often have multiple OpenAPIs, and I am finding more all the time. One significant challenge I have in all of this centers around establishing a master “truth” OpenAPI, or series of definitive OpenAPIs for each entity. I can never be sure that I have a complete definition of any given API, so I want to keep vacuuming up any OpenAPI, Swagger, Postman, or other artifact I can, and compare it with the “truth” copy” I have on indexed. Perpetually layering the additions and changes I come across while scouring the Internet for signs of API life. This perpetual update of API definitions in my index isn’t easy, and any tool that I develop to assist me will be in need constant refinement and evolution to be able to make sense of the API fragments I’m finding across the web.

There are many nuances of API design, as well as the nuances of how the OpenAPI specification is applied when quantifying the design of an API, making the process of doing a “diff” between two OpenAPI definitions very challenging. Rendering common “diff” tools baked into GitHub, and other solutions ineffective when it comes to understanding the differences between two API definitions that may represent a single API. These are some of the things I’m considering as I’m crafting my own OpenAPI “diff” tooling:


  Host - How the host is stored, defined, and applied across sandbox, production, and other implementations injects challenges.
  Base URL - How OpenAPI define their base url versus their host will immediately cause problems in how diffs are established.
  Path - Adding even more instability, many paths will often conflict with host and base URL, providing different fragments that show as differences.
  Verbs - Next I take account of the verbs available for any path, understanding what the differences are in methods applied.
  Summary - Summaries are difficult to diff, and almost always have to be evaluated and weighted by a human being.
  Description -  Descriptions are difficult to diff, and almost always have to be evaluated and weighted by a human being.
  Operation ID - These are usually autogenerated by tooling, and rarely reflect a provider defined standard, making them worthless in “diff”.
  Query Properties - Evaluating query parameters individually is essential to a granular level diff between OpenAPI definitions.
  Path Properties - Evaluating path parameters individually is essential to a granular level diff between OpenAPI definitions.
  Headers - Evaluating headers individually is essential to a granular level diff between OpenAPI definitions.
  Tags - Most providers do not tag their APIs, and they are often not included, and rarely provide much value when applying a “diff”.
  **Request Bodies - Request bodies provide a significant amount of friction for diffs depending on the complexity and design of an API.
  Responses - Responses often provide an incomplete view of an API, and rarely are robust enough to impact the “diff” view.
  Status Codes - Status codes should be evaluated on an individual basis, providing a variety of ways to articulate these statuses.
  Content Types - Content types these days are often application/json, but do provide some opportunities to define unique characteristics.
  Schema Objects - Schema is often not defined, and rarely used as part of a diff unless OpenAPIs are generated from log, HAR, and other files.
  Schema Properties - Schema properties are rarely present in OpenAPIs, making them not something that comes  up on the “diff” radar.
  Security Definitions - Security definitions are the holy grail of automating API indexing, but are rarely present in OpenAPI, and only in Postman Collections.
  References - The use of $ref, or absence of $ref and doing everything inline poses massive challenges to coherently considering “diff” results.
  Scope - The size of the OpenAPI snippet being applied as part of a “diff” helps narrow what needs to be considered by a human or machine.


This reflects the immediate concerns I have approaching the development of a custom “diff” tool for OpenAPI. First I am just trying to establish a strategy for stripping back the layers of OpenAPI definitions, and established a sort of layered user interface for me to manually accept or reject changes to an OpenAPI. An interface that will also allow me to define a sort of rules vocabulary for increasingly automating the decision making process. I’d love it if eventually the diff tool would show me just a single diff, present me with the change it thinks I should make, and allow me to just agree and move to the next “diff”. I have a lot of work to get things to this point.

Like API search, I feel like API diff is something I have to reduce to its basics, and then fumble my way towards finding an acceptable solution. I don’t feel there is a single “diff” tool for JSON or YAML that will have the eye that I demand for analyzing, presenting, and either manually or automatically merging a diff. Like the other layers of my API search engine, diff is something I need to think through, iterate upon, and repeat until I come up with something that helps me merge “diffs” efficiently across thousands of APIs, and hopefully eventually automates and abstract away the most common differences between the APIs that I am spidering and indexing. Like every other area it is something I’m only working on when I have time, but something I will eventually come out the other end with a usable OpenAPI diff tool, that can help me make sense of all the API definitions I’m bombarded with on a daily basis.
]]></content>
    <id>http://apievangelist.com/2019/07/08/thinking-differently-when-approaching-openapi-diffs-and-considering-how-to-layer-each-potential-change/</id>
  </entry><entry>
    <title>Why The Open Data Movement Has Not Delivered As Expected</title>
    <link href="http://apievangelist.com/2019/07/05/why-the-open-data-movement-has-not-delivered-as-expected/"/>
    <updated>2019-07-05T02:00:00Z</updated>
    <content><![CDATA[
I was having a discussion with my friends working on API policy in Europe about API discovery, and the topic of failed open data portals came up. Something that is a regular recurring undercurrent I have to navigate in the world of APIs. Open data is a subset of the API movement, and something I have first-hand experience in, building many open data portals, contributing to city, county, state, and federal open data efforts, and most notably riding the open data wave into the White House and working on open data efforts for the Obama administration.

Today, there are plenty of open data portals. The growth in the number of portals hasn’t decreased, but I’d say the popularity, utility, and publicity around open data efforts has not lived up to the hype. Why is this? I think there are many dimensions to this discussion, and few clear answers when it comes to peeling back the layers of this onion, something that always makes me tear up.


  Nothing There To Begin With - Open data was never a thing, and never will be a thing. It was fabricated as part of an early wave of the web, and really never got traction because most people do not care about data, let alone it being open and freely available.
  It Was Just Meant To Be A Land Grab - The whole open data thing wasn’t about open data for all, it was meant to be open for business for a few, and they have managed to extract the value they needed, enrich their own datasets, and have moved on to greener pastures (AI / ML).
  No Investment In Data Providers - One f the inherent flaws of the libertarian led vision of web technology is that government is bad, so don’t support them with taxes. Of course, when they open up data sets that is goo for us, but supporting them in covering compute, storage, bandwidth, and data refinement or gathering is bad, resulting in many going away or stagnating.
  It Was All Just Hype From Tech Sector - The hype about open data outweighs the benefits and realities on the ground, and ultimately hurt the movement with unrealistic expectations, setting efforts back many years, and are now only beginning to recover now that the vulture capitalists are on to other things.
  Open Data Is Not Sexy - Open data is not easy to discover, define, refine, manage, and maintain as something valuable. Most government, institutions, and other organizations do have the resources to do properly, and only the most attractive of uses have the resources to pay people to do the work properly, incentivizing commercial offerings over the open, and underfunded offerings.
  Open Data Is Alive and Well - Open data is doing just fine, and is actually doing better, now that the spotlight is off of them. There will be many  efforts that go unnoticed, unfunded, and fall into disrepair, but there will also be many fruitful open data offerings out there that will benefit communities, and the public at large, along with many commercial offerings.
  Open Data Will Never Be VC Big - Maybe open data share the spotlight because it just doesn’t have the VC level revenue that investors and entrepreneurs are looking for. If it enriches their core data sets, and can be used to trying their machine learning models, it has value as a raw material, but as something worth shining a light on, open data just doesn’t rise to the scope needed to be a “product” all by itself.


My prognosis on why open data never has quite “made it”, is probably a combination of all of these things. There is a lot of value present in open data as a raw material, but a fundamental aspect of why data is “open”, is so that entrepreneurs can acquire it for free. They aren’t interested in supporting city, county, state, and federal data stewards, and helping them be successful. They just want it mandated that it is publicly available for harvesting as a raw material, for use in the technology supply chain. Open data primarily was about getting waves of open data enthusiasts to do the heavy lifting when it came to identifying where the most value raw data sources exist.

I feel pretty strong that we were all used to initiate a movement where government and institutions opened up their digital resources, right as this latest wave of information economy was peaking. Triggering institutions, organizations, and government agencies to bare fruit, that could be picked by technology companies, and used to enrich their proprietary datasets, and machine learning models. Open doesn’t mean democracy, it mostly means for business. This is the genius of the Internet evolution, is that it gets us all working in the service of opening things up for the “community”. Democratizing everything. Then once everything is on the table, companies grab what they want, and show very little interest in giving anything back to the movement. I know I have fallen for several waves of this ver the last decade.

I think open data has value. I think community-driven, standardized sets of data should continue to be invested in. I think we should get better at discovery mechanisms involving how we find data, and how we enable our data to be found. However, I think we should also recognize that there are plenty of capitalists who will see what we produce as a valuable raw resource, and something they want to get their hands on. Also, more importantly, that these capitalists are not in the businesses of ensuring this supply of raw resource continues to exist in the future. Like we’ve seen with the environment, these companies do not care about the impact their data mining has on the organizations, institutions, government agencies, and communities that produced them, or will be impacted when efforts go unfunded, and unsupported. Protecting our valuable community resources from these realities will not be easy as the endless march of technology continues.
]]></content>
    <id>http://apievangelist.com/2019/07/05/why-the-open-data-movement-has-not-delivered-as-expected/</id>
  </entry><entry>
    <title>API Interoperability is a Myth</title>
    <link href="http://apievangelist.com/2019/07/03/api-interoperatibility-is-a-myth/"/>
    <updated>2019-07-03T02:00:00Z</updated>
    <content><![CDATA[
There are a number of concepts we cling to in the world of APIs. I’ve been guilting of inventing, popularizing, and spreading many myths in my almost decade as the API Evangelist. One of them that I’d like to debunk and be more realistic about is when it comes to API interoperability. When you are focused on just the technology of APIs, as well as maybe the low-level business of APIs, you are an API interoperability believer. Of course everyone wants API interoperability, and that all APIs should work seamlessly together. However, if you at all begin operating at the higher levels of the business of APIs, and spend any amount of time studying the politics of why and how we do APIs at scale, you will understand that API interoperability is a myth.

This reality is one of the reasons us technologists who possess just a low-level understanding of how the business behind our tech operation, are such perfect tools for the higher level business thinkers, and people who successfully operate and manipulate at the higher levels of industries, or even at the policy level. We are willing to believe in API interoperability, and work to convince our peers that it is a thing, and we all work to expose, and open up the spigots across our companies, organizations, institutions, and government agencies. Standardized APIs and schema that play nicely with each other are valuable, but only within certain phases of a companies growth, or as part of a myth-information campaign to convince the markets that a company is a good corporate citizen. However, once a company achieves dominance, or the winds change around particular industry trends, most companies just want to ensure that all roads lead towards their profitability.

Believing in API interoperability without a long term strategy means you are opening up your company to value extraction by your competitors. I don’t care how good your API management is, if your API makes it frictionless to integrate with because you use a standard format, it just means that the automated value harvesters of companies will find it frictionless to get at what you are serving, and more easily weaponize and monetize your digital resources. It pains me to say this, but it is the reality. If you are in the business of making your API easier to connect with, you are in the business of making it easier for your competitors to extract value from you. Does this mean we shouldn’t do APIs, and make them interoperable? No, but it does mean that we shouldn’t be ignorant of the cutthroat, exploitative, and aggressive nature of businesses that operate within our industries. Does it mean we shouldn’t invest in standards? No, but we should be aware that not every company sitting at the table shares the same interests as us, and could be playing a longer game that involves lock-in, proprietary nuances, or even slowing the standards movement in their favor.

I think that storage APIs are a great example of this. In the early days of cloud storage APIs, I remember everyone saying they were AWS S3 compatible—even Google and Microsoft highlighted this. However, as things have progressed, everyone adds their own tweaks, changes, and nuances that make it much harder to get your terabytes of data off their platform. It was easy to get it in, and keep it synced across your providers, but eventually the polarities change, and all roads lead to lock-in, and are not in the service of interoperability. This is just businesses. I’m not condoning it, I am only repeating what my entrepreneurial friends tell me. If you make it easy for your customers to use other services, you are eroding their loyalty to your brand, and eventually they will leave. So you have to make it harder for them over time. Just incrementally. Forget to grease the door hinges. Change the way the doorknob turns. Make the door narrower. Stop following the international or local standards for how you design a door, call it innovation, and reduce the ways in which your customers can easily get out the door.

I call this the Hotel California business model. You can check-in, but you can never leave. Wrap it all in a catchy tune, even call yourself a hotel, but in reality you’ve gotten hooked a technological myth, and you will never actually be able to ever find the door. Anyways, c’mon, I fucking hate the Eagles, don’t we just have some Credence we could play? Anyways, I got off track. Nobody, but us low-level delusional developers believe in API interoperability. The executives don’t give a shit about it. Unless it supports the latest myth-information campaign. In the long run, nobody wants their APIs to work together, we all just want EVERYONE to use OUR APIs! Sure, we also want to be seen as working together on standards groups, and that our APIs are the the standard EVERYONE should follow, ensuring interoperability with us at the center. But, nobody truly believes in API interoperability. If you do, I recommend you do some soul searching regarding where you exist in the food chain. I’m guessing you are a lower level pawn, doing the work of the puppet master in your industry. This is why you won’t find me on many standards bodies, or me blindly pushing interoperability at scale. It doesn’t exist. It isn’t real. Let’s get to work on more meaningful policy level things that will help shape the industry.
]]></content>
    <id>http://apievangelist.com/2019/07/03/api-interoperatibility-is-a-myth/</id>
  </entry><entry>
    <title>Your API and Schema Is That Complex Because You Have Not Done The Hard Work To Simplify</title>
    <link href="http://apievangelist.com/2019/07/02/your-api-and-schame-is-that-complex-because-you-have-not-done-the-hard-work-to-simplify/"/>
    <updated>2019-07-02T02:00:00Z</updated>
    <content><![CDATA[
I find myself looking at a number of my more complex API designs, and saying to myself, “this isn’t complicated because it is hard, it is complicated because I did not spend the time required to simplify it appropriately”. There are many factors contributing to this reality, but I find that more often than not it is because I’m over-engineering something, and I am caught up in the moment focusing on a purely computation approach, and not considering the wider human, business, and other less binary aspects of delivering APIs.

While I am definitely my own worst enemy in many API deliver scenarios, I’d say there are a wide range of factors that are influencing how well, or poorly that I design my API resources, with just a handful of them being:


  Domain - I just do not have the domain knowledge required to get the job done properly.
  Consumer - I just do not have the knowledge I need of my end consumers to do things right.
  Bandwidth - I just do not have the breathing room to properly sit down and make it happen.
  Narcissism - I am the best at this, I know what is needed, and I deem this complexity necessary.
  Lazy - I am just too damn lazy to actually dig in and get this done properly in the first place.
  Caring - I just do not give a shit enough to actually go the extra distance with API design.
  Dumb - This API is dumb, and I really should not be developing it in the first place.


These are just a few of the reasons why I settle for complexity over simplicity in my API designs. It isn’t right. However, it seems to be a repeating pattern in some of my work. It is something that I should be exploring more. For me to understand why my work isn’t always of highest quality possible I need to explore each of these areas and understand where I can make improvements, and which areas I cannot. Of course I want to improve in my work, and reach new heights with my career, but I can’t help be dogged by imperfections that seem out of my control…or are they?

I have witnessed API simplicity. APIs that do powerful and seemingly complicated things, but in an easy and distilled manner. I know that it is possible to do, but I can’t help but feel that 90% of my API designs fall short of this reality. Some get very close, while others look like amateur hour. One thing is clear. If I’m going to deliver high quality simple and intuitive APIs, I’m going to have to work very hard at it. No matter how much experience I have, I can only improve the process so much, and there will always be a significant amount of investment required to take things to the next level.
]]></content>
    <id>http://apievangelist.com/2019/07/02/your-api-and-schame-is-that-complex-because-you-have-not-done-the-hard-work-to-simplify/</id>
  </entry><entry>
    <title>The Basics of My API Rating Formula</title>
    <link href="http://apievangelist.com/2019/07/01/the-basics-of-my-api-ratings-formula/"/>
    <updated>2019-07-01T05:00:00Z</updated>
    <content><![CDATA[
I have been working on various approaches to rating APIs since about 2012. I have different types of algorithms, even having invested in operating one from about 2013 through 2016, which I used to rank my daily intake of API news. Helping me define what the cream on top of each industry being impacted by APIs, while also not losing site of interesting newcomers to the space. I have also had numerous companies and VCs approach me about establishing a formal API rating system—many of whom concluded they could do fairly easily and went off to try, then failed, and gave up. Rating the quality of APIs is subjective and very hard.

When it comes to rating APIs I have a number of algorithms to help me, but I wanted to step back and think of it from a more simpler human vantage point, and after establishing a new overall relationship with the API industry. What elements do I think should exist within a rating system for APIs:


  Active / Inactive - APIs that have no sign of life need a lower rating.
  Free / Paid - What something costs impacts our decision to use or not.
  Openness - Is an API available to everyone, or is a private thing.
  Reliability - Can you depend on the API being up and available.
  Fast Changing - Does the API change a lot, or remain relatively stable.
  Social Good - Does the API benefit a local, regional, or wider community.
  Exploitative - Does the API exploit its users data, or allow others to do so.
  Secure - Does an API adequately secure its resources and those who use it.
  Privacy - Does an API respect privacy, and have a strategy for platform privacy.
  Monitoring - Does a platform actively monitor its platform and allow others as well.
  Observability - Is there visibility into API platform operations, and its processes.
  Environment - What is the environment footprint or impact of API operations.
  Popular - Is an API popular, and something that gets a large amount of attention.
  Value - What value does an API bring to the table in generalized terms.


These are just a handful of the most relevant data points I’d rank APIs on. Using terms that almost anyone can understand. You might not fully understand the technical details of what an API delivers, but you should be able to walk through these overall concepts that will impact your company, organization, institution, or government agency when putting an API to work. Now the more difficult questions around this API rating system, is how do you make it happen. Gathering data to satisfy all of these areas is easier said than done.

Getting the answer to some of these question will be fairly easy, and we can come up with some low tech ways to handle. However, some of them are near impossible to satisfy, let alone do it continually over time. It isn’t easy to gather the data needed to answer these questions. In my opinion no single entity can deliver what is needed, and it will ultimately need to be a community thing if we are going to provide any satisfactory answer to these API rating questions, regularly satisfy them on a regular schedule, and then honestly acknowledge when an API goes dormant, or away altogether. To properly do this, it would have to be done out in the open, and a few things I’d consider introducing would be:


  YAML Core - I would define the rating system in YAML.
  YAML Store - I would store all data gathered as YAML.
  GitHub Base - Everything would be in a series of repositories.
  Observable - The entire algorithm, process, and results are open.
  Evolvable - It would be essential to evolve and adapt over time.
  Weighted - Anyone can weight the questions that matters to them.
  Completeness - Not all the profiles of APIs will be as complete as others.
  Ephemeral - Understanding that nothing lasts forever in this world.
  Collaborative - It should be a collaboration between key entities and individuals.
  Machine Readable - Able for machines to engage with seamlessly.
  Human Readable - Kept accessible to anyone wanting to understand.


I will stop there. I have other criteria I’d like to consider when defining a ranking system. Specifically, a public ranking system, for public APIs. Actually, I’d consider a company, organizations, institution, and government agency ranking system with an emphasis on APIs. Do they do APIs or not? If they do, how many of the questions can we answer pretty easily, with as few resources as possible, while being able to reliably measure using these data points on into the future. It is something we need. It is something that will be very difficult to setup, taking a significant amount of investment over time. It will also rely on the contribute of other entities and individuals. It is something that won’t be easy to make happen. That shouldn’t stop us from doing it.

That is the basics of my API rating formula. Version 2019. This is NOT an idea for a startup. There is revenue to be generated here, but not if approached through the entrepreneurial playbook. It will fail. I’ve seen it happen over and over. This is a request for the right entities and individuals to come together and make it happen. It is dumb that there is no way of understanding which APIs are good or bad. It is also dumb that there is no healthy and active API search engine after APIs having gone so mainstream—another sign of the ineffectiveness of doing not just API rating, but API discovery as a venture backed startup. Sorry, there are just some infrastructural things we’ll all need to invest in together to make this all work at scale. Otherwise we are going to just end up with a chaotic, unreliable network of API-driven services behind our applications. If you’d like to talk API ratings with me, drop me an email at info@apievangelist.com, or tweet at @apievangelist.
]]></content>
    <id>http://apievangelist.com/2019/07/01/the-basics-of-my-api-ratings-formula/</id>
  </entry><entry>
    <title>The Complexity of API Discovery</title>
    <link href="http://apievangelist.com/2019/07/01/the-complexity-of-api-discovery/"/>
    <updated>2019-07-01T02:00:00Z</updated>
    <content><![CDATA[
I can’t get API discovery out of my mind. Partly because I am investing significant cycles in this area at work, but it is also something have been thinking about for so long, that it is difficult to move on. It remains one of the most complex, challenging, and un-addressed aspects of the way the web is working (or not working) online today. I feel pretty strongly that there hasn’t been investment in the area of API discovery because most technology companies providing and consuming APIs prefer things be un-discoverable, for a variety of conscious and un-conscious reasons behind these belief systems. 

What API Discovery Means? Depends On Who You Are…
One of the reasons that API discovery does not evolve in any significant ways is because there is not any real clarity on what API discovery is. Depending on who you are, and what your role in the technology sector is, you’ll define API discovery in a variety of ways. There are a handful of key actors that contribute to the complexity of defining, and optimizing in the area of API discovery.
 

  Provider - As an API provider, being able to discover your APIs, informs your operations regarding what your capabilities are, building a wider awareness regarding what a company, organization, institution, or government agency does, helping eliminate inefficiencies, and allows for more meaning decisions to be made at run-time across operations.
  Consumer - What you need as an internal consumer of APIs, or maybe a partner, or 3rd party developer will significantly shift the conversation around what API discovery means, and how it needs to be “solved”. There is another significant dimension to this discussion, separating human from other system consumption, further splintering the discussion around what API discovery is when you are a consumer of APIs.
  Analyst - As an analyst for specific industries, or technology in general, need to understand the industries they are watching, and how API tooling is being applied, helping them develop an awareness of what is happening, and understand what the future might hold.
  Investor - A small number of investors are in tune with APIs, and even grasp what API discovery means to their portfolio, and the industries they are investing in, generally being unaware of how API discovery will set the tone for how markets behave–providing the nutrients (or lack of) markets need to understand what is happening across industries.
  Journalist - Most journalists grasp the importance of Facebook and Twitter, but only a small percentage understand what APIs are, and how ubiquitous they are, and the benefits they bring to the table when it comes to helping them in their investigations, and research, let alone how it can benefit them in their work when it comes to syndication and exposure for their work–making API discovery pretty critical to what they do.
  University - I am seeing more universities depending on accessible APIs when it comes to research, and an increase in the development of API related curriculum–just as important, I am also seeing the increased development of open source API tooling out of university environments.
  Government - APIs will play an increasing role in regulation, taxation, and government funded / implemented research, making API discovery key to finding the data they need, and being able to have their finger on the pulse of what citizens and businesses are up to on any given week.
  End-User - Last, but definitely not least, the end-user should b e concerned with API discovery, and using it as a low water mark for where they should be doing business, and requiring that the platforms they use have APIs, and have their best interests in mind when allowing for 3rd party access to their data.


How APIs Are Discovered?
Across these different views of the API discovery landscape, there are a variety of ways in which APIs are discovered, and made discoverable. Providing some formal, and some not so formal ways to define the the API landscape, and develop an awareness of the API ecosystems that have risen up within different industries, within institutions and government agencies. These are the ways I am focused on finding APIs, and making APIs findable, inside and outside the firewall.

Our motivations for finding APIs inside or outside the firewall are not always in sync with our motivations for having our APIs be found. This affects our view of the landscape when it comes to how hard API discovery is, and what the possible solutions for it will be. I find people’s view on API discovery to be very relative to their view of the landscape, and very few people in the API sector travel widely enough, exchange ideas externally enough, and lift themselves up high enough to be able to understand API discovery well enough to provide the right tools and services to move the conversation forward.

Within the Firewall
API discovery with the firewall is a real struggle. Most companies I know do not know where all of their APIs are. There is no single up to date truth of where each API is, what it does, let alone the machine readable details of what it delivers. These are a few of the ways in which I have seen groups tackle API discovery within the firewall: 


  Directories - Many employ a catalog, directory, or database of APIs, micro services, and other relevant solutions.
  Git Repositories - Git within the enterprise is a very viable way to manage a large volume of artifacts you can use for discovery.
  Word of Mouth - Talking to people is always a great way to understand what APIs exist across the enterprise landscape.
  Documentation - The documentation for APIs often become the focal point of API discovery because it can be searched.
  Log Files - Harvest what is actually coming across the wire, parse APIs that are in use, and documenting them in some way.


The biggest challenge with API discovery within the firewall is having dedicated resources to keep up with the discovery, documenting, while also keeping catalogs, repositories, documentation, and other key sources of API discovery information up to date. In my experience, rarely do teams ever get the budget to properly invest in API discovery, with things often done as part of skunk works, or in every day operations—as teams can–which is never enough.

Outside the Firewall
This is the aspect of API discovery that gets the most attention when it comes to API discovery. These are the API rock stars, directors, marketplaces, and API showcasing that occurs across tech blogs. API discovery outside the firewall has access to far more tools and services to leverage when getting the word out, or helping you find what you are looking for. The challenge becomes, like most other things on the open web, how do you cut through the noise, and get your APIs found, or find the APIs you are looking for. 


  Directories - You use some of the existing API directories out there. Providing you access to a small subsection of the APIs that these operators can find, and manually publish to their API catalogs. There really isn’t a single source of truth when it comes to API directories, but there are some that have been around longer than others.
  Marketplaces - There have been numerous waves of investment into API marketplaces, trying to centralize the discovery and integration with APIs, hoping to simplify APIs enough that consumers use you as their doorway to the API world—giving the API marketplace provider a unique look at how APIs are consumed.
  Documentation - Public API documentation is how your APIs will be found using Google, which is the number one way people are going to find your API—using a Google search. I’m surprised that Google hasn’t tackled the issue of API discovery already, but I’m guessing they haven’t deemed it valuable enough to tackle, and will most likely swoop in and take dominate once some smaller providers plant the seeds.
  Definitions - API discovery has gotten easier, and more robust with the introduction of API definitions like Swagger, OpenAPI, API Blueprint, RAML, Postman collections and other machine readable formats. Going beyond just static or even dynamic API documentation, and providing a machine readable artifact that can be indexed and used to drive API search.
  Domains - You can drive a lot of interesting API discovery using domains, and building indexes of different types of domains, then conduct several types of searches to see if they have any APIs. Using search engine APIs, Twitter, Github, and other social media APIs to find APIs across the landscape—using domains as the anchor for refining and making API discovery queries more precise.
  Scraping - If you have the URL for an companies, organization, institution, or government agencies API documentation page you can also scrape that page, or pages for more information about how any API operations, and what it does—adding to the index of APIs to be search against.
  Search Engines - While Google doesn’t have a good API anymore, Bing and DuckDuckGo do.It is easy to build up an index of queries to search Bing to uncover potential domains, documentation, definitions, and other artifacts to enrich an API discovery index. With the right key phrase glossary, you can quickly automate a pretty comprehensive search for new APIs.
  GitHub - The social coding platform is a rich one of API related data. Similar to search engines you can easy build a search query vocabulary to uncover a number of domains, documentation, definitions, and other artifacts to enrich an API discovery index.
  Integrated Development Environment (IDE) - The IDE is an untapped market when it comes to helping developers find APIs, and to help API providers reach developers. Microsoft has been increasing API discovery features, while also being extended with plugins by the community. There is no universal API search engine baked into the top IDEs, a definite missed opportunity.
  News - It is pretty easy to harvest press releases, blog posts, and other news sources and use them as rich sources of information for new APIs. Helping seed a list of potential domains to look for documentation and other API artifacts. Publishing a press release, or posting to their blog is the most common way that API providers get the word out, unfortunately it tends to be the only thing they do.
  Tweets - Twitter is also a rich source of information about APIs, with a variety of accounts, hashtags, and other ways to make queries for new APIs more precise. Using the social media platform as.a way of tuning into potential new APIs, as well as the activity around existing APIs in the index.
  LinkedIn - More business, institutions, and government entities are talking about their APIs on LinkedIn, publishing posts, job listings, and other API related goings on. Making it a pretty rich way to find new APIs, and companies who are embarking, or making their way along their API journey.
  Security Alerts - Sadly, security alerts is one way I learn about companies and their APIs—when they become un-secure. Providing information about API providers who operate in the shadows, as well as more information to index when actually rating APIs in the index, but that is another story.


Even once you discover the APIs you are looking for, often times more context is required, and you may or may not have the time or expertise to assess what an API delivers, and what it will take to get up and running with an API.


  Documentation - Where the documentation resides for the API.
  Signup - Where a user can signup to use an API.
  Pricing - What is the pricing for using an API.
  Support - Where do you get support if you need help.
  Terms of Service - Where do I find the legalize behind AP operations.


These are just five of the most common questions APIs consumers are going to ask when they are looking at an API, and would benefit from direct links to these essential building blocks as part of any API search results.  I have over a hundred questions that I like to ask of an API as I’m reviewing, which all reflect what a potential API consumer will be asking when they come across an API out in the wild.

Four Primary Dimensions Of Complexity
I’d say that these are the four main dimensions of complexity I see out there when it comes to API discovery, which makes it really hard to provide a single API discovery solution, and why there hasn’t been more investment in this area. It is difficult to make sense of APIs, and what people are looking for. It is hard to get all API providers on the same page when it comes to investing in API discovery as part of their regular operations. API discovery is something I’ll keep investing in, but it is something that will need wider investment from the community, as well as some bigger players to step up and help move the conversation forward.

Other than API marketplaces, and the proliferation of API definitions, I haven’t seen any big movements in the API discovery conversation. We still have ProgrammableWeb. We still have Google. Not much more. With the number of APIs growing, this is only going to become more of a pain point. I’m interested in investing in API discovery not because I want to help everyone find APIs, or have their APIs found. Im more interested in shining a light on what is going on. I am looking to understand the spread of APIs across the digital landscape, and better see where they are pushing into our physical worlds. My primary objective is not to make sure all APIs are found so they can be used. My primary objects is to make sure all APIs are found so we have some observability into how the machine works.
]]></content>
    <id>http://apievangelist.com/2019/07/01/the-complexity-of-api-discovery/</id>
  </entry><entry>
    <title>Why Schema.org Does Not See More Adoption Across The API Landscape</title>
    <link href="http://apievangelist.com/2019/06/25/why-schema-org-does-not-see-more-adoption-across-the-api-landscape/"/>
    <updated>2019-06-25T05:00:00Z</updated>
    <content><![CDATA[
I’m a big fan of Schema.org. A while back I generated an OpenAPI 2.0 (fka Swagger) definition for each one and published to GitHub. I’m currently cleaning up the project, publishing them as OpenAPI 3.0 files, and relaunching the site around it. As I was doing this work, I found myself thinking more about why Schema.org isn’t the goto schema solution for all API providers. It is a challenge that is multi-layered like an onion, and probably just as stinky, and will no doubt leave you crying.

First, I think tooling makes a big difference when it comes to why API providers haven’t adopted Schema.org by default across their APIs. If more API design and development tooling would allow for the creation of new APIs using Schema.org defined schema, I think you’d see a serious uptick in the standardization of APIs that use Schema.org. In my experience, I have found that people are mostly lazy, and aren’t overly concerned with doing APIs right, they are just looking to get them delivered to meet specifications. If we provide them with tooling that gets the API delivered to specifications, but also in a standardized, they are doing to do it.

Second, I think most API providers don’t have the time and bandwidth to think of the big picture like using standardized schema for all of their resources. Most people are under pressure to more with less, and standards is something that can be easily lost in the shuffle when you are just looking to satisfy the man. It takes extra effort and space to realize common standards as part of your overall API design.  This is a luxury most companies, organizations, and government agencies can not afford, resulting in many ad hoc APIs defined in isolation.

Third, I think some companies just do not care about interoperability. Resulting in them being lazy, or not making it a priority as stated in the first and second points. Most API providers are just concerned with you using their API, or checking the box that they have an API. They do not connect the dots between standardization and it being easier for consumers to put their resources to work. Many platforms who are providing APIs are more interested in lock-in, providing proprietary SDKs and tooling on top of their API. Selling them on the benefits of interoperable open source SDKs and tooling just falls on deaf ears—leaving most API providers to perpetually reinvent the wheel when there is an existing well defined one within reach.

I wish ore API folks cared about Schema.org. I wish I had the luxury of using in more of my own work. If it is up to me, I will always adopt Schema.org for my core API designs, but unfortunately I’m not always the one in charge of what gets decided. I will continue to invest in OpenAPI definitions for all of the Schema.org defined schema. This allows me to have within reach when I’m getting ready to define a new API. If Schema.org ready API definitions are in a neat stack on my desk, the likelihood that I’m going to put to work in the API tooling I’m developing, and actually as the base for an API I’m delivering, increases significantly. Helping me standardize my API vocabulary to something that reaches beyond the tractor beam of daily API bubble.
]]></content>
    <id>http://apievangelist.com/2019/06/25/why-schema-org-does-not-see-more-adoption-across-the-api-landscape/</id>
  </entry><entry>
    <title>Avoiding Complexity and Just Deploying YAML, JSON, and CSV APIs Using GitHub or GitLab</title>
    <link href="http://apievangelist.com/2019/06/24/avoiding-complexity-and-ust-deploying-yaml-json-csv-apis-using-github-or-gitlab/"/>
    <updated>2019-06-24T05:00:00Z</updated>
    <content><![CDATA[
I find that a significant portion of I should be doing when defining, designing, developing, and delivering an API is all of avoiding complexity. Every step away along the API journey I am faced with opportunities to introduce complexity, forcing me to constantly question and say no to architectural design decisions. Even after crafting some pretty crafty APIs in my day, I keep coming back to JSON or YAML within Git, as the most sensible API architectural decision I can make.

Git, with JSON and YAML stored within a repository, fronted with a Jekyll front-end does much of what I need. The challenge with selling this concept to others is that it is a static publish approach to APIs, instead of a dynamic pull of relevant data. This approach isn’t for every API solution, I’m not in the business selling one API solution to solve all of our problems. However, for many of the API uses I’m building for, a static Git-driven approach to publishing machine readable JSON or YAML data is a perfect low cost, low tech solution to delivering APIs.

A Git repository hosted on GitHub or GitLab will store a significant amount of JSON, YAML, or CSV data. Something you can easily shard across multiple repositories within an organization / group, as well as across many repositories within many organization / groups. Both GitHub and GitLab offer free solutions, essentially letting you publish as many repositories as you want. As I said earlier, this is not a solution to all API needs, but when I’m looking to introduce some constraints to keep things low cost, simple, and easy to use and manage—a Git-driven API is definitely worth considering. However, going static for your API will force you to think about how you automate the lifecycle of your data, content, and other resources.

The easiest way to manage JSON, CSV, or YAML data you have on GitHub or GitLab is to use the GitHub or GitLab API, allowing you to update individual JSON, CSV, or YAML files in real-time. If you want to do it in batch process by checking out the repository, making all the changes you want and committing back as a single Git commit using the command line—I’ve automated a number of these to reduce the number of API calls I’m making. If you want to put in an editorial layer you can require submission via pull / merge request, requiring there be multiple eyes on each update to the data behind a static API. It is an imperative, and a declarative API, with an open source approval workflow by default—all for free.

Once you have your data in the Git repository you can make it available using the RAW links provided by GitHub or GitLab. However, I prefer to publish a Jekyll front-end, which can act as the portal landing page for the site, but then you can also manually or dynamically create neat paths that route users to your data using sensible URLs—the best part is you can add a cname and publish your own domain. Making your API accessible to humans, while also providing intuitive, easy to follow URLs to the static data that has been published using the GitHub or GitLab API, or the underlying Git infrastructure to do in bulk.

This is the cheapest, most productive way to deliver simple data and content APIs. The biggest challenges are that you have to begin thinking a little differently about how you manage your data. You have to move from a pull to a push way of delivering data, and embrace the existing CI/CD way of doing things embraced by both GitHub and GitLab. For me, using Git to deliver APIs provides a poor mans way to not just deliver an API, but also ensure it is secure, performant, and something I can automate the management of using existing tools developers are depending on. Over the last couple of years I’ve pushed the limits of this approach by publishing thousands of OpenAPI-driven API discovery portals, and it is something I’m going to be refining and using as the default layer for delivering simple APIs that allow me to avoid unnecessary complexity.
]]></content>
    <id>http://apievangelist.com/2019/06/24/avoiding-complexity-and-ust-deploying-yaml-json-csv-apis-using-github-or-gitlab/</id>
  </entry><entry>
    <title>Organizing My APIs Using OpenAPI Tags</title>
    <link href="http://apievangelist.com/2019/06/19/organizing-my-apis-using-openapi-tags/"/>
    <updated>2019-06-19T05:00:00Z</updated>
    <content><![CDATA[
I like my OpenAPI tags. Honestly, I like tags in general. Almost every API resource I design ends up having some sort of tagging layer. Too help me organize my world, I have a centralized tagging vocabulary that I use across my JSON Schema, OpenAPI, and AsyncAPI, to help me group, organize, filter, publish, and maintain my catalog of API and schema resources.

The tag object for the OpenAPI specification is pretty basic, allowing you to add tags for an entire API contract, as well as apply them to each individual API method. Tooling, such as API documentation uses these tags to group your API resources, allowing you to break down your resources into logical bounded contexts. It is a pretty basic way of defining tags, that can go a long ways depending on how creative you want to get. I am extending tags with an OpenAPI vendor extension, but I also see that there is a issue submitted suggesting they move the specification forward by allowing for the nesting of tags–potentially taking OpenAPI tagging to the next level.

I’m allowing for a handful of extensions to the OpenAPI specification to accomplish the following:


  Tag Grouping - Help me nest, and build multiple tiers of tags for organization APIs.
  Tag Sorting - Allowing me to define a sort order that goes beyond an alphabetical list.


I am building listing, reporting, and other management tools based up OpenAPI tagging to help me in the following areas:


  Tag Usage - Reporting how many resources are available for each tag, and tag group.
  Tag Cleanup - Helping me de-dupe, rename, deal with plural challenges, etc.
  Tag Translations - Translating old tags into new tags, helping keep things meaningful.
  Tag Clouds - Generating D3.js tag clouds from the tags applied to API resources.
  Packages - Deployment of NPM packages based upon different bounded contexts defined by tags.


I am applying tags to the following specifications, stretching my OpenAPI tagging approach to be more about a universal way to organize all my resources:


  JSON Schema - All schema objects have tags to keep organized.
  OpenAPI - Each API method have tags, for easy grouping.
  AsyncAPI - Each pub/sub, event, and message API have APIs.
  APIs.json - Collections of APIs have tags for discoverability.


Tags are an important dimension of API discoverability when it comes to my API definitions. They provide rich metadata that I can use to make sense of my API infrastructure. Without them, the quality of my API definitions tend to trend lower. By evolving the tagging schema, and investing in tooling to help me make sense across the API definitions I’m depending on, I can push the boundaries of how I tag, and evolve it to be the core of how I manage my API definitions.

I’ll keep watching how others are tagging their APIs, although I don’t see too much innovation, and low levels of usage by other API providers as part of their API definitions. I’ll also keep an eye out for other ways in which tag schema are being extended, helping potentially define the future of how the leading API specifications enable tagging. I have a short list of tooling I am developing to help make my life easier, but I’m working hard to just make sure I’m applying tags across my API resources in a consistent way. I find this is the most valuable aspect of API tagging, but eventually I’m guessing that the tooling will make the real difference when it comes to slicing and dicing, and making sense of my API infrastructure at scale.
]]></content>
    <id>http://apievangelist.com/2019/06/19/organizing-my-apis-using-openapi-tags/</id>
  </entry><entry>
    <title>Doing The Hard Work To Define APIs</title>
    <link href="http://apievangelist.com/2019/06/17/doing-the-hard-work-to-define-apis/"/>
    <updated>2019-06-17T02:00:00Z</updated>
    <content><![CDATA[
Two years later, I am still working to define the API driven marketplace that is my digital self. Understanding how I generate revenue from my brand (vomits in mouth a little bit), but also fight off the surveillance capitalists from mining and extracting value from my digital existence. It takes a lot of hard work to define the APIs you depend on to do business, and quantify the digital bits that you are transacting on the open web, amongst partners, and unknowingly with 3rd parties. As an individual, I find this to be a full time job, and within the enterprise, it is something that everyone will have to own a piece of, which in reality, is something that is easier said than done.

Convincing enterprise leadership of the need to be aware of every enterprise capability being defined at the network, system, or application level is a challenge, but doable. Getting consensus on how to do this at scale, across the enterprise will be easier said than done. Identifying how the network, system, and applications across a large organization are being accessed, what schema, messages, and other properties are being exchanged is not a trivial task. It is a massive undertaking to reverse engineer operations, and identify the core capabilities being delivered, then define and document in a coherent way that can be shared with others, and included as part of an organization messaging campaign.

Many will see work to define all enterprise API capabilities as a futile task–something that is impossible to deliver. Many others will not see the value of doing it in the first place, and unable to realize the big picture, they will see defining of APIs and underlying schema as meaningless busy work. Even when you do get folks on-board with the important, having the discipline to see the job through becomes a significant challenge. If moral is low within any organization group, and team members do not have visibility into the overall strategy, the process of defining gears that make the enterprise move forward will be seen as mind numbing accounting work–not something most teams will respond positively about working on.

Once you do define your APIs, you then have to begin investing in defining what the future will hold when it comes to unwinding, evolving, maturing the API infrastructure you are delivering and depending on. This is something that can’t fully move forward until a full or partial accounting of enterprise API capabilities has occurred. If you do not know what is, you will always have trouble defining or controlling what will be. One of the reasons we have so much technical debt is we prefer to focus on what comes next rather than attending to the maintenance required to keep everything clean ,coherent, organized, and well defined. It is always easier to focus on the future, than it is to reconcile with mess we’ve created in the past. This is why it is so easy to sell each wave of enterprise technology solutions, promising to do this work for you–when in reality, most times, you are just laying down the next layer of debt.

Whether it is our personal lives, or our professional worlds, defining the APIs we depend on, as well as the APIs that aren’t useful and become parasitic, as well as the schema objects they produce and exchange is a lot of hard work. Hard work most of us will neglect and outsource for convenience. Making the work become even harder down the road–nobody will care about doing this as we do. The really fascinating part of all of this for me, is that with each cycle of technology that passes through, we keep doubling down on technology being the solution to yesterday’s problem, even though it is the core of yesterday’s problem. In my experience, once you really begin investing in the hard work to define the APIs you depend on, you begin to realize that you don’t need so many of them. That the number of API connections you depend on can actually begin to hurt you, which is something that can wildly grow if you aren’t in tune with what your API landscape consists of in your personal and professional worlds.
]]></content>
    <id>http://apievangelist.com/2019/06/17/doing-the-hard-work-to-define-apis/</id>
  </entry><entry>
    <title>There Is No Single Right Way To Do APIs</title>
    <link href="http://apievangelist.com/2019/06/16/there-is-no-single-right-way-to-do-apis/"/>
    <updated>2019-06-16T02:00:00Z</updated>
    <content><![CDATA[
My time working in the API sector has been filled with a lot of lessons. I researched hard, paid attention, and found a number of surprising realities emerge across the API landscape. The majority of surprises have been in the shadows caused by my computational belief scaffolding I’ve been erecting since the early 1980s. A world where there has to be absolutes, known knowns, things are black and white, or more appropriately 1 or 0. If I studied all APIs, I’d find some sort of formula for doing APIs that is superior to everyone else’s approach to doing APIs. I was the API Evangelist man–I could nail down the one right way to do APIs. (Did I mention that I’m a white male autodidact?)

I was wrong. There is no one right way to do APIs. Sure, there are many different processes, practices, and tools that can help you optimize your API operations, but despite popular belief, there is no single “right way” to do define, design, provide, or consume APIs. REST is not the one solution. Hypermedia is not the one solution. GraphQL is not the one solution. Publish / Subscribe is not the one solution. Event-driven is not the one solution. APIs in general are the the one solution. Anyone telling you there is one solution to doing APIs for all situations is selling you something. Understanding your needs, and what the pros and cons of different approaches are, is the only thing that will help you.

If you are hyper focused on the technology, it is easy to believe in a single right way of doing things. Once you start having to deliver APIs in a variety of business sectors and domains, you will quickly begin to see your belief system in a single right way of doing things crumble. Different types of data require different types of approaches to API enablement. Different industries are knowledgable in different ways of API enablement, with some more mature than others. Each industry will present its own challenges to API delivery and consumption, that will require a different toolbox, and mixed set of skills required to be successful. Your social network API strategy will not easily translate to the healthcare industry, or other domain.

With a focus on the technology and business of APIs, you can still find yourself dogmatic around a single right way of doing things. Then, if you find yourself doing APIs in a variety of organizations, across a variety of industries, you quickly realize how diverse your API toolbox and approach will need to be. Organizations come with all kinds of legacy technical debt, requiring a myriad of approaches to ensuring APIs properly evolve across the API lifecycle. Once a technological approach to delivering software gets baked into operations, it becomes very difficult to unwind, and change behavior at scale across a large organization–there is no single right way to do APIs within a large enterprise organization. If someone is telling you there is, they are trying to sell you the next generation of technology to bake into your enterprise operations, which will have to be unwound at some undisclosed date in the future–if ever.

I’ve always considered my API research and guidance to be a sort of buffet–allowing my readers choose the mix that works for them. However, I still found myself providing industry guides, comprehensive checklists, and other declarative API narratives that still nod towards there being a single, or at least a handful of right ways of doing APIs. I think that APIs will ultimately be like cancer, something we never quite solve, but there will be huge amounts of money spent trying to deliver the one cure. I’ll end with the comparison there, because I don’t want to get me on a rant regarding the many ways in which APIs and cancer will impact the lives of everyday people. In the end, I will still keep studying and understanding different approaches to doing APIs (both good or bad), but you’ll find my narrative to be less prescriptive when it comes to any single way of doing APIs, as well as suggesting that doing APIs in the first place is the right answer to any real world problem.
]]></content>
    <id>http://apievangelist.com/2019/06/16/there-is-no-single-right-way-to-do-apis/</id>
  </entry><entry>
    <title>API Definitions Are Important</title>
    <link href="http://apievangelist.com/2019/06/12/api-definitions-are-important/"/>
    <updated>2019-06-12T02:00:00Z</updated>
    <content><![CDATA[
I found myself scrolling down the home page of API Evangelist and thinking about what topic(s) I thought were still the most relevant in my mind after not writing about APIs for the last six months. Hands down it is API definitions. These machine and human readable artifacts are the single most important thing for me when it comes to APIs I’m building, and putting to work for me.

Having mature, machine readable API definitions for all API that you depend on, is essential. It also takes a lot of hard work to make happen. It is why I went API define first a long time ago, defining my APIs before I ever get to work designing, mocking, developing, and deploying my APIs. Right now, I’m heavily dependent on my:


  JSON Schema - Essential for defining all objects being used across API contracts.
  OpenAPI - Having OpenAPI contracts for al my web APIs is the default–they drive everything.
  AsyncAPI - Critical for defining all of my non HTTP 1.1 API contracts being provided or consumed.
  Postman Collections - Providing me with the essential API + environment definitions for run-time use.
  APIs.json - Helping me define all the other moving parts of API operations, indexing all my definitions.


While there is plenty of other stops along the API lifecycle that are still relevant to me, my API definitions are hands down the most valuable intellectual property I possess. These four API specifications are essential to making my world work, but there are other more formalized specifications I’d love to be able to put to work:


  Orchestrations - I’d liked to see a more standardized, machine readable way for working with many API calls in a meaningful way. I know you can do this with Postman, and I’ve done with OpenAPI, and like Stoplight.io’s approach, but I want an open source solution.
  Licensing - I am not still actively using API Commons, but I’d like to invest in a 2.0 version of the API licensing specification, moving it beyond just the API licensing, and consider SDK, and other layers.
  Governance - I’d like to see a formal way of expressing API governance guidance that can be viewed by a human, or executed as part of the pipeline, ensuring that all API contracts conform to a set of standards.


These area hit on the main areas that concern for me when it comes to defining the contracts I need to further automate the integration and deployment of API resources in my life. While there are definitely other stops along the API lifecycle on my mind, I spend the majority of my time creating, refining, communicating, and moving forward API definitions that define and drive every other stop along the API lifecycle.

API definitions represent API sanity for me. If they aren’t in order, there is disorder. An immature API definition requires investment, socialization amongst stakeholders, and iterating upon before it will ever be considered for publishing. I’ll be exploring the other things that matter for me along the API lifecycle, and then I’m guessing that the rest of this stuff I’ve been researching over the last eight years will either disappear, or just be demoted on the site. We’ll see how this refresh rolls out, but I’m guestimating about 25% of my research will continue to move forward after this reboot.
]]></content>
    <id>http://apievangelist.com/2019/06/12/api-definitions-are-important/</id>
  </entry><entry>
    <title>API Evangelist Is Open For Business</title>
    <link href="http://apievangelist.com/2019/06/10/api-evangelist-is-open-for-business/"/>
    <updated>2019-06-10T02:00:00Z</updated>
    <content><![CDATA[
After six months of silence I've decided to fire API Evangelist back up again. I finally reached a place where I feel like I can separate out the things that caused me to step back in the first place. Mostly, I have a paycheck now, some health insurance, and I don't have to pretend I give a shit about APIs, startups, venture capital, and the enterprise. I'm being paid well to do an API job. I can pay my rent. I can go to the doctor when my health takes a hit. My basic needs are met.

Beyond that, I'm really over having to care about building an API community, making change with APIs, and counteracting all of the negative effects of APIs in the wild. I can focus on exactly what interests me about technology, and contribute to the 3% of the API universe that isn't doing yawnworthy, half-assed, or straight up shady shit. I don't have to respond to all the emails in my inbox just because I need to eat, and have a roof over my head. I don't have to jump, just because you think your API thing is the next API thing. I can just do me, which really is the founding principle of API Evangelist.

Third, I got a kid to put through college, and I'm going to make y'all pay for it. So, API Evangelist is open for business. I won't be producing the number of stories I used to. I'll only be writing about things I actually find interesting, and will explore other models for generating content, traffic, and revenue. So reach out, and pitch me. I'm looking for sponsors, and open to almost anything. Don't worry, I'll be my usual honest self and tell you whether I'm interested or not, and have strong opinions on what should be said, but pitch me. I'm open for business, I'll entertain any business offer keep API Evangelist in forward motion, and generating revenue for me.

If you are interested in sponsoring API Evangelist, it is averaging 2K page views a day, but normally averages 5K a day when it is in full active mode. The Twitter account has 10K followers, and the audience is a pretty damn good representation of the API pie if I don't say so myself. It is a damn shame to squander what I've built over the last nine years just because I like to ride on a sparkly high horse. If I've learned anything during my time as the API Evangelist, it is that revenue drives ALL decisions. So get in on the action. Let me know what you are thinking, and I'll get to work adding your logo to the site, and turning on the other sponsorship opportunities. Ping me at info@apievangelist.com to get the ball rolling.
]]></content>
    <id>http://apievangelist.com/2019/06/10/api-evangelist-is-open-for-business/</id>
  </entry>
</feed>
