<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/death-star.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/31/the-api-space-is-in-the-tractor-beam-of-the-cloud-giants-now/">The API Space Is In The Tractor Beam Of The Cloud Giants Now</a></h3>
			<p><em>31 Aug 2017</em></p>
			<p>A growing number of SMBs, SMEs, and other institutions, organizations, and government agencies are launching APIs, but the age APIs as the core product will thin, and those that do emerge and operate independently will be increasingly absorbed into the cloud platforms they operate on. The tractor beam of AWS, Google, and Azure are becoming to strong for us API providers to resit. We use their platforms to deploy and manage our APIs, we’ve ceded control over our operations to their clouds, it is just a matter of time before each of the APIs we depend on are assimilated into the cloud machine. Sure, we’ll still get access to valuable resources that we couldn’t launch ourselves, things like Google Maps, and resources at scale like Amazon EC2. But our platforms will be closely watching our trajectories and they will make the calculating decision whether what we are doing is valuable enough to acquire, or just suffocate by launching a competitive service. Whoops, sorry! What you are doing is a great idea, but you will have three choices, don’t do it, sell to us, or we’ll drive you out of business. The open landscape of APIs will become thousands of cloud APIs that feel like we have an unlimited amount of resources, but in reality it will just limit and control available resources, allowing the cloud bigcos to steer the space wherever they want to–controlling the creation and distribution of ideas. It was fun while it lasted. We were able to work on our own farms for a while, and use each others free range organic API resources, but pretty soon it will just be the factory model of API consumption. Sure, prices will be low when the cloud giants are doing battle, but when they aren’t we’ll be paying a premium for simple resources. When any newcomer emerges looking to disrupt any area of the API space they’ll be quickly consumed, suffocated, and silenced, removing...[<a href="/2017/08/31/the-api-space-is-in-the-tractor-beam-of-the-cloud-giants-now/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/36349148770_1a9d2692b2_z.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/31/apis-will-just-get-more-unreliable-as-funding-becomes-more-volatile/">APIs Will Just Get More Unreliable As Funding Becomes More Volatile</a></h3>
			<p><em>31 Aug 2017</em></p>
			<p>People love to point out that APIs are unreliable. You can’t depend on them. They go away at any point, and they just aren’t something you want to be building your business on top of. When in reality APIs aren’t reliable it is the business, people, and investment behind them. The reality of the startup game is that us API consumers aren’t actually the customer, we are just numbers in a larger game where startup founders and their investors are looking for enterprise customers to purchase their startup getting the desired exit. The API is just about attracting consumers, who will do the legwork to bring in users, adding to the value of a company. As the startup funding landscape continues to dry up, shift, and evolve towards more riskier and volatile versions of investment like ICOs, things are only going to get worse. Of course, few conversation will place the blame on the people and companies behind, but APIs will continue to be the scapegoat for the instability. It works just like the robots coming for your jobs. You never hear that rich people who own companies, that are making decisions to replace workers with robots are coming for your jobs. Its the robots. Technology in many forms makes for a great blame shield, absorbing the responsibility for the volatility, instability, and scams that are going on across the landscape. In reality, nothing much changes for us API consumers. You need to get to know your API providers, well as the company and people behind them. Study their approach to operating their API. Do they communicate? Do they have proper support? Do they communicate their uptime status? What type of funding is propping them up, and the shape of their business model use. Make sure you always have a plan B if you can, and do not trust that ANY API will be around forever. If possible, come up with failover plans, and run...[<a href="/2017/08/31/apis-will-just-get-more-unreliable-as-funding-becomes-more-volatile/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/fat-riches-russian.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/31/admit-it-you-do-not-respect-your-api-consumers-and-end-users/">Admit It You Do Not Respect Your API Consumers And End Users</a></h3>
			<p><em>31 Aug 2017</em></p>
			<p>Just admit it, you could care less about your API consumers. You are just playing this whole API game because you read somewhere that this is what everyone should be doing now. You figured you can get some good press out of doing an API, get some free work from developers, and look like you are one of the cool kids for a while. You do the song and dance well, you have developed and deployed an API. It will look like the other APIs out there, but when it comes to supporting developers, or actually investing in the community, you really aren’t that interested in rolling up your sleeves and making a difference. You just don’t really care that much, as long as it looks like you are playing the API game. Honestly, you’d do any trend that comes along, but this one has so many perks you couldn’t ignore it. Not only do you get to be API cool, you did all the right things, launched on Product Hunt, and you have a presence at all the right tech events. Developers are lining up to build applications, and are willing to work for free. Most of the apps that get built are worthless, but the SDKs you provide act as a vacuum for data. You’ve managed to double your budget by selling the data you acquire to your partners, and other data brokers. You could give away your API for free, and still make a killing, but hell, you have to keep charging just so you look legit, and don’t raise any alarm bells. It is hard to respect developers who line up and work for free like this. And the users, they are so damn clueless regarding what is going on, they’ll hand over their address book and location in real-time without ever thinking twice. This is just to easy. APIs are such a great racket. You really don’t have to do...[<a href="/2017/08/31/admit-it-you-do-not-respect-your-api-consumers-and-end-users/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/36701179836_a69b280280_z.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/30/your-apis-are-an-invasive-species/">Your APIs Are An Invasive Species</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>We tend to look at APIs as something we opt into. As an API consumer we choose to integrate with these external APIs. We made a conscious decision to put an API to work. We navigate our way to their API portals, learn about an API from its documentation, and take back what we know along with a handful of URIs, and bake the APIs into our internal systems and applications. When in reality we are just worker bees sent to find pollen, and bring the pollen back to the hive, unaware that the pollen contains an invasive species, and rarely do we ever think too deeply about why we are doing all of this–just following orders. We are told by our coworkers, the tech blogosphere, and by API providers that we need these things. Often times we are enticed with dreams of striking it rich, and that we are mining a platform for its riches, but really we are being mined by the platform, making it richer. Facebook, Twitter, Uber, Google, and other platforms want us to think APIs are democratizing, and that we are being empowered. There is always light at the end of the tunnel for the free labor we do as API consumers, and application developers. Along this journey, rarely do we realize the data we giving away, the free labor we’ve offered up without ever being asked, and how we’ve done the heavy lifting for this invasive, disruptive species we’ve invited into our lives. Once embedded, this species is surveilling, extracting, and taking what it needs, only giving back what it needs to keep its host alive. We’ve been tricked by the API mirror. We look at it and see ourselves, and listen to the stories and myths that we tell ourselves, as we read they hype in the tech blogosphere, and on the Redditz and Githubz. We seek out the machine, do the work to integrate it into our...[<a href="/2017/08/30/your-apis-are-an-invasive-species/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/30/you-like-what-i-do-as-the-api-evangelist-sure-i-will-do-more-work-for-free/">You Like What I Do As The API Evangelist? Sure, I Will Do More Work For Free!</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>I get regular emails from folks telling me how much they love what I do, then asking me to work for free. I’m totally happy for folks to inform me about their company, products, case studies, and other API goings on with their company. This is the bread and butter for my storytelling on API Evangelist–please keep it coming. However, the folks who ask me to work for free, what are you thinking? Where does this ethic come from in the tech sector, where folks expect you to work for their startup for free? It is something that has really gotten to ridiculous levels. Join our webinar. Write a story for our blog. Contribute to a white paper. Join our podcast. Speak at our event. Teach a workshop. Come visit our company. Talk about our products and services. Help us craft our strategy. Do some research on this subject so we can use it. Tell us what we are doing wrong. Download, install, and play with our tool, and provide us with feedback. Take this 148 question survey that will only take you 45 minutes. Jump on the phone with us so we can pick your brain. Post our infographic to your site. Share this out via your Twitter account. Vote us up on Hacker News. Commit to our open source project. Help us create an API definition. Can you review this project specification. On, and on, and on. Oh, and hey I just wanted to email you again, because you didn’t respond to the last two emails! Don’t get me wrong. If you know me, and we have history, feel free to ask me for favors, but remember, I do not have a job. I’m happy to help my friends, and love participating in podcasts with smart people, and reviewing my friends writing, because they scratch my back. They throw me bones, and have a history of given me exposure, and paying me for...[<a href="/2017/08/30/you-like-what-i-do-as-the-api-evangelist-sure-i-will-do-more-work-for-free/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/cactus-flower.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/30/sorry-the-stock-options-for-your-api-startup-do-me-more-harm-than-good/">Sorry The Stock Options For Your API Startup Do Me More Harm Than Good</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>I’m really honored that some of my partners are kind enough to offer me a piece of the action in their companies, in exchange for what I do. I really am. However, going forward I’m going to have to decline any stock options in exchange for work, or advising, because it really doesn’t pencil out for me. I know it is the currency you are working with, getting investment in exchange for options, and trying to get knowledge, talent, and other forms of investment in exchange as well. I trust it will work out in your favor, but from my vantage point, there really is no upside in the game. I regularly receive accusations of having and agenda because of real or perceived interest in companies, so with this hit on my brand, and the lack of return from the historic stock options I’ve had historically, it just doesn’t work out. My last advisor options netted me $319, for about 60 hours of work on, and travel costs out of my own pocket. I’m sure if I had more capital investment, and a bigger piece of the action, I’d fare better, but for the stake I get thrown–it just doesn’t do it for me. Plus, I have a manila folder in the filing cabinet of more significant shares that aren’t worth the paper they are printed on, so stock options really doesn’t float my boat in the first place. Please don’t let this hurt your feelings. I know you are heavily invested in the value of your options, but I’m just not there. I’m happy to find other arrangements to make things happen, but with the way that stock options get wielded against me, and used to potentially discredit what I’m doing, it just isn’t worth anymore. As it stands today I only have stake in a single company–Skylight Digital (5%). I have NO stake in any API related company, and I am going to...[<a href="/2017/08/30/sorry-the-stock-options-for-your-api-startup-do-me-more-harm-than-good/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/36609070321_c7122be05f_z.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/30/i-realize-that-this-is-a-hit-on-your-api-budget-but-it-is-my-rent-for-this/">I Realize That This Is A Hit On Your API Budget, But It Is My Rent For This</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>I am the first to admit that I suck at the money game. I just don’t care. Don’t get me wrong, I’ve made a significant amount of money in my career, and command a phat six figure salary when I’ve done the job thingy, and I don’t have a problem asking for a decent rate when I’m consulting. It is just that I don’t care about climbing the money ladder, because I realized in my early 20s that it was never enough. I’ve had business partners run off with hundreds of thousands of dollars in cash, screw me out of the equity in multiple companies, and I’ve experienced what people will do to get at that next rung of the ladder. I quickly saw that you are never happier with each rung you climb, and often times you end up much unhappier the higher up you go–which is why I stay where I am at. If you have been one of my partners you know I suck at invoicing regularly. Honestly, I don’t think about it until I have to. Which regularly I forget an entire month. Unless I’m looking looking at eviction, or have something I need to purchase, I’m more obsessed with working. I love what I do (mostly), and if someone would just drop money in my account each month, I just keep beating the API Evangelist drum, and never look to make beyond what it takes to keep me going each month. Being independent is a hustle, and it takes the support of my partners (thanks 3Scale, Runscope, Restlet, and Tyk) to make this work, along with some side consulting hustle to fill in the gaps. I mostly prefer the low monthly sponsorship money because the consulting hustle is a chore, and chasing down the money can be a full time job. I’d say 1/3 of the time its about getting plugged into corporate system(s), 1/3 of the time smooth sailing,...[<a href="/2017/08/30/i-realize-that-this-is-a-hit-on-your-api-budget-but-it-is-my-rent-for-this/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/36700218276_b6782330e1_z.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/30/holding-little-guys-more-accountable-than-we-do-vcs-and-bigcos/">Holding Little Guys More Accountable Than We Do VCs And Bigcos</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>I spend a lot of time defending my space as the API Evangelist. I’ve had lengthy battles with folks in the comments of my blog for defending women, charging for my services, being pay for play, having secret agendas, and much more. I’ve had my site taken down a handful of times (before I made static on Github Pages), because I stood up for my girlfriend, or just joined in on the wrong (right) cause. When you have been doing this as long as you have, you see the size of the armies of tech bros that are out there, waiting to pounce. It is why I don’t share links to Reddit or Hacker News anymore. I stopped sharing to DZone for same reason, but they’ve since cleaned up their community, brought in some great talent (including women), and I’ve started syndicating there again. Most recently I had someone accuse me of pay for play, even though there was no disclosure statement present, and I have had two other folks accuse me of having an agenda set forth by my partners. If you know me, you understand how ridiculous this is, but this never stops the waves of young men finding their way to my site, and pointing the finger. What I find fascinating about this is these men never point the finger at Techcrunch, or call out API startups (and bigcos) for colluding, sponsoring, pay for play, and the other bullshittery that goes on. They go after folks who are outspoken against the machine, and never after the machine itself. If people don’t like something I said, or what someone I’m writing about is up to, they tend to go after me, without spending any time getting to know me, looking at my background, or looking at the seven years of storytelling on my site–there is a single page to do it! The majority of these folks rarely ever have their own blog, name...[<a href="/2017/08/30/holding-little-guys-more-accountable-than-we-do-vcs-and-bigcos/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-zoomed-in-on-sea_light_dali.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/29/your-microservices-effort-will-fail-just-like-your-api-and-soa-initiatives/">Your Microservices Effort Will Fail Just Like Your API And SOA Initiatives</a></h3>
			<p><em>29 Aug 2017</em></p>
			<p>You are full steam ahead with your microservices campaign. You’ve read Martin Fowlers blog post, and talked about the topic with your team for the last six months. After a couple pilot projects, you are diving in, and have started decoupling the monolith of systems that you depend on to operate your business each day. You have mapped out all the technical details of all code, and backend systems in play, and have targeted about 30% of existing systems for reworking using a microservices strategy. Yet, despite all your planning, your microservices effort will still fail just like your API efforts, and its predecessor the SOA initiative did. Despite all your research, planning, and eye for the technical detail in about 7 months everything will begin to slow, and by month 10 you will begin to get very, very frustrated. You see, you haven’t included any of the human element in your planning. You have thought about the business, cultural, legal, financial, and other non-technical aspects of operating your systems. You’ve done a fine job of developing a strategy for decoupling your monolith database, but you haven’t invested any time in what it will take to educate and bring up to speed all the business users, support, QA, and other folks you will need to have up to speed to actually make all this work. You are so blinded by technological trends, you forget to spend time talking to the people. You are feeling good right now because you have surrounded yourself with yes men. People who are in perfect alignment with what you are doing. They think like you, and have drank the same kool-aid. However, once you start implementing your grand strategy to make every smaller, more micro, you will begin to see friction. Sales folks will begin to see you as a threat as you fragment their landscape, and be seen as taking a piece of their action. Business users will begin...[<a href="/2017/08/29/your-microservices-effort-will-fail-just-like-your-api-and-soa-initiatives/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/kin-lane.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/29/the-reason-your-api-sucks-is-there-are-no-women-and-people-of-color-on-your/">The Reason Your API Sucks Is There Are No Women And People Of Color On Your</a></h3>
			<p><em>29 Aug 2017</em></p>
			<p>I know that many of you are insecure about your APIs. You aren’t transparent with your numbers, and many aspects of your API operations. You are stressed out because you built it, and nobody came. You were able to artificially inflate your new user numbers, and API calls through paid campaigns, and bot activity, but nobody is using it, and you just can’t figure out why. You are asking yourself why don’t anyone see the value your API brings to the table? Why aren’t you getting the traction you thought you would get when you first came up with the idea? You aren’t getting any traction with your API because it sucks. It was a bad idea. Nobody wants it. It sucks because it doesn’t provide any value in a highly competitive space, and you naively thought that if you built it everyone would come. You probably have a number of people around you telling you that your idea is great, and the API will be a hit. You’ve probably had this most of your life, and are used to people telling you that your ideas are great. It is why you feel so uncomfortable around anyone that is critical, because you just aren’t used to being told you that your ideas are dumb. It hurts your feelings. This is why you surround yourself with people who look, act and think like you do. It is why you don’t think women and people of color have the skills needed to work on your dumb, useless ideas. You don’t have the balls to surround yourself with anyone who doesn’t think like you. If you did, you might have been told early on that your idea wasn’t worthwhile, or you might have gotten additional feedback or criticism that would have helped shape it into something useful. I know this is hard for you to hear, and you think you are really smart, and you probably read one...[<a href="/2017/08/29/the-reason-your-api-sucks-is-there-are-no-women-and-people-of-color-on-your/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/36575484422_087495fca9_z.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/29/the-reason-for-your-api-security-breach-you-did-nothing/">The Reason For Your API Security Breach: You Did Nothing</a></h3>
			<p><em>29 Aug 2017</em></p>
			<p>You just got three separate calls, and countless emails alerting to the fact that you just had a major security breach. You don’t know the extent of the damage yet, but it looks like they got into your primary customer database via the APIs you depend on for all your mobile applications. You are sitting in your office chair, sweating, and trying to figure out how this happened. I will tell you, it is because you have done nothing. You have de-prioritized security at every turn, resulting in an open door for any hacker to walk through. Not only have you done nothing, you actually worked against anyone who brought up the topic of API security. You would respond: We don’t have the time. We don’t have the budget. We don’t have the skills. You never listened to anyone of your staff, even that security lady (what was her name?) you had hired last year, and then resigned, with a letter containing over 25 security holes she had been trying to take care of, but because of the toxic environment you’ve created, she was unable to do anything and moved on. You have created an environment where anyone who brings up security concerns feels persecuted, and even that their job is in jeopardy, making “doing nothing” the standard mode across all operations. You have eight separate mobile applications which all use APIs, and all of them using the customer database in question, which also stores credit cards, which is in violation of your PCI compliance–you know, those forms you sign off on each year? You felt these mobile APIs were secure because they were hidden behind your mobile applications, and your developers had given you a application security scan report last year. In this situation you would love to blame these developers, but all roads lead to you when it comes to responsibility for this situation. You begin to feel sick to your stomach thinking...[<a href="/2017/08/29/the-reason-for-your-api-security-breach-you-did-nothing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/36745180055_45289923eb_z.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/29/extract-as-much-value-as-you-can-from-your-api-community-and-give-nothing-back/">Extract As Much Value As You Can From Your API Community And Give Nothing Back</a></h3>
			<p><em>29 Aug 2017</em></p>
			<p>You are in a sweet spot. You got a fat six figure job in the coolest department of your company, building out your API platform. You have a decent budget (never as much as you want) to throw hackathons, run Google and Twitter ads, and you can buy schwag to give away at your events. Sure there is a lot of pressure to deliver, but you are doing pretty well. All you gotta do is convince 3rd party developers to do thing with your companies APIs, develop web, mobile, voice, and other applications that generate buzz and deliver the return on investment your bosses are looking for. It is all about you and your team. Let’s get to work growth hacking! Attract as may new users as we can, and convince them to build as much as we possibly can. Let’s get them to develop SDKs, write articles for us on their blog, speak at our events, favorite things on hacker news, and whatever activities that we can. Your objective is to extract as much value from your API operations as you possibly can, and give nothing back. Expect developers to work for free. Expect your hackathons attendees to come up with the next great idea, build it, and hand it over to you for very little in return. This isn’t a partnership, this is an API ecosystem, and your team is determined to win at all costs. Your API isn’t a two-way street. All roads lead to your success, and your bosses getting what they want. You don’t care that 3rd party developers should be compensated, or that they have any rights to their intellectual property. The 5% of them that successfully build applications, we will offer them a job in exchange for it, or we’ll just replicate it internally, decrease their rate limits, and increase their error rates so that they can’t compete. Sure you want people to still feel inspired, but not...[<a href="/2017/08/29/extract-as-much-value-as-you-can-from-your-api-community-and-give-nothing-back/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/35910978054_906047b6cb_z.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/28/your-internal-dysfunction-is-not-my-api-problem/">Your Internal Dysfunction Is Not My API Problem</a></h3>
			<p><em>28 Aug 2017</em></p>
			<p>You hear a lot of discussion regarding public API vs private API. From my vantage point there is only web APIs that use public DNS, but I find that folks hung up on the separation usually have many other hangups about things they like to keep behind the firewall, and under the umbrella of private. These are usually the same folks who like to tell me that my public API stories don’t apply to them, and when you engage these folks in any ongoing fashion you tend to find that they are looking to keep a whole lot of dysfunction out of view from the public, and all the talk really has very little to do with APIs. I spend my days studying the best practices across the leading API providers, and understanding what is working and what is not working when it comes to operating APIs. I have seven years of research I’m happy to share with folks, and I entertain a number requests to jump on calls, participate in webinars, do hangouts, and go onsite to do workshops and talks. I’m usually happy to do these things, and when it is a government agency, non-profit organization, and sometimes higher educational institutions, I am happy to these things at no charge. I like sharing what I know, and letting folks decide what they can use from the knowledge I’ve aggregated. When I engage with folks I expect folks to not always be trusted–they don’t know me. However, I’m always surprised when folks think I have an agenda, looking to change them too fast, that I’m trying to shove something down their throat, and disrupt their position. First, I am always being invited in. I’m not a sales guy. I do not have anything to sell you except for my knowledge (which revenue goes to just goes back into doing what I do). There is regularly the quiet IT person who has carefully defended their...[<a href="/2017/08/28/your-internal-dysfunction-is-not-my-api-problem/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/Daimler_Reitwagen_color_drawing_1885%20%20DE%20patent%2036423%20-%20Basic%20original%20patent%20first%20motorcycle%20in%20the%20world.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/28/this-weeks-troubling-api-patent/">This Weeks Troubling API Patent</a></h3>
			<p><em>28 Aug 2017</em></p>
			<p>I found myself looped into another API patent situation. I’m going to write this up as I would any other patent story, then I will go deeper because of my deeper personal connection to this one, but I wanted to make sure I called this patent what it is, and what ALL API patents are–a bad idea. Today’s patent is for an automatch process and system for software development kit for application programming interface: Title: Automatch process and system for software development kit for application programming interface Patent# : US 20170102925 A1 Abstract: A computer system and process is provided to generate computer programming code, such as in a Software Development Kit (SDK). The SDK generated allows an application to use a given API. An API description interface of the system is operable to receive API-description code describing one or more endpoints of the API. A template interface is operable to receive one or more templates of code defining classes and/or functions in a programming language which can be selected by the selection of a set of templates. A data store is operable to use a defined data structure to store records of API description code to provide a structured stored description of the API. A code generation module is operable to combine records of API with templates of code which are arranged in sets by the language of the code they contain. The combining of records and code from templates may use pointers to a data structure which is common to corresponding templates in different sets to allow templates of selected languages to be combined with any API description stored. Original Assignee: Syed Adeel Ali, Zeeshan Bhatti, Parthasarathi Roop, APIMatic Limited If you have been in the API space for as long as I have you know that the generation of API SDKs using an API definition is not original or new, it is something that has been going on for quite some time,...[<a href="/2017/08/28/this-weeks-troubling-api-patent/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/google-maps/google-maps-hurricane-harvey.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/28/disaster-api-rate-limit-considerations/">Disaster API Rate Limit Considerations</a></h3>
			<p><em>28 Aug 2017</em></p>
			<p>This API operations consideration won’t apply to every API, but for APIs that provide essential resources in a time of need, I wanted to highlight an API rate limit cry for help that came across my desk this weekend. Our friend over at Pinboard alerted me to someone in Texas asking for some help in getting Google to increase the Google Maps API rate limits for an app they were depending on as Hurricane Harvey: Hey @google @googlemaps @googlemapsapi can you please remove the limit on api access for @atxfloods? This is an emergency and we rely on it.&mdash; Jen Savage (@savagejen) August 27, 2017 The app they depended on had ceased working and was showing a Google Maps API rate limit error, and they were trying to get the attention of Google to help increase usage limits. As Pinboard points out, it would be nice if Google had more direct support channels to make requests like this, but it would be also great if API providers were monitoring API usage, aware of applications serving geographic locations being impacted, and would relax API rate limiting on their own. There are many reasons API providers leverage their API management infrastructure to make rate limit exceptions and natural disasters seems like it should be top of the list. I don’t think API providers are being malicious with rate limits in this area. I just think it is yet another area where technologists are blind to the way technology is making an impact (positive or negative) on the world around us. Staying in tune to the needs of applications that help people in their time of need seems like it will have to components, 1) knowing your applications (you should be doing this anyways) and identifying the ones that have a public service, and 2) staying in tune with natural and other disasters that are happening around the world. We see larger platforms like Facebook and Twitter rolling...[<a href="/2017/08/28/disaster-api-rate-limit-considerations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/36349140070_d5ec39cb34_z.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/28/api-rants-vs-api-research/">API Rants vs. API Research</a></h3>
			<p><em>28 Aug 2017</em></p>
			<p>I know many of you read my blog for the valuable nuggets of information extracted from my regular research into the world of APIs. I spend a great deal of time sifting through very boring, mundane, and sometimes valuable API related goings on. I have managed to muster the energy each week for the last seven years to sift through thousands of feeds, Tweets, and Github repositories looking for nuggets of API wisdom, best practices, and sometimes bad practices, to share here on the blog. Some weeks I find this an easy task, something I really enjoy the process, but most weeks it is a chore–some weeks I don’t give a shit at all. This is one of those weeks. Well, last week was too, but instead of NO blog posts, this week I’m going to shift things up so that I can get on track. I have had series of folks piss in my Cheerios lately, regarding the free and unpaid work that I do, and as a result, I find myself without any writing mojo for the second week in a row, and not caring about sifting through all your API startup blah blah blah. 1/3 is about being rude and bro assholes, 1/3 of it is that APIs are boring and y’all have no imagination, and 1/3 of it is I’m a mentally ill asshole. The result of all of this is that you get a week full of API rants, instead of API research. Sooooooo, if this side of my personality turns you off, I recommend you tuning out API Evangelist for at least a week, until I feel better, and find the energy to do what it is that I do. There will still be plenty of substance in my posts, and things will still be VERY API related, and something that you can apply in your regular work, I will just be taking off all filters, and it will...[<a href="/2017/08/28/api-rants-vs-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/tony-tam.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/22/thank-you-tony/">Thank You Tony</a></h3>
			<p><em>22 Aug 2017</em></p>
			<p>Tony Tam, the creator of the OpenAPI specification, formerly known as Swagger, has announced he will be exiting his role at OAI and SmartBear. Tony says the specification is in good hands with Ron Ratovsky (@webron), Darrel Miller (@darrel_miller), and others in the OAI. Tony doesn’t give any hints about what he’ll be up to, but will be walking away from his baby entirely. I have given Tony a hard time during the transition from Wordnik to SmartBear, and the creation of the OpenAPI, but I am a huge fan of what he has done, and super bummed to see him go–hoping he won’t leave the API community completely. There are many building blocks that go into doing APIs and OpenAPI, or Swagger, is the most significant single building block that has emerged in the seven years I’ve been doing API Evangelist. Swagger has had a profound impact on the world of APIs, and OpenAPI will continue doing this in the future, if the right conditions are still present across the API landscape. Swagger has helped us talk about our APIs. Swagger has helped us collaborate around our APIs. Swagger has opened up a whole lifecycle of API tooling to help us along our journey. I always felt like Swagger reflected Tony’s personality, and with it’s evolution to OpenAPI, and the OpenAPI Initiative means it’s grown beyond it’s creator. OpenAPI is in good hands. I think it is a good time for Tony to step away, and feel like his baby has begun to grow up, becoming much bigger than what he can do on his own (even with Ron’s amazing help). Thank you for all your work Tony. You made your mark on the API space. You managed to develop something that was useful for API documentation and code generation, but quickly became about design, testing, monitoring, and every other stops along the API lifecycle. I am stoked to have had the chance to...[<a href="/2017/08/22/thank-you-tony/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/facebook/facebook-blueprint-screenshot.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/22/looking-at-facebook-blueprint-as-i-study-api-training-programs/">Looking At Facebook Blueprint As I Study API Training Programs</a></h3>
			<p><em>22 Aug 2017</em></p>
			<p>I am preparing a training section of my API Evangelist research, and part of the process involves learning about what other API providers and API service providers are up to in this area. On my list to look through is Facebook Blueprint, their training area for the platform. The courses present there aren’t specifically for the Facebook API, and is targeting primarily business uses, but the approach translates to API focused training materials, and showcases what is a priority for Facebook when it comes to educating their platform consumers. As part of my API training research I want to understand the building blocks employed by Facebook so that I can apply as part of my API Evangelist training efforts, and help other API providers and service providers apply as part of their operations as well. Here are a few of the API training building blocks I found present: Courses - A variety of online courses that teach you about everything Facebook. Webinars - The webinars they provide around the content they are publishing. Live - The live, in person workshops and courses they provide around the world. Case Studies - Case studies of companies who have used Facebook courses. Press - Press about the Facebook Blueprint, and how they are spreading the word. Certifications - Facebook specific certifications that you can archive. Exams - The tests that are available around the facebook courses. How it Works - Some details about how Facebook training works. Policies - The legal side of things, covering all of our bases. FAQ - Some of the frequently asked questions around the training platform. Support &amp; Help - Where you can get support and more help when it comes to training. Facebook breaks down their training into course categories and learning paths, providing two main ways for potential students to find what they are looking for. Facebook Blueprint provides…well, a blueprint that other API providers and service providers can consider when...[<a href="/2017/08/22/looking-at-facebook-blueprint-as-i-study-api-training-programs/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/weed/mendocino-county-split-from-state-on-cannabis+track-and-trace.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/22/county-level-marijuana-regulation-in-california-using-apis/">County Level Marijuana Regulation In California Using APIs</a></h3>
			<p><em>22 Aug 2017</em></p>
			<p>Counties across the State of California are scrambling to get everything in order now that marijuana is legal, and the 3rd party vendors working with the state are using an API to try and bridge the regulatory needs of each county, as they look to regulate the brand new industry. It sounds like the marijuana regulatory API isn’t 100% ready for prime time, but it is interesting to hear that state is looking to “mitigate the burden of counties” when it comes to production of marijuana using APIs. I have been curating news about APIs in use across the growing marijuana industry, but this is the fist story I’ve written on the subject. Now that I’m seeing APIs use as part of the regulatory engine for the industry, things are getting a little more real, and not just be about finding seeds, stores, and other industry data. I’ll keep scratching around to see what I can find out about the software vendors mentioned in the article, and see if I can get my hands on any documentation, or a link to any active portal. I’m curious to see where this marijuana regulatory API train is headed. Since the marijuana industry is a completely new one for cities, counties, and states to manage, there is an opportunity to leverage new technology like APIs as part of the interactions between government entities, with the help of 3rd party providers. Maybe there is even some opportunity for revenue generation on top of these APIs, allowing for government to fund the software development in a way that it could also be used across other government systems. Push things forward with the rollout and expansion of the marijuana industry, and use that to fund other government systems that lack the funds, and are often years behind. The problems states face in working with counties doesn’t stop with this new marijuana industry, and there are so many other aspects of government...[<a href="/2017/08/22/county-level-marijuana-regulation-in-california-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/290x195cloudsecurity2014.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/22/considering-how-machine-learning-apis-might-violate-privacy-and-security/">Considering How Machine Learning APIs Might Violate Privacy and Security</a></h3>
			<p><em>22 Aug 2017</em></p>
			<p>I was reading about how Carbon Black, an endpoint detection and response (EDR) service, was exposing customer data via a 3r party API service they were using. The endpoint detection and response provider allows customers to optionally scan system and program files using the VirusTotal service. Carbon Black did not realize that premium subscribers of the VirusTotal service get access to the submitted files, allowing an company or government agency with premium access to VirusTotal’s application programming interface (API) can mine those files for sensitive data. It provides a pretty scary glimpse at the future of privacy and security in a world of 3rd party APIs if we don’t think deeply about the solutions we bake into our applications and services. Each API we bake into our applications should always be scrutinized for privacy and security concerns, making sure end-users aren’t being subjected to unnecessary situations. This situation sounds like it was both API provider and consumer contributing to the privacy violation, and adjusting platform access levels, and communicating with API consumers would be the best path forward. Beyond just this situation, I wanted to write about this topic as a cautionary tale for the unfolding machine learning API landscape. Make sure we are thinking deeply about what data and content we are making available to platforms via artificial intelligence and machine learning APIs. Make sure we are asking the hard questions about the security and privacy of data and content we are running through machine learning APIs. Make sure we are thinking deeply about what data and content sets we are running through the machine learning APIs, and reducing any unnecessary exposure of personal data, content, and media. It is easy to be captivated by the magic of artificial intelligence and machine learning APIs. It is easy to view APIs as something external, and not much of a privacy or security threat. However, with each API call we are inviting a 3rd party API...[<a href="/2017/08/22/considering-how-machine-learning-apis-might-violate-privacy-and-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/supreme-court-statues.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/21/the-first-question-when-starting-an-api-is-always-should-we-be-doing-this/">The First Question When Starting An API Is Always: Should We Be Doing This?</a></h3>
			<p><em>21 Aug 2017</em></p>
			<p>I was doing some more work on my list of potential female speakers from the API space. I have some slots to fill for @APIStrat, and I saw another API event was looking for suggestions when it came to speakers. A perfect time to invest some more cycles into finding female API talent. Twitter and Github is always where I go for discovery. I picked up where I left off working on this last time, turned on my search tools that use the Twitter and Github API, and got to work enriching the algorithm that drives my API talent search. Next up on my task list was to deploy a name microservice, that would help me filter Twitter and Github users by gender. I’m interested in API folks of all type, but for this round I need to be able to weight by female. I found a list of the top names from the United States which had them broken down by gender. I copied and pasted into a Google Sheet, fired up a Github repository, and published the spreadsheet of data to Github as YAML–giving me a male.yaml, and female.yaml listing of names. I will be be use these names in a variety of web and API applications, but I wanted to be able to help filter any search results by a female name for this project. I understand the limitations of this approach, but it is good enough for what I am looking to accomplish today. Next, I use my new name microservice as a filter for any Twitter or Github account I’m paying attention to as part of my API monitoring. Quickly giving me a list of accounts to look through as I am developing my list of women doing interesting things with APIs. Once I’m done I have a list of Twitter accounts, and Github accounts, I prepare them as a Google Sheet, then get ready to publish the YAML within...[<a href="/2017/08/21/the-first-question-when-starting-an-api-is-always-should-we-be-doing-this/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/border-traffic.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/21/making-sense-of-api-activity-with-webhook-events/">Making Sense Of API Activity With Webhook Events</a></h3>
			<p><em>21 Aug 2017</em></p>
			<p>I was doing some webhooks research as part of my human services work and I found myself studying the types of events used as part of webhook orchestration for Github, Box, Stripe, and Slack. Each of the event type lists for each of these platforms tell a lot about what is possible with each API, and the webhooks that get triggered as part of these events show what is important to developers who are integrating with each of these APIs. These event type lists really help make sense of the API activity for each of these APIs, providing a nice list to follow when developing your integration strategy. What I really like as I look through each of these webhook event lists is that they are usually in pretty plain language, describing events that matter, not just row updates with a timestamp. These events can be very broad, triggering a webhook when anything happens to a resource, or it could be granular and be all about a specific type of change, or anything in between. Each event type represents something API consumers want from an API, and would be normally polling the API for, but since there are webhook events, developers can get a push of data or a ping whenever an event occurs. Another thing that the presence of webhooks, and a robust list of events represent for me is the maturity of a platform. Github, Box, Stripe, and Slack are all very mature and robust platforms. There are meaningful events defined, and the platform behaves as a two way street, accepting requests, but also making requests whenever a meaningful event occurs. I’m getting to a place where I feel like basic webhook infrastructure should be default for all API providers. The problem is I don’t think there are well enough defined models for API providers to follow when they are planning this part of their API operations. Something we will need to tackle...[<a href="/2017/08/21/making-sense-of-api-activity-with-webhook-events/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/drone-recovery/diamond04.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/21/api-foraging-and-wildcraft/">API Foraging And Wildcraft</a></h3>
			<p><em>21 Aug 2017</em></p>
			<p>I was in Colorado this last week at a CA internal gathering listening to my friend Erik Wilde talking about APIs. One concept he touched on was what he called API gardening, where different types of API providers approached the planting, cultivating, and maintenance of their gardens in a variety of different ways. I really like this concept, and will be working my way through the slide deck from his talk, and see what else I can learn from his work. As he was talking I was thinking about a project I had just published to Github, which leverages the Google Sheets API, and the Github API to publish data as YAML, so I could publish as a simple set of static APIs. I’d consider this approach to be more about foraging and wildcrafting, then it is about tending to my API garden. My API garden just happens spread across a variety of services, often free and public space, in many different regions. I will pay for a service when it is needed, but I’d rather tend smaller, wilder, APIs, that grow in existing spaces–in the cracks. I use free services, and build upon the APIs for the services I am already using. I publish calendars to Google Calendar, then pull data and public APIs using the Google APIs. I use the Flickr and Instagram APIs for storing, publishing, sharing, and integrating with my applications using their APIs. I pull data from the Twitter API, Reddit API, and store in Google Sheets. All of these calendar, image, messaging, and link data will ultimately get published to Github as YAML, which then is shared as XML, JSON, Atom, CSV via static APIs. I am always foraging for data using public services, then planting the seeds, and wildcrafting other APIs–in hopes something will eventually grow. Right now Github is my primary jam, but since I’m just publishing static JSON, XML, YAML, and other media types, it can...[<a href="/2017/08/21/api-foraging-and-wildcraft/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-deploy.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/21/api-deployment-comes-in-many-shapes-and-sizes/">API Deployment Comes In Many Shapes And Sizes</a></h3>
			<p><em>21 Aug 2017</em></p>
			<p>Deploying an API is an interesting concept that I’ve noticed folks struggle with a little bit when I bring it up. My research into API deployment was born back in 2011 and 2012 when my readers would ask me which API management provider would help them deploy an API. How you actually deploy an API varies pretty widely from company to company. Some rely on gateways to deploy and API from an existing backend system. Some hand-craft their own API using open source API frameworks like Tyk and deploy alongside your existing web real estate. Others rely on software as a services solutions like Restlet and Dreamfactory to connect to a data or content source and deploy an API in the clouds. Many folks I talk with simply see this as developing their APIs. I prefer to break out development into define, design, deploy, and then ultimately manage. In my experience, a properly defined and designed API can be deployed into a variety of environments. The resulting OpenAPI or other definition can be used to generate server side code necessary to deploy an API, or maybe used in a gateway solution like AWS API Gateway. For me, API deployment isn’t just about the deployment of code behind a single API, it includes the question about where we are deploying the code, acknowledging that there are many places available when it comes to deploying our API resources. API deployment can be permanent, ephemeral, or maybe just a sandbox. API deployment can be in a Docker container, which by default deploys APIs for controlling the underlying compute for the API you deploying. Most importantly, API deployment should be a planned, well-honed event. APIs should be able to be redeployed, load-balanced, and taken down without friction. It can be easy to find one way of deploying APIs, maybe using similar practices surrounding web deployments, or dependent on one gateway or cloud service. Ideally, API deployment comes in many...[<a href="/2017/08/21/api-deployment-comes-in-many-shapes-and-sizes/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/runscope/runscope-200-ok.jpeg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/18/http-status-codes-and-the-politics-of-apis/">HTTP Status Codes And The Politics Of APIs</a></h3>
			<p><em>18 Aug 2017</em></p>
			<p>The more I learn about the world of APIs, the more I understand how technology, business, and politics are all woven together into one often immovable monolith. Many things in the world of APIs seem purely like a technical thing, but in reality they are wrapped in, and wielded intentionally and unintentionally as part of larger business, and sometimes a personal agenda. An example of this can be found with the presence, or lack of presence with HTTP status codes, which the default status is usually 200 OK, 404 not found, or 500 internal error. While these seem like very granular technical details of whether or not an HTML, XML, CSV, or JSON document is returned or not as part of a single web request, there usage often dictates what is happening behind the firewall, and often times more importantly, what is not happening. I find people’s awareness that HTTP status codes exist (or not) a significant sign of their view of the wider web world. If they are aware they exist they most likely have some experience engaging with other experienced partners using the web. If they don’t, they most likely live a pretty isolated existence–even if they do have a web presence. Beyond just knowing that HTTP status codes exist, understanding the importance of, and the nuance surrounding each individual one demonstrates you are used to engaging with external actors at scale, leveraging web technology. I have to put out there that I DO NOT have an intimate knowledge of all HTTP status codes, because I have not exercised them as part of large scale projects, but it is something I do grasp the scope and importance of from the projects I have worked on. This is not a boolean thing, you knowing HTTP status codes or not. This is the result of many journeys, with many partners, across many different types of digital resources. You can tell how many journeys someone has...[<a href="/2017/08/18/http-status-codes-and-the-politics-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/good-api/good-api-agency.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/17/where-are-all-the-api-focused-agencies/">Where Are All The API Focused Agencies?</a></h3>
			<p><em>17 Aug 2017</em></p>
			<p>Earlier this week at the CA API Academy virtual gathering I spoke at in Boulder CO, the question around why there aren’t more API focused agencies came up. We were talking about the need for consulting services around common areas of API operations like design, deployment, management, testing, as well as training around API lifecycle related topics. We are seeing some movement in the area of API focused agencies, but not enough to cover the current demand. We are seeing full service shops like APIvista, and Good API emerge. There is also movement on the agency level when it comes to integration platform as a service (iPaaS), over at Left Hook Digital, helping companies leverage Zapier, and integrate with API platforms. There is definitely significant movement in the number of API focused agencies, but we are going to need more to meet the demand for API design, deployment, management, testing, and other stops along the API lifecycle. I’m guessing it will take a couple years for this side of the API business to mature. Then I’m figuring we will begin see to see more specialized API agencies emerge, helping with evangelism, support, API design, and maybe even industry focused iPaaS, similar to what we are seeing with LeftHook. As mainstream companies are waking up to the potential of APIs, they are going to need a lot of professional help to ensure they are successful in their API journey. We are going to need a number of general service, specialized, regional, and industry specific API agencies to help get us through the next couple of years. I am focusing on trying to scale the API training portion of this need. I am ramping up development of API training courses that span my API lifecycle and API stack research. Helping API providers and consumers navigate the world of APIs. I’m talking with partners about working together to develop and distribute API training, and will be working to...[<a href="/2017/08/17/where-are-all-the-api-focused-agencies/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/uspto/uspto-pair-bulk-data-api.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/17/the-patent-application-information-retrieval-bulk-data-api/">The Patent Application Information Retrieval Bulk Data API</a></h3>
			<p><em>17 Aug 2017</em></p>
			<p>
I stumbled across the Patent Application Information Retrieval Bulk Data API from the US Patent Office the other day. It provides a much more usable approach to getting at patent information than what I am using at the moment. Right now I am downloading XML files and searching for the occurrence of a handful of keywords. If I want to make a change I have to fire up a new AWS instance, change the code, and reprocess the downloaded files. The Patent Application Information Retrieval Bulk Data API gives me a much more efficient interface to work with.

The Patent Application Information Retrieval Bulk Data API contains the bibliographic, published document and patent term extension data tabs in Public PAIR from 1981 to present, with some additional data dating back to 1931. It has leveraged COTS semantics, maintains an open architecture, and the query syntax follows the standard Apache Solr search syntax, with API responses following the Solr formats. Providing for a much more powerful interface for querying patent data, which goes back further back in time then what I’ve been doing currently. I’m really interested in doing an API patent search for the 1990s, or maybe even earlier.

The Patent Application Information Retrieval Bulk Data API is a well designed API, with an attractive API portal and documentation, driven with an OpenAPI. The USPTO provide access to the patent data in the way that I think all government agencies should be doing it. You can use the API, or get at a JSON or XML download of the data. The API is “part of the US Patent and Trademark Office’s (USPTO) commitment to fostering a culture of open government as described by the 2013 Executive Order to make open and machine-readable data the new default for government information.” Which is pretty cool in my book, something we desperately are needing to become a reality across all federal agencies in 2017.

[<a href="/2017/08/17/the-patent-application-information-retrieval-bulk-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-asterisk.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/16/wildcard-webhook-events/">Wildcard Webhook Events</a></h3>
			<p><em>16 Aug 2017</em></p>
			<p>
I have been studying the approach of a variety of webhook implementations in preparation for an API consulting project I’m working on. Even though I’m very familiar with how webhooks works, and confident in my ability to design and develop a solution, I’m ALWAYS looking to understand what leading API providers are up to, and how I can improve my knowledge and awareness.

With his round of research, Github has provided me with several webhook nuggets for my API storytelling notebook. One of their web features I though was the notion of a wildcard webhook event:


  Wildcard Event - We also support a wildcard (*) that will match all supported events. When you add the wildcard event, we’ll replace any existing events you have configured with the wildcard event and send you payloads for all supported events. You’ll also automatically get any new events we might add in the future.


I have been working to identify a set of objects and associated webhook events, and the notion of a wildcard event is interesting. It seems like you could apply this globally, or to specific objects / resources, allowing you to get pushes for any events that occur. I’m not sure I’ll have time to apply this feature in my current project, but it is worth adding to my webhook toolbox for future projects.

There are three other features I’ve extracted from Github’s approach to webhooks that I’ve added to my API storytelling notebook, to hopefully craft into future blog posts. I’ll be adding these all as potential webhook building blocks that API providers can consider. I’m hoping to find some more time and money to invest into my webhook research this fall, and be able to publish a formal guide for the world of webhooks. I’m always surprised by the lack of formal guidance when it comes to webhooks, and is something I’d like to see change in the near future.

[<a href="/2017/08/16/wildcard-webhook-events/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/ca-panel.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/16/the-importance-of-api-stories/">The Importance Of API Stories</a></h3>
			<p><em>16 Aug 2017</em></p>
			<p>I am an API storyteller before am an API architect, designer, or evangelist. My number one job is to tell stories about the API space. I make sure there is always (almost) 3-5 stories a day published to API Evangelist about what I’m seeing as I conduct my research on the sector, and thoughts I’m having while consulting and working on API projects. I’ve been telling stories like this for seven years, which has proven to me how much stories matter in the world of technology, and the worlds that it is impacting–which is pretty much everything right now. Occasionally I get folks who like to criticize what I do, making sure I know that stories don’t matter. That nobody in the enterprise or startups care about stories. Results are what matter. Ohhhhh reeeaaaly. ;-) I hate to tell you, it is all stories. VC investment in startups is all about the story. The markets all operate on stories. Twitter. Facebook. LinkedIn. Medium. TechCrunch. It is all stories. The stories we tell ourselves. The stories we tell each other. The stories we believe. The stories we refuse to believe. It is all stories. Stories are important to everything. The mythical story about Jeff Bezos’s mandate that all employees needed to use APIs internally is still 2-3% of my monthly traffic, down from 5-8% for the last couple of years, and it was written in 2012 (five years ago). I’ve seen this story on the home page of the GSA internal portal, and framed hanging on the wall in a bank in Amsterdam. Stories are important. Stories are still important when they aren’t true, or partially true, like the Amazon mythical tale is(n’t). Stories are how we make sense of all this abstract stuff, and turn it into relatable concepts that we can use within the constructs of our own worlds. Stories are how the cloud became a thing. Stories are why microservices and Devops is...[<a href="/2017/08/16/the-importance-of-api-stories/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-lifecycle.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/16/the-85-stops-along-the-api-lifecycle-that-i-track-on/">The 85 Stops Along The API Lifecycle That I Track On</a></h3>
			<p><em>16 Aug 2017</em></p>
			<p>I am preparing a talk for tomorrow, and I needed a new list of each stop along the API lifecycle, and since each of my project exist as Github repositories, and are defined as a YAML and JSON data store, I can simply define a new liquid template for generating a new HTML listing of all the stops along the API lifecycle–after generating this list I figured I’d share here as a story. Here are the 85 stops along the API lifecycle landscape from my vantage point as the API Evangelist: Definitions Design Versioning Hypermedia DNS Low Hanging Fruit Scraping Database Deployment Rogue Microservices Algorithms Search Machine Learning Proxy Virtualization Containers Management Serverless Portal Getting Started Documentation Frequently Asked Questions Support Communications Road Map Issues Change Log Monitoring Testing Performance Caching Reliability Authentication Encryption Vulnerabilities Breaches Security Terms of Service (TOS) Surveillance Privacy Cybersecurity Reclaim Transparency Observability Licensing Copyright Accessibility Branding Regulation Patents Discovery Client Command Line Interface Bots Internet of Things Industrial Network IDE SDK Plugin Browsers Embeddable Visualization Analysis Logging Aggregation iPaaS Webhooks Integrations Migration Backups Real Time Orchestration Voice Spreadsheets Investment Monetization Plans Partners Certification Acquisitions Evangelism Showcase Deprecation I’m always presenting my API lifecycle research as a listing, or in a linear fashion. I always feel like I should be creating an actual lifecycle visualization, but then I always end up feeling like I should just invest in my subway API map work, and create more robust way to represent how the API lifecycle truly looks. Anyways, these 85 areas represent the scope of my API industry research, and provide a framework for thinking about not just the individual API lifecycle, but also the bigger picture of our API operations and partnerships. Not all of these areas apply to every API provider, but they do provide one perspective of the API landscape that all API providers can learn from. If there are any other stops along the lifecycle you think should...[<a href="/2017/08/16/the-85-stops-along-the-api-lifecycle-that-i-track-on/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-alphabet-apple.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/16/api-kindergarten-for-business-and-it-leaders/">API Kindergarten For Business And IT Leaders</a></h3>
			<p><em>16 Aug 2017</em></p>
			<p>
I’m working on a number of API courses and lessons lately. Some of these are API 101 courses, while others are more advanced courses for the seasoned API provider, and consumer. As I think about what is needed when it comes to classes and workshops across the API sector, I’m considering doing an API Kindergarten series, where business and IT leaders can learn the basics of doing business with APIs.

The curriculum for the API kindergarten program include hands on lessons on how to play nicely, get along with others, the importance of sharing, and helping them learn the important soft skills like not shitting your pants. I’m always surprised at the lack of basic skills by company, organizational, institutional, and government leadership when it comes to the essentials of why APIs work, and think a little primer on things might help some realize they shouldn’t be doing APIs in the first place, or maybe prevent some major crisis down the road.

The number of folks who tell me directly that they are all in on this API thing, yet when it comes to practice clearly are not has grown significantly in 2017. I feel like we need a whole series or curriculum to help make sure business and IT leadership is up for the challenge is desperately needed. Just wait, until I begin working on my sex edit course for the middle schoolers, where we teach them about safely integrating, and what is appropriate API behavior, and what is not. Honestly, I could spend days finding equivalences between the real world and doing APIs (not real world) and creating classes around them–fun stuff!

[<a href="/2017/08/16/api-kindergarten-for-business-and-it-leaders/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bots/bots-slack-search.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/15/which-platforms-have-control-over-the-conversation-around-their-bots/">Which Platforms Have Control Over The Conversation Around Their Bots</a></h3>
			<p><em>15 Aug 2017</em></p>
			<p>I spend a lot of time monitoring API platforms, thinking about different ways of identifying which ones are taking control of the conversation around how their platforms operate. One example of this out in the wild can be found when it comes to bots, by doing a quick look at which of the major bot platforms own the conversation around this automation going on via their platforms. First you take a look at Twitter, by doing a quick Google search for Twitter Bots: Then you take a look at Facebook, by doing a quick Google search for Facebook Bots: Finally take a look at Slack, by doing a quick Google search for Slack Bots: It is pretty clear who owns the conversation when it comes to bots on their platform. While Twitter and Facebook both have information and guidance about doing bots they do not own the conversation like Slack does. Something that is reflected in the search engine placement. It is also something that sets the tone of the conversation that is going on within the community, and defines the types of bots that will emerge on the platform. As I’ve said before, if you have a web or mobile property online today, you need to be owning the conversation around your API or someone eventually will own it for you. The same comes to automation around your platform, and the introduction of bots, and automated users, traffic, advertising, and other aspects of doing business online today. Honestly, I wouldn’t want to be in the business of running a platform these days. It is why I work so hard to dominate and own my own presence, just so that I can beat back what is said about me, and own the conversation on at least Google, Twitter, LinkedIn, Facebook, and Github. Seems like to me, if you are going to enable automation on your platform via APIs, it should be something that you own...[<a href="/2017/08/15/which-platforms-have-control-over-the-conversation-around-their-bots/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/webhooks/RESTHooks.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/15/where-to-begin-with-webhooks-for-the-human-services-data-api/">Where To Begin With Webhooks For The Human Services Data API</a></h3>
			<p><em>15 Aug 2017</em></p>
			<p>I am getting to work on a base set of webhook specification for my human services data API work, and I wanted to take a fresh drive through a handful of the leading APIs I’m tracking on. I’m needing to make some recommendations regarding how human services data APIs should be pushing information via APIs, as we as providing APIs. Webhooks are fascinating to me because they really are just APIs in reverse. Webhooks are just an API request, where the target URL is a variable, allowing an API call to be made from a platform, to any target URL, on an triggering events, or on a schedule as a job. Here are six of the API providers I took a look at while doing this webhook research: Box Gumroad Venmo Github Stripe Slack All of these API providers offer webhooks, allowing developers to create an API call that will be fired off when a specific event occurs. These events are usually tied to a specific object. Box is documents. Github is a repository. Stripe is a payment. With human services it will be an organization, location, or service. There are a handful of key concepts at play when it comes to webhooks, making them an important part of the equation: Object - The object in which an event is occurring. For this project it is organizations, locations, services, contacts, and potentially other elements of API operations. Events - This is a list of events that can occur against all the objects that will trigger the execution of a webhook. Target - The URL of the webhook. This is the variable of the outgoing API call, allowing them to be defined by API consumers, nd executed by the API provider. Fat - The webhook will carry a payload, submitting a predefined schema, usually of the associated object to the target. Ping - The webhook does not carry a payload, simply pinging the target of a...[<a href="/2017/08/15/where-to-begin-with-webhooks-for-the-human-services-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/elastic-search/elasticsearch-security-apis.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/15/the-elasticsearch-security-apis/">The ElasticSearch Security APIs</a></h3>
			<p><em>15 Aug 2017</em></p>
			<p>I was looking at the set of security APIs over at Elasticsearch as I was diving into my API security research recently. I thought the areas they provide security APIs for the search platform was worth noting and including in not just my API security research, but also search, deployment, and probably overlap with my authentication research. Authenticate API - The Authenticate API enables you to submit a request with a basic auth header to authenticate a user and retrieve information about the authenticated user. Clear Cache API - The Clear Cache API evicts users from the user cache. You can completely clear the cache or evict specific users. User Management APIs - The user API enables you to create, read, update, and delete users from the native realm. These users are commonly referred to as native users. Role Management APIs - The Roles API enables you to add, remove, and retrieve roles in the native realm. To use this API, you must have at least the manage_security cluster privilege. Role Mapping APIs - The Role Mapping API enables you to add, remove, and retrieve role-mappings. To use this API, you must have at least the manage_security cluster privilege. Privilege APIs - The has_privileges API allows you to determine whether the logged in user has a specified list of privileges. Token Management APIs - The token API enables you to create and invalidate bearer tokens for access without requiring basic authentication. The get token API takes the same parameters as a typical OAuth 2.0 token API except for the use of a JSON request body. Come to think of it, I’ll add this to my API management research as well. Much of this overlaps with what should be a common set of API management services as well. Like much of my research, there are many different dimensions to my API security research. I’m looking to see how API providers are securing their APIs, as well...[<a href="/2017/08/15/the-elasticsearch-security-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-zoomed-in-on-sea_light_dali.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/15/addressing-bulk-api-operations-as-separate-set-of-services/">Addressing Bulk API Operations As Separate Set Of Services</a></h3>
			<p><em>15 Aug 2017</em></p>
			<p>Part of the feedback I’ve received from the Human Services Data API (HSDA) evolution from v1.0 to v1.1 was that the API didn’t allow for volume or bulk GET, POST, PUT, or DELETE. This was intentionally in the incremental release which focused on just making sure the API reflected 100% of the surface are for the Human Services Data Specification (HSDS). I wanted to separate out the needs of bulk API consumers, so that I could think about it separately from the more simple, micro-use integrations the default Human Services Data API would accommodate. I don’t want the industrial grade needs of database and system administrators overriding the simple access needs of other individual API consumers. To kick off my human services bulk API definition I wanted to spend some time looking at other bulk implementations from a handful of leading providers: Intercom ElasticSearch SalesForce Diffbot As I went through these implementations, and searched through Stack Overflow about bulk HTTP POSTs, I really didn’t see much difference on the technical front. Bulk APIs are primarily about acknowledging heavy duty data consumption, and primarily used HTTP POST for allowing the submission of either a) large individual POST, or b) large number of POST. Technically there isn’t much to them, where you start finding the nuance of bulk APIs over regular APIs is in the process surrounding the API implementation, things like having tasks, jobs, history, notifications, webhooks, email, and logging. Meaning there is just a lot more process and expectation around these APIs, which also most likely translates into more robust background architecture, and rules regarding the process involved with access. Bulk APIs seem more about a separation of concerns. Bulk GET and POSTs require more infrastructure, and to do it properly you need process, and checks and balances to make sure a bulk operation is successfully executed, and there is sufficient history, notifications, and other events around bulk transactions. I’d say that bulk API operations...[<a href="/2017/08/15/addressing-bulk-api-operations-as-separate-set-of-services/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/algo-microservices.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/14/some-microservice-thoughts-around-my-human-services-api-work/">Some Microservice Thoughts Around My Human Services API Work</a></h3>
			<p><em>14 Aug 2017</em></p>
			<p>The Human Services Data API I have been working on is about defining a set of API paths for working with organizations, locations, and services that are delivering human services in cities around the world. As I’m working to evolve the OpenAPI for the Human Services Data API (HSDA), I’m constantly mindful of bloat, unnecessary expansion of features, and always working to separate things by concern. My thoughts have evolved this due to a hackathon I attended this week in San Francisco where a team at Optmizely worked to decouple an existing human services application from its backend and help teach it to speak Human Services Data Specification (HSDS)–allowing it to speak a common language around the services that us humans depend on daily. As the hackathon team was decoupling the single page application (React) from the API backend (Firebase) I took the API calls behind and published to Github as two JSON files. One of the files was locations, which contained 217 human service locations in San Francisco, and metadata, which contained a handful of categories being used to organize and display the locations. In this situation, there is no notion of an organization, just 217 locations, offering human services across five categories. This legacy application, and forward engineering hackathon project was quickly becoming a microservices project, ironically it is a microservice project that was about delivering human services. ;-) Looking at this unfolding project through a microservices lens, I needed to provide a single service. In the context of Link-SF, the original project, I needed to offer a service that would deliver 217 locations where people can find human services in the areas of food, housing, hygiene, medical, and technology via an web, or mobile application. To help me achieve my goals I began to step through each of the steps of the lifecycle of any self-contained microservices: Github - Each of my services begins with a Github repository, so I created a...[<a href="/2017/08/14/some-microservice-thoughts-around-my-human-services-api-work/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/14/investing-the-time-to-learn-api-best-practices-so-you-do-not-reinvent-the/">Investing The Time To Learn API Best Practices So You Do Not Reinvent The</a></h3>
			<p><em>14 Aug 2017</em></p>
			<p>I was on a call the other day with a group of people who are in the trenches of organizations and companies working hard to deliver human services in cities around the country. We were meeting to kick of the design phase of a new type of API, and after they shared all their thoughts via project documentation, they were asking me to help identify examples of best practices from the space. The group felt they didn’t have the time, or the awareness of what is going on to be able to identify the best practices that already exist across the space. This is one of the reasons I stay out of the weeds of individual projects. I may help define, design, and even shadow the deployment and management, but I work hard to avoid the tractor beam of ongoing projects so that I can pay attention to the bigger picture and help share stories about what I’m seeing. I feel like there should be people like me in each industry helping shine a light on, and aggregating of best practices when it comes to the API life cycle. There is just too much work to be done, and it helps to have folks who have domain expertise, not just lightly understanding what is going on across many sectors like I do. I think it is fine for the sector to depend on API analysts like me, but I think that groups should also be investing in the time to pick up their heads up and pay attention to what else is going on when it comes to APIs in their industry. I understand that many groups are busy keeping systems operational, and dealing with real world problems, but dedicated reading of blogs, white papers, and tuning into social channels for other API providers is important as well. Each decision made on API design, deployment, and management should be established from time spent reading, and...[<a href="/2017/08/14/investing-the-time-to-learn-api-best-practices-so-you-do-not-reinvent-the/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_dali_three.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/14/decoupling-the-business-of-my-human-services-microservice/">Decoupling The Business Of My Human Services Microservice</a></h3>
			<p><em>14 Aug 2017</em></p>
			<p>I’ve been looking at my human services API work through a microservices lens, triggered by the deployment of a reduced functionality version of the human services implementation I was working on this week. I’m thinking a lot about the technical side of decoupling services using APIs, but I want to also take a moment and think about the business side of decomposing services, while also making sure they are deployed in a way that meets both the provider and consumer side of the business equation. My human services microservice implementation is in the public service, which is a space where the business conversation often seems to disappear behind closed doors, but in reality needs to be front and center with each investment (commit) made into any service. Let’s take a moment to look at the monetization strategy and operational plan for my human service microservice. Yes, a public data microservice should have a monetization strategy and plan for operating and remaining sustainable. The goals for this type of microservice will be radically different than it would be for a commercial microservice, but it should have one all the same. Monetization - How am I evaluating the investment into this project alongside any value that is generated, which I can potentially capture or exchange some value for some money to keep going. Acquisition - What did it take for me to acquire the data and skills necessary to make the deployment possible. Development - What time was invested in setting up the platform, developing the schema, data, definitions, code, and visual elements. Operations - What does it take to operate the service? Maintain it, answer questions, provide support, and other realities of providing an online service today. Direct Value - What are the direct benefits of having this service available to people looking for human services, or organizations looking to provide human services. Indirect Value - What are the indirect benefits of having this service available,...[<a href="/2017/08/14/decoupling-the-business-of-my-human-services-microservice/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/aws-answers-icons.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/14/api-platform-faq-and-qa-responsibility/">API Platform FAQ And QA Responsibility</a></h3>
			<p><em>14 Aug 2017</em></p>
			<p>The discussion around whether or not you should be hosting your own questions and answers (QA) and frequently asked questions (FAQ) for your API has continued, with many of the leading API pioneers asserting responsibility over the operations of these important API resources. Amazon noticed that answers about their platform on Quora and Stack Exchange were usually out of date and often just plain wrong, prompting them to launch their own QA solution. I have written about using API providers using Stack Overflow for may years now. It the last few years I’ve had my readers push back on this for a variety of reasons, from the Stack Overflow community being primarily a white male bro-fest, to finding things being unreliable, out of date, and often a pretty hostile and unfriendly place for people to try and learn about APIs. I’d say that I still use Stack Overflow for about 40% of my querying of API and programming related subjects, but since I’m a white male who has been doing software for 30 years, I’m a little more resistant to the bro-fest. But, I get it, and hear what folks are saying, and get it is not always a suitable environment. Going back and forth on this subject, I’m back in the camp where API providers should be investing in operating their own QA, FAQ, and support forums. It’s definitely requires a significant amount of investment, policing, and sometimes taking stances that are unpopular, but if you are in this for the long game, it will be worth it. After watching AWS for a decade, you can see how incorrect information about your API operations can really begin to become a liability, and you might want to keep a tighter grip on where your API consumers go look for their answers. An added bonus is that you also get to set the tone for the types of questions that get answered, and the inclusiveness that...[<a href="/2017/08/14/api-platform-faq-and-qa-responsibility/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/erik-wilde/link-relation-types-for-web-services.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/11/link-relation-types-for-apis/">Link Relation Types for APIs</a></h3>
			<p><em>11 Aug 2017</em></p>
			<p>
I have been reading through a number of specifications lately, trying to get more up to speed on what standards are available for me to choose from when designing APIs. Next up on my list is Link Relation Types for Web Services, by Erik Wilde. I wanted to take this informational specification and repost here on my site, partially because I find it easier to read, and the process of breaking things down and publishing as a posts helps me digest the specification and absorb more of what it contains.

I’m particularly interested in this one, because Erik captures what I’ve had in my head for APIs.json property types, but haven’t been able to always articulate as well as Erik does, let alone published as an official specification. I think his argument captures the challenge we face with mapping out the structure we have, and how we can balance the web with the API, making sure as much of it becomes machine readable as possible. I’ve grabbed the meat of Link Relation Types for Web Services and pasted here, so I can break down, and reference across my storytelling.

[<a href="/2017/08/11/link-relation-types-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-widgets.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/11/embeddable-api-tooling-discovery-with-json-home/">Embeddable API Tooling Discovery With JSON Home</a></h3>
			<p><em>11 Aug 2017</em></p>
			<p>I have been studying JSON Home, trying to understand how it sizes up to APIs.json, and other formats I’m tracking on like Pivio. JSON Home has a number of interesting features, and I thought one of their examples was also interesting, and was relevant to my API embeddable research. In this example, JSON Home was describing a widget that was putting an API to use as part of its operation.

Here is the snippet from the JSON Home example, providing all details of how it works:



JSON Home seems very action oriented. Everything about the format leads you towards taking some sort of API driven action, something that makes a lot of sense when it comes to widgets and other embeddables. I could see JSON Home being used as some sort of definition for button or widget generation and building tooling, providing a machine readable definition for the embeddable tool, and what is possible with the API(s) behind.

I’ve been working towards embeddable directories and API stacks using APIs.json, providing distributed and embeddable tooling that API providers and consumers can publish anywhere. I will be spending more time thinking about how this world of API discovery can overlap with the world of API embeddables, providing not just a directory of buttons, badges, and widgets, but one that describes what is possible when you engage with any embeddable tool. I’m beginning to see JSON Home similar to how I see Postman Collections, something that is closer to runtime, or at least deploy time. Where APIs.json is much more about indexing, search, and discovery–maybe some detail about where the widgets are, or maybe more detail about what embeddable resources are available.

[<a href="/2017/08/11/embeddable-api-tooling-discovery-with-json-home/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-data-gov/9299911959_bdc195fb56_o.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/11/about-api-data-gov/">About api.data.gov</a></h3>
			<p><em>11 Aug 2017</em></p>
			<p>I’m going to borrow, modify, and improve on the content from api.data.gov, because it is an important effort I want my readers to be aware of, because I want more of them to help apply educate other federal agencies regarding why it is a good idea to bake api.data.gov into their API operations, and help apply pressure until EVERY federal agency is up and running using a common API management layer. Ok, so what is api.data.gov? api.data.gov is a free API management service for federal agencies. Our aim is to make it easier for you to release and manage your APIs. api.data.gov acts as a layer above your existing APIs. It transparently adds extra functionality to your APIs and helps deal with some of the repetitive parts of managing APIs. Here are the features of api.data.gov: You’re in control: You still have complete control of building and hosting your APIs however you like. No changes required: No changes are required to your API, but when it’s accessed through api.data.gov, we’ll transparently add features and handle the boring stuff. Focus on the APIs: You’re freed from worrying about things like API keys, rate limiting, and gathering usage stats, so you can focus on building the next great API. Make it easy for your users: By providing a standard entry point to participating APIs, it’s easier for developers to explore and use APIs across the federal government. api.data.gov handles the API keys for you: API key signup: It’s quick and easy for users to signup for an API key and start using it immediately. Shared across services: Users can reuse their API key across all participating api.data.gov APIs. No coding required: No code changes are required to your API. If your API is being hit through api.data.gov, you can simply assume it’s from a valid user. api.data.gov tracks all the traffic to your API and give you tools to easily analyze it: Demonstrate value: Understand how your...[<a href="/2017/08/11/about-api-data-gov/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/open-referral/optimizely/optimizely-hackathon.JPG" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/11/a-hack-day-event-to-help-the-linksf-app-speak-human-services-data/">A Hack Day Event To Help The Link-SF App Speak Human Services Data</a></h3>
			<p><em>11 Aug 2017</em></p>
			<p>I went up to San Francisco on Wednesday to participate in a social good hack day at Optimizely. They held their event at their downtown offices, where 20+ employees showed up to hack on some social good projects. Open Referral and our partner Benetech had suggested Human Services Data Specification (HSDS) as a possible project, which resulted in us being one of the hack projects for the event. The Open Referral Human Services Data Specification (HSDS) team consisted of five Optimizely developers. Derek Hammond - Software Engineer Michael Fields - Software Engineer Zachary Power - Software Engineering Intern Quinton Dang - Software Engineer Asa Schachar - Engineering Manager The overall strength of the team leaned toward being front-end web and mobile developers, so we decided to “forward engineer” the Link-SF application, which provides a simple web or mobile application to help folks find a variety of human services in a handful of categories like food, housing, hygiene, medical, and technology. Link-SF is an ongoing collaboration between the Tenderloin Technology Lab and Zendesk, Inc., and we wanted to help contribute to their work, while also making the application potentially deployable by other cities and regions. Once the team got to work on the project they identified that we could get at the data behind the SF application. The team decide they would forward engineer the dataset, the API, and the UI for the web and mobile application, making it all speak Human Services Data Specification (HSDS)–here is what they did: Took the Link-SF datasets and saved as a single JSON file. Converted the JSON schema to use HSDS – the changes weren’t significant. Made it so that the app reads location data from a Github repository (you can change this to your url) Updated the taxonomy fetch to use the new data Ensured the UI worked with the new data You can find the project in an Optimizely Github repository, which they are going to invest...[<a href="/2017/08/11/a-hack-day-event-to-help-the-linksf-app-speak-human-services-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/soldier_computer_dark_dali.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/10/patent-number-9325732-computer-security-threat-sharing/">Patent Number 9325732: Computer Security Threat Sharing</a></h3>
			<p><em>10 Aug 2017</em></p>
			<p>The main reason that I tend to rail against API specific patents is that much of what I see being locks up reflects the parts and pieces that are making the web work. I see things like hypermedia, and other concepts that are inherently about sharing, collaboration, and reuse–something that should never be patented. This concept applies to other patents I’m seeing, but rather than being about the web, it is about trust, and sharing of information. Things that shouldn’t be locked up, and exist within realms where the concept of patents actually hurt the web and APIs. Today’s patent is out of Amazon, who are prolific patenters of web and API concepts. This one though is about the sharing of security threat sharing. Outlining something that should be commonplace on the web. Title - Computer security threat sharing Number - 09325732 Owner - Amazon Technologies, Inc. Abstract - A computer security threat sharing technology is described. A computer security threat is recognized at an organization. A partner network graph is queried for security nodes connected to a first security node representing the organization. The first security node is connected to at least a second security node representing a trusted security partner of the organization. The second security node is associated with identification information. The computer security threat recognized by the organization is communicated to the trusted security partner using the identification information associated with the second security node. I’m sorry. I just do not see this as unique, original, or remotely a concept that should be patentable. Similar to a previous patent I wrote about on trust, I just don’t think that sharing of security information needs to be locked up. The USPTO should recognize this. I feel like this type of patent shows how broken the patent process is, and how distorted company’s views on what is a patentable idea. Honestly, these types of patents feel lazy to me, and lack any creativity,...[<a href="/2017/08/10/patent-number-9325732-computer-security-threat-sharing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-bot-api.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/10/observability-in-botnet-takedown-by-government-on-private-infrastructure/">Observability In Botnet Takedown By Government On Private Infrastructure</a></h3>
			<p><em>10 Aug 2017</em></p>
			<p>I’m looking into how to make API security more transparent and observable lately, and looking for examples of companies, institutions, organizations, politicians, and the government are calling for observability into wherever APIs are impacting our world. Today’s example comes out of POLITICO’s Morning Cybersecurity email newsletter, which has become an amazing source of daily information for me, regarding transparency around the take down of bot networks. “If private companies cooperate with government agencies - for example, in the takedown of botnets using the companies’ infrastructure - they should do so as publicly as possible, argued the Center for Democracy &amp; Technology . “One upside to compulsory powers is that they presumptively become public eventually, and are usually overseen by judges or the legislative branch,” CDT argued in its filing. “Voluntary efforts run the risk of operating in the dark and obscuring a level of coordination that would be offensive to the general public. It is imperative that private actors do not evolve into state actors without all the attendant oversight and accountability that comes with the latter.” I’ve been tracking on the transparency statements and initiatives of all the API platforms. At some point I’m going to assemble the common building blocks of what is needed for executing platform transparency, and I will be including these asks of the federal government. As the Center for Democracy &amp; Technology states this relationship between the public and private sector when it comes to platform surveillance needs to be more transparent and observable in all forms. Bots, IoT, and the negative impacts of API automation needs to be included in the transparency disclosure stack. If the government is working with platform to discover, surveil, or shutdown bot networks there should be some point in which operations should be shared, including the details of what was done. We need platform transparency and observability at the public and private sector layer of engagement. Sure, this sharing of information would be...[<a href="/2017/08/10/observability-in-botnet-takedown-by-government-on-private-infrastructure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/public-market.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/10/my-focus-on-public-apis-also-applies-internally/">My Focus On Public APIs Also Applies Internally</a></h3>
			<p><em>10 Aug 2017</em></p>
			<p>A regular thing I hear from folks when we are having conversations about the API lifecycle, is that I focus on public APIs, and they are more interested in private APIs. Each time I hear this I try to take time and assess which parts of my public API research wouldn’t apply to internal APIs. You wouldn’t publish your APIs to pubic API search engines like APIs.io or ProgrammableWeb, and maybe not evangelizing your APIs at hackathons, but I’d say 90% of what I study is applicable to internal APIs, as well as publicly available APIs. With internal APIs, or private network partner APIs you still need a portal, documentation, SDKs, support mechanisms, and communication and feedback loops. Sure, how you use the common building blocks of API operations that I track on will vary between private and public APIs, but this shifts from industry to industry, and API to API as well–it isn’t just a public vs. private thing. I would say that 75% of my API industry research is derived from public API operations–it is just easier to access, and honestly more interesting to cover than private ones. The other 25% of internal API conversation I’m having, always benefit from thinking through the common API building blocks of public APIs, looking for ways they can be more successful with internal and partner APIs. I’d say that a significant part of the mindshare for the microservices philosophy is internally focused. I think this is something that will come back to hurt some implementations, cutting out many of the mechanisms and common elements required in a successful API formula. Things like portals, documentations, SDKs, testing, monitoring, discovery, support, communications all contribute to an API working, or not working. I’ve said it before, and I’ll say it again. I’m not convinced that there is any true separation in public vs private APIs, and there remains to be a great deal we can learn from public API...[<a href="/2017/08/10/my-focus-on-public-apis-also-applies-internally/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/freeway_atari_missle.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/10/image-logging-with-amazon-s3-api/">Image Logging With Amazon S3 API</a></h3>
			<p><em>10 Aug 2017</em></p>
			<p>I have been slowly evolving my network of websites in 2017, overhauling the look of them, as well as how they function. I am investing cycles into pushing as much of my infrastructure towards being as static as possible, minimizing my usage of JavaScript wherever I can. I am still using a significant amount of JavaScript libraries across my sites for a variety of use cases, but whenever I can, I am looking to kill my JavaScript or backend dependencies, and reduce the opportunity for any tracking and surveillance. While I still keep Google Analytics on my primary API Evangelist sites, as my revenue depends on it, whenever possible I keep personal projects without any JavaScript tracking mechanisms. Instead of JavaScript I am defaulting to image logging using Amazon S3. Most of my sites tend to have some sort of header image, which I store in a common public bucket on Amazon S3, all I have to do is turn on logging, and then get at logging details via the Amazon S3 API. Of course, images get cached within a user’s browser, but the GET for my images still gives me a pretty good set of numbers to work with. I’m not concerned with too much detail, I just generally want to understand the scope of traffic a project is getting, and whether it is 5, 50, 500, 5,000, or 50,000 visitors. My two primary CDNs are Amazon S3 and Github. I’m trying to pull together a base strategy for monitoring activity across my digital footprint. My business presence is very different than my personal presence, but with some of my personal writing, photography, and other creations I still like to keep a finger on the pulse of what is happening. I am just looking to minimize the data gathering and surveillance I am participating in these days. Keeping my personal and business websites static, and with a minimum footprint is increasingly important to me....[<a href="/2017/08/10/image-logging-with-amazon-s3-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/versioneye/version-eye-containers.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/09/open-sourcing-your-api-like-versioneye/">Open Sourcing Your API Like VersionEye</a></h3>
			<p><em>09 Aug 2017</em></p>
			<p>I’m always on the hunt for healthy patterns that I would like to see API providers, and API service providers consider when crafting their own strategies. It’s what I do as the API Evangelist. Find common patterns. Understand the good ones, and the bad ones. Tell stories about both, helping folks understand the possibilities, and what they should be thinking about as they plan their operations. One very useful API that notifies you about security vulnerabilities, license violations and out-dated dependencies in your Git repositories, has a nice approach to delivering their API, as well as the other components of their stack. You can either use VersionEye in the cloud, or you can deploy on-premise: versioneye-core - Models, Services &amp; Mails for VersionEye crawl_r - VersionEye crawlers implemented in Ruby. versioneye-security - Security Crawler for VersionEye versioneye-api - JSON REST API for VersionEye versioneye-tasks - Thin wrapper around the versioneye-core. versioneye - VersionEye.com VersionEye also has their entire stack available as Docker images, ready for deployment anywhere you need them. I wanted have a single post that I can reference when talking about possible open source, on-premise, continuous integration approaches to delivering API solutions, that actually have a sensible business model. VersionEye spans the areas that I think API providers should consider investing in, delivering SaaS or on-premise, while also delivering open source solutions, and generating sensible amounts of revenue. Many APIs I come across do not have an open source version of their API. They may have open source SDKs, and other tooling on Github, but rarely does an API provider offer up an open source copy of their API, as well as Docker images. VersionEye’s approach to operating in the cloud, and on-premise, while leveraging open source and APIs, as well as dovetailing with existing continuous integration flows is worth bookmarking. I am feeling like this is the future of API deployment and consumption, but don’t get nervous, there is still plenty of...[<a href="/2017/08/09/open-sourcing-your-api-like-versioneye/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/09/continuous-integration-and-deployment-for-government-procurement/">Continuous Integration And Deployment For Government Procurement</a></h3>
			<p><em>09 Aug 2017</em></p>
			<p>I was reading the Open by Default Portal Procurement Pilot for the Treasury Board of Canada, where section 6, Licensing states: “To support the objectives of the open government initiative, the Solution must be open source and licensed in accordance with the Massachusetts Institute of Technology License (“MIT License”). Under the resulting contract, the Contractor will be required to deposit the Solution’s source code on the GitHub platform (https://github.com) – under the MIT License.” This just seems like the way it should be for all government technology solutions. I’ve heard the naysayers in federal government say that proprietary software is the best route, but if it drives public infrastructure, in my opinion the code should be publicly available in this way. Honestly, code should be deployed at regular intervals throughout the development and deployment process, opening up the code to QA and security audits by the public, and 3rd parties. I hope this approach evolves into more of a continuous deployment and integration workflow when it comes to delivering software in government, where vendors have to plugin, open up their delivery cycles to more scrutiny, and leverage Github as the center of each procurement step from start to finish. Heck, let’s connect payments to each stop along the way. I’m a proponent of this not just to make the delivery of government software more observable and accountable. I want this process out in the open to help other agencies learn from the journey. Tune into the process, and maybe reuse, build upon, and evolve existing solutions as part of their operations. I will keep an eye on what is going on up in Canada, when it comes to requiring vendors publish code to Github. I also know there are similar efforts in the U.S. and other countries which I’ll also start scratching at and learning more about. It is definitely a healthy pattern I’d like to see more of, and I will continue to invest...[<a href="/2017/08/09/continuous-integration-and-deployment-for-government-procurement/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/GOSINT/gosint.gif" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/09/an-open-source-api-security-intelligence-gathering-processing-and/">An Open Source API Security Intelligence Gathering, Processing, And</a></h3>
			<p><em>09 Aug 2017</em></p>
			<p>I was reading about GOSINT, the open source intelligence gathering and processing framework over at Cisco. “GOSINT allows a security analyst to collect and standardize structured and unstructured threat intelligence. Applying threat intelligence to security operations enriches alert data with additional confidence, context, and co-occurrence. This means that you are applying research from third parties to your event data to identify similar, or identical, indicators of malicious behavior.” The framework is written in Go, with a front-end in JavaScript frontend, and usage of APIs as threat intelligence sources. When you look at configuration section on the README for GOSINT, you’ll see information for setting up threat intelligence feeds, including Twitter API, Alien Vault the Open Threat Community API, VirusTotal API, and the Collaborative Research Into Threats (CRITS). GOSINT acts as an API aggregator for a variety of threat information, which then allows you to scour the information for threat indicators, which you can evolve over time, providing a pretty interesting model for not just threat information sharing, but also API driven aggregation, curation and sharing. GOSINT also has the notion of behaving as a “transfer station”, where you can export refined data as CSV or CRITS format. Right here seems like an opportunity for some Github integration, adding continuous integration and deployment to open source intelligence and processing workflows. Making sure refined, relevant threat information is available where it is needed, via existing API deployment and integration workflows. Wouldn’t take much to publish CSV, YAML, and JSON files to Github which can then be used to drive distributed dashboards, visualizations, and other awareness building tools. Plus, the refined threat information is now published as CSV/JSON/YAML on Github where it can be ingested by any system of application with access to the Github repository. GOSINT is just one of the interesting tooling I’m coming across as I turn up the volume on my API security research, thanks to the investment of ElasticBeam my API security partner....[<a href="/2017/08/09/an-open-source-api-security-intelligence-gathering-processing-and/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/twitter/twitter-websites-embeddable.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/09/a-fresh-look-at-the-embeddable-tools-built-on-the-twitter-api/">A Fresh Look At The Embeddable Tools Built On The Twitter API</a></h3>
			<p><em>09 Aug 2017</em></p>
			<p>
Over the years I have regularly showcased Twitter as an example API driven embeddable tools like buttons, badges, and widgets. In 2017, after spending some time in the Twitter developer portal, it is good to see Twitter still investing in their embeddable tools. The landing page for the Twitter embeddables still provides the best example out there of the value of using APIs to drive data and content across a large number of remote web sites.

Twitter has distinct elements of their web embeddables:


  Tweet Button - That classic tweet button, allowing users to quickly Tweet from any website.
  Embedded Tweets - Taking any Tweet and embedding on a web page showing its full content.
  Embedded Timeline - Showing curated timelines on any website using a Twitter embeddable widget.
  Follow Button - Helping users quickly follow your Twitter account, or your companies Twitter account.
  Twitter Cards - Present link summaries, engaging images, product information, or inline video as embeddable cards in timeline.


Account interactions, messaging, posting, and other API enabled function made portable using JavaScript allowing it to be embedded and executed on any website. JavaScript widgets, buttons, and other embeddables are still a very tangible, useful example of APIs in action. Something I can talk about to anyone about, helping them understand why you might want to do APIs, or at least know about APIs.

We bash on Twitter a lot in the API community. However, after a decade of operation, you have to give it to them. They are still doing it. They are still keeping it simple with embeddable tools like this. I can confidently say that APIs are automating some serious illness on the Twitter API platform at the moment, and there are many things I’d like to be different with the Twitter API, but I am still pleased that I can keep finding examples from the Twitter platform to showcase on API Evangelist seven years of writing about them.

[<a href="/2017/08/09/a-fresh-look-at-the-embeddable-tools-built-on-the-twitter-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_dark_dali.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/08/reducing-developers-to-a-transaction-with-apis-microservices-serverless/">Reducing Developers To A Transaction With APIs, Microservices, Serverless,</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>A topic that keeps coming up in discussions with my partner in crime Audrey Watters (@audreywatters) about our podcast is around the future of labor in an API world. I have not written anything about this, which means I’m still in early stages of any research into this area, but it has come up in conversation, and reflected regularly in my monitoring of the API space, I need to begin working through my ideas in this area. A process that helps me better see what is coming down the API pipes, and fill the gaps in what I do not know. Audrey has long joked about my API world using a simple phrase: “reducing everything to a transaction”. She says it mostly in jest, but other times I feel like she wields it as the Cassandra she channels. I actually bring up the phrase more than she does, because it is something I regularly find myself working in the service of as the API Evangelist. By taking a pro API stance I am actively working to reduce legacy business, institutional, and government processes down and breaking them down into a variety of individual tasks, or if you see things through a commercial lens, transactions. Microservices A microservices philosophy is all about breaking down monoliths into small bite size chunks, so they can be transacted independently, scaled, evolved, and deprecated in isolation. Microservices should do one thing, and do it well (no backtalk). Microservices should do what it does as efficiently as possible, with as few dependencies as possible. Microservices are self-contained, self-sufficient, and have everything they need to get the job done under a single definition of a service (a real John Wayne of compute). And of course, everything has an API. Microservices aren’t just about decoupling the technology, they are are about decoupling the business, and the politics of doing business within SMB, SME, enterprises, institutions, and government agencies–the philosophy for reducing everything to...[<a href="/2017/08/08/reducing-developers-to-a-transaction-with-apis-microservices-serverless/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/gypsy-eyes_blue_circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/08/patent-9397835-web-of-trust-management-in-a-distributed-system/">Patent #9397835, Web of trust management in a distributed system</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>I found a couple more API patents in my notebook that I wanted to get published. I try to take time regularly to publish the strangest API related patents I can find. Today’s patent is out of Amazon, which I find to be a fascinating outlet for patent storytelling. It isn’t squarely in the realm of APIs like some of my others, but I think tells a fascinating story by itself, showing how the web and the concept of a patent are colliding. Title - Web of trust management in a distributed system Number - 9397835 Owner - Amazon Technologies, Inc. Publication Date - 2016-07-19 Application Date - 2014-05-21 Abstract - A web of trust is used to validate states of a distributed system. The distributed system operates based at least in part on a domain trust. A root of trust issues the domain trust issues a domain trust. Domain trusts are updatable in accordance with rules of previous domain trusts so that a version of a domain trust is verifiable by verifying a chain of previous domain trust versions._ I like that trust is being patented. Digital trust as a patentable concept that Amazon can now delegate if they choose. I’m just fascinated by what concepts are now fair game for patenting, as they enter into the digital realm. Now I’m curious how many physical trust patents might exist. Is the management of trust patented in the physical world in any way? I guess I could see some of the components for determining trust could be patented, but I find the fact that trust, or more specifically trust management is patentable, as a troubling thing. It’s no secret that I’m anti API patents. I’m rarely convinced of the uniqueness of anything digital, warranting the issuing of a patent by the USPTO. I have to say that in a world where trust is patentable, the environment for suspect behavior will flourish. Pretty much what we...[<a href="/2017/08/08/patent-9397835-web-of-trust-management-in-a-distributed-system/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-url-square.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/08/my-url-shortener-is-just-an-api-with-postman-as-my-client/">My URL Shortener Is Just An API With Postman As My Client</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>
I have my own URL shortener for API Evangelist called apis.how. I use it to track the click through rates for some of my research projects, and partner sponsorships. I’ve had the URL shortener in operation for about two years now, and I still do not have any type of UI for it, relying 100% on Postman for adding, searching, and managing the URLs I am shortening, and tracking on.

My URL shortener just hasn’t raised to a level of priority where I’ll invest any time into an administrative interface, or dashboard for my URL shortener. I used Bitly and Google for a while, but I really just needed a simple shortening with basic counts, nothing more. When I bought the domain I launched a handful of API endpoints to support, allowing me to add, update, search, and remove URLs, as well as track the click throughs, and query how many clicks a link received for each mont. I can easily accomplish all of this through the Postman interface, making basic calls to my simple API–no over-engineering necessary.

I have been considering running a daily job that pulls view counts for URLs and publishing to Github as YAML, where I can drive a simple visualization, but honestly I’m not that numbers oriented. I like what API clients like Postman and Restlet provide. Even though I’m equipped to make calls using JavaScript, PHP, or CURL, I prefer just accessing via my web client–no coding necessary. I wouldn’t manage all my systems in this way, but for really basic ones like my URL shortener, I’m not sure I will ever actually evolve it beyond just being a simple API.

[<a href="/2017/08/08/my-url-shortener-is-just-an-api-with-postman-as-my-client/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/drone-recovery/babyfoot04.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/08/http-as-a-substrate/">HTTP as a Substrate</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>I am spending a significant amount of time reading RFCs lately. I find the documents to be very cumbersome to read, but the more you read, the more tolerant you become. When I browse through RFCs I’m always reminded of how little I actually know about the web. In an effort to push forward my education, and maybe yours along the way, I’m going to be cherry picking specific sections of the interesting RFCs I’m digesting here on the blog. Today’s RFC is 3205, filed under Best Current Practice”, and is on the use of HTTP as a Substrate. _Recently there has been widespread interest in using Hypertext Transfer Protocol (HTTP) [1] as a substrate for other applications- level protocols. Various reasons cited for this interest have included: familiarity and mindshare, compatibility with widely deployed browsers, ability to reuse existing servers and client libraries, ease of prototyping servers using CGI scripts and similar extension mechanisms, authentication and SSL or TLS, the ability of HTTP to traverse firewalls, and cases where a server often needs to support HTTP anyway. The Internet community has a long tradition of protocol reuse, dating back to the use of Telnet as a substrate for FTP and SMTP. However, the recent interest in layering new protocols over HTTP has raised a number of questions when such use is appropriate, and the proper way to use HTTP in contexts where it is appropriate. Specifically, for a given application that is layered on top of HTTP: Should the application use a different port than the HTTP default of 80? Should the application use traditional HTTP methods (GET, POST, etc.) or should it define new methods? Should the application use http: URLs or define its own prefix? Should the application define its own MIME-types, or use something that already exists (like registering a new type of MIME-directory structure)? This memo recommends certain design decisions in answer to these questions. This memo is intended as...[<a href="/2017/08/08/http-as-a-substrate/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/json-web-token/json-web-token.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/08/api-message-integrity-with-json-web-token-jwt/">API Message Integrity with JSON Web Token (JWT)</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>
I don’t have any production experience deploying JSON Web Tokens (JWT), but it has been something I’ve been reading up on, and staying in tune with for some time. I often reference JWT as the leading edge for API authentication, but there is one aspect of JWT I think is worth me referencing more often–message integrity. JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.

JWT can not only be used for authentication of both message sender/receiver, it can ensure the message integrity as well, leveraging a digital signature hash value of the message body to ensure the message integrity during transmission. It adds another interesting dimension to the API security conversation, and while not be applicable in all APIs, I know many that it would make a lot of sense. Many of the networks we use today and applications we depend on today are proxied, creating an environment where message integrity should always come into question, and JWT gives us another tool in our toolbox to help us keep things secure.

I’m working my way through each layer of API operations, looking for aspects of API security that are often obscured, hidden, or just not discussed as they should be. I feel like JWT is definitely one track of API security that has evolved the conversation significantly over the last couple years, and is something that can make a significant impact on the space with just a little more storytelling and education. I’m going to make sure API request and response message integrity is a regular part of my API security storytelling, curriculum, and live talks that I develop.

[<a href="/2017/08/08/api-message-integrity-with-json-web-token-jwt/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/power-lines-empty-space_copper_circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/07/the-subtle-ways-in-which-power-asserts-itself-in-face-of-api-engagements/">The Subtle Ways In Which Power Asserts Itself In Face Of API Engagements</a></h3>
			<p><em>07 Aug 2017</em></p>
			<p>I’m rarely surprised by, but still often caught off guard by the subtle ways in which power asserts itself when faced with change the introduced by API projects. In my 30 years as a database professional I’ve seen numerous overt, covert, and subversive ways in which existing holders of power (data), but I often still get blindsided by the creative, and subtle ways in which folks defend what they already have, and work to keep things from changing. While doing unfunded work to define industry level API specifications, and help move forward the API conversation in multiple industries, I’ve been encountering two pockets of friction I want to understand better, so I can develop some sort of grease, that might make things smoother. There are two main pockets of practitioners in this setting, implementors (those you publish an API), and vendors (those who currently sell solutions to implementors). My role in any industry as the API Evangelist is to help ultimately define and showcase healthy, common API definitions, that can be reused across the API lifecycle–from design to deprecation. The Vendor Question Trying to understand the culture, and motivations of any SMB, SME, or enterprise is often a full time job I do not have the time or resources for. I work pretty hard to understand any of the key players in any industry being touched by APIs, but will still have much I need to learn. One thing I do know, is that I should never take a company for face value, because behind the mask there is always much more going on. Honestly, APIs are actually a pretty good barometer of what is going on behind the scenes, something I don’t think some API providers fully grasp before they decide to jump into the API game. The most obvious thing a vendor in an industry will do is ignore you. When approached about participating in discussions around a common API definition, they’ll just...[<a href="/2017/08/07/the-subtle-ways-in-which-power-asserts-itself-in-face-of-api-engagements/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://bioscopic.files.wordpress.com/2010/11/barneyoldfield.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/07/slow-moving-ransomware-as-the-new-business-model/">Slow Moving Ransomware As The New Business Model</a></h3>
			<p><em>07 Aug 2017</em></p>
			<p>I was reading about the difficulties the City of New York was having when it comes to migrating off of the Palantir platform, while also reading about the latest cybersecurity drama involving ransomware. I’m spending a lot of time studying cybersecurity lately, partly because they involve APIs, but mostly because it is something that is impacting every aspect of our lives, including our democracy, education, and healthcare. One thing I notice on the cybersecurity stage, is that everything is a much more extreme, intense, representation of what is going on in the mainstream tech industry. Ransomware is software that gets installed on your desktop or servers and locks up all your data until you pay the software developer (implementor) a ransom. Ransomware is just a much faster moving version of what many of us in the software industry call vendor lock-in. This is what you are seeing with Palantir, and the City of New York. What tech companies do is get you to install their software on your desktop or servers, or convince you to upload all your data into the cloud, and use their software. This is business 101 in the tech industry. You either develop cloud-based software, something that runs on-premise, or you are a mix of both. Ideally, your customers become dependent on you, and they keep paying your monthly, quarterly, or annual subscriptions (cough cough ransom). Here is where the crafty API part of the scheme comes in. Software providers can also make APIs that allow your desktop and server to integrate with their cloud solutions, allowing for much deeper integration of data, content, and algorithms. The presence of APIs SHOULD also mean that you can more easily get your data, content, and algorithms back, or have kept in sync the whole time, so that when you are ready to move on, you don’t have a problem getting your data and content back. The problem is, that APIs “CAN” enable this,...[<a href="/2017/08/07/slow-moving-ransomware-as-the-new-business-model/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorithmia/machine-learning-citation.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/07/providing-code-citations-in-machine-learning-apis/">Providing Code Citations In Machine Learning APIs</a></h3>
			<p><em>07 Aug 2017</em></p>
			<p>
I was playing around with the Style Thief, an image transfer API from Algorithmia, and I noticed the citation for the algorithm behind. The API is an adaptation of Anish Athalye’s Neural Style Transfer, and I thought the algorithmic citation of where the work was derived from was an interesting thing to take note of for my machine learning API research.

I noticed on Algorithmia’s page there was a Bibtex citation, which referenced the author, and project Github repository:

@misc{athalye2015neuralstyle,
   author = {Anish Athalye},
   title = {Neural Style},
   year = {2015},
   howpublished = {\url{https://github.com/anishathalye/neural-style}},
   note = {commit xxxxxxx}
}

This provides an interesting way to address citation in not just machine learning, but with open source driving algorithmic APIs in general. It gives me food for thought when it comes to what licensing I should be considering when wrapping open source software with an API. I’ve been thinking about dependencies a lot lately when it comes to APIs and their definitions, and I’d consider citation or attribution to be in a similar category. I guess rather then technical dependency, it is more in the business and legal dependency category.

Similar to how Pivio allows you to reference dependencies for your microservices, I’m thinking that API Commons, or some other format like Bibtext could provide a machine readable citation, that could be indexed as part of an APIs.json index. Allowing us API designers, architect, and providers to provide proper citation for where our work is derived. These aren’t just technical dependencies, but also business and political dependencies, that we should ensuring are present with each API we deploy, providing an observable provenance of where ideas come from, and a honest look at how we build on the work of each other.

[<a href="/2017/08/07/providing-code-citations-in-machine-learning-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-services.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/07/api-industry-standards-negotiation-by-media-type/">API Industry Standards Negotiation By Media Type</a></h3>
			<p><em>07 Aug 2017</em></p>
			<p>I am trying to help push forward the conversation around the API definition for the Human Services Data Specification (HSDS) in a constructive way amidst a number of competing interests. I was handed a schema for sharing data about about organizations, locations, and services in a CSV format. I took this schema and exposed it with a set of API paths, keeping the flat file structure in tact, making no assumptions around how someone would need to access the data. I simply added the ability to get HSDS over the web as JSON–I would like to extend to be HTML, CSV, JSON, and XML, reaching as wide as possible audience with the basic implementation. As we move forward discussions around HSDS and HSDA I’m looking to use media types to help separate the different types of access people are looking for using media types. I don’t want to leave folks who only have basic CSV export or import capabilities behind, but still wanted to provide guidance for exchanging HSDA over the web. To help organize higher levels of demand on the HSDS schema I’m going to break out into some specialized media types as well as the default set: Human Services Data Specification (HSDS) - text/csv - Keeping data package basic, spreadsheet friendly, yet portable and exchangeable. Human Services Data API (HSDA) - application/json and text/xml, text/csv, and text/html - Governing access at the most basic level, keeping true to schema, but allowing for content negotiation over the web. Human Services Data API (HSDA) Hypermedia - (application/hal+json and application/hal+xml) - Allowing for more comprehensive responses to HSDA requests, addressing searching, filtering, pagination, and relationship linking between any HSDS returend. Human Services Data API (HSDA) Bulk - (application/vnd.hsda.bulk) - Focusing on heavy system to system bulk transfers, and eventually syncing, backups, archives, and migrations. Dealing with the industrial levels of HSDA operations. Human Services Data API (HSDA) Federated - (application/vnd.hsda.federated) - Allowing for a federated HSDA...[<a href="/2017/08/07/api-industry-standards-negotiation-by-media-type/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_copper_circuit_2.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/04/when-you-see-api-rate-limiting-as-security/">When You See API Rate Limiting As Security</a></h3>
			<p><em>04 Aug 2017</em></p>
			<p>I’m neck deep into my assessment of the world of API security this week, a process which always yields plenty of random thoughts, which end up becoming stories here on the blog. One aspect of API security I keep coming across in this research is the concept of API rate limiting as being security. This is something I’ve long attributed with API management service providers making their mark on the API landscape, but as I dig deeper I think there is more to this notion of what API security is (or isn’t). I think it has more to do with API providers, than companies selling their warez to these API providers. The API management service providers have definitely set the tone for API security conversation(good), by standing up a gateway, and providing tools for limiting what access is available–I think many data, content, and algorithmic stewards are very narrowly focus on security being ONLY about limiting access to their valuable resources. Many folks I come across see their resources as valuable, when they begin doing APIs they have a significant amount of concern around putting their resources on the Internet, and once you secure and begin rate limiting things, all security concerns appear to have been dealt with. Competitors, and others just can’t get at your valuable resources, they have to come through the gate–API security done. Many API providers I encounter have unrealistic views of the value of their data, content, and algorithms, and when you match this with their unrealistic views about how much others want access to this valuable content you end up with a vacuum which allows for some very narrow views of what API security is. To help support this type of thinking, I feel like the awareness generated from API management is often focused on generating revenue, and not always about understanding API abuse, and is also something can create blindspots when it comes to database, server, and DNS...[<a href="/2017/08/04/when-you-see-api-rate-limiting-as-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/mining-machine-learning.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/04/understanding-the-words-we-use-to-describe-machine-learning-apis/">Understanding The Words We Use To Describe Machine Learning APIs</a></h3>
			<p><em>04 Aug 2017</em></p>
			<p>I spend a lot of time trying out new APIs, working to understand what it is they do, or do not do. I have a pretty robust way of looking at APIs, profiling the company, and the APIs they offer, but when I’m wading through the marketing content, API documentation, and other resources, I am regularly stumped by the language that is used to describe what an API does. Honestly, this problem isn’t exclusive to machine learning APIs, but with the recent explosion in artificial intelligence, machine learning, deep learning, cognitive and other types of algorithmic voodoo, the words being used seem to have gone to entirely new levels. I am interested in understanding what it is an API does. I want to go from zero to understanding in 2.34 seconds. I don’t want to wade through marketing, and documentation to understand what an API does. I want to find simple, concise language that properly describes an API. In the world of artificial intelligence, this can be difficult to do, and is something that varies from provider to provider. Some machine learning API providers are better at describing what they do, while others seem to prefer hype, and fluff when it comes to explaining what is actually possible. As I continue my work profiling Amazon, Microsoft, and Google APIs I want to develop an approach to helping me separate what an API does, and what a provider says it does. I am going to continue profiling each API using OpenAPI, and labeling them with a common set of tags I’m using to quantify what machine learning APIs actually do. As I’m doing this I also decided to add an extra tag field called x-hype-tags, which gives me a way to track each of the additional words I found in the marketing and documentation, that I may not be actually using to describe what the API does–maintaining much of the API providers intent. One thing that...[<a href="/2017/08/04/understanding-the-words-we-use-to-describe-machine-learning-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/04/including-api-dependencies-within-your-api-definition/">Including API Dependencies Within Your API Definition</a></h3>
			<p><em>04 Aug 2017</em></p>
			<p>I was learning about Pivio, a discovery specification for microservices the other day, and found their focus on microservice dependency to be pretty interesting. API dependencies has been an area I have found myself increasingly thinking about, as well as tracking on in my API research. I’m pretty close to publishing a project dedicated to understanding API, and microservices dependencies, which would overlap with containers, serverless, and other aspects of the API lifecycle that are all about breaking down the monolith. Each service definition using Pivio has a depends_on object, which allows for defining both internal and external service dependencies. Here is a snippet from a sample Pivio document to help articulate this interesting feature: This is where you can start connecting the cords between all of your services, something that is applicable to any APIs, whether you’ve drank the microservices kool-aid or not. I find it interesting that Pivio has internal, and external. I’d love to see an OpenAPI linked off each of the services it depends on. I also am fascinated with the first question for external, why? What a great first question for any dependency–it should also be available for internal services as well. Every API provider should be inquiring why a dependency exist whenever possible, and having it quantified in this way just opens up more chances for this question to get asked. Seeing dependencies quantified in Pivio makes me happy. It has been something I’ve wanted to reflect in APIs.json for some time now. Currently, I do not know of any way to quantify the relationship between APIs, and Pivio provides a glimpse at one possible way we might be able to map this world out. I have been learning more about Cytoscape, a network data integration, analysis, and visualization framework. Having a machine readable API dependencies definition would allow me to create a network visualization of any API or microservices discovery catalog. It wouldn’t take much work at all...[<a href="/2017/08/04/including-api-dependencies-within-your-api-definition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/contrafabulists/machine+learning.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/03/when-describing-your-machine-learning-apis-work-extra-hard-to-keep-things/">When Describing Your Machine Learning APIs Work Extra Hard To Keep Things</a></h3>
			<p><em>03 Aug 2017</em></p>
			<p>I’m spending a significant amount of time learning about machine learning APIs lately. Some of what I’m reading is easy to follow, while most of it is not. A good deal of what I’m reading is technically complex, and more on the documentation side of the conversation. Other stuff I come across is difficult to read, not because it is technical, but because it is more algorithmic marketing magic, and doesn’t really get at what is really going on (or not) under the hood. If you are in the business of writing marketing copy, documentation, or even the API design itself, please work extra hard to keep things simple and in plain language. I read so much hype, jargon, fluff, and meaningless content about artificial intelligence and machine learning each day, I take pleasure anytime I find simple, concise, and information descriptions of what ML APIs do. In an exploding world of machine learning hype your products will stand out if they are straight up, and avoid the BS, which will pretty quickly turn off the savvy folks to whatever you are peddling. Really, this advice applies to any API, not just machine learning. It’s just the quantity of hype we are seeing around AI and ML in 2017 is reaching some pretty extreme levels. Following the hype is easy. Writing fluffy content doesn’t take any skills. Writing simple, concise, plain language names, descriptions, and other meta data for artificial intelligence and machine learning APIs takes time, and a significant amount of contemplation regarding the message you want to be sending. The ML APIs I come across that get right to the point, are always the ones that stick around in my mind, and find a place within my research and storytelling. We are going to continue to see an explosion in the number of algorithmic APIs, delivering across the artificial intelligence, machine learning, deep learning, cognitive, and other magical realms. The APIs that deliver...[<a href="/2017/08/03/when-describing-your-machine-learning-apis-work-extra-hard-to-keep-things/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/services/api-discovery.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/03/different-search-engines-for-api-discovery/">Different Search Engines For API Discovery</a></h3>
			<p><em>03 Aug 2017</em></p>
			<p>
I was learning about the microservices discovery specification Pivio, which is a schema for framing the conversation, but also an uploader, search, and web interface for managing a collection of microservices. I found their use of ElasticSearch as the search engine for their tooling worth thinking about more. When we first launched APIs.json, we created APIs.io as the search engine–providing a custom developed public API search engine. I hadn’t thought of using ElasticSearch as an engine for searching APIs.json treated as a JSON document.

Honestly, I have been relying on the Github API as the search engine for my API discovery. Using it to uncover not just APIs.json, but OpenAPI, API Blueprint, and other API specification formats. This works well for public discovery, but I could see ElasticSearch being a quick and dirty way to launch a private or public engine for an API discovery, catalog, directory, or type of collection. I will add ElasticSearch, and other platforms I track on as part of my API deployment research as a API discovery building block, evolving the approaches I’m tracking on.

It is easy to think of API discovery as directories like ProgrammableWeb, or marketplaces like Mashape, and public API search engines like APIs.io–someone else’s discovery vehicle, which you are allowed to drive when you need. However, when you begin to consider other types of API discovery search engines, you realize that a collection of API discovery documents like JSON Home, Pivio, and APIs.json can quickly become your own personal API discovery vehicle. I’m going to write a separate piece on how I use Github as my API discovery engine, then I think I’ll step back and look at other approaches to searching JSON or YAML documents to see if I can find any search engines that might be able to be fine tuned specifically for API discovery.

[<a href="/2017/08/03/different-search-engines-for-api-discovery/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/json-home/json-home-widget-example.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/03/api-discovery-using-json-home/">API Discovery Using JSON Home</a></h3>
			<p><em>03 Aug 2017</em></p>
			<p>I’m have finally dedicated some time to learning more about Home Documents for HTTP APIs, or simply JSON Home. I see JSON Home as a nice way to bring together the technical components for an API, very similar to what I’ve been trying to accomplish with APIs.json. One of the biggest differences I see is that I’d say APIs.json was born out of the world of open data and APIs, where JSON Home is born of the web (which actually makes better sense). I think the JSON Home description captures the specifications origins very well: The Web itself offers one way to address these issues, using links [RFC3986] to navigate between states. A link-driven application discovers relevant resources at run time, using a shared vocabulary of link relations [RFC5988] and internet media types [RFC6838] to support a “follow your nose” style of interaction - just as a Web browser does to navigate the Web. JSON Home provides any potential client with a machine readable set of instructions it can follow, involving one, or many APIs–providing a starting page for APIs which also enables: Extensibility - Because new server capabilities can be expressed as link relations, new features can be layered in without introducing a new API version; clients will discover them in the home document. Evolvability - Likewise, interfaces can change gradually by introducing a new link relation and/or format while still supporting the old ones. Customisation - Home documents can be tailored for the client, allowing different classes of service or different client permissions to be exposed naturally. Flexible deployment - Since URLs aren’t baked into documentation, the server can choose what URLs to use for a given service. JSON Home, is a home page specification which uses JSON to provide APIs with a a launching point for the interactions they offer, by providing a coherent set links, all wrapped in a single machine readable index. Each JSON begins with a handful of values:...[<a href="/2017/08/03/api-discovery-using-json-home/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/open311/open311-api-list.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/02/when-cities-use-a-common-api-definition-to-report-nonemergency-issues/">When Cities Use A Common API Definition To Report Non-Emergency Issues</a></h3>
			<p><em>02 Aug 2017</em></p>
			<p>I am taking a deeper look at Open311, as part of some wider municipal level API research and development I am doing. I am going to be helping evolve an OpenAPI for the project, as well as JSON schema for the API and underlying data model. As I’m working my way through the Open311 portal reacquainting myself with the open format for reporting of non-emergency issues within cities, I came across the list of cities who have implemented Open311, and get a glimpse at what the future of APIs at the city level can be. When you land on the Open311 GeoReport v2 Servers listing page you get a table of the twenty-one cities who have published an Open311 API, with the name, country, API discovery document, API key request location, documentation, production and sandbox environment URLs. Twenty-one separate cities, twenty-one separate APIs for reporting non-emergency issues, all using the same API definition. This is the way that all APIs at the city, county, state, and federal levels should work. They should leverage common schema and API definition, providing a federated list of resources that can be integrated into any application. Imagine when all cities have a common 311 API for reporting of non-emergency issues, as well as a 211 API for finding human services within a city. Imagine when the entire stack of city services all use common API definition(s), and schema across all API requests and responses. In this environment any city will be able take existing SDKs, or open source application, tools, or plugin, and put to work for a new city, with little or no changes. This is the way city, county, state, and federal agency software and application development should work. There should be a buffet of open source solutions for each layer of government operations. When it comes to this level of API operations within cities, Open311 is the furthest along down the road in the journey. This is...[<a href="/2017/08/02/when-cities-use-a-common-api-definition-to-report-nonemergency-issues/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-metrics/apimetrics-cloud-location-performance-map.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/02/understanding-global-api-performance-at-the-multicloud-level/">Understanding Global API Performance At The Multi-Cloud Level</a></h3>
			<p><em>02 Aug 2017</em></p>
			<p>APIMetrics has a pretty addictive map showing the performance of API calls between multiple cloud providers, spanning many global regions. The cloud location latency map “shows relative performance of a standard, reference GET request made to servers running on all the Google locations and via the Google global load balancer. Calls are made from AWS, Azure, IBM and Google clouds and data is stored for all steps of the API call process and the key percentiles under consideration.” It is interesting to play with the destination of the API calls, changing the region, and visualizing how API calls begin to degrade to different regions. It really sets the stage for how we should start thinking about the deployment, monitoring, and testing of our APIs. Region, by region, getting to know where our consumers are, and making sure APIs are deployed within the cloud infrastructure that delivers the best possible performance. It’s not just testing your APIs in a single location from many locations, it is also rethinking where your APIs are deployed, leveraging a multi-cloud reality and using all the top cloud provider, while also making API deployment by region a priority. I’m a big fan of what APIMetrics is doing with the API performance visualizations and mapping. However, I think their approach to using HTTPbin is a significant part of this approach to monitoring and visualizing API performance at the multi-cloud level, while also making much of the process and data behind it all public. I want to put some more thought into how they are using HTTPbin behind this approach to multi-cloud API performance monitoring. I feel like there is potential her for applying this beyond just API performance, and think about other testing, security, and critical aspects of reliability and doing business online with APIs today. After thinking where else this HTTPbin approach to data gathering could be applied, I want to think more about how the data behind APIMetrics cloud location...[<a href="/2017/08/02/understanding-global-api-performance-at-the-multicloud-level/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/runscope/runscope-connected-services.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/02/making-sure-your-api-service-connects-to-other-stops-along-the-api-lifecycle/">Making Sure Your API Service Connects To Other Stops Along The API Lifecycle</a></h3>
			<p><em>02 Aug 2017</em></p>
			<p>I am continuing my integration platform as a service research, and spending a little bit of time trying to understand how API providers are offering up integrations with other APIs. Along the way, I also wanted to look at how API service providers are doing it as well, opening themselves up to other stops along n API lifecycle. To understand how API service providers are allowing their users to easily connect to other services I’m taking a look at how my partners are handling this, starting with connected services at Runscope. Runscope provides ready to go integration of their API monitoring and testing services with twenty other platforms, delivering a pretty interesting Venn diagram of services along the API lifecycle: Slack - Slack to receive notifications from Runscope API test results and Traffic Alerts. Datadog - Datadog to create events and metrics from Runscope API test results. Splunk Cloud - Splunk Cloud to create events for API test results. PagerDuty - A PagerDuty service to trigger and resolve incidents based on Runscope API test results or Traffic Alerts. Amazon Web Services - Amazon Web Services to import tests from API Gateway definitions. Ghost Inspector - Ghost Inspector to run UI tests from within your Runscope API tests. New Relic Insights - New Relic Insights to create events from Runscope API test results. Microsoft Teams - Microsoft Teams to receive notifications from Runscope API test results. HipChat - HipChat to receive notifications from Runscope API test results and Traffic Alerts. StatusPage.io - StatusPage.io to create metrics from Runscope API test results. Big Panda - Big Panda to create alerts from Runscope API test results. Keen IO - Keen IO to create events from Runscope API test results. VictorOps - A VictorOps service to trigger and resolve incidents based on Runscope API test results or Traffic Alerts. Flowdock - Flowdock to receive notifications from Runscope API test results and Traffic Alerts. AWS CodePipeline - Integrate your Runscope...[<a href="/2017/08/02/making-sure-your-api-service-connects-to-other-stops-along-the-api-lifecycle/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/ads-txt/ads.txt-about.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/02/learning-about-realtime-advertising-bidding-transparency-using-ads-txt/">Learning About Real-Time Advertising Bidding Transparency Using Ads.txt</a></h3>
			<p><em>02 Aug 2017</em></p>
			<p>I was learning about real-time bidding transparency using Ads.txt from Lukasz Olejnik. The mission of the ads.txt project is to “increase transparency in the programmatic advertising ecosystem. Ads.txt stands for Authorized Digital Sellers and is a simple, flexible and secure method that publishers and distributors can use to publicly declare the companies they authorize to sell their digital inventory.” While Ads.txt isn’t an API, it is an open, machine readable definition that is working to make advertising more transparent and observable to everyone, not just people in the ad-tech space. Ads.txt works similar to robots.txt, and is a simple text file that lives in the root of a domain, listing the companies that have permission to sell advertising. The format is new, so there isn’t a lot of adoption yet, but you can see one in action over at the Washington Post. Helping make platforms observable is why I perform as the API Evangelist. I see them as one of the important tools we have for making systems, algorithms, and platforms more observable, and less of a black box. I see ads.txt having similar potential for the world of advertising, and something that eventually could have an API, for helping make sense of the very distributed, brokered, and often dark world of online advertising. Honestly, I know almost nothing about online advertising. I have a basic level of understanding of Google Analytics, Adwords, and Adsense, as well as reading the blogs, and documentation for many advertising APIs I come across in my regular monitoring of the API space. I am just interested in ads.txt as an agent of observability, and pulling back the current on who is buying and selling our digital bits online. I am adding ads.txt to my API definitions research. This will allow me to keep an eye on the project, see where it goes, and think about the API level for aggregation of the ad network data, on maybe Github or...[<a href="/2017/08/02/learning-about-realtime-advertising-bidding-transparency-using-ads-txt/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/eia/eia-spreadsheet-add-ons.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/01/u-s-energy-information-administration-excel-addin-and-google-addon/">U.S. Energy Information Administration Excel Add-In and Google Add-On</a></h3>
			<p><em>01 Aug 2017</em></p>
			<p>I was looking through a number of federal government API implementations last week in preparation of a talk I did in Washington DC. The result of research like this is always a notebook full of interesting stories to tell about what federal agencies are up to with APIs. Today’s story is out of the U.S. Energy Information Administration (EIA), with their Excel Data Add-In and Google Add-On tooling which allows you to download energy data from EIA’s data API and economic data from the St. Louis Federal Reserve’s Economic Data (FRED) API directly into your spreadsheet(s). I’m regularly looking out for innovative uses of spreadsheets when it comes to deploying, as well as consuming APIs, because I believe it is the best way we have to turn average business users into API consumers, by piping in data into the environment they are already using each day. EIA’s data API contains 1.6 million energy series, and the St. Louis Federal Reserve’s API contains 240,000 economic series. Making valuable federal agency maintained data available within spreadsheets like this using APIs is something ALL other agencies should be emulating. First, agencies need to be doing public APIs, then they need to make sure they are also investing in spreadsheet tooling like the EIA is. I’m adding this example of using Microsoft Excel and Google Sheets as an API client for not just federal government, but also for such valuable commerce and energy data, to my APIs and spreadsheets research. I’m also going to be on the hunt for open source solutions for delivering spreadsheet API connectivity like this. There should be a wealth of open source tooling that federal agencies can put to work when it comes to delivering data to spreadsheets, both internally, and externally with private sector partners. In a time where it is easy to get pretty depressed on a daily basis about APIs in the federal government, it makes me happy to find shining...[<a href="/2017/08/01/u-s-energy-information-administration-excel-addin-and-google-addon/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/taxii/taxii-logo.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/01/the-trusted-automated-exchange-of-intelligence-information-taxii/">The Trusted Automated Exchange of Intelligence Information (TAXII)</a></h3>
			<p><em>01 Aug 2017</em></p>
			<p>I recently wrote about the opportunity around developing an aggregate threat information API, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of the Trusted Automated Exchange of Intelligence Information (TAXII), as one possible approach to defining a common set of API definitions and tooling for the exchange of threat intelligence. The description of TAXII from the project website describes it well: Trusted Automated Exchange of Intelligence Information (TAXII) is an application layer protocol for the communication of cyber threat information in a simple and scalable manner. TAXII is a protocol used to exchange cyber threat intelligence (CTI) over HTTPS. TAXII enables organizations to share CTI by defining an API that aligns with common sharing models. TAXII is specifically designed to support the exchange of CTI represented in STIX. I breezed through the documentation for TAXII version 2.0, and it looks pretty robust, and a project that has made some significant inroads towards accomplishing what I’d like to see out there for sharing threat intelligence. I’m still understanding the overlap of TAXII, the transport mechanism for sharing cyber threat intelligence, and STIX, the structured language for cyber threat intelligence, but it looks like a robust, existing approach defining the schema and an API for sharing threat intelligence. Next, I am going to gather my thoughts around both of these existing definitions, and look at establishing an OpenAPI that represents STIX and TAXII, providing a machine readable definition for sharing threat intelligence. I think having an OpenAPI will provide a blueprint that can be used to define a handful of server side implementations in a variety of programming languages. I was happy to be directed to this existing work, saving me significant time and energy when it comes to this conversation....[<a href="/2017/08/01/the-trusted-automated-exchange-of-intelligence-information-taxii/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-threat-info-sharing-api.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/01/craft-an-openapi-for-an-existing-threat-intelligence-sharing-api-specification/">Craft An OpenAPI For An Existing Threat Intelligence Sharing API Specification</a></h3>
			<p><em>01 Aug 2017</em></p>
			<p>I wrote about the opportunity around developing an aggregate threat information API, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of the Structured Threat Information Expression (STIX), a structured language for cyber threat intelligence, and Trusted Automated Exchange of Intelligence Information (TAXII), and transport mechanism for sharing cyber threat intelligence. This is why I write about my projects openly like this, so that my readers can help me identify existing approaches for tackling whatever I am focusing on. I prefer to never reinvent the wheel, and build on top of any existing work that is already available. I’m thinking the next step is to craft an OpenAPI fo TAXII, and STIX. Creating a machine readable blueprint for deploying, managing, and documenting a threat intelligence API. I couldn’t find any existing work on an OpenAPI definition, so this seems like a logical place to begin working to build on, and augment the work of the Cyber Threat Intelligence Technical Committee. Clearly, the working group has created a robust set of specifications, but I’d like to help move it closer to implementation with an OpenAPI. I have created a Github organization to help organize any work on this project. I have forked the project for STIX and TAXII there, as well as started a planning repository to coordinate any work I’m contributing to the conversation. I have also created a repository for working on and publishing the OpenAPI that will define the project. Once we have this, I’d like to start thinking about the development of a handful of server side implementations in maybe Node.js, Python, PHP, or other common programming language. Here are the next steps I’d like to see occur around this project: OpenAPI - Create an OpenAPI for...[<a href="/2017/08/01/craft-an-openapi-for-an-existing-threat-intelligence-sharing-api-specification/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/versioneye/01-veye-licenses-7679418f2968513011985476db94b59b0ab65abd5c030a6a92189fc0e1170722.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/08/01/api-sdk-licensing-notifications-using-versioneye/">API SDK Licensing Notifications Using VersionEye</a></h3>
			<p><em>01 Aug 2017</em></p>
			<p>
I have been watching VersionEye for a while now. If you aren’t familiar, they provide a service that will notify you of security vulnerabilities, license violations and out-dated dependencies in your Git repositories. I wanted to craft a story specifically about their licensing notification services, which can check all your open source dependencies against a license white list, then notify you of violations, and changes at the SDK licensing level.

The first thing I like here, is the notion of an API SDK licensing whitelist. The idea that there is a service that could potentially let you know which API providers have SDKs that are licensed in a way that meets your integration requirements. I think it helps developers who are building applications on top of APIs understand which APIs they should or shouldn’t be using based upon SDK licensing, while also providing an incentive for API providers to get their SDKs organized, including the licensing–you’d be surprised at how many API providers do not have their SDK house in order.

VersionEye also provides CI/CD integration, so that you can stop a build based on a licensing violation. Injecting the politics of API operations, from an API consumers perspective, into the application lifecycle. I’m interested in VersionEye’s CI/CD, as well as security vulnerabilities, but I wanted to make sure this approach to keeping an eye on SDK licensing was included in my SDK, monitoring, and licensing research, influencing my storytelling across these areas. Some day all API providers will have a wide variety of SDKs available, each complete with clear licensing, published on Github, and indexed as part of an APIs.json. We just aren’t quite there yet, and we need more services like VersionEye to help build awareness at the API SDK licensing level to get us closer to this reality.

[<a href="/2017/08/01/api-sdk-licensing-notifications-using-versioneye/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/carryload_diego_rivera1.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/">You See Duplicate Work While I See Common Patterns</a></h3>
			<p><em>31 Jul 2017</em></p>
			<p>Someone asked me on Twitter recently how I deal the duplicate work required to manage a large volume of OpenAPIs. All the same things you have to do when crafting the headers, parameters, responses, and schema across every OpenAPI you are crafting. My response was that I don’t see these things as repetitive or duplicate work, I see these things as common patterns across the resources I am making available. They main reason I think they seem repetitive is the tooling we are currently using needs to play catch up, and help us better apply common patterns across all our APIs–dealing with the duplicate, repetitive work for us. I’m confident that open source API design tooling like Apicurio are going to help us better manage the common patterns we should be applying across our OpenAPIs. I’m also hopeful that OpenAPI 3.0 contributes to reuse of parameters, schema, errors, and other common building blocks across the request and response surface of our API. I’m counting on OpenAPI + Apicurio as well as other API definition and design tooling to step up and do the hard work wen it comes to helping us manage the common patterns across our APIs, and make the reuse of common patterns across our APIs a good thing, and never a burden. This reuse shouldn’t just being within any company, and we should be reusing and sharing patterns from across the space, including common web concepts and standards. The fact that you use ISO 8601 for all your dates, while employing a handful of date field names over and over across your systems isn’t repetition or duplicate work, that is consistency, and sensible reuse of common API patterns. It is the job of API design service and tooling providers to help us get over this hump, and craft notebooks, catalogs, collections, dictionaries, and stores of the common patterns we will need to be applying (over and over again) across our API definitions–OpenAPI....[<a href="/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/gsa/gsa-prototype-api-portal.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/31/professional-api-deployment-templates/">Professional API Deployment Templates</a></h3>
			<p><em>31 Jul 2017</em></p>
			<p>I wrote about the GSA API prototype the other day. It is an API prototype developed by the GSA, providing an API that is designed in alignment with GSA API design guidelines, complete with an API portal for delivering documentation, and other essential resources any API deployment will need. The GSA provides us with an important approach to delivering open source blueprints that other federal agencies can fork, reverse engineer and deploy as their own custom API implementation. We need more of this in the private sector. We need a whole buffet of APIs that do a variety of different things, in every language, and platform or stack that we can imagine. Need a contact directory API, or maybe a document storage API, URL shortener API–here is a forkable, downloadable, open source solution you can put to work immediately. We need the WordPress for APIs. Not the CMS interface WordPress is known for, just a simple API that is open source, and can be easily deployed by anyone in any common hosting, and serverless environments. Making the deployment of common API patterns a no-brainer, and something anyone can do, anywhere in the cloud, on-premise, or on-device. Even though these little bundles of API deployment joy would be open source, there would be a significant amount of opportunity routing folks to other add-on, and premium services on top of any open source API deployment, as well as providing cloud deployment opportunities for developers–similar to the separation between WordPress.com and WordPress.org. I could see a variety of service providers emerge that could cater to the API deployment needs of various industries. Some could focus on more data specific solutions, while others could focus on content, or even more algorithmic, machine learning-focused API deployment solutions. If linked data, hypermedia, gRPC, and GraphQL practitioners want to see more adoption in their chosen protocol, they should be publishing, evolving, and maintaining robust, reverse engineer-able, forkable, open source examples of the...[<a href="/2017/07/31/professional-api-deployment-templates/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_dark_dali.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/31/making-the-business-of-apis-more-modular-before-you-do-the-tech/">Making The Business Of APIs More Modular Before You Do The Tech</a></h3>
			<p><em>31 Jul 2017</em></p>
			<p>I have been immersed in how APIs are being done in the federal government for the last week or so, looking for positive API behavior I can showcase and focus on in my storytelling. I was walking through each step of my API lifecycle, sizing up the federal government for each area I track the private sector on when it comes to APIs. I was taking a look at the areas of microservices, containerization, and serverless. You know the modularization of IT infrastructure in government? I couldn’t find much rubber meeting the road when it comes to microservices or containerization in my research, but I did see hints of modularizing the business aspects of doing APIs in the federal government. Over at 18F you can find some interesting discussion around micro-procurement, “a procurement model that breaks what would traditionally be a large, monolithic contract into several shorter-term, lower dollar amount contracts.” I feel that breaking down the business of defining, designing, deploying, managing, and even testing your APIs into small projects is an important first step for many companies, organizations, institutions, and agencies. While not all organizations will be the same, many will need to break down the business of procuring API design, deployment, and management services, before they can even getting to work breaking down the technical components of what is needing to be delivered. I have been working with my partners at Skylight on our approach to breaking down and delivering API projects, discussing the technical, business, and political aspects of doing things in as modular, bite-size chunks as we possibly can. This involves exploring the other side of the micro-procurement coin, with micro-consulting, providing API related services in small, modular projects. I’m exploring the delivery of API training and curriculum, as well as white paper, guide, and other content-centric services using a micro-consulting approach–keeping engagements small, focused, and delivering additional services that support one or many individual API implementations. Micro procurement and...[<a href="/2017/07/31/making-the-business-of-apis-more-modular-before-you-do-the-tech/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/shipping-energy-trucking.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/31/balancing-domain-expertise-with-the-disruptive-power-of-upstarts-who-do-apis/">Balancing Domain Expertise With The Disruptive Power Of Upstarts Who Do APIs</a></h3>
			<p><em>31 Jul 2017</em></p>
			<p>APIs aren’t good, or bad, nor are they neutral. APIs do the bidding of their providers, and sometimes their consumers. In my experience APIs are more often used for bad than they are ever used for good, something I try to be as vocal as I can about, while working hard to shine a light on the good that is possible. After many years of trying to help folks understand APIs, one of the biggest challenges I face involves the unrealistic rhetoric of startups. The overoptimistic vision and promises of what APIs will do, coupled with an an often limiting awareness of the challenges and complexity of industries where APIs are targeting, making for a pretty toxic, non-cooperative environment for actually getting anything done. I work hard to keep APIs alive in a variety of industries that have seen multiple waves of startups trumpeting their disruption and change horns, while also often belittling and underestimating the people within the industry. I recently came across a post recently that captures the challenge we all face when we are looking to make change within established, and often entrenched industries using APIs. I feel this paragraph captures it well: The new players, and the venture capital/private equity money backing them, think they are entering a world full of Luddites. Yet the brokers we’ve talked to—and we know it’s not everybody—are quite IT-oriented. In a world where visibility is paramount, they are keenly aware of technology’s role in keeping them competitive. They are investing in IT and will continue to do so as prices drop. Meanwhile, many bring vast experience in mastering the physical part of the solution that the startups can’t touch. It is interesting to come across this friction in the freight brokerage industry. It is something I’ve seen in industry after industry, and with each wave of startups doing APIs. In some spaces startups will find success, but in others they will find themselves stopped cold...[<a href="/2017/07/31/balancing-domain-expertise-with-the-disruptive-power-of-upstarts-who-do-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/state-2017/bw-government.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/27/state-of-apis-in-the-federal-government/">State of APIs In The Federal Government</a></h3>
			<p><em>27 Jul 2017</em></p>
			<p>This is my talk from Washington DC with Steve Willmott of 3Scale by Red Hat about transforming enterprise IT with containers, APIs, and integration, where I assess the current state of APIs in the federal government, and the opportunity in the private sector when it comes to working with government data. API Evangelist My name is Kin Lane. I am the API Evangelist. I have studied the technology, business, and politics of Application Programming Interfaces, or more commonly known as APIs, full time since 2010. I spend time looking through the growing number of APIs available today, as well as the evolving group of service providers selling their solutions to API providers. I take what I learn across the space and publish as over 80 independent research projects that I run on Github, covering a growing number of stops along the API lifecycle. In 2011, I began studying and writing about federal government APIs. I have long had an interest in politics, and how our government works (or doesn’t work), which was in alignment with thinking about how I could take what I’ve learned about APIs and apply to the federal government. By 2013, my research and storytelling about APIs attracted the attention of folks in government, which led to an invitation to come work on open data and API projects at multiple agencies. This move took my work to new levels, opening up some interesting doors that have opened my eyes to the scope of APIs within federal government. Presidential Fellow In the summer of 2013 I was invited to be part of the round two Presidential Innovation Fellowship, and work at the Department of Veterans affairs doing web service and API inventory, as well as assist with the wider open data efforts of the Obama administration. I worked in DC until the government shutdown in October, when I decided to leave my position so that I could continue doing my work around veterans...[<a href="/2017/07/27/state-of-apis-in-the-federal-government/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/raven-fence.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/26/we-have-a-hostile-ceo-which-requires-a-shift-in-our-api-strategy/">We Have A Hostile CEO Which Requires A Shift In Our API Strategy</a></h3>
			<p><em>26 Jul 2017</em></p>
			<p>As I work my way through almost one hundred federal government API developer portals, almost 500 APIs, and 133 Github accounts for federal agencies the chilling effect of the change of leadership in this country becomes clear. You can tell the momentum across hundreds of federal agency built up over the last five years is still moving, but the silence across blogs, Twitter accounts, change logs, and Github repos shows that the pace of acceleration is in jeopardy. When you are browsing agency developer portals you come across phrases like this, “As part of the Open Government Initiative, the BusinessUSA codebase is available on the BusinessUSA GitHub Open Source Repository.” With the link to the Open Government Initiative leading to a a page on the White House website that has been removed–something you can easily find on the Obama archives. I am coming across numerous examples like this of how the change in leadership has created a vacuum when it comes to API and open data leadership, at a time when we should be doubling down on sharing of data, content, and putting algorithms to work across the federal government. After several days immersed in federal government developer areas it is clear we have a hostile CEO that will require us to shift in our API strategy. After six months it is clear that the current leadership has no interest transparency, observability, or even the efficiency in government that is achieved from focusing opening up data via public, but secure APIs. This doesn’t mean the end of our open data and API efforts, it just means we lose the top down leadership we’ve enjoyed for the last eight years when it came to technology in government, and efforts will have to shift to a more bottom up approach, with agencies and departments often setting their own agenda. This is nothing new, and it won’t be the last time we face this working with APIs across...[<a href="/2017/07/26/we-have-a-hostile-ceo-which-requires-a-shift-in-our-api-strategy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/steve_and_i_apistrat_2016.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/25/i-am-speaking-on-state-of-apis-in-federal-government-thursday-in-dc/">I Am Speaking On State Of APIs In Federal Government Thursday In DC</a></h3>
			<p><em>25 Jul 2017</em></p>
			<p>
I am joining my friend Steve Willmott in DC this week to talk about federal government APIs. We will  be gathering at Tysons’ Biergarten between 1:30 and 5:00 PM this Thursday to talk APIs. Both Steve and I will be speaking individually, with some QA, and a happy hour afterwards as an opportunity for more discussion.

I am looking forward for the opportunity to hanging with my friend Steve, as the last time we’ve hung out and spoke together was APIStrat in Boston, but at APIStat we are always running a conference, and not actually focused on our views of the APIs space. So, I am eager to learn more detail about what 3Scale is up to as part of the Red Hat machine, and specifically some of the containerization, microservices, and virtualization discussions they are leading lately.

Anyways, I will be in DC all day Thursday. Come join the conversation. I won’t have much time in DC, so the gathering will be the best opportunity to grab a moment of my time. I’ll be talking about the state of APIs in federal government, something I’m reminded during my research and preparation for my talk is probably the most important discussion we should be having in the API space right now.

Looking forward to seeing you all in DC. Thanks to Red Hat for bringing me out to DC, and making this conversation possible. I’ll see you Thursday.

Event Details:
Date: Thursday, July 27, 2017

Time: 1:30 p.m. – 5:00 p.m.
Registration: 1:30 – 2:00 p.m.
Presentations: 2:00 – 3:30 p.m.
Happy Hour: 3:30 – 5:00 p.m.

Location:
Tysons’ Biergarten
8346 Leesburg Pike
Tysons, VA 22182

[<a href="/2017/07/25/i-am-speaking-on-state-of-apis-in-federal-government-thursday-in-dc/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/18f/9302707420_dbc7c2c437_o.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/25/api-management-across-all-government-agencies/">API Management Across All Government Agencies</a></h3>
			<p><em>25 Jul 2017</em></p>
			<p>This isn’t a new drum beat for me, but is one I wanted to pick it up again as part of the federal government research and speaking I’m doing this month. It is regarding the management of APIs across federal government. In short, helping agencies successfully secure, meter, analyze, and develop awareness of who is using government API resources. API management is a commodity in the private technology sector, and is something that has been gaining momentum in government circles, but we have a lot more work ahead to get things where we need them. The folks over at 18F have done a great job of helping bake API management into government APIs using API Umbrella, resulting in these twelve federal agencies: BusinessUSA.gov API Department of Agriculture Department of Commerce Department of Education Federal Communications Commission Federal Election Commission Food and Drug Administration General Services Administration National Aeronautics and Space Administration National Institutes of Health National Renewable Energy Laboratory Regulations.gov API This doesn’t just mean that each of these agencies are managing their APIs. It also means that all of these agencies are managing their APIs in a consistent way, using a consistent tool. Something that is allowing these agencies to effectively manage: API Key Usage - How to use your API key after signing up. Web Service Rate Limits - Daily and hourly rate limits on accessing api.data.gov APIs. General Web Service Errors - General error codes that can be returned by any api.data.gov API. HTTPS Usage - Information about HTTPS usage on api.data.gov. I know that both 18F and USDS are working are hard on this, but this is an area we need agencies to step up in, as well as the private sector. We need any vendor doing API deployment projects for any agency to work together to make sure their agency is using a standardized approach. This means that vendors should make the investment when it comes to reaching out to...[<a href="/2017/07/25/api-management-across-all-government-agencies/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/18f/vulnerabilities-disclosure-policy.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/25/adding-vulnerability-disclosure-to-my-api-building-block-recommendations/">Adding Vulnerability Disclosure To My API Building Block Recommendations</a></h3>
			<p><em>25 Jul 2017</em></p>
			<p>I am working through the almost 100 federal government agency developer portals and the almost 500 APIs that exist across these agencies, looking for the good and bad of APIs in government at this level. One of interesting building blocks I’ve stumbled across, that I would like to shine a light on for other public and private sector API providers to consider in their own operations is a vulnerability disclosure. I feel that 18F description of their vulnerability disclosure says it best: As part of a U.S. government agency, the General Services Administration (GSA)’s Technology Transformation Service (TTS) takes seriously our responsibility to protect the public’s information, including financial and personal information, from unwarranted disclosure. We want security researchers to feel comfortable reporting vulnerabilities they’ve discovered, as set out in this policy, so that we can fix them and keep our information safe. This policy describes what systems and types of research are covered under this policy, how to send us vulnerability reports, and how long we ask security researchers to wait before publicly disclosing vulnerabilities. This should be default across all federal, state, county, and municipal government agencies. Hell, it should be default across all companies, organizations, and institutions. One of the reasons we have so much dysfunction in the security realm that elevates the discussion to theatrical levels with cybersecurity is that we aren’t having honest conversations about the vulnerabilities that exist. Few platforms want these conversations to occur, let alone set the tone of the conversation in such an open way. Without any guidance, and fear of retaliation, developers and analysts who find vulnerabilities will continue to hold back on what they find. Vulnerability disclosure seems like something that ALL API provides should possess. There is no reason you can’t fork the GSA vulnerability policy and share it as the official tone of the vulnerability disclosure conversation on your platform. Encouraging all API developers to understand what the tone of the conversation...[<a href="/2017/07/25/adding-vulnerability-disclosure-to-my-api-building-block-recommendations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/census/census-api-updates.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/25/a-lack-of-communication-around-federal-government-apis/">A Lack Of Communication Around Federal Government APIs</a></h3>
			<p><em>25 Jul 2017</em></p>
			<p>I personally understand the challenges with communicating publicly when you work for the federal government. It is one of the top reasons I do not work in federal government anymore. It would kill me if I couldn’t blog each day without friction–it is how I create and ideate. Even with this understanding I find myself regularly frustrated with the lack of communication by owners of APIs across federal government agencies. There are numerous agencies who do successfully communicate around their APIs and open data projects, but the majority of APIs I come across have little, or no communication around their API operations. Have a blog, Twitter, or Github account might seem like a nice to have, but in reality they are often the only sign that anyone is home, and an API is reliable, and make the the difference between choosing to integrate with an API, or not. A blog or Twitter account, and whats new feature box on the home page of an API developer portal can send the winning (or losing) signal that an API is actually active and alive. Developers come across a lot of APIs that are dormant or abandoned, and the presence of common communication channels (blog, Twitter, Facebook, LinkedIn, Github) are the signal we often need before we are willing to invest the time into learning another new API, or signing up for yet another developer account. I know that it is possible to handle API communications in a healthy way at government agencies–18F, Census, and others are doing it right. There is some serious storytelling friction occurring in government. I see the same illness in corporate and other institutional API platforms–geeks and IT folks aren’t always the best at getting the word out about what they are doing. However, I think there is additional friction at the government level. We’ve seen a significant increase in blogging, and social network usage usage across government agencies, we need to investigate...[<a href="/2017/07/25/a-lack-of-communication-around-federal-government-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/hack-education/hack-education-personalize-learning-and-the-power-of-the-gates-foundation-to-shape-education-policy.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/24/the-hack-education-gates-foundation-grant-data-has-an-api/">The Hack Education Gates Foundation Grant Data Has An API</a></h3>
			<p><em>24 Jul 2017</em></p>
			<p>I have been helping my partner in crime Audrey Watters (@audreywatters) adopt my approach to managing data project(s) using Google Sheets and Github, as part of her work on ed-tech funding. She is going through many of the leading companies, and foundations behind the funding of technology used across the education sector, and doing the hard work of connecting the dots behind how technology gets funded in this critical layer of our society. I want Audrey (and others), to be self-sufficient when it comes to managing their data projects, which is why I’ve engineered it to use common services (Google Sheets, Github), with any code and supporting elements as self-contained as possible–something Github excels at when it comes to managing data, content, and code. While Audrey is going to town creating spreadsheets and repos, I wanted to highlight a single area of her research into the grants that the Gates Foundation are handing out. She has worked hard to normalize data across many years (1998-2017) of PDF and HTML data into a single Google Sheet, then she has published as individual YAML files which live on Github–making her work forkable and reusable by anyone. Once published, Audrey is the first person to fork the YAML, and put to work in her storytelling around ed-tech funding, but each of her project repos also come with an API for her research by default. Well, ok, it isn’t a full-blown searchable API, but in addition to being able to get data in YAML format, she has a JSON API for each year of the Gates Foundation grants (ie. 2016). Increasing the surface area when it comes to collaborating and building on top of her work, which can be forked using Github, or accessed via the machine readable YAML and JSON files. While she is busy creating new Google Sheets and repos for other companies, I wanted to add one more tool to her toolbox, an APIs.json index for...[<a href="/2017/07/24/the-hack-education-gates-foundation-grant-data-has-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/lessons/google-sheet-to-github.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/">First Handful Of Lessons Using My Google Sheet Github Approach</a></h3>
			<p><em>24 Jul 2017</em></p>
			<p>With my recent shift to using Google Sheets as my data backend for my research, and my continued usage of Github as my data project publishing platform, I started pushing out some new API related lessons. I wanted to begin formalizing my schema and process for this new approach to delivering lessons with some simple topics, so I got to work taking my 101, and history of APIs work, and converting them into a multi-step lesson. Some of my initial 101 API lessons are: API 101 (Website) (Github Repo) (Google Sheet) - Just a general overview of what is API, targeting average user. API Provider 101 (Website) (Github Repo) (Google Sheet) - Working to evolve an opening pitch to would be API providers. API Consumer 101 (Website) (Github Repo) (Google Sheet) - Working to get better at providing information for API consumers. The History of APIs (Website) (Github Repo) (Google Sheet) - Continuing to expand on my history of APIs story. I will keep working those 101 lessons. Editing, polishing, expanding, and as I found out with this revision–removing some elements of APIs that are fading away. While my 101 stories are always working to reach as wide as possible, my wider research is always based in two sides of the API coin, with information about providing APis, while also keep my API consumer hat on, and thinking about the needs of developers and integrators. Now that I have the 101 lessons under way I wanted to focus on my API life cycle research, and work on creating a set of high level lessons for each of the 80+ stops I track on along a modern API life cycle. So I got to work on the lesson for API definitions, which I think is the most important stop along any API life cycle–one that actually crosses with every other line. Definitions (Website) (Github Repo) (https://docs.google.com/spreadsheets/d/13WXRAA30QMzKXRu-dH8gr-UrAQlLLDAD9pBAmwUPIS4/edit#gid=0) After kicking off a lesson for my API life cycle...[<a href="/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/training/gargoyle_light_dali.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/24/finding-things-i-want-to-write-about-when-apis-are-dumb/">Finding Things I Want To Write About When APIs Are Dumb</a></h3>
			<p><em>24 Jul 2017</em></p>
			<p>You ever wake up some days, and find yourself not caring about APIs, or much else in the realm of technology? No? Well, I do. Regularly. I find myself in this headspace on this fine Monday morning, and without a weeks worth of stories scheduled, it is a very bad place to be as the API Evangelist. Part of this problem is me–I am a pain in my ass. However, a another portion of it is just about staying motivated, engaged, and producing compelling (ha) content on a regular basis for the blog, and other projects I’m working on. There are almost a hundred stories in my notebook and all of them seem really, really dumb to me this morning. I can’t seem to muster up the energy to take any of them and turn into even a three paragraph API blah blah blah story. It’s just words right? I should be able to do it. I churn out meaningless API words all the time, non-stop for the last seven years! I should be able to do it today. What is wrong with you man? C’mon, you should be able to just turn it on, and the words will flow. Not today. Like many days before I am going to need to trick myself into turning on the faucet. The best place to start (for me) when I have lost my writing mojo, is to find a project I truly care about 100%. This is why I work on the human services API project, and look for ways that I can help my partner in crime Audrey Watters (@audreywatters) with her Hack Education work, as she is always focused on the most critical area we face when it comes to our use of technology–education. Understanding how technology is helping, or hurting us when it comes to educating every human on earth is serious business, and something that might just help pull me from my writing...[<a href="/2017/07/24/finding-things-i-want-to-write-about-when-apis-are-dumb/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/24/federal-government-apis-in-a-trump-administration/">Federal Government APIs In A Trump Administration</a></h3>
			<p><em>24 Jul 2017</em></p>
			<p>I haven’t written much about APIs in the federal government since the election. I’m still having conversations, and investing time into monitoring what is going on in the federal government, but honestly in the name of self-care I have to turn my head from what is going on with the current administration. It’s no secret that I’m not a Trump supporter, and honestly I have trouble not getting angry with Trump supporters when it comes to making the federal government more transparent and observable with data and APIs. The current tone the administration is taking when it comes to transparency, observability, and accountability will take us decades to recover from, making conversations about federal government APIs very difficult to have in many scenarios. Luckily, I’m regularly reminded that there are MANY good people at government agencies who are doing amazing things, allowing me find more energy for thinking about APIs in federal government. I’ve been preparing for a talk I’m doing in DC this week with 3Scale by Red Hat, which is priming the pump for a presentation I’m doing for the General Services Administration later in August. Both of these talks give me the chance to think about federal government, and invest some energy into finding the good that is going on in the federal government when it comes to APIs. It will also give me some time to take a look at what challenges exist when doing APIs at the federal level of government, with some acknowledgement of the current leadership in the White House. First, I’m going to go agency by agency, taking a fresh look at anything API going on at the top level agencies, with a quick secondary look at some of the lesser known agencies. After this, I want to take a look at who is behind any API project that I’m coming across–understanding what I can about the internal groups doing APIs, any inter-agency efforts, including efforts out...[<a href="/2017/07/24/federal-government-apis-in-a-trump-administration/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/stix/stix-logo.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/21/structured-threat-information-expression-stix/">Structured Threat Information Expression (STIX)</a></h3>
			<p><em>21 Jul 2017</em></p>
			<p>I wrote about the opportunity around developing an aggregate threat information API, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of the Structured Threat Information Expression (STIX), as one possible schema for definining and sharing the information I’m talking about. Here is a quick summary of STIX is from the website: Structured Threat Information Expression (STIX™) is a language for describing cyber threat information in a standardized and structured manner to enable the exchange of cyber threat intelligence (CTI). STIX characterizes an extensive set of CTI to include indicators of adversary activity, as well as contextual information characterizing cyber adversary motivations, capabilities, and activities and best courses of action for defense and mitigation. I haven’t dug into STIX too much, so I’m not making recomendations on the value it brings to the table yet, but I want to make sure we take a look at any existing work that was already on the, and make sure we aren’t reinventing the wheel with any part of an aggregated threat information API. At first glance STIX looks like a pretty damn good start for a potential API schema, that speaks a common language, and is seeing adoption with other existing threat information storage and sharing providers. I am adding STIX to my research into threat information sharing, and wider API security research. I am currently diving deeper into API security thanks to investment from Elastic Beam, and I will be publishing a guide, as well as an API security white paper as part of the work. I’m going to try and provide some intelligence to a group of folks who expressed interest in developing an aggregate threat information sharing API. I’m hoping to better flesh out my thoughts on how API...[<a href="/2017/07/21/structured-threat-information-expression-stix/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/google/google-apps-connected.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/21/requiring-all-platform-partners-use-the-api-so-there-is-a-registered/">Requiring ALL Platform Partners Use The API So There Is A Registered</a></h3>
			<p><em>21 Jul 2017</em></p>
			<p>I wrote a story about Twitter allowing users to check or uncheck a box regarding sharing data with select Twitter partners. While I am happy to see this move from Twitter, I feel the concept of information sharing being simply being a checkbox is unacceptable. I wanted to make sure I praised Twitter in my last post, but I’d like to expand upon what I’d like to see from Twitter, as well as ALL other platforms that I depend on in my personal and professional life. There is no reason that EVERY platform we depend on couldn’t require ALL partners to use their API, resulting in every single application of our data be registered as an official OAuth application. The technology is out there, and there is no reason it can’t be the default mode for operations. There just hasn’t been the need amongst platform providers, as as no significant demand from platform users. Even if you don’t get full access to delete and adjust the details of the integration and partnership, I’d still like to see companies, share as many details as they possibly can regarding any partner sharing relationships that involve my data. OAuth is not the answer to all of the problems on this front, but it is the best solution we have right now, and we need to have more talk about how we can make it is more intuitive, informative, and usable by the average end-users, as well as 3rd party developers, and platform operators. API plus OAuth is the lowest cost, widely adopted, standards based approach to establishing a pipeline for ALL data, content, and algorithms operate within that gives a platform the access and control they desire, while opening up access to 3rd party integrators and application developers, and most importantly, it gives a voice to end-users–we just need to continue discussing how we can keep amplifying this voice. To the folks who will DM, email, and Tweet...[<a href="/2017/07/21/requiring-all-platform-partners-use-the-api-so-there-is-a-registered/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/desert_dragon_light_dali.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/21/misconceptions-about-what-openapi-isnt-still-slowing-conversations/">Misconceptions About What OpenAPI Is(nt) Still Slowing Conversations</a></h3>
			<p><em>21 Jul 2017</em></p>
			<p>I’ve been pushing forward conversations around my Human Services Data API (HSDA) work lately, and hitting some friction with folks around the finer technical details of the API. I feel the friction around these API conversations could be streamlined with OpenAPI, but with most folks completely unaware of what OpenAPI is and does, there is friction. Then for the handful of folks who do know what OpenAPI is and does, I’m seeing the common misconceptions about what they think it is slowing the conversation. Let’s start with the folks who are unaware of what OpenAPI is. I am seeing two main ways that human services data vendors and implementations have conversations about what they need: 1) documentation, and 2) code. The last wave of HSDA feedback was very much about receiving a PDF or Word documentation about what is expected of an application and an API behind it. The next wave of conversations I’m having are very much here are some code implementations to demonstrate what someone is looking to articulate. Both very expensive waves of articulating and sharing what is needed for the future, or to develop a shared understanding. My goal throughout these conversations is to help folks understand that there are other more efficient, low costs ways to articulate and share what is needed–OpenAPI. Beyond the folks who are not OpenAPI aware, the second group of folks who see OpenAPI as a documentation tool, or code generation tool. Interestingly enough a vantage point that is not very far evolved beyond the first group. Once you know what you have, you document it using OpenAPI, or you generate some code samples from it. Relinquishing OpenAPI to a very downstream tool, something you bring in after all the decisions are made. I had someone say to me, that OpenAPI is great, but we need a way to be having a conversation about each specific API request, the details of the that request, with a...[<a href="/2017/07/21/misconceptions-about-what-openapi-isnt-still-slowing-conversations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/plivo/plivo-support-knowledge-base.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/20/the-plivo-support-portal-and-knowledge-base/">The Plivo Support Portal And Knowledge Base</a></h3>
			<p><em>20 Jul 2017</em></p>
			<p>
I’m always watching out for how existing API providers are shifting up their support strategies in their communities as part of my work. This means staying into tune with their communications, which includes processing their email newsletters and developer updates. Staying aware of what is actually working, and what is not working, based upon active API service providers who are finding ways to make it all work.

Plivo opted out to phase out direct emails at the end of the month, and pushing developers to use the Plivo support portal, and the ticketing system. The support portal provides a knowledge base which provides a base of self-service support before any developer actually uses the support ticketing system to:


  Create, manage, respond to and check the status of your support ticket(s)
  Select improved ticket categories for more efficient ticket routing and faster resolution
  Receive resolution suggestions from our knowledge base before you submit a ticket to help decrease resolution time


Email only support isn’t always the most optimal way of handling support, and using a ticketing system definitely provides a nice trail to follow for both sides of the conversations. The central ticketing system also provides a nice source of content to feed into the self-service support knowledge base, keeping self-service support in sync with direct support activity.

I’m going to continue to track on which API providers offer a ticketing solution, as well as a knowledge base. I’m feeling like these are what I’m going to recommend to new API providers as what I consider to be default support building blocks that EVERY API platform should be starting with, covering the self-service and direct support requirements of a platform. I’m going to start pushing 1-3 support solutions like ZenDesk, also giving API providers some options when it comes to quickly delivering adequate support for their platforms.

[<a href="/2017/07/20/the-plivo-support-portal-and-knowledge-base/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-site-policy.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/20/managing-platform-terms-of-service-in-a-site-policy-repository/">Managing Platform Terms of Service In A Site Policy Repository</a></h3>
			<p><em>20 Jul 2017</em></p>
			<p>Github is releasing an update to their platform Terms of Service and Corporate Terms of Service. Guess what platform their are using to manage the evolution, and release of their terms of service? Github of course! They are soliciting feedback, along with clarifications and improvements to their terms of service, with an emphasis on helping making things more readable! #nice Github has provided a deadline for everyone to submit comments by the end of the month, then they’ll spend about a week going through the comments before making any changes. It provides a pretty useful way for any platform to manage their terms of service in a way that gives the community a voice, and provides some observability into the process for everyone else who might not feel confident enough to chime in on the process. This can go a long way towards building trust with the community, even if they don’t directly participate in the process. Managing terms of service using Github makes sense for all providers, not just Github. It provides an open, transparent, and participatory way to move forward one of the most important documents that is governing API consumption. It is logical that the drafting, publishing, and evolution of platform terms be done out in the open, where the community can watch and participate. Pushing forward the design of the legal document in sync with the design, deployment, management, SDKs and other aspects of API operations. Bringing the legal side of things out of the shadows, and making it part of the conversation within the community. Eventually, I’d like to see the terms of service, privacy policies, service level agreements, and other legal documents that govern API operations managed and available on Github like this. It gives the wider API community the chance to play a more significant role in hammering out the legal side of API operations, ensuring this are easier to follow and understand, and maybe even standardized across...[<a href="/2017/07/20/managing-platform-terms-of-service-in-a-site-policy-repository/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/charles-to-openapi/har-conversion.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/20/charles-proxy-generated-har-to-openapi-using-api-transformer/">Charles Proxy Generated HAR To OpenAPI Using API Transformer</a></h3>
			<p><em>20 Jul 2017</em></p>
			<p>I was responding to Jean-Philippe M. (@jpmonette) tweet regarding whether or not I had moved forward my auto generation of OpenAPIs from traffic captured by Charles Proxy. It is one of many features of my internal systems I have not gotten around to finishing, but thankfully he actually answered his own question, and found a better solution than even I had–using my friends over at API Transformer. I had been exploring ways for speeding up the process of generating OpenAPI specs for the APIs that I’m reviewing, something that becomes very tedious when working with large APIs, as well as just profiling the sheer number of APIs I am looking profile as part of my work. I haven’t been profiling many APIs lately, but the approach Jean-Philippe M. came up is petty damn easy, leaving me feeling pretty silly that I hadn’t connected the dots myself. Here is what you do. Fire up Charles Proxy: Then open up Postman, and make any API calls. Of course you could also proxy mobile application or website API calls through your Charles Proxy, but Postman is a great way to for a majority of the APIs I depend on. After you’ve made the calls to all the APIs you are looking to generate an OpenAPI for, save your Charles Proxy session as a .har file, which is the last option on the dropdown menu available while saving. Then you head over to API Transformer and upload your .har file, and select OpenAPI (Swagger) 2.0 as the output–push convert. API Transformer will then push a fresh OpenAPI to your desktop, or allow you to publish via a portal, and generate an SDK using APIMATIC. Automated (mostly) generation of OpenAPI definitions from API traffic you generate through your browser, Postman, Restlet Client, mobile application, or other tooling. I have abandoned my internal systems, except for my stack of APIs, and depending mostly on 3rd party services like Charles Proxy, Postman,...[<a href="/2017/07/20/charles-proxy-generated-har-to-openapi-using-api-transformer/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bots-satellites.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/20/100k-view-of-bot-space-from-the-api-evangelist-perspective/">100K View Of Bot Space From The API Evangelist Perspective</a></h3>
			<p><em>20 Jul 2017</em></p>
			<p>I had a friend ask me for my thoughts on bots. It is a space I tend to rant about frequently, but isn’t an area I’m moving forward any meaningful research in, but it does seem to keep coming up and refuses to ever go way. I think bots are a great example of yet another thing that us technologists get all worked up about and think is the future, but in reality, while there will only be a handful of viable use cases, and bots will cause more harm, than they ever will do any good, or fully enjoy a satisfactory mainstream adoption. First, bots aren’t new. Second, bots are just automation. Sure, there will be some useful automation implementations, but more often than not, bots will wreak havoc and cause unnecessary noise. Conveniently though, no matter what happens, there will be money to be made deploying and defending against each wave of bot investment. Making bots is pretty representative of how technology is approached in today’s online environment. Lot’s of tech. Lot’s of investment. Regular waves. Not a lot of good sense. Top Bot Platforms Ok, where can you deploy and find bots today? These are the dominant platforms where I am seeing bots emerge: Twitter - Building bots on the public social media platform using their API. Facebook - Building Facebook messenger bots to unleash on the Facebook Graph. Slack - Building more business and productivity focused bots on Slack. There are other platforms like Telegram, and folks developing interesting Github bots, but these three platforms dominate the conversation when it comes to bots in 2017. Each platform brings it’s own tone when it comes to what bots are capable of doing, and who is developing the bots. Another important thing to note across these platforms is that Slack is really the only one working to own the bot conversation on their platform, while on Facebook and Twitter allow the developer community...[<a href="/2017/07/20/100k-view-of-bot-space-from-the-api-evangelist-perspective/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-box.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/19/the-most-important-aspect-of-the-api-discussion-is-learning-to-think-outside/">The Most Important Aspect Of The API Discussion Is Learning To Think Outside</a></h3>
			<p><em>19 Jul 2017</em></p>
			<p>
There are many good things to come out of doing APIs properly. Unfortunately there are also many bad things that can come out of doing APIs badly, or with misaligned expectations. It is easy to focus on the direct benefits of doing APIs like making data resources available to partners, or maybe developing a mobile application. I prefer looking for the more indirect benefits, which are more human, more than they are ever technical.

As I work with different groups on a variety of API definitions and strategies, one very significant part of the process I see, is people being forced to think outside their box. APIs are all about engaging around data, content, and algorithms on the web, with 3rd parties that operate outside your box. You are forced to lookup, and outward a bit. Not everyone I engage with is fully equipped to do this, for a variety of reasons, but overall the API process does make folks just a little more critical than they do with even their websites.

The web has come with a number of affordances. Those same affordances aren’t always present in API discussions forcing folks to have more conversations around why we are doing APIs (an answer shouldn’t always be yes), and discussing the finer details not just storing your data, and managing your schema, but doing in a way that will play nicely with other external systems. You may be doing things one way internally, and it might even be working for you, but it is something that can only get better with each outside partner, or consumer you are exposed to along your journey. Even with all of the internal politics I encounter in my API conversations, the API process always leaves me enjoying almost any outcome.

[<a href="/2017/07/19/the-most-important-aspect-of-the-api-discussion-is-learning-to-think-outside/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/elastic-beam/elasticbeam-security.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/19/more-investment-in-api-security/">More Investment In API Security</a></h3>
			<p><em>19 Jul 2017</em></p>
			<p>I’m getting some investment from ElasticBeam to turn up the volume on my API security research, so I will be telling more stories on the subject, and publishing an industry guide, as well as a white paper in coming weeks. I want my API security to become a first class area of my API research, along side definitions, design, deployment, management, monitoring, testing, and performance. Much of my API security research is built on top of OWASP’s hard work, but honestly I haven’t gotten very far along in it. I’ve managed to curated a handful of companies who I’ve come across in my research, but haven’t had time to dive in deeper, or fully process all the news I’ve curated there. It takes time to stay in tune with what companies are up to, and I’m thankful for ElasticBeam’s investment to help me pay the bills while I’m heads down doing this work. I am hoping that my API security research will also help encourage you to invest more into API security. As I do with my other partners, I will find ways of weaving ElasticBeam into the conversation, but my stories, guides, and white papers will be about the wider space–which Elastic Beam fits in. I’m hoping they’ll compliment Runscope as my partner when it comes to monitoring, testing, and performance (see how I did that, I worked Runscope in too), adding the security dimension to these critical layers of operating a reliable API. One thing that attracted me to conversations with ElasticBeam was that they were developing a solution that could augment existing API management solutions like 3Scale and Amazon Web Services. I’ll have a talk with the team about integrating with Tyk, DreamFactory, and Restlet–my other partners. Damn I’m good. I got them all in here! Seriously though, I’m thankful for these partners investing in what I do, and helping me tell more stories on the blog, and produce more guides and...[<a href="/2017/07/19/more-investment-in-api-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/airtable/airtable-integrations-page.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/19/does-your-platform-have-an-integrations-page/">Does Your Platform Have An Integrations Page?</a></h3>
			<p><em>19 Jul 2017</em></p>
			<p>
I’m continuing to come across more dedicated integration pages for the API platforms I’m test driving, and keeping an eye on. This time it is out of spreadsheet and database hybrid AirTable, that allows you to easily deploy an API complete with a portal, with a pretty robust integrations page for their platform. Airtable’s dedicated integrations page is made easier since they use Zapier, which helps them aggregate over 750+ APIs for possible integration.

Airtable is pretty slick all by itself, but once you start wiring it up to some of the other API driven platforms we depend on, it becomes a pretty powerful tool for data aggregation, and then publishing as an API. I don’t understand why a Zapier-driven API integrations page isn’t default for every API platform out there. API consumption today isn’t just about deploying web or mobile applications, it is about moving data and content around the web–making sure it is where we need it, when we need it.

I’m playing with different variations of the API integrations page lately. I’m exploring the idea of how I can encourage some higher education folks I know, and government open data folks I know to be Zapier advocates within their organizations, and publish a static integrations page, showing the integrations solutions available around the platforms they depend on. Dedicated integration pages help API developers understand the potential of any API, and they help non-developers also understand the potential, but in a way they can easily put into action to solve problems in their world. I’m going to keep beating the API integration page drum, and now that Zapier has their partner API you will also hear me talking about Zapier a lot more.

[<a href="/2017/07/19/does-your-platform-have-an-integrations-page/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/netsil/1-P8w_-2-oCz0QfV6OENawJQ.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/19/containerized-microservices-monitoring-driving-api-infrastructure/">Containerized Microservices Monitoring Driving API Infrastructure</a></h3>
			<p><em>19 Jul 2017</em></p>
			<p>While I track on what is going on with visualizations generated from data, I haven’t seen much when it comes to API driven visualizations, or specifically visualization about API infrastructure, that is new and interesting. This week I came across an interesting example in a post from Netsil about mapping microservices so that you can monitor them. They are a pretty basic visualization of each database, API, and DNS element for your stack, but it does provide solid example of visualizing not just the deployment of database and API resources, but also DNS, and other protocols in your stack. Netsil microservices visualization is focused on monitoring, but I can see this type of visualization also being applied to design, deployment, management, logging, testing, and any other stop along the API lifecycle. I can see API lifecycle visualization tooling like this becoming more common place, and play more of a role in making API infrastructure more observable. Visualizations are an important of the storytelling around API operations that moves things from just IT and dev team monitoring, making it more observable by all stakeholders. I’m glad to see service providers moving the needle with helping visualize API infrastructure. I’d like to see more embeddable solutions deployed to Github emerge as part of API life cycle monitoring. I’d like to see what full life cycle solutions are possible when it comes to my partners like deployment visualizations from Tyk and Dreamfactory APIs, and management visualizations with 3Scale APIs, and monitoring and testing visualizations using Runscope. I’ll play around with pulling data from these provides, and publishing to Github as YAML, which I can then easily make available as JSON or CSV for use in some basic visualizations. If you think about it, thee really should be a wealth of open source dashboard visualizations that could be embedded on any public or private Github repository, for every API service provider out there. API providers should be able to...[<a href="/2017/07/19/containerized-microservices-monitoring-driving-api-infrastructure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorithmia/algorithmia-enterprise.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2017/07/18/specialized-collections-of-machine-learning-apis-could-be-interesting/">Specialized Collections Of Machine Learning APIs Could Be Interesting</a></h3>
			<p><em>18 Jul 2017</em></p>
			<p>I was learning more about CODEX, from Algorithmia, their enterprise platform for deploying machine learning API collections on premise or in the cloud. Algorithmia is taking the platform in which their algorithmic marketplace is deployed on and making it so you can deploy it anywhere. I feel like this is where the algorithmic-centered API deployment is heading, potentially creating some very interesting, and hopefully specialized collections of machine learning APIs. I talked about how the economics of what Algorithmia is doing interests me. I see the potential when it comes to supporting machine learning APIs that service an image or video processing pipeline–something I’ve enjoyed thinking about with my drone prototype. Drone is just one example of how specialized collections of machine learning APIs could become pretty valuable when they are deployed exactly where they are needed, either on-premise or in any of the top cloud platforms. Machine learning marketplaces operated by the cloud giants will ultimately do fine because of their scale, but I think where the best action will be at is delivering curated, specialized machine learning models, tailored to exactly what people need, right where they need them–no searching necessary. I think recent moves by Google to put TensorFlow on mobile phones, and Apple making similar moves show signs of a future where our machine learning APIs are portable, operating on-premise, on-device, and on-network. I see Algorithmia having two significant advantages right now. 1) they can deploy their marketplace anywhere, and 2) they have the economics, as well as the scaling of it figured out. Allowing for specialized collections of machine learning APIs to have the metering, and revenue generation engines built into them. Imagine a future where you can deploy and machine learning and algorithmic API stack within any company or institution, or the factory floor in an industrial setting, and out in the field in an agricultural or mining situation–processing environmental data, images, or video. Exploring the possibilities with real...[<a href="/2017/07/18/specialized-collections-of-machine-learning-apis-could-be-interesting/">Read More</a>]</p>
			<p><hr /></p>
	  

		<!-- Pagination links -->
		<table width="100%">
			<tr>
				<td align="left" width="33%">
				  
				    <a href="/blog/page9" class="previous">
				      &#8592; Previous
				    </a>
				  
			</td>
			<td align="center" width="33%">
			  <span class="page_number ">
			    Page: 10 of 37
			  </span>
			</td>
			<td align="right" width="33%">
		  
		    <a href="/blog/page11" class="next">Next &#8594;</a>
		  
			</tr>
			</tr>
		</table>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
	<hr>
	<p align="center">
		relevant work:
		<a href="http://apievangelist.com">apievangelist.com</a> |
		<a href="http://adopta.agency">adopta.agency</a>
	</p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
