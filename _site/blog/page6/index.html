<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/05/07/graphql-just-get-out-of-my-way-and-give-me-what-i-want/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/16_38_600_500_0_max_1_0_-2.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/05/07/graphql-just-get-out-of-my-way-and-give-me-what-i-want/">GraphQL, Just Get Out Of My Way And Give Me What I Want</a></h3>
			<p><em>07 May 2018</em></p>
			<p>One of the arguments I hear for why API providers should be employing GraphQL is that they should just get out of developers way, and let them build their own queries so that they can just get exactly the data that they want. As an application developer we know what we want from your API, do not have us make many different calls, to multiple endpoints–just give us one API and let us ask for exactly what we need. It is an eloquent, logical argument when you operate and live within a “known bubble”, and you know exactly what you want. Now, ask yourself, will every API consumer know what they want? Maybe in some scenarios, every API consumer will know what they want, but in most situations, developers will not have a clue what they want, need, or what an API does. This is where the GraphQL as a replacement for REST argument begins breaking down. In a narrowly defined bubble, where every developers knows the schema, knows GraphQL, and knows they want–GraphQL can make a lot of sense. In the autodidact alpha developer startup world this argument makes a lot of sense, and gets a lot of traction. However, not everyone lives in this world, and in this real world, API design can become very important. Helping people understand what is possible, learn the schema behind an API, and become more familiar with an API, until they have a better understanding of what they might want. I’m not saying GraphQL doesn’t have a place when you have a significant portion of your audience knowing what they want, I’m just saying that you shouldn’t leave everyone else behind. To further turn this argument on its head, as a developer, if I know what I want, why make me build a query at all? Just give me a single URL with what I want! Don’t make me do the heavy lifting, and work to craft...[<a href="/2018/05/07/graphql-just-get-out-of-my-way-and-give-me-what-i-want/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/05/07/doing-away-with-social-logins-due-to-a-lack-of-trust/"><img src="https://s3.amazonaws.com/kinlane-productions2/social-login.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/05/07/doing-away-with-social-logins-due-to-a-lack-of-trust/">Doing Away With Social Logins Due To A Lack Of Trust</a></h3>
			<p><em>07 May 2018</em></p>
			<p>I received an email from my CRON job API (EasyCRON) provider this morning about discontinuing the usage of social logins to their service with Gmail, Facebook, etc. Something that I think is a sign of things to come in response to the recent (and continued) bad behavior by many of the leading technology platforms. EasyCRON gave the following response for removing social logins from their service: In order to: 1) prevent any confusion that could be caused by using third party platform’s authentication API, 2) decouple from those data greedy platforms, 3) and keep our system simple, we decide to stop using third party platform’s authentication API on EasyCron. As much as I prefer one click login using my Github or Google (rarely use Facebook), I can’t argue with their logic. I’m a big fan of minimizing data sharing, surveillance, and of course keeping things simple. I wish we could trust tech companies to be good citizens with our data, but they seem to prefer repeatedly demonstrating they are more concerned with profits than they are about us. I’m still on board with using Github login for my infrastructure related services, but will definitely stop using Google, Twitter, Facebook, and any other purely advertising related service for authentication purposes. As much of a pain in the ass as it might be to create yet another login, it is more of a pain in the ass to be surveilled, and exploited on a daily basis by technology platforms. I wish people would have more respect, and be better behaved, but things are what they are, and we should respond accordingly. Thanks for fucking this one up for us all Facebook, Google, Twitter, and other platforms who just can’t see the big picture over their short term profit. I am going to stop advising API platforms utilize social logins as part of their API management and on-boarding process, due to it putting developers and consumers at risk...[<a href="/2018/05/07/doing-away-with-social-logins-due-to-a-lack-of-trust/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/05/07/api-management-in-realtime-instead-of-after-the-media-shitstorm/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/109_204_800_500_0_max_0_1_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/05/07/api-management-in-realtime-instead-of-after-the-media-shitstorm/">API Management In Real-Time Instead Of After The Media Shitstorm</a></h3>
			<p><em>07 May 2018</em></p>
			<p>I have been studying API management for eight years now. I’ve spent a lot of time understanding the approach of leading API providers, and the services and tools put out there by API service providers from 3Scale to Tyk, and how the cloud service providers like AWS are baking API management into their clouds. API management isn’t the most exciting aspect of doing APIs, but I feel it is one of the most important elements of doing APIs, delivering on the business and politics of doing APIs, which can often make or break a platform and the applications that depend on it. Employing a common API management solution, and having a solid plan in place, takes time and investment. To do it properly takes lots of regular refinement, investment, and work. It is something that will often seem unnecessary–having to review applications, get to know developers, and consider how they fit into the picture picture. Making it something that can easily be to pushed aside for other tasks on a regular basis–until it is too late. This is frequently the case when your API access isn’t properly aligned with your business model, and there are no direct financial strings attached attached to new API users, or a line distinguishing active from inactive users, let alone governing what they can do with data, content, and algorithms that are made accessible via APIs. The API management layer is where the disconnect between API providers and consumers occur. It is also where the connection, and meaningful engagement occurs when done right. In most cases API providers aren’t being malicious, they are just investing in the platform as their leadership has directed, and acting upon the stage their business model has set into motion. If your business model is advertising, then revenue is not directly connected to your API consumption, and you just turn on the API faucet to let in as many consumers as you can possibly attract....[<a href="/2018/05/07/api-management-in-realtime-instead-of-after-the-media-shitstorm/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/05/01/synthetic-healthcare-records-for-your-api-using-synthea/"><img src="https://s3.amazonaws.com/kinlane-productions2/synthea/synthea-patient-data.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/05/01/synthetic-healthcare-records-for-your-api-using-synthea/">Synthetic Healthcare Records For Your API Using Synthea</a></h3>
			<p><em>01 May 2018</em></p>
			<p>I have been working on several fronts to help with API efforts at the Department of Veterans Affairs (VA) this year, and one of them is helping quantify the deployment of a lab API environment for the platform. The VA doesn’t want it called it a sandbox, so they are calling it a lab, but the idea is to provide an environment where developers can work with APIs, see data just like they would in a live environment, but not actually have access to live patient data before they can prove their applications are reviewed and meet requirements. One of the projects being used to help deliver data within this environment is called Synthea. Providing the virtualized data will be made available through VA labs API–here is the description of what they do from their website: Synthea is an open-source, synthetic patient generator that models the medical history of synthetic patients. Our mission is to provide high-quality, synthetic, realistic but not real, patient data and associated health records covering every aspect of healthcare. The resulting data is free from cost, privacy, and security restrictions, enabling research with Health IT data that is otherwise legally or practically unavailable. Synthea data contains a complete medical history, including medications, allergies, medical encounters, and social determinants of health, providing data can be used without concern for legal or privacy restrictions by developers to support a variety of data standards, including HL7 FHIR, C-CDA and CSV. Perfect work loading up into sandbox and lab API environments, allowing to developers to safely play around with building healthcare applications, without actually touching production patient data. I’ve been looking for solutions like this for other industries. Synthea even has a patient data generator available on Github, which is something I’d love to see for every industry. Sandbox and labs environment should be default for any API, especially APIs operating within heavily regulated industries. I think Synthea provides a pretty compelling model for the...[<a href="/2018/05/01/synthetic-healthcare-records-for-your-api-using-synthea/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/30/facebook-and-twitter-only-now-beginning-to-police-their-api-applications/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/desert-dragon_horiz_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/30/facebook-and-twitter-only-now-beginning-to-police-their-api-applications/">Facebook And Twitter Only Now Beginning To Police Their API Applications</a></h3>
			<p><em>30 Apr 2018</em></p>
			<p>I’ve been reading about all the work Facebook and Twitter have been doing over the last couple of weeks to begin asserting more control over their API applications. I’m not talking about the deprecation of APIs, that is a separate post. I’m focusing on them reviewing applications that have access to their API, and shutting off access to the ones who are’t adding value to the platform and violating the terms of service. Doing the hard work to maintain a level of quality on the platform, which is something they should have been doing all along. I don’t want to diminish the importance of the work they are doing, but it really is something that should have been done along the way–not just when something goes wrong. This kind of behavior really sets the wrong tone across the API sector, and people tend to focus on the thing that went wrong, rather than the best practices of what you should be doing to maintain quality across API operations. Other API providers will hesitate launching public APIs because they’ll not want to experience the same repercussions as Facebook and Twitter have, completely overlooking the fact that you can have public APIs, and maintain control along the way. Setting the wrong precedent for API providers to emulate, and damaging the overall reputation of operating public APIs. Facebook and Twitter have both had the tools all along to police the applications using their APIs. The problem is the incentives to do so, and to prioritize these efforts isn’t there, due to an imbalance with their business model, and a lack of diversity in their leadership. When you have a bunch of white dudes with a libertarian ethos pushing a company towards profitability with a advertising driven business model, investing in quality control at the API management layer just isn’t a priority. You want as may applications, users, and activity as you possibly can, and when you don’t see...[<a href="/2018/04/30/facebook-and-twitter-only-now-beginning-to-police-their-api-applications/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/30/a-readme-for-your-microservice-github-repository/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-readme.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/30/a-readme-for-your-microservice-github-repository/">A README For Your Microservice Github Repository</a></h3>
			<p><em>30 Apr 2018</em></p>
			<p>I have several projects right now that are needed a baseline for what is expected of microservices developers when it comes to the README for their Github repository. Each microservice should be a self-contained entity, with everything needed to operate the service within a single Github repository. Making the README the front door for the service, and something that anyone engaging with a service will depend on to help them understand what the service does, and where to get at anything needed to operate the service. Here is a general outline of the elements that should be present in a README for each microservice, providing as much of an overview as possible for each service: Title - A concise title for the service that fits the pattern identified and in use across all services. Description - Less than 500 words that describe what a service delivers, providing an informative, descriptive, and comprehensive overview of the value a service brings to the table. Documentation - Links to any documentation for the service including any machine readable definitions like an OpenAPI definition or Postman Collection, as well as any human readable documentation generated from definitions, or hand crafted and published as part of the repository. Requirements - An outline of what other services, tooling, and libraries needed to make a service operate, providing a complete list of EVERYTHING required to work properly. Setup - A step by step outline from start to finish of what is needed to setup and operate a service, providing as much detail as you possibly for any new user to be able to get up and running with a service. Testing - Providing details and instructions for mocking, monitoring, and testing a service, including any services or tools used, as well as links or reports that are part of active testing for a service. Configuration - An outline of all configuration and environmental variables that can be adjusted or customized as part...[<a href="/2018/04/30/a-readme-for-your-microservice-github-repository/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/27/venture-capital-obfuscating-the-opportunities-for-value-exchange-at-the-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80_140_800_500_0_max_0_1_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/27/venture-capital-obfuscating-the-opportunities-for-value-exchange-at-the-api/">Venture Capital Obfuscating The Opportunities For Value Exchange At The API</a></h3>
			<p><em>27 Apr 2018</em></p>
			<p>p&gt;&lt;/p&gt;When I talk to government agencies, non-profit organizations, or organizations doing internal APIs I regularly receive pushback about the information I share regarding API management, service composition, rate limiting, access tiers, and the other essential ingredients of the business of APIs. People tell me they aren’t selling access to their APIs, they don’t need to be measuring APIs like publicly available commercial APIs do. They don’t need a free, pro, and other tiers of access, and they won’t be invoicing their consumers like the majority of APIs I showcase from the API universe. This phenomenon is just one of the many negative side effects of venture capital, and Silicon Valley startups driving the conversation around APIs. Most people can’t even see the value exchange that potentially happens at the API management layer, and only see one way commercial revenue. When in reality API management is all about measuring and developing an awareness of the value exchanged around all of an organizations digital assets. With charging consumers for access just one the many ways in which value exchange can be measured, quantified, and reporting upon, by invoicing the consumer. There are so many other ways in which value can be exchanged, incentivized, measured, and reported upon, but many people have tuned into a single dominant narrative, and are completely unaware there is more than one way of doing things. Each API request can be recorded at the API management level. Whether it originated internally, from partners, or through public applications. Each resource path, HTTP verb (GET, POST, PUT, PATCH, DELETE), application and end-user token(s) can is recorded. The value generated by ANY API resource, and the exchange with consumers can be measured. Charging for GETs is such a narrow, and unimaginative view of what is possible. Just because many tech startups are interested in charging to GET data, and extracting value by encouraging unrestrained POST, PUT, PATCH, doesn’t mean this is how all API platforms should...[<a href="/2018/04/27/venture-capital-obfuscating-the-opportunities-for-value-exchange-at-the-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/27/existing-processes-and-culture-grinding-down-new-api-efforts/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64_176_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/27/existing-processes-and-culture-grinding-down-new-api-efforts/">Existing Processes And Culture Grinding Down New API Efforts</a></h3>
			<p><em>27 Apr 2018</em></p>
			<p>I’m always amazed what large organizations can achieve. They definitely have more resources, more momentum, and do way more than any single individual can achieve. However, as an individual actor I’m always also amazed at how large organization culture seems to always work to defend itself from change, and actively work to grind down API efforts I’m involved in. Even before I walk in the front door there are processes in place that work to ensure any effective API strategy gets ground down so that it won’t be as effective. NDAs, intellectual property, background checks, schedules, logistics, leadership buy-in, physical security, legacy incidents, software, processes all seem to begin doing their work. Grinding down on anything new. I would ask folks to leave their baggage at home, but in most cases they aren’t even aware of the number, or the size of the suitcases they travel with. This is how things are done! This is the way things are! This is our standard way of operating! Even when we just want to have a conversation. Even when I’m just looking to share some knowledge with you and your team. I haven’t even begun talking about getting paid, I just want to share some wisdom with y’all, and help your team figure all of this out. The machine isn’t setup for this time of knowledge sharing. This is why API can be so hard to achieve for some. The machine is setup to extract, aggregate, store, and control value–not share it. So when the machine encounters another entity that just wants to share, no string attached, it just doesn’t want it was designed to do. Even though many have made the conscious effort to open up doors and invite people in to share knowledge, innovate, and explore, the vacuum already in place will often consume anyone involved, sucking the oxygen out of the room, and oxidizing anything new, pulling nutrients and value into the central system....[<a href="/2018/04/27/existing-processes-and-culture-grinding-down-new-api-efforts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/27/balancing-virtual-api-evangelism-with-inperson-api-evangelism/"><img src="https://s3.amazonaws.com/kinlane-productions2/IMG_6067.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/27/balancing-virtual-api-evangelism-with-inperson-api-evangelism/">Balancing Virtual API Evangelism With In-Person API Evangelism</a></h3>
			<p><em>27 Apr 2018</em></p>
			<p>I haven’t published any stories this week on API Evangelist. I’ve been on the road in Lyon and Paris, France. Giving talks, and conducting workshops about my API lifecycle and governance work. Often times when I go on the road I try to pre-populate the blog with stories, but I’ve been so busy lately with travel and projects that I just didn’t have the time. Resulting in the blog not resembling its usual stream of API rants and stories. While this bothers me, I understand the balance between virtual API evangelism and the need to be present in-person from time to time. While I feel like I can reach more people virtually, I feel like at least 30% of the time I should be present in-person. It helps re-enforce what I know, and allows me to evolve my work, and learn knew things by exercising my API knowledge on the ground in the trenches within existing organizations, and at conferences, Meetups, and workshops. While exhausting, and often costly, in-person gatherings help build relationships, allowing me to establish deeper connections with people. Helping my work penetrate the thick bubbles that exist around us in our personal and professional lives. I’m always exhausted after traveling and shaking so many hands, but ultimately it is worth it if I do it in a thoughtful and logical way. In-person experiences are valuable, but I still feel that consistent, smart, and a syndicated virtual experience can reach more people, and make a more significant impact. It costs a lot less that traveling and attending conferences too. A robust presence isn’t easy to setup, and takes time to establish, but once in motion it can be the most effective way to build an audience. After eight years of doing API Evangelist, with some of it exclusively operating on the road, I can say that the sustained virtual presence will have the biggest impact, and provide a much more evergreen exposure that...[<a href="/2018/04/27/balancing-virtual-api-evangelism-with-inperson-api-evangelism/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/20/delivering-large-api-responses-as-efficiently-as-possible/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/carryload_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/20/delivering-large-api-responses-as-efficiently-as-possible/">Delivering Large API Responses As Efficiently As Possible</a></h3>
			<p><em>20 Apr 2018</em></p>
			<p>I’m participating in a meeting today where one of the agenda items will be discussing the different ways in which the team can deal with increasingly large API responses. I don’t feel there is a perfect response to this question, and the answer should depend on a variety of considerations, but I wanted to think through some of the possibilities, and make sure the answers were on the tip of my tongue. It helps to exercise these things regularly in my storytelling so when I need to recall them, they are just beneath the surface, ready to bring forward. Reduce Size Pagination I’d say the most common approach to send over large amounts of data is to break things down into smaller chunks, based upon rows being sent over, and paginate the responses. Sending over a default amount (ie. 10,25,100), and require consumers either ask for larger amount, as well as request each additional page of results in a separate API request. Providing consumers with a running count of how many pages, what the current page is, and the ability to paginate forward and backward through the pages with each request. It is common to send pagination parameters through the query parameters, but some providers prefer handle it through headers (ie. Github). Organizing Using Hypermedia Another approach, which usually augments and extends pagination is using common hypermedia formats for messaging. To paginate results, many API providers use hypermedia media types as a message format, because the media types allow for easy linking to paginate results, as well as providing the relevant parameters in the body of the response. Additionally, hypermedia would further allow you to intelligently break down large responses into different collections, beyond just simple pagination. Then use the linking that is native to hypermedia to provide meaningful links with relations to the different collections of potential responses. Allowing API consumers to obtain all, or just the portions of information they are looking for....[<a href="/2018/04/20/delivering-large-api-responses-as-efficiently-as-possible/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/19/machine-readable-api-regions-for-use-at-discovery-and-runtime/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/aws-cloud-regions.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/19/machine-readable-api-regions-for-use-at-discovery-and-runtime/">Machine Readable API Regions For Use At Discovery And Runtime</a></h3>
			<p><em>19 Apr 2018</em></p>
			<p>I wrote about Werner Vogel of Amazon’s post considering the impact of cloud regions a couple weeks back. I feel that his post captured an aspect of doing business in the cloud that isn’t discussed enough, and one that will continue to drive not just the business of APIs, but also increasingly the politics of APIs. Amidst increasing digital nationalism, and growing regulation of not just the pipes, but also platforms, understanding where your APIs are operating, and what networks you are using will become very important to doing business at a global scale. It is an area I’m adding to my list of machine readable API definitions I’d like to add to the APIs.json stack. The goal with APIs.json is to provide a single index where we can link to all the essential building blocks of a APIs operations, with OpenAPI being the first URI, which provides a machine readable definition of the surface area of the APIs. Shortly after establishing the APIs.json specification, we also created API Commons, which is designed to be a machine readable specification for describing the licensing applied to an API, in response to the Oracle v Google API copyright case. Beyond that, there hasn’t been many other machine readable resources, beyond some existing API driven solutions used as part of API operations like Github and Twitter. There are other API definitions like Postman Collections and API Blueprint that I reference, but they are in the same silo as OpenAPI operates within. Most of the resources we link to are still human-centered URLs like documentation, pricing, terms of service, support, and other essential building blocks of API operations. However, the goal is to evolve as many of these as possible towards being more machine readable. I’d like to see pricing, terms of services, and aspects of support become machine readable, allowing them to become more automated and understood not just at discovery, but also at runtime. I’m envisioning that...[<a href="/2018/04/19/machine-readable-api-regions-for-use-at-discovery-and-runtime/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/18/rest-and-grpc-side-by-side-in-new-google-endpoints-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions2/google-cloud-platform/google-cloud-endpoints-portal-docs.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/18/rest-and-grpc-side-by-side-in-new-google-endpoints-documentation/">REST and gRPC Side by Side In New Google Endpoints Documentation</a></h3>
			<p><em>18 Apr 2018</em></p>
			<p>Google has been really moving forward with their development, and storytelling around gRPC. Their high speed to approach to doing APIs that uses HTTP/2 as a transport, and protocol buffers (ProtoBuf) as its serialized message format. Even with all this motion forward they aren’t leaving everyone doing basic web APIs behind, and are actively supporting both approaches across all new Google APIs, as well as in their services and tooling for deploying APIs in the Google Cloud–supporting two-speed APIs side by side, across their platform. When you are using Google Cloud Endpoints to deploy and manage your APIs, you can choose to offer a more RESTful edition, as well as a more advanced gRPC edition. They’ve continued to support this approach across their service features, and tooling, by now also documenting your APIs. As part of their rollout of a supporting API portal and documentation for your Google Cloud Endpoints, you can automatically document both flavors of your APIs. Making a strong case for considering to offer both types of APIs, depending on the types of use cases you are looking to solve, and the types of developers you are catering to. In my experience, simpler web APIs are ideal for people just getting going on their API journey, and will accommodate the evolution of 60-75% of the API deployment needs out there. Where some organizations further along in their API journey, and those providing B2B solutions, will potentially need higher performance, higher volume, gRPC APIs. Making what Google is offering with their cloud API infrastructure a pretty compelling option for helping mature API providers shift gears, or even helping folks understand that they’ll be able to shift gears down the road. You get an API deployment and management solution that simultaneously supports both speeds, but also the other supporting features, services, and tooling like documentation delivers at both speeds. Normally I am pretty skeptical of single provider / community approaches to delivering alternative approaches...[<a href="/2018/04/18/rest-and-grpc-side-by-side-in-new-google-endpoints-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/18/openapi-makes-me-feel-like-i-have-a-handle-on-what-an-api-does/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/OpenAPI_Pantone.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/18/openapi-makes-me-feel-like-i-have-a-handle-on-what-an-api-does/">OpenAPI Makes Me Feel Like I Have A Handle On What An API Does</a></h3>
			<p><em>18 Apr 2018</em></p>
			<p>APIs are hard to talk about across large groups of people, while ensuring everyone is on the page. APIs don’t have much a visual side to them, providing a tangible reference for everyone to use by default. This is where OpenAPI comes in, helping us “see” an API, and establish a human and machine readable document that we can produce, pass around, and use as a reference to what an API does. OpenAPI makes me feel like I have a handle on what an API does, in a way that can actually have a conversation around with other people–without it, things are much fuzzier. Many folks associate OpenAPI with documentation, code generation, or some other tooling or service that uses the specification–putting their emphasis on the tangible thing, over the definition. While working on projects, I spend a lot of time educating folks about what OpenAPI is, what it is not, and how it can facilitate communication across teams and API stakeholders. While this work can be time consuming, and a little frustrating sometimes, it is worth it. A little education, and OpenAPI adoption can go a long way to moving projects along, because (almost) everyone involved is able to be actively involved in moving API operations forward. Without OpenAPI it is hard to consistently design API paths, as well as articulate the headers, parameters, status codes, and responses being applied across many APIs, and teams. If I ask, “are we using the sort parameter across APIs?” If there is no OpenAPI, I can’t get an immediate or timely answer, it is something that might not be reliably answered. Making OpenAPI a pretty critical conversation and collaboration driver across the API projects I’m working on. I am not even getting to the part where we are deploying, managing, documenting, or testing APIs. I’m just talking about APIs in general, and making sure everyone involved in a meeting is on the same page when we are...[<a href="/2018/04/18/openapi-makes-me-feel-like-i-have-a-handle-on-what-an-api-does/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/17/my-response-to-the-va-microconsulting-work-statement-on-api-outreach/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/state-2017/kin-lane-presidential-innovation-fellow.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/17/my-response-to-the-va-microconsulting-work-statement-on-api-outreach/">My Response To The VA Microconsulting Work Statement On API Outreach</a></h3>
			<p><em>17 Apr 2018</em></p>
			<p>The Department of Veterans Affairs (VA) is listening to my advice around how to execute their API strategy and adopting a micro-approach to not just delivering services, but also to the business of moving the platform forward at the federal agency. I’ve responded to round one, and round two of the RFI’s, and now they have submitted a handful of work statements on Github, so I wanted to provide an official response, share my thoughts on each of the work statements, and actually bid for the work. First, Here is The VA Background The Lighthouse program is moving VA towards an Application Programming Interface (API) first driven digital enterprise, which will establish the next generation open management platform for Veterans and accelerate transformation in VA’s core functions, such as Health, Benefits, Burial and Memorials. This platform will be a system for designing, developing, publishing, operating, monitoring, analyzing, iterating and optimizing VA’s API ecosystem. Next, What The VA Considers The Play As the Lighthouse Product Owner, I must have a repeatable process to communicate with internal and external stakeholders the availability of existing, new, and future APIs so that the information can be consumed for the benefit of the Veteran. This outreach/evangelism effort may be different depending on the API type. Then, What The VA Considers The Deliverable A defined and repeatable strategy/process to evangelize existing, new, and future APIs to VA’s stakeholders as products. This may be in the form of charts, graphics, narrative, or combination thereof. Ultimately, VA wants the best format to accurately communicate the process/strategy. This strategy/process may be unique to each type of API. Here Is My Official Response To The Statement API Evangelism is always something that is more about people, than it is about the technology, and should always be something that speaks to not just developers, being inclusive to all stakeholders involved in, and being served by a platform. Evangelism is all about striking the right balance around...[<a href="/2018/04/17/my-response-to-the-va-microconsulting-work-statement-on-api-outreach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/17/my-response-to-the-va-microconsulting-work-statement-on-api-landscape-analysis/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/state-2017/kin-lane-presidential-innovation-fellow.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/17/my-response-to-the-va-microconsulting-work-statement-on-api-landscape-analysis/">My Response To The VA Microconsulting Work Statement On API Landscape Analysis</a></h3>
			<p><em>17 Apr 2018</em></p>
			<p>The Department of Veterans Affairs (VA) is listening to my advice around how to execute their API strategy and adopting a micro-approach to not just delivering services, but also to the business of moving the platform forward at the federal agency. I’ve responded to round one, and round two of the RFI’s, and now they have submitted a handful of work statements on Github, so I wanted to provide an official response, share my thoughts on each of the work statements, and actually bid for the work. First, Here is The VA Background The Lighthouse program is moving VA towards an Application Programming Interface (API) first driven digital enterprise, which will establish the next generation open management platform for Veterans and accelerate transformation in VA’s core functions, such as Health, Benefits, Burial and Memorials. This platform will be a system for designing, developing, publishing, operating, monitoring, analyzing, iterating and optimizing VA’s API ecosystem. Next, What The VA Considers The Play As the VA Lighthouse Product Owner I have identified two repositories of APIs and publicly available datasets containing information with varying degrees of usefulness to the public. If there are other publicly facing datasets or APIs that would be useful they can be included. I need an evaluation performed to identify and roadmap the five best candidates for public facing APIs. The two known sources of data are: Data.gov - https://catalog.data.gov/organization/va-gov and www.va.gov/data Vets.gov - https://github.com/department-of-veterans-affairs/vets-api Then, What The VA Considers The Deliverable Rubric/logical analysis for determining prioritization of APIs Detail surrounding how the top five were prioritized based on the rubric/analysis (Not to exceed 2 pages unless approved by the Government) Deliverables shall be submitted to VA’s GitHub Repo. Here Is My Official Response To The Statement To describe my approach to this work statement, and describe what I’ve been already doing in this area for the last five years, I’m going to go with a more logical analysis, as opposed a rubric. There...[<a href="/2018/04/17/my-response-to-the-va-microconsulting-work-statement-on-api-landscape-analysis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/16/my-graphql-thoughts-after-almost-two-years/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/16_38_600_500_0_max_1_0_-2.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/16/my-graphql-thoughts-after-almost-two-years/">My GraphQL Thoughts After Almost Two Years</a></h3>
			<p><em>16 Apr 2018</em></p>
			<p>It has been almost a year and half since I first wrote my post questioning that GraphQL folks didn’t want to do the hard work of API design, which I also clarified that I was keeping my mind open regarding the approach to delivering APIs. I’ve covered several GraphQL implementations since then, as well as my post on waiting the GraphQL assault out-to which I received a stupid amount of, “dude you just don’t get it!”, and “why you gotta be so mean?” responses. GraphQL Is A Tool In My Toolbox I’ll start with my official stance on GraphQL. It is a tool in my API toolbox. When evaluating projects, and making recommendations regarding what technology to use, it exists alongside REST, Hypermedia, gRPC, Server-Sent Events (SSE), Websockets, Kafka, and other tools. I’m actively discussing the options with my clients, helping them understand the pros and cons of each approach, and working to help them define their client use cases, and when it makes sense I’m recommending a query language layer like GraphQL. When there are a large number of data points, and a knowledgeable, known group of developers being targeted for building web, mobile, and other client applications, it can make sense. I’m finding GraphQL to be a great option to augment a full stack of web APIs, and in many cases a streaming API, and other event-driven architectural considerations. GraphQL Does Not Replace REST Despite what many of the pundits and startups would like to be able to convince everyone of, GraphQL will not replace REST. Sorry, it doesn’t reach as wide of an audience as REST does, and it still keeps APIs in the database, and data literate club. It does make some very complex things simpler, but it also makes some very simple things more complex. I’ve reviewed almost 5 brand new GraphQL APIs lately where the on-boarding time was in the 1-2 hour range, rather than minutes with many other...[<a href="/2018/04/16/my-graphql-thoughts-after-almost-two-years/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/16/if-github-pages-could-be-turned-on-by-default-i-could-provide-a-run-on-github-button/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/run-on-github-button.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/16/if-github-pages-could-be-turned-on-by-default-i-could-provide-a-run-on-github-button/">If Github Pages Could Be Turned On By Default I Could Provide A Run On Github Button</a></h3>
			<p><em>16 Apr 2018</em></p>
			<p>My world runs on Gitub. 100% of my public website projects run on Github Pages, and about 75% of my public web applications run on Gitub Pages. The remaining 25% of it all is my API infrastructure. However, I’m increasingly pushing my data and content APIs to run entirely on Github with Github Pages as frontend, and the Github repo as the backend, with the Github API as the transport. I’d rather be serving up static JSON and YAML from repositories, and building JavaScript web applications that run using Jekyll, than dynamic server-side web applications. It is pretty straightforward to engineer HTML, CSS, and JavaScript applications that run entirely on Github Pages, and leverage JSON and YAML stored in the underlying Github repo as the database backend, using the Github API as the API backend. Your Github OAuth token becomes your API key, and Github organization and user structure becomes the authentication and access management layer. These apps can run publicly, and when someone wants to write, as well as read data to the application, they just need to authenticate using Github–if they have permission, then they get access, and can write JSON or YAML data to the underlying repo via the API. I’m a big fan of building portable micro applications using this approach. The only road block for me to make them easier to use by other users, is the ability to fork them into their own Github account with a single click. I have a Github OAuth token for any authenticated user, and with the right scopes I can easily fork a self-contained web application into their account or organization, giving them full control over their own version of the application. The problem is that each user has to enable Github Pages for the repository before they can view the application. It is a simple, but pretty significant hurdle for many users, and is something that prevents me from developing more applications...[<a href="/2018/04/16/if-github-pages-could-be-turned-on-by-default-i-could-provide-a-run-on-github-button/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/10/sharing-api-definition-patterns-across-teams-by-default/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/carryload_smoking_cigarette.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/10/sharing-api-definition-patterns-across-teams-by-default/">Sharing API Definition Patterns Across Teams By Default</a></h3>
			<p><em>10 Apr 2018</em></p>
			<p>This is a story, in a series I’m doing as part of the version 3.0 release of the Stoplight.io platform. Stoplight.io is one the few API service providers I’m excited about what when it comes to what they are delivering in the API space, so I jumped at the opportunity to do some paid work for them. As I do, I’m working to make these stories about the solutions the provide, and refrain from focusing on just their product, hopefully maintaining my independent stance as the API Evangelist, and keeping some of the credibility I’ve established. One of the things I’m seeing emerge from the API providers who are further along in their API journey, is a more shared experience when it comes to not just the API design process, but in action throughout the API lifecycle. You can see this reality playing out with the API definitions, and the move from Swagger 2.0 to the OpenAPI 3.0, where there is a more modular approach to defining an API, encouraging reuse of common patterns across API paths within a single definition, as well as across many different API definitions. Expanding on how API definitions can move an API through the lifecycle, and beginning to apply that across projects and teams, and moving APIs through lifecycle much more quickly, and consistently. Emulating the Models We Know We hear a lot about healthy API design practices, following RESTful philosophies, and learning from the API design guides published by leading API providers. While all of these areas have a significant influence on what happens during the API design and development process, the most significant influence on what developers know, it is just what they are exposed to in their own experience. The models we are exposed to often become the models we employ, demonstrating the importance of adopting a shared experience when it comes to defining, designing, and putting common API models to work for us in our...[<a href="/2018/04/10/sharing-api-definition-patterns-across-teams-by-default/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/09/the-closedapi-specification/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/fence_ww2_dresden.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/09/the-closedapi-specification/">The ClosedAPI Specification</a></h3>
			<p><em>09 Apr 2018</em></p>
			<p>You’ve heard of OpenAPI, right? It is the API specification for defining the surface area of your web API, and the schema you employ–making your public API more discoverable, and consumable in a variety of tools services. OpenAPI is the API definition for documenting your API when you are just getting started with your platform, and you are looking to maximize the availability and access of your platform API(s). After you’ve acquired all the users, content, investment, and other value, ClosedAPI is the format you will want to switch to, abandoning OpenAPI, for something a little more discreet. Collect As Much Data As You Possibly Can Early on you wanted to be defining the schema for your platform using OpenAPI, and even offering up a GraphQL layer, allowing your data model to rapidly scale, adding as may data points as you possible can. You really want to just ingest any data you can get your hands on the browser, mobile phones, and any other devices you come into contact with. You can just dump it all into big data lake, and sort it out later. Adding to your platform schema when possible, and continuing to establish new data points that can be used in advertising and targeting of your platform users. Turn The Firehose On To Drive Activity Early on you wanted your APIs to be 100% open. You’ve provided a firehose to partners. You’ve made your garden hose free to EVERYONE. OpenAPI was all about providing scalable access to as many users as you can through streaming APIs, as well as lower volume transactional APIs you offer. Don’t rate limit too heavily. Just keep the APIs operating at full capacity, generating data and value for the platform. ClosedAPI is for defining your API as you begin to turn off this firehose, and begin restricting access to your garden hose APIs. You’ve built up the capacity of the platform, you really don’t need your digital...[<a href="/2018/04/09/the-closedapi-specification/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/09/i-will-be-speaking-at-the-api-conference-in-london-this-week/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-conference/api-conference-keynote-details.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/09/i-will-be-speaking-at-the-api-conference-in-london-this-week/">I Will Be Speaking At The API Conference In London This Week</a></h3>
			<p><em>09 Apr 2018</em></p>
			<p>I am hitting the road this week heading to London to speak at the API Conference. I will be giving a keynote on Thursday afternoon, and conducting an all day workshop on Friday. Both of my talks are a continuation of my API life cycle work, and pushing forward my use of a transit map to help me make sense of the API life cycle. My keynote will be covering the big picture of why I think the transit model works for making sense of complex infrastructure, and my workshop is going to get down in the weeds with it all. My keynote is titled, “Looking At The API Life Cycle Through The Lens Of A Municipal Transit System”, with the following abstract, “As we move beyond a world of using just a handful of internal and external APIs, to a reality where we operate thousands of microservices, and depend on hundreds of 3rd party APIs, modern API infrastructure begins to look as complex as a municipal transit system. Realizing that API operations is anything but a linear life cycle, let’s begin to consider that all APIs are in transit, evolving from design to deprecation, while still also existing to move our value bits and bytes from one place to another. I would like to share with you a look at how API operations can be mapped using an API Transit map, and explored, managed, and understood through the lens of a modern, Internet enabled “transit system”.” My workshop is titled, “Beyond The API Lifecycle And Beginning To Establish An API Transit System”, with the following abstract, “Come explore 100 stops along the modern API life cycle, from definition to deprecation. Taking the learnings from eight years of API industry research, extracted from the leading API management pioneers, I would like to guide you through each stop that a modern API should pass across, mapped out using a familiar transit map format, providing an API...[<a href="/2018/04/09/i-will-be-speaking-at-the-api-conference-in-london-this-week/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/09/details-of-the-api-evangelist-partner-program/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-blue-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/09/details-of-the-api-evangelist-partner-program/">Details Of The API Evangelist Partner Program</a></h3>
			<p><em>09 Apr 2018</em></p>
			<p>I’ve been retooling the partner program for API Evangelist. There are many reasons for this, and you can read the full backstory I have written a narrative for these changes to the way in which I partner. I need to make a living, and my readers are expecting me to share relevant stories from across the sector on my blog. I’m also tired of meaningless partner arrangements that never go anywhere, and I’m looking to incentivize my partners to engage with me, and the API community in an impactful way. I’ve crafted a program that I think will do that. While there will still be four logo slots available for partnership, the rest of the references to API services providers and the solutions they provide will be driven by my partner program. If you want to be involved, you need to partner with me. All that takes is to email me that you want to be involved. It doesn’t cost you a dime, all you have to do is reach out, let me know you are interested, and be willing to play the part of an active API service provider. If you are making an impact on the API space, then you’ll enjoy the following exposure across my work: API Lifecycle - Your services and tooling will be listed as part of my API lifecycle research, and displayed on the home page of my website, and across my network of sites. Short Form Storytelling - Being referenced as part of my stories when I talk about the areas of the API sector in which your services and tooling provides solutions. Long Form Storytelling - Similar to short form, when i’m writing white papers, and guides, I will use your products and services as examples to highlight the solutions I’m talking about. Story Ideas - You will have access a list of working story ideas I’m working through, and able to add to the list, as...[<a href="/2018/04/09/details-of-the-api-evangelist-partner-program/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/09/crafting-a-productive-api-industry-partner-program/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/09/crafting-a-productive-api-industry-partner-program/">Crafting A Productive API Industry Partner Program</a></h3>
			<p><em>09 Apr 2018</em></p>
			<p>I struggle with partner relationships. I’ve had a lot of partners operating API Evangelist over the years. Some good. Some not so good. And some amazing! You know who you are. It’s tough to fund what I do as the API Evangelist. It’s even harder to fund who I am as Kin Lane. I’ve revamped my approach to partnering several times now trying to find the right formula for me, my partners, and for my readers. As the partner requests pile up, and I fall short for some of my existing partners, while delivering as expected for others, it is time for me to take another crack at shaping my partner program. A Strong Streamdata.io Partnership Base A cornerstone of my new approach to partnering is based upon my relationship with Streamdata.io. They are my primary partner, and supporter of API Evangelist. They have not just helped provide me with the financial base I need to live and operate API Evangelist, they are investing in, and helping grow my existing API lifecycle and API Stack work. We are also working in concert to formalize my API lifecycle work into a growing consultancy, and pushing forward my API Stack research, syndicating it as the Streamdata.io API Gallery, and refining the my ranking system for both API providers, and API service providers. Without Streamdata.io this latest round of API Evangelist wouldn’t be happening. Difficulties With The Current Mode Of Partnering One of the biggest challenges I have right now with partnering is that my partners want me to produce content about them. Writing stories for pay just isn’t a good idea for their brand, or for mine. I know it is what people want, but it just doesn’t work. The other challenge I have is people tend to want predictable stories on a schedule. I know it seems like I’m a regular machine, churning out content, but honestly when it flows, it all works. When it doesn’t...[<a href="/2018/04/09/crafting-a-productive-api-industry-partner-program/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/05/seamless-api-lifecycle-integration-with-github-gitlab-and-bitbucket/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_161_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/05/seamless-api-lifecycle-integration-with-github-gitlab-and-bitbucket/">Seamless API Lifecycle Integration With Github, Gitlab, And BitBucket</a></h3>
			<p><em>05 Apr 2018</em></p>
			<p>This is a story, in a series of stories that I’m doing as part of the version 3.0 release of the Stoplight.io platform. Stoplight.io is one the few API service providers I’m excited about what when it comes to what they are delivering in the API space, so I jumped at the opportunity to do some paid work for them. As I do, I’m working to make these stories about the solutions the provide, and refrain from focusing on just their product, hopefully maintaining my independent stance as the API Evangelist, and keeping some of the credibility I’ve established over the years. Github, Gitlab, and Bitbucket have taken up a central role in the delivery of the valuable API resources we are using across our web, mobile, and device-based applications. These platforms have become integral parts of our development, and software build processes, with Github being the most prominent player when it comes to defining how we deliver not just applications, but increasingly API-driven applications on the web, our mobile phones, and common Internet connected objects becoming more ubiquitous across our physical worlds. A Lifecycle Directory Of Groups and Users These social coding platforms come with the ability to manage different groups, projects, as well as the users involved with moving project forward. While version control isn’t new, Github is credited with making this aspect of managing code a very social endeavor which can be done publicly, or privately within select groups. Providing a rich environment for defining who is involved with each microservice that is being moved along the API lifecycle, allowing teams to leverage the existing organization and user structure provided by these platforms as part of their API lifecycle organizational structure. Repositories As A Wrapper For Each Service Five years ago you would most likely find just code within a Github repository. In 2018, you will find schema, documentation, API definitions, and numerous artifacts stored and evolved within individual repositories. The role...[<a href="/2018/04/05/seamless-api-lifecycle-integration-with-github-gitlab-and-bitbucket/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/05/nest-branding-and-marketing-guidelines/"><img src="https://s3.amazonaws.com/kinlane-productions2/nest/nestreview-workswithnest.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/05/nest-branding-and-marketing-guidelines/">Nest Branding And Marketing Guidelines</a></h3>
			<p><em>05 Apr 2018</em></p>
			<p>I’m always looking out for examples of API providers who have invested energy into formalizing process around the business and politics of API operations. I’m hoping to aggregate a variety of approaches that I can aggregate into a single blueprint that I can use in my API storytelling and consulting. The more I can help API providers standardize what they do, the better off the API sector will be, so I’m always investing in the work that API providers should be doing, but doesn’t always get prioritized. The other day while profiling the way that Nest uses Server-Sent Events (SSE) to stream activity via thermostats, cameras, and the other devices they provide, and I stumbled across their branding policies. It provides a pretty nice set of guidance for Nest developers in respect to the platform’s brand, and something you don’t see with many other API providers. I always say that branding is the biggest concern for new API providers, but also the one that is almost never addressed by API providers who are in operation–which doesn’t make a whole lot of sense to me. If I had a major corporate brand, I’d work to protect it, and help developers understand what is important. The Nest marketing program is intended to qualify applications, and then allow them to use the “Works with Nest” branding in your marketing and social media. To get approved you have submit your product or service for review. As part of the review process verifies that you are in compliance with all of their branding policies, including: User interface guide User experience guide Trademark policy To apply for the program you have to email them with all the following details regarding the marketing efforts our your product or service where you will be using the “Works with Nest” branding: Description of your marketing program Description of your intended audience Planned communication (list all that apply): Print, Radio, TV, Digital Advertising, OOH, Event,...[<a href="/2018/04/05/nest-branding-and-marketing-guidelines/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/05/helping-stoplight-io-get-the-word-out-about-version-3-0/"><img src="https://s3.amazonaws.com/kinlane-productions2/stoplight/stoplight-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/05/helping-stoplight-io-get-the-word-out-about-version-3-0/">Helping Stoplight.io Get The Word Out About Version 3.0</a></h3>
			<p><em>05 Apr 2018</em></p>
			<p>I’ve been telling stories about what the Stoplight.io team has been building for a couple of years now. They are one of the few API service provider startups left that are doing things that interest me, and really delivering value to their API consumers. In the last couple of years, as things have consolidated, and funding cycles have shifted, there just hasn’t been the same amount of investment in interesting API solutions. So when Stoplight.io approached me to do some storytelling around their version 3.0 release, I was all in. Not just because I’m getting paid, but because they are doing interesting things, that I feel are worth talking about. I’ve always categorized Stoplight.io as an API design solution, but as they’ve iterated upon the last couple of versions, I feel they’ve managed to find their footing, and are maturing to become one of the few truly API lifecycle solutions available out there. They don’t serve every stop along the API lifecycle, but they do focus on a handful of the most valuable stops, and most importantly, they have adopted OpenAPI as the core of what they do, allowing API providers to put Stoplight.io to work for them, as well as any other solutions that support OpenAPI at the core. As far as the stops along the API lifecycle that they service, here is how I break them down: Definitions - An OpenAPI driven way of delivering APIs, that goes beyond just a single definition, and allows you to manage your API definitions at scale, across many teams, and services. Design - One of the most advanced API design GUI solutions out there, helping you craft and evolve your APIs using the GUI, or working directly with the raw JSON or YAML. Virtualization - Enabling the mocking and virtualization of your APIs, allowing you to share, consume, and iterate on your interfaces long before you have deliver more costly code. Testing - Provides the ability...[<a href="/2018/04/05/helping-stoplight-io-get-the-word-out-about-version-3-0/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/03/openapi-is-the-contract-for-your-microservice/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/OpenAPI_Pantone.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/03/openapi-is-the-contract-for-your-microservice/">OpenAPI Is The Contract For Your Microservice</a></h3>
			<p><em>03 Apr 2018</em></p>
			<p>I’ve talked about how generating an OpenAPI (fka Swagger) definition from code is still the dominate way that microservice owners are producing this artifact. This is a by-product of developers seeing it as just another JSON artifact in the pipeline, and from it being primarily used to create API documentation, often times using Swagger UI–which is also why it is still called Swagger, and not OpenAPI. I’m continuing my campaign to help the projects I’m consulting on be more successful with their overall microservices strategy by helping them better understand how they can work in concert by focus in on OpenAPI, and realizing that it is the central contract for their service. Each Service Begins With An OpenAPI Contract There is no reason that microservices should start with writing code. It is expensive, rigid, and time consuming. The contract that a service provides to clients can be hammered out using OpenAPI, and made available to consumers as a machine readable artifact (JSON or YAML), as well as visualized using documentation like Swagger UI, Redocs, and other open source tooling. This means that teams need to put down their IDE’s, and begin either handwriting their OpenAPI definitions, or being using an open source editor like Swagger Editor, Apicurio, API GUI, or even within the Postman development environment. The entire surface area of a service can be defined using OpenAPI, and then provided using mocked version of the service, with documentation for usage by UI and other application developers. All before code has to be written, making microservices development much more agile, flexible, iterative, and cost effective. Mocking Of Each Microservice To Hammer Out Contract Each OpenAPI can be used to generate a mock representation of the service using Postman, Stoplight.io, or other OpenAPI-driven mocking solution. There are a number of services, and tooling available that takes an OpenAPI, an generates a mock API, as well as the resulting data. Each service should have the ability to...[<a href="/2018/04/03/openapi-is-the-contract-for-your-microservice/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/03/github-is-your-asynchronous-microservice-showroom/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-github.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/03/github-is-your-asynchronous-microservice-showroom/">Github Is Your Asynchronous Microservice Showroom</a></h3>
			<p><em>03 Apr 2018</em></p>
			<p>When you spend time looking at a lot of microservices, across many different organizations, you really begin to get a feel for the ones who have owners / stewards that are thinking about the bigger picture. When people are just focused on what the service does, and not actually how the service will be used, the Github repos tend to be cryptic, out of sync, and don’t really tell a story about what is happening. Github is often just seen as a vehicle for the code to participate in a pipeline, and not about speaking to the rest of the humans and systems involved in the overall microservices concert that is occurring. Github Is Your Showroom Each microservices is self-contained within a Github repository, making it the showcase for the service. Remember, the service isn’t just the code, and other artifacts buried away in the folders for nobody to understand, unless you understand how to operate the service or continuously deploy the code. It is a service. The service is part of a larger suite of services, and is meant to be understood and reusable by other human beings in the future, potentially long after you are gone, and aren’t present to give a 15 minute presentation in a meeting. Github is your asynchronous microservices showroom, where ANYONE should be able to land, and understand what is happening. README Is Your Menu The README is the centerpiece of your showroom. ANYONE should be able to land on the README for your service, and immediately get up to speed on what a service does, and where it is in its lifecycle. README should not be written for other developers, it should be written for other humans. It should have a title, and short, concise, plain language description of what a service does, as well as any other relevant details about what a service delivers. The README for each service should be a snapshot of a service...[<a href="/2018/04/03/github-is-your-asynchronous-microservice-showroom/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/02/the-api-stack-profiling-checklist/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-stacks.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/02/the-api-stack-profiling-checklist/">The API Stack Profiling Checklist</a></h3>
			<p><em>02 Apr 2018</em></p>
			<p>I just finished a narrative around my API Stack profiling, telling the entire story around the profiling of APIs for inclusion in the stack. To help encourage folks to get involved, I wanted to help distill down the process into a single checklist that could be implemented by anyone. The Github Base Everything begins as a Github repository, and it can existing in any user or organization. Once ready, I can fork and publish as part of the API stack, or sync with an existing repository project. Create Repo - Create a single repository with the name of the API provider in plain language. Create README - Add a README for the project, articulating who the target is and the author. OpenAPI Definition Profiling the API surface area using OpenAPI, providing a definition of the request and response structure for all APIs. Head over to their repository if you need to learn more about OpenAPI. Ideally, there is an existing OpenAPI you can start with, or other machine readable definition you can use as base–look around within their developer portal, because sometimes you can find an existing definition to start with. Next look on Github, as you never know where there might be something existing that will save you time an energy. However you approach, I’m looking for complete details on the following: info - Provide as much information about the API. host - Provide a host, or variables describing host. basePath - Document the basePath for the API. schemes - Provide any schemes that the API uses. produces - Document which media types the API uses. paths - Detail the paths including methods, parameters, enums, responses, and tags. definitions - Provide schema definitions used in all requests and responses. To help accomplish this, I often will scrape, and use any existing artifacts I can possible find. Then you just have to roll up your sleeves and begin copying and pasting from the existing API...[<a href="/2018/04/02/the-api-stack-profiling-checklist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/02/my-api-stack-profiling-process/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-stack/logos/the-api-stack-entities-snapshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/02/my-api-stack-profiling-process/">My API Stack Profiling Process</a></h3>
			<p><em>02 Apr 2018</em></p>
			<p>I am escalating conversations I’m having with folks regarding how I profile and understand APIs as part of my API Stack work. It is something I’ve been evolving for 2-3 years, as I struggle with different ways to attack and provide solutions within the area of API discovery. I want to understand what APIs exist out there as part of my industry research, but along the way I want to also help others find the APIs they are looking for, while also helping API providers get their APIs found in the first place. All of this has lead me to establish a pretty robust process for profiling API providers, and document what they are doing. As I shift my API Stack work into 2nd gear, I need to further formalize this process, articulate to partners, and execute at scale. Here is my current snapshot of what is happening whenever I’m profiling an API and adding it to my API Stack organization, and hopefully moving it forward along this API discovery pipeline in a meaningful way. It is something I’m continuing to set into motion all by myself, but I am also looking to bring in other API service providers, API providers, and API consumers to help me realize this massive vision. Github as a Base Every new “entity” I am profiling starts with a Github repository and a README, within a single API Stack organization for providers. Right now this is all automated, as I’m transition to this way of doing things at scale. Once in motion, each provider I’m profiling will become pretty manual. What I consider an “entity” is fluid. Most smaller API providers have a single repository, while other larger ones are broken down by line of business, to help reduce API definitions and conversation around APIs down to the smallest possible unit we can. Every API service provider I’m profiling gets its own Github repository, subdomain, set of definitions, and conversation...[<a href="/2018/04/02/my-api-stack-profiling-process/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/04/02/an-openapi-service-dependency-vendor-extension/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/71_113_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/04/02/an-openapi-service-dependency-vendor-extension/">An OpenAPI Service Dependency Vendor Extension</a></h3>
			<p><em>02 Apr 2018</em></p>
			<p>I’m working on a healthcare related microservice project, and I’m looking for a way to help developers express their service dependencies within the OpenAPI or some other artifact. At this point I’m feeling like the OpenAPI is the place to articulate this, adding a vendor extension to the specification that can allow for the referencing of one or more other services any particular service is dependent on. Helping make service discovery more machine readable at discovery and runtime. To help not reinvent the wheel, I am looking at using the Schema.org Web API type including the extensions put forth by Mike Ralphson and team. I’d like the x-api-dependencies collection to adopt a standardized schema, that was flexible enough to reference different types of other services. I’d like to see the following elements be present for each dependency: versions (OPTIONAL array of thing -&gt; Property -&gt; softwareVersion). It is RECOMMENDED that APIs be versioned using [semver] entryPoints (OPTIONAL array of Thing -&gt; Intangible -&gt; EntryPoint) license (OPTIONAL, CreativeWork or URL) - the license for the design/signature of the API transport (enumerated Text: HTTP, HTTPS, SMTP, MQTT, WS, WSS etc)&lt;/p&gt; apiProtocol (OPTIONAL, enumerated Text: SOAP, GraphQL, gRPC, Hydra, JSON API, XML-RPC, JSON-RPC etc) webApiDefinitions (OPTIONAL array of EntryPoints) containing links to machine-readable API definitions webApiActions (OPTIONAL array of potential Actions) Using the Schema.org Web type would allow for a pretty robust way to reference dependencies between services in a machine readable way, that can be indexed, and even visualized in services and tooling. When it comes to evolving and moving forward services, having dependency details baked in by default make a lot of sense, and ideally each dependency definition would have all the details of the dependency, as well as potential contact information, to make sure everyone is connected regarding the service road map. Anytime a service is being deprecated, versioned, or impacted in any way, we have all the dependencies needed to make an educated decision...[<a href="/2018/04/02/an-openapi-service-dependency-vendor-extension/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/30/using-postman-to-explore-the-triathlon-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/triathalon/w2WBMG2bTD6TahNs2MG3_150919-chicago-elite-m-web-msj-43.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/30/using-postman-to-explore-the-triathlon-api/">Using Postman to Explore the Triathlon API</a></h3>
			<p><em>30 Mar 2018</em></p>
			<p>I’m taking time to showcase any API I come across who have published their OpenAPI definitions to Github like New York Times, Box, Stripe, SendGrid, Nexmo, and others have. I’m also taking the time to publish stories showcasing any API provider who similarly publish Postman Collections as part of their API documentation. Next up on my list is the Triathlon API, who provides a pretty sophisticated API stack for managing triathlons around the world, complete with a list of Postman Collections for exploring and getting up and running with their API. Much like Okta, which I wrote about last week, the Triathlon API has broken their Postman Collections into individual service collections, and provides a nice list of them for easy access. Making it quick and easy to get up and running making calls to the API. Something that ALL API providers should be doing. Sorry, but Postman Collections should be a default part of your API documentation, just like OpenAPI definition should be as the driver of your interactive API docs, and the rest of your API lifecycle. Every provider should be maintaining their OpenAPI definitions, as well as Postman Collections on Github, and baking them into their API documentation. Your OpenAPI should be the central truth for your API operations, and then you can easily import it, and generate Postman Collections as you design, test, and evolve your API using the Postman development suite. I know there are many API providers who haven’t caught up to this approach to delivering API resources, but it is something they need to tune into, and make the necessary shift in how you are delivering your resources. In addition to regular stories like this on the blog, you will find me reaching out to individual API providers asking if they have an OpenAPI and / or Postman Collections. I’m personally invested in getting API providers to adopt their API definition formats. I want to see their APIs...[<a href="/2018/03/30/using-postman-to-explore-the-triathlon-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/30/the-openapi-3-0-gui/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi-gui/openapi-gui-dashboard.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/30/the-openapi-3-0-gui/">The OpenAPI 3.0 GUI</a></h3>
			<p><em>30 Mar 2018</em></p>
			<p>I have been editing my OpenAPI definitions manually for some time now, as I just haven’t found a GUI editor that works well with my workflow. Swagger editor really isn’t a GUI solution, and while I enjoy services like Stoplight.io, APIMATIC, and others, I’m very picky about what I adopt within my world. One aspect of this is that I’ve been holding out for a solution that I can run 100% on Github using Github Pages. I’ve delusionally thought that someday I would craft a slick JavaScript solution that I could run using only Github Pages, but in reality I’ve never had the time to pull together. So, I just kept manually editing my OpenAPI definitions on the desktop using Atom, and publish to Github as I needed. Well, now I can stop being so dumb, because my friend Mike Ralphson (@permittedsoc) has created my new favorite OpenAPI GUI, that is OpenAPI 3.0 compliant by default. As Mike defines it, “OpenAPI-GUI is a GUI for creating and editing OpenAPI version 3.0.x JSON/YAML definitions. In its current form it is most useful as a tool for starting off and editing simple OpenAPI definitions. Imported OpenAPI 2.0 definitions are automatically converted to v3.0.” He provides a pretty simple way to get up and running with OpenAPI GUI because, “OpenAPI-GUI runs entirely client-side using a number of Javascript frameworks including Vue.JS, jQuery and Bulma for CSS. To get the app up and running just browse to the live version on GitHub pages, deploy a clone to GitHub pages, deploy to Heroku using the button below, or clone the repo and point a browser at index.html or host it yourself - couldn’t be simpler.” – “You only need to npm install the Node.js modules if you wish to use the openapi-gui embedded web server (i.e. not if you are running your own web-server), otherwise they are only there for PaaS deployments.” Portable, Forkable API Design GUI The forkability of...[<a href="/2018/03/30/the-openapi-3-0-gui/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/30/obtaining-a-scalable-api-definition-driven-api-design-workflow-on-github/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/G0520270_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/30/obtaining-a-scalable-api-definition-driven-api-design-workflow-on-github/">Obtaining A Scalable API Definition Driven API Design Workflow On Github</a></h3>
			<p><em>30 Mar 2018</em></p>
			<p>There is a lot of talk of an API design first way of doing things when it comes to delivering microservices. I’m seeing a lot of organizations make significant strides towards truly decoupling how they deliver the APIs they depend on, and continue to streamline their API lifecycle, as well as governance. However, if you are down in the weeds, doing this work at scale, you know how truly hard it is to actualy get everything working in concert across many different teams. While I feel like I have many of the answers, actually achieving this reality, and assembling the right tools and services for the job is much harder then it is in theory. There was a question posted on API Craft recently that I think gets at the challenges we all face: We plan to use Open API 3 specification for designing API’s that are required to build our enterprise web application. These API’s are being developed to integrate the backend with frontend. They are initially planned to be internal/private. To roll out an API First strategy across multiple teams (~ 30) in our organization we want to recommend and centrally deploy a standard set of tools that could be used by teams to design and document API’s. I am new to the swagger tool set. I understand that there is a swagger-editor tool that can help in API design while swagger-ui could help in API documentation. Trying them I realized a few problems 1. How would teams save their API’s centrally on a server? Swagger editor does not provide a way to centrally store them. 2. How can we get a directory look up that displays all the designed API’s? 3. How can we integrate the API design and API documentation tool? 4. How can the API specifications be linked with the implementation (java) to keep them up-to-date? 5. How do we show API dependencies when one api uses the other one?...[<a href="/2018/03/30/obtaining-a-scalable-api-definition-driven-api-design-workflow-on-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/29/the-impact-of-availability-zones-regions-and-api-deployment-around-the-globe/"><img src="https://s3.amazonaws.com/kinlane-productions2/azure/azure-regions-map.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/29/the-impact-of-availability-zones-regions-and-api-deployment-around-the-globe/">The Impact Of Availability Zones, Regions, And API Deployment Around The Globe</a></h3>
			<p><em>29 Mar 2018</em></p>
			<p>Werner Vogels shared a great story looking back at 10 years of compartmentalization at AWS, where he talks about the impact Amazon has made on the landscape by allowing for the deployment of resources into different cloud regions, zones, and jurisdictions. I agree with him regarding the significant impact this has had on how we deliver infrastructure, and honestly isn’t something that gets as much recognition and discussion as it should. I think this is partly due to the fact that many companies, organizations, institutions, and governments are still making their way to the cloud, and aren’t far enough in their journeys to be able to sufficiently take advantage of the different availability zones. In Werner’s piece he focuses on the availability, scalability, and redundancy benefits of operating in different zones. Which I think gets at the technical benefits of this benefit of operating infrastructure in the cloud, but there are also significant business, and even political considerations at play here. As the web matures, the business and political implications of being able to operate precisely within a specific region, zone, and jurisdiction is becoming increasingly important. Sure, you want your API infrastructure to be reliable, redundant, and failover when there has been an outage in a specific region, but increasingly clients are asking for APIs to be delivered close to where business occurs, and regulatory bodies are beginning to mandate that digital business gets done within specific borders as well. Regions have become a top level priority for Amazon, Azure, and Google. Clearly, they are also becoming a top level priority for their customers who operate within their clouds. It is one of those things I notice evolving across the technology landscape and have felt the need to pay attention to more as I see more activity and chatter. I’ve begun documenting which regions each of the cloud providers are operating in, and have been increasing the number of stories I’m writing about the...[<a href="/2018/03/29/the-impact-of-availability-zones-regions-and-api-deployment-around-the-globe/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/28/sharing-the-apis-json-vision-again/"><img src="https://s3.amazonaws.com/kinlane-productions2/apis-json/apis-json-home-page-2018.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/28/sharing-the-apis-json-vision-again/">Sharing The APIs.json Vision Again</a></h3>
			<p><em>28 Mar 2018</em></p>
			<p>I know many folks in the API sector don’t know about APIs.json, and if they do they often think it is yet another API definition format, competing wit OpenAPI, Postman Collections, and others. So, I want to take a moment to share the vision again, and maybe convert one or two more folks to the possibilities around having a machine readable format for the entire operations. This is where APIs.json elevates the conversation, is it isn’t just about defining an API, it is about defining API operations, looking to make things more discoverable, and executable by default. APIs.json is a JSON format, as the name implies, but admittedly a bad name, as I’ve already started created APIs.yaml editions, which defines the entire API operations, not just any single API. It starts with the basics of the API providers, with: name, description, image, tags, created, and modified. Then it has a collection for defining one or many actual APIs. Repeating the need for a name, description, image, and tags, but this time for each API–admittedly this might be redundant in many cases. Each API also has a humanURL and baseURL for each API, providing two key links that developers will need. After that, we start getting to the meat of APIs.json, with the properties collection. The properties collection is an array of URLs and types, which can point to many different building blocks of API operations. Common elements that should be present with every API provider like: Documentation - URL to the API documentation. Plans &amp; Pricing - Landing page for the API tiers. Terms of Service - Where do you find the legal department. Sign Up - How do you sign up for an API. These elements are often times meant for humans, but the goal of APIs.json is to also provide machine readable URLs of common building blocks, including: OpenAPI - Where do you find the OpenAPI definition? Postman Collection - Is there a...[<a href="/2018/03/28/sharing-the-apis-json-vision-again/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/28/data-driven-apps-at-scale-using-github-and-jekyll/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/43_113_800_500_0_max_0_-5_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/28/data-driven-apps-at-scale-using-github-and-jekyll/">Data Driven Apps At Scale Using Github And Jekyll</a></h3>
			<p><em>28 Mar 2018</em></p>
			<p>I’ve pushed my Github driven, Jekyll fueled, application delivery process to a new levels the last couple of weeks. I’ve been investing more cycles into my API Stack work, as I build out an API Gallery for Streamdata.io. All of my work as the API Evangelist has long lived as independent Github repositories, with a JSON or YAML core that drives the static Jekyll UI for my websites, applications, and API developer portals. Designed to be independent, data and API driven micro-solutions to some related aspect of my API research. I’m just turning up the volume on what already exists. I’m taking this to the next level with my approach to API discovery. API Stack used to be a single Github repository with huge number of APIs.json and OpenAPI definitions, but after hitting the ceiling with the size of the repository, I’ve begun to shard projects by entity, topic, and collection. So far I have 285 entities, with 9850 API paths, spanning 397 topics. Each entity, and topic exists within its own Github repository, acting as an individual set of API definitions, including an OpenAPI, Postman Collection, and APIs.json index. Allowing me to move forward each API definition for an entity independently, and begin aggregating APIs into interesting collections by topic, industry, or other relevant area. There is no backend database for these projects. Well, there is a central system I use to help me orchestrate things, but each project is standalone, with everything it needs stored in the Github repository. It is something that took some engineering, and time to setup and first, but now that it is setup, managing it is actually pretty easy. The approach has also allowing me to scale it without any additional performance issues. Each entity I add becomes its own repo, and unless I hit some magic number for the number of repositories I can have within an organization, I should be able to scale to several thousand...[<a href="/2018/03/28/data-driven-apps-at-scale-using-github-and-jekyll/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/28/a-decade-later-twitters-api-plan-slowly-begins-to-take-shape/"><img src="https://s3.amazonaws.com/kinlane-productions2/twitter/twitter-bird-blue-on-white.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/28/a-decade-later-twitters-api-plan-slowly-begins-to-take-shape/">A Decade Later Twitters API Plan Slowly Begins To Take Shape</a></h3>
			<p><em>28 Mar 2018</em></p>
			<p>It’s been a long, long road for Twitter when it comes to realizing the importance of having a plan when it comes to their API management strategy. Aside from monetizing the firehose through former parter, and now acquired solution Gnip, Twitter has never had any sort of plan when it came to providing access tiers for their API. I try to revisit the Twitter developer portal a couple times a year, but I’m going to have to increase the number of visits as there seems to be more rapid shifts towards getting control over their API management layer in recent months. The API plan matrix that has emerged as part of Twitter’s pricing page provides a view of the unfolding plan for the social media platform. The Twitter API plan doesn’t show the entire scope, as it doesn’t cover the streaming layer of the API, but it does provide an important first step towards bringing coherence to how developers can access the API, and pay for premium levels of access. This conversation isn’t just about Twitter making money or Twitter charging for access to our data, this is about Twitter taking control over who has access to our data, and the platform. It is up to Twitter whether or not they focus on revenue, or the needs of end-users, something that we will only realize as their API plan continues to unfold. This is the layer where Twitter will begin to reign in bots, and other malicious activity, but it is something that won’t be easy, and undoubtedly cause a lot of pain and frustration for developers, and end-users. It is something they should have done a long time ago, but as we are seeing play out with Facebook, to incentivize the growth their investors and shareholders desired, these platform needed to look the other way at the API management layer. To achieve the eyeballs, clicks, and engagements they desired, it required them to ignore...[<a href="/2018/03/28/a-decade-later-twitters-api-plan-slowly-begins-to-take-shape/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/27/sharing-your-api-first-principles/"><img src="https://s3.amazonaws.com/kinlane-productions2/zalando/zalando-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/27/sharing-your-api-first-principles/">Sharing Your API First Principles</a></h3>
			<p><em>27 Mar 2018</em></p>
			<p>I’ve been publishing regular posts from the API design guides of API providers I’ve been studying. API providers who publish their API design guides tends to be further along in their API journey. These API providers tend to have more experience and insight, and are often worth studying further, and learning from. I’ve bee getting a wealth of valuable information from the German fashion and technology company Zalando, who has shared some pretty valuable API first principles. In a nutshell Zalando’s API First principles focuses on two areas: define APIs outside the code first using a standard specification language get early review feedback from peers and client developers By defining APIs outside the code, Zalando wants to facilitate early review feedback and also a development discipline that focus service interface design on: profound understanding of the domain and required functionality generalized business entities / resources, i.e. avoidance of use case specific APIs clear separation of WHAT vs. HOW concerns, i.e. abstraction from implementation aspects — APIs should be stable even if we replace complete service implementation including its underlying technology stack Moreover, API definitions with standardized specification format also facilitate: single source of truth for the API specification; it is a crucial part of a contract between service provider and client users infrastructure tooling for API discovery, API GUIs, API documents, automated quality checks Their guidance continues by stating: An element of API rirst are also a review process to get early review feedback from peers and client developers. Peer review is important for us to get high quality APIs, to enable architectural and design alignment and to supported development of client applications decoupled from service provider engineering life cycle. It is important to learn, that API First is not in conflict with the agile development principles that we love. Service applications should evolve incrementally — and so its APIs. Of course, our API specification will and should evolve iteratively in different cycles; however, each...[<a href="/2018/03/27/sharing-your-api-first-principles/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/27/oauth-has-many-flaws-but-it-is-the-best-we-have-at-the-moment/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/capital-battle_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/27/oauth-has-many-flaws-but-it-is-the-best-we-have-at-the-moment/">OAuth Has Many Flaws But It is The Best We Have At The Moment</a></h3>
			<p><em>27 Mar 2018</em></p>
			<p>I was enjoying the REST API Notes newsletter from my friend Matthew Reinbold (@libel_vox) today, and wanted to share my thoughts on his mention of my work while it was top of my mind. I always enjoy what Matt has to say, and regularly encourage him writing on his blog, and keep publishing his extremely thoughtful newsletter. It is important for the API sector to have many thoughtful, intelligent voices breaking down what is going on. I recommend subscribing to REST API Notes if you haven’t, you won’t regret it. Anyways, Matt replied with the following about my Facebook response: Kin Lane did a fine job identifying the mechanisms most companies already have in place to mitigate the kind of bad behavior displayed by Cambridge Analytica. It is a good starting point. However, I do want to challenge one of his assertions. Kin implies that OAuth consent is a sufficient control for folks to manage their data. While better than nothing, I maintain that most consumers are incapable of making informed decisions. It’s not a question of their intelligence. It is a question of complexity and incentives for the business to be deliberately opaque. I agree. To quote my other friend Mehdi Medjaoui (@medjawii), “OAuth is flawed, but it is the best we have”. I would say that “consumers are incapable of making informed decisions”, because we’ve crafted the world this way, and our profit margins depend on customers not being able to make informed decisions. It is how markets work things out, and “smart” people get ahead, and all that bullshit. You see this same thing playing out at the terms of service level as well. As a consumer of online services, I am regularly incapable of being able to make informed decisions around how my data is being used, in exchange for using a free web application. Is it because I’m not smart? No, it is because terms of service are purposefully confusing,...[<a href="/2018/03/27/oauth-has-many-flaws-but-it-is-the-best-we-have-at-the-moment/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/27/government-has-benefitted-from-lack-of-oversight-at-the-social-api-management/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/76_33_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/27/government-has-benefitted-from-lack-of-oversight-at-the-social-api-management/">Government Has Benefitted From Lack Of Oversight At The Social API Management</a></h3>
			<p><em>27 Mar 2018</em></p>
			<p>There are many actors who have benefitted from Facebook not properly management their API platform. Collecting so many data points, tracking so many users, and looking the other way as 3rd party developers put them to use in a variety of applications. Facebook did what is expected of them, and focused on generating advertising revenue from allowing their customers to target the many data points that are generated by the web and mobile applications developed on top of their platform. I hear a lot of complaining about this being just about democrats being upset that it was the republicans who were doing this, when in reality there are so many camps that are complicit in this game, if you are focusing in on any single camp you are probably looking to shift attention and blame from your own camp, in my opinion. I’m all for calling everyone out, even ourselves for being so fucking complicit in this game where we allow ourselves to be turned into a surveillance marketplace. Something that benefits the Mark Zuckerbergs, Cambridge Analyticas, and the surveillance capitalist platform, but also providing a pipeline to government, and law enforcement. If government is going to jump on the bandwagon calling for change at Facebook, they need to acknowledge that they’ve been benefitting from the data free for all that is the Facebook API. We’ve heard stories about 3rd party developers building applications for law enforcement to target Black Lives Matters, all the way up to Peter Thiel’s (a Facebook Investor) Palantir who actively works with NSA, and other law enforcement. If we are going to shine a light on what is happening on Facebook, let’s make it a spotlight and call out ALL of the actors who have been sucking user’s data out of the API, and better understand what they have been doing. I’m all for a complete audit of how the RNC, DNC, and EVERY other election, policing, and government entity...[<a href="/2018/03/27/government-has-benefitted-from-lack-of-oversight-at-the-social-api-management/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/27/acknowledging-that-why-i-do-apis-is-very-different-than-why-others-are-doing/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/76_16_800_500_0_max_0_1_1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/27/acknowledging-that-why-i-do-apis-is-very-different-than-why-others-are-doing/">Acknowledging That Why I Do APIs Is Very Different Than Why Others Are Doing</a></h3>
			<p><em>27 Mar 2018</em></p>
			<p>When I started doing API Evangelist in 2010 I was still very, very, very naive about why people were doing APIs. While I do not always expect everyone doing APIs to be ethical and sensible with the reasons behind doing APIs, I have been regularly surprised at how many different views there are out there regarding why we should be doing APIs in the first place. The lesson for me is to never assume that someone is doing APIs for the same reasons I am doing APIs, because rarely that will ever be the case–hopefully minimizing the chances I’ll get continue to be surprised down the road. After watching Salesforce, then Amazon, Flickr, Twitter, Facebook, and Google Maps doing APIs, I saw the potential for APIs to open up access to resources like never before. I also saw the potential for not just opening up these new opportunities to developers, but OAuth was beginning to give end-users a voice in how their data and content could be put to use. I saw the opportunity for a new type of partnership between platforms, developers, and end-users emerging that could change how we do business online, how government works, and much, much more. Sadly my white male privilege, and powers of denial prevented me from seeing the many other ways in which APIs were being seen as an opportunity. I find that the reasons people publish regarding why they are doing APIs rarely size up with the real reasons why they are actually doing APIs. Sometimes you can read between the lines by evaluating their overall business model, or lack of one. Other times you can find some telling signs present in the design of their API, as the paths, parameters, and technical details tend to tell a more accidentally honest view of what is happening behind the curtain. However, when it comes to the theater of “open APIs”, everything quickly becomes a funhouse of mirrors when...[<a href="/2018/03/27/acknowledging-that-why-i-do-apis-is-very-different-than-why-others-are-doing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/26/nexmo-manages-their-openapi-3-0-definition-using-github/"><img src="https://s3.amazonaws.com/kinlane-productions2/nexmo/nexmo-api-specifications.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/26/nexmo-manages-their-openapi-3-0-definition-using-github/">Nexmo Manages Their OpenAPI 3.0 Definition Using Github</a></h3>
			<p><em>26 Mar 2018</em></p>
			<p>I’m big on supporting API providers that publish their OpenAPI definitions to Github. It is important for the wider API community that ALL API definitions are machine readable, and available in a way that can be forked, and integrated into continuous integration pipelines. I’m not even talking about the benefits to the API providers when it comes to managing their own API lifecycle. I’m just focusing on the benefits to API consumers, and helping make on-boarding, integration, and keeping in sync with the road map as frictionless as possible. To help incentivize API providers doing this I’m committed to writing up stories for each API provider that publishes their OpenAPI, APIs.json, or Postman Collections to Github. Bonus points if you are doing it in an interesting way that further benefits your operations, as well as your community. Today’s API provider to showcase is the SMS, voice and phone verifications API provider Nexmo, who tweeted the Github repository at me, which contains their OpenAPI definition for their APIs. As they say, it is a work in progress, but it provides a damn good start for a machine readable definition for their API(s), and I mean c’mon, aren’t all of our APIs a working progress? Nexmo uses their API definition as “a single point of truth that can be used end-to-end can we used to:” Planning Shared during product discussions for planning API functionality Implementation Inform engineering during development Testing As the basis for testing or mocking API endpoints Documentation For producing thorough and interactive documentation Tooling To generate server stubs and client SDKs. Nexmo has adopted version 3.0 of the OpenAPI definition, which is forward leaning for the API provider. They also provide a list of resources, tooling, and their API definition available as Ruby packages for easier integration. Another thing they do that I think is interesting, is they list the owner, and contributors for each API definition they have published, or are working on....[<a href="/2018/03/26/nexmo-manages-their-openapi-3-0-definition-using-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/26/a-regulatory-framework-for-facebook-and-other-platforms-is-already-in-place/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/docks_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/26/a-regulatory-framework-for-facebook-and-other-platforms-is-already-in-place/">A Regulatory Framework For Facebook And Other Platforms Is Already In Place</a></h3>
			<p><em>26 Mar 2018</em></p>
			<p>There is lots of talk this week about regulating Facebook after the Cambridge Analytica story broke. Individuals, businesses, lawmakers, and even Facebook are talking about how we begin to better regulate not just Facebook, but the entire data industry. From my perspective, as the API Evangelist, the mechanisms are already in place, they just aren’t being used to their fullest by providers, with no sufficient policy in place at the federal level to incentivize healthy behavior by API providers, data brokers, and 3rd party application developers. API Management Is Already In Place Modern API platforms, Facebook included, leverage what is called an API management layer, to help manage the applications being developed on their platforms. Every developer who wants to access platform APIs has to sign up, submit the name and details of their application(s), and then receive an API key which they have to include with each call they make to a platform’s API when requesting ANY data, content, or usage of an algorithm. This means that all the platforms should already be in tune with every application that is using their platform, unless they allow internal groups, and specific partners to bypass the API management layer. An Application Review Process Every application submitting for API keys has to go through some sort of review process. The bar for this review process might be as low as requiring you to have an email address, or as high as submitting drivers and business licenses, and verify how you are storing your data. Facebook has an application review process in place, and is something they are promising to tighten up a bit in response to Cambridge Analytica. How loose, or tight a platform is with their application review process is always in alignment with the platform’s business model, and how the value their user’s privacy and security. Govern Data Access OAuth Using Scopes Most platforms use a standard called OAuth for enabling 3rd party developers access...[<a href="/2018/03/26/a-regulatory-framework-for-facebook-and-other-platforms-is-already-in-place/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/21/investment-in-api-security-will-continue-to-fall-short-while-there-is-no/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64_128_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/21/investment-in-api-security-will-continue-to-fall-short-while-there-is-no/">Investment In API Security Will Continue To Fall Short While There Is No</a></h3>
			<p><em>21 Mar 2018</em></p>
			<p>I have a lot of conversations with folks down in the trenches about API security, and what they are doing to be proactive when it comes to keeping their API infrastructure secure. The will and the desire amongst folks I talk to regarding API security is present. They want to do what it takes to truly understand what is needed to keep their APIs secure, but many have their hands tied because lack of resources to actually do what is needed. Every API team I know is short-handed, and doing the best they can with what they have available to them. A lack of investment in API security isn’t always intentional, it ends up just begin a reality of the priorities on the ground within the organizations they work in. While I’m sure leaders within these companies are concerned about breaches within their API infrastructure, the urgency to invest in this area isn’t always a priority. Despite an increase in high profile, often API-induced breaches, IT and API groups are still not given the amount of resources they need to do something about potential security incidents. Other than the stress and bad press of a breach, there really are no consequences in the United States. We have seen this play out over and over, and when high profile breaches like Equifax go unpunished, other corporate leaders fully understand that there will no consequences, so why invest in preventative measures–we will just respond to it “if” it happens. This is why GDPR, and other similar legislation will become important to the API security industry. Without real civil or criminal penalties involved with breaches, and even heavier penalties for poorly handled breaches, companies just aren’t going to care. Data is just a replaceable commodity, and a company can recover from the hit to their brand when a breach does occur. Making the investment in proactive API security training, staffing, services, and processes an unnecessary thing. Reflecting how...[<a href="/2018/03/21/investment-in-api-security-will-continue-to-fall-short-while-there-is-no/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/21/facebooks-business-model-is-out-of-alignment-with-their-api-management-layer/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/21/facebooks-business-model-is-out-of-alignment-with-their-api-management-layer/">Facebooks Business Model Is Out Of Alignment With Their API Management Layer</a></h3>
			<p><em>21 Mar 2018</em></p>
			<p>All the controls for Facebook to have done something about the Cambridge Analytics situation are already in place. The Facebook API management layer has everything needed to have prevented the type of user data abuse we’ve experienced, and honestly the user data abuse that has happens in many other applications, and will continue to occur. The cause of this behavior is rooted in Facebook’s business model, and a wider culture that is fueled by venture capital investment, and backdoor data brokering. It is just how the big data / app economy is funded, and when your business model is all based upon advertising, user engagement to generate data / signals, and being about leveraging that behavior for targeting–you are never going to reign in your API management layer. In alignment with my other story today around lack of investment in security, Facebook just doesn’t have the incentive to invest in policing their API management layer. There is no motivation to thoroughly vet new applications, let alone regularly review and audit what each application is doing as they approach 10K tokens, 100K, or 270K tokens. Sure you can’t get at all the friends data anymore, so they did work tighten things down at the API management schema layer, but most action we’ve seen is in response to bad situations, and not preventative. If you are properly managing the access to your APIs, and have the resources to monitor and respond to activity in real time, you are going to see the bad actors, and respond accordingly. Minimizing the damage that occurs to end-user, developing a robust set of patterns to keep an eye out for, and just get better at keeping your platform data consumption tight. The problem is when your primary business model is centered around advertising, and providing a wealth of controls around targeting users based upon the data points they provide, you want as many apps, as many data points, and as many...[<a href="/2018/03/21/facebooks-business-model-is-out-of-alignment-with-their-api-management-layer/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/20/sendgrid-managing-their-openapi-using-github/"><img src="https://s3.amazonaws.com/kinlane-productions2/sendgrid/sendgrid-github-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/20/sendgrid-managing-their-openapi-using-github/">SendGrid Managing Their OpenAPI Using Github</a></h3>
			<p><em>20 Mar 2018</em></p>
			<p>This is a post that has been in my API notebook for quite a while. I feel it is important to keep showcasing the growing number of API providers who are not just using OpenAPI, but also managing them on Github, so I had to make the time to talk about the email API provider SendGrid managing their OpenAPI using Github. Adding to the stack of top tier API providers managing their API definitions in this way. SendGrid is managing announcements around their OpenAPI definition using Github, allowing developers to signup for email notifications around releases and breaking changes. You can use the Github repository to stay in tune with the API roadmap, and contribute feature requests, submit bug reports, and even submit a pull request with the changes you’d like to see. Going beyond just an API definition being about API documentation, and actually driving the API road map and feedback loop. This approach to managing an API definition, while also making it a centerpiece of the feedback loop with your community is why I keep beating this drum, and showcasing API providers who are doing this. It is a way to manage the central API contract, where the platform provider and API consumers both have a voice, and producing a machine readable artifact that can be then used across the API lifecycle, from design to deprecation. Elevating the API definition beyond just a bi-product of creating API docs, and making it a central actor in everything that occurs as part of API operations. I’m still struggling with convincing API providers that they should be adopting OpenAPI. That it is much more than an artifact driving interactive documentation. Showcasing companies like SendGrid using OpenAPI, as well as their usage of Github, is an important part of me convincing other API providers to do the same. If you want me to write about your API, and what you are working to accomplish with your API...[<a href="/2018/03/20/sendgrid-managing-their-openapi-using-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/20/breaking-down-your-postman-api-collections-into-meaningful-units-of-compute/"><img src="https://s3.amazonaws.com/kinlane-productions2/postman/postman-icon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/20/breaking-down-your-postman-api-collections-into-meaningful-units-of-compute/">Breaking Down Your Postman API Collections Into Meaningful Units Of Compute</a></h3>
			<p><em>20 Mar 2018</em></p>
			<p>I’m fascinated with the unit of compute as defined by a microservice, OpenAPI definition, Postman Collection, or other way of quantifying an API-driven resource. Asking the question, “how big or how small is an API?”, and working to define the small unit of compute needed at runtime. I do not feel there is a perfect answer to any of these questions, but it doesn’t mean we shouldn’t be asking the questions, and packaging up our API definitions in a more meaningful way. As I was profiling APIs, and creating Postman Collections, the Okta team tweeted at me, their own approach to delivering their APIs. They tactically place Run in Postman buttons throughout their API documentation, as well as provide a complete listing of all the Postman Collections they have. Showcasing that they have broken up their Postman Collections along what I’d consider to be service lines. Providing small, meaningful collections for each of their user authentication and authorization APIs: Collections Click to Run Authentication API Access Management (OAuth 2.0) OpenID Connect Client Registration Sessions Apps Events Factors Groups Identity Providers (IdP) Logs Admin Roles Schemas Users Custom SMS Templates Okta’s approach delivers a pretty coherent, microservices approach to crafting their Postman Collections, providing separate API runtimes for each service they bring to the table. Which I think gets at what I’m looking to understand when it comes to defining and presenting our APIs. It can be a lot more work to create your Postman Collections like this, rather than just creating one single collection, with all API paths, but I think from a API consumer standpoint, I’d rather have them broken down like this. I may not care about all APIs, and I’m just looking at getting my hands on a couple of services–why make me wade through everything? I have imported the Postman Collections for the Okta API, and added to my API Stack research. I’m going to convert them into OpenAPI definitions so...[<a href="/2018/03/20/breaking-down-your-postman-api-collections-into-meaningful-units-of-compute/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/19/general-data-protection-regulation-gdpr-forcing-us-to-ask-questions-about/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/face-2_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/19/general-data-protection-regulation-gdpr-forcing-us-to-ask-questions-about/">General Data Protection Regulation (GDPR) Forcing Us To Ask Questions About</a></h3>
			<p><em>19 Mar 2018</em></p>
			<p>I’ve been learning more about the EU General Data Protection Regulation (GDPR) recently, and have been having conversation about compliance with companies in the EU, as well as the US. In short, GDPR requires anyone working with personal data to be up front about the data they collect, making sure what they do with that data is observable to end-users, and takes a privacy and security by design approach when it comes to working with all personal data. While the regulations seems heavy handed and unrealistic to many, it really reflects a healthy view of what personal data is, and what a sustainable digital future will look like. The biggest challenge with becoming GDPR compliant is the data mess most companies operate in. Most companies collect huge amounts of data, believing it is essential to the value they bring to the table, without no real understanding of everything that is being collected, and any logical reasons behind why it is gathered, stored, and kept around. A “gather it all”, big data mentality has dominated the last decade of doing business online. Database groups within organizations hold a lot of power and control because of the data they possess. There is a lot of money to be made when it comes to data access, aggregation, and brokering. It won’t be easy to unwind and change the data-driven culture that has emerged and flourished in the Internet age. I regularly work with companies who do not have coherent maps of all the data they possess. If you asked them for details on what they track about any given customer, very few will be able to give you a consistent answer. Doing web APIs has forced many organizations to think more deeply about what data they posses, and how they can make it more discoverable, accessible, and usable across systems, web, mobile, and device applications. Even with this opportunity, most large organizations are still struggling with what data...[<a href="/2018/03/19/general-data-protection-regulation-gdpr-forcing-us-to-ask-questions-about/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/19/facebook-cambridge-analytica-and-knowing-what-api-consumers-are-doing-with-our-data/"><img src="https://s3.amazonaws.com/kinlane-productions2/facebook/suspending-cambridge-analytica-facebook.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/19/facebook-cambridge-analytica-and-knowing-what-api-consumers-are-doing-with-our-data/">Facebook, Cambridge Analytica, And Knowing What API Consumers Are Doing With Our Data</a></h3>
			<p><em>19 Mar 2018</em></p>
			<p>I’m processing the recent announcement by Facebook to shut off the access of Cambridge Analytica to it’s valuable social data. The story emphasizes the importance of having a real time awareness and response to API consumers at the API management level, as well as the difficulty in ensuring that API consumers are doing what they should be with the data and content being made available via APIs. Access to platforms using APIs is more art than science, but there are some proven ways to help mitigate serious abuses, and identify the bad actors early on, and prevent their operation within the community. While I applaud Facebook’s response, I’m guessing they could have taken more action earlier on. Their response is more about damage control to their reputation, after the fact, than it is about preventing the problem from happening. Facebook most likely had plenty of warning signs regarding what Aleksandr Kogan, Strategic Communication Laboratories (SCL), including their political data analytics firm, Cambridge Analytica, were up to. If they weren’t than that is a problem in itself, and Facebook should be investing in more policing of their API consumers activity, as they claim they are doing in their release. If Aleksandr Kogan has that many OAuth tokens for Facebook users, then Facebook should be up in his business, better understanding what he is doing, where his money comes from, and who is partners are. I’m guessing Facebook probably had more knowledge, but because it drove traffic, generated ad revenue, and was in alignment with their business model, it wasn’t a problem. They were willing to look the other way with the data sharing that was occurring, until it became a wider problem for the election, our democracy, and in the press. Facebook should have more awareness, oversight, and enforcement at the API management layer of their platform. This situation I think highlights another problem of doing APIs, and ensuring API consumers are behaving appropriately with the...[<a href="/2018/03/19/facebook-cambridge-analytica-and-knowing-what-api-consumers-are-doing-with-our-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/16/68-stops-in-a-comprehensive-api-strategy-transit-map/"><img src="https://s3.amazonaws.com/kinlane-productions2/transit/calcagno-2017-01-01.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/16/68-stops-in-a-comprehensive-api-strategy-transit-map/">68 Stops In A Comprehensive API Strategy Transit Map</a></h3>
			<p><em>16 Mar 2018</em></p>
			<p>I am refining my comprehensive list of stops that I highlight as part of the API lifecycle, or what I call API Transit–the stops that each of the services we deliver will have “pass through” at some point. I’m sharing this list with other team members as part of existing consulting arrangements I’m engaged in with Streamdata.io, as well as baking into my API transit work for some workshops I’m doing in April. All of these are available on the API lifecycle section of API Evangelist, but I will be pushing them to become a first class citizen on the home page of API Evangelist once again. Here are 68 potential stops I’d consider for any microservice to be exposed to, as part of a larger API transit strategy map: Definitions - From the simplest definition of what a service does, all the way to the complete OpenAPI definition for each service. Design - The consistent design of each service leverage existing patterns, as well as the web to deliver each service. Versioning - Plan for handling changes, and the inevitable evolution of each service, its definitions, and supporting code. DNS - The domain addressing being used for routing, management and auditing of all service traffic. Database - The backend database schema, infrastructure, and other relevant information regarding how data is managed. Compute - How is the underlying compute delivered for each service using containers, serverless, cloud, and on-premise infrastructure. Storage - What does storage of all heavy objects, media, and other assets involved with the delivery of services. Deployment - Details services developed, and deployed, including frameworks and other libraries being used. Orchestration - Defining what the build, syndication, and orchestration of service deployments and integrations look like, and are executed. Dependencies - Identifying all backend service, code and infrastructure dependencies present for each service. Search - Strategy for understanding what search will look like for each individual service, and play a role in...[<a href="/2018/03/16/68-stops-in-a-comprehensive-api-strategy-transit-map/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/14/rules-for-extending-your-api-with-each-version/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_19_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/14/rules-for-extending-your-api-with-each-version/">Rules for Extending Your API With Each Version</a></h3>
			<p><em>14 Mar 2018</em></p>
			<p>I’m spending time learning from the API design guides of other leading API providers, absorbing their healthy practices, and assimilating them into my own consulting and storytelling. One API design guide I am learning a lot from is out of the Adidas Group, which contains a wealth of wisdom regarding not just the design of your API, but also the deployment and evolution of the API resources we are publishing. One particularly interesting piece of advice I found within Adidas API design guidance were their rules for extending an API, which I think is pretty healthy advice for an API developer to think about. Any modification to an existing API MUST avoid breaking changes and MUST maintain backward compatibility. In particular, any change to an API MUST follow the following Rules for Extending: You MUST NOT take anything away (related: Minimal Surface Principle , Robustness Principle) You MUST NOT change processing rules You MUST NOT make optional things required Anything you add MUST be optional (related Robustness Principle) NOTE: These rules cover also renaming and change to identifiers (URIs). Names and identifiers should be stable over the time including their semantics. First of all, I think many API developers aren’t even thinking about what constitutes a breaking change most of the time. So having any guidance that makes them pause and think about this topic is a good thing. Second, I think we should be sharing more stories about when things break, helping folks think more about these elements–the problem is that many folks are embarrassed they introduced a breaking changes, and would rather not talk about it, let alone make it publicly known. I am working my way through the API Stylebook, learning from all the API design guides it has aggregated. There is a wealth of knowledge in there to learn from, and contains topics that make for great stories here on the blog. I wish more API providers would actively publish their...[<a href="/2018/03/14/rules-for-extending-your-api-with-each-version/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/14/linkedin-account-validation-for-new-api-consumers/"><img src="https://s3.amazonaws.com/kinlane-productions2/dawex/requiring-linkedin-validation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/14/linkedin-account-validation-for-new-api-consumers/">LinkedIn Account Validation For New API Consumers</a></h3>
			<p><em>14 Mar 2018</em></p>
			<p>I was on-boarding with the data marketplace Dawex the other day, and I thought their on-boarding process was interesting. It is pretty rigid, requiring users to validate themselves in multiple ways, but it provides some interesting approaches to knowing more about who your API developers are. Dawex has the basic level email and phone number validation, but they have added a 3rd dimension, validating who you are using your LinkedIn profile. After signing up, I was required to validate my email account–pretty standard stuff. Then I was asked to enter a code sent to my cell phone via SMS–not as common, but increasingly a way that platforms are using to verify you. Then I was asked to OAuth and connect my LinkedIn account to my Dawex profile. I’ve seen sign up and login using LinkedIn, but never using it as a 3rd type of validating who you are, and that you are truly a legitimate business user. I haven’t been verified yet, it says it will take up to 72 hour is what the notification at the top of my dashboard says, but it caught my attention. Dawes also has a complete, Know Your Customer (KYC) process, which takes the validation to another level, and something I’ll write about separately. I think the social profile validation provides an interesting look at how platforms validate who we are. In an environment where API developers will often sign up for multiple accounts, and do other shady things with anonymous accounts, I can get behind requiring consumers to prove who they are. I also think that providing robust, active, verifiable social media accounts using Github, Twitter, LinkedIn, and Facebook makes a lot of sense. I’d say that all four of these profiles represent who I am as a person, as well as a business. I have talked about using Github to validate and rate API consumers before. I could see a world where I get validated upon signup...[<a href="/2018/03/14/linkedin-account-validation-for-new-api-consumers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/14/an-openapi-vendor-extension-for-defining-your-api-audience/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/97_193_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/14/an-openapi-vendor-extension-for-defining-your-api-audience/">An OpenAPI Vendor Extension For Defining Your API Audience</a></h3>
			<p><em>14 Mar 2018</em></p>
			<p>The clothing marketplace Zalando has an interesting approach to classifying their APIs based upon who is consuming them. It isn’t just about APIs being published publicly, or privately, they actually have standardized their definition, and have established an OpenAPI vendor extension, so that the definition is machine readable and available via their OpenAPI. According to the Zalando API design guide, “each API must be classified with respect to the intended target audience supposed to consume the API, to facilitate differentiated standards on APIs for discoverability, changeability, quality of design and documentation, as well as permission granting. We differentiate the following API audience groups with clear organisational and legal boundaries.” component-internal - The API consumers with this audience are restricted to applications of the same functional component (internal link). All services of a functional component are owned by specific dedicated owner and engineering team. Typical examples are APIs being used by internal helper and worker services or that support service operation. business-unit-internal - The API consumers with this audience are restricted to applications of a specific product portfolio owned by the same business unit. company-internal - The API consumers with this audience are restricted to applications owned by the business units of the same the company (e.g. Zalando company with Zalando SE, Zalando Payments SE &amp; Co. KG. etc.) external-partner - The API consumers with this audience are restricted to applications of business partners of the company owning the API and the company itself. external-public - APIs with this audience can be accessed by anyone with Internet access. Note: a smaller audience group is intentionally included in the wider group and thus does not need to be declared additionally. The API audience is provided as API meta information in the info-block of the Open API specification and must conform to the following specification: #/info/x-audience: type: string x-extensible-enum: - component-internal - business-unit-internal - company-internal - external-partner - external-public description: | Intended target audience of the API. Relevant...[<a href="/2018/03/14/an-openapi-vendor-extension-for-defining-your-api-audience/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/14/a-visual-view-of-api-responses-within-our-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions2/webhose/webhose-io-visual-documentation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/14/a-visual-view-of-api-responses-within-our-documentation/">A Visual View Of API Responses Within Our Documentation</a></h3>
			<p><em>14 Mar 2018</em></p>
			<p>Interactive API documentation is nothing new. We’ve had Swagger UI, and other incarnations for over five years now. We also have API explorers, and full API lifecycle client solutions like Postman to help us engage with APIs, and be able to quickly see responses from the APIs we are consuming. In my effort to keep pushing forward the API documentation conversation I’ve been beating the drum for more visual solutions to be baked into our interactive documentation for a while now, encouraging providers to make the responses we receive much more meaningful, and intuitive for consumers. To help drum up awareness to this aspect of API documentation I’m always on the lookout for any interesting examples of it in the wild. There was the interesting approach out of the Food and Drug Administration (FDA), and now I’ve seen one out of the web data feeds API provider Webhose.io. When you are making API calls in their interactive dashboard you get a JSON response for things like news articles on the left hand side, but you also get an interesting slider that will show a visual representation of the JSON response on the right side–making it much more readable to non-developers. It provides a nice way to quickly make sense of API responses. Making them more accessible. Making it something that even non-developers can do. Essentially providing a reverse view source (view results?) for API responses. Taking the raw JSON, and providing an HTML lens for the humans trying to make decisions around the usage of a particular API. View source is how I learned HTML back in the mid 1990s, and I can see visualization tools for API responses helping average businesses users learn JSON, or at least make it a little less intimidating, and something they feel like they put to work for them. I really feel like more visualizations baked into API documentation is the future of interactive API docs. Being able to...[<a href="/2018/03/14/a-visual-view-of-api-responses-within-our-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/13/seeing-messy-api-design-practices-as-an-opportunity/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/13439120_10154285627219813_3054276594176638940_n_copper_circuit_2.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/13/seeing-messy-api-design-practices-as-an-opportunity/">Seeing Messy API Design Practices As An Opportunity</a></h3>
			<p><em>13 Mar 2018</em></p>
			<p>I’m generating OpenAPI definitions for a wide variety of APIs currently, and I’m regularly stumbling on the messiness of the API design practices being deployed. When you are exposed to a large number of different APIs it is easy to get frustrated, begin ranting, and bitching about how ignorant people are of healthy, sensible API practices. This is the easy route. Making sense of it all, finding the interesting signals and patterns, and extracting where the opportunity are takes a significant amount of effort (so does biting tongue). After profiling almost 500 API providers, I have almost 25K separate API paths indexed using OpenAPI, and APIs.json. I’ve tagged each API path using its OpenAPI definition. Pulling words from the path name, and any summary and description provided within the API documentation. Doing my best to describe the value contained within each API resource. Then I started grouping by these tags, to see what it produced. Sometimes the API paths it lists makes total sense, but most of the time it makes no sense at all. Then, I started noticing interesting patterns in how people describe their resources. Grouping things like “favorites” across all types of APIs, revealing some pretty interesting perspectives, and honest views of the resources being exposed. Something as simple as “activities” can mean 30 or 40 different things when applied across CRM, storage, DNS, travel or sports APIs. At first, I’m like this shit is broken. The more time I spend with the mess, the more I’m starting to think there is more to this mess than meets the eye. I could be wrong. I often am. It is likely just my contrarian view of things, and my unique view of the API landscape in action. Where many people see duplicative work, I see common patterns. I just see the landscape differently than people who are just looking to get their work done, sell a product or service, and find an exit...[<a href="/2018/03/13/seeing-messy-api-design-practices-as-an-opportunity/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/13/multilingual-machine-learning-api-deployments/"><img src="https://s3.amazonaws.com/kinlane-productions2/paralleldots/multi-lingual-website-676x507.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/13/multilingual-machine-learning-api-deployments/">Multi-Lingual Machine Learning API Deployments</a></h3>
			<p><em>13 Mar 2018</em></p>
			<p>Machine learning API ParallelDots has a story on launching their APIs in multiple languages. Allowing them to “serve a truly global customer base with following language options for our key APIs (Sentiment Analysis, Emotion Analysis, and Keyword generator)”. Something that I think more APIs providers are going to have to think about in coming years, as the need for API resources expands around the globe. I’m tracking on the localization efforts of different API providers, and I’d say that having service availability in different regions, and multi-lingual support are the top two areas I’m seeing movement. I see two driving forces behind this, 1) the customers are demanding localization for their businesses, and 2) the governments in those countries are imposing regulations and other laws that dictate where resources can be stored and operate. I guess, something that can be seen as markets working things out, if you also believe in the value of regulations. I also see a negative in all of this, specifically in this case, the imperialistic aspects of artificial intelligence and machine learning. Meaning, the models are trained here in western countries, but then being applied, injected, and imposed upon people in other countries. Selling the service as some sort of truth, watermark, and organic solution to sentiment, emotion, and intelligence without actually localizing the models. I’m not making any assumptions around ParralelDots motivations, just pointing out that this will be a problem, and should be ignored–if it makes you mad, then you are probably part of the problem. As I conduct my API research, I will keep tracking on localization efforts like this. When I have the bandwidth I will dive in deeper and better understand how providers are defining these localization layers to their APIs in their API design, deployment, documentation, and other elements. I’ll be keeping an eye on which industries are moving fastest when it comes to API localization, and try to understand where it is deemed...[<a href="/2018/03/13/multilingual-machine-learning-api-deployments/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/13/google-releases-a-protocol-buffer-implementation-of-the-fast-healthcare/"><img src="https://s3.amazonaws.com/kinlane-productions2/fhir/FHIR_logo-1080x675.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/13/google-releases-a-protocol-buffer-implementation-of-the-fast-healthcare/">Google Releases a Protocol Buffer Implementation of the Fast Healthcare</a></h3>
			<p><em>13 Mar 2018</em></p>
			<p>Google is renewing its interest in the healthcare space by releasing a protocol buffer implementation of the fast healthcare interoperability resources (FHIR) standard. Protocol buffers are “Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler”. Its the core of the next generation of APIs at Google, often using HTTP/2 as a transport, while also living side by side with RESTful APIs, which use OpenAPI as the definition, in parallel to what protocol buffers deliver. It’s a smart move by Google. Providing a gateway for healthcare data to find its way to their data platform products like Google Cloud BigQuery, and their machine learning solutions built on Tensorflow. They want to empower healthcare providers with powerful solutions that help onboard their data, and be able to connect the dots, and make sense of it at scale. However, I wouldn’t stop with protocol buffers. I would also make sure they also invest in API infrastructure on the RESTful side of the equation, developing OpenAPI specs alongside the protocol buffers, and providing translation between, and tooling for both realms. While I am a big supporter of gRPC, and protocol buffers, I’m skeptical of the complexity it brings, in exchange for higher performance. Part of making sense of health care data will require not just technical folks being able to make sense of what is going on, but also business folks, and protocol buffers, and gRPC solutions will be out of reach of these users. Web APIs, combined with YAML OpenAPI has begun to make the API contracts involved in all of this much more accessible to business users, putting everything within their reach. In our technological push forward, let’s not forget the simplicity of web APIs, and exclude business healthcare users as IT has done historically. I’m happy to see more FHIR-compliant APIs emerging on the landscape. PSD2 for banking, and FHIR for healthcare are the two best examples we...[<a href="/2018/03/13/google-releases-a-protocol-buffer-implementation-of-the-fast-healthcare/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/13/api-as-a-product-principles-from-zalando/"><img src="https://s3.amazonaws.com/kinlane-productions2/zalando/zalando-api-guidelines.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/13/api-as-a-product-principles-from-zalando/">API As A Product Principles From Zalando</a></h3>
			<p><em>13 Mar 2018</em></p>
			<p>As I’m working through the API design guides from API leaders, looking for useful practices that I can include in my own API guidance, I’m finding electronic commerce company Zalando’s API design guide full of some pretty interesting advice. I wanted to showcase the section about their API as a product principles, which I think reflects what I hear many companies striving for when they do APIs. From the Zalando API design guide principles: Zalando is transforming from an online shop into an expansive fashion platform comprising a rich set of products following a Software as a Platform (SaaP) model for our business partners. As a company we want to deliver products to our (internal and external) customers which can be consumed like a service. Platform products provide their functionality via (public) APIs; hence, the design of our APIs should be based on the API as a Product principle: Treat your API as product and act like a product owner Put yourself into the place of your customers; be an advocate for their needs Emphasize simplicity, comprehensibility, and usability of APIs to make them irresistible for client engineers Actively improve and maintain API consistency over the long term Make use of customer feedback and provide service level support RESTful API as a Product makes the difference between enterprise integration business and agile, innovative product service business built on a platform of APIs. Based on your concrete customer use cases, you should carefully check the trade-offs of API design variants and avoid short-term server side implementation optimizations at the expense of unnecessary client side obligations and have a high attention on API quality and client developer experience. API as a Product is closely related to our API First principle which is more focused on how we engineer high quality APIs. Zalando provides a pretty coherent vision for how we all should be doing APIs. I like this guidance because it helps quantify something we hear a...[<a href="/2018/03/13/api-as-a-product-principles-from-zalando/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/12/thinking-about-api-status-codes-for-destroying-entities-using-delete/"><img src="https://s3.amazonaws.com/kinlane-productions2/deliveroo/deliveroo-new-visual-branding-logo.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/12/thinking-about-api-status-codes-for-destroying-entities-using-delete/">Thinking About API Status Codes For Destroying Entities Using DELETE</a></h3>
			<p><em>12 Mar 2018</em></p>
			<p>
I am pulling together some API design guidance for some projects I’m consulting on, so I’m spending time reviewing the API design guides the leading API providers who have published them publicly. Learning from what they are doing across their own companies, organizations, institutions, and government agencies when it comes to sensible API governance.

Today, I am learning from the British food delivery company, Deliveroo, and documenting their guidance for which HTTP status codes should be returned for any API methods that use DELETE:

If it exists, it should return status:


  204 No Content if the entity was successfully destroyed,
  404 Not Found if the entity does not exist
  410 Gone if the entity is known to have existed but no longer does.


Additionally 4xx response codes may be used:


  412 Precondition Failed
  415 Unsupported if using versioning and the server doesn’t support the specified version.


I have to admit, I haven’t been properly publishing status codes any DELETE methods I provide. I’ve just been returning a 200 if successful, and 404 if it didn’t exist. I hadn’t put any further thought into the proper way of handling it. I just haven’t had the time, or the knowledge within reach to be able to handle properly.

I know that the RESTafarians enjoy debating these finer details of API design, and using HTTP, but for the rest of us, we just need some guidance from y’all. Which is why I spend time learning from existing API leaders who publish their API design guidance, and sharing as individual stories here on the blog, as well as including in my official project consulting guidance.

[<a href="/2018/03/12/thinking-about-api-status-codes-for-destroying-entities-using-delete/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/12/the-importance-of-tags-in-openapi-definitions-for-machine-learning-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/contrafabulists/machine+learning.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/12/the-importance-of-tags-in-openapi-definitions-for-machine-learning-apis/">The Importance Of Tags In OpenAPI Definitions For Machine Learning APIs</a></h3>
			<p><em>12 Mar 2018</em></p>
			<p>I am profiling APIs as part of my partnership with Streamdata.io, and my continued API Stack work. As part of my work, I am creating OpenAPI, Postman Collections, and APIs.json indexes for APIs in a variety of business sectors, and as I’m finishing up the profile for ParallelDots machine learning APIs, I am struck (again) by the importance of tags within OpenAPI definitions when it comes to defining what any API does, and something that will have significant effects on the growing machine learning, and artificial intelligence space. While profiling ParallelDots, I had to generate the OpenAPI definition from the Postman Collection they provide, which was void of any tags. I went through the handful of API paths, manually adding tags for each of the machine learning resources. I’m adding tags like sentiment, emotions, semantics, taxonomy, and classification, to each path. Trying to capture what resources were available, allowing for the discovery, filtering, and execution of each individual machine learning model being exposed using a simple web API. While the summary and description explain what each API does to developers, the tags are really the precise meaning in a machine readable context. In the fast moving, abstract realm of machine learning, and artificial intelligence it can be difficult to truly understand what each API does, or doesn’t do. I struggle with it, and I’m pretty well versed in what is possible and not possible with machine learning. Precise tagging provide us with a single definition of what each machine learning API does, setting a baseline of understanding for putting ML APIs to work. Something that if I consistently apply across all of the machine learning APIs I’m profiling, I can can begin honing and dialing in my catalog of valuable API resources, and begin creating a coherent map of what is possible with machine learning APIs–helping ground us in this often hyped realm. Once I have a map of the machine learning landscape established, I...[<a href="/2018/03/12/the-importance-of-tags-in-openapi-definitions-for-machine-learning-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/12/some-common-api-data-types-to-put-to-use/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/carryload_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/12/some-common-api-data-types-to-put-to-use/">Some Common API Data Types To Put To Use</a></h3>
			<p><em>12 Mar 2018</em></p>
			<p>I’m continuing my exploration of the API design guidance published by leading API providers. This time, I am taking a look at the API design guide publish by Adidas, specifically the portion of it addressing what some of the common API types should be. Providing some API design essentials, that all of us API providers should be baking into our APIs by default. Here is the short list of default standards Adidas is supporting across their API operations: Date and Time Format - Date and Time MUST always conform to the ISO 8601 format e.g.: 2017-06-21T14:07:17Z (date time) or 2017-06-21 (date), it MUST use the UTC (without time offsets). Duration Format - Duration format MUST conform to the ISO 8601 format standard e.g.: P3Y6M4DT12H30M5S (three years, six months, four days, twelve hours, thirty minutes, and five seconds). Time Interval Format - Time Interval format MUST conform to the ISO 8601 format standard e.g.: 2007-03-01T13:00:00Z/2008-05-11T15:30:00Z. Standard Time Stamps - Where applicable, a resource representation SHOULD contain the standard timestamps: createdAt, updatedAt, and finishedAt, using the ISO 8601 format. Language Code Format - Language codes MUST conform to the ISO 639 e.g.: en for English. Country Code Format - Country codes MUST conform to the ISO 3166-1 alpha-2 e.g.: DE for Germany. Currency Format - Currency codes MUST conform to the ISO 4217 e.g.: EUR for Euro. These are the common data formats we are all using across our APIs. We should be working to support standard formats wherever possible, embracing the wider web, and investing in the longevity and usability of our API designs. Making our APIs sure that they speak to as wide of audience as they possibly can, and operate in a consistent way, no matter which API you are integrating with. I want to also give a shout out to the API Stylebook, where I’m easily finding these API design guides that I’m using for these stories, and including in my API design...[<a href="/2018/03/12/some-common-api-data-types-to-put-to-use/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/12/a-machine-readable-baseline-for-api-providers-in-my-api-stack-work/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-definitions-api-stack.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/12/a-machine-readable-baseline-for-api-providers-in-my-api-stack-work/">A Machine Readable Baseline For API Providers In My API Stack Work</a></h3>
			<p><em>12 Mar 2018</em></p>
			<p>I’m rebooting my API Stack work as part of my partnership with Streamdata.io. I’m spending a significant portion of my day profiling API providers, documenting what it is they bring to the table. Historically I’ve published the resulting APIs.json and OpenAPI definition(s) to a single Github repository driving theapistack.com. The primary folder was already getting too big, and since I’m looking at adding at least a thousand more API providers to the stack, I am going to need to shard things out a bit. To help accommodate the increased scale, I’m breaking up the API Stack into two separate Github organizations. One for individual API providers called api-stack-providers, and a second for individual topics called api-stack-topics. I’ve began publishing individual repositories for each API provider that I am monitoring, establishing a self-contained, continuously deployable, and integratabtle set of definitions for each API. I’m needing each API provider to be independent from each other, and I’m even publishing individual API definitions for each API path, distilling things down to the smallest possible unit of compute possible. Each repository contains three main API definitions, using the ParallelDots machine learning API as an example: APIs.json - The entire index of the API provider’s operation. OpenAPI - The definition of the surface area of entire API. Postman Collection - An execute-time ready definition for the Postman client. I’m also publishing an individual OpenAPI for each API path within a listings folder, allowing me to provide the detailed execute-time definition I’m needing to work with each API. I’m also considering how I can do this with the Postman Collections. Next, I’m looking to understand how I can include the multiple source definitions within the repository, providing original copies of API definitions that I’m pulling from sources like APIs.guru, Any API, and directly from API providers. Sharing the entire history behind how each set of API definitions has come together, and hopefully encouraging others to submit pull requests with fresh copies....[<a href="/2018/03/12/a-machine-readable-baseline-for-api-providers-in-my-api-stack-work/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/09/the-postman-api-network/"><img src="https://s3.amazonaws.com/kinlane-productions2/postman/postman-api-network.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/09/the-postman-api-network/">The Postman API Network</a></h3>
			<p><em>09 Mar 2018</em></p>
			<p>The Postman API Network is one of the recent movements in the API discovery space I’ve been working to get around to covering. As Postman continues its expansion from being just an API client, to a full lifecycle API development solution, they’ve added a network for discovering existing APIs that you can begin using within Postman in a single click. Postman Collections make it ridiculously easy to get up and running with an API. So easy, I’m confounded why ALL APIs aren’t publishing Postman Collections with Run in Postman Buttons published in their API docs. The Postman API Network provides a catalog of APIs in over ten categories, with links to each API’s documentation. All of the APIs in the network have a Run in Postman button available as part of their documentation, which includes them in the Postman API Network. It is a pretty sensible approach to building a network of valuable APIs, who all have invested in there being a runtime-ready, machine readable Postman Collection for their APIs. One of the more interesting approaches I’ve seen introduced to help solve the API discovery problem in the eight years I’ve been doing API Evangelist. I’ve been talking to Abhinav Asthana (@a85) about the Postman API Network, and working to understand how I can contribute, and help grow the catalog as part of my work as the API Evangelist. I’m a fan of Postman, and an advocate of it as an API lifecycle development solution, but I’m also really keen on bringing comprehensive API discovery solutions to the table. With the Postman API Network, and other API discovery solutions I’m seeing emerge recently, I’m finding renewed energy for this area of my work. Something I’ll be brainstorming and writing about more frequently in coming months. Streamdata.io has been investing in me moving forward the API discovery conversation, to build out their vision of a Streamdata.io API Gallery, but also to contribute to the overall API...[<a href="/2018/03/09/the-postman-api-network/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/09/keeping-track-of-federal-government-open-source-projects-using-the-code-gov/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/code-gov/code-gov-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/09/keeping-track-of-federal-government-open-source-projects-using-the-code-gov/">Keeping Track Of Federal Government Open Source Projects Using The Code.gov</a></h3>
			<p><em>09 Mar 2018</em></p>
			<p>Open source software is increasingly driving the federal government, despite the wishes of companies like Oracle. I’ve been watching an interesting project grow within the federal government to help quantify open source across the federal government called Code.gov, which “leverages the power of code sharing and collaboration to help the US Government cut down on duplicative software development and save millions of taxpayer dollars for the American people.” Something I think we can all get behind, when it comes to the collision of technology and government we are seeing play out. Code.gov allows you to browse open source projects by government agency, see the details for the project, visit the repository, and contact the project owners. The project even has a help wanted section where you can roll up your sleeves and actually contribute to specific projects, and help agencies push forward their project. You can follow up on what is happening using the Code.gov roadmap, and stay in tune with whats next for the very important project. However, the aspect I’m most interested in, is the evolution of the Code.gov API. There is an open source API behind the Code.gov project, providing programmatic access to government agencies repositories, the code, types of programming languages used, status, and other critical details for projects. There is an OpenAPI-driven, Swagger UI documentation for the project, providing a nice look at the programmatic surface for the project. You can use the API they have provided, or you can fork the API and operate yourself, providing an interesting look at delivering open source APIs that span multiple federal agencies–a blueprint that should be considered for other types of projects. Code.gov reflects what I want to see come out of the federal government when it comes to technology. Open source. API-driven. Doing on thing, and doing it well. I can see spinoffs of the project emerge, focusing on just the environment, transportation, healthcare, and other verticals, helping shine a light...[<a href="/2018/03/09/keeping-track-of-federal-government-open-source-projects-using-the-code-gov/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/09/defining-the-smallest-unit-possible-for-use-at-api-runtime/"><img src="https://s3.amazonaws.com/kinlane-productions2/peachtree/peachtree-email-validation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/09/defining-the-smallest-unit-possible-for-use-at-api-runtime/">Defining The Smallest Unit Possible For Use At API Runtime</a></h3>
			<p><em>09 Mar 2018</em></p>
			<p>I’m thinking a lot about what is needed at API runtime lately. How we document and provide machine readable definitions for APIs, and how we provide authentication, pricing, and even terms of service to help reduce friction. As Mike Amundsen (@mamund) puts it, to enable “find and bind”. This goes beyond simple API on-boarding, and getting started pages, and looks to make APIs executable within a single click, allowing us to put them to use as soon as we find them. The most real world example of this in action can be found with the Run in Postman button, which won’t always deal with the business and politics of APIs at runtime, but will deal with just about everything else. Allowing API providers to publish Run in Postman Buttons, defined using a Postman Collection, which include authentication environment details, that API consumers can use to quickly fire up an API in a single click. One characteristic I’ve come across that contributes to Postman Collections being truly executable is that they reflect the small unit possible for use at API runtime. You can see an example of this in action over at Peachtree Data, who like many other API providers have crafted Run in Postman buttons, but instead of doing this for the entire surface area of their API, they have done it for a single API path. Making the Run in Postman button much more precise, and executable. Taking it beyond just documentation, to actually being more of a API runtime executable artifact. This is a simple shift in how Postman Collections can be used, but a pretty significant one. Now instead of wading through all of Peachtree’s APIs in my Postman, I can just do an address cleanse, zip code lookup, or email validation–getting down to business in a single click. This is an important aspect of on-boarding developers. I may not care about wading through and learning about all your APIs right now....[<a href="/2018/03/09/defining-the-smallest-unit-possible-for-use-at-api-runtime/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/08/your-obsessive-focus-on-the-api-resource-is-hindering-meaningful-events-from/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/berlin_wall_graham_sutherland.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/08/your-obsessive-focus-on-the-api-resource-is-hindering-meaningful-events-from/">Your Obsessive Focus On The API Resource Is Hindering Meaningful Events From</a></h3>
			<p><em>08 Mar 2018</em></p>
			<p>I’ve been profiling a number of market data APIs as part of my work with Streamdata.io to identify valuable sources of data that could be streamed using their service. A significant portion of the APIs I come across are making it difficult for me to get at the data they have because of their views around the value of the data, intellectual property, and maintaining control over it in an API-driven world. These APIs don’t end up on the list of APIs I’m including in the profiling work, the gallery / directory, and don’t get included in any of the stories I’m telling, as a result of this tight control. The side effect of this is I end up getting repeated sales emails and phone calls asking if I am still interested in their data. If there was just one or two of these, I’d jump on phones and explain, but because I’m dealing with 50+ of them, I just don’t have the bandwidth, and I have to move on. The thing is, I’m personally not interested in their data. I’m interested in other people being interested in their data, and being an enabler to helping them to get at it. However, since I can’t actually profile the APIs, create OpenAPI definitions for the request and response structure for inclusion in the API gallery / directory I’m building, I really don’t need their APIs in my work. I know these platforms are protective of their data because it is valuable. They should be. However modern API management allows for them to open up the sampling of everything they have to offer, without giving away the farm. This allows enablers, analysts, and storytellers like me to test drive things, profile what they have to offer, and include within our applications. Then my users find what they want, head over to the source of the data, sign up for API keys, talk to their sales staff about...[<a href="/2018/03/08/your-obsessive-focus-on-the-api-resource-is-hindering-meaningful-events-from/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/08/imagine-a-mock-only-api-development-reality/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/gears_smoking_cigarette.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/08/imagine-a-mock-only-api-development-reality/">Imagine A Mock Only API Development Reality</a></h3>
			<p><em>08 Mar 2018</em></p>
			<p>Imagine if we didn’t actually write code when we developed our APIs initially. What if the bar for putting an API into production was so high, that many APIs never actually made it to that level. I spend a lot of time mocking and prototyping APIs, but I don’t have a strict maturity model determining when I actually bring an API to life using code. I’d love it if I never actually write code for many of my API prototypes and just mocked them, and iterated upon the mocks instead of thinking they needed actually have a database backend, and code or gateway front-end. What if I only executed these stops along the API lifecycle, instead of actually writing any code: Define - Define what I want my API to do, and what types of resources will be involved. Design - Actually design my API and underlying schema using an OpenAPI definition. Mock - Publish a mock representation of my API, complete with virtualized data behind responses. Document - Make sure and publish documentation to a simple developer portal. Plan - A overview of what it will cost, and the model for managing consumption. Testing - Set up tests to ensure each API is up and doing what it was intended to do. Communicate - Actually write stories, and communicate around what an API does. Support - Establish and maintain feedback loops around the API to listen to consumers. Road Map - Gather feedback into a road map, and plan next version, complete with a change log. In this life cycle I would never actually write any code. I would never actually setup a database backend. Everything would be mocked and virtualized, but seem as real as I possibly can. Even maybe providing different types of virtualized datasets and responses, so people can experience different types of responses or scenarios. I’d work hard to make sure each API was as complete, and function like a...[<a href="/2018/03/08/imagine-a-mock-only-api-development-reality/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/08/axway-asking-for-an-openapi-of-the-streamdata-io-api-so-they-can-screenshot-it/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/beachclouds_clean_view.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/08/axway-asking-for-an-openapi-of-the-streamdata-io-api-so-they-can-screenshot-it/">Axway Asking for an OpenAPI of The Streamdata.io API So They Can Screenshot It</a></h3>
			<p><em>08 Mar 2018</em></p>
			<p>We are working closely with Axway on a number of projects over here at Streamdata.io. After we got out of a meeting with their team the other day we received an email from them asking if we had an OpenAPI definition for a demo Streamdata.io market data API. They were wanting to include it in some marketing materials, and needed a screenshot of it. To be able to generate the visual they desired, they needed an OpenAPI to make it tangible enough for capturing in a screenshot and presenting as part of a larger story. This may sound like a pretty banal thing, but when you step back and realize the importance of OPenAPI when it comes to communication, and making something very abstract a tangible, visual thing, it becomes more significant. You can tell someone there is a market data API, but taking a screenshot of documentation generated via an OpenAPI which displays the market data paths, a couple of parameters like stocker ticker symbol and maybe date range, and then plug in some actual values like the ticker symbol for AAPL, and show the JSON response takes things to a new level. This is OpenAPI empowered storytelling, marketing and communications in my book. Elevating what OpenAPI brings to the table to new stops along the API life cycle. This isn’t just about documentation. This is about making an abstract API concept more visual, more meaningful, and able to be captured in an image. Axway is trying to demonstrate the value of their API solutions, coupled potentially with Streamdata.io services, in a single image–providing a lot more rich context, and visualizations that amplify their marketing materials. This isn’t just documenting what is going on so that developers know what to do with an API, this is telling stories so that business users understanding what is possible with an API–using a machine readable format like OpenAPI to help deliver the 1000 words the image will...[<a href="/2018/03/08/axway-asking-for-an-openapi-of-the-streamdata-io-api-so-they-can-screenshot-it/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/07/where-do-we-start-the-api-conversation-at-our-large-organization/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cityscape_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/07/where-do-we-start-the-api-conversation-at-our-large-organization/">Where Do We Start The API Conversation At Our Large Organization?</a></h3>
			<p><em>07 Mar 2018</em></p>
			<p>It can be tough to know where to start with APIs at your large organization. Everyone is already in motion, in a variety of groups, geographic regions, serving different lines of business. The API conversation is already occurring across your organization, it is just happening in a distributed and fragmented way, without any coherent orchestration, and strategy moving it forward in unison. I’d like to sit down with your team(s), and understand what APIs look like across your organization, and help work through a first draft of your API strategy, and define how we can better orchestrate existing efforts, and begin establishing an API governance framework for bringing the API vision into focus. Let’s put together a schedule for me to sit down with a variety of teams in a partial day, full day, or multi-day series of workshops. Where we can work to define a more comprehensive, and coherent strategy for APIs. One that is rooted in what is already happening, but works to brig together the disparate voices from each team into a single conversation about how APIs can make change across your organization in a more consistent, and meaningful way. Let’s sit down and begin to define what APIs mean at your organization and map of what is already occurring, and develop framework that will allow us to begin to move things forward. Define Objectives I’d like to learn more about the objectives behind your investment in an organization-wide API strategy, and understand more around the motivations, vision behind what you are looking to achieve. I’d like to cut through the hype, and get to the real reasons you want to be doing APIs. Internal - What benefits should APIs bring to internal teams within the organization? Partner - How will APIs be used to further partner engagements, and maximize their deliverables? Public - What can be achieved by performing APIs in the public sphere, and engaging 3rd party developers, and media?...[<a href="/2018/03/07/where-do-we-start-the-api-conversation-at-our-large-organization/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/07/us-companies-getting-ahead-of-eu-regulations/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/gargoyle-nd-paris_atari_missle.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/07/us-companies-getting-ahead-of-eu-regulations/">US Companies Getting Ahead Of EU Regulations</a></h3>
			<p><em>07 Mar 2018</em></p>
			<p>I find it to be a telling sign of the culture at US companies when it comes to their response, or lack of response to EU regulations. I’ve been curating stories from the API providers that I track on when it comes to GDPR or PSD2, keeping a notebook or research that is ready when I get around to diving in deeper. The companies who are openly talking about these regulations and being proactive about responding to them, are usually the API providers who have a strong stance in the US market, and are poised to, or already expanding this reach to Europe. Companies who haven’t made any noise are probably not concerned with the European market, or just hoping the regulations fizzle out I guess? After diving into my curation notebook the first company to stand out is Auth0, with a variety of blog posts, and resources on navigating both PSD2 and GDPR. Auth0 is in a good position to provide critical authentication and user information management APIs to other companies who are working to comply with the regulations, so it makes sense that they would be getting ahead of all of this. I fully grasp that many companies are simply issuing their press releases stating they can help with GDPR or PSD2, but you can quickly cut through the fluff by looking at how much they’ve invested in their response, materials, and services. Auth0 has a pretty extensive knowledge-base on GDPR, providing PSD2 guidance on their blog, as well as investing in their EU region for a couple of years–demonstrating it is more than just a press release. I’m working my way through the list of US API providers, and service providers who are being active on the subject of EU regulations. I feel it is an important sign of the strength of the company, and demonstrates a healthy understanding of how regulations aren’t always bad, and that they can actually help industries...[<a href="/2018/03/07/us-companies-getting-ahead-of-eu-regulations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/06/explaining-api-security-to-organizational-leadership/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/castle-on-hill-edinburgh_propaganda_leaflets.JPG" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/06/explaining-api-security-to-organizational-leadership/">Explaining API Security To Organizational Leadership</a></h3>
			<p><em>06 Mar 2018</em></p>
			<p>I’ve been tasked with helping explain API security to senior leadership, and wanted to work through my ideas here on the blog. For this audience, I’m not going to get down into the weeds regarding the technical specification behind OAuth, and other approaches, and try to keep things high level, introducing folks to the art that is API security. The phrase API security represents a balance of concepts because APIs are by nature about providing access, while security is about controlling and sometimes limiting access, resulting in a new way of getting business done on the open web. First, What Are APIs? APIs are not the latest trend, or vendor solution, they are the next evolution in the web. Web sites and applications return HTML via a URL, and meant to display information to humans in a browser, while APIs return JSON or XML of the same information, but meant to be used in other applications and systems. API security is designed to allow access to our digital resources using the web, while also securing it in a way to ensure only the intended audience is able to obtain access. APIs are designed to securely provide access to data, content, media, and algorithms using the same web that us humans use to access information online via our browsers. Access Using Secure URLs APIs use web URLs get read and write data, content, media, and to allow engagement with algorithms. If you want a list of press releases, you visit https://api.example.com/press/. If you want a list of contacts from the CRM, you visit https://api.example.com/contacts/. The URL for all API resources should be encrypted by default, protecting all requests and responses in transit. Providing the first layer of security for APIs, ensuring only approved consumers can view data, content, media, and valuable algorithms being transmitted online. Registration Always Required For APIs A common misconception about web APIs is that they are all like Twitter, and are publicly...[<a href="/2018/03/06/explaining-api-security-to-organizational-leadership/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/"><img src="https://s3.amazonaws.com/kinlane-productions2/capital-one/capital-one-banking-home-page.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/">Capital One DevExchange Provides An Important Banking API Blueprint</a></h3>
			<p><em>06 Mar 2018</em></p>
			<p>When you take a look at the banking API landscape in the United States, there is one clear leader in the game–Capital One. Their DevExchange program is miles ahead of every one of their competitors, giving them a significant head start when it comes to the banking API economy. Their approach to delivering APIs meets all of my minimum requirements for any successful API platform, and even exceeds it, providing what I’d consider to be a leading example blueprint that all banking API providers should be following. The Capital One DevExchange begins as any API operation should, with a dedicated portal located at developer.[domain]: developer.capitalone.com After landing on the home page for the Capital One DevExchange you get everything you need to get up and running with the APIs they have: Getting started Authentication Documentation Errors Login Registration The Capital One DevExchange provides four main groups of public APIs currently, started with access to account information in the following areas: Retrieve account products - /deposits/account-products (GET) Retrieve account product details - /deposits/account-products/{productId} (GET) Create new account application - /deposits/account-applications (POST) Retrieve out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (GET) Answer out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (PUT) Retrieve account application details - /deposits/account-applications/{applicationId} (GET) As well as some credit card offers, showcasing the products they have available: Retrieve product listings - /credit-offers/products (GET) Retrieve card products - /credit-offers/products/cards (GET) Retrieve card products - /credit-offers/products/cards (GET) Retrieve card products by type - /credit-offers/products/cards/{cardType} (GET) Retrieve card product details - /credit-offers/products/cards/{cardType}/{productId} (GET) Which you can actually sign up for and do a pre-qualification via APIs: Create prequalification check - /credit-offers/prequalifications (POST) Create prequalification acknowledgment - /credit-offers/prequalifications/{prequalificationId} (POST) Create applicant key - /credit-offers/applicant-details POST Get access to Capital One rewards via APIs: Retrieve rewards accounts - /rewards/accounts (GET) Retrieve rewards account details - /rewards/accounts/{rewardsAccountReferenceId} (GET) And details about merchants involved in transactions: Retrieve merchant data - /merchant-insights/merchants (GET) Refresh merchant details - /merchant-insights/merchants/{merchantId} (GET) This version of the API...[<a href="/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/docks_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud/">The Need for Standardized API Plans and Pricing to Compete with Cloud</a></h3>
			<p><em>05 Mar 2018</em></p>
			<p>Google launched their Cloud Billing Catalog API, providing access to thee pricing for their cloud API catalog the other day. Azure has their billing API, and AWS has their cost explorer API service, showing that programmatic access to what API resources cost, as well as management of usage, billing, invoicing, and other aspects of doing business with APIs is becoming the normal mode of operating an API platform. I’ve long used AWS, Google, and increasingly Azure as a blueprint for what us smaller API providers should be doing. They are full of positive and negative lessons for any API provider. However, I’m starting to see what they are doing as not just a blueprint, but potentially something that will force many of us API providers out of businesses if we cannot emulate what they are doing at scale. The tractor beam that is the cloud providers is strong. They bring a lot of benefits to the table. So much so, it is getting harder and harder for independent API providers to compete. Offering benefits to consumers that will become deal breakers with using other 3rd party API providers services, pushing API consumers to stay within their chosen cloud platform walled garden. As a developer, if I can programmatically manage the plans, pricing, and billing for ALL the APIs I use via AWS, Google, and Azure, but I have to manually manage this across many different 3rd party providers, I am going to be hesitant when it comes to adopting any new services that aren’t within my cloud domain. As I depend on more APIs, the benefits of being able to programmatically manage the business of my API consumption is becoming increasingly critical. If the individual 3rd party API providers I use don’t begin to offer APIs for managing the business of my API consumption, and adopting a standardized interface across all the APIs I depend on, I’m going to favor my cloud native API solutions...[<a href="/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/death-valley-national-park_dali_three_just_road.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and/">Serverless, Like Microservices Is About Understanding Our Dependencies And</a></h3>
			<p><em>05 Mar 2018</em></p>
			<p>I am not buying into all the hype around the recent serverless evolution in compute. However, like most trends I study, I am seeing some interesting aspects of how people are doing it, and some positive outcomes for teams who are doing it sensibly. I am not going all in on serverless when it comes to deploying or integrating with APIs, but I am using it for some projects, when it makes sense. I find AWS Lambda to be a great way to get in between the AWS API Gateway and backend resources, in order to conduct a little transformation. Keeping serverless as just one of many tools in my API deployment and integration toolbox, yet never going overboard with any single solution. I put serverless into the same section of my toolbox as I do microservices. Serverless is all about decoupling how I deploy my APIs, helping me keep things doing small, meaningful things behind each of my API paths. Serverless forces me to think through how I am decoupling my backend, and pushes me to identify dependencies, acknowledge constraints, and get more creative in how I write code in this environment. To do one thing, and do it well, I need an intimate understanding of where my data and other backend resources are, and how I can distill a unit of compute down to the smallest unit as I possibly can. Identifying, reducing, and being honest about what my dependencies are is essential to serverless working or not working for me. The challenge I’m having with serverless at the moment is that it can be easy to ignore many of the benefits I get from dependencies, while just assuming all dependencies are bad. Specifically around the frameworks I deploy as part of my API solutions. When coupled with AWS API Gateway this isn’t much of a problem, but all by itself, AWS Lambda doesn’t have all fo the other HTTP, transformation, header, request,...[<a href="/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks-when-it-comes-to/"><img src="https://s3.amazonaws.com/kinlane-productions2/legos/lego-millenium-falcon-instructions.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks-when-it-comes-to/">People Seem to Want Lego Kits and Not a Bucket of Lego Blocks When It Comes To</a></h3>
			<p><em>05 Mar 2018</em></p>
			<p>While I wish everyone saw the modular potential of APIs and microservices like I do, I’ve come to the realization that most people are just interested in ready to go kits that walk them through every detail of doing APIs, rather than actually playing, learning, evolving, and learning to be productive with a big bucket of APIs. I’m not just focusing on business users here, I am talking about a significant portion of the developers I come across, who really don’t seem that interested in learning to apply API concepts, and understanding when and where to use them, they just want a set of instructions that walk them through each step of deploying an API. I actually am a proponent of there being more boxed, lego style API kits that teach you how to build the product API, task API, press release API, and other common implementations. Robust, detailed, ready-to-go API implementations that would walk people through each and every step of defining, designing, deploying, managing, monitoring, testing, and documenting their API. I feel like this would significantly help folks think through what are healthy API practices, and be introduced to different ways of thinking around APIs. However, I do not want people to become reliant on only being able to operate within this paradigm, and not actually be able to fix, evolve, and deliver their own custom API solutions, using the healthy practices they are being introduced to. People seem to just want shortcuts, and things done for them. People want the solutions packaged and delivered to their doorstep. They seem unable to be able to find the solutions on their own, or even be able to absorb a lesson delivered via a packaged solution. I’m not sure what the cure for this condition is. It is hard to tell whether it is vendor induced, or (would be) API provider induced. Have people been conditioned by vendors to just be spoon fed solutions? Or...[<a href="/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks-when-it-comes-to/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/05/an-openapidriven-api-governance-rules-engine/"><img src="https://s3.amazonaws.com/kinlane-productions2/rules/9968073905_95ce575233_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/05/an-openapidriven-api-governance-rules-engine/">An OpenAPI-Driven, API Governance Rules Engine</a></h3>
			<p><em>05 Mar 2018</em></p>
			<p>Phil Sturgeon (@philsturgeon) alerted me to a pretty cool project he is cooking up, called Speccy. Which provides a rules engine for validating your OpenAPI definitions. “Taking off from where Mike Ralphson started with linting in swagger2openapi, Speccy aims to become the rubocop or eslint of OpenAPI”, and to “sniff your files for potentially bad things. “Bad” is objective, but you’ll see validation errors, along with special rules for making your APIs better.” Helping make sure your API definitions are as consistent as they possibly can be, and deliver on your API governance strategy (you have one right?) With Speccy, there are a default set of rules, things like ensuring you have a summary or a description for each API path: { "name": "operation-summary-or-description", "object": "operation", "enabled": true, "description": "operation should have summary or description", "or": ["summary", "description"] } Or making sure you add descriptions to your parameters: { "name": "parameter-description", "object": "parameter", "enabled": true, "description": "parameter objects should have a description", "truthy": "description" } Or making sure you include tags for each aPI path: { "name": "operation-tags", "object": "operation", "enabled": true, "description": "operation should have non-empty tags array", "truthy": "tags", "skip": "isCallback" } Then you can get more strict by requiring contact information: { "name": "contact-properties", "object": "contact", "enabled": true, "description": "contact object should have name, url and email", "truthy": [ "name", "url", "email" ]&lt;br /&gt; } And make sure youi have a license applied to your API: { "name": "license-url", "object": "license", "enabled": true, "description": "license object should include url", "truthy": "url" } Speccy is available as a Node package, which you can easily run at the command line. Speccy is definitely what is needed out there right now, helping us validate the growing number of OpenAPI definitions in our life. As many companies are thinking about how they can apply API governance across their operations, they should be looking at contributing to Speccy. It is something I’ve been talking with API service...[<a href="/2018/03/05/an-openapidriven-api-governance-rules-engine/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/"><img src="https://s3.amazonaws.com/kinlane-productions2/schema-org/schema-org.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/">Thoughts On The Schema.Org WebAPI Type Extension</a></h3>
			<p><em>02 Mar 2018</em></p>
			<p>I&rsquo;m putting some thought into the Schema.Org WebAPI Type Extension proposal by Mike Ralphson (Mermade Software) and Ivan Goncharov (APIs.guru), to &ldquo;facilitate better automatic discovery of WebAPIs and associated machine and human-readable documentation&rdquo;. It&rsquo;s an interesting evolution in how we define APIs, in terms of API discovery, but I would also add potentially at &ldquo;execute time&rdquo;. Here is what a base WebAPI type schema could look like: Then the proposed extensions could include the following: versions (OPTIONAL array of thing -&gt; Property -&gt; softwareVersion). It is RECOMMENDED that APIs be versioned using [semver] entryPoints (OPTIONAL array of Thing -&gt; Intangible -&gt; EntryPoint) license (OPTIONAL, CreativeWork or URL) - the license for the design/signature of the API transport (enumerated Text: HTTP, HTTPS, SMTP, MQTT, WS, WSS etc) apiProtocol (OPTIONAL, enumerated Text: SOAP, GraphQL, gRPC, Hydra, JSON API, XML-RPC, JSON-RPC etc) webApiDefinitions (OPTIONAL array of EntryPoints) containing links to machine-readable API definitions webApiActions (OPTIONAL array of potential Actions) The webApiDefinitions (EntryPoint) contentType property contains a reference to one of the following conten types: OpenAPI / Swagger in JSON - application/openapi+json or application/x-openapi+json OpenAPI / Swagger in YAML - application/openapi RAML - application/raml+yaml API Blueprint in markdown - text/vnd.apiblueprint API Blueprint parsed in JSON - application/vnd.refract.parse-result+json API Blueprint parsed in YAML - application/vnd.refract.parse-result+yaml Then the webApiActions property brings a handful of actions to the table, with the following being suggested: apiAuthentication - Links to a resource detailing authentication requirements. Note this is a human-readable resource, not an authentication endpoint apiClientRegistration - Links to a resource where a client may register to use the API apiConsole - Links to an interactive console where API calls may be tested apiPayment - Links to a resource detailing pricing details of the API I fully support extending the Schema.org WebAPI vocabulary in this way. It adds all the bindings needed to make the WebAPI type executable at runtime, as well as it states at discovery time. I like the transport and protocol...[<a href="/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/vancouver_diego_rivera1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/">An Observable Industry Level Directory Of API Providers And Consumers</a></h3>
			<p><em>01 Mar 2018</em></p>
			<p>I’ve been breaking down the work on banking APIs coming out of Open Banking in the UK lately. I recently took all their OpenAPI definitions and published as a demo API developer portal. Bringing the definitions out of the shadows a little bit, and showing was is possible with the specification. Pushing the project forward some more today I published the Open Banking API Directory specification to the project, showing the surface area of the very interesting, and important component of open banking APIs in the UK. The Open Banking Directory provides a pretty complete, albeit rough and technical approach to delivering observability for the UK banking industry API ecosystem actor layer. Everyone involved in the banking API ecosystem in UK has to be registered in the directory. It provides profiles of the banks, as well as any third party players. It really provides an unprecedented, industry level look at how you can make API ecosystems more transparent and observable. This thing doesn’t exist at the startup level because nobody wants to be open with the number of developers, or much else regarding the operation of their APIs. Making any single, or industry level API ecosystem, operate as black boxes–even if they claim to be an “open API”. Could you imagine if API providers didn’t handle their own API management layer, and an industry level organization would handle the registration, certification, directory, and dispute resolution between API providers and API consumers? Could you imagine if we could see the entire directory of Facebook and Twitter developers, understand what businesses and individuals were behind the bots and other applications? Imagine if API providers couldn’t lie about the number of active developers, and we knew how many different APIs each application developers used? And it was all public data? An entirely different API landscape would exist, with entirely different incentive models around providing and consuming gAPIs. The Open Banking Directory is an interesting precedent. It’s not just...[<a href="/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/4882162452_fa3126b38d_b_spagetti_accident.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">What We Need To Be Machine Readable At API Run Time</a></h3>
			<p><em>28 Feb 2018</em></p>
			<p>I had breakfast with Mike Amundsen (@mamund) and Matt McLarty (@MattMcLartyBC) of the CA API Academy team this morning in midtown this morning. As we were sharing stories of what each other was working on, the topic of what is needed to execute an API call came up. Not the time consuming find an API, sign up for an account, figure out the terms of service and pricing version, but all of this condensed into something that can happen in a split second within applications and systems. How do we distill down the essential ingredients of API consumption into a single, machine readable unit that can be automated into what Mike Amundsen calls, “find and bind”. This is something I’ve been thinking a lot about lately as I work on my API discovery research, and there are a handful of elements that need to be present: Authentication - Having keys to be able to authentication. Surface Area - What is the host, base url, path, headers, and parameters for a request. Terms of Service - What are the legal terms of service for consumption. Pricing - How much does each API request cost me? We need these elements to be machine readable and easily accessible at discover and runtime. Currently the surface area of the API can be described using OpenAPI, that isn’t a problem. The authentication details can be included in this, but it means you already have to have an application setup, with keys. It doesn’t include new users into the equation, meaning, discovering, registering, and obtaining keys. I have a draft specification I call “API plans” for the pricing portion of it, but it is something that still needs a lot of work. So, in short, we are nowhere near having this layer ready for automation–which we will need to scale all of this API stuff. This is all stuff I’ve been beating a drum about for years, and I anticipate it...[<a href="/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/28/the-business-of-running-government-as-a-microservices-platform/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/capital-battle_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/28/the-business-of-running-government-as-a-microservices-platform/">The Business of Running Government As A Microservices Platform</a></h3>
			<p><em>28 Feb 2018</em></p>
			<p>I recently wrote a response to a recent Department of Veterans Affairs RFI which contained a section about the business of operating government as a microservices platform.I know that many folks wouldn’t make it that far in the 10K word response, so I wanted to break it out into its own post. I feel pretty strongly about the potential of decoupling how we deliver technology across government, but for this to be successful we are also going to have to decouple the business and politics of it all as well. This post reflects my current research and thinking about the business of APIs in government, and is part of some ongoing work I am doing around API management, public data, and how we begin to think differently about how government engages with the public in a digital age. There are many interpretations of what is a microservice, but for the purposes of this post, it is a simple set of APIs that meet one precise set of government services. The API definition, database, back-end code, management layer, documentation, support and all other essential elements are self-contained, and usually stored in a single Github, or Bitbucket repository, when delivering microservices. Each microservice possesses its own technical, business, and political contract, outline how the service will be delivered, managed, supported, communicated, and versioned. These contracts can be realized individually, or grouped together as a larger, aggregate contract that can be submitted, while still allowing each individual service within that contract to operate independently. Decoupling The Business Of Delivering Government Digital Services The microservices approach isn’t just about the technical components. It is about making the business of delivering vital government services more modular, portable, and scalable. Something that will also decouple and shift the politics of delivering critical services to veterans. Breaking things down into much more manageable chunks that can move forward independently at the contract level. Helping both simplify, and streamline the deliver of services...[<a href="/2018/02/28/the-business-of-running-government-as-a-microservices-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/28/an-open-banking-api-portal-blueprint/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-banking/open-banking-accounts-documentation-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/28/an-open-banking-api-portal-blueprint/">An Open Banking API Portal Blueprint</a></h3>
			<p><em>28 Feb 2018</em></p>
			<p>I have been learning all about the banking API efforts out of Open Banking in the UK lately. They are evolving a set of read / write account and transaction API, as well as public data APIs for some of the common information 3rd party developers are looking to get their hands on. I'm intrigued with the traction the organization has gotten, and I want to be able to fully understand what they are developing, as well help contribute where I can. To help me understand the API specification, as well as hopefully contribute to the conversation, I am publishing an blueprint API portal for the API. It is a demo portal, running on Github, which uses the API Evangelist graphical look, but I am also publishing documentation for v1.1.1 of the account and payments API, as well as v2.1 of the public data APIs. I'm looking to publish the API specification like any bank would, but it won't actually be a live API--yet. I'd like to turn it into a mock API, with some virtualized data to demonstrate what is possible. I've only had time to publish the overview of the project, and the documentation for each current version. I have a todo list of things I would like to invest in when I have more time. Eventually, I want it to be a complete, forkable Open Banking API portal that any bank in the UK could publish. Then I'm looking to create country specific versions to help push French, German, and other banks to push a portal. It doesn't have to be my solution that the banks use, but hopefully they'll at least use what I have provided as a blueprint. The goal isn't just to get them to use the portal, it is to get them implementing their bank's API developer portal in a standardized way--similar to the API specification from Open Banking, but this is the portal specification. I will spend...[<a href="/2018/02/28/an-open-banking-api-portal-blueprint/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/27/what-is-the-streamdata-io-api-gallery/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/27/what-is-the-streamdata-io-api-gallery/">What Is The Streamdata.io API Gallery?</a></h3>
			<p><em>27 Feb 2018</em></p>
			<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation. What is the Streamdata.io API Gallery? It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas. What are we looking to accomplish with the Streamdata.io API Gallery? Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible. What is the opportunity around the Streamdata.io API Gallery? Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow. Why is this relevant to Streamdata.io, and their road map? It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which...[<a href="/2018/02/27/what-is-the-streamdata-io-api-gallery/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_146_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting/">Three Areas I Would Like To Cover When We Sit Down For An API Consulting</a></h3>
			<p><em>27 Feb 2018</em></p>
			<p>I’m putting together some presentations for a handful of upcoming engagements, where I’m wanting to help my audience understand what an initial engagement will look like. While I am looking to have just a handful of bullets that can live on a single, or handful of slides, I also want a richer narrative to go along with it. To achieve this I rely on my blog, which helps me work my way through the details of what I do, and distill things down into something that I can deliver on the ground within the companies, organizations, institutions, and government agencies I am conducting business with. When I am sitting down with a new audience, and working to help them understand how I can help them begin, jumpstart, revive, and move forward with their API journey, I’m usually breaking things into three main areas: Landscape Mapping - Establish a map of what currently is within an organization. Internal Resources - What existing web services, APIs, teams, and resources exist? External Objectives - What are the external objectives of doing APIs? Strategy Development - Craft a coherent strategy for moving forward with APIs. API Lifecycle - Lay out a step by step list of stops along a modern API life cycle. API Support - Identify how the strategy and operations will be supported within an organization. API Evangelism - Consider how the message around API operations will spread internally, and externally. Execution - Identify a clear set of next steps regarding how APIs will evolve. Infrastructure - What services, tooling, and other API infrastructure is needed? Resources - What resources have been identified for moving the API conversation forward? Governance - What is the governance strategy for measuring, reporting upon, and enforcing the deliver of APIs across the API lifecycle presented. When I present to a new group of people within an organization, this is the outline I am looking to flesh out. I have to understand...[<a href="/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/27/mapping-out-the-api-landscape/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/27/mapping-out-the-api-landscape/">Mapping Out The API Landscape</a></h3>
			<p><em>27 Feb 2018</em></p>
			<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation. What is the Streamdata.io API Gallery? It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas. What are we looking to accomplish with the Streamdata.io API Gallery? Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible. What is the opportunity around the Streamdata.io API Gallery? Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow. Why is this relevant to Streamdata.io, and their road map? It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which...[<a href="/2018/02/27/mapping-out-the-api-landscape/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/26/the-banking-api-actors-in-the-uk/"><img src="https://s3.amazonaws.com/kinlane-productions2/shakespeare/shakespeare.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/26/the-banking-api-actors-in-the-uk/">The Banking API Actors In The UK</a></h3>
			<p><em>26 Feb 2018</em></p>
			<p>I’ve been profiling the work of the Open Banking Implementation Entity when nit comes to banking API standards in the UK. As part of my getting up to speed on the banking ecosystem in the UK, and Europe, I’ve been posting a series of small blog posts, outlining different aspects of how things work, and who the players are. While going through the Open Banking documentation, I came across a great list of the “actors” int he Open Banking API ecosystem, which taught me a lot about who is involved, and was worth reposting here as a list. The Open Banking eco-system consists of a number of actors, which may be a natural person or an entity: Payment Service User (PSU) - Person - Payment Services User is a natural or legal person making use of a payment service as a payee, payer or both Payment Service Provider (PSP) - Legal Entity - A legal entity (and some natural persons) that provide payment services as defined by PSD2 Article 4(11) Account Servicing Payment Service Provider (ASPSP) - Legal Entity - Account Servicing Payment Service Providers provide and maintain a payment account for a payer as defined by the PSRs and, in the context of the Open Banking Ecosystem are entities that publish Read/Write APIs to permit, with customer consent, payments initiated by third party providers and/or make their customers’ account transaction data available to third party providers via their API end points. Third Party Providers / Trusted Third Parties (TPP) - Legal Entity - Third Party Providers are organisations or natural persons that use APIs developed to Standards to access customer’s accounts, in order to provide account information services and/or to initiate payments. Third Party Providers are either/both Payment Initiation Service Providers (PISPs) and/or Account Information Service Providers (AISPs). Payment Initiation Service Provider (PISP) - Legal Entity - A Payment Initiation Services Provider provides an online service to initiate a payment order at the request...[<a href="/2018/02/26/the-banking-api-actors-in-the-uk/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/"><img src="https://s3.amazonaws.com/kinlane-productions2/starling/starling-home-page.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/">One Of The Best API Getting Started I Have Come Across</a></h3>
			<p><em>26 Feb 2018</em></p>
			<p>
I’m working my way through banking and Fintech companies in the UK, and I stumbled across the Starling banking API. I began doing my usual clicking around as I do with any API, looking at the documentation, the getting started, and other primary links. After landing on the Starling getting started page, I have to say that it is the single best example of a getting started page I have ever come across in my time as API Evangelist. It is robust, informative, well laid out, and has everything you need to well, get started.

The Starling getting started page is broken up into six separate sections:

1) Register Your Application
2) Setup Starter Kit
3) Play in the Sandbox
4) Personal Access
5) Going Live
6) Contact Us

Each getting started section has a simple, concise description with relevant visuals and code samples, as well as possession simple action buttons, like sign, login, register application, and the other meaningful things you need to get started. The Starling getting started is going to become my go to example of how to create an API getting started page. You can really tell whoever put it together spent a lot of time refining it, and walking through it until it was 100% complete.

Starling even has a sandbox, marketplace, and a join Slack button. I can’t rave about their approach enough. I’m going to turn it into a case study regarding how to create a getting started page, and showcase on the home page of the site. I wish every API put as much energy into their getting started page as Starling has. It would take the friction out of on-boarding APis, and make it a much more pleasant experience.

[<a href="/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/24/round-two-of-the-department-of-veterans-affairs-lighthouse-platform-rfi/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_113_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/24/round-two-of-the-department-of-veterans-affairs-lighthouse-platform-rfi/">Round Two Of The Department of Veterans Affairs Lighthouse Platform RFI</a></h3>
			<p><em>24 Feb 2018</em></p>
			<p>I’m spending some more time thinking about APIs at the Department of Veterans Affairs (VA), in response to round two of their request for information (RFI). A couple months back I had responded to an earlier RFI, providing as much information as I could think of, for consideration as part of their API journey. As a former VA employee, and son of two Vietnam Vets (yes two), you can say I’m always willing to invest some in APIs over at the VA. To provide a response, I have taken the main questions they asked, broken them out here, and provided answers to the best of my ability. In my style, the answers are just free form rants, based upon my knowledge of the VA, and the wider API space. It is up to the VA, to decide what is relevant to them, and should be included in their agency API strategy. 2. Current Scope While the acquisition strategy for Lighthouse has not yet been formalized, VA envisions that the program will consist of multiple contracts. For example, a contract for recommending policy and standards to form governance would likely be separate from an API build team. The key high level activities below are anticipated to be included within these contracts, and VA is requesting feedback from industry on how these activities should be aligned between multiple contracts. The list below is not inclusive of all tasks required to support this program. Additionally, VA intends to provide the IAM solution and the provisioning of necessary cloud resources to host the proposed technology stack. VA’s current enterprise cloud providers are Microsoft Azure and Amazon Web Services. Microservice Focused Operational &amp; Implementation Lighthouse should embrace a microservices way of doing things, so that the platform can avoid legacy trappings when it comes to delivering software at the VA, which have resulted in large, monolithic systems, possessing enormous budgets, and entrenched teams, that are able to develop a resistance...[<a href="/2018/02/24/round-two-of-the-department-of-veterans-affairs-lighthouse-platform-rfi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/22/aws-iamlike-policies-for-aws-api-gateway-and-marketplace-billing/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/server-cloud1_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/22/aws-iamlike-policies-for-aws-api-gateway-and-marketplace-billing/">AWS IAM-Like Policies For AWS API Gateway And Marketplace Billing</a></h3>
			<p><em>22 Feb 2018</em></p>
			<p>The primary reason I’ve been adopting more AWS solutions as part of my API stack, and using tools I have historically felt lock me into the AWS ecosystem, is the available of AWS identity and access management (IAM). I just cannot deliver secure at this level as a small business owner, and their robust solution lets me dial in exactly what I need when it comes to defining who has access to what across my API infrastructure. I can define different policies, and apply them at the API management layer using both AWS Lambda and AWS API Gateway. Keeping everything separated, yet with a single API stack as the point of entry, for all consumers and applications. I want all of this security goodness, but for the business of my APIs. Similar to the engine that drives the relationship between me as an AWS Marketplace user and AWS, I want a framework for applying business policies at the plan level within AWS API Gateway. I want to determine who has access to which resources, as well as what they can use, but I want to be able to meter this usage, and charge different rates. Compute, storage, and bandwidth for my partners is different than for retail API consumers, with a mix of resource and API call based metrics. The AWS monetization policies would reflect the AWS Marketplace framework, giving me a mix of metering and contract based billing, reflecting single or multi-dimensional usage across the eight areas of consumption they support currently. I want to be able to establish common monetization policies across all my microservices, and allow product managers to implement them consistently at scale using AWS API Gateway. Like security, these API product managers shouldn’t be experts in the economics of the services being offered, they should just be able to apply from a common pool of business policies, and provide feedback on how to evolve, when appropriate. This concept is very...[<a href="/2018/02/22/aws-iamlike-policies-for-aws-api-gateway-and-marketplace-billing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/21/what-is-open-banking-in-the-uk/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-banking/open-banking-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/21/what-is-open-banking-in-the-uk/">What Is Open Banking In The UK?</a></h3>
			<p><em>21 Feb 2018</em></p>
			<p>I am profiling banks in the UK as part of an effort move forward my API Stack work, and populate the Streamdata.io API Gallery. One significant advantage that banks in the UK have over other countries in the EU, and even in the US, is the help of Open Banking. To help profile the organization, I’ll just borrow from their website to define who they are and what they do. The Open Banking Implementation Entity was created by the UK’s Competition and Markets Authority to create software standards and industry guidelines that drive competition and innovation in UK retail banking. In 2016, The Competition and Markets Authority (CMA) published a report on the UK’s retail banking market which stated that older, larger banks do not have to compete hard enough for customers’ business, and smaller and newer banks were finding it difficult to grow and access the UK banking market. To solve this problem, they proposed a number of remedies including Open Banking, which defines API standards that are intended to help level that playing field. The role of Open Banking is to: Design the specifications for the Application Programming Interfaces (APIs) that banks and building societies use to securely provide Open Banking Support regulated third party providers and banks and building societies to use the Open Banking standards Create security and messaging standards Manage the Open Banking Directory which allows regulated participants like banks, building societies and third party providers to enroll in Open Banking Produce guidelines for participants in the Open Banking ecosystem Set out the process for managing disputes and complaints This approach to standardizing API definitions is the type of leadership that is needed to move API conversation forward in ALL industries. I know in the US, many enjoy viewing regulations as always bad, but this type of organizational designation can go a long way towards moving an industry forward in a concerted fashion. Doing the hard work to establish a...[<a href="/2018/02/21/what-is-open-banking-in-the-uk/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/"><img src="https://s3.amazonaws.com/kinlane-productions2/sabre/sabre-travel-signup-keys.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/">Provisioning A Default App And Keys For Your API Consumers On Signup</a></h3>
			<p><em>21 Feb 2018</em></p>
			<p>
I sign up for a lot of APIs. I love anything that reduces friction when on-boarding, and allows me to begin making an API call in 1-3 clicks. I’m a big fan of API providers that allow me to signup using my Github OAuth, preventing me from having to sign up for yet another account. I’m also a big fan of providers who automatically provision an application for me as part of the signup, and have my API keys waiting for me as soon as I’ve registered.

While signing up for the Sabre travel API I saw that they provisioned my application as part of the API sign up process in a way that was worth showcasing. Saving me the time and hassle of having to add a new application after I’ve signed up. Stuff like this might seem like a pretty small detail when developing an API on-boarding process, but when you are signing up for many different APIs, and trying to manage your time–these little details add up to be a significant time saver.

Ideally, API providers would auto-provision a default application along with the signup, but I like the idea of also giving me the option to name my application while registering. When crafting your API registration flow, make sure you spend time signing up multiple times, and try to put yourself in your API consumers shoes. I even recommend signing up for an account each week, repeatedly experiencing what your consumers will be exposed to. I also recommend spending time signing up for other APIs on a regular basis, to experience what they offer–you will always surprised by what I find.

[<a href="/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/enums/bitcoin-pools.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/">An Opportunity Around Providing A Common OpenAPI Enum Catalog</a></h3>
			<p><em>21 Feb 2018</em></p>
			<p>I’m down in the details of the OpenAPI specification lately, working my way through hundreds of OpenAPI definitions, trying to once again make sense of the API landscape at scale. I’m working to prepare as many API path definitions as I possibly can to be runnable within one or two clicks. OpenAPI definitions, and Postman Collections are essential to making this happen, both of which require complete details on the request surface area for an API. I need to know everything about the path, as well as any headers, path, or query parameters that need to included. A significant aspect of this definition being complete includes default, and enum values being present. If I can’t quickly choose from a list of values, or run with a default value, when executing an API, the time to seeing a live response grows significantly. If I have to travel back to the HTML documentation, or worse, do some Googling before I can make an API call, I just went from seconds to potentially minutes or hours before I can see a real world API response. Additionally, if there are many potential values available for each API parameter, enums become critical building blocks to helping me understand all the dimensions of an API’s surface area. Something that should have been considered as part of the API’s design, but often just gets left as part of API documentation. When playing with a Bitcoin API with the following path /blocks/{pool_name}, I need to the list of pools I can choose from. When looking to get a stock market quote from an API with the following path, /stock/{symbol}/quote, I need a list of all the ticker symbols. Having, or not having these enum values at documentation, and execute time, are essential. Many of these lists of values are so common, developers take them for granted. Assuming that API consumers just have them laying around, and really aren’t worth including in documentation. You’d...[<a href="/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/21/an-open-banking-in-the-uk-openapi-template/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-banking/open-banking-openapi.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/21/an-open-banking-in-the-uk-openapi-template/">An Open Banking in the UK OpenAPI Template</a></h3>
			<p><em>21 Feb 2018</em></p>
			<p>After learning more about what Open Banking is doing for APIs in the UK, I realized that I needed an OpenAPI template for the industry specification. There are six distinct schema available as part of the project, and I wanted a complete OpenAPI to describe which paths were available, as well as the underlying response schema. I got work crafting one from the responses that were available within the Open Banking documentation.

Open Banking had schema available for their API definitions, but OpenAPI is the leading API and data specification out there today, so it makes sense that there should be an OpenAPI available, helping all participating banking API providers take advantage of all the tooling available within the OpenAPI community. To help support, I have published my Open Banking OpenAPI definition as a Github Gist:



I’ve applied this OpenAPI definition to the 17 banks they have listed, and will be including them in the next publishing of my API Stack project. Open Banking provides a common definition that can be used across many banks, and an OpenAPI template allows me to quickly apply the common template to each individual bank. Generating bank specific documentation, SDK and code samples, monitoring, tests, and other client tooling. Helping me put the valuable data being made available via each API to work.

I’d like to see more organizations like Open Banking emerge. I’d also like to help ensure they all make OpenAPI templates available for any API and schema specifications they establish. The API lifecycle is increasingly OpenAPI defined, and when you make your guidance available in the OpenAPI format, you are enabling actors within any industry to quickly get up and running with designing, deploying, managing, testing, monitoring, and almost every other stop along a modern API lifecycle. Increasing the chances of adoption for any API standards you are putting out there.

[<a href="/2018/02/21/an-open-banking-in-the-uk-openapi-template/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/20/relationship-between-openapi-path-summary-tags-and-aysncapi-topics/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_160_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/20/relationship-between-openapi-path-summary-tags-and-aysncapi-topics/">Relationship Between OpenAPI Path, Summary, Tags and AysncAPI Topics</a></h3>
			<p><em>20 Feb 2018</em></p>
			<p>I’m working my way through several hundred OpenAPI definitions that I have forked from APIs.guru, Any API, and have automagically generated from API documentation scrape scripts I have developed over time. Anytime I evolve a new OpenAPI definition, I first make sure the summary, description, and tags are as meaningful as they possibly can. Sadly this work is also constrained by how much time I have to spend with each API, as well as how well designed their API is in the first place. I have a number of APIs that help me enrich this automatically, by mining the API path, applying regular expressions, but often times it takes a manual review to add tags, polish summaries, and make the OpenAPI details as meaningful as I possibly can, in regards to what an API does. As I’m taking a break from this work, I’m studying up on AsyncAPI, trying to get my head around how I can be crafting API definitions for the message-based, event-driven, streaming APIs I’m profiling alongside my regular API research. One of the areas the AsyncAPI team is pushing forward is around the concept of a topic–_“to create a definition that suites most use cases and establish the foundation for community tooling and better interoperability between products when using AsyncAPI.”_ or to elaborate further, “a topic is a string representing where an AsyncAPI can publish or subscribe. For the sake of comparison they are like URLs in a REST API.” Now I’m thinking about the relationships between the API design elements I’m wrestling with in my API definitions, and how the path, summary, and tags reflect what Async is trying to articulate with their topics discussion. {organization}.{group}.{version}.{type}.{resources}.{event} organization - the name of the organization or company. group - the service, team or department in charge of managing the message.. version - the version of the message for the given service. This version number should remain the same unless changes in the...[<a href="/2018/02/20/relationship-between-openapi-path-summary-tags-and-aysncapi-topics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64_116_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/">People Who Provide Enum For Their OpenAPI Definitions Are Good People</a></h3>
			<p><em>20 Feb 2018</em></p>
			<p>I’m processing a significant amount of OpenAPI definitions currently, as well as crafting a number of them from scraped API documentation. After you work with a lot of OpenAPI definitions, aiming to achieve a specific objective, you really get to know which aspects of the OpenAPI are the most meaningful, and helpful when they are complete. I talked about the importance of summary, description, and tags last week, and this week I’d like to highlight how helpful it is when the stewards of OpenAPI definitions include enum values for their parameters, and I think they are just good people. ;-) Enums are simply just a list of potential values for each of the parameters you outline as part of your API definition. So if you have state as a parameter for use in the request of your API, you have a list of the 50 US states as the enum. If you the parameter is color, you have just the color black, because we all know it is the only color(all the colors). ;-) If you provide a parameter that will accept a standard set of inputs, you should consider providing an enum list to help your consumers understand the potential for that parameter. Outlining the dimensions of the parameter in a simple JSON or YAML array of every single possible value. I can’t articulate how many times I have to go looking for a list of values. Sometimes it is present within the description for the OpenAPI, but often times I have to go back to the portal for the API, and follow a link to a page that lists out the values. That is, if an API provider decides to provide this information at all. The thoughtful ones do, the even more thoughtful ones put it in their OpenAPI definitions as enum values. Anytime I come across a list of enums that I can quickly build an array, select, and other common aspects...[<a href="/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/16/using-jekyll-and-openapi-to-evolve-my-api-documentation-and-storytelling/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/68_158_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/16/using-jekyll-and-openapi-to-evolve-my-api-documentation-and-storytelling/">Using Jekyll And OpenAPI To Evolve My API Documentation And Storytelling</a></h3>
			<p><em>16 Feb 2018</em></p>
			<p>I’m reworking my API Stack work as independent sets of Jekyll collections. Historically I just dumped all APIs.json, and OpenAPIs into the central data folder, and grouped them into folders by company name. Now I am breaking them out into tag based collections, using a similar structure. Further evolving how I document and tell stories using each API. I have been published a single OpenAPI for each platform, but now I’m publishing a separate OpenAPI for each API path–we will see where this goes, it might ultimately end up biting me in the ass. I’m doing this because I want to be able to talk about a single API path, and provide a definition that can be viewed, interpreted, and executed against, independent of the other paths–Jekyll+OpenAPI is helping me accomplish this. With each API provider possessing its own APIs.json index, and each API path having its own OpenAPI definition, I’m able to mix up how I document and tell stories around these APIs. I can list them by API provider, or by individual API path. I can filter based upon tags, and provide execute-time links that reference each individual unit of API. I have separate JavaScript functions that can be referenced if the API path is GET, POST, or PUT. I can even inherit other relevant links like API sign up or terms of service as part of its documentation. I can reference all of this as part of larger documentation, or within blog posts, and other pages throughout the website–which will be refreshed whenever I update the OpenAPI definition. If you aren’t familiar with how Jekyll works. It is a static content solution, that allows you do develop collections. You can put CSV, JSON, or YAML into these collections (folders), and they become objects you can reference using Liquid syntax. So if I put Twitter’s APIs.json, and OpenAPI into a folder within my social collection, I can reference as site.social.twitter which is the...[<a href="/2018/02/16/using-jekyll-and-openapi-to-evolve-my-api-documentation-and-storytelling/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/68_113_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using/">Insecurity Around Providing Algorithmic Transparency And Observability Using</a></h3>
			<p><em>16 Feb 2018</em></p>
			<p>I’m working on a ranking API for my partner Streamdata.io to help quantify the efficiencies they bring to the table when you proxy an existing JSON web API using their service. I’m evolving an algorithm they have been using for a while, wrapping it in a new API, and applying it across the APIs I’m profiling as part of my API Stack, and the Streamdata.io API Gallery work. I can pass the ranking API any OpenAPI definition, and it will poll and stream the API for 24 hours, and return a set of scores regarding how real time the API is, and what the efficiency gains are when you use Streamdata.io as a proxy for the API. As I do this work, I find myself thinking more deeply about the role that APIs can play in helping make algorithms more transparent, observable, and accountable. My API ranking algorithm is pretty crude, but honestly it isn’t much different than many other algorithms I’ve seen companies defend as intellectual property and their secret sauce. Streamdata.io is invested in the ranking algorithm and API being as transparent as possible, so that isn’t a problem here, but each step of the process allows me to think through how I can continue to evangelize other algorithm owners to use APIs, to make their algorithms more observable and accountable. In my experience, most of the concerns around keeping algorithms secret stem from individual insecurities, and nothing actually technical, mathematical, or proprietary. The reasons for the insecurities are usually that the algorithm isn’t that mathematically sophisticated (I know mine isn’t), or maybe it is pretty flawed (I know mine is currently), and people just aren’t equipped to admit this (I know I am). I’ve worked for companies who venomously defend their algorithms and refuse to open them up, because in the end they know they aren’t defensible on many levels. The only value the algorithm possesses in these scenarios is secrecy, and...[<a href="/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/15_190_800_500_0_max_0_1_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi/">The Importance of the API Path Summary, Description, and Tags in an OpenAPI</a></h3>
			<p><em>15 Feb 2018</em></p>
			<p>I am creating a lot of OpenAPI definitions right now. Streamdata.io is investing in me pushing forward my API Stack work, where I profile API using OpenAPI, and index their operations using APIs.json. From the resulting indexes, we are building out the Streamdata.io API Gallery, which shows the possibilities of providing streaming APIs on top of existing web APIs available across the landscape. The OpenAPI definitions I’m creating aren’t 100% complete, but they are “good enough” for what we are needing to do with them, and are allowing me to catalog a variety of interesting APIs, and automate the proxying of them using Streamdata.io. I’m finding the most important part of doing this work is making sure there is a rich summary, description, and set of tags for each API. While the actual path, parameters, and security definitions are crucial to programmatically executing the API, the summary, description, and tags are essential so that I can understand what the API does, and make it discoverable. As I list out different areas of my API Stack research, like the financial market data APIs, it is critical that I have a title, and description for each provider, but the summary, description, and tags are what provides the heart of the index for what is possible with each API. When designing an API, as a developer, I tend to just fly through writing summary, descriptions, and tags for my APIs. I’m focused on the technical details, not this “fluff”. However, this represents one of the biggest disconnects in the API lifecycle, where the developer is so absorbed with the technical details, we forget, neglect, or just don’t are to articulate what we are doing to other humans. The summary, description, and tags are the outlines in the API contract we are providing. These details are much more than just the fluff for the API documentation. They actually describe the value being delivered, and allow this value to be...[<a href="/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/15/some-common-features-of-an-api-application-review-process/"><img src="https://s3.amazonaws.com/kinlane-productions2/kelly-taylor-app-approval-tweet.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/15/some-common-features-of-an-api-application-review-process/">Some Common Features Of An API Application Review Process</a></h3>
			<p><em>15 Feb 2018</em></p>
			<p>I received a tweet from my friend Kelly Taylor with USDS, asking for any information regarding establishing an “approve access to production data” for developers. He is working on an OAuth + FHIR implementation for the Centers for Medicare and Medicaid Services (CMS) Blue Button API. Establishing a standard approach for on-boarding developers into a production environment always makes sense, as you don’t want to give access to sensitive information without making sure the company, developer, and application has been thoroughly vetted. As I do with my work, I wanted to think through some of the approaches I’ve come across in my research, and share some tips and best practices. The Blue Button API team has a section published regarding how to get your application approved, but I wanted to see if I can expand on, while also helping share this information with other readers. This is a relevant use case that I see come up regularly in healthcare, financial, education, and other mainstream industries. Virtualization &amp; Sandbox The application approval conversation usually begins with ALL new developers being required to work with a sandboxed set of APIs, only providing production API access to approved developers. This requires having a complete set of virtualized APIs, mimicking exactly what would be used in production, but in a much safer, protected environment. One of the most important aspects of this virtualized environment is that there also needs to be robust sets of virtualized data, providing as much parity regarding what developers will experience when they enter the production environment. The sandbox environment needs to be as robust and reliable as the production, which is a mistake I see made over and over from providers, where the sandbox isn’t reliable, or as functional, and developers never are able to reach production status in a consistent and reliable way. Doing a Background Check Next, as reflected in the Blue Button teams approach, you should be profiling the company and...[<a href="/2018/02/15/some-common-features-of-an-api-application-review-process/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/15/how-big-or-small-is-an-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/31_156_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/15/how-big-or-small-is-an-api/">How Big Or Small Is An API?</a></h3>
			<p><em>15 Feb 2018</em></p>
			<p>I am working to build out the API Gallery for Streamdata.io, profiling a wide variety of APIs for inclusion in the directory, adding to the wealth of APIs that could be streamed using the service. As I work to build the index, I’m faced with the timeless question regarding, what is an API? Not technically what an API does, but what is an API in the context of helping people discover the API they are looking for. Is Twitter an API, or is the Twitter search/tweets path an API? My answer to this question always distills down to a specific API path, or as some call it an API endpoint. Targeting a specific implementation, use case, or value generated by a single API provider. Like most things in the API sector, words are used interchangeably, and depending on how much experience you have in the business, you will have much finer grained definitions about what something is, or isn’t. When I’m talking to the average business user, the Twitter API is the largest possible scope–the entire thing. In the context of API discovery, and helping someone find an API to stream or to solve a specific problem in their world, I’m going to resort to a very precise definition–in this case, it is the specific Twitter API path that will be needed. Depending on my audience, I will zoom out, or zoom in on what constitutes a unit of API. The only consistency I’m looking to deliver is regarding helping people understand, and find what they looking for–I’m not worried about always using the same scope in my definition of what an API is. You can see an example of this in action with the Alpha Vantage market data API I’m currently profiling, and adding to the gallery. Is Alpha Vantage is a single API, or 24 separate APIs? In the context of the Streamdata.io API Gallery, it will be 24 separate APIs. In the...[<a href="/2018/02/15/how-big-or-small-is-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/intrinio/intrinio-app-showcase.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/">A Really Nice API Application Showcase Over At The Intrinio Market Data API</a></h3>
			<p><em>15 Feb 2018</em></p>
			<p>I am profiling financial market data APIs currently, and as I’m doing my work profiling APIs, I’m always on the hunt for interesting elements of their API operations that I can showcase for my readers. While looking at the financial market data API from Intrinio, I found that I really, really like their application showcase, which providers a pretty attractive blueprint for how we can showcase what is being develop on top of our APIs.

The Intrinio application showcase is just clean looking, and has the bells and whistles you’d expect like categories, search, detail or list view, and detail pages providing you all the information you need about the application, and where you can find tutorials, code, and other relevant resources.



Another thing I really like is it isn’t just about web and mobile applications. They have spreadsheet integrations, and help walk you through how to “apply” each type of integration. This is what the application in API means to me. It isn’t always just about finished web, mobile, and device applications. It is about applying the resources available via the programmatic interfaces to some problem you have in your world.

Anyways, the Intrinio application showcase is totally worth profiling as part of my research. It is a great blueprint for other API providers to follow when crafting their own application showcases. This post give me a single URL that I can share with folks, and reference throughout my stories, white papers, guides, and talks. I’d love to see this become the standard for how API providers showcase their applications, keeping things simple, clean, and bringing value to their consumers.

[<a href="/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page5" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page7" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
