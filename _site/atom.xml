<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>API Evangelist</title>
  <updated>2019-08-16T02:00:00Z</updated>
  <link rel="self" href="https://apievangelist.com/atom.xml"/>
  <author><name>Kin Lane</name></author>
  <id>https://apievangelist.com/atom.xml</id>
	<entry>
    <title>Seeing API Consumers As Just The Other Ones</title>
    <link href="http://apievangelist.com/2019/08/16/seeing-api-consumers-as-just-the-other-ones/"/>
    <updated>2019-08-16T02:00:00Z</updated>
    <content><![CDATA[
As API providers, it can be easy to find ourselves in a very distant position from the consumers of our APIs. In recent weeks I have been studying the impacts of behavioral approaches to putting technology to work, something that has led me to the work of Max Meyer, and his Psychology of the Other-One (1921). I haven’t read his book yet, but have finished other works citing his work on how to “properly” study how animals (including humans) behave. While the psychological impact of all of this interests me, I’m most interested in how this perspective has amplified and driven how we use technology, and specifically how APIs can be used to create or bridge the divide between us (API providers) and our (API consumers).

While web and mobile technology is often portrayed as connecting and bringing people together, it also can be used to establish a separation between providers and consumers. We often get caught up in the scale and growth of delivering API infrastructure, and we forget that our API consumers are humans, and we can end up just seeing them as personas, humans, or just a demographic. Of course, as API providers, we can’t be expected to make a direct connection with every single consumer, but we also have to be wary of becoming so distant from their reality that we can’t make a connection with them at all. Leaving our products, services, and tooling something that doesn’t serve them in any way, and we fail to meet our own business objectives behind what we were building in the first place.

There will aways be some distance between API provider and consumer. However, we have to regularly work to narrow this divide, otherwise negative forces can make their way in between us and consumers. If we simply see your API consumers and end-users as the “other ones”, it will make supporting, and investing in their success much more difficult. Trust with API consumers, and the end-users of the applications they develop is tough to achieve, and even harder to maintain-—something that is increasingly more difficult when you simply see them as the other ones, those over there, and just nameless faceless database entries. It is our job as API community managers, customer success engineers, evangelists, and marketers to ensure that this all to common divide doesn’t grow between us and our consumers.

Technology has the potential to bring us together, and connect us in many new and interesting ways-—APIs are at the center of this technological evolution. However, without the proper care and attention, it also has the potential to push us further apart. Dehumanizing people along the way, and reducing them simply to database entries or just a series of transactions. As evangelists, we can’t let this happen. We have to work extra hard to get to know our consumers, and reach out to better understand who they are, what they need, and ensure we are in tune with their view of the API resources we are making available. This is something that applies to our intentional, unintentional, and malicious API consumers. The more visibility we have into who our API consumers are, and what they are up to, the more success we will have in achieving our objectives, and sensibly scaling our communities-—keeping the balance between us the API provider, and our consumers, and seeing them as more than just the “other ones”.
]]></content>
    <id>http://apievangelist.com/2019/08/16/seeing-api-consumers-as-just-the-other-ones/</id>
  </entry><entry>
    <title>Four Phases Of Internal API Evangelism</title>
    <link href="http://apievangelist.com/2019/08/16/four-phases-of-internal-api-evangelism/"/>
    <updated>2019-08-16T02:00:00Z</updated>
    <content><![CDATA[
General evangelism around what APIs are, as well as more precise advocacy around specific APIs or groups of API resources takes a lot of work, and repetition. Even as a seasoned API evangelist I can never assume my audience will receive and understand what it is that I am evangelizing, and I regularly find myself having to reassess the impact (or lack of) that I’m making, retool, refresh, and repeat my messaging to get the coverage and saturation I’m looking for. After a decade of doing this, I cannot tell which is more difficult, internal or external public evangelism, but I do find that after almost 10 years, I’m still learning from each audience I engage with—-proving to me that no single evangelism strategy will ever reliably work, and I need a robust, constantly evolving toolbox if I am going to be successful.

I have many different tools in my internal evangelism toolbox, but I find that the most important aspect of what I do is repetition. I never assume that my audience understand what I’m saying after a single session, or simply by reading one wiki page, blog post, or white paper. Quality internal evangelism requires regular delivery and enforcement, and an acknowledgement that my first engagements with teams will not have the impact I desire. When it comes to internal evangelism, I tend to encounter the following phases when engaging with internal teams:


  Silence - The first time I present material to internal teams I almost always encounter silence. Teams will often listen dutifully, but rarely will engage with me, ask questions, and want to get to the details of what is going on. I can rarely assess the state of things, and find that the silence stems from a range of emotions, ranging from not caring at all, to being very distrustful of what I am presenting. I can never assume that teams will care, trust me, and that silence is always a sign of the work that lies ahead of me, and I immediately get to work scheduling additional sessions with each team I’m trying to reach.
  Challenges - Usually by our second or third encounter with internal teams I will begin to get a little more than just silence, however it almost always comes in some aggressive form. Developers love to challenge what you are proposing, tearing things down before they ever understand what is happening. It is actually part of the natural cycle for them. Technical folks are used to taking things apart, ripping them into pieces, to see what they are all about, and what they are made of. It is easy to take this type of response in a negative and personal way, but it is the way technical minded folks see the world, especially when faced with something they do not understand, and are uncomfortable with. To survive this phase you have to have a lot of self-confidence and know your stuff, otherwise you will be eaten alive.
  Questions - After surviving the aggressive challenges about what APIs are all about and what they mean to a company, organization, institution, or government agency, I usually begin getting some more constructive questions. Moving beyond the desire to rip you to pieces, and actually start the process of actually understanding what is happening when it comes to providing and consuming of APIs. Again, you have to really know your stuff to be able to survive this line of questioning from often very smart, very technical, and inquisitive folks. However, if you come prepared, this is where you start seeing the ROI on your internal evangelism efforts.
  Engagement - I usually do not see real engagement from teams for at least 1-6 sessions. Depending on the team, they’ll take more or less time to get through the painful aspects of understanding what is going on. Depending on the type of development team, what their experience levels are, and the environments they’ve been working in, they will respond very differently. You will have to be patient to be able to reach the phase where teams actually become engaged, are able to contribute to the conversation, and take what you’ve presented and apply it to their daily work. Moving beyond just evangelism, and actually beginning to see real business value from sharing of API knowledge.


Depending on the organization culture, these four phases could take days, weeks, or months. Not all teams will be ready for what you are evangelizing. You cannot assume that teams understand what you are talking about, and that they see you as a messenger of a positive future. I’d say about 70% of the time I am seen as a hostile actor, coming in to disrupt, change ,and mess with people’s world. I don’t care how well honed my message, materials, and vision is, if I cannot connect with teams on a human and professional level–I will fail. I’ve been practicing my delivery of 101, intermediate, and advanced API material for a decade, and I still get eaten alive on a regular basis within startups, the enterprise, government agencies, and at conferences. There is no amount of preparation that will shield you from the intensity that internal development teams can bring to the table–this game isn’t for rookies.

Internal API evangelism and advocacy within the enterprise is not something you can do from high up on the mountain. You have to be able to come down from your management, architect, and executive horse, and be able to see things from the view of those in the trenches trying to get work done on a daily basis. If you can’t be seen as someone looking to build bridges between management and development ranks, your information will never be received. No amount of evangelism will cross the Grand Canyon that exists between business and technical groups in some organizations, if you aren’t willing to build bridges. Something that will take some serious planning, repetition, and tactical agility when it comes to the delivery of relevant information that is tailored for your intended audience. Internal evangelism is hard work, and something that will take regular auditing and retooling before you will ever see the impact you desire.
]]></content>
    <id>http://apievangelist.com/2019/08/16/four-phases-of-internal-api-evangelism/</id>
  </entry><entry>
    <title>An Observable Regulatory Provider Or Industry API Management Gateway</title>
    <link href="http://apievangelist.com/2019/08/15/an-observable-regulatory-provider-or-industry-api-management-gaeway/"/>
    <updated>2019-08-15T02:00:00Z</updated>
    <content><![CDATA[
I wrote a separate piece on an API gateway specification standard recently, merging several areas of my research and riffing on a recent tweet from Sukanya Santhanam (@SukanyaSanthan1). I had all these building blocks laying around as part of my research on API gateways, but also from the other areas of the API lifecycle that I track on. Her tweet happened to coincide with other thoughts I had simmering, so I wanted to jump on the opportunity to publish some posts, and see if I could slow jam a conversation in this area. Now, after I defined what I’d consider to be a core API gateway set of building blocks, I wanted to take another crack at refining my vision for how we make it more observable and something that could be used as a tech sector regulatory framework.

Copying and pasting from my API gateway core specification, here is what my v1 draft vision for an API gateway might be:


  Paths - Allowing many different API paths to exist.
  Schema - Allowing me to manage all of my schema.
  Integrations - Providing backend lego architecture.
    
      Resource - Allow for integration with other APIs.
      Database - Provide a stack of database integrations.
      Other - Define whole buffet of integration definitions.
    
  
  Requests - Define all of my HTTP 1.1 requests
    
      Methods - Providing me with my HTTP verbs.
      Path Parameters - Able to define path parameters.
      Query Parameters - Able to define query parameters.
      Bodies - Providing control over the request body.
      Headers - Full management of HTTP request headers.
      Encoding - Defining the media types in in use for requests.
      Validate - Providing validation for all incoming requests.
      Mappings - Allowing for mapping of requests to backend.
      Transformations - Transformation before sending to backend..
      Examples - Ensuring there are samples for each request.
      Schema - Able to reference all schema used in requests.
      Tags - Being able to organize API requests using tags.
    
  
  Responses - Define all of my HTTP 1.1 responses.
    
      Status Codes - Providing the ability to define HTTP status codes.
      Headers - Full management of all HTTP response headers.
      Encoding - Defining the media types in in use for responses.
      Schema - Able to reference all schema used in responses.
      Examples - Ensuring there are samples for each response.
    
  
  Stages - Able to stage APIs under any platform defined environment.
  Publishing - Allowing for conscious publishing of APIs into production.
  Versioning - Providing semantic versioning as header or in the path.
  Policies - Defining policies for API, and schema access by consumers.
  Licensing - Ensure that data and APIs are properly licensed for consumption.
  Plans - Crafting a handful of standard access tiers for different consumers.
  Rate Limiting - Define the rate limits for all APIs within each plan offered.
  Domains - Allow for default and custom domains associated with APIs.
  Certificates - Provide management and usage of certificates for encryption.
  Tags - Allow APIs, as well as their individual paths, and requests to be tagged.
  Dependencies - Inform on the dependencies between APIs, including 3rd party.
  Regions - Allow for multi-region deployment of APIs, with full DNS support.
  Contact - Ensure there is contact information for every API owner.
  Logging - Standardize the logging for all API traffic to one or many locations.
  Monitoring - Provide basic monitoring of all APIs from alternate locations.
  Status - Offer a real time status dashboard and notification for all APIs.
  Terms of Service - Allow for the publishing of one or many TOS applying to APIs.
  Authentication - Provide a handful of standard authentication mechanisms.
  Authorization - Enable fine grade authorization across APIs and schema.
  Consumers - Allow for consumers to sign up and maintain access accounts.
  Keys - Require consumers define their applications and use API keys with API calls.
  Documentation - Automatically publish documentation for all APIs that are published.
  Reporting - Provide reporting on all gateway activity across each API and the lifecycle.
    
      Platform - Deliver platform specific API consumption report.
      Consumer - Provide consumer specific API consumption reports.
    
  


That is a pretty long list. I know there are other building blocks missing, but this represents my first pass through my API research. It reflects the building blocks I want available when I put an API gateway to work in any cloud, on-premise, or on-device use case. However, in this post I wanted to go beyond what I’d consider the core API gateway, and talk about how we make it more observable, and something that could be applied to regulate an industry. Not something that happens behind the scenes, but something that happens out in the open, bringing in some sunlight, and pulling back the curtain on the black boxes some companies enjoy operating. While this won’t solve all our problems, I think it will provide a pretty good first step towards bringing some much needed observability to the tech sector, using common solutions that already exist. Here are a handful of building blocks I think could contribute to this conversation.


  Open Source - Ensure that the gateway is open source, and something that is developed out in the open, and can be forked an operated by anyone.
  Organizational - Provide the suggested framework for the operating organization, and what staffing and other resources are required to operate an instance of this model, providing a (hopefully) neutral entity to bring to life.
  Provider Directory - Require that all providers who have APIs published as part of the platform be profiled and available within a single directory, breaking down the resources they have published, as well as usage, monitoring, and other relevant information.
  Consumer Directory - Require that all consumers who have access to the platform publish a profile, share how their are using APIs, and offer a summary of their authentication, authorization, usage levels, and data points.
  Research Access - Provide access to researchers as part of the consumer management, but offering wider access to platform data and consumption based upon agreed upon plans.
  Media Access - Allow for access my media organization who are looking to understand what is happening across a platform, and the impact on consumers via applications.
  Industry Access - Provide access to industry analysts as part of the consumer management, but offering wider access to platform data and consumption based upon agreed upon plans.
  Auditor Access - Provide complete auditor access to the entire platform, allowing certified auditors to come in and review resources, consumers, logs, and any other aspects of the gateway operations.
  Schema Catalog - A complete catalog of all the types of data that is tracked and made available across all APIs, and used in desktop, web, mobile, and device applications.
  Monetization - Allow for the monetization of provider participation, consumer, researcher, and industry access, allowing for well defined plans, rate limits, and observability into revenue that is generated.
  Issues - Provide the ability for the public, consumers, researchers, and industry to submit issues in a safe, moderated, and accessible space, ensuring there is observability into all issues across the gateway.
  Disputes -  Ensure there are trained professionals to help address disputes on the platform rising from issues reported by the public, consumers, researchers, and industry analysts using the gateway.
  Reporting - Provide reporting to key stakeholders.
    
      Private - Provide comprehensive reporting accessible to trusted internal stakeholders.
      Public - Publish a line of regular public reports on platform usage and consumption.
      Researcher - Provide a set of reports just for researchers based upon plans and agreements.
      Media - Provide a rich set of reports that help keep media understanding what is going on.
      Auditors - Give auditors a full set of reports, including summary and detail access.
    
  


This model isn’t without precedent. Last year I spent a couple months studying the approach by UK regulators to bring more observability into the banking sector, and the formation of Open Banking organization (https://www.openbanking.org.uk/). Learning more about what open banking was in the UK (http://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/),  how it provides a common API definition (http://apievangelist.com/2018/02/21/open-banking-in-the-uk-openapi-template/), who the common stakeholders were (http://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/), and exploring how I could emulate this approach as an open source API industry template (http://open.banking.blueprint.apievangelist.com/). In 2019, I want to go even further with this open source API Blueprint, understand how it can be used to define a common open source API gateway specification, while adding the necessary building blocks to ensure there is observability at the industry level.

I am proposing that this model be defined, standardized, and applied at the single provider, or industry level. If required, an independent entity can be setup to operate the API gateway as an independent platform, funded by regulators, and the monetization layer that leverages a mix of providers, consumers, researchers, and industry analyst access to the platform. While there will be private layers of the platform, the goal is to provide as much as possible out in the open, operating as many public API platforms have been operating for the last 20 years. Ironically, some of the worst behaved technology platforms have operated using this model in the past, but sadly have been actively tightening down access levels. I’m proposing we take their formula, open source it, and operate it independently, out in the open, and make them pay for it.

As I said before, this will not solve all problems. However it will define a layer that ALL desktop, web, mobile, and device applications can be required to go through. Requiring that provides only develop applications on top of APIs deployed within regulated API gateways. Requiring that all internal, partner, and 3rd party public consumers access API via the single, or regionally distributed gateways. If auditors ever find that an API provider is leverage APIs not listed in the API gateway directory, or their partners and the public have access to data that is not defined in schema within the API gateway—then there is a problem, and enforcement can follow. Of course, all of this isn’t as simple as proposed in this post, but it provides the scaffolding and blueprint for how it can be applied. There is no reason this can’t be applied to regulate technology companies, and applied to existing industries that are already regulated, helping bring more observability into already existing regulatory practices.

Nothing I’m proposing here is rocket science, or theoretical. It is based upon proven practices of tech companies. All I’m saying now, is rather than just advising companies, organizations, institutions, and government follow these best practices, I’m saying that we should begin working to establish a standard, and craft policy that requires everyone to participate. I know many tech folks don’t like the idea of regulation, but for certain industries, and certain platforms, it might be a positive thing to inject some regulation into the equation, and begin doing things out in the open, rather than behind the curtain. If you have any questions or comments on this blueprint, feel free to email me at info@apievangelist.com, or submit an issue on the GitHub repository for this blueprint.
]]></content>
    <id>http://apievangelist.com/2019/08/15/an-observable-regulatory-provider-or-industry-api-management-gaeway/</id>
  </entry><entry>
    <title>An API Platform Reliability Engineer At Stripe</title>
    <link href="http://apievangelist.com/2019/08/15/an-api-platform-reliability-engineer-at-stripe/"/>
    <updated>2019-08-15T02:00:00Z</updated>
    <content><![CDATA[
I find that the most interesting and telling API building blocks come out of the companies who are furthest along in their API journey, and have made the conscious effort to heavily invest in their platforms. I try to regularly visit API platforms who are doing the most interesting work on a regular basis, because I am almost always able to find some gem of an approach that I can showcase here on the blog.

This weeks gem is from API rockstar Stripe, and their posting for a reliability engineer for their API platform. Here is a summary from their job posting:

As a Reliability Engineer, you’ll help lead an active area of high impact by defining and building proactive ways to further hone the reliability of our API. You’ll collaborate with team members across Engineering, as well as with our business, sales and operations teams to determine areas of greatest leverage.

You Will:


  Work with engineers across the company to identify key areas for reliability improvement.
  Gather requirements and make thoughtful tradeoffs to ensure we are focusing our efforts on the most impactful projects.
  Work on services and tools to proactively improve the quality and reliability of our production API.
  Debug production issues across services and multiple levels of the stack. Improve operational standards, tooling, and processes.


I’ve studied thousands of APIs over almost a decade, and seeing a company invest this heavily in API reliability is a rare thing. For me, this demonstrates two things, that Stripe takes their API seriously, and that it takes a huge amount of investment and resources to do APIs right. Something I don’t think many API providers realize as they try to do APIs as a side project, and wonder why they aren’t seeing the results they want.

I find that the job postings for API providers is one of the most telling signals I can harvest to understand how committed a company is to their APIs. Human Resources is one of the most areas to be investing in when it comes to your API operations, and the frequency and type of API job postings tells a lot about the API journey a company is on. Hiring API engineers is an important role to be hiring for, but it will also take hiring someone dedicated to reliability to make the impact with your platform that you desire.
]]></content>
    <id>http://apievangelist.com/2019/08/15/an-api-platform-reliability-engineer-at-stripe/</id>
  </entry><entry>
    <title>Some Of The API Gateway Building Blocks</title>
    <link href="http://apievangelist.com/2019/08/05/some-of-the-api-gateway-building-blocks/"/>
    <updated>2019-08-05T02:00:00Z</updated>
    <content><![CDATA[
The inspiration for this post wasn’t fully mine, I’m borrowing and building upon what Sukanya Santhanam (@SukanyaSanthan1) tweeted out the other day. It is a good idea, and something that should be open sourced and moved forward. I’ve been studying with API management since 2010, and using gateways for over 15 years. I’ve watched gateways evolve, and partnered regularly with API management and gateway providers (Shout out to Tyk). Modern API gateways aren’t your grandfather’s SOA tooling, they’ve definitely gone through several iterations. While I still prefer hand rolling and forging my APIs out back in my woodshed on an anvil, I find myself working with a lot of different API gateways lately.

I’ve kept feeling like I needed to map out the layers of what I’d consider to be a modern API gateway, and begin providing links to the most relevant API gateways out there, and the most common building blocks for an API gateway. Now that you can find API gateways baked into the fabric of the cloud, it is time that we work to standardize the definition of what they can deliver. I’m not looking to change what already is. Actually, I’m looking to just document and build on what already is. As with every other stop along the API lifecycle I’m looking to just map out the common building blocks, and establish a blueprint going forward the might influence existing API gateway providers, as well as any newcomers.

After going through my API gateway research for a while, I quickly sketched out these common building blocks for helping deploy, manage, monitor, and secure your APIs:


  Paths - Allowing many different API paths to exist.
  Schema - Allowing me to manage all of my schema.
  Integrations - Providing backend lego architecture.
    
      Resource - Allow for integration with other APIs.
      Database - Provide a stack of database integrations.
      Other - Define whole buffet of integration definitions.
    
  
  Requests - Define all of my HTTP 1.1 requests
    
      Methods - Providing me with my HTTP verbs.
      Path Parameters - Able to define path parameters.
      Query Parameters - Able to define query parameters.
      Bodies - Providing control over the request body.
      Headers - Full management of HTTP request headers.
      Encoding - Defining the media types in in use for requests.
      Validate - Providing validation for all incoming requests.
      Mappings - Allowing for mapping of requests to backend.
      Transformations - Transformation before sending to backend..
      Examples - Ensuring there are samples for each request.
      Schema - Able to reference all schema used in requests.
      Tags - Being able to organize API requests using tags.
    
  
  Responses - Define all of my HTTP 1.1 responses.
    
      Status Codes - Providing the ability to define HTTP status codes.
      Headers - Full management of all HTTP response headers.
      Encoding - Defining the media types in in use for responses.
      Schema - Able to reference all schema used in responses.
      Examples - Ensuring there are samples for each response.
    
  
  Stages - Able to stage APIs under any platform defined environment.
  Publishing - Allowing for conscious publishing of APIs into production.
  Versioning - Providing semantic versioning as header or in the path.
  Policies - Defining policies for API, and schema access by consumers.
  Licensing - Ensure that data and APIs are properly licensed for consumption.
  Plans - Crafting a handful of standard access tiers for different consumers.
  Rate Limiting - Define the rate limits for all APIs within each plan offered.
  Domains - Allow for default and custom domains associated with APIs.
  Certificates - Provide management and usage of certificates for encryption.
  Tags - Allow APIs, as well as their individual paths, and requests to be tagged.
  Dependencies - Inform on the dependencies between APIs, including 3rd party.
  Regions - Allow for multi-region deployment of APIs, with full DNS support.
  Contact - Ensure there is contact information for every API owner.
  Logging - Standardize the logging for all API traffic to one or many locations.
  Monitoring - Provide basic monitoring of all APIs from alternate locations.
  Status - Offer a real time status dashboard and notification for all APIs.
  Terms of Service - Allow for the publishing of one or many TOS applying to APIs.
  Authentication - Provide a handful of standard authentication mechanisms.
  Authorization - Enable fine grade authorization across APIs and schema.
  Consumers - Allow for consumers to sign up and maintain access accounts.
  Keys - Require consumers define their applications and use API keys with API calls.
  Documentation - Automatically publish documentation for all APIs that are published.
  Reporting - Provide reporting on all gateway activity across each API and the lifecycle.
    
      Platform - Deliver platform specific API consumption report.
      Consumer - Provide consumer specific API consumption reports.
    
  


I’m going to add these to my API gateway research. I’m sure there are other building blocks out there, but I think this is a good start. It reflects what I think makes API gateways different from API management. It has the design, deployment, and backend integration portion of the conversation, as well as the key API management features expected. I see API gateways as a Venn diagram of API lifecycle features. Providing a single blueprint, tooling, and appliance that will help you deliver, manage, distribute, and scale your API infrastructure.

You can find this outline published over at GitHub. I will me managing it as a living document and opening up to feedback via GitHub issues. I’m going to evolve this as a core API gateway specification—eventually defining APIs and schema for each layer of the stack. This will take some time because I will have to profile several of the existing API gateway APIs and mine them for logical patterns. Borrowing from their API designs, and schema, layering them together to create a common set of building blocks. I’m going to also begin iterating upon what I’d call an observable regulatory set of API gateway building blocks to augment this existing list. Establishing my vision of not just an API gateway standard, which can be used internally, as well as openly as part of an industry-wide effort to deliver consistent APIs for a collective purpose.
]]></content>
    <id>http://apievangelist.com/2019/08/05/some-of-the-api-gateway-building-blocks/</id>
  </entry><entry>
    <title>A Look At Behavioral API Patents</title>
    <link href="http://apievangelist.com/2019/08/05/a-look-at-behavioral-api-patents/"/>
    <updated>2019-08-05T02:00:00Z</updated>
    <content><![CDATA[
I have been studying uses of behavioral technology lately. Riffing off my partner in crimes work on the subject, but putting my own spin on it, and adding APIs (of course) into the mix. Applying on of my classic techniques, I figured I’d start with a patent search for “behavioral application programming interfaces”. I find patents to be a “wonderful” source of inspiration and understanding when it comes to what is going on with technology. Here are the top results for my patent search, with title, abstract, and a link to understand more about each patent.

User-defined coverage of media-player devices on online social networks
In one embodiment, a method includes detecting, by a media-player device including multiple antennas, a client system of a user is within a wireless communication range of the media-player device. In response to the detection, the media-player device broadcasts an authentication key for the user of the client system. The media-player device then registers the user to the media-player device based on the authentication key being verified by the client system. The media-player device further receives from the client system instructions to adjust a power level of each of the multiple antennas. The instructions are determined based on broadcast signals received at the client system and on a respective position of the client system associated with each received broadcast signal. The respective position of the client system is determined with respect to a position of the media-player device.

Controlling use of vehicular Wi-Fi hotspots by a handheld wireless device
A system and method of controlling use of vehicular Wi-Fi hotspots by a handheld wireless device includes: detecting that the handheld wireless device is communicating via a Wi-Fi hotspot; determining at the handheld wireless device that that the Wi-Fi hotspot is provided by a vehicle; and enabling one or more default prohibitions against transmitting low-priority data from the handheld wireless device via a cellular wireless carrier system while the handheld wireless device communicates with the Wi-Fi hotspot provided by the vehicle.

System and method for collecting data
The passive data collection method is sometime more reliable because the direct query method might not be available or possible through a 3.sup.rd party channel application. Accordingly, an improved data collection method is provided. The method includes: running a channel application located on a first layer of an operating system of a user device; receiving an application interface (API) call, from the channel application, for a graphic rendering module located on a second layer of the operating system, wherein the graphic rendering module is a non-video playback module; intercepting metadata sent to the graphic rendering module; determining identifying information of a content based on the intercepted metadata; and storing the determined identifying information of the content.

System and method for analytics with automated whisper mode
A service session is facilitated via a packet switched network; in the service session, user equipment participates in an interactive communication exchange with an agent via a first interaction mode, and the interactive communication exchange is based on a user inquiry. The interactive communication exchange is monitored and a determination is made that a consultation service would facilitate resolution of the user inquiry. A service resource is associated with the service session responsive to determining that the consultation service would facilitate the resolution; the service resource provides consultation to the agent via a second interaction mode without exposing the consultation to the user equipment. The consultation elevates an experience level employed in the first service session towards resolution of the user inquiry.

Method, device, and system for displaying information associated with a web page
Embodiments of the present application relate to a method, device, and system for displaying information. The method includes receiving a web page access request, in response to receiving the web page access request, displaying a first web page and obtaining designated information associated with the first web page, the first web page being associated with the web page access request and the designated information including content of the first web page, receiving an instruction to navigate to a second web page, in response to receiving the instruction to navigate to the second web page, communicating the designated information to a server associated with the second web page, and displaying the second web page, the second web page including information communicated by the server associated with the second web page.

Large-scale page recommendations on online social networks
In one embodiment, a method includes accessing user-concept scores for a first set of users, wherein each user-concept score is associated with a user-concept pair; calculating recommended user-concept scores for a subset of user-concept pairs in a second set of users. The first set of users may be discrete from the second set of users. A recommendation-algorithm may compute the recommended user-concept scores for a user-concept pair by optimizing an objective function comprising a plurality of predicted rating functions. Each predicted rating function may be determined using a user score, a concept score, a user-bias value associated with the user, as well as a concept-bias value associated with the concept. Finally, the method may include sending recommendations for one or more concepts based on the recommended user-concept scores for the second set of users.

Technologies for secure personalization of a security monitoring virtual network function
Technologies for secure personalization of a security monitoring virtual network function (VNF) in a network functions virtualization (NFV) architecture include various security monitoring components, including a NFV security services controller, a VNF manager, and a security monitoring VNF. The security monitoring VNF is configured to receive provisioning data from the NFV security services controller and perform a mutually authenticated key exchange procedure using at least a portion of the provisioning data to establish a secure communication path between the security monitoring VNF and a VNF manager. The security monitoring VNF is further configured to receive personalization data from the VNF manager via the secure communication path and perform a personalization operation to configure one or more functions of the security monitoring VNF based on the personalization data. Other embodiments are described and claimed.

Systems and methods for implementing intrusion prevention
System and methods are provided for implementing an intrusion prevention system in which data collected at one or more remote computing assets is analyzed against a plurality of workflow templates. Each template corresponding to a different threat vector and comprises: (i) a trigger definition, (ii) an authorization token, and (iii) an enumerated countermeasure responsive to the corresponding threat vector. When a match between the data collected at the one or more remote computing assets and a trigger definition of a corresponding workflow template is identified, an active threat is deemed to be identified. When this occurs the authorization token of the corresponding workflow template is enacted by obtaining authorization from at least two authorization contacts across established trust channels for the at least two authorization contacts. Responsive to obtaining this authorization, the enumerated countermeasure of the corresponding workflow template is executed.

Identity and trustworthiness verification using online and offline components
Methods and systems for verifying the identity and trustworthiness of a user of an online system are disclosed. In one embodiment, the method comprises receiving online and offline identity information for a user and comparing them to a user profile information provided by the user. Furthermore, the user's online activity in a third party online system and the user's offline activity are received. Based on the online activity and the offline activity a trustworthiness score may be calculated.

Protecting sensitive information from a secure data store
In embodiments of the present invention improved capabilities are described for the steps of receiving an indication that a computer facility has access to a secure data store, causing a security parameter of a storage medium local to the computer facility to be assessed, determining if the security parameter is compliant with a security policy relating to computer access of the remote secure data store, and in response to an indication that the security parameter is non-compliant, cause the computer facility to implement an action to prevent further dissemination of information, to disable access to network communications, to implement an action to prevent further dissemination of information, and the like.

Local data aggregation repository
Apparatuses, systems, methods, and computer program products are disclosed for a local repository of aggregated data. A hardware device comprises a local repository of data aggregated, for a user, from a plurality of third party service providers. A hardware device comprises a local authentication module configured to secure, on the hardware device, aggregated data and electronic credentials of a user for a plurality of third party service providers. A hardware device comprises an interface module configured to provide access controls to a user defining which of a plurality of other third party service providers the user authorizes to access aggregated data, and to provide the aggregated data to the authorized other third party service providers.

Mapping and display for evidence based performance assessment
A computer-implemented method for providing a user with a performance indicator score includes receiving a first transaction message that includes historical clinical-trial performance data from one or more processors at a clinical research organization and receiving a second transaction message with health records data with parameters indicative of insurance claims data. The received historical clinical-trial performance data and the prescription data is translated into an updated database. Related records within the updated database are identified and one or more key performance indicators included in the data at the updated database for a first physician are identified. A score for each of the one or more key performance indicators are calculated and a performance indicator score record for the first physician is generated based on the calculated scores for each of the one or more key performance indicators. A multi-dimensional chart for organizing and evaluating investigators is generated.

Programmable write word line boost for low voltage memory operation
A system and method for efficient power, performance and stability tradeoffs of memory accesses under a variety of conditions are described. A system management unit in a computing system interfaces with a memory and a processing unit, and uses boosting of word line voltage levels in the memory to assist write operations. The computing system supports selecting one of multiple word line boost values, each with an associated cross-over region. A cross-over region is a range of operating voltages for the memory used for determining whether to enable or disable boosting of word line voltage levels in the memory. The system management unit selects between enabling and disabling the boosting of word line voltage levels based on a target operational voltage for the memory and the cross-over region prior to updating the operating parameters of the memory to include the target operational voltage.

Ray compression for efficient processing of graphics data at computing devices
A mechanism is described for facilitating ray compression for efficient graphics data processing at computing devices. A method of embodiments, as described herein, includes forwarding a set of rays to a ray compression unit hosted by a graphics processor at a computing device, and facilitating the ray compression unit to compress the set of rays, wherein the set of rays are compressed into a compressed representation.

Transactionally deterministic high speed financial exchange having improved, efficiency, communication, customization, performance, access,  trading opportunities, credit controls, and fault tolerance
The disclosed embodiments relate to implementation of a trading system, which may also be referred to as a trading system architecture, having improved performance which further assures transactional determinism under increasing processing transaction loads while providing improved trading opportunities, fault tolerance, low latency processing, high volume capacity, risk mitigation and market protections with minimal impact, as well as improved and equitable access to information and opportunities.

Product notification and recommendation technology utilizing detected activity
An exemplary system and method provides a product notification and recommendation technology for monitoring a computing device to detect particular use-cases of device activity and providing a notification through a user interface that indicates at least one product corresponding to the detected particular use-cases. In this way, the product notification and recommendation technology adds a new dimension of usage-based personalization to targeted advertising that results in timely product and service recommendations.

Presentation of content items in view of commerciality
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for enhancing selecting relevant and diverse advertisements. In one aspect, a method includes receiving an initial query, selecting one or more additional queries relating to the initial query, including selecting additional queries having a greatest commerciality, identifying one or more content items for each of the additional queries, the one or more content items forming a content block, and providing a content block and an associated additional query to a client device to be displayed along with search results associated with the initial query.

Protecting privacy of personally identifying information when delivering targeted assets
Techniques are disclosed herein for protecting personally identifying information (PII) and behavioral data while delivering targeted assets. In one aspect, a profile is created based on a template and desired characteristics of users to receive one or more targeted assets. The template provides a framework for the user characteristics. One or more clients are provided the template. A manifest that identifies the targeted assets is encrypted based on the profile. The encrypted manifest is sent to the one or more clients. A user profile is generated at a client based on a template. The client attempts to decrypt the encrypted manifest based on the profile created at the client. The client sends a request for any targeted assets that were identified through the attempt to decrypt the encrypted manifest.

Vector-based characterizations of products and individuals with respect to customer service agent assistance
Systems, apparatuses, and methods are provided herein for providing customer service agent assistance. A system for providing customer service agent assistance comprises a customer profile database storing customer partiality vectors for a plurality of customers, the customer partiality vectors comprise customer value vectors, a communication device, and a control circuit. The control circuit being configured to: provide a customer service agent user interface on a user device associated with a customer service agent, associate a particular customer with the customer service agent, retrieving at least one customer value vector for the particular customer from the customer profile database, and cause, via the communication device, the at least one customer value vector of the particular customer to be displayed on the customer service agent user interface of the user device.

Sparse neural control
Aspects herein describe new methods of determining optimal actions to achieve high-level goals with minimum total future cost. At least one high-level goal is inputted into a user device along with various observational data about the world, and a computational unit determines, though a method comprising backward and forward sweeps, an optimal course of action as well as emotions. In one embodiment a user inputs a high-level goal into a cell phone which senses observational data. The cell phone communicates with a server that provides instructions. The server determines an optimal course of action via the method of backward and forward sweeps, and the cell phone then displays the instructions and emotions to the user.

Framework for classifying an object as malicious with machine learning for deploying updated predictive models
According to one embodiment, an apparatus comprises a first analysis engine and a second analysis engine. The first analysis engine analyzes an object to determine if the object is malicious. The second analysis engine is configured to (i) receive results of the analysis of the object from the first analysis engine and (ii) analyze, based at least in part on the analysis by the first analysis engine, whether the object is malicious in accordance with a predictive model. Responsive to the first analysis engine and the second analysis engine differing in determinations as to whether the object is malicious, information associated with an analysis of the object by at least one of the first analysis engine and the second analysis engine is uploaded for determining whether an update of the predictive model is to occur. An update of the predictive model is subsequently received by the classification engine.

Continuous user authentication
A method of enabling continuous user authentication, comprising: setting up an authentication server to provide authentication data to an enterprise server in parallel to a remote user session with the enterprise server, when the user is using a touch screen device; extracting samples from a user's behavior, to build a library of user specific parameters; and tracking user behavior to authenticate the user, the tracking comprises initial identification of a user of the touch screen device when starting a session with the enterprise server and continuous authentication of the user during the session with the enterprise server.

System and method for decentralized autonomous healthcare economy platform
A system and method for a decentralized autonomous healthcare economy platform are provided. The system and method aggregates all of the healthcare data into a global graph-theoretic topology and processes the data via a hybrid federated and peer to peer distributed processing architectures.

Safety features for high level design
This disclosure relates generally to electronic design automation using high level synthesis techniques to generate circuit designs that include safety features. The algorithmic description representation can be specified in a first language and include at least one programming language construct associated with a first safety data type. Compiling the algorithmic description may involve identifying the at least one construct, accessing a first safety data type definition associated with the first safety data type, and generating a second representation of the circuit design based on the algorithmic description representation and the first safety data type definition. The second representation can be provided in a second language and include at least one safety feature for a portion of the circuit design associated with the at least one construct.

Configuring a programmable device using high-level language
A method of preparing a programmable integrated circuit device for configuration using a high-level language includes compiling a plurality of virtual programmable devices from descriptions in said high-level language. That compiling includes compiling configurations of configurable routing resources from programmable resources of said programmable integrated circuit device, and compiling configurations of a plurality of complex function blocks from programmable resources of said programmable integrated circuit device. A machine-readable data storage medium may be encoded with a library of such compiled configurations. A virtual programmable device may include a stall signal network and routing switches of the virtual programmable device may include stall signal inputs and outputs.

System and process for simulating the behavioral effects of timing violations between unrelated clocks
According to one aspect, embodiments of the invention provide a CDC simulation system comprising a timing analysis module configured to receive a circuit design, analyze the circuit design to identify at least one CDC, and generate a report including information related to the at least one CDC, a CDC simulation module configured to communicate with the timing analysis module and to receive the report from the timing analysis module, and a test bench module configured to communicate with the CDC simulation module, to receive the circuit design, and to operate a test bench code to simulate the operation of the circuit design, wherein the CDC simulation module is further configured to edit a top level of the test bench code, based on the received report, such that the test bench module is configured to identify timing violations in the circuit design due to the at least one CDC.

Deep compositional frameworks for human-like language acquisition in virtual environments
Described herein are systems and methods for human-like language acquisition in a compositional framework to implement object recognition or navigation tasks. Embodiments include a method for a model to learn the input language in a grounded and compositional manner, such that after training the model is able to correctly execute zero-shot commands, which have either combination of words in the command never appeared before, and/or new object concepts learned from another task but never learned from navigation settings. In embodiments, a framework is trained end-to-end to learn simultaneously the visual representations of the environment, the syntax and semantics of the language, and outputs actions via an action module. In embodiments, the zero-shot learning capability of a framework results from its compositionality and modularity with parameter tying.

Efficient word encoding for recurrent neural network language models
Systems and processes for efficient word encoding are provided. In accordance with one example, a method includes, at an electronic device with one or more processors and memory, receiving a user input including a word sequence, and providing a representation of a current word of the word sequence. The representation of the current word may be indicative of a class of a plurality of classes and a word associated with the class. The method further includes determining a current word context based on the representation of the current word and a previous word context, and providing a representation of a next word of the word sequence. The representation of the next word of the word sequence may be based on the current word context. The method further includes displaying, proximate to the user input, the next word of the word sequence.

Displaying temporary profile content items on communication networks
In one embodiment, a method includes accessing, from a data store of the communication network, user information associated with a first user of the communication network, identifying one or more entities associated with the communication network that are relevant to the first user based on the user information, and retrieving, for each identified entity, one or more content frames associated with the entity. The method includes ranking the one or more content frames based on the user information. The method also includes sending, to a client device of the first user, one or more of the content frames for display to the first user in ranked order, wherein each content frame is selectable by the first user to display the selected content frame in association with a particular content item for a specified period of time.

Method and system for mining frequent and in-frequent items from a large transaction database
The technique relates to a system and method for mining frequent and in-frequent items from a large transaction database to provide the dynamic recommendation of items. The method involves determining user interest for an item by monitoring short item behavior of at least one user then selecting a local category, a neighborhood category and a disjoint category with respect to the item clicked by the at least one user based on long term preferences data of a plurality of users of the ecommerce environment thereafter selecting one or more frequent and infrequent items from each of the selected local, neighborhood and disjoint category items and finally generating one or more dynamic recommendations based on the one or more items selected from the local category, the neighborhood category and the disjoint category and the one or more selected frequent and infrequent items.

Clustering based process deviation detection
Systems and methods for data analysis include correlating event data to provide process instances. The process instances are clustered, using a processor, by representing the process instances as strings and determining distances between strings to form a plurality of clusters. One or more metrics are computed on the plurality of clusters to monitor deviation of the event data.

Automated system and method to customize and install virtual machine configurations for hosting in a hosting environment
Some embodiments provide a method for automated configuration of a set of resources for hosting a virtual machine at a particular node in a hosting system. The hosting system includes several nodes for hosting virtual machines. The method, at a first virtual machine operating using a first set of resources of the particular node, receives a user-specified virtual machine configuration for a second virtual machine to be hosted on a second set of resources of the particular node. The method retrieves, to the first virtual machine, a software image from a computer readable hardware medium storing several software images based on the user-specified virtual machine configuration. The method modifies the retrieved software image according to the user-specified virtual machine configuration. The method configures the second set resources using the modified software image.

Systems and methods of monitoring a network topology
The technology disclosed relates to maintaining up to date software version data in a network. In particular, it relates to accessing a network topology that records node data and connection data including processes running on numerous hosts grouped into local services on the hosts, the local services running on multiple hosts grouped into service clusters and sub-clusters of service clusters, and network connections used by the service clusters to connect the hosts grouped into service connections. It further relates to collecting current software version information for the processes, updating the network topology with the current software version for particular process running on a particular host when it differs from a stored software version in the network topology, reassigning the particular host to a sub-cluster within the service cluster according to the current software version, and monitoring the updated sub-cluster within the service cluster.

Power management of memory chips based on working set size
Briefly, in accordance with one or more embodiments, an apparatus comprises a memory comprising one or more physical memory chips, and a processor to implement a working set monitor to monitor a working set resident in the one or more physical memory chips. The working set monitor is to adjust a number of the physical memory chips that are powered on based on a size of the working set.

Mobile communication terminal providing adaptive sensitivity of a click event
A mobile communication terminal has a controller and a touch display. The touch display is arranged to display at least a first graphical object and a second graphical object, receive a touch, and determine a touch position and a touch duration for the touch. The controller is configured to receive the touch position and the touch duration, determine a graphical object, among the first graphical object and the second graphical object, corresponding to the touch position, determine if the touch duration exceeds a reference time threshold, and if so, generate a click event for the corresponding graphical object. The first graphical object is associated with a first time threshold, and the second graphical object is associated with a second time threshold. The first time threshold is different from the second time threshold. The controller is further configured to retrieve the first time threshold if the corresponding graphical object is the first graphical object and use the first time threshold as the reference time threshold, and retrieve the second time threshold if the corresponding graphical object is the second graphical object and use the second time threshold as the reference time threshold. The first time threshold is either higher or lower than said second time threshold depending on one or more of the following: a size, shape or/and color of the corresponding graphical object, a distance from the corresponding graphical object to a neighboring graphical object, a relative location of the corresponding graphical object in a touch area of the touch display, and a level of the corresponding graphical object in a menu hierarchy.

Control system user interface
Embodiments include systems and methods comprising a gateway located at a premise forming at least one network on the premise that includes a plurality of premise devices. A sensor user interface (SUI) is coupled to the gateway and presented to a user via a remote device. The SUI includes at least one display element. The at least one display element includes a floor plan display that represents at least one floor of the premise. The floor plan display visually and separately indicates a location and a current state of each premise device of the plurality of premise devices.

Optimizing transportation networks through dynamic user interfaces
 The present disclosure relates to providing a dynamic graphical user interface for efficiently presenting users with relevant ride information throughout the fulfillment of a ride request. In some embodiments, the system detects a trigger event during a ride, and based on detecting the trigger event, the system expands or collapses an information portion within a graphical user interface. When in a collapsed state, for example, the information portion of the graphical user interfaces includes a first set of content. Upon detecting a trigger event, the system dynamically expands the information portion to provide a second set of content that includes information associated with the detected trigger.

Human-to-mobile interfaces
A method of character recognition for a personal computing device comprising a user interface capable of receiving inputs that are to be recognized through data input means which are receptive to keyed, tapped or a stylus input, said device being adapted to facilitate a reduction in the number of physical keying actions, tapping actions or gestures required to create a data string to less than the number of characters within said data string: storing a set of data strings each with a priority indicator associated therewith, wherein the indicator is a measure of a plurality of derivatives associated with the data string; recognizing an event; looking up the most likely subsequent data string to follow the event from the set of data strings based on one or more of the plurality of derivatives; ordering the data strings for display based on the priority indicator of that data string.

Personalized autonomous vehicle ride characteristics
Systems and method are provided for controlling a vehicle. In one embodiment, a method includes: obtaining ride preference information associated with a user, identifying a current vehicle pose, determining a motion plan for the vehicle along a route based at least in part on the ride preference information, the current vehicle pose, and vehicle kinematic and dynamic constraints associated with the route, and operating one or more actuators onboard the vehicle in accordance with the motion plan. The user-specific ride preference information influences a rate of vehicle movement resulting from the motion plan.

Feasible lane routing
 Systems and method are provided for controlling a vehicle. The systems and methods calculate lane plan data including a set of lane plans defining a route from a start location to a destination location, solve a motion planning algorithm to produce solved lane plan data defining a solved lane plan and a trajectory therefor, receive forthcoming distance data representing a forthcoming distance, determine a feasible lane based on the solved lane plan data within the forthcoming distance, remove a lane plan from the lane plan data to produce feasible lane plan data including a feasible lane plan defining a route from the start location to the destination location, and control motion of the vehicle based on the feasible lane plan data.

Methods and systems for diagnosing eyes using aberrometer
Configurations are disclosed for a health system to be used in various healthcare applications, e.g., for patient diagnostics, monitoring, and/or therapy. The health system may comprise a light generation module to transmit light or an image to a user, one or more sensors to detect a physiological parameter of the user's body, including their eyes, and processing circuitry to analyze an input received in response to the presented images to determine one or more health conditions or defects.

Mobile localization using sparse time-of-flight ranges and dead reckoning
Mobile localization of an object having an object positional frame of reference using sparse time-of-flight data and dead reckoning can be accomplished by creating a dead reckoning local frame of reference, including an estimation of object position with respect to known locations from one or more Ultra Wide Band transceivers. As the object moves along its path, a determination is made using the dead-reckoning local frame of reference. When the object is within a predetermine range of one or more of the Ultra Wide Band transceivers, a &quot;conversation&quot; is initiated, and range data between the object and the UWB transceiver(s) is collected. Using multiple conversations to establish accurate range and bearing information, the system updates the object's position based on the collected data.

Method and apparatus for identifying defects in a chemical sensor array
An apparatus including an array of sensors including a plurality of chemical sensors and a plurality of reference sensors, each chemical sensor coupled to a corresponding reaction region for receiving at least one reactant, and each reference sensor comprising a field effect transistor having a gate coupled to a corresponding reference line and an access circuit for accessing the chemical sensors and the reference sensors and a controller to apply bias voltages to the reference lines to select corresponding reference sensors, acquire output signals from the selected reference sensors, and identify one or more defects in the access circuit based on differences between the acquired output signals and expected output signals.

Systems and methods for restoring cognitive function
Systems and methods for restoring cognitive function are disclosed. In some implementations, a method includes, at a computing device, separately stimulating one or more of lateral and medial entorhinal afferents and other structures connecting to a hippocampus of an animal subject in accordance with a plurality of predefined stimulation patterns, thereby attempting to restore object-specific memories and location-specific memories; collecting a plurality of one or more of macro- and micro-recordings of the stimulation of hippocampalentorhinal cortical (HEC) system; and refining the computational model for restoring individual memories in accordance with a portion of the plurality of one or more of macro- and micro-recordings.

Personal emergency response (PER) system
A system includes one or more sensors to detect activities of a mobile object; and a processor coupled to the sensor and the wireless transceiver to classify sequences of motions into groups of similar postures each represented by a model and to apply the models to identify an activity of the object.

Providing visualization data to a co-located plurality of mobile devices
A computer-implemented method according to one embodiment includes identifying a plurality of mobile devices, determining a relative location of each of the plurality of mobile devices, and assigning visualization data to each of the plurality of mobile devices, based on the relative location of each of the plurality of mobile devices.

Systems and methods for automatically detecting users within detection regions of media devices
 Systems and methods are presented for detecting users within a range of a media device. A detection region may be defined that is within the range of the media device and smaller than the range. The detection region may be stored. It may be determined whether a user is within the detection region. The media device may be activated and settings associated with the user may be applied when a user is within the detection region. In some embodiments, settings associated with a user may be compared to provided media content when the user is within the detection region. The content may change when the settings conflict with the media content. Reminders may be provided to or directed to a plurality of users within the range of the media device.

Fractional-readout oversampled image sensor
 Signals representative of total photocharge integrated within respective image-sensor pixels are read out of the pixels after a first exposure interval that constitutes a first fraction of a frame interval. Signals in excess of a threshold level are read out of the pixels after an ensuing second exposure interval that constitutes a second fraction of the frame interval, leaving residual photocharge within the pixels. After a third exposure interval that constitutes a third fraction of the frame interval, signals representative of a combination of at least the residual photocharge and photocharge integrated within the pixels during the third exposure interval are read out of the pixels.

Fraud detection in interactive voice response systems
Systems and methods for call detail record (CDR) analysis to determine a risk score for a call and identify fraudulent activity and for fraud detection in Interactive Voice Response (IVR) systems. An example method may store information extracted from received calls. Queries of the stored information may be performed to select data using keys, wherein each key relates to one of the received calls, and wherein the queries are parallelized. The selected data may be transformed into feature vectors, wherein each feature vector relates to one of the received calls and includes a velocity feature and at least one of a behavior feature or a reputation feature. A risk score for the call may be generated during the call based on the feature vectors.

The why and how of behavioral for each of these technological approaches varies. However, it does provide a nice slice of the pie when it comes to the different angles of how behavioral approaches is being applied. I purposely left the company name off of each of these to make folks click in to see who is behind each one. I’ll spoil it a little, and say the usual suspects like Facebook are behind some of them, but others are behind these different ways in which technological is being used to understand and shift our behavior. Of course, for good. ;-)

I’ll be doing more work to understand what is behind the intent of these patents. My concern around understanding more about how and what is considered behavioral in the API space is more about surveillance, than ever is about truly understanding what the good and bad of behavioral is. This is just the first of many patent searches that I will conduct. I’ll keep evolving my vocabulary for searching and discovering APIs in this area, evolving my results over time, and learning from what companies are up to when it comes to behavioral manipulation via APIs.
]]></content>
    <id>http://apievangelist.com/2019/08/05/a-look-at-behavioral-api-patents/</id>
  </entry><entry>
    <title>Reverse Engineering Mobile APIs To Show A Company Their Public APIs</title>
    <link href="http://apievangelist.com/2019/08/02/reverse-engineering-mobile-apis-to-show-a-company-their-public-apis/"/>
    <updated>2019-08-02T02:00:00Z</updated>
    <content><![CDATA[
One story I tell a lot when talking to folks about APIs, is how you can reverse engineer a mobile phone to map out the APIs being used. As the narrative goes, many companies that I talk with claim they do not have any public APIs when first engage with them. Then I ask them, “Do you have any mobile applications?”. To which the answer is almost always, “Yes!”.  Having anticipated this regular conversation, if I am looking to engage with a company in the area of API consulting, I will have made the time to reverse engineer their application to produce a set of OpenAPI definitions that I can then share with them, showing that they indeed have public APIs.

The process isn’t difficult, and I’ve written about this several times. To reverse engineer a mobile application, all you have to do is download the targeted application to your phone, configure your phone to use your laptop as a proxy, and be running Charles Proxy on your laptop. I’m not going to share a walkthrough of this, it is easy enough to Google and find the technical details of doing it, I’m more looking to just educate the average business person that this is possible. Once you have your mobile phone proxied through Charles, it will capture every call made home to the mother ship, logging the request and response structure of all APIs being used by the mobile application-—which uses public DNS for routing, making it a public API.

I have an API that I developed which I can upload the resulting Charles Proxy output file, and convert all the API calls into an OpenAPI. Making quick work of documenting the APIs behind a mobile application. Which, when you share with someone who is under the belief that their mobile APIs are private APIs, it can make quite a splash. Usually you get a response, like “well, we don’t allow access to the general public for our APIs”. To which I respond, any consumer of your application is a consumer of your APIs. If you use public DNS to handle the transport for your APIs-—they are public APIs. There is no secret force field created by mobile applications to keep your APIs secure or protected. Some applications have invested in SSL pinning to prevent proxying with tooling like Charles Proxy, but there is still ways around—-albeit it requires a significant more work.

As I’m spending more time crafting API discovery tooling and agents, I’m revisiting my work to reverse engineer mobile applications and generate OpenAPI from proxy logs. I’d like to find a way to emulate mobile applications and script the exploration of them. I find having to click through every option and feature within an application pretty mind numbing, and I’d like to automate it a little more. I feel like mobile, browsers, and  internet of things API discovery will be one of the next frontiers when it comes to mapping out the API landscape. I’m guessing there will always be truly public APIs launching, but the majority of APIs will continue to operate in the shadows of our browsers, mobile phones, and other devices that are becoming ubiquitous in our lives.
]]></content>
    <id>http://apievangelist.com/2019/08/02/reverse-engineering-mobile-apis-to-show-a-company-their-public-apis/</id>
  </entry><entry>
    <title>Didn’t We Already Do That?</title>
    <link href="http://apievangelist.com/2019/08/02/didnt-we-already-do-that/"/>
    <updated>2019-08-02T02:00:00Z</updated>
    <content><![CDATA[
When you are in the API game you hear this phrase a lot, “didn’t we already do that?”. It is a common belief system that because something was already done, that it means it will not work ever again. When you are operate solely in a computational world, you tend to see things as 1s and 0s, and if something was tried and “didn’t work”, there is no resetting of that boolean switch for some reason. We excel at believing things are done, without ever unpacking why something failed, or how the context may have changed.

One of the things I’m going to do over the next couple months is go through the entire SOA toolbox and take accounting of everything we threw away, and evaluate what the possible reasoning were behind it—-good and bad. I strongly believe that many folks who abandoned SOA, willingly or unwillingly, and especially the people who enjoy saying, “didn’t we already do that”, haven never spent time unpacking why we did, why it didn’t work, let alone whether or not it might work today. I think this paradigm reflects one of the fundamental illnesses we encounter in the tech sector-—we have a dysfunctional and distorted relationship with the past (this is by design).

I’ve written about this before, but I’ll say it again. Can you imagine saying, “didn’t we already do that” about other non-technical things. Fashion. Art. Music. Stories. Law. Why do we say it in technology? When it comes to SOA, there are many reasons why it didn’t work overall, and most of the reasons were not technical. So why would we not want to re-evaluate some of the technologies and practices to see if there would be a new opportunity to apply an old pattern or approach? This question isn’t just something I’ve heard a handful of times. There have been literally a hundred plus blog posts on API Evangelist where people have commented this—-especially when it comes to JSON Schema and OpenAPI.

Anyways. I’m going to revisit my SOA history. My therapist said enough time has past and I’ve healed properly, so I can begin digging around in this part of my past. I’m guessing there are a lot of practices, tooling, and patterns we can rethink in a JSON, YAML, containerized, CI/CD, cloudy world. Alongside this work I’m going to be assessing what the preferred open source tool are for the API lifecycle, making sure it represents my vision of a diverse API toolbox, tracking on tooling for JSON Schema, OpenAPI, and AsyncAPI that will help us service the entire API lifecycle for both internal and external API delivery. There is a lot of work that has been done in the past that we should be learning from, and I’m more than happy to dive in, do the research, and shine a light on what we’ve left behind.
]]></content>
    <id>http://apievangelist.com/2019/08/02/didnt-we-already-do-that/</id>
  </entry><entry>
    <title>The Future Of APIs Will Be In The Browser</title>
    <link href="http://apievangelist.com/2019/08/01/the-future-of-apis-will-be-in-the-browser/"/>
    <updated>2019-08-01T02:00:00Z</updated>
    <content><![CDATA[
I have been playing with the new browser reporting API lately, and while it isn’t widely supported, it does work in Chrome, and soon Firefox. I won’t go into too much technical detail, but the API provides an interesting look at reporting on APIs usage in the browser. Offering a unique view into the shadows of what is happening behind the curtain in our browser when we are using common web applications each day. I have been proxying my web traffic for a long time to produce a snapshot at the domains who are operating beneath the covers, but it is interesting for browsers to begin baking in a look at the domains who are violating, generating errors, and other shenanigans.

As I’m contemplating the API discovery universe I can’t help but think of the how “API innovation” is occurring within the browser.  When I say “API innovation”, I don’t mean the kind that got us all excited from 2005 through 2010, or the golden days from 2010 through 2015-—I am talking the exploitative kind. Serving advertisers, trackers, and other exploitative practices. Most people would scoff at me calling these things APIs, but they are using the web to deliver machine readable information, so they are APIs. I’ve been tracking on the APIs I use behind the scene in my browser using Charles Proxy for a while now, but I’m feeling I should formalize my analysis.

I’m thinking I’ll take a sampling of domain, maybe 250+, and automate the browsing of each page, while also running through Charles Proxy. Then aggregate all of the domains that are loaded, and categorize them by media type–just to give me a sampling of the APIs in operation behind the scenes of some common sites. I’m sure most are advertising or social related, but I’m guessing there are a lot of other surprises in there. While some of the APIs will be publicly showcased in some way, there are no doubt a number of APIs being used that do not have a public presence, documentation, or other visible element. While I am interested in learning how the public APIs I track on are used, I’m more interested in painting a picture of the shadow APIs that are running behind the JavaScript libraries, and other embeddable in use across the web.

Developer portals and API documentation are not the only way to find APIs. Reverse engineering mobile and web applications will continue to be a significant player when it comes to understanding the next generation of APIs. When I look across the web, all I see are APIs. I know I’m biased, but I think there is something to this. I don’t think all companies are interested in doing APIs the same way many of us API evangelist, pundits, analysts and believers are. They like APIs, but aren’t that interested in showcasing their practices, and sharing patterns with the rest of the community. I’m guessing they are more interested in penetrating our worlds via our browsers, and capturing some of the valuable behavioral exhaust we all produce on a daily basis.
]]></content>
    <id>http://apievangelist.com/2019/08/01/the-future-of-apis-will-be-in-the-browser/</id>
  </entry><entry>
    <title>About Giving Away API Knowledge For Free</title>
    <link href="http://apievangelist.com/2019/08/01/giving-away-api-knowledge-for-free/"/>
    <updated>2019-08-01T02:00:00Z</updated>
    <content><![CDATA[
I’m in the business of providing access to the API knowledge accumulated over the last decade. Despite what many people claim, I do not know everything about APIs, but after a decade I have picked up a few things along the way. Historically, I have really enjoyed sharing my knowledge with people, but I’m increasingly becoming weary of sharing to openly because of the nature of how business gets conducted online. Specifically, that there are endless waves of folks who want to tap what I know without giving anything in return, who work at companies who enjoy a lot of resources. I know people have read the startup handbook, which tells them to reach out to people who know and get their view, but when everyone does this, and doesn’t give anything in return, it is, well…I guess business as usual? Right? ;-(

Taking a sampling from the usual week in my inbox, I’ll get a handful of requests reflecting these personas:


  Analysts / Consultants - Other analysts reaching out to share information, and get my take on what I’m seeing. There is usually reciprocity here, so I’m usually happy to engage, especially if I know them personally, and have worked with them before.
  Startup Founders - I get a wide range of startup founders reaching out, many of which I do not know, wanting to get validation of their idea, and understand the marketplace they are targeting—usually if I know them, or they come with a reference I’ll engage.
  Venture Capitalists - There is a regular stream of VCs wanting to know what is happening, get my take on things, but they usually are just interested  listin validating what they already know, and get introduced to some new concepts.
  Students - There is a growing number of students reaching out, and increasingly PHD students who are working on something API related as part of their studies.


This represents the usual suspects. There are plenty of other outliers, but this represent the regular drumbeat of people making their way into my inbox. Depending on the day, my mood, and the way in which they reach out, I’ll decide to engage or not engage. However, as things are getting much tighter, especially as my time is at a premium each week, and my patience for the API sector decreases, I’m beginning to push back more. One of my biggest pet peeves is when people who have funding, or venture capitalists want to tap my knowledge without compensation. I’m guessing the privilege level with these folks is pretty high, and they are just used to engaging with other people of means—-completely oblivious to the fact that some of us don’t come from wealthy families, and are just making things work on our own.

In coming months I’ll be publishing a range of guides, white papers, and blueprints for people to purchase. Also, as usual I am open to paid consulting time. Beyond that, you’ll have to gather what you can from my short form blog posts, and the research I openly publish across my network of sites. Don’t expect much from me if you cold email me-—however, feel free to do so. The more creative the outreach, and value demonstrated by your pitch might just influence me. However, if you are just cold emailing or calling me, without understanding that I am trying to piece together a living from my work, your outreach will probably not get the response you are looking for. I’m sorry to be so cold about this, but we can’t all just be giving away our knowledge for free, and a little support of my work goes a long way–it shows that you understand and respect how much time I’ve invested in what I know about the space.
]]></content>
    <id>http://apievangelist.com/2019/08/01/giving-away-api-knowledge-for-free/</id>
  </entry><entry>
    <title>The Challenges Of API Discovery Conversations Being Purely Technical</title>
    <link href="http://apievangelist.com/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/"/>
    <updated>2019-07-31T02:00:00Z</updated>
    <content><![CDATA[
Ironically one of the biggest challenges facing API discovery on the web, as well as within the enterprise, is that most conversations focus purely on the technical, rather than the human and often business implications of finding and putting APIs to work. The biggest movement in the realm of API discovery in the last couple years has been part of the service mesh evolution of API infrastructure, and how your gateways “discover” and understand the health of APIs or microservices that provide vital services to applications and other systems. Don’t get me wrong, this is super critical, but it is more about satisfying a technical need, which is also being fueled by an investment wave-—it won’t contribute to much to the overall API discovery and search conversation because of it’s limited view of the landscape.

Runtime API discovery is critical, but there are so many other times we need API discovery to effectively operate the enterprise. Striving for technical precision at runtime is a great goal, but enabling all your groups, both technical and business to effectively find, understand, engage, and evolve with existing APIs should also be a priority. It can be exciting to focus on the latest technological trends, but doing the mundane profiling, documentation, and indexing of existing API infrastructure can have a much larger business impact. Defining the technical details of your API Infrastructure using OpenAPI, Postman, and other machine readable formats is just the beginning, ideally you are also working define the business side of things along the way.

I find that defining APIs using OpenAPI and JSON Schema to be grueling work. However, I find documenting the teams and owners behind APIs, the licensing, dependencies (both technical and business), pricing, and other business aspects of an API to be even more difficult. Over the last decade we’ve gotten to work standardizing how define the technical surface area of our APIs, but we’ve done very little work to standardize how we license, price, own, collaborate, and track on the other business implications of delivering APIs. This is one reason Steve Willmott and I created the APIs.json format, to help drive this discussion. Providing a machine readable API format to transcend the technical details of APIs, and allow us to better define the operational side of making sure APIs are discoverable.

APIs.json is about defining everything about your APIs that JSON Schema, OpenAPI, and AsyncAPI will not. Where your documentation is, how to find SDKs, what the terms and conditions are, or maybe the licensing behind your API. We designed the API specification to be flexible, and something that can be extended. There are a handful of default property types you can use when applying the format, but ultimately it is about pushing you to define your own using x- extensions. Helping API providers think through what the common building blocks of their API operations are, and provide them with a simple JSON or YAML format for indexing all of these elements for use in your API catalog, or publishing to the root of your developer portal. Helping augment what OpenAPI, JSON Schema, and AsyncAPI have done, but providing a single place for you to hang all of your API artifacts.

I’m working hard to continue refining my catalog of 3K+ APIs.json files. I’m working on better ways to validate or invalidate what I have indexed, and provide a single search interface for them. Once I’ve refreshed the catalog, and synced them with the evolution of the available APIs over at APIs.io, I will publish a fresh list of the companies I’m tracking on. I feel like one of the most critical business aspects of API discovery we consistently overlook, ignore, or are in denial of, is whether an API is still active, and anyone is home. This is a rampant illness in the cataloging of public APIs, but also something that you can find all over the enterprise. We need to do a better job of understand where are APIs are, but also be more honest about which APIs are used, do not have an owner, or are straight 404’ing and shouldn’t be listed in any active API catalog.
]]></content>
    <id>http://apievangelist.com/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/</id>
  </entry><entry>
    <title>Differences Between API Observability Over Monitoring, Testing, Reliability, and Performance</title>
    <link href="http://apievangelist.com/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/"/>
    <updated>2019-07-31T02:00:00Z</updated>
    <content><![CDATA[
I’ve been watching the API observability coming out of Stripe, as well as Honeycomb for a couple years now. Then observability of systems is not a new concept, but it is one that progressive API providers have embraced to help articulate the overall health and reliability of their systems. In control theory, observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs. Everyone (except me) focuses only on the modern approaches for monitoring, testing, performance, status, and other common API infrastructure building blocks to define observability. I insist on adding the layers of transparency and communication, which I feel are the critical aspects of observability—-I mean if you aren’t transparent and communicative about your monitoring, testing, performance, and status, does it really matter?

I work to define observability as a handful of key API building blocks that every API provider should be investing in:


  Monitoring - Actively monitoring ALL of your APIs to ensure they are up and running.
  Testing - Performing tests to ensure APIs aren’t just up but also doing what they are intended to.
  Performance - Adding an understanding of how well your APIs are delivering to ensure they perform as expected.
  Security - Actively locking down, scanning, and ensuring all your API infrastructure is secure.


Many folks rely on the outputs from these areas to define observability, but there are a couple more ingredients needed to make it observable:


  Transparency - Sharing the practices and results from each of these areas is critical.
  Communication - If you aren’t talking about these things regularly they do not exist.
  Status - Providing real time status updates for al these areas is essential.


You can be actively observing the outputs from monitoring, testing, performance, and security operations, but if this data isn’t accessible to other people on your team, within or company, partners, and for the public as required, then things aren’t observable. Of course, I’m not talking about making ALL API activity public, but I’m saying, if you are a public API, and you aren’t providing transparency, communication, and status of your monitoring, testing, performance, and security—-then you aren’t observable.

I know many folks will disagree with me on this part, but that is ok. I am used to it. So far, I haven’t seen much embrace of the observability concept, with many providers either not understanding it, or not grasping the meaningful impact it will have on their operations. So I’m not holding my breath that folks will buy into my portion of it. However it is my self appointed role to make sure the bar is high, even if nobody adopts the same set of rules. In the end, API observability isn’t some new trendy buzzword, it is one of a handful of meaningful constructs that exist to help make us all better, but most likely will get lost in the shuffle of doing APIs each day. :-(
]]></content>
    <id>http://apievangelist.com/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/</id>
  </entry><entry>
    <title>Peer API Design Review Sessions</title>
    <link href="http://apievangelist.com/2019/07/30/peer-api-design-review-sessions/"/>
    <updated>2019-07-30T02:00:00Z</updated>
    <content><![CDATA[
Public APIs have always benefitted from something that internal APIs do not always received—-feedback from other people. While the whole public API thing didn’t play out as I originally imagined, there is still a lot of benefit in letting other see, and provide open feedback on your API. It is painful for many developers to receive feedback on their API, and it is something that can be even more painful when it is done publicly. This is why so many of us shy away from establishing feedback loops around our APIs. It is hard work to properly design, develop, and then articulate our API vision to an external audience. It is something I understand well, but still suffer from when it comes to properly accessioning peer review and feedback on my API designs.

I prefer opening up to peer reviews of my API designs while they are still just mocks. I’m less invested in them at this point, and it is easier to receive feedback on them. It is way less painful to engage in an ongoing discussion fo what an API should (and shouldn’t) do early on, then it is to define the vision, deliver an API as code or within a gateway, and then have people comment on your baby that you have given birth to. It hurts to have people question your vision, and what you’ve put forth. Especially for us fragile white men who who aren’t often very good at accepting critical feedback, and want to just be left to our own devices. I’d much prefer just being a coder, but around 2008 through 2010 I saw the benefits to my own personal development when I opened up my work to my peers and let a little sunlight in. I am a better developer because of it.

One tool in my API toolbox that is growing in importance is the peer, and open API design review sessions. Taking an OpenAPI draft, loading it into Swagger Editor, firing up a Zoom or Google Hangout, and inviting others to openly share in the design of an API. I find it isn’t something everyone is equipped to do, but many are open to learning, or at least curious about how it works. Curious is good. It is a start. I think many folks aren’t fluent in the API design process, and are often afraid to appear like they don’t know what they are doing, and having an open discussion throughout the API design process helps them learn out in the open. Using a process that helps everyone involved learn together, and lower their guard a little bit when it comes to new ideas, new ways of doing things, and discussing the overall developer experience (DX) of delivering a quality API.

Peer API Design reviews is something I’d love to see more API design tooling support. If nothing else, more people just doing it with existing tools. You may not have fully embraced a complete API design first approach within your enterprise group, but openly discussing API design patterns is important. It is critical for any API developer to receive feedback on their design from other stakeholders, and other API development peers. It is important that we allow ourselves to open up to this feedback and sometimes criticism of our designs, based upon what others know, sharing potential views on how an API can reduce friction for consumers. Ideally, this process is also made accessible to non-developer stakeholders, and even business owners, but I’m thinking this is another post all by itself—-for right now, I just want to advocate for more peer API design review sessions.
]]></content>
    <id>http://apievangelist.com/2019/07/30/peer-api-design-review-sessions/</id>
  </entry><entry>
    <title>API For Processing Common Logging Formats And Generating OpenAPI Definitions</title>
    <link href="http://apievangelist.com/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/"/>
    <updated>2019-07-30T02:00:00Z</updated>
    <content><![CDATA[
I’ve invested a lot of time in the last six months into various research, scripts, and tooling to help me with finding APIs within the enterprise. This work is not part my current role, but as a side project to help me get into the mindset of how to help the enterprise understand where their APIs are, and what APIs they are using. Almost every enterprise group I have consulted for has trouble keeping tabs on what APIs are being consumed across the enterprise, and I’m keen on helping understand what the best ways are to help them get their API houses in order.

While there are many ways to trace out how APIs are being consumed across the enterprise, I want to start with some of the basics, or the low hanging when it came to API logging within the enterprise. I’m sure there are a lot of common logging locations to tackle, but my list began with some of the common cloud platforms in use for logging of operations to begin my work—focusing on the following three cloud logging solutions:


  Amazon CloudFront - Beginning with the cloud leader, and looking at how the enterprise is centralizing their logs with CloudFront.
  Google StackDriver - Next, I found Google’s multi-platform approach interesting and worth evaluating as part of this work.
  Azure Logging - Of course, I have to include Azure in all of this as they are a fast growing competitor to Amazon in this space.


After establishing a short list of cloud platforms logging solutions, I began looking at which of the common web server formats I should be looking for within these aggregate logging locations, trying to map out how the enterprise is logging web traffic. Providing me with a short list of the three most common web server formats I should be looking at when it comes to mapping the enterprise API landscape—-providing artifacts of the APIs that enterprise groups are operating.


  Apache Log File - The most ubiquitous open source web server out there is the default for many API providers.
  NGINX Log File - The next most ubiquitous open source web server is definitely something I should be looking for.
  IIS Log File - Then of course, many Microsoft web server folks are still using IIS to serve up their API infrastructure.


These three web server logging formats represent a significant slice of the API logging pie. If I can identify these logging formats across common cloud logging locations, I feel that I can provide a pretty significant solution for finding the APIs that are in use across the enterprise. However, I didn’t just want to be looking a the web server logging for understanding what APIs are being served up, I also wanted to look at the exhaust from how APIs are being consume by looking at these two web browser and proxy traffic formats:


  HAR File - Allowing for the discovery of APIs that are used in web and browser applications across common use cases.
  Charles Proxy JSON Session - Using a common proxy application to reverse engineer web and mobile application API calls.


These cloud logging solutions, web server formats, as well as browser and proxy solutions give me a pretty interesting look at the API discovery pie. I have scripts to help identify these common formats, and then automatically produce OpenAPI definitions from them. It is pretty easy to run these scripts in a variety of ways to help automatically produce a catalog of OpenAPI definitions from them, automating the mapping of the API landscape within he enterprise. I have all of these scripts working for me in a variety of capacities, the next step is to further automate them, organize them into more of a usable suite of API tooling, then unleash them on a larger set of enterprise logs.

All of my scripts currently run as APIs, as I’m API-first, but I’m currently exploring ways in which I can better execute them at the command line, and as autonomous solutions that can be installed within the enterprise, without any external connections or dependencies. I have a list of ways in which I want to add more value on top of these API discovery solutions, allowing me to generate revenue from them. However right now, I am more interested in ensuring they help automate the API landscape across the majority of enterprise logging solutions. Once I dial this in, I will be looking for more ways to implement the existing functionality, as well as evolve to cover other platforms and formats. I’m just looking to deliver a basic solution for understanding where the hell all the APIs are in the enterprise, before I look to bake in more advanced features.
]]></content>
    <id>http://apievangelist.com/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/</id>
  </entry><entry>
    <title>API Storytelling Within The Enterprise</title>
    <link href="http://apievangelist.com/2019/07/28/api-storytelling-within-the-enterprise/"/>
    <updated>2019-07-28T02:00:00Z</updated>
    <content><![CDATA[
Storytelling is important. Storytelling within the enterprise is hard. Reaching folks on the open web is hard work to, but there is usually an audience that will eventually tune in, and over time you can develop and cultivate that audience. The tools you have at your disposal within the enterprise are much more prescribed and often times dictated–even controlled. I also find that they aren’t always as effective as they are made out to be, with the perception being one thing, and the reach, engagement, and noise being then much harder realities you face when trying to get a message out.

Email might seem like a good idea, and is definitely a critical tool when reaching specific individuals or groups, but as a general company wide thing, it quickly becomes exponentially ineffective with each person you add on as CC. I’d say that you are better off creating a daily or weekly email newsletter if you are going to be sending across large groups of the enterprise rather than participating in the constant email barrage that occurs on a daily basis. Email is an effective tool when used properly, but I’d say I haven’t perfected the art of using email to reach my intended audience within the enterprise.

My preferred storytelling format is relatively muted within the enterprise — people rarely read blogs in this world. Blog reading is something you do out on the web apparently. This means I have to get pretty creative when it comes to getting your stories out. It doesn’t mean you shouldn’t be using this format of storytelling, but you just can’t count on folks to regularly consume a blog, or subscribe to an RSS feed. You can still have a blog, but you have to find other ways of slipping the links into existing conversations, documentation, and other avenues in which people consume information within the enterprise.

I would say this reality of reading within the enterprise is why I try to write more white papers and guides. I know that many folks across the enterprise prefer to consume their reading materials as a PDF on their laptop, desktop, or tablet. While this is definitely not my preferred way of consuming information, I have to remember that it is the primary way in which enterprise folks can cut through the noise, and find some quiet time to digest 6-8 pages of API blah blah blah during their busy day. While I will keep pumping out short form content on the blog, I will also be investing much more into creating longer form white papers and guides that have a greater opportunity of penetrating the enterprise.

I know that enterprise folks are caught up in the daily shit-storm and can’t always get to my blog, or spend too much time on Twitter. Making content more portable, and something they can email around, download and potentially consume later is important. As I work within the enterprise more I am realizing how critical this is for folks, including myself. I found myself firing back up my Pocket app on my iPad, so that I can queue things up for later. Reminding how difficult it is to tell stories within the enterprise and that I cannot discount tools like the PDF when it comes to reaching my intended audience. You really have to understand your audience, and work to meet them at their level, regardless of the tools you use to get information and be influenced by the deluge of storytelling we are inundated with on a daily basis.
]]></content>
    <id>http://apievangelist.com/2019/07/28/api-storytelling-within-the-enterprise/</id>
  </entry><entry>
    <title>APIs and Browser Bookmarklets</title>
    <link href="http://apievangelist.com/2019/07/24/apis-and-browser-bookmarklets/"/>
    <updated>2019-07-24T02:00:00Z</updated>
    <content><![CDATA[
I have quite a few API driven bookmarklets I use to profile APIs. I recently quit using Google Chrome, so I needed to migrate all of them to Firefox. I saw this work as an opportunity to better define and organize them, as they had accumulated over the years without any sort of strategy. When I need some new functionality in my browser I would create a new API, and craft a bookmarklet that would accomplish whatever I needed. I wish I had more browser add-on development skills, something I regular try to invest in, but I find that bookmarklets are the next best thing when it comes to browser and API interactions.

There are a number of tasks I am looking to accomplish when I’m browsing the web pages of an API provider. The first thing I want to do is record their domain, then retrieve as much intelligence about the company behind the domain in a single click of the bookmarklet. This was the first bookmarklet and API I developed. Since then, I’ve made numerous others to record the pricing page, parse the terms of service, OpenAPI, and other valuable API artifacts from across the landscape. Bookmarklets are a great way to provide just a little more context combined with a URL pointer, for harvesting, processing, and possibly some human review. Allowing me to augment, enrich, and automate how I consume information as I’m roaming around the web, researching specific topics, and do what I do.

At this point I am actually glad I didn’t invest a lot of energy into developing Chrome browser extension, because it wouldn’t have easily translated to a Firefox world. Since I have been investing in APIs plus bookmarklets, I can easily import, or copy and paste my bookmarklets over. I”m spending the time to go through them, inventory them, and better organize them for optimal usage, so the migration is a little more work than just import and export. Another aspect of this work that I am thankful for is that I abstracted away is the usage of other 3rd party APIs. My very first bookmarklet which profiles the domain of the website I’m looking at has used several different business intelligence solutions, all of which I have been priced out of using, so I’ve resorted to other ways to obtain the profile information I need–the API continues to work despite the APIs I use under the hood.

Browsers are an area of my API research that is significantly deficient. I am working to invest a little more time here, focusing on the migration and evolution of my API driven bookmarklets, but also playing around with the Browser Reporting API, which is some pretty interesting HEADER voodoo. I can’t help but feel like the browsers will continue to play an increasingly important role when it comes to APIs. Not just because of browser APIs like the Reporting API, but also because of the hidden APIs web and mobile applications use, as well as the above the tables APIs we leverage within the browser—-like my bookmarklets. I find the browser a more interesting place to study how APIs are being put to work than with startups these days. I feel like it is where the “innovation” is occurring these days, and sadly, it isn’t the good kind of “innovation” everyone so passionately believes in—-it is the more exploitative ad-driven “innovation” that is pretty invasive in our lives.
]]></content>
    <id>http://apievangelist.com/2019/07/24/apis-and-browser-bookmarklets/</id>
  </entry><entry>
    <title>Absolutism Around API Tools Increases Friction And Failure</title>
    <link href="http://apievangelist.com/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/"/>
    <updated>2019-07-24T02:00:00Z</updated>
    <content><![CDATA[
I know you believe your tools are the best. I mean, from your vantage point, they are. I also know that when you are building a new API tool, your investors want you to position your tooling as the best. The one solution to rule them all. They want you to work hard to make your tools the best, but also make sure and demonize other tooling as being inferior, out of date, and something the dinosaurs use. I know this absolute belief in your tooling feels good and right, but you are actually creating friction for your users, and potentially failure or at least conflict within their teams. Absolutism, along with divide and conquer strategies for evangelizing API tooling works for great short term financial strategies, but doesn’t do much to help us on the ground actually developing, managing, and sustaining APIs.

Ironically, there are many divers factors that contribute to why API tooling providers and their followers resort to absolutism when it comes to marketing and evangelizing their tools. Much of which has very little to do with the merits of the tools being discussed, and everything about those who are making the tools. I wanted to explore a few of them so they are available on the tip of my tongue while working within the enterprise.


  No Due Diligence On What Is Out There - Most startups who are developing API tooling do not spend the time understanding what already exists across the landscape, and get outside of the echo chamber to learn what real world companies are using to get the job done each day.
  No Learning Around Using Existing Tools - Even if startups are aware of existing tools, patterns, and processes, they rarely invest the time to actually understand what existing tools deliver—spending time to deeply understand how existing tools are being put to use by their would-be customers.
  Lack Of Awareness Around The Problem - There is a reason investors prefer young engineers when it comes to developing the next set of disruptive tooling, because they rarely understand the scope of problems being solved, and provide great fuel for short to mid-term growth strategies.
  Aggressive Male Dominated Environment - Young white men are perfect for this approach to delivering tooling that isn’t about the tool, but about a larger economic strategy, putting us passionate, privileged souls at the helm, and push them to do the disruptive bidding with very little awareness of the big picture.
  No Empathy For Others You Encounter - API tooling that takes an absolutist approach is rarely about empowering others, or understanding and providing solutions to their problems—lacking in empathy for other tooling providers, tooling consumers, or the companies left with each round of tech debt.
  Lack Of Diverse Experience In Industry - Entrepreneurs who ride each wave of API tooling absolutism and state their API tool is the one solution often lack experience in a variety of industries, and rarely have diverse experience outside of the - Silicon Valley echo chamber, and across multiple industries or geographic regions.
  VC Backed With Aggressive Growth - The aggressive absolutist approach of each wave of API tooling is almost always fueled by aggressive funding cycles, and have very little to do with the actual application of API tooling—operating the puppet strings which most API tooling providers and consumers on the front line do not see.


If you are in the business of tearing down someone else to deliver your tool, your tool will die by the same approach–someday. There is always a better funded, more aggressive solution to emerge on the market. Even if your tool has managed to achieve some level of market success, there will be a time when you let your guard down, and someone comes along to begin taking jabs at you. With each cycle of absolutism assault, the merits of the tooling mean very little. Perception always trumps reality, and there are always armies of developers waiting by in the wings to adopt what is new, and begin raising a pitchfork to attack what was. There is no allegiance and loyalty in this game.

I know. I know. This is just business. I just don’t get the game. Smart people have to make money! Yes, there are also many of us who are responsible for keeping the lights on. That aren’t as disloyal as you are, willing to jump from job to job, startup to startup. There are many of us who have been doing this a lot longer than you, and are willing to be responsible for the tech debt we incur along the way, and we do not mind doing the hard work to clean up your messes. I know that API tool absolutism makes you feel knowledgable and in control now, but just wait until you’ve ridden a few waves, and you’ve had many of your valuable tooling taken away from you because of this game. Then you will begin to see the other side of this, and better understand the toll of this business approach. Eventually you will grow weary of it, but fortunate for you, there will always be a fresh crop of recruits to wage this battle, and there is no rest for the wicked. #liveByDisruption #DieByDisruption
]]></content>
    <id>http://apievangelist.com/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/</id>
  </entry><entry>
    <title>The Higher Level Business Politics That I Am Not Good At Seeing In The API Space</title>
    <link href="http://apievangelist.com/2019/07/23/the-higher-level-business-poitics-that-i-am-not-good-at-seeing-in-the-api-space/"/>
    <updated>2019-07-23T02:00:00Z</updated>
    <content><![CDATA[
I have built successful startups. I’m good at the technology of delivering new solutions. I am decent at understanding and delivering much of business side of bringing new technological solutions to market. What I’m not good at is the higher level business politics that occur. These are the invisible forces behind businesses that I’m good at seeing, playing in, and almost always catch me off guard, too late, or just simply piss me off that they are going on behind the scenes. Unfortunately it is in this realm where most of the money is to be made doing APIs, resulting in me being written out of numerous successful API efforts, because I’m not up to speed on what is going on.

Startups are great vehicles for higher level economic strategies. They are the playground of people with access to resources, and have economic visions that rarely jive with what is happening on the ground floors. Startup strategies count on a handful at the top understanding the vision, with most at the bottom levels not being able to see the vision, and small group of disposable stooges in the middle, ensuring that the top level vision is realized—at all costs. You can work full time at a startup, and even enjoy a higher level position, and still never see the political goings on that are actually motivating the investment in your startup. This is by design. The whole process depends on the lower levels working themselves to the bone, working on, marketing, and selling one vision, while there are actually many other things going on above, usually with a whole other set of numbers.

After 30 years of playing in this game I still stuck at seeing the higher level influences. I’ve seen shiny API tooling solution after shiny API tooling solution arrive on the market, and I still fall for the shine. Ooooohhh, look at that. It will solve X, or Y problem. I really like the vision of those team members. Their timing is perfect. They seem to have the right funding, and mindshare of developers. Then I begin to see some of the usual tell-tale signs of direction coming from up above. It will be subtle signals, like the change in pricing tiers, a quickness to support a standard on import, but very slow to support export. A shift in the marketing strategy. A public “pivot”. There are a diverse of signals you can tune into that will help predict where an API startup is headed, often times away from the original tooling vision, and the needs of the end-users.

With so much experience, you’d think I’d be better at this. I’m not good at it, because I hate playing these games. I like making money, but not in the way that follow the usual VC fueled playbook. To make money at scale you have to be willing to play by multiple playbooks, keeping one or more of them secret from your teams and end-users. This just isn’t me. I prefer being more transparent. I like building real businesses. I like developing real tools. This is what I’m good at. I’m not good at the higher level games required to build wealth for myself or others. It is this reality that leaves me so reluctant to share my knowledge with VCs, talk to and support new startups, and leaves me so cranky on a regular basis when I tell stories in the space. I know y’all think this is business as usual, and are looking to get your piece of the pie, but I operate at a different level, and refuse to go there.
]]></content>
    <id>http://apievangelist.com/2019/07/23/the-higher-level-business-poitics-that-i-am-not-good-at-seeing-in-the-api-space/</id>
  </entry><entry>
    <title>API Provider And Consumer Developer Portals</title>
    <link href="http://apievangelist.com/2019/07/23/api-provider-and-consumer-developer-portals/"/>
    <updated>2019-07-23T02:00:00Z</updated>
    <content><![CDATA[
I’ve been studying API developer portals for almost a decade. I’ve visited the landing pages, portals, websites, and other incarnations from thousands of API providers. I have an intimate understanding of what is needed for API providers to attract, support, and empower API consumers. One area I’m deficient in, and I also think it reflects a wider deficiency in the API space, is regarding how to you make an API portal service both API providers and API consumers. Providing a single portal within the enterprise where everyone can come and understand how to deliver or consume an API.

There are plenty of examples out there now when it comes to publishing an API portal for your consumers, but only a few that show you how to publish an API. I’d say the most common example are API marketplaces that allow both API consumers and providers to coexist, but this model isn’t exactly what you want within the enterprise. One thing the model lacks is the on-boarding of new developers when it comes to actually developing an API. Suffering from many of the same same symptoms API management service providers have historically suffered from—-not providing true assistance when it comes to delivering a quality API.

When I envision an API portal that serves both providers and consumers, either publicly or privately, I envision just as much assistance when it comes to delivering a new API as we provide for new consumers of an API. Helping with API definition, design, deployment, management, testing, monitoring, documentation, and other critical stops along the API lifecycle. We need to see more examples of the split between API provider and consumers, equally helping both sides of the coin get up to speed, and be successful with what they are looking to achieve. I think we’ve spend almost 15 years investing in perfecting and monetizing the API portal with a focus not he consumer, and now we need to invest on helping make the portal easier for new API providers to step up and learn how to properly publish their API.

The modern API management solution is still tailored for the mystical API provider who knows how do to everything, where most do not understand the full API lifecycle. It would be an opportunity for an API management provider to go beyond just one or a handful of stops along the API lifecycle, and properly invest in on boarding new APIs. I think one reason why all of this suffers is that venture capitalists have never prioritized education and training for both API providers or consumers—-directing API service providers to only lightly invest when it comes to these API educational resources. Now that APIs have gone mainstream, we are going to need an industrial grade enterprise solution for delivering API portals that help onboard both API providers and consumers, and provide them both with what they need to navigate the entire lifecycle of the API solutions they are providing and applying.
]]></content>
    <id>http://apievangelist.com/2019/07/23/api-provider-and-consumer-developer-portals/</id>
  </entry><entry>
    <title>The Role Having Awareness Of Your API Traffic Plays In API Security</title>
    <link href="http://apievangelist.com/2019/07/22/the-role-having-awareness-of-your-api-traffic-plays-in-api-security/"/>
    <updated>2019-07-22T05:00:00Z</updated>
    <content><![CDATA[
One of the biggest reasons we adopt new technology, and justify the development of new technology, is we do not want to do the heavy lifting when it comes to what we already have in place. A common illness when it comes to API security that I’ve been battling since 2015 is that you will have API security addressed once you adopted an API management solution. Your APIs require API keys, and thus they are secure. No further work necessary. The unwillingness or lack of knowledge regarding what is needed next, leaves a vacuum for new technology providers to come in and sell you the solution for what is next, when you should be doing more work to use the tools you already have.

When it comes to API management, most vendors sold it as purely a security solution, and when companies implement it they become secure. Missing the entire point for why we do API management-—to develop an awareness of our API usage and consumption. Having keys for your APIs is not enough. You actually have to understand how those API consumers are putting API resources to work, otherwise your API security will always be deficient. Some of the fundamentals of API management you should be employing as part of your API security are:


  Registration - Make all developers sign of for API usage, establishing the terms of use.
  API Keys - Require all developers internal or external to use API keys for every application.
  API Usage - Which APIs are being used by all API consumers putting them to use in applications.
  API Errors - Understanding what the errors being generated are, and who is responsible for them.
  Logging - The logging of all API traffic, reconciling against what you know as reported usage.
  Invoicing - Invoicing of all consumers for their usage, even if they aren’t paying you money.
  Reporting - Provide reports on API usage for all stakeholders, to regularly develop awareness.


These are the fundamentals of API management, however API keys and tokens seem to be the part that people feel is API security. Where API security is really all about actually developing a real-time awareness of who is using your API resources. Leaving your finger on the pulse so that when anything changes, or error rates are elevated, you already have a base level of awareness and can easily respond by shutting off keys, or limiting overall access to resources by offending applications.

There is much more that can be done in the name of API security. This is just a list of the elements of API management that contribute to API security, which are often neglected. Having API management does not equal API security. Properly applying API management contributes to API security, it is never API security by itself. If you aren’t doing API management properly, you are more likely to fall for the next generation of API security providers who are machine learning focused, promising to do the hard work of managing awareness for you, so you don’t have to. Your unwillingness to do the work in the first place, and properly understand the role that awareness of your traffic, makes you a ripe target for selling the next wave of API security solutionism. Good luck with that!
]]></content>
    <id>http://apievangelist.com/2019/07/22/the-role-having-awareness-of-your-api-traffic-plays-in-api-security/</id>
  </entry><entry>
    <title>Happy Path API Testing Bias</title>
    <link href="http://apievangelist.com/2019/07/22/happy-path-api-testing-bias/"/>
    <updated>2019-07-22T02:00:00Z</updated>
    <content><![CDATA[
I see a lot of happy path bias when it comes to the development of APIs, but specifically when it comes to crafting testing to ensure APIs are delivering as expected. Happy path is a term used in testing to describe the desired outputs a developer and product owner is looking for. Making the not so happy path being about testing for outcomes that a developer and product owner is not wanting to occur. When it comes to API development most developers and product owners are only interested in the happy path, and will almost always cut corners, minimize the investment in, or completely lack an imagination when it comes to less than happy path API testing.

There are many reasons why someone will have a bias towards the happy path when developing an API. Every API provider is invested in achieving the happy path for delivering, providing, and consuming an API. This is what generates revenue. However, in this quest for revenue, we often become our own worst enemy. Shining a spotlight on the happy path, while being completely oblivious to what the not so happy paths will look like for end users. Why do we do this?


  Greed - We are so interested in getting an API up and running, used in our applications, and generating behavioral surplus, we are more than willing to ignore all other possible scenarios if we can easily meet our revenue goals by ignoring the unhappy path and there are no consequences.
  Tickets - Most development occurs using JIRA or other software development “tickets”, which tell developers what they are supposed to do to meet the requirements of their employment—tickets are written with the happy path in mind, and developers are rarely willing to do more.
  Imagination - While many of us technologists think we are imaginative creatures, most of us are pretty stuck in a computational way of thinking, and elaborating, iterating, and exploring beyond the initial happy path design of our API just does not exist.
  Use Software - Most of us developers do not actually use the platform we are developing, setting the stage for where we really don’t understand the problem being solve, further siloing us into seeing only the happy path that have been handed to us as part of initial product vision.
  White Male - The majority of us API developers are white men, or developers who report to white men, leaving entire shadows regarding how our APIs will be used and abused—when you are privileged, the happy path is always easier to see and walk on.
  Apathy - The majority of us are just doing our jobs, and we really do not have any excitement, passion, or interest in our jobs. We are just doing what we are told, and if our bosses do not specifically point out every single unhappy path, we don’t care.
  Velocity - Things move fast at almost any company delivering APIs, and it is easy to not have time to be able to step back and sufficiently think about what the happy paths might be when we are delivering APIs that deliver some functionality amidst a fast pace environment.
  Experience - Another reason for overlooking unhappy paths is we just do not have the experience to know about them. Startups and many technology focused companies like hiring young, low pay developers to get the job done, and they won’t always have the experience to see in the shadows.
  By Design - The product owners do not want the less than happy or unhappy paths patched, as they are there by design, and support the overall business model, which is usually advertising. Encouraging abuse, and exploitation of APIs, or at least ensuring they are much lower priorities.


There are few incentives to develop quality software these days. Revenue drives much of why we are delivering APIs, and incentivizing developers to think out of the box when it comes to API testing just doesn’t exist. Plus, it takes a lot of work to write first class tests alongside your code. Most developers are conditioned to see tests as secondary, and the thing you do only when you have the time. Making quality unhappy and less than happy path API testing always left on the cutting room floor, never making it into the final product.

You can see this bias playing out in the APIs behind Facebook, Twitter, and other advertising driven platforms. The abuse of APIs are often overlooked if it generates clicks, traffic, and increases the eyeballs. Secondarily I’d say that the consequences for when unhappy paths are identified for APIs is almost non-existent. There is no accountability for poorly designed APIs, or APIs that allow for uses beyond their intended purpose. In this environment, most API providers will never prioritize API testing, and incentivize developers to properly explore how an API can be misused, abused, or just not deliver the functionality promised. Ensuring that much of API usage exists on the unhappy path by design.
]]></content>
    <id>http://apievangelist.com/2019/07/22/happy-path-api-testing-bias/</id>
  </entry><entry>
    <title>What Is An Application?</title>
    <link href="http://apievangelist.com/2019/07/18/what-is-an-appication/"/>
    <updated>2019-07-18T02:00:00Z</updated>
    <content><![CDATA[
I have struggled asking this question in many discussions I’ve had around the world, at technology conferences, on consulting projects, and in the back rooms of dimly lit bars. What is an application? You get ten different answers if you ask this question to ten different people. I’d say the most common response is to reference the applications on a mobile device. These are the relevant. Most accessible. The most active and visible form of application in our modern life. Older programmers see them as desktop applications, where younger programmers see them as web applications, with varying grades of server applications in between. If you operate at the network layer, you’ve undoubtedly bastardized the term to mean several different things. Personally, I’ve always tried to avoid these obvious and tangible answers to this question, looking beyond the technology.

My view of what an application is stems from a decade of studying the least visible, and least tangible aspect of an application, its programming interface. When talking to people about applications, the first question I ask folks is usually, “do you know what an API is”? If someone is API savvy I will move to asking, “when it comes to application programming interface (API), who or what is being programmed? Is it the platform? The application? Or, is it the end-user of the applications?” I’ve spent a decade thinking about this question, playing it over and over in my end, never quite being satisfied with what I find. Honestly, the more I scratch, the more concerned I get, and the more I’m unsure of exactly what an “application” is, and precisely who are what is actually being programmed. Let’s further this line of thinking by looking at the definitions of “application”:


  noun - The act of applying.
  noun - The act of putting something to a special use or purpose.
  noun - A specific use to which something is put.
  noun - The capacity of being usable; relevance.
  noun - Close attention; diligence.
  noun - A request, as for assistance, employment, or admission to a school.
  noun - The form or document on which such a request is made.
  noun - Computers A computer program designed for a specific task or use.
  adjective - Of or being a computer program designed for a specific task or use.


This list comes from my friends over at Wordnik (https://www.wordnik.com/words/application), who I adore, have an API (https://developer.wordnik.com/), and who have contributed to this discussion by playing a leading role in introducing the API sector to the OpenAPI (fka Swagger) specification to the API community. That is whole other layer of significance when it comes to the semantics and meaning behind the word application, which I will have to write more about in a future post. In short, words matter. Really, words are everything. The meaning behind them. The intent. The wider understanding and belief. This is why APIs, and web applications like Wording are important. Helping us make sense of the world around us. Anyways, back to what is an application?

I like the first entry on the list — the act of applying. But, each of these definitions resonate with my view of the landscape. Yet, I’d say that the most common answer to this question hover towards the second half of this list, not the first half of it. Most application developers would say they are programming the application—the interface is for them. If you are an API developer you believe that are the one programming the application, where higher up on the platform decision making chain, they are the ones programming the application, and applying their vision of the world. Something that isn’t always visible at the lower levels, by API developers delivering APIs, the application developers consuming them, or the end-users of the tangible applications being delivered. I see the end desktop, web, or mobile as an application. I see the API as an application. I see the network connecting the two as an application. I also see the wider ideology being applied across all these layers, even when it is out of view to the outer layers.

One of the biggest imbalances in my belief system around technology, a result of be operating at the lower levels of business, institutional, or government, is that I am the one “applying” and “programming”. I was developing and delivering the application. These interfaces served me. After studying the machine closer. Tracking on the cycles. Documenting the results over the course of many cycles. I began to realize that there is more to this “application” thing than what I”m seeing. Maybe I was too close to the gears and the noise of the machine to see the bigger picture. I’m in the role of application developer not because I’m good at what I do. I’m there because I conveniently think I’m in control of this supply chain. As I worked my way up the supply chain, and became an API developer I continue to believe that I was the one “applying” and “programming”. However after over a decade of doing that, I’m realizing that I am not the one calling the shots. I’m applying something for someone else, and that applications were much more than just an iPhone or Android application, or even a TCP, FTP, STMP, HTTP, HTTP/2, or other protocol application. How you answer this questions depends on where you operate within the machine.

After climbing my way up through the layers of the machine, and finding my way to hatch at the top, finally getting some air in my lungs—-I still do not find myself in a better position. With more knowledge, just comes more concerns. Sometimes I wish I could climb back down to the lower levels, and enjoy the warmth and comfort of the inner workings of the machine. However, no I can’t find comfort in the toil that used to comfort me online late into the night. I can’t ignore what we are applying when we work in the service of the application. We are doing the hard work to mine, develop, integrate, and extract value. We are doing the dirty work of applying the vision of others. We are doing the hard work of laying the digital railroad tracks for the Internet tycoons. We are imposing their vision on the world. The special use or purpose of APIs do not always serve us. The usability and relevance is only minimally focused on us. The close attention and diligence is centered on maximum extraction and value generation for the platform. All wrapped in a computer program, designed for a specific task or use. Obfuscating the true application, with digital eye candy that keeps us always connected, and always open to something new being applied or directed in our life.
]]></content>
    <id>http://apievangelist.com/2019/07/18/what-is-an-appication/</id>
  </entry><entry>
    <title>What Makes You Think Your GraphQL Consumers Will Want To Do The Work</title>
    <link href="http://apievangelist.com/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/"/>
    <updated>2019-07-18T02:00:00Z</updated>
    <content><![CDATA[
Data work is grueling work. I’ve been working with databases since my first job developing student information databases in 1988 (don’t tell my wife). I’ve worked with Cobol, Foxpro, SQL Server, Filemaker, Access, MySQL, PostGres, and now Aurora databases over the last 30 years. I like data. I can even trick myself into doing massive data and schema refinement tasks on a regular basis. It is still grueling work that I rarely look forward to doing. Every company I’ve worked for has a big data problem. Data is not easy work, and the more data you accumulate, the more this work grows out of control. Getting teams of people to agree upon what needs to happen when it comes to schema and data storage, and actually execute upon the work in a timely, cost effective, and efficient way is almost always an impossible task. Which leaves me questioning (again), why GraphQL believers think they are going to be successfully in offshoring the cognitive and tangible work load to understand what data delivers, and then successfully apply it to a meaningful and monetizable business outcome.

Don’t get me wrong. I get the use cases where GraphQL makes sense. Where you have an almost rabid base of known consumers, who have a grasp on the data in play, and possesses an awareness of the schema behind. I’m have made the case for GraphQL as a key architectural component within a platform before. The difference in my approach over the majority of GraphQL believers, is that I’m acknowledging there is a specific group of savvy folks who need access to data, and understand the schema. I’m also being honest about who ends up doing the heavy data lifting here—-making sure this group wants it. However, I have an entirely separate group of users (the majority) who do not understand the schema, and have zero interest in doing the hard work to understand the schema, navigate relationships, and develop queries—-they just want access. Now. They don’t want to have to think about the big picture, they want one single bit of data, or a series of bits.

I’ve worked hard to engage in debate with GraphQL believers, and try to help provide them with advice on their approach. They aren’t interested. They operate within the echo chamber. They see a narrow slice of the API pie, and passionately believe their tool is the one solution. Like many API tooling peddlers who originate from within the echo chamber, they are hyper focused on the technology of managing our data. They think all data wranglers are like them. They think all data-driven companies are like theirs. They do not see the diverse types of API solutions that I see working across companies, institutions, organizations, and government agencies. They are not always aware of the business and political barriers that lie in between a belief in an API tool, and achieving a sustained implementation across the enterprise. They have that aggressive, tenacious startup way of penetrating operations, something that works well within the echo chamber, but it is an approach that will lose strength, and even begin to work against you outside the eco chamber within mainstream business operations.

I definitely see some interesting and useful tooling coming out of the GraphQL community. GraphQL is a tool in my API toolbox right along with REST, Hypermedia, Siren, HAL, JSON API, Alps, JSON Schema, Schema.org, OpenAPI, Postman Collections, Async API, Webhooks, Kafka, NATS, NGINX, Docker, and others. It has a purpose. It isn’t the silver bullet for me getting a handle on large amounts of data. It is one thing I consider when I’m trying to make sense of large data sets, and depending on the platform, the resources I have on staff, and the consumers, or the industry I am operating in–I MAY apply GraphQL. However, the times I will be able to successfully get it in the door, past legal, approved by leadership, accepted by internal developers, and then accepted and applied successfully by external developers, will be much fewer because of the aggressive echo chamber, investor-driven approach, which does not help sell the tool to my enterprise customers who have been investing in evolving their schema, and developing a suite of internal and external APIs over the last 20+ years. You might help me sell to a Silicon Valley savvy company, but you aren’t helping me in the mainstream enterprise.

This post will get the usual lineup of critical Tweets and comments. It’s fine. Once again the fact that I’m trying to help will be lost on believers. You will be more successful if you are honest about how the cognitive and  tangible workload is being shifted here. You are refusing to do the hard work to properly define and organize your schema, and provide meaningful imperative API capabilities that any developer can easily use, over providing a single declarative interface (and some tooling) to empower consumers to make sense of the schema, and access platform capabilities on their own. This works well with folks who are willing to take on the cognitive load of knowing the schema, knowing GraphQL, then are willing to accept the real work of crafting queries to get at what they desire. There are a whole lot of assumptions there that do not apply in all situations. I’d say about 12% of people I’ve worked with in my career would sign up for this. The rest of them, just want to get their task accomplished—get out of their way and give them an interface that is capable of accomplishing it for them.

Nobody wants to do data shit work. If you do, get help. It is a job that is only growing because our ability to generate, harvest, and store data has grown. There is a belief that data provides answers, without being really honest about what it takes to actually clean, refine, and organize the data, let alone any truthfulness regarding whether or not the answers are ever even there after we do invest in doing the dirty data work. Everyone is drowning in data. Everyone is chasing more data. Few are managing it well. This type of environment makes new data tooling very sexy. However, sexy tools rarely change the actual behavior on the ground within the enterprise. People still aren’t going to change behaviors overnight. GraphQL tooling will not solve all of our data problems. Ideally, we would see more interoperability of tooling between GraphQL and other API design, deployment, management, testing, monitoring, mocking, and client tooling. Like we are seeing with IBM Loopback, Postman, and others. Ideally, we would see less rhetoric around GraphQL being the one solution to rule them all. Ideally, we’d see less investor-driven rhetoric, and more real world solutions for applying through the mainstream business world. But, I’m guessing I’ll see the same response I’ve been getting on these posts since 2016. ;-(
]]></content>
    <id>http://apievangelist.com/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/</id>
  </entry><entry>
    <title>The Many Ways In Which APIs Are Taken Away</title>
    <link href="http://apievangelist.com/2019/07/17/the-many-ways-in-which-apis-are-taken-away/"/>
    <updated>2019-07-17T02:00:00Z</updated>
    <content><![CDATA[
APIs are notorious for going away. There are so many APIs that disappear I really stopped tracking on it as a data point. I used keep track of APIs that were shuttered so that I could play a role in the waves of disgruntled pitchfork mobs rising up in their wake–it used to be a great way to build up your Hacker News points! But, after riding the wave a couple hundred waves of APIs shuttering, you begin to not really not give a shit anymore—-growing numb to it all. API deprecation grew so frequent, I wondered why anyone would make the claim that once you start an API you have to maintain it forever. Nope, you can shut down anytime. Clearly.

In the real world, APIs going away is a fact of life, but rarely a boolean value, or black and white. There are many high profile API disappearances and uprising, but there are also numerous ways in which some API providers giveth, and then taketh away from API consumers.:


  Deprecation - APIs disappear regularly both communicated, and not so communicated, leaving consumers scratching their heads.
  Disappear - Companies regularly disappear specific API endpoints acting like they were never there in the first place.
  Acquisition - This is probably one of the most common ways in which high profile, successful APIs disappear.
  Rate Limits - You can always rate limit away users make APIs inaccessible, or barely usable for users, essentially making it go away.
  Error Rates - Inject elevated error rates either intentionally or unintentionally can make an API unusable to everyone or select audience.
  Pricing Tiers - You can easily be priced out of access to an API making it something that acts just like deprecating for a specific group.
  Versions - With rapid versioning, usually comes rapid deprecation of earlier versions, moving faster than some consumers can handle.
  Enterprise - APIs moving from free or paid tier, into the magical enterprise, “call us” tier is a common ways in which APIs go away.
  Dumb - The API should not have existed in the first place and some people just take a while to realize it, and then shut down the API.


I’d say Facebook, Twitter, and Google shutting down, or playing games in any of these areas have been some of the highest profile. The sneaky shuttering of the Facebook Audience Insight API was one example, but didn’t get much attention. I’d say that Parse is one that Facebook handled pretty well. Google did it with Google+ and Google Translate, but then brought back it with a paid tier. LinkedIn regularly locks down and disappears its APIs. Twitter has also received a lot of flack for limiting, restricting, and playing games with their APIs. In the end, many other APIs shutter after waves of acquisitions, leaving us with as my ex-wife says–“nothing nice”!

Face.com, Netflix, 23andMe, Google Search, ESPN, and others have provided us with lots of good API deprecation stories, but in reality, most APIs go aware without much fanfare. You are more likely to get squeezed out by rate limits, errors, pricing, and other ways of consciously making an API unusable. If you really want to understand the scope of API deprecation visit the leading deprecated API directory ProgrammableWeb—-they have thousands of APIs listed that do not exist anymore. In the end it is very difficult to successfully operate an API, and most API providers really aren’t in it for the long haul. The reasons why APIs stay in existence are rarely a direct result of them being directly financially viable. Developers squawk pretty loudly when the free API they’ve been mooching off of disappears, but there are many, many, many other APIs that go away and nobody notices, or nobody talks about. In the world of APIs there are very few things you can count on being around for very long, and you should always have a plan B and C for every API you depend on.
]]></content>
    <id>http://apievangelist.com/2019/07/17/the-many-ways-in-which-apis-are-taken-away/</id>
  </entry><entry>
    <title>Paying for API Access</title>
    <link href="http://apievangelist.com/2019/07/17/paying-for-api-access/"/>
    <updated>2019-07-17T02:00:00Z</updated>
    <content><![CDATA[
APIs that I can’t pay for more access grinds my gears. I am looking at you GitHub, Twitter, Facebook, and a few others. I spend $250.00 to $1500.00 a month on my Amazon bill, depending on what I’m looking to get done. I know I’m not the target audience for all of these platforms, but I’m guessing there is a lot more money on the table than is being acknowledged. I’m guessing that the reason companies don’t cater to this, is that there are larger buckets of money involved in what they are chasing behind the scenes. Regardless, there isn’t enough money coming my way to keep my mouth shut, so I will keep bitching about this one alongside the inaccessible pricing tiers startups like to employ as well. I’m going to keep kvetching about API pricing until we are all plugged into the matrix—-unless the right PayPal payment lands in my account, then I’ll shut up. ;-)

I know. I know. I’m not playing in all your reindeer startup games, and I don’t understand the masterful business strategy you’ve all crafted to get rich. I’m just trying to do something simple like publish data to GitHub, or do some basic research on an industry using Twitter. I know there are plenty of bad actors out there who want also access to your data, but it is all something that could be remedied with a little pay as you go pricing, applied to some base unit of cost applied to your resources. If I could pay for more Twitter and GitHub requests without having to be in the enterprise club, I’d be very appreciative. I know that Twitter has begun expanding into this area, but it is something that is priced out of my reach, and not the simple pay as you go pricing I prefer with AWS, Bing, and other APIs I happily spend money on.

If you can’t apply a unit of value to your API resources, and make them available to the masses in a straightforward way—-I immediately assume you are up to shady tings. Your business model is much more loftier than a mere mortal like me can grasp, let alone afford. I am just an insignificant raw material in your supply chain—-just be quiet! However, this isn’t rocket science. I can’t pay for Google Searches, but I can pay for Bing searches. I can’t pay for my GitHub API calls. I’m guessing at some point I’ll see Bing pricing go out of reach as Microsoft continues to realize the importance of investing at scale in the surveillance economy—-it is how you play in the big leagues. Or maybe they’ll be like Amazon, and realize they can still lead in the surveillance game while also selling things to the us lower level doozers who are actually building things. You can still mine data on what we are doing and establish your behavioral models for use in your road map, while still generating revenue by selling us lower level services.

The problem here ultimately isn’t these platforms. It is me. Why the hell do I keep insisting on using these platforms. I can always extricate myself from them. I just have to do it. I’d much rather just pay for my API calls, and still give up my behavioral data, over straight extraction and not getting what I need to run my business each day. I feel like the free model, with no access to pay for more API calls is a sign of a rookie operation. If you really want to operate at scale, you should be obfuscating your true intentions with a real paid service. If you are still hiding behind the free model, you are just getting going. The grownups are already selling services for a fair price as a front, while still exploiting us at levels we can’t see, so ultimately they can compete with all of us down the road. Ok, in all seriousness, why can’t we get on a common model for defining API access, and setting pricing. I’m really tired of all the games. It would really simplify my life if I could pay for what I use of any resource. I’m happy to pay a premium for this model, just make sure it is within my reach. Thanks.
]]></content>
    <id>http://apievangelist.com/2019/07/17/paying-for-api-access/</id>
  </entry>
</feed>
