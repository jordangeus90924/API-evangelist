<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-toolbox.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/16/secure-api-deployment-from-mysql-json-and-google-spreadsheets-with-3scale/">Secure API Deployment From MySQL, JSON and Google Spreadsheets With 3Scale</a></h3>
			<p><em>16 Nov 2013</em></p>
			<p>
I'm doing a lot more API deployments from dead simple data sources since I started working in the federal government. As part of these efforts I'm working to put together a simple toolkit that newbies to the API world can use to rapidly deploy APIs as well.
A couple of weeks ago I worked through the simple, open API implementations, and this week I want to show how to secure access to the API by requiring an AppID and AppKey which will allow you to track on who has access to the API.
I'm using 3Scale API Management infrastructure to secure the demos. 3Scale has a free base offering that allows anyone to get up and running requiring API keys, analytics and other essentials with very little investment.
Currently I have four separate deployment blueprints done:

MySQL to API
Local JSON to API
Public Google Spreadsheet to API
Private Google Spreadsheet to API

All of these samples are in PHP and uses the Slim PHP REST framework. They are meant to be working examples that you can use to seed your own API deployment.
You can find the entire working repository, including Slim framework at Github.
[<a href="/2013/11/16/secure-api-deployment-from-mysql-json-and-google-spreadsheets-with-3scale/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/openshift-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/16/modular-apis-driven-from-github-blueprints-using-openshift/">Modular APIs Driven From Github Blueprints Using OpenShift</a></h3>
			<p><em>16 Nov 2013</em></p>
			<p>I'm working on a variety of ways that anyone can easily deploy API on common cloud platforms. I'm working through a series of open and secure, modular API demos written in PHP, using the Slim framework. All of these demos are pretty basic, but currently I'm deploying them on Amazon EC2, because it is where most of my infrastructure runs and a top platform with wide user base. Ultimately my goal is to make them as simple to deploy as I possibly can, and while EC2 is definitely the leader, I know there are even simpler ways to launch simple APIs in the clouds. I was doing a survey of which PaaS platforms government agencies allow their workers to deploy projects on, via the US government API group and was reminded of OpenShift, from Redhat. If you aren't familiar with OpenShift, it is a PaaS platform that allows you to define applications in a variety of languages, then deploy, automate, manage and scale in a very modular way. With OpenShift I can define a very modular application, such as an API running PHP, using Slim framework and even apply what is known as a database cartridge, providing MySQL or Postgres storage for the API--all with a single command line. I can instruct each application to derive its code from a Git location, allowing me to manage a central library of applications, then deploy specific instances as needed. While I'm heavily using Github as the primary place to deploy single page applications, that are completely written in HTML, CSS, JavaScript and JSON, I still need server side locations to rapidly deploy API driven resources in PHP, Python, Ruby and Node. OpenShift is a much more modular approach that AWS, and while I'll still be using Amazon for many of my deployments I'm going to play around more with what is possible on OpenShift. What I really like about this approach, in addition to the modular design...[<a href="/2013/11/16/modular-apis-driven-from-github-blueprints-using-openshift/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/github-matrix.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/16/introducing-data-ongithub/">Introducing Data Ongithub</a></h3>
			<p><em>16 Nov 2013</em></p>
			<p>I am a big fan of opening up data in city, county, state and federal government and across companies of all shapes and sizes. One thing I've learned in my 20+ years of working with data is that when it comes to data management, the spreadsheet is king. While there are always centralized database systems at companies and government organizations, the spreadsheet is actually how data is generally managed, shared and distributed by the average person. Data Ongithub is a project that is focused on tapping the wealth of data available in spreadsheet form and provide a simple platform for converting and managing open data, that anyone can use. While working on open data initiatives I consistently see two major hurdles for individuals who are opening up data: How Do I Convert My Data To JSON? JSON is the reigning open data format. It is a light-weight, portable, machine readable format and is the preferred format of programmers over CSV or XML. Even with this popularity there is no common way for individuals to easily convert common data formats like Excel, CSV or XML to JSON without being a programmer. Where Do I Put My Converted JSON Data Files? Once users are able to convert their data in JSON, they quickly struggle with where they should put these files to make publicly available. Even though website hosting is widely available and inexpensive, users have not associated open data with online website storage. Especially within government organizations, individuals struggle with easy storage of their open data files. Data Ongithub looks to address both these concerns by delivering a lightweight application for conversion of Excel and CSV files to JSON formats, that is developed completely in client-side JavaScript, which means it can run anywhere in any browser. Since it is JavaScript, Data Ongithub lends itself to running completely on the social coding platform Github, which in this scenario acts as the backend storage for the application. Data...[<a href="/2013/11/16/introducing-data-ongithub/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-google-drive-icon.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/16/deploy-secure-api-public-google-spreadsheet-to-api/">Deploy Secure API: Public Google Spreadsheet to API</a></h3>
			<p><em>16 Nov 2013</em></p>
			<p>I'm working on a series of simple scripts that help people deploy APIs from some of the most common data sources. I'm starting with a series of PHP scripts, and a couple weeks ago I did a public Google Spreadsheet to API demo, and this week I want to show how to secure access to the API by requiring an AppID and AppKey which will allow you to track on who has access to the API. For this PHP implementation, I'm using the SLIM framework, which provides a dead simple REST framework you can use to deploy an API from a variety of data sources. To begin deploying an API from your Google Spreadsheet datastore, download the REST library and upload to your server that runs PHP. Slim is pretty straightforward to work with, to add each API endpoint you just add a single PHP file under methods. For this how-to guide we are going to add a simple endpoint from our spreadsheet products data store. Next you just add an include reference in the index page for your slim implementation. Everything up until now was the same as the open public Google Spreadsheet to API solution, but not on the index page we will wrap the entry point to the API, with a 3Scale API Management layer. 3Scale is free to sign up and you pay as you scale, so all it takes to get going is register for a 3Scale account and choose the base account, and under your account settings you will find your key to link this code to your account. This API just uses ID, Name, Price and Description of the product, and queries by a simple query parameter. You can use this as a template for your own product database, adding and removing fields as you need, or completely retrofitting for any database table. That is it, now you have a simple product API that pulls a list of...[<a href="/2013/11/16/deploy-secure-api-public-google-spreadsheet-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-google-docs.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/16/deploy-secure-api-private-google-spreadsheet-to-api/">Deploy Secure API: Private Google Spreadsheet to API</a></h3>
			<p><em>16 Nov 2013</em></p>
			<p>I'm working on a series of simple scripts that help people deploy APIs from some of the most common data sources. &nbsp;I'm starting with a series of PHP scripts, and last week I did a private Google Spreadsheet to API demo, and this week I want to show how to secure access to the API by requiring an AppID and AppKey which will allow you to track on who has access to the API. For this PHP implementation, I'm using the SLIM framework, which provides a dead simple REST framework you can use to deploy an API from a variety of data sources. To begin deploying an API from your Google Spreadsheet datastore, download the REST library and upload to your server that runs PHP. Slim is pretty straightforward to work with, to add each API endpoint you just add a single PHP file under methods. For this how-to guide we are going to add a simple endpoint from our private spreadsheet products data store. This project also depends on the Google Drive API and I use the Google API PHP Client to connect to Google and provide necessary oAuth connectivity.&nbsp; Before all of this works you need to have an oAuth token, which I created a simple script to handle: I leave it to you to figure out where you want to store your oAuth tokens, and other goods. I use a config.php file, but can easily be done from database or other: Next you just add an include reference in the index page for your slim implementation. Everything up until now was the same as the private Google Spreadsheet to API solution, but not on the index page we will wrap the entry point to the API, with a 3Scale API Management layer. 3Scale is free to sign up and you pay as you scale, so all it takes to get going is register for a 3Scale account and choose the base account,...[<a href="/2013/11/16/deploy-secure-api-private-google-spreadsheet-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-mysql.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/16/deploy-secure-api-mysql-to-api/">Deploy Secure API: MySQL to API</a></h3>
			<p><em>16 Nov 2013</em></p>
			<p>I'm working on a series of simple scripts that help people deploy APIs from some of the most common data sources. I'm starting with a series of PHP scripts, and a couple weeks ago I did a MySQL to API demo, and this week I want to show how to secure access to the API by requiring an AppID and AppKey which will allow you to track on who has access to the API. For this PHP implementation, I'm using the SLIM framework, which provides a dead simple REST framework you can use to deploy an API from a variety of data sources. To begin deploying an API from your MySQL database, download the REST library and upload to your server that runs PHP. Slim is pretty straightforward to work with, to add each API endpoint you just add a single PHP file under methods. For this how-to guide we are going to add a simple endpoint from our products database. Next you just add an include reference in the index page for your slim implementation. Everything up until now was the same as the open MySQL to API solution, but not on the index page we will wrap the entry point to the API, with a 3Scale API Management layer. 3Scale is free to sign up and you pay as you scale, so all it takes to get going is register for a 3Scale account and choose the base account, and under your account settings you will find your key to link this code to your account. This API just uses ID, Name, Price and Description of the product, and queries by a simple query parameter. You can use this as a template for your own product database, adding and removing fields as you need, or completely retrofitting for any database table. That is it, now you have a simple product API that pulls a list of products from a MySQL database. There are...[<a href="/2013/11/16/deploy-secure-api-mysql-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-json-data-store.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/16/deploy-secure-api-json-to-api/">Deploy Secure API: JSON to API</a></h3>
			<p><em>16 Nov 2013</em></p>
			<p>I'm working on a series of simple scripts that help people deploy APIs from some of the most common data sources. I'm starting with a series of PHP scripts, and a couple weeks ago I did a JSON to API demo, and this week I want to show how to secure access to the API by requiring an AppID and AppKey which will allow you to track on who has access to the API. For this PHP implementation, I'm using the SLIM framework, which provides a dead simple REST framework you can use to deploy an API from a variety of data sources. To begin deploying an API from your JSON datastore, download the REST library and upload to your server that runs PHP. Slim is pretty straightforward to work with, to add each API endpoint you just add a single PHP file under methods. For this how-to guide we are going to add a simple endpoint from our JSON products data store. Next you just add an include reference in the index page for your slim implementation. Everything up until now was the same as the open JSON to API solution, but not on the index page we will wrap the entry point to the API, with a 3Scale API Management layer. 3Scale is free to sign up and you pay as you scale, so all it takes to get going is register for a 3Scale account and choose the base account, and under your account settings you will find your key to link this code to your account. This API just uses ID, Name, Price and Description of the product, and queries by a simple query parameter. You can use this as a template for your own product database, adding and removing fields as you need, or completely retrofitting for any database table.&nbsp; There are any number of reasons you would want to secure an API driven from a JSON file, to offer...[<a href="/2013/11/16/deploy-secure-api-json-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/smithsonian-statue-hackathon.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/15/php-libraries-for-smithsonian-api-to-support-hackathon-this-weekend/">PHP Libraries For Smithsonian API to Support Hackathon This Weekend</a></h3>
			<p><em>15 Nov 2013</em></p>
			<p>
I have a long list of little projects I'm working on across government, and since I'm not being paid for some of this work now (except for the support of my amazing partners), the publicity and page views for this work is all I got! :-) So I'm publishing the stories around everything I do.
This week I got a sneak peak of the Smithsonian EDAN API which provides access to meta data of collections housed at the Smithsonian Institute, along with the ability to tag items and build collections.
The Smithsonian is holding a hackathon this weekend and I wanted to contribute what I could, so I pulled together a PHP code library as I explored and hacked not the Smithsonian EDAN API. The library isn't a beautiful product ready SDK, but should help the average PHP developer get up to speed on the API and save some time building their app.
You can find the Smithsonian EDAN API PHP library on Github, learn more about the hackathon this weekend on Eventbrite, and follow any other work I'm doing for the Smithsonian around open data and APIs on my Smithsonian Data project on Github.
[<a href="/2013/11/15/php-libraries-for-smithsonian-api-to-support-hackathon-this-weekend/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/fda-recall.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/15/interactive-feedback-on-the-fda-recall-api/">Interactive Feedback On The FDA Recall API</a></h3>
			<p><em>15 Nov 2013</em></p>
			<p>
I have a long list of little projects I'm working across government, and since I'm not being paid for some of this work now (except for the support of my amazing partners), the publicity and page views for this work is all I got! :-) So I'm publishing the stories around everything I do.
Up next: I was asked for some feedback on the FDA Recall API that is in pre-production currently, and I thought, what better way to offer feedback on API design than actually sculpting a Swagger definition of the changes and making it usable via Github.
Overall the FDA Recall API design was perfectly suitable for launch, even though the interface isn't perfect and some of the data is pretty messy, but I'm a big fan of just getting things out there, letting developers hack on it and gather feedback for the next iteration.
The existing FDA Recall API allows you to pull weekly recalls for 5 separate product types including Food, Drugs, Veterinarian, Devices and Biologics. Using the API you can pull each week listing of recalls as well as pull detail on individual recalls.
To get a handle on where they should take the interface I needed to hack on it like I was a developer, so I setup a proxy that runs on AWS EC2 and started coding against each endpoint, then adjusting a Swagger definition until I had something that was acceptable for a v.01.
You can find the result of this work over at the FDA Recall API working project I stood up on Github, it should work until they make any adjustments to the pre-production API. You can follow any work I do on FDA open data and APIs at the main project site I setup.
[<a href="/2013/11/15/interactive-feedback-on-the-fda-recall-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/IFTTT___Connect_YouTube_to_SoundCloud.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/15/apis-give-you-the-ability-to-define-and-execute-on-the-actions-that-are-most-important-to-you/">APIs Give You The Ability To Define And Execute On The Actions That Are Most Important To You</a></h3>
			<p><em>15 Nov 2013</em></p>
			<p>I am a programmer with a full understanding of how to deploy, consume and put APIs to use. Even with this knowledge and ability I'm continually blown away by the opportunities APIs afford, even without possessing any programming skills whatsoever. One API I depend on for my daily monitoring of the API space is the Pinboard API. As I browse the web, using the Pinboard bookmarklet I can pin any web page for use later, then using the Pinboard API I pull these curated pages into my monitoring, reporting and story development platform. To take this feature one step further, when I star a Tweet on Twitter, Pinboard watches this and automatically bookmarks the Tweet, adding it to my curation as well. This approach to using APIs has allowed me to define how I use bookmarking as well as what a favorited Tweet means in my personal world of online curation. Using APIs I'm able to wield this power, and drive my story curation and development lifecycle. Of course I wrote the code that interfaces with Pinboard API, but it still shows the ability to define and execute actions in my world using APIs. As I was pondering this self-defined action that I use on a daily basis, I was trying to think of any new actions that I could define and add to my toolbox. I was looking through the Youtube videos from my recent #APIStrat conference, and I thought how nice it would be to turn these into audio segments available as podcasts through SoundCloud. After a quick search I found an IFTTT recipe that does this automatically when I flag any Youtube video for watching later and automatically uploads as an audio file to SoundCloud. Sweet! This type of action again shows the power of being able to define and execute your own actions that you can take in your everyday worlds, but in this case you don't have to be a...[<a href="/2013/11/15/apis-give-you-the-ability-to-define-and-execute-on-the-actions-that-are-most-important-to-you/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-github.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/12/if-gov-then-that/">If Gov Then That</a></h3>
			<p><em>12 Nov 2013</em></p>
			<p>
I just found an interesting brainstorm going on via Github, about how to make government more efficient and interoperable using APIs, that was created by government consultant Leah Bannon (@leahbannon), called If Gov Then That.
The idea?  Promote gov APIs by making them dead simple and suggesting clever recipes (like IFTTT and Zapier)
Leah has started a rough list of some possible recipes, but needs your help to brainstorm what you'd like to see for developing interoperability and automation recipes in our government.
You can contribute via the Github project, on the conversation thread she started on the US Government APIs Google Group, and she will be working on as part of Code for DC.
[<a href="/2013/11/12/if-gov-then-that/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-google-docs.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/12/deploy-api-private-google-spreadsheet-to-api/">Deploy API: Private Google Spreadsheet to API</a></h3>
			<p><em>12 Nov 2013</em></p>
			<p>I'm working on a series of simple scripts that help people deploy APIs from some of the most common data sources. I'm starting with a series of PHP scripts, and next up is a private Google Spreadsheet to API, using JSON stored in Github. For this PHP implementation, I'm using the SLIM framework, which provides a dead simple REST framework you can use to deploy an API from a variety of data sources. To begin deploying an API from your Google Spreadsheet datastore, download the REST library and upload to your server that runs PHP. Slim is pretty straightforward to work with, to add each API endpoint you just add a single PHP file under methods. For this how-to guide we are going to add a simple endpoint from our JSON products data store. This project also depends on the Google Drive API and I use the Google API PHP Client to connect to Google and provide necessary oAuth connectivity.&nbsp; Before all of this works you need to have an oAuth token, which I created a simple script to handle: I leave it to you to figure out where you want to store your oAuth tokens, and other goods. I use a config.php file, but can easily be done from database or other: Next you just add an include reference in the index page for your slim implementation: This API just uses ID, Name, Price and Description of the product, and queries by a simple query parameter. You can use this as a template for your own product database, adding and removing fields as you need, or completely retrofitting for any database table. That is it, now you have a simple product API that pulls a list of products from a private Google Spreadsheet data store. This project is more excercise than anything else. I don't recommend deploying a public API driectly off a private Google Spreadsheet. Next up, will be to secure it using...[<a href="/2013/11/12/deploy-api-private-google-spreadsheet-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-google-docs.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/12/common-data-sources-to-api-definition/">Common Data Sources To API Definition</a></h3>
			<p><em>12 Nov 2013</em></p>
			<p>
I've been working through several demos of how to go from common data sources like MySQL and Google Spreadsheet to API over the last couple of weeks.  So far I have five basic working demos:

MySQL to API
Local JSON to API
Github JSON to API
Public Google Spreadsheet to API
Private Google Spreadsheet to API

These demos are meant to show how easy it is to deploy a simple API from common places you already have data stored. Right now I'm building them in PHP, but will also be building Python, Ruby and Node.JS versions.
These exercises are meant for my newbie readers but are also helping me see API deployment in different ways. I realize how difficult it is to write code that will work for all scenarios. There is a lot of custom work on the data store side, as well as possible the API side, making me feel like it is best to use an API definition in the middle.
With this in mind, the next phase of my API deployment tools will be about generating API definitions, which currently will be Swagger, but will also develop RAML and API Blueprint versions of the same code eventually.
Once I have some data store to Swagger tooling I will evolve some of the code I have for generating APIs from Swagger definitions.
Then I will have the different custom parts decoupled from each other, all centered around an API definition and data model which I can publish in API Commons and evolve in an open, collaborative environment where everyone can benefit, and contribute to my API work.
[<a href="/2013/11/12/common-data-sources-to-api-definition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-google-drive-icon.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/12/9-essential-languages-for-your-api-code-libraries/">9 Essential Languages For Your API Code Libraries</a></h3>
			<p><em>12 Nov 2013</em></p>
			<p>
I was working with Google APIs over the last couple days while building Google Spreadsheet to API tools. It gave me a chance to look around the Google Developers Area and rediscover some of the positive approaches the API pioneer brings to the table.
In this post I wanted to highlight the Google Drive Code Libraries. While the Google approach isn't perfect, I think it sets a good bar for what can be achieved by API providers when delivering their own API code libraries.

I think the languages represented are the baseline for any modern API, and all API providers should consider providing the following languages:

.NET
Go
Java
JavaScript
Node.js
Objective-C
PHP
Python
Ruby

Languages like Go and Node.js are definitely forward leaning, but represent very fast growing areas of the API integration space.
Java and Objective-C represent the mobile space, something API providers can't be ignoring in 2013.
If your target audience is the enterprise, you have to have .NET and Java as part of your API library.
PHP, Python and Ruby are the web staples, that are default for any API that is catering to general web developers.
It should get easier to generate code libraries automatically for API providers who have developed some sort of API definition for their APIs, but for others you will still have eto hand-roll your own code lbiraries.
[<a href="/2013/11/12/9-essential-languages-for-your-api-code-libraries/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/dwolla-open-source-iphone-app.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/08/dwolla-open-sources-mobile-payment-app/">Dwolla Open Sources Mobile Payment App</a></h3>
			<p><em>08 Nov 2013</em></p>
			<p>
Every API provider should provide code samples in a variety of languages, helping developers get up and running as fast as possible.
Dwolla is taking this one step further and providing a fully functional iOS app that developers can fork, tweak and use as they wish.
I've talked about what I call "starter kits" before, when I showcased Google+ sample social applications in a variety of languages, but it is still something I don't see from very many API providers.
Complete mobile applications is a great way to showcase what a complete API integration will look like, helping developers see the end goal clearly, while also teaching them best practices.
Do you have any applications you could open source and allow developers to reverse engineer? Or maybe an app you could developer to show developers what is possible!
[<a href="/2013/11/08/dwolla-open-sources-mobile-payment-app/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/slashdb-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/08/automatic-rest-api-for-databases-as-complete-amazon-machine-image/">Automatic REST API for Databases As Complete Amazon Machine Image</a></h3>
			<p><em>08 Nov 2013</em></p>
			<p>
SlashDB aka /db, has recently been added to the Amazon Marketplace, providing a complete database to API solution as an Amazon Machine Image (AMI).
Companies can use the /db Amazon image to automatically generate REST APIs from common relational databases like Microsoft SQL Server, Oracle, MySQL, PostGreSQL, which includes Amazon RDS.
/db charges based upon the number of databases you launch and the number of users that are using the API for the database, and you will have to pay for the regular charges for any Amazon EC2 instances.
I like the idea of building API solutions and launching as Amazon Machine Images. I think ready-to-go platform solutions like this for AWS, Google, Heroku and other top cloud platforms are good for potential API providers.
[<a href="/2013/11/08/automatic-rest-api-for-databases-as-complete-amazon-machine-image/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-github.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/07/apis-ongithub/">APIs Ongithub</a></h3>
			<p><em>07 Nov 2013</em></p>
			<p>I've created a new playground for some of my work. Pretty much everything at API Evangelist runs on Github, and each new project I produce starts its life as a Github repository. To support this work I established a new domain called ongithub. My first series in the ongithub realm is based upon an approach I'm using to deploy very simple APIs, while also introducing and educating people about APIs. I have a handful of simple API designs from working in the government, so I married them with data models derived from schema.org, and publish over 10 very simple API design patterns. Every one of these demo APIs began as a simple Swagger specification, which I quickly spun into a web API using the Slim REST Framework, then generated a simple API microsite complete with interactive documentation. These Swagger specifications were the seed of the recent API Commons that 3Scale and API Evangelist just launched. I have plenty of other API designs laying around, but this seemed like a good way to seed the API Commons with some simple and common designs. You can find the following 11 designs at api.ongithub.com: &nbsp; Businesses This is a simple API specification for a listing of businesses. Events This is a simple API specification for a listing of events. Images This is a simple API specification for a listing of photos and images. Jobs This is a simple API specification for a listing of jobs. Offices This is a simple API specification for a listing of offices. Places This is a simple API specification for a listing of places. People This is a simple API specification for a listing of people. Press This is a simple API specification for a listing of news and press releases. Products This is a simple API specification for a listing of products. Programs This is a simple API specification for a listing of programs. Videos This is a simple API specification for...[<a href="/2013/11/07/apis-ongithub/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/api-commons-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/06/putting-the-open-in-api-with-the-api-commons/">Putting The Open In API With The API Commons</a></h3>
			<p><em>06 Nov 2013</em></p>
			<p>Steve Willmott(@njyx) from API infrastructure provider 3Scale and API Evangelist launched a new partner project yesterday at Defrag, that we are calling API Commons. The mission with API commons is to provide a simple and transparent mechanism for the copyright free sharing and collaborative design of API specifications, interfaces and data models. For a space that is about open access and interoperability the API industry has been very closed about their API designs, and after helping the EFF urge the courts to block copyright claims in the Oracle v. Google API fight, Steve and I thought it would be a good idea to introduce an API commons that would help put the "open" into API designs and data models, and back into the API space in gernal. API Commons is build entirely on Github and is meant to act as a common index of API definitions and data models that anyone can add by generating what we are calling an API Commons Manifest that points to your API definition(s). Using the existing layers of Github, anyone can fork, favorite or follow the best API design patterns and eventually help establish some clear best practices across any government or business sector. Everything you need to get going is on the site at apicommons.org. You can join the discussion on the Google Group or by following us on Twitter. We are in early stages, but will be dedicating a great deal of time over the holidays to get more API definitions in the commons. 3Scale and I strongly believe that API Commons is what the API industry needs to help deal with some of the clutter in the number of API designs, incentivize the development of open and common tooling around the best API designs and bring reuse, collaboration and copyright concerns out into the open. Also check out some of the great news coverage of API Commons over the last 24 hours: New API Commons Platform...[<a href="/2013/11/06/putting-the-open-in-api-with-the-api-commons/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/open-government-partnership.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/11/06/open-data-and-api-efforts-rendered-useless-when-privacy-is-ignored/">Open Data And API Efforts Rendered Useless When Privacy Is Ignored</a></h3>
			<p><em>06 Nov 2013</em></p>
			<p>On the second anniversary of the Open Government Partnership (OGP), where we are celebrating a "global effort to encourage transparent, effective, and accountable governance", and that: OGP has grown to 60 countries that have made more than 1000 commitments to improve the governance of more than two billion people around the globe. OGP is now a global community of government reformers, civil society leaders, and business innovators working together to develop and implement ambitious open government reforms and advance good governance. That is some pretty significant platform growth! While reading this I'm reminded of how any amount of perceived growth and value delivered via an "open data or API platform" can be immediately muted by the omission of very fundamental building blocks like privacy. Let's review the building blocks of the Open Government Alliance: Expand Open Data -&nbsp;Open Data fuels innovation that grows the economy and advances government transparency and accountability. Government data has been used by journalists to uncover variations in hospital billings, by citizens to learn more about the social services provided by charities in their communities, and by entrepreneurs building new software tools to help farmers plan and manage their crops. Building upon the successful implementation of open data commitments in the first U.S. National Action Plan, the new Plan will include commitments to make government data more accessible and useful for the public, such as reforming how Federal agencies manage government data as a strategic asset, launching a new version of Data.gov, and expanding agriculture and nutrition data to help farmers and communities. Modernize the Freedom of Information Act (FOIA) -&nbsp;The FOIA encourages accountability through transparency and represents a profound national commitment to open government principles. Improving FOIA administration is one of the most effective ways to make the U.S. Government more open and accountable. Today, the United States announced a series of commitments to further modernize FOIA processes, including launching a consolidated online FOIA service to improve customers&rsquo; experience and...[<a href="/2013/11/06/open-data-and-api-efforts-rendered-useless-when-privacy-is-ignored/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-google-docs.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/22/deploy-api-public-google-spreadsheet-to-api/">Deploy API: Public Google Spreadsheet to API</a></h3>
			<p><em>22 Oct 2013</em></p>
			<p>
I'm working on a series of simple scripts that help people deploy APIs from some of the most common data sources. I'm starting with a series of PHP scripts, and next up is a public Google Spreadsheet to API, using JSON stored in Github.
For this PHP implementation, I'm using the SLIM framework, which provides a dead simple REST framework you can use to deploy an API from a variety of data sources.   To begin deploying an API from your Google Spreadsheet datastore, download the REST library and upload to your server that runs PHP.
Slim is pretty straightforward to work with, to add each API endpoint you just add a single PHP file under methods. For this how-to guide we are going to add a simple endpoint from our JSON products data store.

Next you just add an include reference in the index page for your slim implementation:

This API just uses ID, Name, Price and Description of the product, and queries by a simple query parameter. You can use this as a template for your own product database, adding and removing fields as you need, or completely retrofitting for any database table.  That is it, now you have a simple product API that pulls a list of products from a Google Spreadsheet data store.
[<a href="/2013/10/22/deploy-api-public-google-spreadsheet-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-github.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/22/deploy-api-github-json-to-api/">Deploy API: Github JSON to API</a></h3>
			<p><em>22 Oct 2013</em></p>
			<p>
I'm working on a series of simple scripts that help people deploy APIs from some of the most common data sources. I'm starting with a series of PHP scripts, and next up is JSON to API, using JSON stored in Github.
For this PHP implementation, I'm using the SLIM framework, which provides a dead simple REST framework you can use to deploy an API from a variety of data sources.   To begin deploying an API from your JSON datastore located at Github, download the REST library and upload to your server that runs PHP.
Slim is pretty straightforward to work with, to add each API endpoint you just add a single PHP file under methods. For this how-to guide we are going to add a simple endpoint from our JSON products data store.

Next you just add an include reference in the index page for your slim implementation:

This API just uses ID, Name, Price and Description of the product, and queries by a simple query parameter. You can use this as a template for your own product database, adding and removing fields as you need, or completely retrofitting for any database table.  That is it, now you have a simple product API that pulls a list of products from a JSON data store stored at Github.
[<a href="/2013/10/22/deploy-api-github-json-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-toolbox.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/22/api-deployment-from-mysql-json-github-and-google-spreadsheets/">API Deployment From MySQL, JSON, Github and Google Spreadsheets</a></h3>
			<p><em>22 Oct 2013</em></p>
			<p>
I'm doing a lot more API deployments from dead simple data sources since I started working in the federal government. As part of these efforts I'm working to put together a simple toolkit that newbies to the API world can use to rapidly deploy APIs as well.
Currently I have four separate deployment blueprints done:

MySQL to API
Local JSON to API
Github JSON to API
Google Spreadsheet to API

All of these samples are in PHP and uses the Slim PHP REST framework. They are meant to be working examples that you can use to seed your own API deployment.
I'm also including these in my government API workshop at #APIStrat this week, hoping to get other people equipped with the necessary skills and tools they need to get APIs in the wild.
You can find the entire working repository, including Slim framework at Github.
[<a href="/2013/10/22/api-deployment-from-mysql-json-github-and-google-spreadsheets/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-mysql.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/21/deploy-api-mysql-to-api/">Deploy API: MySQL to API</a></h3>
			<p><em>21 Oct 2013</em></p>
			<p>
I'm working on a series of simple scripts that help people deploy APIs from some of the most common data sources. I'm starting with a series of PHP scripts, and first up is MySQL to API.
For this PHP implementation, I'm using the SLIM framework, which provides a dead simple REST framework you can use to deploy an API from a variety of data sources.   To begin deploying an API from your MySQL database, download the REST library and upload to your server that runs PHP.
Slim is pretty straightforward to work with, to add each API endpoint you just add a single PHP file under methods. For this how-to guide we are going to add a simple endpoint from our products database.

Next you just add an include reference in the index page for your slim implementation:

This API just uses ID, Name, Price and Description of the product, and queries by a simple query parameter. You can use this as a template for your own product database, adding and removing fields as you need, or completely retrofitting for any database table.  That is it, now you have a simple product API that pulls a list of products from a MySQL database.
[<a href="/2013/10/21/deploy-api-mysql-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-json-data-store.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/21/deploy-api-json-to-api/">Deploy API: JSON to API</a></h3>
			<p><em>21 Oct 2013</em></p>
			<p>
I'm working on a series of simple scripts that help people deploy APIs from some of the most common data sources. I'm starting with a series of PHP scripts, and next up is JSON to API.
For this PHP implementation, I'm using the SLIM framework, which provides a dead simple REST framework you can use to deploy an API from a variety of data sources.   To begin deploying an API from your JSON datastore, download the REST library and upload to your server that runs PHP.
Slim is pretty straightforward to work with, to add each API endpoint you just add a single PHP file under methods. For this how-to guide we are going to add a simple endpoint from our JSON products data store.

Next you just add an include reference in the index page for your slim implementation:

This API just uses ID, Name, Price and Description of the product, and queries by a simple query parameter. You can use this as a template for your own product database, adding and removing fields as you need, or completely retrofitting for any database table.  That is it, now you have a simple product API that pulls a list of products from a JSON data store.
[<a href="/2013/10/21/deploy-api-json-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bureau-of-labor-statistics-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/19/government-api-opportunities-bureau-of-labor-statistics/">Government API Opportunities: Bureau of Labor Statistics</a></h3>
			<p><em>19 Oct 2013</em></p>
			<p>I'm working to expand my awareness of APIs in our federal government by spending time each week discovering, reviewing and trying to brainstorm ways to expand and evolve existing government API efforts. Today I was reviewing the Bureau of Labor Statistics API, where I was pleased to find this valuable labor data available via single series, multiple series, one or more series specifying years. The API is a pretty straightforward web API, which could use some polishing, but overall it provides machine readable access to this very important data as I would expect. When I look at federal government APIs I'm trying to find at least one or two ways to help move the API forward, either as constructive criticism for the API providers or ways that the public (me included) can help evolve the community or the API itself from the outside. My current contributions to the Bureau of Labor Statistics is to add support for the Series ID to the API stack. The Series ID is the single, central parameter you pass to each of the API endpoints, but nowhere does it link to or help the users understand the Series ID, which plays a central role in API operations. This type of omission by API providers is common. You have the domain expertise in your area, you know the Series ID is the central character, but to people stumbling across or intentionally pulling up the Bureau of Labor Statistics API, this might not be common knowledge. With this in mind, the roadmap of the Bureau of Labor Statics API should include an API for all of the building blocks and meta data for the Series ID. There is a lot of rich data available on a Series ID, and while it would take a significant amount of work to develop additional APIs around this data, I think it would significantly add value to the Bureau of Labor Statistics API and increase adoption....[<a href="/2013/10/19/government-api-opportunities-bureau-of-labor-statistics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-government.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/17/shutdown-of-government-open-data-and-apis-is-not-government-services-business-as-usual/">Shutdown of Government Open Data and APIs Is NOT Government Services Business As Usual</a></h3>
			<p><em>17 Oct 2013</em></p>
			<p>During the recent federal government shutdown many sources of open data and APIs were suddenly rendered unavailable, including the flagship Data.gov. As government workers went home and lights were turned off at federal agencies, so too were the servers that hosted much of the open data and APIs that have been opened up in the last couple years. Across the web I've encountered discussions from many individuals who state this is how government services work. When government funding disappears, the government services go away, suck it up. I'm sorry, but this is unacceptable in the Internet age. If you see things this way, you are part of the machine that allows government services to be used as a political tool. I agree, that human powered government services go away when funding disappears, but in the age of open data, APIs and the cloud, services are designed and deployed to be self-service and highly available. When you launch APIs in the cloud, you bundle your budget, service level agreement and the tech into a single package. My funding went away this month and couldn't afford my AWS bill on 10/1, but my server didn't shut down the minute the shutdown happened. I had until the end of the month. Ideally the federal government could go with reserved instances annually or at least quarterly, securing the funding needed to outlast any shutdown. Even with my heavy usage of AWS for much of my infrastructure, the majority of my world runs on open repositories on Github. I have carefully crafted my public presence using open formats like HTML, CSS, JavaScript and JSON and host these as openly licensed, public repositories that I can operate as no cost the social coding platform. There is no excuse for government open data data and API services to go away during a shutdown like we just experienced. Open data and APIs represent a new, self-service future for government services. This model won't...[<a href="/2013/10/17/shutdown-of-government-open-data-and-apis-is-not-government-services-business-as-usual/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/universal-library-sign.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/17/api-and-oauth-literacy-is-as-important-as-financial-literacy-in-the-api-economy/">API and OAuth Literacy Is As Important As Financial Literacy in the API Economy</a></h3>
			<p><em>17 Oct 2013</em></p>
			<p>The primary mission of API Evangelist is to spread awareness of APIs amongst the masses, expanding the audience beyond just the IT crowd, and developer community. Initially I wanted to make sure business leaders understood the potential of APis, so that they funded API initiatives within their companies. In 2012 I feel that APIs have hit a critical mass, and while I still evangelize APIs to business leaders I'm shifting a portion of my focus to the average Internet user. APIs impact almost every aspect of our daily lives from logging into Facebook on our mobile phones to purchasing gasoline at the corner gas station. As API usage spreads across business, the government and the Internet of Things (IoT), the everyday citizen will be using APIs more and more each day. While many of these citizens will never hack on an API at the code level, I'm seeing a need emerge for everyone individual to be aware of APIs, much like they need a certain level of economic and financial awareness in every day life. When it comes to our world of finances, every single adult must have a certain level of awareness of how our financial system operates. You don't need to understand the inner workings of banking and global markets, but you need to know how to setup a bank account, apply for credit or debit cards, balance your checkbook and pay your taxes. As our lives move online and the API economy grows, our data is fast becoming the online currency on Internet platforms like Facebook, Twitter and Pinterest, the need for us to understand the mechanisms at play in this new digital economy increases. Virtually every platform we use online employs APIs in some capacity. These platforms use APIs and often an open authentication standard called oAuth to allow us to give access to our data that resides on platforms to the applications and other systems we use daily. We login...[<a href="/2013/10/17/api-and-oauth-literacy-is-as-important-as-financial-literacy-in-the-api-economy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/mapbox-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/16/foundry-group-makes-investment-in-the-open-standards-api-driven-javascript-approach-of-mapbox/">Foundry Group Makes Investment In The Open Standards, API Driven, JavaScript Approach Of MapBox</a></h3>
			<p><em>16 Oct 2013</em></p>
			<p>On June 29th, 2006, Google launched Google Maps API allowing developers to put Google Maps on their own sites using JavaScript. The API launch was just shy of 6 months after the release of Google Maps as an application, and was in direct response to the number of rogue apps developed that were hacking the application--demonstrating the demand for a JavaScript based, API driven mapping solution for developers. Fast-forward 7 years, and maps are a central fixture of virtually every web and mobile application we depend on daily. While Google Maps is still the heavyweight in the space, their now classic map interface, proprietary tooling and search centric mindset leaves a huge opportunity for disruption in the app economy, and the venture capital firm Foundry Group is betting that startup mapping provider MapBox is the solution that will de-throne Google, with a 10M investment in MapBox&nbsp;earlier today. What makes MapBox such a good investment? At first look, it is clear that MapBox is winning over developers and existing players like Foursquare and Evernote by providing very clean, attractive mapping solutions that contain street, terrain and satellite layers, but when you take a closer look at the platform, you see the MapBox allure isn't just about maps. The value of MapBox is also about their approach to delivering a platform that begins with a heavy focus on open specifications, embracing of APIs, heavy investment in JavaScript, and a knowledge of modern cloud architecture, with strong support for mobile apps. Open SpecificationsIn 2013, when you are looking to deploy a true platform, you have to shed your self-centered approach to technology, do your homework on what are the best practices and standards that exist across your industry, and don't re-invent the wheel or try to keep things closed and proprietary, making everything you do a two-way street that benefits the entire ecosystem. This is how you establish a true platform ecossytem, one that developers will believe in...[<a href="/2013/10/16/foundry-group-makes-investment-in-the-open-standards-api-driven-javascript-approach-of-mapbox/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/kinlane-openva-api-visual-notes.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/15/visual-notes-from-my-talk-on-apis-and-the-future-of-education-at-openva/">Visual Notes From My Talk On APIs And The Future of Education At OpenVA</a></h3>
			<p><em>15 Oct 2013</em></p>
			<p>Audrey and I went up to University of Mary Washington yesterday and participated in the #OpenVa discussion, where I gave a presentation on the importance of APIs and how they will play a significant role in the future of education.
During the talk Giulia Forsythe (@giuliaforsythe) sketched some visual notes that I think are pretty damn cool!

You can view her blog post and other sketches on her blog at Minding the Future [Visual Notes} #OpenVA.  You can also find the slides for my talk on Github under OpenVA - The Future of Educaiton.
I had a great time brainstorming with everyone up at UMW, and will have more posts in the near future about the other plotting and scheming we did regarding the future of education and APIs.
[<a href="/2013/10/15/visual-notes-from-my-talk-on-apis-and-the-future-of-education-at-openva/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/APIStrat-Home-Page-Slice-2.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/12/two-weeks-until-api-strategy-amp-practice-in-san-francisco/">Two Weeks Until API Strategy &amp; Practice in San Francisco</a></h3>
			<p><em>12 Oct 2013</em></p>
			<p>
I'm working through the schedule for API Strategy &amp; Practice Conference (#APIStrat) in San Francisco, preparing for the 3Scale / API Evangelist produced event October 23rd through 25th.
I'm pretty excited about the lineup we've managed to assemble including keynotes from Pamela Fox (@pamelafox) from Khan Academy, Daniel Jacobson (@daniel_jacobson) of Netflix, Wynn Netherland (@pengwynn) from Github and Kristin Calhoun (@KCalhoun) the director of the Public Media Platform, just to name a few of them.
We've worked hard to put together sessions that would speak to all aspects of the API space including creation, design and documentation of APIs, API discovery, hypermedia APIs, API testing &amp; debugging, API marketing &amp; evangelism and business models. The goal was to cover topics from basic to advanced, while covering APIs from start to finish.
I'm particularly stoked about the location of the conference. It is right downtown at the Parc55 Hotel, easy walking distance to and from anything.  I always hate conferences that are off the beaten path, and Parc55 will make it easy to hang out and eat, drink and network around the event before and after.
The New York City edition of #APIStrat back in February was sold out with over 350 people in attendance. San Francisco is also on track to be sold out, this time with over 600 people in attendance.  Steve Willmott(@njyx) of 3Scale and I have a blast MC'ing the entire event and facilitating the API conversations, both business and technology, and gearing up to make sure #APIStrat flows.
Make sure and GET REGISTERED. It was painful to turn away everyone last time, and I'd hate for you to not be able to get in. It is going to be an informative three days with the people who make the API space move forward. I look forward to seeing you there.
[<a href="/2013/10/12/two-weeks-until-api-strategy-amp-practice-in-san-francisco/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/delray-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/12/from-extract-transform-amp-load-to-input-process-and-output-with-delray/">From Extract, Transform &amp; Load To Input, Process and Output With Delray</a></h3>
			<p><em>12 Oct 2013</em></p>
			<p>
I spend a lot of time finding valuable data sets and manually converting, processing and outputting into more usable formats, so that they can be used in APIs that drive mobile and web applications.
I am always on the lookout for new tools that will help me be more efficient in to my work. I'm currently test driving a new platform called Delray that focuses on taking an older concept of extract, transform and load (ETL) and bringing it into the API age by allowing me to define common data resources as inputs, process them one time or on schedule and output data in a cleaner, more usable format.

Using Delray I can define an input from CSV, JSON, MSSQL and other common formats, and save this as the input for my workflow.

Next I can setup a configure a cleansing stage to process the data, allowing me to trim whitespace, replace space, make lowercase and other common things I tend to do manually with my data sets.

Finally I can output the CSV inputed data as a JSON file for use in my APIs and other open data efforts.

Delray represents the next generation of tools that will turn anyone into a data steward, allowing non-developers to take control of critical data flows within your business, organization or government agency.
Opening up data in machine readable format is not just for IT and developers anymore.
[<a href="/2013/10/12/from-extract-transform-amp-load-to-input-process-and-output-with-delray/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/tag-cloud-education.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/12/access-interoperability-privacy-and-security-of-technology-will-set-the-stage-for-the-future-of-education/">Access, Interoperability, Privacy and Security Of Technology Will Set The Stage For The Future of Education</a></h3>
			<p><em>12 Oct 2013</em></p>
			<p>In 2010 when I started API Evangelist I saw the technological potential of APIs, but while the rest of the online space was focused on what APis could do for developers, I was focused on what APIs could do for the average person. APIs don't just open up access for developers, they open up access for end-users, introducing interoperability, data portability and ultimately tools that give them control over their own data, content and other valuable resources. This realization has been central to my mission at API Evangelist, which is about educating the masses about APIs. What is an API? Why are APIs important? I strongly feel that APIs empower end-users to make better decisions about which platforms they use, which applications they adopt, and gives them more ownership, control and agency in their own worlds. When you help an individual understand they can host their own Wordpress blog and migrate from the cloud hosted version of Wordpress, or migrate their blog from Blogger to Wordpress via APIs, you are giving the gift of web literacy. Leading technology platforms like Amazon, Google, eBay and Flickr have long realized the potential of opening up APIs and empowering end-users. Since then, thousands of platform providers have also realized that opening up APIs enables developers and end-users to innovate around their platform and services, and that there is much more opportunity for growth, expansion and revenue when end-users are API literate. Users are much more likely to adopt a platform and deeply integrate it into their personal or business lives, if they are able to connect it with their other cloud services, taking control and optimizing their information and work flow. Helping business owners, developers and end-users understand the potential that APIs introduce is essential to the future of education, and will be the heart of a healthy and thriving economy. There is a key piece of technology that reflects this new paradign and is currently operating and...[<a href="/2013/10/12/access-interoperability-privacy-and-security-of-technology-will-set-the-stage-for-the-future-of-education/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="https://s3.amazonaws.com/kinlane-productions2/events/defrag-2013/DEFRAG-2013-2.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/11/how-to-defrag-your-brain-and-tech-career-in-november/">How To Defrag Your Brain and Tech Career In November</a></h3>
			<p><em>11 Oct 2013</em></p>
			<p>I spend the year going from city to city, attending conferences, meet-ups and hackathons--speakng, networking and doing the things that makes my world go around. Every November I find myself a little spun out from the year and need to Defrag my brain, re-organize what I know and re-enforce the most meaningful relationships across my professional career. This is is done in Broomfield Colorado each November at the Defrag Conference. I arrive the evening before the conference at the Omni Interlocken Resort. It is late at night, the lobby is empty and I walk up to the counter and quickly get my room from the young lady running the desk. I turn around to head to the elevators and notice Brad Feld(@bfeld) sitting on the couch, lost in his phone. I spend some time engaging him about the latest trends in technology and investing and the current political climate in the country before I head off to bed. Where else do you get casual focus time with folks like Brad, then at Defrag? In the morning I wake up, grab some coffee and pastries and head for the main stage. As I walk in I feel like I've walked into a 1980's rock concert as I'm blasted a classic rock soundtrack from Tom Petty to AC/DC. I grab a seat in the dark and as I'm waiting for keynotes to start I notice I've accidently joined a table of the cloud computing mafia ranging from Alex Williams(@alexwilliams)&nbsp;of TechCrunch to cloud pundit Ben Kepes (@benkepes) of Diversity.net.nz. The morning moves by fast with a engaging array of talks from Amber Case of Esri, to Paul Kedrosky of Bloomber and Kauffman Foundation. After they are done, I roll out of the main stage and get in line for some lunch, which is the only conference lunch I will actually eat, as the Omni Interlocken catering is not the normal fare. As soon as I'm done I weave...[<a href="/2013/10/11/how-to-defrag-your-brain-and-tech-career-in-november/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/baas-trends.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/09/the-backend-as-a-service-space-is-maturing/">The Backend As A Service Space Is Maturing</a></h3>
			<p><em>09 Oct 2013</em></p>
			<p>
I just got off the phone with a new Backend as a Service provider BizMobify, who is looking to deliver BaaS services to the enterprise.  The timing for the call couldn't be better, as I'm updating up my BaaS white paper this week, and one thing I'm expanding is looking at it through the enterprise lens.
As I dust off my research on BaaS I'm re-visiting my BaaS research site and re-watching the BaaS Panel from API Strategy &amp; Practice in NYC last February. This is helping me understand where the space what last winter and early spring.
As we move into the last quarter of 2013 I'm reminded of how fast the BaaS space is maturing. There are new providers continuing to enter the space, but I also see continued energy and releases from the BaaS leaders like Parse and Kinvey.
Kinvey, AnyPresence and StackMob will all be at #APIStrat on October 24th and 25th in San Francisco. While we won't be having another BaaS focused panel, they will be sharing their insight on the space in separate sessions.
From my vantage point I see BaaS providers being a key channel for API providers to reach developers in 2014, and definitely an area I will keep tracking on and working to understand.
If you are an API provider, you should be paying attention to BaaS providers, because they offer a channel for your API resources to reach developers. Developers may not care about your resource all by itself, but when bundled with the best of breed BaaS tools as well as other API resources, it may look much more appealing.
[<a href="/2013/10/09/the-backend-as-a-service-space-is-maturing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/data-gov-shutdown.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/10/01/can-we-depend-on-federal-government-apis/">Can We Depend On Federal Government APIs?</a></h3>
			<p><em>01 Oct 2013</em></p>
			<p>
The federal government shutdown today. At the Department of Veterans Affairs we are still working through next monday, so it was business as usual today for me.
One of my projects is preparing a list of APIs and data assets for a hackathon in NYC that is focusing on developing apps for veterans, called The Feast. My goal is to aggregate a list of as many usable assets as possible so developers don't have to go to multiple locations to locate them.
Many of the assets are available at VA.Gov, but there are others that I knew were on Data.Gov. As soon as the page loaded at Data.Gov, I got a friendly message:
Due to the lapse in federal government funding, this website is not available. We sincerely regret this inconvenience.
Basically, no data for me. No datasets for download, no APIs allowing me access information.  Which leaves me questioning the entire reason I'm in government. I came here to open up data and build APIs, then convince you to use them in your applications, widgets, visualizations and other innovation projects.
It is my way to find a way around ANY obstacle. There is nothing that will stop me. I will keep working no matter what. However, this is a pretty big obstacle, something that makes me question my faith in APIs in government. I can deal with just about anything, but if I evangelize APIs, that can be turned off at any point, for an unknown amount of time? Ummm&hellip;WTF? Unnacceptable!!
There is always a work-around. I will find a solution that will work for me(and you), but it leaves me questioning quite a bit about this API evangelism from within the federal government.
[<a href="/2013/10/01/can-we-depend-on-federal-government-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/the-ohana-api.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/29/providing-access-to-services-that-help-americans-with-their-food-security-using-apis/">Providing Access To Services That Help Americans With Their Food Security Using APIs</a></h3>
			<p><em>29 Sep 2013</em></p>
			<p>
I had the pleasure to connect with the talented Code for America fellow, Moncef Belyamani(@monfresh)&nbsp;this week and talk about a very meaningful API project, called the Ohana API.
"The Ohana API is a project from the San Mateo County team of Code for America fellows that is aiming to create open access to community social services, with an initial emphasis on food security."
I couldn't' think of a more important use of APIs, than making sure people can find the soical services they need--especially services that ensure people are fed.
I'm also impressed with the approach of Code for America in giving Github a central role in the project. The API project, the API wrapper in Ruby and a cool API to PDF generator are all available on Github.
The Oahana API is only in alpha, they are looking for people to help the Code for America team take it to the next level with approaches to keep the data current and developing an SMS interface.
The Ohana API project itself, and the model used by Code for America, provides an important blueprint for how technology can be applied, and make a difference in our daily lives in a scalable way.
This type of work keeps me coming back back and working on API Evangelist, even after three years of covering a space that often leaves me pretty discouraged.
[<a href="/2013/09/29/providing-access-to-services-that-help-americans-with-their-food-security-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-barcode.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/25/start-with-your-public-website-when-you-begin-your-inventory-for-data-apis/">Start With Your Public Website When You Begin Your Inventory For Data APIs</a></h3>
			<p><em>25 Sep 2013</em></p>
			<p>I'm working on taking inventory of data assets at the Department of Veterans Affairs. While eventually this will include private data assets, in the beginning we are focusing on data that can be made public without being worried about personally identifiable information or health information.
There are a lot of hurdles to get over, including educating people about what is a data asset, as well as the process of identifying, preparing, publishing and listing the data assets in a publicly available marketplace.
We are at the point where everyone is looking around for potential data assets that can be included in the current open data inventory cycle.  It is proving to be a challenge for folks to find datasets they can include. To help the process along I've set my focus on the public website, va.gov, and finding data that has already been published in various formats on the site.
Data and content is already available on the public web site represents the low hanging fruit when it comes to identifying open data that should be made available in machine readable formats like JSON and XML as well as available in APIs.
This approach works in both the public and private sector. If you feel it is valuable enough to have on your already, it is a perfect candidate to make available as open data or API.  Once you get the hang of identifying, processing and publishing this open data and APIs, you can start the much more difficult process of looking for harder to identify data sets and resources.
[<a href="/2013/09/25/start-with-your-public-website-when-you-begin-your-inventory-for-data-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-government.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/25/github-can-be-the-post-and-put-layer-for-federal-government-apis/">Github Can Be the POST and PUT Layer For Federal Government APIs</a></h3>
			<p><em>25 Sep 2013</em></p>
			<p>I'm playing with different approaches to rapidly design, develop, deploy and manage APIs using Github. While about 90% of what I'm building runs on Github, there is still about 10% that runs on Amazon EC2. There are just some aspects of a proper API interface that I can't do on Github. My recent prototypes use swagger and allow for much of the API interactions to occur via Github. I a working to carve off any elements I can from the architecture, including using JSON files stored at Github as the database backend for the API. If you look at my recent Dev Hub prototype, you can browse the API interface, thanks to Swagger, and when you make API calls to the endpoints via Amazon EC2, the REST interface is just acting as a search, filter and REST facade for the JSON files that are actually stored on Github--eliminating the need for a database backend. This approach allows me to develop light-weight REST facades for JSON data stores as well as for other APIs. I'm playing around with different ways that I can use this thought process to push government APIs to the next level, and building on my earlier thoughts today on the need for writable APIs in federal government. I'm evolving earlier designs I have in my archives of BeyondGET or OtherVerbs, an Augmented API Platform, where I'm looking to provide an augmented layer that makes existing APIs writable. Marrying these legacy thoughts with my current approaches using Github plus EC2 APIs, I strongly feel that Github has huge potential for being a POST, PUT and DELETE layer for federal government APIs. Using a REST facade on EC2 I can easily proxy existing government APIs, then using a swagger definition I could seamlessly weave in a POST, PUT and DELETE layer that would write to JSON files hosted at Github using the Github API. I'm in early thoughts on this work, and will start...[<a href="/2013/09/25/github-can-be-the-post-and-put-layer-for-federal-government-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-pen-hand.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/25/a-huge-need-for-writeable-apis-in-government/">A Huge Need for Writeable APIs in Government</a></h3>
			<p><em>25 Sep 2013</em></p>
			<p>I asked a question on Twitter last night: Any examples of government APIs that allow for write (POST, PUT, PATCH) capabilities? I'm looking for existing agencies who have implemented already. While I was asking for examples of APIs, by motivations were specifically about finding an example of terms of use for a government writable API. After spending some time looking and listening to peoples responses, there isn't much in the wild when it comes to writable government APIs, however it is clear that there is a huge demand for writable APIs, and a lot of opinion that this could be the thing that changes how government operates for the better. The best examples that my followers pointed me to was with the Open 311 iniative: Washington, DC Chicago, IL Montgomery County MD There was another DoD API that allows users to POST information, and then only allows them to only update their own information, but wasn't exaclty the open write I was looking for. Clause Logic Service Clay Johnson (@cjoh), the author of The Information Diet, has a great post on how writable APIs can help fix the government procurement process. Demonstrating one solid example of how government could be more efficient through the sensible use of APIs. The biggest case study I can find of writable APIs in federal government was the IRS Modernized e-File (MeF) system, but the goal of this research is to find an existing example of terms of use for writable APIs in the federal government, and after looking through all the e-File documentation I couldn't find any terms of use that covered developers submitting tax forms to the IRS via MeF web services. I will spend more time looking, I'm sure they are in there somewhere. If you know of any examples of of the federal government using APIs and allowing the public to to submit data, content or forms through APIs, please let me know. It is something...[<a href="/2013/09/25/a-huge-need-for-writeable-apis-in-government/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/csv-converter-github.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/24/excel-and-csv-conversion-to-json-and-xml-in-javascript-that-runs-100-on-github/">Excel and CSV Conversion to JSON and XML in JavaScript That Runs 100% on Github</a></h3>
			<p><em>24 Sep 2013</em></p>
			<p>When it comes to building applications within the federal government, there are numerous road-blocks to innovation. I'm currently assisting with the inventorying of open data assets at the Department of Veterans Affairs, as well as across numerous other federal agencies. The two biggest bottlenecks of this process are: File Conversion - Converting Excel and CSV data assets into JSON and XML File Storage - Where do you put data assets once you have available in CSV, JSON and XML When I hit these road-blocks, it is my nature to find quick and dirty technical solutions (hacks) to get around these obstacles, and do it in a way that can be taught to others, allowing them to overcome these challenges in their own worlds. The solution I've employed involved discovering and forking of a kick ass data conversion tool called Mr. Data Converter, and quickly the tool retrofitted with some of my own enhancements: HTML5 File API - Allows me to acces file content without server side technology. oAuth.io - Dead simple, client side oAuth for Github and other platforms. Github.js - A JavaScript API wrapper for Github, enabling client-side interaction. I quickly stripped down Mr. Data Converter to have only the features I desired, added a file upload capabilities that used the File API to access CSV files without a server side layer, then after authenticating with oAuth.io via Github, I used Github.js to post the original CSV and converted JSON or XML files directly to the same Github repo that the file conversion application runs in. This approach allows me to run the Excel / CSV conversion app 100% on Github using Github Pages--an blueprint that allows anyone to fork and run within their own Github accounts. I'll spend more time hardening the code, and documenting it, to make it easier to use, and empower anyone to use in their own open data inventorying initiatives. You can see it in action live on Github...[<a href="/2013/09/24/excel-and-csv-conversion-to-json-and-xml-in-javascript-that-runs-100-on-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/fuck-yeah-markdown-api.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/22/html-to-markdown-api/">HTML to Markdown API</a></h3>
			<p><em>22 Sep 2013</em></p>
			<p>
I'm slowly getting my blog world in order after the move from my own proprietary blogging platform to using Github + Jekyll hosted using Github Pages.
I've been using HTML pages for blog posts at API Evangelist, Kin Lane and other blogs, with 3 years of blog posts at API Evangelist and about 6 years at Kin Lane. There is a lot of legacy content to move from my EC2 driven blogs to Github.
Every time I would try and publish the posts as is, Github would reject my commit when it hit posts that didn't have compliant HTML, making it near impossible to publish everything.  I was trying to clean up as much of it as I could, but it wasn't good enough.  I needed a way to convert to markdown and clean house.
Thankfully, Ben Balter(@BenBalter) from Github recommend a very cool API called Fuck Yeah Markdown, which takes my legacy HTML pages and converts it to much cleaner markdown.
When I first started using Jekyll I wasn't really sold on markdown, in my mind I didn't mind hand rolling my HTML tags--I have been doing it for years. After Ben suggested I use markdown in my newly minted Github Jekyll projects I started to see the benefits.  It is way easier to manage content that is being published as a blog, page or otherwise when it is markdown.
I am just finishing up converting all of API Evangelist and Kin Lane to use markdown, and will be using the Fuck Yeah Markdown API to convert blog posts from HTML generated in my blog editor to markdown before publishing to Github.
[<a href="/2013/09/22/html-to-markdown-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-irs-logo.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/15/irs-modernized-efile-mef-a-blueprint-for-public-amp-private-sector-partnerships-in-a-21st-century-digital-economy-draft/">IRS Modernized e-File (MeF): A Blueprint For Public &amp; Private Sector Partnerships In A 21st Century Digital Economy (DRAFT)</a></h3>
			<p><em>15 Sep 2013</em></p>
			<p>Download as PDF The Internal Revenue Service is the revenue arm of the United States federal government, responsible for collecting taxes, the interpretation and enforcement of the Internal Revenue code. The first income tax was assessed in 1862 to raise funds for the American Civil War, and over the years the agency has grown and evolved into a massive federal entity that collects over $2.4 trillion each year from approximately 234 million tax returns. While the the IRS has faced many challenges in its 150 years of operations, the last 40 years have demanded some of the agency's biggest transformations at the hands of technology, more than any time since its creation. In the 1970s, the IRS began wrestling with the challenge of modernizing itself using the latest computer technology. This eventually led to a pilot program in 1986 of an new Electronic Filing System (EFS), which aimed in part to gauge the acceptance of such a concept by tax preparers and taxpayers. By the 1980s, tax collection had become very complex, time-consuming, costly, and riddled with errors, due to what had become a dual process of managing paper forms while also converting these into a digital form so that they could be processed by machines. The IRS despereatly needed to establish a solid approach that would enable the electronic submission of tax forms. It was a rocky start for the EFS, and Eileen McCrady, systems development branch and later marketing branch chief, remembers, &ldquo;Tax preparers were not buying any of it--most people figured it was a plot to capture additional information for audits." But by 1990, IRS e-file operated nationwide, and 4.2 million returns were filed electronically. This proved that EFS offered a legitimate approach to evolving beyond a tax collection process dominated by paper forms and manual filings. Even Federal Agencies Can't Do It Alone Even with the success of early e-file technology, the program did not get the momentum it needed without...[<a href="/2013/09/15/irs-modernized-efile-mef-a-blueprint-for-public-amp-private-sector-partnerships-in-a-21st-century-digital-economy-draft/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/tag-cloud-internet-of-things.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/14/api-monetization-in-the-internet-of-things-nordic-apis/">API Monetization In The Internet of Things @ Nordic APIs</a></h3>
			<p><em>14 Sep 2013</em></p>
			<p>I have a panel this week at Nordic APIs called Business Models in an Internet of Things, with Ellen Sundh (@ellensundh) of Coda Collective, David Henricson Briggs of Playback Energy, Bradford Stephens of Ping Identity and Ronnie Mitra(@mitraman/a&gt;) of Layer 7 Technologies. My current abstract for the panel is: As we just begin getting a hold on monetization strategies and business models for APIs delivering data and resources for mobile development. How will we begin to understand how to apply what we have learned for the Internet of Things across our homes, vehicles, sensors and other Internet enabled objects that are being integrating with our lives. In preparation for the event I am working through my thoughts around potential monetization strategies and business models that will emerge in this fascinating adn scary new world where everything can be connected to the Internet---creating an Internet of Things (IoT). Where Is The Value In The IoT? When it comes to monetizing APIs of any type, there first has to be value. When it comes IoT where is the value for end-users? Is it the device themselves, is it the ecosystem of applications built around a device or will it be about the insight derived from the data exhaust generated from these Internet connected devices? Evolving From What We Know After almost 10 years of operating web APIs, we are getting a handle on some of the best approaches to monetization and building business models in this new API economy. How much of this existing knowledge will transfer directly to the IoT? Freemium, tiered plans, paid API access and advertising--which of these existing models will work, and which won't. Another existing model to borrow from when it comes to IoT is the telco space. The world of cellphone and smart phones are the seeds of IoT and one of the biggest drivers of the API economy. How will existing telco business models be applied to the world of...[<a href="/2013/09/14/api-monetization-in-the-internet-of-things-nordic-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/ramps-to-play-components-600.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/13/using-excel-for-crowdsourced-data-gathering-and-reporting/">Using Excel For Crowdsourced Data Gathering And Reporting</a></h3>
			<p><em>13 Sep 2013</em></p>
			<p>
I was impressed with some of the data journalism behind the recent NPR story, Playgrounds For Everyone, a community-edited guide to accessible playgrounds.
The story is definitely an important one, but it is the data behind it I think is significant to highlight. You can download the data of the 1700 playgrounds in 20 different cities in a CSV and JSON format. Something I think is ripe for an API, by the way.
Another interesting aspect is they are asking for submissions from the public, and they even provide a template Google spreadsheet, providing a framework for how the public should gather and organize data into a standard way, that NPR can import.
While I think this project could go further, I think it is an excellent example of using data journalism in public reporting.  The only suggestions I have is making the project a Github repository so the story, JSON and CSV can be versioned, forked and downloaded much more easily.
I think Google Spreadsheets and Excel templates are a perfectly acceptable way to gather data from the public and 3rd party sources. It allows you to solicit data from others in a format that they understand, while still making sure it is structured enough to easily merge with a master database.
Additionaly, it would be pretty easy to add the ability for users to email their spreadsheets to a central email address, and programmatically convert to JSON, and CSV, then commit to the Github repository that contains the master JSON and CSV files.  This way the repository administrator could accept or deny submissions as a pull request.
I'm enjoying seeing these scrappy spreadsheet, CSV and JSON solutions to data storytelling. Even if they aren't perfect I like seeing people play with different approaches, in hopes of finding an approach that works for them.
Lots to learn from. Nice work NPR.
[<a href="/2013/09/13/using-excel-for-crowdsourced-data-gathering-and-reporting/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-danger.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/13/the-perils-of-api-transport-over-the-public-internet/">The Perils Of API Transport Over The Public Internet</a></h3>
			<p><em>13 Sep 2013</em></p>
			<p>George Reese has a very interesting post from last week over at O'Reilly. It is about an earlier post he did on the unpublished Tesla REST API.&nbsp;I'll let you read the post, "The Myth of the Private API"--I highly recommend it. Reese talks about the mistakes made by Tesla, Phillips and other Internet of Things companies, when they take advantage of the power of web APIs, intending them to be private, but do not put any thought into what happens when you deploy APIs using the public without securing your API endpoints from unintended use. I find a particular statement he made, fascinating: I sincerely believe that ultimately there is no such thing as a private API for consumption over the public Internet. After reading his post, I have to agree. I think many technology companies are just considering the Internet to be some sort of constant, magic transport layer for anything we want to use it for. I think this can be true to a point, but as the Intenret matures, we have slow down a bit and consider deeply the impact of our actions, and the way we use Internet enabled technology. I wrote about a piece last week, which was about the first FTC case against an Internet of Things manufactuer, camera maker TrendNet--where much like Tesla, they took no considerations for the fact they were using the open Internet to drive their technology, and more importantly no thought regarding the privacy of their consumers. The world of APIs fascinates me. It reminds me of the bug zappers, where we are attracted by the openness and power of web APIs, but as you get closer and closer to the API light, you can easily get burned or zapped by the very thing that drew you in. I strongly believe in the power of web APIs, but I think there will be a lot of unintended consequences from opening up data, resources and...[<a href="/2013/09/13/the-perils-of-api-transport-over-the-public-internet/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-question-mark.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/13/a-masking-scrubbing-anonymizing-api/">A Masking, Scrubbing, Anonymizing API</a></h3>
			<p><em>13 Sep 2013</em></p>
			<p>In government there is a fear of exposing public data via APIs--rightfully so. This is not just a government concern, it exists in all industries within each an every business and organization. We all possess private data, and when opening up API driven resources, we need to make sure none of this is exposed in un-desired ways. I find it hard to believe, that after almost 10 years of public APIs, there isn't a reasonable solution to masking, scrubbing and anonymizing data that is made available via APIs. I wrote about research into finding a solution at UC Berkeley a while back, but to date I have not seen any real solutions to this problem yet. I was talking with another Presidential Innovation Fellow (PIF) the other day about possible solutions for making sure Personally Identifiable Information (PII) doesn't get exposed via government APIs. Afterwards, I got to thinking about possible API options, and I don't think it would be that difficult to get started with a basic solution. My thoughts are, that you could provide a simple API proxy, that would terminate requests from any Swagger defined APIs and easily iterate through each value and apply a series of regular expressions against it to look for common PII or other data that shouldn't be exposed. The proxy could automatically replace with template values like John or Jane Doe for names, 1234 Street for addresses, etc. API providers could set a list of areas they are concerned about exposing with the API proxy configuration, and it would enforce all filtering required. The proxy could also look for other common patterns, and make recommendations of other areas that could be masked, scrubbed or anonymized that the API provider didn't consider. Technically it sounds like a pretty simple solution, that could get smarter and faster over time at identifying sensitive information, to better serve API providers. This type of proxy could be default in healthcare, education and...[<a href="/2013/09/13/a-masking-scrubbing-anonymizing-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/spreadsheet-basic.gif" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/09/the-spreadsheet-will-play-a-central-role-in-the-api-space/">The Spreadsheet Will Play A Central Role In The API Space</a></h3>
			<p><em>09 Sep 2013</em></p>
			<p>
The more I immerse myself in government, I'm reminded of the central role that the spreadsheet plays in our business and government operations--primarily Microsoft Excel, but also in some circles, the Google Docs Spreadsheet.
While it is government that is bringing the spreadsheet front and center for me again, I'm reminded of days while working on SAP events and working on budgets, sessions and registrations lists that were past around in complex series of spreadsheets. After this I go further back in time, to the early 2000s when I worked in the non-profit sector, where database management was completely done in a myriad of group and individual spreadsheets.
Beyond the spreadsheet being the central villain in global operations, I'm seeing it emerge as a character across the API landscape with Octobpart Electronics providing bill of materials management in Microsoft Excel that is driven from their API, CrunchBase adding 13,689 Companies and 1,462 venture rounds as an Excel download, and Twilio allowing users to make calls and send SMSs from spreadsheets.
As much as us API geeks would love for people to deploy clean, sensibly designed APIs, that meet our visions of the future of APIs--the reality is that much of the worlds data is managed via the spreadsheet.  If we are truly going to deliver on the API economy, we have to consider the spreadsheet.
This spreadsheet bridge is not just about allowing users to publish data via APIs from spreadsheets, but also enabling every day users to cosume valuable API driven data and resources from their native spreadsheet. It has to be a two way street.
Whether we like it or not, the spreadsheet will continue to play a central role in the API space, and represents the future of acessing valuable corporate, non-profit and government resources for everyone.
[<a href="/2013/09/09/the-spreadsheet-will-play-a-central-role-in-the-api-space/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-gears.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/07/loosely-coupled-services/">Loosely Coupled Services</a></h3>
			<p><em>07 Sep 2013</em></p>
			<p>Building off a similar topic this week, I was asked to dumb down or explain what I meant by "Loosely Coupled Services", alongside a "Library of Modular Services". In this case, loosely couple means independent technical and data services, in the same way you would access services that people are familar with in the mainstream business world. Think of the services you would access and put to use around your home. Common household services like electrician, plumber, tile layer and sheet rocker. These each represent independent services you would access to tackle a home improvement project. While there is some overlap in these services, generally each service technician specializes in one area, doing one thing and doing it well. The slight overlap between an electrician and plumber would be considered the "loosely coupled" part, where there are dependencies between each specialized service, ie. the plumber needs an electrician to wire the dishwasher after he /she plumbs it. The concept of loosely coupled services works well when it comes to APIs. The goal is to define APIs to do one thing and do it well, establishing a library of modular services that can be used across a company and organization, its partners as well as provide public access to the most commonly requested resources. A simple example might be product catalog. You can deliver a web service that allows for accessing, searching and pulling details of products a company offers. All the web service knows how to do is find, list and provide info on products, nothing else. This product catalog will have loosely coupled dependencies with other inventory, shopping cart and coupon services, but in the end its very specialized. Any organization or company will have a wide variety of potential services that can be defined. It is best to define these in as granular of a way as possible, keeping them entirely independent of other services, except for slightly coupling that allow them to...[<a href="/2013/09/07/loosely-coupled-services/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/universal-library-sign.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/07/library-of-modular-services/">Library of Modular Services</a></h3>
			<p><em>07 Sep 2013</em></p>
			<p>I'm always looking for simpler and more concise language to describe API, while writing stories and white papers for my audience. I recently used the phrase "library of modular web services", in a presentation outline at the Department of Veterans Affairs (VA). This white paper was intended for a non-technical, state government audience, and my collaborator on the presentation asked if I could dumb this phrase down a little for the audience, providing a simpler explanation. In this case, I think certain phrases get co-opted by the developer and IT crowd, borrowing from the physical world, resulting in them having a perceived technical meaning, but when in reality they are still very rooted in their past, and can be easily explained by taking users back to their previous meanings. One perfect example of this is the phrase, a "Library of Modular Services". Think of each book, in a physical library as an analogy of a single API service. Much like when building an application on top of API services, when you are developing a research paper in a library, you need a multitude of resources to bring your paper together. When you visit the library to conduct your research you can browse the library directory, find a wide variety of books you will need. Early on you will be testing out many of these books, then setting aside the books that do not deliver the value and information you are looking for. When you find the specific resources you need, this is when the modular approach comes into play. For example, if you need a book on a specific artist from history, you aren't required to check-out the entire art history section, or even check-out all impressionists artists, you can go for a very specific artist like Monet or pick and choose from the impressionist resources you need. Using this library analogy for API services works very well. As a company or organization, instead of...[<a href="/2013/09/07/library-of-modular-services/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-github.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/05/i-am-always-amazed-at-how-little-people-understand-about-github/">I Am Always Amazed At How Little People Understand About Github</a></h3>
			<p><em>05 Sep 2013</em></p>
			<p>I work with some seriously smart people on a day to day basis, virtually across the web, and in person on some of the projects I'm working on in federal government. Much like APIs, Github is fast becoming a ubiquitous technology that people are using to manage their community, code, documents and much more. Several times each week I encounter situations where Github is referenced as a potential platform for managing a new project, or cited wen talking about how to solicit feedback, engagement and participation across existing projects. I'm always amazed that in about 75% of these Github conversations, someone chimes in about how Github wouldn't be appropriate because of the barrier to entry for many users. I find this barrier to entry perspective very interesting. Many of the uses of Github require no knowledge of, or the need to touch code. One meeting in particular, was about providing feedback on the next steps of data.gov, when a participant referenced that for users who weren't technical, but wanted to provide feedback, they should use a separate forum to submit feedback. To understand this I pulled up the data.gov site, clicked on link to Github repository behind the project, clicked on issues tab, then submit new issue. A text box came up much like it would on Facebook or the proposed forum. In reality there are no technical hurdles for these users, except their own perception that Github is a social coding site for programming. This type of perception is a holdover from the last 30 years of IT and developers making sure they were keepers of the knowledge and instilling fear in users, that treading in this realm is only for the brave, often male world of developers, database and IT folk. In the world of software as a service (SaaS) and APIs, this is no longer a reality, but the perception is still there. As I work hard to evangelize API tools and...[<a href="/2013/09/05/i-am-always-amazed-at-how-little-people-understand-about-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/trendnet-camera.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/05/building-internet-of-things-products-you-better-secure-it-says-the-ftc/">Building Internet of Things Products? You Better Secure It, Says the FTC</a></h3>
			<p><em>05 Sep 2013</em></p>
			<p>
The Federal Trade Commission(FTC) just settled a case with web-enabled camera maker TRENDnet, signaling the government agency's first action against an Internet of Thing's company.
The FTC's complaint alleges that TRENDnet was labeling their cameras as secure, when in reality the camera had faulty software that left them open to online viewing and audio access to anyone who had the Internet address of the camera.
The FTC / TRENDnet case is the first, in what I predict is a future filled with security and privacy violations by Internet of Things products and companies.  As we blindly race forward with the exciting Internet of Things, many companies will disregard the security of their hardware and software, just like TRENDnet did.
Hopefully as an industry we can help make sure this doesn't get out of hand, and don't require the federal government to police the situation. As technology providers, we need to make sure the software, hardware and APIs that drive the Internet of Things is properly secured, protecting our customers and the industry.
[<a href="/2013/09/05/building-internet-of-things-products-you-better-secure-it-says-the-ftc/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-magnifying-glass.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/05/api-of-api-keyword-searches/">API of API Keyword Searches</a></h3>
			<p><em>05 Sep 2013</em></p>
			<p>
I was working on series of API endpoints this week, each of them had a basic search parameter, allowing you pass a keyword to filter your API request.  Pretty standard stuff.
After deploying the APIs I wanted to make sure I tracked what people were searching for, so I could use in reporting and other tools. Then I got to thinking, that it would make sense to go ahead and launch an API of API search queries, allowing other users to be able to discover and benefit from insights derived from other API users searches.
Just a random thought as I'm playing around more with APIs in government. I definitely like the stimulation I get from designing, deploying and evolving APIs at my new gig, in addition to my existing monitoring of the API space.
[<a href="/2013/09/05/api-of-api-keyword-searches/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/nordic-apis-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/04/am-i-going-to-see-you-nordicapis-in-sweden-september-18th-amp-19th/">Am I Going to See You @NordicAPIs in Sweden, September 18th &amp; 19th?</a></h3>
			<p><em>04 Sep 2013</em></p>
			<p>There is a growing number of API conferences going on this year in the US, but the hunger for API knowledge isn't just something going on in this country, we are seeing a demand for API information and conversation growing internationally. One place that is exploding is in Europe and specifically in the Scandinavian region, and there is one must-go-to event that is driving the API conversation--the Nordic API Conference, September 18th and 19th in Stockholm, Sweden. I will be heading out for the conference and giving a talk I'm calling "The Politics of APIs is the Future", my current abstract is: We have found the right balance of technology for APIs, using simple lightweight protocols, built on HTTP. The business of APIs around good documentation, marketing and support to developers and monetization strategies are being proven. The next challenge we face in the API space will be around terms of use, privacy, deprecation and security. In addition to my talk, I will be moderating a panel on the Internet of Things, but specifically on business models, with Ronnie Mitra, Bradford Stephens, David Henricson Briggs and Ellen Sundh: As we just begin getting a hold on monetization strategies and business models for APIs delivering data and resources for mobile development. How will we begin to understand how to apply what we have learned for the Internet of Things across our homes, vehicles, sensors and other Internet enabled objects that are being integrating with our lives. The Nordic APIs is exactly two weeks away, so its not too late to buy a plane ticket and engage in API conversations with global thought leaders fromt the space, in beautiful Stockholm. I've never been to Sweden and looking forward to going. I get really pumped by the passion and energy for APIs, interoperability and opening up government businesses outside the United States. If you can make time, and afford the trip, I look forward to seeing you in...[<a href="/2013/09/04/am-i-going-to-see-you-nordicapis-in-sweden-september-18th-amp-19th/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-irs-logo.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/02/private-sector-sharing-the-load-through-government-apis/">Private Sector Sharing The Load Through Government APis</a></h3>
			<p><em>02 Sep 2013</em></p>
			<p>When it comes to APIs, people respond to stories about real world examples, even more than solid technological implementations. If you can demonstrate how APIs are actually providing a solution, you can reach more people than just talking about the technological nuts and bolts. With this in mind I'm working through telling stories around how the IRS leveraged web services to incentivize private sector to develop applications that would provide tax solutions for the every day tax payer. In my short year and half working for government I've heard the example of "TurboTax" used to describe an example of how the federal government can leverage technology to deliver partner driven solutions, and better serve the public. Obviously this is an example that resonates with leaders in government, but one that I think needs a lot more work to actually flush out the model more deeply, while keeping it as something that anyone can understand and is able to repeat in their own circles. The story of the IRS e-File program for developers is a important blueprint for how a forms driven federal government system, can deploy APIs and share the burden of delivering important civic services with the private sector. Describing this as a "Turbo Tax Solution", is an extremely simplified analogy, whereTurbo Tax is just one application within an ecosystem of private sector, trusted IRS partners. As with every other federal agency, the IRS faced the problem of modernizing its systems to keep up with current technologies, while also continuing to improve the US tax process, so that it would better serve americans, and make the massive federal agency more efficient, which resulted in the design of an e-File system, where tax professionals could submit electronic tax form filings via IRS systems. While a web-based approach to modernizing the IRS tax process is a large part of the evolution of the US tax process, it wasn't enough. The IRS isn't in the business of...[<a href="/2013/09/02/private-sector-sharing-the-load-through-government-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-portal.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/09/02/baseline-for-federal-government-open-data-and-api-portals/">Baseline for Federal Government Open Data and API Portals</a></h3>
			<p><em>02 Sep 2013</em></p>
			<p>I have a whole list of projects around open data and APIs at the Department of Veterans Affairs (VA). Additionally I have numerous other open data and API projects I'd like to tackle across other federal agencies. As I do with other areas of my work, I needed a standardized way to stabilize the datasets and APis I will need for my projects, in the same way any open data and API provider should do for their consumers. To help support my work, and hopefully the work of others I wanted to create a baseline portal that I could use at the VA, for showing what is possible when hanging open datasets and APIs, in a full featured portal. The success of any open data and / or API portal starts with the technical building blocks, like data and APIs, but have a set of business and political building blocks that are essential to their adoption and growth. I've spent the last three years studying the business and politics of APIs. During these three years I've looked at almost 10,000 API developer portals, and I've established a base set of what I consider the building blocks of successful API portals, with a handful in which I consider essential to success. I've always wanted a simple API portal template that would reflect this research, and my new Developer @ VA&nbsp;portal&nbsp;is the first step towards achieving this. Developer @ VA is an early stage prototype, whcih I've built to satisfy this need of mine, specifically for my VA projects. I will be polishing this portal and replicating it as a single template that can be used for any API and / or open data portal. This portal exists purely as a Github repository and runs on Github Pages, using a Jekyll for managing its pages and blog. Everything else is HTML, CSS, Javascript and JSON, allowing it to be able to run on any server, including other...[<a href="/2013/09/02/baseline-for-federal-government-open-data-and-api-portals/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/y-u-no-guy-why-u-no-pay-attention.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/28/pick-your-head-up-regularly-heads-down-is-good-but-being-aware-cannot-be-ignored/">Pick Your Head Up Regularly, Heads Down is Good, But Being Aware Cannot Be Ignored</a></h3>
			<p><em>28 Aug 2013</em></p>
			<p>One important thing I've learned while running API Evangelist, is the importance of picking your head up from your work on a regular basis, and tuning into the world around me. When you are running your API initiative it can be also be easy to go heads down coding, addressing technical issues, managing support channels and dealing with the general day-to-day, internal activity of running a company. Don't get me wrong, I'm big on going radio silent, closing the Gmail tab, shutting down TweetDeck, LinkedIn, Facebook, Google+, Skype and my other communication channels. Since I stopped using Google Reader to monitor feeds, and setup my own internal curation, I can easily tune out my API industry monitoring and curation for days sometimes. This is all intended to get shit done and reduce distractions. Even with my regular tuning out of the world and focusing on work, I make sure and pick my head up, turn on communication channels, read and curate blog posts and generally take a good look at the world around me. I've worked too hard, building up momentum with my content creation, search engine optimization and social media presence to let it all slide. I can coast for days, or even weeks, but if I drop the ball for too long, I will not only stop any forward motion and growth--I risk losing traction and quickly becoming irrelevant. My number one mission with the API Evangelist Network is to educate myself, then secondarily educate the masses about the API space. If I don't pick my head up regularly, tune into the API space and understand the latest trends, technologies and players--I'm doing myself a disservice, as well as the greater interest of the API industry, and whoever I work for. When running your company, make sure you find time to go heads down, focusing on your work, but make sure you pick your head up regularly and tune into your community, your...[<a href="/2013/08/28/pick-your-head-up-regularly-heads-down-is-good-but-being-aware-cannot-be-ignored/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/api-strategy-and-practice-san-francisco-october-23-24-25-half.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/28/kicking-the-api-strategy-amp-practice-conference-into-full-gear/">Kicking The API Strategy &amp; Practice Conference Into Full Gear</a></h3>
			<p><em>28 Aug 2013</em></p>
			<p>
While we officially launched API Strategy &amp; Practice, San Francisco edition back in May, we've been pretty quiet during the summer months. Well, now summer is coming to a close, and we are now less than 60 days away from the API community conversation that is #APIStrat.
To start ramping things up, I'm going to start showcasing the amazing line-up of speakers we have for the event, and giving the kick-ass sponsors who have stepped up to support the event, the "love" they deserve.
Early bird pricing has ended, but you have less than a month to get in on the mid range pricing. Don't miss out like so many did in February when #APIStrat New York City sold out--buy your tickets now.
Even with large number of speakers we have lined up, we still have some slots left open, and will be making some announcements of other big names to the lineup-so stay tuned.
I will be exploring the wide range of topics and tracks we have planned here on API Evangelist as well as the #APIStrat blog, so subscribe to our RSS feeds and tune in on Twitter for more details.
[<a href="/2013/08/28/kicking-the-api-strategy-amp-practice-conference-into-full-gear/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-greed.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/27/the-api-space-often-seems-to-more-about-money-intellectual-property-and-competition-than-interoperability-sometimes/">The API Space Often Seems To More About Money, Intellectual Property and Competition, Than Interoperability Sometimes</a></h3>
			<p><em>27 Aug 2013</em></p>
			<p>I used to think that the API space is resistant to defining standards around REST, data formats, webhooks, hypermedia, api definitions and other key areas of the space, because after the top down, strict structure of SOA, the community just wanted to let the space organically define the best approaches. The API world just seemed like a wild west of strong minded individuals, who had a sort of "markets will work it out" approach, and the best approach will win in the end. You know, kind of like the Amazon cloud API battle? By 2013, the only thing we've come to agreement on is around oAuth, and by many accounts that was a failure. I used to think that many of the bitter battles by the RESTafarians about API design, RESTfulness and Hypermedia were because these were very smart, stubborn folks trying to craft the best approach possible. After many years of advocating for open source tooling, open events in the space and sharing of common API design patterns, I don't think there is much goodwill for any of it, because companies are more interested in their intellectual property(IP), seeming competitive and ultimately making the most money and pleasing their VCs. Now don't get me wrong, there are some very fine companies, do good in the space. I'm not saying the entire sector has lots its way. However the majority of the vibe from major players is, we don't want to share design patterns, open tooling, work together to support industry events and gatherings, we will just do it all ourselves, keeping the IP and value for themselves. I understand I'm less interested in the whole business and VC aspect of this than many of you are, but I think APIs got their start in openness, collaboration and sharing in the spirit of transparency and interoperability. It seems like like after 10 years and finally getting some traction in the space, we are starting to...[<a href="/2013/08/27/the-api-space-often-seems-to-more-about-money-intellectual-property-and-competition-than-interoperability-sometimes/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-telescope.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/27/sitemap-for-apis/">Sitemap for APIs</a></h3>
			<p><em>27 Aug 2013</em></p>
			<p>
When it comes to API discovery, as an industry we haven't been able to find a satisfactory technological solution yet. While I often feel the right approach hasn't emerged yet, I think we are just overlooking "good enough" solutions, because we are waiting for the holy grail of API discovery.
I can't imagine that indexing, search and discovery of the myriad of web APIs out there is that much harder than indexing, search and discovery of the billions of HTML documents available online.  Sure, when you are talking about programmatic interfaces, you need a little more precision, but I think us technologists are caught up our own beliefs that APIs should be perfect.
It would make sense that we adopt some evolution of the common sitemaps format, retrofit it to be JSON, accommodate open data catalogs, and allow for various API definitions in Swagger, I/O Docs or even API blueprint.  Sure these formats won't have the precision of an evolution of the precious WSDL, or some actually agreed upon standard, but it will get us over the hump we are in.
ProgrammableWeb is 8 years old now, and in 2013, we still don't have any next step, let lone an actual usable solution for API discovery? I just have a hard time believing this is a technological problem, that it is more the stubbornness of the leaders in the space to just take any meaningful step towards API discovery, and lead.
There isn't going to be any money in API discovery, so this isn't something a single startup can emerge to solve. It is something we will all have to discuss and play with until we can find something will get us to the next step, together.
[<a href="/2013/08/27/sitemap-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/OAuth2.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/27/oauth-101/">OAuth 101</a></h3>
			<p><em>27 Aug 2013</em></p>
			<p>With APIs beginning to enter the mainstream consciousness, it is time to spend more time educating the masses about OAuth. We've had plenty of conversations between two of the OAuth legs, provider and developer, but we now need to bring the third leg into the conversation--the user. First, what is OAuth? - An open protocol to allow secure authorization in a simple and standard method from web, mobile and desktop applications. Whether you like it or not, OAuth has become the industry standard for accessing resources, being served up via APIs, that are being consumed through desktop, web and the fast growing mobile space. OAuth Platforms &amp; Data Providers If you are an online platform, OAuth is something you need to understand. At a minimum, if you require users to establish an account, you need to consider allowing users to create their accounts and login in the future using other popular OAuth providers like Facebook, Twitter and Google. Next if you want to provide access to your platform user's data via an API, you need to take a deeper dive into OAuth, and consider establishing yourself as an OAuth provider. OAuth for Desktop, Web and Mobile Developers In 2013, if you are a developer, you are probably using APIs. OAuth has been very intimidating for developers for quite some time, but with the increased availability of quality OAuth clients, better implementations and educational materials from API providers, and standardized approaches by startups like OAuth.io--OAuth is something you shouldn't fear anymore. You need OAuth as a default tool in your developer toolbox. Everyday Online User Like the term API, OAuth is something that should be added to the vocabulary of every tech savvy user. You should understand that OAuth exists, and that it gives you the ability to create accounts and login to your favorite platforms without filling out endless new forms and sharing your passwords unnecessarily. The platforms you use daily, like Facebook, Twitter, LinkedIn...[<a href="/2013/08/27/oauth-101/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-qa.jpeg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/25/api-testing-and-monitoring-finding-a-home-in-your-companies-existing-qa-process/">API Testing and Monitoring Finding A Home In Your Companies Existing QA Process</a></h3>
			<p><em>25 Aug 2013</em></p>
			<p>I've been doing API Evangelist for three years now, a world where selling APIs to existing companies outside of Silicon Valley, and often venture capital firms is a serious challenge. While APis have been around for a while in many different forms, this new, more open and collaborative approach to APis seems very foreign, new and scary for some companies and investors--resulting in them often very resistant to it. As part of my storytelling process, I'm always looking for ways to dovetail API tools and services into existing business needs and operations, making them much more palatable to companies across many business sectors. Once part of the API space I'm just getting a handle on is the area&nbsp;API integration, which includes testing, monitoring, debugging, scheduling, authentication and other key challenges developers face when building applications that depend on APIs. I was having a great conversation with Roger Guess of TheRightAPI the other day, which I try to do regularly. We are always brainstorming ideas on where the space is going and the best way to tell stories around API integration, that will resonate with existing companies. Roger was talking about the success they are finding dovetailing their testing, monitoring and other web API integration services with a company's existing QA process--something that I can see will resonate with many companies. Hopefully your company already has a full developed QA cycle for your development team(s), including, but not limited to, automated, unit and regression testing--something where API tests, monitoring, scheduling and other emerging API integration building blocks will fit in nicely. This new breed of APi integration tools don't have to be some entirely new approach to development, chances are you are already using APIs in your development and API testing and monitoring can just be added to your existing QA toolbox. I will spend more time looking for stories that help relate some of these new approaches to your existing QA processes, hopefully finding news...[<a href="/2013/08/25/api-testing-and-monitoring-finding-a-home-in-your-companies-existing-qa-process/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-start.jpeg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/24/with-apis-in-your-company-start-small-and-read-api-evangelist/">With APIs In Your Company, Start Small And Read API Evangelist</a></h3>
			<p><em>24 Aug 2013</em></p>
			<p>I thoroughly enjoy the assortment of emails, LinkedIn messages and phone calls I get from people in the SMB and enterprise, letting me know the role my blog plays in them starting, cultivating and evolving their own API initiatives. I received once such call this week, from an unnamed individual, at an unnamed company, letting me know the role API Evangelist played in providing the information they needed to find success. Like many other companies who reach out to me, their efforts aren't to the point where they feel comfortable telling stories publicly, so I'm happy to keep anonymous, until they are ready. The stories that come out of these companies are all very similar. The API initiatives were started by single person, or small group of passionate individuals who start small, find safe and sensible wins, while keeping risk and failures to a minimum. They start with data and resources that are not mission critical, but still offer value to either internal, partner or public developers. These innovators usually start with a handful of trusted partners, keeping the experimentation very controlled in a safe environment, before opening up to a wider base of partners, and then when ready, to a self-service public audience. This approach allows API innovators to find small successes and report these wins to business leaders and stakeholders, before moving forward with other efforts. Taking this approach in small, iterative cycles, providing decision makers with the necessary reporting and education, allows for you to slowly change internal culture. API change does not happen overnight, and it is easy to fail if you try to go big in companies who aren't quite ready. I can't get enough of these stories, I can't wait until these program mature, where I can tell them publicly on the blog. Until then, remember that when you are starting with a brand new API initiative within your company, start small, find success, minimize risk, repeat and tell...[<a href="/2013/08/24/with-apis-in-your-company-start-small-and-read-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/github-csv-table-view.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/22/view-csv-and-tsv-data-files-in-table-views-directly-on-github/">View CSV and TSV Data Files In Table Views Directly On Github</a></h3>
			<p><em>22 Aug 2013</em></p>
			<p>
Github is really doing some cool stuff to help open data folks manage and share their data.
They just launched the ability to render data from .csv (comma-separated) and .tsv (tab-separated) files as an interactive table, including headers and row numbering.  They even let you link to a specific row for sharing specific data from the file.
As I'm working on opening up government data, I'm pushing for agencies to use Github when publishing and sharing CSV, TSV, XML and JSON files. These kind of features really go a long way in helping me achieve my goals.
Make sure and also check out what Github has done around 3D models and geographic data, pretty cool stuff.
[<a href="/2013/08/22/view-csv-and-tsv-data-files-in-table-views-directly-on-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/audrey-kin-paris.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/22/to-the-audrey-watters-haters/">To The Audrey Watters Haters</a></h3>
			<p><em>22 Aug 2013</em></p>
			<p>I've had the joy of watching a lot of you bash on my girlfriend, from the misogynistic douche bags telling her to get back in the kitchen because she questions their technology startup, to the recent @FakeAudreyWatters Twitter account spending a good portion of their day mocking her. Audrey and I are very different personalities when it comes to expressing ourselves in an online environment, but we share one common thing--a passion for helping encourage meaningful dialogue about where we are going with this whole technology roller coaster. Audrey is extremely passionate about education and where we are going with the education of our kids, and how we are applying technology in the classroom. If she is critical of your startup, idea or otherwise, you may not like it, but there might actually be a reason behind it--why don't you engage her in dialogue about it? She spends at least 12 hours a day, 7 days a week, researching, writing and stressing over her work(unpaid). I spend every moment with her and I see how much thought she puts into all of this. She is a very positive person, and extremely intellectually stimulating to be around, which is why I'm with her. Together, as a team we have achieved some amazing things in the last three years. I strongly believe Audrey and I have positively impacted the online world by helping folks think critically about education, and across numerous other industries through analysis of APis. I depend on Audrey to bounce ideas off, ranging from oAuth to API terms of use. API Evangelist would not exist without her. When I was first with Audrey she often frustrated me with her confrontational approach, but over the course of four years, I have found it to be essential to what I do. She battles me on ideas, sometimes to the point where we both stomp off, but in every case, I've been able to apply her critical...[<a href="/2013/08/22/to-the-audrey-watters-haters/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/census.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/22/delivering-value-for-developers-is-first-when-it-comes-to-the-census-bureau-apis/">Delivering Value For Developers Is First When It Comes To The Census Bureau APIs</a></h3>
			<p><em>22 Aug 2013</em></p>
			<p>I wrote a piece about providing a full download vs. API last month, where I used the Census Bureau API as an example. The post got the attention of the folks at the Census, and they invited me out for a discussion yesterday about their API strategy. The Census Bureau API team asked me what I thought of their API developer area, and while I have lots of suggestions of where they could do, I first focused on what they have already delivered. I don't know if you understand the size of the census, but it is a massive undertaking, resulting in data at a scope that we are only seeing matched in the last few years, with the Google's and Twitters of the world--the only difference is the census has been going on since 1790. it gives you an ideas of the serious big data potential behind the Census Bureau API. Faced with data at this scope, I understand that delivering a simple, web API is not easy. I was impressed with the teams efforts before I came to D.C., but now I'm really impressed that such a simple, web API could be delivered out of such a large government entity. Many other agencies are still trying to even learn what an API is, and even if they do attempt at delivering APIs, the result is often the much more technical cousin of web APIs, SOAP web services. I totally respect the simple start to the Census Bureau API. The current API landing gives you brief introduction to the API, shows you how to access the data via the URL endpoints, gives you example responses, error codes and addresses what you need to use the APIs in JavaScript. They also give you quick access to getting API keys, a developer forum, application gallery and of course you still get full access to Census downloads. What a great start. So simple. Giving developers exactly what...[<a href="/2013/08/22/delivering-value-for-developers-is-first-when-it-comes-to-the-census-bureau-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/harvest-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/21/time-tracking-platform-harvest-moves-api-docs-and-app-showcase-to-github/">Time Tracking Platform Harvest Moves API Docs and App Showcase to Github</a></h3>
			<p><em>21 Aug 2013</em></p>
			<p>
Time Tracking API platform Harvest has embraced Github as part of their API ecosystem. I'm always on the hunt for examples of API providers using Github, so I figured I'd showcase Harvest's creative use of the social coding platform.
Starting with their documentation, the Harvest team has moved the API documentation to a Github repository, allowing developers to "watch" the API, get updates when changes are made, asks questions or even contribute to the API docs by submitting a pull request.
Harvest is also using the wiki portion of their Github repo for a developer application gallery they are calling Community Creations and Hacks, where they showcase innovative uses of the Harvest API--currently displaying 20 integrations by Harvest users.
I'm currently tracking on 11 separate uses of Github for API management, and always on the hunt for new ways to use Github to support API ecosystems. Nice move Harvest!
[<a href="/2013/08/21/time-tracking-platform-harvest-moves-api-docs-and-app-showcase-to-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-toolbox.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/21/api-skills-alongside-web-in-developers-toolbox/">API Skills Alongside Web In Developers Toolbox</a></h3>
			<p><em>21 Aug 2013</em></p>
			<p>As I immerse myself in the federal government, I have left my private sector world where web APIs have become commonplace. Sure I still worked hard to get outside of Silicon Valley and reach out further into land of "normals", helping spread the API gospel, but in Washington D.C. I seriously have my work cut out for me. I have a great support system within the Office of Science and Technology Policy (OSTP) at the White House, but as I go deeper within specific agencies I see fewer web APIs, and fewer people who understand them. When I do come across a web service, it makes me hopeful, but still these SOAP driven services may get the job done for programmers, but lack the simplicity needed to get wider adoption. This is only my second week in DC, I'm spending a lot of time going through websites, getting to know the Department of Veterans Affairs and other agencies, and recognize there is a lot of web talent, even though much is probably contracted, within the Federal Government. I can't help but think how CMS platforms like Wordpress, Drupal and others have helped our government publish valuable information and resources via the web, but we still are lacking a complimentary web API movement. The value of websites has been accepted across federal government, what can we do to get web APIs to the same level? Web services are embraced, but they are still a very black box, developer centric tool, something out of reach of the common gov worker. But web APIs aren't much different than the websites being produced, they just trade their HTML output for data formats like XML and JSON--I'm looking to change this. In 2013 and 2014 I want to pose the question in within government, why aren't open data and API skills part of the federal government's web developer toolbox, alongside the skills you already posses. You can increase the productivity...[<a href="/2013/08/21/api-skills-alongside-web-in-developers-toolbox/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/Swagger-Screenshot-1.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/16/what-api-documentation-do-you-suggest/">What API Documentation Do You Suggest?</a></h3>
			<p><em>16 Aug 2013</em></p>
			<p>As I spend time in Washington DC, I get a lot of questions regarding API design, deployment and management. It is quite likely my writing will evolve here at API Evangelist during the next year. You will see me quickly scrub answers that I'm giving to questions that I receive from any number of federal agencies. One question I received today was a classic one: what are some good examples of API docs? But in this particular case it wasn't just for an API, there is also a full download of data available as well. First, regardless of whether or not it is data download (CSV, XML, JSON) or API you need to provide a wrapper area, or portal, that will onboard users and helps them understand what you are delivering, enabling them to go from 0-60 with as little friction as possible. This is what I have studied for the last 3 years. After looking at thousands of APIs areas, you start to see patterns. You will find&nbsp;my research in the form of "building blocks" on a section of my site. Even though most of my research is focused on APIs, these patterns can apply universally to data download or API delivery. But specifically to question of what API documentation I like? I also recommend looking at: Stripe Twilio Parse Full Contact Box Dwolla When looking through these make sure and take notice of how&nbsp;Dwolla separates their intro page, between non-devs and devs. This is a very significant concept for delivering onboarding materials for users where not everyone will be a developer. Beyond those suggestions for API docs, here are&nbsp;Thirty APIs To Look At When Planning Your API, if you are looking for more examples of quality API areas + documentation. Another thing to consider when actually deploying specifically an API, is making your API documentation "interactive". There are several solutions for doing this right now, but the leading solution (in my opinion) is...[<a href="/2013/08/16/what-api-documentation-do-you-suggest/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/api-craft-logo.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/16/api-craft-soa-vs-api/">API Craft - SOA vs API?</a></h3>
			<p><em>16 Aug 2013</em></p>
			<p>
This is from a question I answered on the API Craft Forum tonight titled: SOA vs API?
I happened to look at the slide and also related video but unfortunately still do not have&nbsp;&nbsp;a crystal clear view.Is my understanding correct?

SOA and API are complementary paradigms/technologies. APIs are a facade to SOA to expose them to the outside world?
SOA was aimed to provide an integration mechanism/guidelines for enterprise while APIs are generally aimed to make the backend systems public? Also, APIs can be internal to the organization.

My response:
Your understanding is in the ballpark.&nbsp;&nbsp;APIs were one tool in the SOA toolbox. Except you are seeing APIs as just technology.  APIs have jumped out of the enterprise toolbox, and found success in the richer oxygen environment of the Internet, escaping from the claustrophobic environment of the restrictive enterprise network in which strict governance was imposed, and technologists decided everything.  After going outside the firewall, APIs became about the simplicity of REST + JSON over HTTP, putting these resources closer to actual problem owners--flipping the governance of SOA on its head, making APIs more about partnering, collaboration, transparency and openness, and not just about control. After escaping the governance and the bottleneck of traditional IT, this new breed of APIs allowed for new types of innovation, business models and opportunities amongst not just open developers, but actual business owners. A new world emerged that the governance overlords of SOA could never achieve or even see.  Allowing the&nbsp;humans to win over the machines!&nbsp;&nbsp;Making for a new formula for success that can be applied in public, partner or internal environments.
[<a href="/2013/08/16/api-craft-soa-vs-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/public-media-logo.jpeg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/15/moving-beyond-the-constraints-of-commercial-api-design-with-the-public-media-platform/">Moving Beyond the Constraints of Commercial API Design With The Public Media Platform</a></h3>
			<p><em>15 Aug 2013</em></p>
			<p>There are just a handful of API platforms that I feel have greatly impacted the world of APIs and were significant in leading the space in important ways. These platforms include, but are not limited to Amazon Web Services, EBay, Salesforce, Google Maps and Twitter. All of these companies have changed the way we build applications and conduct business, by using APIs. In 2013, as we watch 50-100 public APIs launch each week, it can be difficult to see any sign of meaningful impementations in each wave of new API deployments. Just as I'm getting completely depressed about this lack of true API innovation, I was introduced to a new Public Media Platform that is pushing forward the tech, business and politics of APIs. The Public Media Platform (PMP) is a non-profit organization formed by public media's leading organizations, APM, NPR, PBS, PRI and PRX, to establish digital content repository and distribution system that will enable users to easily discover and interact with the news, information, cultural and educational content that is produced across the public media landscape. PMP has come together to develop a completely API driven platform, that will empower producers of public media to store and share text, digital video, audio, images and related meta data. This approach to media delivery will increase the distribution and reach, lower operating costs, while also handling the business rules, rights management necessary for media delivery in an online and mobile world. By lowering these barriers of entry, and simplifying operations and distribution, PMP will open up opportunities for these leading players to get their work in front of the large public media audience. However, this isn't just about benefiting these existing players, who are investing in the platform, once deployed, the platform will be opened up to any media producer. With this newly developed ecosystem, any developer will be able to build tools and applications that will help public media stations and producers deliver content...[<a href="/2013/08/15/moving-beyond-the-constraints-of-commercial-api-design-with-the-public-media-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/oauth-io-gear.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/13/simplifying-oauth-with-oauth-io/">Simplifying oAuth With oAuth.io</a></h3>
			<p><em>13 Aug 2013</em></p>
			<p>
Securely accessing API with oAuth can be one of the bigger pains in the ass for developers. Poor understanding of how oAuth works and often lack of good client libraries, can make API authentication a hurdle developers can't jump over.
Now developers don't have to stress out over oAuth. The tech savvy team over at Webshell.io has created a dead simple oAuth solution that allows you to integrate with over 70 of the common API providers, with just three lines of JavaScript, called oAuth.io.
What I like about oAuth.io the most, is that it is a true client side oAuth solution. With as many pure client-side, JavaScript, Single Page Apps I'm building, this type of oAuth solution just rocks!
I really enjoy dead simple solutions like oAuth.io, that take powerful, but complex things and abstracts away the complexity, making it something anyone can use. I recommend taking a look at oAuth.io and saving yourself the overhead of API integration using oAuth.
If that wasn't enough, It gets even better. They have also open sourced the oAuth.io core under the name&nbsp;oAuthd.
Nice work Webshell.io team!
[<a href="/2013/08/13/simplifying-oauth-with-oauth-io/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/apache-solr-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/13/have-a-bunch-of-documents-launch-an-api-with-apache-solr/">Have A Bunch Of Documents? Launch An API With Apache Solr</a></h3>
			<p><em>13 Aug 2013</em></p>
			<p>
In this computer driven business world its easy to end up with a mess of documents across a company's network.  No matter how much you try to organize, name and add metadata to documents, providing a quality search mechanism can be tough. Let alone one that you can use in web portals, web sites and even mobile and tablet applications.
One common way to to solve this problem historically has been using the Lucene Java Search Library, but in the age of web and mobile having an added API for integrating across all the place you will need search, is even better.  One of the quickest ways to deploy a document search API is using Apache Solr which is built on top of the Lucene Java Search Library.
Apache Solr delivers a full-text search server that provides a REST-like API, where you put documents in it via XML, JSON or CSV, and then you can query it via API and receive XML, JSON, CSV results, which enables you to deliver a quick, cost effective, open source API driven search for your documents.
There are many types of APIs, from data to programmatic resources--when it comes to document search APIs for knowledge bases, catalogs and other document stores, Apache Solr is a solid option.
[<a href="/2013/08/13/have-a-bunch-of-documents-launch-an-api-with-apache-solr/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/kinlane-white-house-3-web.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/11/my-first-day-in-washington-dc-as-presidential-innovation-fellow/">My First Day in Washington DC As Presidential Innovation Fellow</a></h3>
			<p><em>11 Aug 2013</em></p>
			<p>Tomorrow, will be my first official day in Washington D.C. as a Presidential Innovation Fellow. I'm super excited, yet also very calm about what is happening. It seems like it is so meant to be, that I don't have any questions or reservations, I just want to get to work. I was not in the realm, where I would take a job from anyone. I was doing very well, educating the masses about APIs as THE API Evangelist. However when the I got the call to be a Presidential Innovation Fellow, I did not hesitate. Yes. Of course I would take the role. As a Presidential Innovation Fellow I will be assigned to the "VA Modernization Team", along with two other very talented individuals, Molly Ruskin (@mollieruskin) and Ben Willman (@benwillman). While we are working together as a team, I will be specifically assigned to the Digits-2-Digits (D2D) project which is: An enterprise Digits-to-Digits Electronic Claims Submission Service that will provide a common access point to standardize, centralize, and integrate the universal collection of Benefits Claim Forms and supporting evidence data to produce a streamlined, paperless Veteran/Service member-centric claims process. I couldn't think of a more meaningful project to be part of. I've gone through the project and begun to acquaint myself with its inner workings, and I'm impressed with the amount of work and detail that has gone on so far, in such a short period of time. I'm really thankful for this opportunity, to really put APIs, and my experience to use for such a meaningful cause. I'm looking forward to the talented folks I will be working with over the next year, and the amount that I will learn on this new adventure in my life and career. In closing wanted to give a quick shout out to my father Jim Bignall (top photo) who I never got to know, and my step-father John Ronald Kehoe (bottom photo) who impacted my life...[<a href="/2013/08/11/my-first-day-in-washington-dc-as-presidential-innovation-fellow/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-gears.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/09/overview-of-api-integration/">Overview of API Integration</a></h3>
			<p><em>09 Aug 2013</em></p>
			<p>Overview of API Integration
Integrating with a single API can be tough, let alone multiple APIs. I see integration being about on-boarding, exploration, education, authentication, code samples, testing and sand-boxing of APIs--pretty much everything between the discovery of APIs and going into production.
The goal is to identify building blocks, tools, services and other knowledge that will help developers integrate APIs into their web, mobile and internet of things applications. API Evangelist as always been about the business of APIs from the API provider perspective, but now I'm looking to evolve it, and include the business of API consumption.
Some of the companies included in this research span other areas like API discovery or management, but they are also providing valuable tools and services that help ensure developers will be successful. Once I get, what I feel are the key areas of both deploying and consuming APIs up, I will work on the overlaps, and interoperability between these areas.
API integration tools and services are proving to be a very critical area to the space, one that will help stabilize the world of APIs and balance out much of the inconsistency that developers face when building apps that span multiple API providers or even depend on aggregate APis.
This area of API evangelist is brand new and will be evolving as I continue my research. &nbsp;If you have tools or approaches to development of apps using APIs, please let me know. I'd love to hear more about how you approach integration. As with all of my projects, this is a continuous work in progress, so stay tuned.


[<a href="/2013/08/09/overview-of-api-integration/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/runscope-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/08/evolving-beyond-open-the-runscope-community-projects-model/">Evolving Beyond Open: The Runscope Community Projects Model</a></h3>
			<p><em>08 Aug 2013</em></p>
			<p>The word "open" gets thrown around a lot in the API space. So much in fact, the term has lost just about any meaning. We don't say "open API" anymore, we identify an API as public, partner or private, evolving to a time where open APIs have merged with open source, and success means building community. In 2013, successful application development centers around community, which includes not just a healthy selection of valuable, stable API resources, but also a strong assortment of open source tools and cloud services that are community driven, and benefit all. A strong example of this evolution can be seen in the Netflix ecosystem, where a "public" API has been largely considered a failure, but a strong API driven community of device vendors has flourished, as well as the release of a suite of open source API and cloud tooling from the online movie and TV streaming platform. Another examples can be seen from the new API testing and integration provider Runscope. In addition to providing a suite of services for developers to use when developing API driven applications, Runscope also actively maintains four separate community projects: API Digest (apidigest.com) API Jobs (api-jobs.com) API Changelog (apichangelog.com) RequestBin (requestb.in) API Digest and API Jobs were pet projects of Runscope founder John Sheehan, prior to starting the company, but he has continued the same approach in a more formal manner, in which Sheehan calls the Runscope 'Community Projects' model, and adding API Changelog and most recently acquiring RequestBin. All of these Runscope projects are "free-forever open to the community sites that I hope are valuable resources related to what we're doing", according to Sheehan. "our overriding philosophy for all this is this: the more tools the better. We obviously benefit from the links, etc. but the bigger picture is we're working in a nascent area of dev tools and we benefit more from general exposure to the ideas we're working on than trying...[<a href="/2013/08/08/evolving-beyond-open-the-runscope-community-projects-model/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/repustate-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/08/delivering-your-api-as-a-virtual-appliance/">Delivering Your API as a Virtual Appliance</a></h3>
			<p><em>08 Aug 2013</em></p>
			<p>
Sentiment analysis and social media analytics API provider Repustate is providing a virtualized, portable, on-premise version of its API--adding a new twist to the concept of API deployment.
Using this new approach, Repustate is targeting the 5% of their user base who do not wish to transmit their data across the Internet, possibly because of legal or security concerns, and providing them with an on-premise version of their API, configured as a portable virtual appliance.
While the concept of on-premise vs cloud is nothing new, packaging up your API as a virtual appliance that can run anywhere is not that common.  I'm investigating various, emerging approaches to deploying virtual containers that can assist API providers in providing portable versions of their valuable API resources.
At this point, I'm not sure how much demand there is for on-premise API resources, but I could see huge potential for the deployment of virtualized API stacks within a corporate firewall, enough that I will keep highlighting interesting approaches like Repustates, and providing blueprints for designing and deploying virtualized API containers and appliances.
[<a href="/2013/08/08/delivering-your-api-as-a-virtual-appliance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/splunk-logo.gif" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/08/all-you-can-eat-buffet-of-developer-tools-at-splunk/">All You Can Eat Buffet of Developer Tools at Splunk</a></h3>
			<p><em>08 Aug 2013</em></p>
			<p>During my monitoring of the API space this week, I came across an interesting blog post from data platform Splunk, showcasing the tools they provide for developers who are putting their real-time data platform to use. Splunk has an amazing smorgasbord of developer tools, but there are a handful that I feel need highlighting, so that other data API providers can consider as part of their own offerings: Command Line Interface (CLI) - Monitor, configure and search Splunk via a terminal/shell interface or wrapping the commands in a shell script REST API - Most of the functionality in Splunk exposed as a suite of REST endpoints Software Development Kits (SDK) - Development kits for accelerating integration with the Splunk API in six different languages: Python, Java, JavaScript, PHP, Ruby, C# Apps and Add-ons - Packaging up of the various configurations, searches, knowledge objects, UI components and customizations, inputs, role definitions, field extractions etc. that you might typically create via Splunk on the web Scripted inputs - Out of the box, Splunk has simple generic input options available for getting data from a file or receiving data over TCP/UDP Modular Inputs - Modular Inputs build upon Scripted Inputs by elevating the creation of custom input add-ons to first class citizen status in Splunk Custom search commands - Splunk&rsquo;s Search language is incredibly powerful and extensive for deriving a wide range of analytical insights from your indexed data Custom alert scripts - Splunk alerting channels provided by defaults are Email and RSS. But let&rsquo;s say for example that you wanted to send alerts via SMS, to a Messaging Queue, as an SNMP or directly to a trouble ticket system Custom REST endpoints - Splunk&rsquo;s REST API is very thorough, but this can also be extended with your own custom REST endpoints that you could then integrate with programmatically Obviously Splunk is a big data platform, so the idea of custom search and other building blocks, may not...[<a href="/2013/08/08/all-you-can-eat-buffet-of-developer-tools-at-splunk/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-umbrella.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/02/the-federal-government-deploys-their-own-api-management-solution/">The Federal Government Deploys Their Own API Management Solution</a></h3>
			<p><em>02 Aug 2013</em></p>
			<p>Some of the smart people in Washington D.C. have decided to roll their own API management solution, providing their own open source solution to many of the common problems faced by API providers like issuing API keys, rate limit and provide analytics. Available at api.data.gov/about, but powered by an open source project called API Umbrella, the project aims to be a free and open API management option for federal agencies. API Umbrella provides some of the most common aspects of API management: Documentation API Keys Rate Limiting Analytics The roadmap for API Umbrella is also interesting, looking to deliver: Self-service web admin Interactive documentation User management Granular permissions and rate limits While creating, yet another API management solution may not seem like the best idea at first, I can see why federal agencies would do it. Because of terms of use, costs and vendor lock-in with proprietary solutions, they are just seeking a simpler, cost effective alternative to solutions available in the private sector. I'd love to see a blueprint established for the features available in API Umbrella, the current implementation for the gatekeeper is in Node.js and the web interface is in Ruby--it would be nice to have an open blueprint which would allow for others to contribute PHP, Python and other iterations of the same model. Providing a way for organizations to choose an option that fits with the resources they have available, while still providing interoperability. It would also be great to see other city, county and state organizations adopt, deploy and evolve API Umbrella, providing a solution for other types of government organizations. I'm not intimate with the various needs of different types of government, but I'm sure there are some nuances that might make a difference. As I ramp up my time working in DC this year I will spend more time getting intimate with API Umbrella, as well as other open data portal solutions like CKAN, and hopefully provide...[<a href="/2013/08/02/the-federal-government-deploys-their-own-api-management-solution/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-padlock.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/08/02/securing-your-api-101/">Securing Your API 101</a></h3>
			<p><em>02 Aug 2013</em></p>
			<p>I get a lot of questions from folks about the why, when and how of securing an API. And in keeping in sync with my audience, I wanted to provide a plain english story of securing your API, avoiding &nbsp;the often enterprise, service oriented architecture (SOA) jargon, which can confuse the situation for many who are just getting their feet wet. So, you have created a basic API from your data or other resource, that returns XML or JSON for users--now you want to secure it. The reasons for securing your API will vary on a case by case basis. Your might be releasing data that you only want a handful of select people to access, or you might be looking to reduce the amount of server and bandwidth resources it takes to support an API, or you might just want to intimately understand who accesses an API and why and how they are using it. Or D) All The Above! There are a handful of approaches to providing access to your API: Open - Just keeping your API wide open, accessible to anyone who has the URL. Obviously this isn't secured, but provides contrast for other options BasicAuth - Basic authentication is a native part of HTTP and the Internet, which employs a username / password combination that is passed when accessing an API, which the server then authenticates allowing or denying access Key - Requiring an application or user key which is usually a combination of alpha and numeric characters, which is obtained by each developers through a API registration process, providing a unique access key for each consumer oAuth - oAuth is an open authorization standard, that is usually employed when there is more sensitive, user specific data available via an API. oAuth is considered to be standard approach to securing API access, but increases the technical learning curve for API users I'm also investigating another approach that is an alternative to...[<a href="/2013/08/02/securing-your-api-101/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/octopart-logo.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/31/giving-excel-power-users-the-api-driven-resources-they-need/">Giving Excel Power Users The API Driven Resources They Need</a></h3>
			<p><em>31 Jul 2013</em></p>
			<p>I've been an advocate for non-developers being one of the most important audiences for your API, since I started API Evangelist. I know us developers love to believe we are the primary force behind the success of APIs like Twitter--it is partially true, we are. But we also had the assistance of the power Twitter user in getting the good word out. Tech savvy journalists, marketing and other folks who get the platform, API, and value that lies within, while also possessing a large or very influential audience. Another example of this dynamic I'm seeing unfold in slow motion is from web APIs that are driving the Excel spreadsheet. I've posted before about using Google Spreadsheets in conjunction with APIs, and I just saw a another great example of using Microsoft Excel from the electronics parts provider Octoparts. Octoparts walks us through an API to Microsoft Excel solution, that has some pretty obvioius potential, by providing a direct connection between their product catalog to customers and partners via their API and an Excel spreadsheet. The particular approach requires the usage of DataNitro, a plugin for Excel that allow you to run Python functions within your spreadsheets. Once installed, you can import products using the Ocotoparts APIs, complete with functions for filtering by manufacture, SKU, price and other key catalog fields. This is a pretty significant example of how APIs can be used by non-developers, and specifically power Excel users. I don't know about your experience, but pretty much every industry I've worked in, is being run by Excel wielding business and marketing power users. This isn't some small group of niche developers, this group of users represents the engine of business in our everyday world. Think of the potential, when we educate these everyday users about APIs, then provide them with simple API usage examples, allowing them to access the valuable resources they need within an organization or externally from partners, government and other public...[<a href="/2013/07/31/giving-excel-power-users-the-api-driven-resources-they-need/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/technology-business-politics-apis.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/28/warming-up-with-the-100k-view-of-api-management/">Warming Up With The 100K View of API Management</a></h3>
			<p><em>28 Jul 2013</em></p>
			<p>This is my first blog post, following a very nice week, offline in Maine. I have purposely setup the API Evangelist network to be a mesh network of API research project nodes, allowing anyone to to start from ground zero and work their way through the different aspects of the API universe.  The dirty little secret, is I set this up to benefit me first, then you, the reader.  After I get back to the world from time off, I'm very disoriented and I need a way to start over and work my way back through all of my projects.
Each time I pick my research up, I need to work my way through it, refresh my memory, and hopefully constantly realign my view of the universe, not just after vacations, but on a week to week basis. So this morning I started with the 100K view of providing APIs.
When I started API Evangelist, I acknowledged that the technology of APIs was being worked out, and the business of APIs was something that needed researching. Now in 2013, I'm seeing the same theme emerge around the politics of APIs.
This venn diagram is a rough, 100K view of what I consider to reflect a healthy strategy for an API provider. Depending on each company's goals, they may give more attention to any of these building blocks.

If you are coming from the enterprise you will see much of this as SOA governance, which I have spoken about in the past being the legacy inverse of API management, and will continue to articulate in future stories as I can.
Now that I'm warmed up with the 100K view of the API space, I'm ready to finish work on my API management, discovery and integration white papers I was working on before I left, as well as my weekly API monitoring and analysis.
[<a href="/2013/07/28/warming-up-with-the-100k-view-of-api-management/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-puzzle-3.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/28/providing-apis-design-deployment-management/">Providing APIs - Design, Deployment, Management</a></h3>
			<p><em>28 Jul 2013</em></p>
			<p>
Since 2010 I have had a page on this site dedicated to "API Management". In 2013 the API universe expanded, and I've broken my work in this area into three separate, open source API research projects.

Design
Deployment
Management

I'm grouping these projects under a single umbrella, I'm now calling "providing APIs".  If you are a company or organization looking to provide API driven resources to partners, the public or even use them for internal operations, this section is for you.
I will be adding several other API research areas in the near future, including:

Monetization
Evangelism
Security
Embeddable

For each of the projects you will find building blocks, companies, tools, news and analysis. All projects stick to my new Hacker Storytelling format, making sure my research is open, distributable and machine readable.
[<a href="/2013/07/28/providing-apis-design-deployment-management/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/KinLane-04-2012-Headshot-3-250.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/28/primary-surface-area-of-my-online-identity/">Primary Surface Area of My Online Identity</a></h3>
			<p><em>28 Jul 2013</em></p>
			<p>
I am always working to define myself, in both the physical world and the virtual one that has become an increasing part of my identity. As part of this work, I'm spending some time trying to understand the primary surface area of online identity. Sure there are other aspects, but this is the face that most of you will see.
Name: Kin Lane
Title: API Evangelist
Company: API Evangelist
Location: Hermosa Beach, CA
Description:&nbsp;API Evangelist, Hacker, Tech Gypsy, Beer Snob and Father
Photo:&nbsp;https://s3.amazonaws.com/kinlane-productions2/kin-lane/KinLane-04-2012-Headshot-3-250.jpg&nbsp;
Email: [email&#160;protected]
Phone:&nbsp;(541) 357-9073
Blog Profile(s):

Kin Lane - http://kinlane.com
API Evangelist - http://apievangelist.com
API Voice - http://apivoice.com
API Stack - http://theapistack.com
Hacker Storytelling - http://hackerstorytelling.com

Social Profile(s):

Twitter -&nbsp;https://twitter.com/kinlane&nbsp;
Facebook -&nbsp;https://www.facebook.com/kinlane&nbsp;
LinkedIn -&nbsp;http://www.linkedin.com/in/kinlane&nbsp;
Google -&nbsp;https://plus.google.com/106460238807821851374/about&nbsp;
Github -&nbsp;https://github.com/kinlane&nbsp;

Device Profile(s):

Laptop - Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.71 Safari/537.36
IPhone - Mozilla/5.0 (iPhone; CPU iPhone OS 6_1_3 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) CriOS/28.0.1500.12 Mobile/10B329 Safari/8536.25
Tablet - Mozilla/5.0 (Linux; Android 4.2.2; Nexus 7 Build/JDQ39) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19

[<a href="/2013/07/28/primary-surface-area-of-my-online-identity/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-analytics-2.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/28/analytics-for-both-api-providers-and-consumers/">Analytics For Both API Providers and Consumers</a></h3>
			<p><em>28 Jul 2013</em></p>
			<p>
I'm going through all of my bulding blocks and tools for API management, as I'm updating and expanding this area of the API Evangelist network. After doing this work, I'm surprised that are aren't any open source or even independent analytics solutions that serve both the API provider and consumer.
Generally analytics for API providers and consumers are provided by one of the numerous API management providers.But what I'm looking for is stand alone analytics solutions that API providers could deploy on their own, or even an API integrator could put to use to track their own API usage across multiple APIs.
I learned about using StatsD and Graphite from the InfoChimps team, and I've discussed the potential of using Google Analytics API as a layer of your API deployment or integration. There are also a lot of mobile analytics packages, but there is no straightforward solution for API providers and consumers.

Maybe it is because API management providers are meeting the needs of API providers, and I think the new breed of API integration providers like Runscope and TheRightAPI will be addressing the consumer side of the demand.  Or maybe I'm just not looking hard enough, and there is another solution out there.
At first thought, it seems logical to use Google Analytics, and track your API alongside your website operations.  It could be that these two are connected?  Regardless, with the number of public and open APIs, as well as the growing number of APIs it seems that a focused solution would emerge, and possibly an open source solution.
I'm sure I'm just missing some existing analytics that could be easily converted and applied to server side APIs. If you have an existing and are not using an API management solution, how do you track the usage of your API resources? I'd love to understand this more.
[<a href="/2013/07/28/analytics-for-both-api-providers-and-consumers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/KinLane-04-2012-Headshot-3-250.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/21/the-next-decade-is-going-to-be-a-wild-ride/">The Next Decade is Going to Be A Wild Ride</a></h3>
			<p><em>21 Jul 2013</em></p>
			<p>I turned 41 in June. As I retool, reboot and readjust during what I guess could be called "vacation", I'm reflecting on the last year and the major shift that has gone on in my world. First off, turning 40 was hard. Usually birthdays come and go without any concern, but 40 was not easy for me. I'll spare you the details, but the first half of 2012 was a very trying period, and by the time my birthday came around in June I was not a happy camper. With this in mind, looking back at the last 12 months, I can see it was about transition, and preparing for the period of my life. While the first part of 2012 was an uphill drive, during the second half things seem to fall into place, allowing me to shift gears, pick up speed, setting a significantly new pace for what I see as the next decade of my life. In 2010, I realized my career trajectory, after achieving a VP level position, working on SAP and Google events was not satisfying for me, and I needed a course correction. I wanted to make sure I was doing something meaningful, but spoke to my experience in architecting distributed, data driven sites and applications. I had just had a very meaningful experiences in scaling architecture for SAP and Google, powered by application programming interfaces (API) and the cloud. I seen the potential of APIs, in not just designing distributed apps, but make them scalable, portable, collaborative and modular--opening up not just a new way to design web applications, but mobile applications, while also introducing companies to new, more transparent ways of conducting business. Its been 3 years since I made the decision to do API Evangelist, dedicating my world to studying not just the technical, but the business and politics of providing and consuming APIs. While studying the space, I've worked hard to tell the story of...[<a href="/2013/07/21/the-next-decade-is-going-to-be-a-wild-ride/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/flickr-logo.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/19/the-flickr-api-will-be-unavailable/">The Flickr API Will Be Unavailable</a></h3>
			<p><em>19 Jul 2013</em></p>
			<p>
I received an email from Flickr today about a planned API outage later this month. I'm an afficinado of the interesting ways API providers communicate around maintenance and outages within their ecosystem, and wanted to share:
We see that you own an active API key for the Flickr API, and wanted to let you know that Flickr is scheduling a maintenance outage for Thursday, July 25th, from 4 to 10pm PDT.
During the outage window, the Flickr API will be unavailable. As such, you may want to adjust your app to gracefully handle the error that will result when it calls our API during the outage. (If your app is not yet public, this will only impact your personal use of the API.)
Specifically, a call to the Flickr API will result in the following error: 105: Service currently unavailable (Site Disabled). Here are example responses to expect:  XML (REST):   JSONP: jsonFlickrApi({ "stat": "fail", "code": "105", "message": "Service currently unavailable (Site Disabled)" }) JSON: { "stat": "fail", "code": "105", "message": "Service currently unavailable (Site Disabled)" } 
We hope this helps, and thank you for being a member of our API community,
The Flickreenos
When it comes to communication around your API, it can be more art, than science. I don't think there is a precise formula for keeping people informed of what is coming down the pipes with your API. But if you keep it active, personal and informative--you can do alright.
While the Flickr API might not be in the limelight all the time, they've been doing it a while, and I can't help but cheer for the image and photo API pioneer.
[<a href="/2013/07/19/the-flickr-api-will-be-unavailable/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-dump-truck.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/19/take-that-data-dump-access-to-your-organizations-database-and-build-your-api/">Take That Data Dump Access To Your Organizations Database And Build Your API</a></h3>
			<p><em>19 Jul 2013</em></p>
			<p>I have a long, winding history of database administration in my past. I've been managing databases since my first job working on school district databases in the State of Oregon in 1988. So I've been that database administrator you are asking for data access from, and I've personally managed and interacted with too many of these DBAs to keep track. I was discussing strategies for getting access to your organizations central database today, and realized I don't have enough of these data acquisition stories on API Evangelist. This data access quest will start with a burning need to build a website, mobile app or provide access to a 3rd party partner to some data. You know that this data is in your organizations database, all you need is access to make your project come to life. In many organizations this can be easier said than done. There could be numerous reasons why this will be difficult ranging from technical to political, and be fuly prepared for it to often be political, masked as technical.&nbsp; Database access bottlenecks can range from security concerns to cranky, unfriendly DBAs or possibly just that the central database system has been in operation for many years and cannot handle large number of additional requests. Unwinding the politics of database access can often be more time consuming than the technical side of database access. In many organizations your time is better spent just getting what you need and moving on. Make it only about access, and not looking to change the legacy process. This legacy is why the most common way of getting at your organizations data will be a data dump to a network or FTP location, in a CSV, XML or JSON format. If your database administrator offers this to you, take it. You can make use of this, design the application you need need, without the headache of battling to change things or getting direct access. A data...[<a href="/2013/07/19/take-that-data-dump-access-to-your-organizations-database-and-build-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/download.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/19/providing-full-data-download-vs-api/">Providing Full Data Download vs API</a></h3>
			<p><em>19 Jul 2013</em></p>
			<p>I've had so many discussions on this topic, I felt like I had written a post on it, but when the topic came up again today, I realized I hadn't. This is a regular conversation I get into with open data folks, regarding whether you should deploy open data as download vs an API. I'll spare you having to read my entire rant, the answer is if you can, both. You should always provide a full data download, and additionally API access when the resources are available. There is no versus argument, there are scenarios where both are valuable and offer benefits over the other. As a developer, I just want quick, complete access to data. I'm always in a hurry to just get to work, and I have the skills to parse, store, transform this data for whatever reason I will need. So let's just get to work. Give me a download and get the hell out of my way. Pretty straight forward logic. This is all true, but it is a very short-sighted, narrowly focused perspective and shouldn't impede larger, long term goals around data acquisition, management and access. Taking your data, keying it up, and asking users to request a key and access via an API, might be a wall for the eager alpha dev, but there are many that could benefit from well thought out API end points, search &amp; filtering tools and the ability to offload all operations to someone else. Not all developers will be equipped to deal with the overhead of a full download. Looking at it from a different vantage point, as a steward of data, an API can provide you with some very granular analytics on how people are using your data or even, not properly using your data. This valuable insight can provide an extreme amount of value in the future of how you gather your data, organize and make accessible. This is something you...[<a href="/2013/07/19/providing-full-data-download-vs-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-analytics-2.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/19/a-glimpse-at-the-future-of-api-driven-analytics/">A Glimpse At The Future of API Driven Analytics</a></h3>
			<p><em>19 Jul 2013</em></p>
			<p>
I'm rapidly expanding the areas I track on in the world of APIs, trying to understand the myriad ways APIs are influencing interoperability between companies and platforms, being applied alongside voice or used in automobiles,&nbsp;our homes, and beyond.
One area I'm currently researching, is on API usage in the very hyped world of data. I'm wading through numerous startups, looking for innovative approaches to data + APIs. One specific area of this research, that I'm hoping to have published soon is around API driven data analytics.
To provide a quick glimpse into what I mean when I talk about the future of data analytic APIs, I wanted to showcase two startups that resemble what I see coming down the pipes:





ZoomData - Zoomdata is the next generation data visualization system that easily allows companies and people to understand data visually in real time.



  

Polychart - Polychart is a data analytics and visualization company that helps turn numbers and statistics into charts and dashboards to drive more innovative and accurate decision-making.




While these companies are providing pretty straightforward analytic solutions that run in the cloud, what I find innovative is their flexible data sources. Much like reciprocity providers are evolving ETL (Extract Transfer Load), moving beyond just internal data sources, to API driven cloud sources--this new breed of analytics providers are allowing you to visualize your data, directly from the cloud platforms where the data is generated and stored.
As more of our personal and professional lives move into the clouds, we are going to need new ways to visualize and find insight across the huge amounts of data we are generating daily, directly from these sources. This new breed of API driven analytic companies are going to help us get there.
Look for more research published on this in the near future.
[<a href="/2013/07/19/a-glimpse-at-the-future-of-api-driven-analytics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/ben-balter.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/17/from-github-issue-to-story-to-resource/">From Github Issue To Story to Resource</a></h3>
			<p><em>17 Jul 2013</em></p>
			<p>
As an evangelist, your responsibility is to get the word out about an API, and provide users with the resources they will need to be successful.
While executing on your evangelism strategy, there are numerous ways to generate stories for your blog, but one of my favorite approaches is to cherry pick the best issues on your Github repositories.
If you are like me, your using Github repos to manage everything from documentation and code samples to presentations and terms of use. One of the perks of using Github, is you immediately gain the benefits of the Github issue management system.
While many of the issues submitted on your Github repos will be mundane and not very story-worthy. Always keep an eye out for ones that are ripe for picking and turning into an actual stories for your blog and social media accounts.
Earlier today, I received some great feedback via the issue management system on the API Evangelist repo.  Ben Balter (@benbalter) from Github recommended that I use a permalink frontmatter to simplify my file structure. In my migration from a PHP blog to using Jekyll on Github Pages, I hadn't considered this, and when I learn something new, I want to share it by crafting into a story and posting it on the blog.
Not only will this Github issue become a story, providing much needed SEO and education for my users, I will also turn this into a more structured resource, and provide as part of future educational content.
Pulling stories and educational content from relevant Github issues can be a great way to generate valuable content for your platform, and if it is done in real-time it isn't really that much work.
[<a href="/2013/07/17/from-github-issue-to-story-to-resource/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/next-data-gov-screenshot.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/16/the-next-iteration-of-government-data/">The Next Iteration of Government Data</a></h3>
			<p><em>16 Jul 2013</em></p>
			<p>Our federal government is working on the next iteration of Data.gov, and just published a first look at what is being called Next.Data.Gov. This next version is meant to build off the momentum of the recent open data policy, which is looking to embed open data practices across our federal government, changing the way it operates. To plan for the next iteration of Data.gov, the General Services Administration (GSA), a handful of Presidential Innovation Fellows, and OSTP staff studied usage patterns from the last version, and discovered that people were hungry for more examples of how government data is used. Equipped with this understanding, they are making sure the next version included a communication stream that will enable each data community to communicate how datasets are being used and making an impact. The Data.Gov team also updated the search, adding data visualizations using D3.js, used open source fonts and implemented a responsive design for Next.Data.Gov. Next.Data.Gov is also built on top of both Wordpress and CKAN, delivering the CMS and data management capabilities, using proven open source tools. Beyond the approach to requirements gather, responsive design and adopting open source tools, what I think is the most important aspect of Next.Data.Gov is that the planning, design and deployment process is open and collaborative--which has the potential to alter how our government conducts business on a daily business. The next iteration of OUR federal government data portal is being planned publicly using Github, Quora and Twitter. Historically much of our government planning process is done behind closed doors, by government agencies, with the assistance of contractors. The innovation I'm seeing come from washington in the form of Open Data Policy, Healthcare.gov and now Next.Data.Gov makes me very optimistic for the future of how our government works, and I am honored to be going to DC to help. If we can tackle the process of opening up government, agency by agency, project by project, make the process...[<a href="/2013/07/16/the-next-iteration-of-government-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/bw-story-bubble.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/16/stories-are-essential-to-success-of-open-data-in-government/">Stories Are Essential To Success of Open Data in Government</a></h3>
			<p><em>16 Jul 2013</em></p>
			<p>
When I first started API Evangelist, I quickly saw that there was more to this whole API game, beyond just the technology. Simple, logical, technological solutions like REST and JSON are essential, but there was more to the success of an API than just the tech behind.
After looking at hundreds of the most successful APIs, one pattern for success consistently emerged, storytelling. The most successful API platforms like Amazon, Twitter and Twilio all had one thing in common, stories were told about how their APIs were being used, and the value that was created.
While reading the story around the Federal Government redesign of Data.gov I was happy to see a major revelation that came from requirements gathering for the next iteration called Next.Data.Gov. Where after studying usage patterns around Data.gov, the redesign team found that many visitors were hungry for stories of how government data was being used, looking for clear examples of the potential of open government data.&nbsp;
Storytelling is essential for any open data or API platform. Consumers need examples of how data and resources are currently being used. A process that will stimulate the imagination and increase the potential for new ideas and development.
I can't stress enough the importance of storytelling. If you do not have the resources to tell your own stories, make sure there are ways that the community can tell their own stories.
It doesn't matter how solid your technological implementation is, if there aren't stories for users to discover and learn about the potential of open data and API resources, even the best platform implementation will fail.
So go forth and tell the stories of your API, and the value it has created.
[<a href="/2013/07/16/stories-are-essential-to-success-of-open-data-in-government/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/lego_docker.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/15/virtual-containers-stacks-apis-and-application-management/">Virtual Containers, Stacks, APIs And Application Management</a></h3>
			<p><em>15 Jul 2013</em></p>
			<p>I've been trying to organize my thoughts around emerging trends in using virtualized app containers, that are providing a much more modular approach to deploying and managing application backends. This is an evolution of earlier thoughts I've had around virtual API stacks. From the last week, I'm looking at three separate approaches to application and API management using virtualized containers: Docker - The Linux Container EngineDocker is an open-source engine which automates the deployment of applications as highly portable, self-sufficient containers. Docker containers are both hardware-agnostic and platform-agnostic, allowing them to run anywhere, freeing you from having to use a particular language, framework or packaging system--making them great building blocks for deploying and scaling web apps, databases and backend services without depending on a particular stack or provider. Heroku - Application Instances with Portable Features and DependenciesHeroku fork lets you create unique, running instances of existing applications in a single command, making it fast and simple to set up homogenous development, staging and production environments. Heroku pipelines then lets you define the relationship between apps and easily promote from one app to another using what is known as a "slug". On Heroku, a slug is a bundle of your source, fetched dependencies, the language runtime, and compiled/generated output of the build system, ready for execution in development or production environments. StackMob - BaaS Development and Production Workflows StackMob has its own unique approach application development workflow which includes use integrated SDKs, and custom configuration for establishing an applications separate development and production environments. Then using this separate environment, Stackmob offers multiple concurrent APIs in production through the use of API snapshots which ensures backwards compatibility of your app over time. Docker, definitely offers the widest opportunity for what I'm talking about because it is an open source container solution. But it is interesting to see how Heroku is applying this concept to app management at the PaaS level, and how Stackmob is providing a very...[<a href="/2013/07/15/virtual-containers-stacks-apis-and-application-management/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/dbx-press-datastores.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/15/dropbox-as-default-file-storage-api-for-apps/">Dropbox As Default File Storage API For Apps</a></h3>
			<p><em>15 Jul 2013</em></p>
			<p>
I was working my way through all the Dropbox news that came out of the DBX Conference over the weekend. There was some pretty savvy moves by the cloud storage provider, continuing its evolution towards being the default storage for web and mobile applications.
After reading this weekend, I have three key stories still open in tabs:

The Datastore API: a new way to store and sync app data
Meet the Dropbox Saver
The Chooser, now for iOS and Android

Dropbox is already the preferred choice for app storage system for many developers, but with the introduction of the datastore API, they will definitely be stepping up the integration of Dropbox into future apps.
For the end users, the ability to save files to dropbox and choose files from their mobile devices will only increase their demand for native Dropbox functionality in web and mobile apps--feeding the cycle.
The combination of making developers lives easier, while also providing the features that end users needs will continue to establish Dropbox as a default file system API for application developers.
Makes you consider how other APIs can emulate some of what Dropbox is dish'n.
[<a href="/2013/07/15/dropbox-as-default-file-storage-api-for-apps/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/baas-trends.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/14/simple-api-example-company-listing/">Simple API Example: Company Listing</a></h3>
			<p><em>14 Jul 2013</em></p>
			<p>I'm always looking for dead simple examples of what an API is to help average folk understand what APIs are, and how they can be applied in their worlds. My current research on backend as a service providers (BaaS) providers a great example. On my research site i provide listing of BaaS providers for my readers: This listing of companies is a pretty common occurence on the web. As a web developer, if I want to show this listing I use HTML, which is markup that provides a visual formatting for this listing: HTML makes content presentable for humans. Now if I want to make this same content available for other web sites and applications, instead of using HTML, I will use another open format called JSON, which is easy for other computers and applications to consume. In this particular case, I provide an HTML listing of BaaS providers, as well as a JSON listing. I actually use the JSON file to display the HTML, using a JavaScript templating library called Mustache. In addition to a static HTML and JSON listing of BaaS providers, I also provide an API which returns the same content as the JSON file, but allows you to do queries and get only the BaaS companies you are looking for. All of my data works this wa by providing: HTML - An HTML listing meant for humans to view. JSON - A JSON listing meant for developers to use in other sites and apps. API - An API allowing querying, filtering and other dynamic access. This process always starts with API driven data, and I produce the HTML from my APIs. This is the beauty of API first. You can generate your web content from it, being the first user of all your APIs. Then encourage others to syndicate content by providing them with simple scripts for displaying HTML listings, driven from JSON and / or your API. The only difference...[<a href="/2013/07/14/simple-api-example-company-listing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/open-machine-readable.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/13/hacker-storytelling-open-and-machine-readable-by-default/">Hacker Storytelling: Open and Machine Readable By Default</a></h3>
			<p><em>13 Jul 2013</em></p>
			<p>The primary reason I'm switching all of my 35+ research projects in the API space to my new Hacker Storytelling format, is to make everything I do open and machine readable by default. Every project I'm working on is a Github project, with the public side of the research always available via an API Evangelist subdomain and pointed at a Github page. I publish a series of static pages, wrapped in an API evangelist template, combined with a blog to help organize my curated news and analysis in a chronological way. Along with all the pages and blog I make all data available as JSON files, which I display across the research project using JavaScript templates. My goal is to make every bit of my research machine readable by default, and ensure the widest possible re-use and distribution. One active example of this in the wild, is my research around backend as a service, also known as Baas. I've been monitoring all the providers in the space for the last year, and tracking on some of the common building blocks they use, much like I have been doing with APIs for last three years. As part of my BaaS research I've published two datasets: BaaS Providers Building Blocks You can view the data on the website, using JavaScript templates that display the JSON data on HTML pages. When you view the page that lists the BaaS providers or the common BaaS building blocks, you can choose to get at the data behind the display as JSON, via API, as well as being able to grab the script I use to display the JSON data. All of my research is ongoing, and I publish news, analysis and new companies and research on a regular basis. My goal with this evolving approach, is to remove machine readable from being after thought, making it default in all my work. With data available via JSON and API, everything published...[<a href="/2013/07/13/hacker-storytelling-open-and-machine-readable-by-default/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/dam-breach.jpg" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/12/apis-will-expose-toxic-material-behind-firewall/">APIs Will Expose Toxic Material Behind Firewall</a></h3>
			<p><em>12 Jul 2013</em></p>
			<p>After looking back at 2012, I wanted a January 1, 2013 blog post for my blogs.  My first blog post of 2012 was my tour schedule for January, 2012.  While it was a pretty busy time for evangelizing and hackathons, I wanted something a little deeper.  I&rsquo;m not sure what, but I will play with the format year to year, until I find what I&rsquo;m looking for.
Every year I rewrite my bio, based upon where I am.  I&rsquo;ve been doing this since 2009.   This year I will write inaugural blog posts along with my bio rework, and post to each of my active blogs.  We&rsquo;ll see if it resonates again in 12 months and I do it again in 2014.
[<a href="/2013/07/12/apis-will-expose-toxic-material-behind-firewall/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/openi-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/11/balancing-commercial-noncommercial-privacy-and-ownership-in-the-world-of-apis/">Balancing Commercial, Non-Commercial, Privacy and Ownership In The World of APIs</a></h3>
			<p><em>11 Jul 2013</em></p>
			<p>I just spent 30 minutes on the phone with an important group in the European Union called OPENi, which is aiming to be an open-source, web-based, framework for integrating applications with cloud-based services via APIs. Straight from the organization's mouth: OPENi aims at inspiring innovation in the European mobile applications industry, by radically improving the interoperability of cloud-based services and trust in personal cloud storage through the development of a consumer-centric, open source mobile cloud applications platform. I will be providing feedback and guidance as a member of the group's user advisory board. While on the call today, there was a mention of concerns around being seen as the usual, heavy handed EU entity that is too focused on user privacy and ownership, which could prevent the group from being well received, but also ignore the economic opportunities APIs and interoperability afford businesses, developers and end users. This is a reminder for me of how EU tech companies, and the countries where they reside, are often perceived around the globe and here in the US. But also stands in stark contrast to the illness I see from the Silicon Valley, and U.S side of the discussion. In a world where everything is about economic opportunity, and the data in these API pipes are the new oil, aka the new resource, that is meant to be extracted, with little or no concern for privacy and ownership of the end-users. In 2014, there is a huge opportunity for all of us to meet in the middle, acknowledging that there is great economic opportunity around APIs, cloud services, interoperability and the data that flows between platforms. But there is even greater economic opportunity if we acknowledge and respect the privacy, ownership of users of the web, mobile and Internet of Things we develop in the cloud. I would also add a 3rd dimension to this discussion. That there is also a massive non-commercial opportunity for APIs, interoperability and...[<a href="/2013/07/11/balancing-commercial-noncommercial-privacy-and-ownership-in-the-world-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/salesforce-logo.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/10/deploying-your-api-from-paas-using-salesforce-and-heroku/">Deploying Your API From PaaS Using Salesforce and Heroku</a></h3>
			<p><em>10 Jul 2013</em></p>
			<p>The world of API management is expanding, and to support this growth I've broke up my research into three separate buckets: API design, API deployment and API management. Zooming in on API deployment, one of the ares I'm looking to educate the masses about, is quick and dirty API deployment using existing PaaS. Providing ways for companies, and their internal problem owners to rapidly deploy APIs via their existing tools and services. One great example I recently found is from Jeff Douglas (@jeffdonthemic), Platform Architect @CloudSpokes &amp; Sr. Tech. Consultant @Appirio, on using Heroku to deploy an API from your Salesforce organization. Jeff's walkthrough is a little choppy, but it is full of extremely valuable nuggets of information, and he says he's working on a more polished version of it to present. The approach is pretty straightforward. You are simply deploying an API on Heroku, that uses code to make calls to your Salesforce account via the Salesforce Apex API. The API you launch on the Heroku PaaS platform will abstract away your custom code that you use to pull specific Salesforce resources, allowing you to scale and securely deliver existing data in your Salesforce platform, and ultimately use Saleforce as an administrative interface for your API(s). Jeff gives examples of how you can do this in Ruby, Node.js and even Java, and talks about approaches to versioning the API and providing documentation. He also recommends that for higher traffic APIs you should look at existing API management companies to help you secure, meter, develop service packages, and provide you with billing and analytics options. I'd like to see starter kits developed in PHP, Python, Ruby, Node and Java that allow for quick API deployment using data from Saleforce on not just Heroku, but other popular PaaS platforms like Amazon and Google. There are a lot of companies who use Salesforce to manage vital resources, and providing the business and marketing folks within these organizations...[<a href="/2013/07/10/deploying-your-api-from-paas-using-salesforce-and-heroku/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px; margin-bottom: -25px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/forkme_right_red.png" alt="API Evangelist" width="325" align="left" /></div>
			<h3><a href="/2013/07/10/another-embeddable-tool-that-will-go-down-in-history-github-ribbons/">Another Embeddable Tool That Will Go Down In History: Github Ribbons</a></h3>
			<p><em>10 Jul 2013</em></p>
			<p>
Simple, embeddable tools can be one of the most powerful tool in an API evangelists toolbox.  While there was a wide variety of forces that were at play in the social API evolution, contributing to the success of Twitter and Facebook, embeddable buttons, badgers widgets definitely played a leading role.
One embeddable implementation that will go down in history alongside the Twitter share button and Facebook like button, is the Github Ribbon. The Github Ribbon is the thin little ribbon you are increasingly seeing on projects, sites and applications (notice them rolling out across the API Evangelist network), letting you know you that a site's source is available on Github, and available for you to download or fork.
Github has created a simple page where you can find an assortment of embeddable Github Ribbons, in multiple colors and positioning. If you are not satisfied with the colors, they also provide the PSD and even the original font that is used. A complete, fully branded, embeddable tool that encourages use.
While I actually prefer the simplicity of the Github Ribbons, my only suggestion to Github would be to either provide JavaScript version or tracking image version, that would allow each Github Ribbon to ping home. You could also integrate with the Github API to offer tighter repository integration. Think of the network visualizations you could create, mapping out the distributed world of Github implementations across languages, countries, etc.
Even without the tracking and API integration, Github Ribbons is an extremely powerful approach to branding, and great example of using embeddable tools in the wild.
[<a href="/2013/07/10/another-embeddable-tool-that-will-go-down-in-history-github-ribbons/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page27" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page29" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
