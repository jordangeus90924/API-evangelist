<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }
    
    .container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
    }
    .video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }    

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/06/explaining-api-security-to-organizational-leadership/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/castle-on-hill-edinburgh_propaganda_leaflets.JPG" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/06/explaining-api-security-to-organizational-leadership/">Explaining API Security To Organizational Leadership</a></h3>
			<p><em>06 Mar 2018</em></p>
			<p>I’ve been tasked with helping explain API security to senior leadership, and wanted to work through my ideas here on the blog. For this audience, I’m not going to get down into the weeds regarding the technical specification behind OAuth, and other approaches, and try to keep things high level, introducing folks to the art that is API security. The phrase API security represents a balance of concepts because APIs are by nature about providing access, while security is about controlling and sometimes limiting access, resulting in a new way of getting business done on the open web. First, What Are APIs? APIs are not the latest trend, or vendor solution, they are the next evolution in the web. Web sites and applications return HTML via a URL, and meant to display information to humans in a browser, while APIs return JSON or XML of the same information, but meant to be used in other applications and systems. API security is designed to allow access to our digital resources using the web, while also securing it in a way to ensure only the intended audience is able to obtain access. APIs are designed to securely provide access to data, content, media, and algorithms using the same web that us humans use to access information online via our browsers. Access Using Secure URLs APIs use web URLs get read and write data, content, media, and to allow engagement with algorithms. If you want a list of press releases, you visit https://api.example.com/press/. If you want a list of contacts from the CRM, you visit https://api.example.com/contacts/. The URL for all API resources should be encrypted by default, protecting all requests and responses in transit. Providing the first layer of security for APIs, ensuring only approved consumers can view data, content, media, and valuable algorithms being transmitted online. Registration Always Required For APIs A common misconception about web APIs is that they are all like Twitter, and are publicly...[<a href="/2018/03/06/explaining-api-security-to-organizational-leadership/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/"><img src="https://s3.amazonaws.com/kinlane-productions2/capital-one/capital-one-banking-home-page.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/">Capital One DevExchange Provides An Important Banking API Blueprint</a></h3>
			<p><em>06 Mar 2018</em></p>
			<p>When you take a look at the banking API landscape in the United States, there is one clear leader in the game–Capital One. Their DevExchange program is miles ahead of every one of their competitors, giving them a significant head start when it comes to the banking API economy. Their approach to delivering APIs meets all of my minimum requirements for any successful API platform, and even exceeds it, providing what I’d consider to be a leading example blueprint that all banking API providers should be following. The Capital One DevExchange begins as any API operation should, with a dedicated portal located at developer.[domain]: developer.capitalone.com After landing on the home page for the Capital One DevExchange you get everything you need to get up and running with the APIs they have: Getting started Authentication Documentation Errors Login Registration The Capital One DevExchange provides four main groups of public APIs currently, started with access to account information in the following areas: Retrieve account products - /deposits/account-products (GET) Retrieve account product details - /deposits/account-products/{productId} (GET) Create new account application - /deposits/account-applications (POST) Retrieve out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (GET) Answer out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (PUT) Retrieve account application details - /deposits/account-applications/{applicationId} (GET) As well as some credit card offers, showcasing the products they have available: Retrieve product listings - /credit-offers/products (GET) Retrieve card products - /credit-offers/products/cards (GET) Retrieve card products - /credit-offers/products/cards (GET) Retrieve card products by type - /credit-offers/products/cards/{cardType} (GET) Retrieve card product details - /credit-offers/products/cards/{cardType}/{productId} (GET) Which you can actually sign up for and do a pre-qualification via APIs: Create prequalification check - /credit-offers/prequalifications (POST) Create prequalification acknowledgment - /credit-offers/prequalifications/{prequalificationId} (POST) Create applicant key - /credit-offers/applicant-details POST Get access to Capital One rewards via APIs: Retrieve rewards accounts - /rewards/accounts (GET) Retrieve rewards account details - /rewards/accounts/{rewardsAccountReferenceId} (GET) And details about merchants involved in transactions: Retrieve merchant data - /merchant-insights/merchants (GET) Refresh merchant details - /merchant-insights/merchants/{merchantId} (GET) This version of the API...[<a href="/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/docks_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud/">The Need for Standardized API Plans and Pricing to Compete with Cloud</a></h3>
			<p><em>05 Mar 2018</em></p>
			<p>Google launched their Cloud Billing Catalog API, providing access to thee pricing for their cloud API catalog the other day. Azure has their billing API, and AWS has their cost explorer API service, showing that programmatic access to what API resources cost, as well as management of usage, billing, invoicing, and other aspects of doing business with APIs is becoming the normal mode of operating an API platform. I’ve long used AWS, Google, and increasingly Azure as a blueprint for what us smaller API providers should be doing. They are full of positive and negative lessons for any API provider. However, I’m starting to see what they are doing as not just a blueprint, but potentially something that will force many of us API providers out of businesses if we cannot emulate what they are doing at scale. The tractor beam that is the cloud providers is strong. They bring a lot of benefits to the table. So much so, it is getting harder and harder for independent API providers to compete. Offering benefits to consumers that will become deal breakers with using other 3rd party API providers services, pushing API consumers to stay within their chosen cloud platform walled garden. As a developer, if I can programmatically manage the plans, pricing, and billing for ALL the APIs I use via AWS, Google, and Azure, but I have to manually manage this across many different 3rd party providers, I am going to be hesitant when it comes to adopting any new services that aren’t within my cloud domain. As I depend on more APIs, the benefits of being able to programmatically manage the business of my API consumption is becoming increasingly critical. If the individual 3rd party API providers I use don’t begin to offer APIs for managing the business of my API consumption, and adopting a standardized interface across all the APIs I depend on, I’m going to favor my cloud native API solutions...[<a href="/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/death-valley-national-park_dali_three_just_road.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and/">Serverless, Like Microservices Is About Understanding Our Dependencies And</a></h3>
			<p><em>05 Mar 2018</em></p>
			<p>I am not buying into all the hype around the recent serverless evolution in compute. However, like most trends I study, I am seeing some interesting aspects of how people are doing it, and some positive outcomes for teams who are doing it sensibly. I am not going all in on serverless when it comes to deploying or integrating with APIs, but I am using it for some projects, when it makes sense. I find AWS Lambda to be a great way to get in between the AWS API Gateway and backend resources, in order to conduct a little transformation. Keeping serverless as just one of many tools in my API deployment and integration toolbox, yet never going overboard with any single solution. I put serverless into the same section of my toolbox as I do microservices. Serverless is all about decoupling how I deploy my APIs, helping me keep things doing small, meaningful things behind each of my API paths. Serverless forces me to think through how I am decoupling my backend, and pushes me to identify dependencies, acknowledge constraints, and get more creative in how I write code in this environment. To do one thing, and do it well, I need an intimate understanding of where my data and other backend resources are, and how I can distill a unit of compute down to the smallest unit as I possibly can. Identifying, reducing, and being honest about what my dependencies are is essential to serverless working or not working for me. The challenge I’m having with serverless at the moment is that it can be easy to ignore many of the benefits I get from dependencies, while just assuming all dependencies are bad. Specifically around the frameworks I deploy as part of my API solutions. When coupled with AWS API Gateway this isn’t much of a problem, but all by itself, AWS Lambda doesn’t have all fo the other HTTP, transformation, header, request,...[<a href="/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks-when-it-comes-to/"><img src="https://s3.amazonaws.com/kinlane-productions2/legos/lego-millenium-falcon-instructions.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks-when-it-comes-to/">People Seem to Want Lego Kits and Not a Bucket of Lego Blocks When It Comes To</a></h3>
			<p><em>05 Mar 2018</em></p>
			<p>While I wish everyone saw the modular potential of APIs and microservices like I do, I’ve come to the realization that most people are just interested in ready to go kits that walk them through every detail of doing APIs, rather than actually playing, learning, evolving, and learning to be productive with a big bucket of APIs. I’m not just focusing on business users here, I am talking about a significant portion of the developers I come across, who really don’t seem that interested in learning to apply API concepts, and understanding when and where to use them, they just want a set of instructions that walk them through each step of deploying an API. I actually am a proponent of there being more boxed, lego style API kits that teach you how to build the product API, task API, press release API, and other common implementations. Robust, detailed, ready-to-go API implementations that would walk people through each and every step of defining, designing, deploying, managing, monitoring, testing, and documenting their API. I feel like this would significantly help folks think through what are healthy API practices, and be introduced to different ways of thinking around APIs. However, I do not want people to become reliant on only being able to operate within this paradigm, and not actually be able to fix, evolve, and deliver their own custom API solutions, using the healthy practices they are being introduced to. People seem to just want shortcuts, and things done for them. People want the solutions packaged and delivered to their doorstep. They seem unable to be able to find the solutions on their own, or even be able to absorb a lesson delivered via a packaged solution. I’m not sure what the cure for this condition is. It is hard to tell whether it is vendor induced, or (would be) API provider induced. Have people been conditioned by vendors to just be spoon fed solutions? Or...[<a href="/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks-when-it-comes-to/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/05/an-openapidriven-api-governance-rules-engine/"><img src="https://s3.amazonaws.com/kinlane-productions2/rules/9968073905_95ce575233_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/05/an-openapidriven-api-governance-rules-engine/">An OpenAPI-Driven, API Governance Rules Engine</a></h3>
			<p><em>05 Mar 2018</em></p>
			<p>Phil Sturgeon (@philsturgeon) alerted me to a pretty cool project he is cooking up, called Speccy. Which provides a rules engine for validating your OpenAPI definitions. “Taking off from where Mike Ralphson started with linting in swagger2openapi, Speccy aims to become the rubocop or eslint of OpenAPI”, and to “sniff your files for potentially bad things. “Bad” is objective, but you’ll see validation errors, along with special rules for making your APIs better.” Helping make sure your API definitions are as consistent as they possibly can be, and deliver on your API governance strategy (you have one right?) With Speccy, there are a default set of rules, things like ensuring you have a summary or a description for each API path: { "name": "operation-summary-or-description", "object": "operation", "enabled": true, "description": "operation should have summary or description", "or": ["summary", "description"] } Or making sure you add descriptions to your parameters: { "name": "parameter-description", "object": "parameter", "enabled": true, "description": "parameter objects should have a description", "truthy": "description" } Or making sure you include tags for each aPI path: { "name": "operation-tags", "object": "operation", "enabled": true, "description": "operation should have non-empty tags array", "truthy": "tags", "skip": "isCallback" } Then you can get more strict by requiring contact information: { "name": "contact-properties", "object": "contact", "enabled": true, "description": "contact object should have name, url and email", "truthy": [ "name", "url", "email" ]&lt;br /&gt; } And make sure youi have a license applied to your API: { "name": "license-url", "object": "license", "enabled": true, "description": "license object should include url", "truthy": "url" } Speccy is available as a Node package, which you can easily run at the command line. Speccy is definitely what is needed out there right now, helping us validate the growing number of OpenAPI definitions in our life. As many companies are thinking about how they can apply API governance across their operations, they should be looking at contributing to Speccy. It is something I’ve been talking with API service...[<a href="/2018/03/05/an-openapidriven-api-governance-rules-engine/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/"><img src="https://s3.amazonaws.com/kinlane-productions2/schema-org/schema-org.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/">Thoughts On The Schema.Org WebAPI Type Extension</a></h3>
			<p><em>02 Mar 2018</em></p>
			<p>I&rsquo;m putting some thought into the Schema.Org WebAPI Type Extension proposal by Mike Ralphson (Mermade Software) and Ivan Goncharov (APIs.guru), to &ldquo;facilitate better automatic discovery of WebAPIs and associated machine and human-readable documentation&rdquo;. It&rsquo;s an interesting evolution in how we define APIs, in terms of API discovery, but I would also add potentially at &ldquo;execute time&rdquo;. Here is what a base WebAPI type schema could look like: Then the proposed extensions could include the following: versions (OPTIONAL array of thing -&gt; Property -&gt; softwareVersion). It is RECOMMENDED that APIs be versioned using [semver] entryPoints (OPTIONAL array of Thing -&gt; Intangible -&gt; EntryPoint) license (OPTIONAL, CreativeWork or URL) - the license for the design/signature of the API transport (enumerated Text: HTTP, HTTPS, SMTP, MQTT, WS, WSS etc) apiProtocol (OPTIONAL, enumerated Text: SOAP, GraphQL, gRPC, Hydra, JSON API, XML-RPC, JSON-RPC etc) webApiDefinitions (OPTIONAL array of EntryPoints) containing links to machine-readable API definitions webApiActions (OPTIONAL array of potential Actions) The webApiDefinitions (EntryPoint) contentType property contains a reference to one of the following conten types: OpenAPI / Swagger in JSON - application/openapi+json or application/x-openapi+json OpenAPI / Swagger in YAML - application/openapi RAML - application/raml+yaml API Blueprint in markdown - text/vnd.apiblueprint API Blueprint parsed in JSON - application/vnd.refract.parse-result+json API Blueprint parsed in YAML - application/vnd.refract.parse-result+yaml Then the webApiActions property brings a handful of actions to the table, with the following being suggested: apiAuthentication - Links to a resource detailing authentication requirements. Note this is a human-readable resource, not an authentication endpoint apiClientRegistration - Links to a resource where a client may register to use the API apiConsole - Links to an interactive console where API calls may be tested apiPayment - Links to a resource detailing pricing details of the API I fully support extending the Schema.org WebAPI vocabulary in this way. It adds all the bindings needed to make the WebAPI type executable at runtime, as well as it states at discovery time. I like the transport and protocol...[<a href="/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/vancouver_diego_rivera1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/">An Observable Industry Level Directory Of API Providers And Consumers</a></h3>
			<p><em>01 Mar 2018</em></p>
			<p>I’ve been breaking down the work on banking APIs coming out of Open Banking in the UK lately. I recently took all their OpenAPI definitions and published as a demo API developer portal. Bringing the definitions out of the shadows a little bit, and showing was is possible with the specification. Pushing the project forward some more today I published the Open Banking API Directory specification to the project, showing the surface area of the very interesting, and important component of open banking APIs in the UK. The Open Banking Directory provides a pretty complete, albeit rough and technical approach to delivering observability for the UK banking industry API ecosystem actor layer. Everyone involved in the banking API ecosystem in UK has to be registered in the directory. It provides profiles of the banks, as well as any third party players. It really provides an unprecedented, industry level look at how you can make API ecosystems more transparent and observable. This thing doesn’t exist at the startup level because nobody wants to be open with the number of developers, or much else regarding the operation of their APIs. Making any single, or industry level API ecosystem, operate as black boxes–even if they claim to be an “open API”. Could you imagine if API providers didn’t handle their own API management layer, and an industry level organization would handle the registration, certification, directory, and dispute resolution between API providers and API consumers? Could you imagine if we could see the entire directory of Facebook and Twitter developers, understand what businesses and individuals were behind the bots and other applications? Imagine if API providers couldn’t lie about the number of active developers, and we knew how many different APIs each application developers used? And it was all public data? An entirely different API landscape would exist, with entirely different incentive models around providing and consuming gAPIs. The Open Banking Directory is an interesting precedent. It’s not just...[<a href="/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/4882162452_fa3126b38d_b_spagetti_accident.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">What We Need To Be Machine Readable At API Run Time</a></h3>
			<p><em>28 Feb 2018</em></p>
			<p>I had breakfast with Mike Amundsen (@mamund) and Matt McLarty (@MattMcLartyBC) of the CA API Academy team this morning in midtown this morning. As we were sharing stories of what each other was working on, the topic of what is needed to execute an API call came up. Not the time consuming find an API, sign up for an account, figure out the terms of service and pricing version, but all of this condensed into something that can happen in a split second within applications and systems. How do we distill down the essential ingredients of API consumption into a single, machine readable unit that can be automated into what Mike Amundsen calls, “find and bind”. This is something I’ve been thinking a lot about lately as I work on my API discovery research, and there are a handful of elements that need to be present: Authentication - Having keys to be able to authentication. Surface Area - What is the host, base url, path, headers, and parameters for a request. Terms of Service - What are the legal terms of service for consumption. Pricing - How much does each API request cost me? We need these elements to be machine readable and easily accessible at discover and runtime. Currently the surface area of the API can be described using OpenAPI, that isn’t a problem. The authentication details can be included in this, but it means you already have to have an application setup, with keys. It doesn’t include new users into the equation, meaning, discovering, registering, and obtaining keys. I have a draft specification I call “API plans” for the pricing portion of it, but it is something that still needs a lot of work. So, in short, we are nowhere near having this layer ready for automation–which we will need to scale all of this API stuff. This is all stuff I’ve been beating a drum about for years, and I anticipate it...[<a href="/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/28/the-business-of-running-government-as-a-microservices-platform/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/capital-battle_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/28/the-business-of-running-government-as-a-microservices-platform/">The Business of Running Government As A Microservices Platform</a></h3>
			<p><em>28 Feb 2018</em></p>
			<p>I recently wrote a response to a recent Department of Veterans Affairs RFI which contained a section about the business of operating government as a microservices platform.I know that many folks wouldn’t make it that far in the 10K word response, so I wanted to break it out into its own post. I feel pretty strongly about the potential of decoupling how we deliver technology across government, but for this to be successful we are also going to have to decouple the business and politics of it all as well. This post reflects my current research and thinking about the business of APIs in government, and is part of some ongoing work I am doing around API management, public data, and how we begin to think differently about how government engages with the public in a digital age. There are many interpretations of what is a microservice, but for the purposes of this post, it is a simple set of APIs that meet one precise set of government services. The API definition, database, back-end code, management layer, documentation, support and all other essential elements are self-contained, and usually stored in a single Github, or Bitbucket repository, when delivering microservices. Each microservice possesses its own technical, business, and political contract, outline how the service will be delivered, managed, supported, communicated, and versioned. These contracts can be realized individually, or grouped together as a larger, aggregate contract that can be submitted, while still allowing each individual service within that contract to operate independently. Decoupling The Business Of Delivering Government Digital Services The microservices approach isn’t just about the technical components. It is about making the business of delivering vital government services more modular, portable, and scalable. Something that will also decouple and shift the politics of delivering critical services to veterans. Breaking things down into much more manageable chunks that can move forward independently at the contract level. Helping both simplify, and streamline the deliver of services...[<a href="/2018/02/28/the-business-of-running-government-as-a-microservices-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/28/an-open-banking-api-portal-blueprint/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-banking/open-banking-accounts-documentation-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/28/an-open-banking-api-portal-blueprint/">An Open Banking API Portal Blueprint</a></h3>
			<p><em>28 Feb 2018</em></p>
			<p>I have been learning all about the banking API efforts out of Open Banking in the UK lately. They are evolving a set of read / write account and transaction API, as well as public data APIs for some of the common information 3rd party developers are looking to get their hands on. I'm intrigued with the traction the organization has gotten, and I want to be able to fully understand what they are developing, as well help contribute where I can. To help me understand the API specification, as well as hopefully contribute to the conversation, I am publishing an blueprint API portal for the API. It is a demo portal, running on Github, which uses the API Evangelist graphical look, but I am also publishing documentation for v1.1.1 of the account and payments API, as well as v2.1 of the public data APIs. I'm looking to publish the API specification like any bank would, but it won't actually be a live API--yet. I'd like to turn it into a mock API, with some virtualized data to demonstrate what is possible. I've only had time to publish the overview of the project, and the documentation for each current version. I have a todo list of things I would like to invest in when I have more time. Eventually, I want it to be a complete, forkable Open Banking API portal that any bank in the UK could publish. Then I'm looking to create country specific versions to help push French, German, and other banks to push a portal. It doesn't have to be my solution that the banks use, but hopefully they'll at least use what I have provided as a blueprint. The goal isn't just to get them to use the portal, it is to get them implementing their bank's API developer portal in a standardized way--similar to the API specification from Open Banking, but this is the portal specification. I will spend...[<a href="/2018/02/28/an-open-banking-api-portal-blueprint/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/27/what-is-the-streamdata-io-api-gallery/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/27/what-is-the-streamdata-io-api-gallery/">What Is The Streamdata.io API Gallery?</a></h3>
			<p><em>27 Feb 2018</em></p>
			<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation. What is the Streamdata.io API Gallery? It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas. What are we looking to accomplish with the Streamdata.io API Gallery? Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible. What is the opportunity around the Streamdata.io API Gallery? Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow. Why is this relevant to Streamdata.io, and their road map? It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which...[<a href="/2018/02/27/what-is-the-streamdata-io-api-gallery/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_146_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting/">Three Areas I Would Like To Cover When We Sit Down For An API Consulting</a></h3>
			<p><em>27 Feb 2018</em></p>
			<p>I’m putting together some presentations for a handful of upcoming engagements, where I’m wanting to help my audience understand what an initial engagement will look like. While I am looking to have just a handful of bullets that can live on a single, or handful of slides, I also want a richer narrative to go along with it. To achieve this I rely on my blog, which helps me work my way through the details of what I do, and distill things down into something that I can deliver on the ground within the companies, organizations, institutions, and government agencies I am conducting business with. When I am sitting down with a new audience, and working to help them understand how I can help them begin, jumpstart, revive, and move forward with their API journey, I’m usually breaking things into three main areas: Landscape Mapping - Establish a map of what currently is within an organization. Internal Resources - What existing web services, APIs, teams, and resources exist? External Objectives - What are the external objectives of doing APIs? Strategy Development - Craft a coherent strategy for moving forward with APIs. API Lifecycle - Lay out a step by step list of stops along a modern API life cycle. API Support - Identify how the strategy and operations will be supported within an organization. API Evangelism - Consider how the message around API operations will spread internally, and externally. Execution - Identify a clear set of next steps regarding how APIs will evolve. Infrastructure - What services, tooling, and other API infrastructure is needed? Resources - What resources have been identified for moving the API conversation forward? Governance - What is the governance strategy for measuring, reporting upon, and enforcing the deliver of APIs across the API lifecycle presented. When I present to a new group of people within an organization, this is the outline I am looking to flesh out. I have to understand...[<a href="/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/27/mapping-out-the-api-landscape/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/27/mapping-out-the-api-landscape/">Mapping Out The API Landscape</a></h3>
			<p><em>27 Feb 2018</em></p>
			<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation. What is the Streamdata.io API Gallery? It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas. What are we looking to accomplish with the Streamdata.io API Gallery? Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible. What is the opportunity around the Streamdata.io API Gallery? Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow. Why is this relevant to Streamdata.io, and their road map? It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which...[<a href="/2018/02/27/mapping-out-the-api-landscape/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/26/the-banking-api-actors-in-the-uk/"><img src="https://s3.amazonaws.com/kinlane-productions2/shakespeare/shakespeare.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/26/the-banking-api-actors-in-the-uk/">The Banking API Actors In The UK</a></h3>
			<p><em>26 Feb 2018</em></p>
			<p>I’ve been profiling the work of the Open Banking Implementation Entity when nit comes to banking API standards in the UK. As part of my getting up to speed on the banking ecosystem in the UK, and Europe, I’ve been posting a series of small blog posts, outlining different aspects of how things work, and who the players are. While going through the Open Banking documentation, I came across a great list of the “actors” int he Open Banking API ecosystem, which taught me a lot about who is involved, and was worth reposting here as a list. The Open Banking eco-system consists of a number of actors, which may be a natural person or an entity: Payment Service User (PSU) - Person - Payment Services User is a natural or legal person making use of a payment service as a payee, payer or both Payment Service Provider (PSP) - Legal Entity - A legal entity (and some natural persons) that provide payment services as defined by PSD2 Article 4(11) Account Servicing Payment Service Provider (ASPSP) - Legal Entity - Account Servicing Payment Service Providers provide and maintain a payment account for a payer as defined by the PSRs and, in the context of the Open Banking Ecosystem are entities that publish Read/Write APIs to permit, with customer consent, payments initiated by third party providers and/or make their customers’ account transaction data available to third party providers via their API end points. Third Party Providers / Trusted Third Parties (TPP) - Legal Entity - Third Party Providers are organisations or natural persons that use APIs developed to Standards to access customer’s accounts, in order to provide account information services and/or to initiate payments. Third Party Providers are either/both Payment Initiation Service Providers (PISPs) and/or Account Information Service Providers (AISPs). Payment Initiation Service Provider (PISP) - Legal Entity - A Payment Initiation Services Provider provides an online service to initiate a payment order at the request...[<a href="/2018/02/26/the-banking-api-actors-in-the-uk/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/"><img src="https://s3.amazonaws.com/kinlane-productions2/starling/starling-home-page.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/">One Of The Best API Getting Started I Have Come Across</a></h3>
			<p><em>26 Feb 2018</em></p>
			<p>
I’m working my way through banking and Fintech companies in the UK, and I stumbled across the Starling banking API. I began doing my usual clicking around as I do with any API, looking at the documentation, the getting started, and other primary links. After landing on the Starling getting started page, I have to say that it is the single best example of a getting started page I have ever come across in my time as API Evangelist. It is robust, informative, well laid out, and has everything you need to well, get started.

The Starling getting started page is broken up into six separate sections:

1) Register Your Application
2) Setup Starter Kit
3) Play in the Sandbox
4) Personal Access
5) Going Live
6) Contact Us

Each getting started section has a simple, concise description with relevant visuals and code samples, as well as possession simple action buttons, like sign, login, register application, and the other meaningful things you need to get started. The Starling getting started is going to become my go to example of how to create an API getting started page. You can really tell whoever put it together spent a lot of time refining it, and walking through it until it was 100% complete.

Starling even has a sandbox, marketplace, and a join Slack button. I can’t rave about their approach enough. I’m going to turn it into a case study regarding how to create a getting started page, and showcase on the home page of the site. I wish every API put as much energy into their getting started page as Starling has. It would take the friction out of on-boarding APis, and make it a much more pleasant experience.

[<a href="/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/24/round-two-of-the-department-of-veterans-affairs-lighthouse-platform-rfi/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_113_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/24/round-two-of-the-department-of-veterans-affairs-lighthouse-platform-rfi/">Round Two Of The Department of Veterans Affairs Lighthouse Platform RFI</a></h3>
			<p><em>24 Feb 2018</em></p>
			<p>I’m spending some more time thinking about APIs at the Department of Veterans Affairs (VA), in response to round two of their request for information (RFI). A couple months back I had responded to an earlier RFI, providing as much information as I could think of, for consideration as part of their API journey. As a former VA employee, and son of two Vietnam Vets (yes two), you can say I’m always willing to invest some in APIs over at the VA. To provide a response, I have taken the main questions they asked, broken them out here, and provided answers to the best of my ability. In my style, the answers are just free form rants, based upon my knowledge of the VA, and the wider API space. It is up to the VA, to decide what is relevant to them, and should be included in their agency API strategy. 2. Current Scope While the acquisition strategy for Lighthouse has not yet been formalized, VA envisions that the program will consist of multiple contracts. For example, a contract for recommending policy and standards to form governance would likely be separate from an API build team. The key high level activities below are anticipated to be included within these contracts, and VA is requesting feedback from industry on how these activities should be aligned between multiple contracts. The list below is not inclusive of all tasks required to support this program. Additionally, VA intends to provide the IAM solution and the provisioning of necessary cloud resources to host the proposed technology stack. VA’s current enterprise cloud providers are Microsoft Azure and Amazon Web Services. Microservice Focused Operational &amp; Implementation Lighthouse should embrace a microservices way of doing things, so that the platform can avoid legacy trappings when it comes to delivering software at the VA, which have resulted in large, monolithic systems, possessing enormous budgets, and entrenched teams, that are able to develop a resistance...[<a href="/2018/02/24/round-two-of-the-department-of-veterans-affairs-lighthouse-platform-rfi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/22/aws-iamlike-policies-for-aws-api-gateway-and-marketplace-billing/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/server-cloud1_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/22/aws-iamlike-policies-for-aws-api-gateway-and-marketplace-billing/">AWS IAM-Like Policies For AWS API Gateway And Marketplace Billing</a></h3>
			<p><em>22 Feb 2018</em></p>
			<p>The primary reason I’ve been adopting more AWS solutions as part of my API stack, and using tools I have historically felt lock me into the AWS ecosystem, is the available of AWS identity and access management (IAM). I just cannot deliver secure at this level as a small business owner, and their robust solution lets me dial in exactly what I need when it comes to defining who has access to what across my API infrastructure. I can define different policies, and apply them at the API management layer using both AWS Lambda and AWS API Gateway. Keeping everything separated, yet with a single API stack as the point of entry, for all consumers and applications. I want all of this security goodness, but for the business of my APIs. Similar to the engine that drives the relationship between me as an AWS Marketplace user and AWS, I want a framework for applying business policies at the plan level within AWS API Gateway. I want to determine who has access to which resources, as well as what they can use, but I want to be able to meter this usage, and charge different rates. Compute, storage, and bandwidth for my partners is different than for retail API consumers, with a mix of resource and API call based metrics. The AWS monetization policies would reflect the AWS Marketplace framework, giving me a mix of metering and contract based billing, reflecting single or multi-dimensional usage across the eight areas of consumption they support currently. I want to be able to establish common monetization policies across all my microservices, and allow product managers to implement them consistently at scale using AWS API Gateway. Like security, these API product managers shouldn’t be experts in the economics of the services being offered, they should just be able to apply from a common pool of business policies, and provide feedback on how to evolve, when appropriate. This concept is very...[<a href="/2018/02/22/aws-iamlike-policies-for-aws-api-gateway-and-marketplace-billing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/21/what-is-open-banking-in-the-uk/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-banking/open-banking-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/21/what-is-open-banking-in-the-uk/">What Is Open Banking In The UK?</a></h3>
			<p><em>21 Feb 2018</em></p>
			<p>I am profiling banks in the UK as part of an effort move forward my API Stack work, and populate the Streamdata.io API Gallery. One significant advantage that banks in the UK have over other countries in the EU, and even in the US, is the help of Open Banking. To help profile the organization, I’ll just borrow from their website to define who they are and what they do. The Open Banking Implementation Entity was created by the UK’s Competition and Markets Authority to create software standards and industry guidelines that drive competition and innovation in UK retail banking. In 2016, The Competition and Markets Authority (CMA) published a report on the UK’s retail banking market which stated that older, larger banks do not have to compete hard enough for customers’ business, and smaller and newer banks were finding it difficult to grow and access the UK banking market. To solve this problem, they proposed a number of remedies including Open Banking, which defines API standards that are intended to help level that playing field. The role of Open Banking is to: Design the specifications for the Application Programming Interfaces (APIs) that banks and building societies use to securely provide Open Banking Support regulated third party providers and banks and building societies to use the Open Banking standards Create security and messaging standards Manage the Open Banking Directory which allows regulated participants like banks, building societies and third party providers to enroll in Open Banking Produce guidelines for participants in the Open Banking ecosystem Set out the process for managing disputes and complaints This approach to standardizing API definitions is the type of leadership that is needed to move API conversation forward in ALL industries. I know in the US, many enjoy viewing regulations as always bad, but this type of organizational designation can go a long way towards moving an industry forward in a concerted fashion. Doing the hard work to establish a...[<a href="/2018/02/21/what-is-open-banking-in-the-uk/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/"><img src="https://s3.amazonaws.com/kinlane-productions2/sabre/sabre-travel-signup-keys.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/">Provisioning A Default App And Keys For Your API Consumers On Signup</a></h3>
			<p><em>21 Feb 2018</em></p>
			<p>
I sign up for a lot of APIs. I love anything that reduces friction when on-boarding, and allows me to begin making an API call in 1-3 clicks. I’m a big fan of API providers that allow me to signup using my Github OAuth, preventing me from having to sign up for yet another account. I’m also a big fan of providers who automatically provision an application for me as part of the signup, and have my API keys waiting for me as soon as I’ve registered.

While signing up for the Sabre travel API I saw that they provisioned my application as part of the API sign up process in a way that was worth showcasing. Saving me the time and hassle of having to add a new application after I’ve signed up. Stuff like this might seem like a pretty small detail when developing an API on-boarding process, but when you are signing up for many different APIs, and trying to manage your time–these little details add up to be a significant time saver.

Ideally, API providers would auto-provision a default application along with the signup, but I like the idea of also giving me the option to name my application while registering. When crafting your API registration flow, make sure you spend time signing up multiple times, and try to put yourself in your API consumers shoes. I even recommend signing up for an account each week, repeatedly experiencing what your consumers will be exposed to. I also recommend spending time signing up for other APIs on a regular basis, to experience what they offer–you will always surprised by what I find.

[<a href="/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/enums/bitcoin-pools.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/">An Opportunity Around Providing A Common OpenAPI Enum Catalog</a></h3>
			<p><em>21 Feb 2018</em></p>
			<p>I’m down in the details of the OpenAPI specification lately, working my way through hundreds of OpenAPI definitions, trying to once again make sense of the API landscape at scale. I’m working to prepare as many API path definitions as I possibly can to be runnable within one or two clicks. OpenAPI definitions, and Postman Collections are essential to making this happen, both of which require complete details on the request surface area for an API. I need to know everything about the path, as well as any headers, path, or query parameters that need to included. A significant aspect of this definition being complete includes default, and enum values being present. If I can’t quickly choose from a list of values, or run with a default value, when executing an API, the time to seeing a live response grows significantly. If I have to travel back to the HTML documentation, or worse, do some Googling before I can make an API call, I just went from seconds to potentially minutes or hours before I can see a real world API response. Additionally, if there are many potential values available for each API parameter, enums become critical building blocks to helping me understand all the dimensions of an API’s surface area. Something that should have been considered as part of the API’s design, but often just gets left as part of API documentation. When playing with a Bitcoin API with the following path /blocks/{pool_name}, I need to the list of pools I can choose from. When looking to get a stock market quote from an API with the following path, /stock/{symbol}/quote, I need a list of all the ticker symbols. Having, or not having these enum values at documentation, and execute time, are essential. Many of these lists of values are so common, developers take them for granted. Assuming that API consumers just have them laying around, and really aren’t worth including in documentation. You’d...[<a href="/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/21/an-open-banking-in-the-uk-openapi-template/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-banking/open-banking-openapi.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/21/an-open-banking-in-the-uk-openapi-template/">An Open Banking in the UK OpenAPI Template</a></h3>
			<p><em>21 Feb 2018</em></p>
			<p>After learning more about what Open Banking is doing for APIs in the UK, I realized that I needed an OpenAPI template for the industry specification. There are six distinct schema available as part of the project, and I wanted a complete OpenAPI to describe which paths were available, as well as the underlying response schema. I got work crafting one from the responses that were available within the Open Banking documentation.

Open Banking had schema available for their API definitions, but OpenAPI is the leading API and data specification out there today, so it makes sense that there should be an OpenAPI available, helping all participating banking API providers take advantage of all the tooling available within the OpenAPI community. To help support, I have published my Open Banking OpenAPI definition as a Github Gist:



I’ve applied this OpenAPI definition to the 17 banks they have listed, and will be including them in the next publishing of my API Stack project. Open Banking provides a common definition that can be used across many banks, and an OpenAPI template allows me to quickly apply the common template to each individual bank. Generating bank specific documentation, SDK and code samples, monitoring, tests, and other client tooling. Helping me put the valuable data being made available via each API to work.

I’d like to see more organizations like Open Banking emerge. I’d also like to help ensure they all make OpenAPI templates available for any API and schema specifications they establish. The API lifecycle is increasingly OpenAPI defined, and when you make your guidance available in the OpenAPI format, you are enabling actors within any industry to quickly get up and running with designing, deploying, managing, testing, monitoring, and almost every other stop along a modern API lifecycle. Increasing the chances of adoption for any API standards you are putting out there.

[<a href="/2018/02/21/an-open-banking-in-the-uk-openapi-template/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/20/relationship-between-openapi-path-summary-tags-and-aysncapi-topics/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_160_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/20/relationship-between-openapi-path-summary-tags-and-aysncapi-topics/">Relationship Between OpenAPI Path, Summary, Tags and AysncAPI Topics</a></h3>
			<p><em>20 Feb 2018</em></p>
			<p>I’m working my way through several hundred OpenAPI definitions that I have forked from APIs.guru, Any API, and have automagically generated from API documentation scrape scripts I have developed over time. Anytime I evolve a new OpenAPI definition, I first make sure the summary, description, and tags are as meaningful as they possibly can. Sadly this work is also constrained by how much time I have to spend with each API, as well as how well designed their API is in the first place. I have a number of APIs that help me enrich this automatically, by mining the API path, applying regular expressions, but often times it takes a manual review to add tags, polish summaries, and make the OpenAPI details as meaningful as I possibly can, in regards to what an API does. As I’m taking a break from this work, I’m studying up on AsyncAPI, trying to get my head around how I can be crafting API definitions for the message-based, event-driven, streaming APIs I’m profiling alongside my regular API research. One of the areas the AsyncAPI team is pushing forward is around the concept of a topic–_“to create a definition that suites most use cases and establish the foundation for community tooling and better interoperability between products when using AsyncAPI.”_ or to elaborate further, “a topic is a string representing where an AsyncAPI can publish or subscribe. For the sake of comparison they are like URLs in a REST API.” Now I’m thinking about the relationships between the API design elements I’m wrestling with in my API definitions, and how the path, summary, and tags reflect what Async is trying to articulate with their topics discussion. {organization}.{group}.{version}.{type}.{resources}.{event} organization - the name of the organization or company. group - the service, team or department in charge of managing the message.. version - the version of the message for the given service. This version number should remain the same unless changes in the...[<a href="/2018/02/20/relationship-between-openapi-path-summary-tags-and-aysncapi-topics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64_116_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/">People Who Provide Enum For Their OpenAPI Definitions Are Good People</a></h3>
			<p><em>20 Feb 2018</em></p>
			<p>I’m processing a significant amount of OpenAPI definitions currently, as well as crafting a number of them from scraped API documentation. After you work with a lot of OpenAPI definitions, aiming to achieve a specific objective, you really get to know which aspects of the OpenAPI are the most meaningful, and helpful when they are complete. I talked about the importance of summary, description, and tags last week, and this week I’d like to highlight how helpful it is when the stewards of OpenAPI definitions include enum values for their parameters, and I think they are just good people. ;-) Enums are simply just a list of potential values for each of the parameters you outline as part of your API definition. So if you have state as a parameter for use in the request of your API, you have a list of the 50 US states as the enum. If you the parameter is color, you have just the color black, because we all know it is the only color(all the colors). ;-) If you provide a parameter that will accept a standard set of inputs, you should consider providing an enum list to help your consumers understand the potential for that parameter. Outlining the dimensions of the parameter in a simple JSON or YAML array of every single possible value. I can’t articulate how many times I have to go looking for a list of values. Sometimes it is present within the description for the OpenAPI, but often times I have to go back to the portal for the API, and follow a link to a page that lists out the values. That is, if an API provider decides to provide this information at all. The thoughtful ones do, the even more thoughtful ones put it in their OpenAPI definitions as enum values. Anytime I come across a list of enums that I can quickly build an array, select, and other common aspects...[<a href="/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/16/using-jekyll-and-openapi-to-evolve-my-api-documentation-and-storytelling/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/68_158_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/16/using-jekyll-and-openapi-to-evolve-my-api-documentation-and-storytelling/">Using Jekyll And OpenAPI To Evolve My API Documentation And Storytelling</a></h3>
			<p><em>16 Feb 2018</em></p>
			<p>I’m reworking my API Stack work as independent sets of Jekyll collections. Historically I just dumped all APIs.json, and OpenAPIs into the central data folder, and grouped them into folders by company name. Now I am breaking them out into tag based collections, using a similar structure. Further evolving how I document and tell stories using each API. I have been published a single OpenAPI for each platform, but now I’m publishing a separate OpenAPI for each API path–we will see where this goes, it might ultimately end up biting me in the ass. I’m doing this because I want to be able to talk about a single API path, and provide a definition that can be viewed, interpreted, and executed against, independent of the other paths–Jekyll+OpenAPI is helping me accomplish this. With each API provider possessing its own APIs.json index, and each API path having its own OpenAPI definition, I’m able to mix up how I document and tell stories around these APIs. I can list them by API provider, or by individual API path. I can filter based upon tags, and provide execute-time links that reference each individual unit of API. I have separate JavaScript functions that can be referenced if the API path is GET, POST, or PUT. I can even inherit other relevant links like API sign up or terms of service as part of its documentation. I can reference all of this as part of larger documentation, or within blog posts, and other pages throughout the website–which will be refreshed whenever I update the OpenAPI definition. If you aren’t familiar with how Jekyll works. It is a static content solution, that allows you do develop collections. You can put CSV, JSON, or YAML into these collections (folders), and they become objects you can reference using Liquid syntax. So if I put Twitter’s APIs.json, and OpenAPI into a folder within my social collection, I can reference as site.social.twitter which is the...[<a href="/2018/02/16/using-jekyll-and-openapi-to-evolve-my-api-documentation-and-storytelling/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/68_113_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using/">Insecurity Around Providing Algorithmic Transparency And Observability Using</a></h3>
			<p><em>16 Feb 2018</em></p>
			<p>I’m working on a ranking API for my partner Streamdata.io to help quantify the efficiencies they bring to the table when you proxy an existing JSON web API using their service. I’m evolving an algorithm they have been using for a while, wrapping it in a new API, and applying it across the APIs I’m profiling as part of my API Stack, and the Streamdata.io API Gallery work. I can pass the ranking API any OpenAPI definition, and it will poll and stream the API for 24 hours, and return a set of scores regarding how real time the API is, and what the efficiency gains are when you use Streamdata.io as a proxy for the API. As I do this work, I find myself thinking more deeply about the role that APIs can play in helping make algorithms more transparent, observable, and accountable. My API ranking algorithm is pretty crude, but honestly it isn’t much different than many other algorithms I’ve seen companies defend as intellectual property and their secret sauce. Streamdata.io is invested in the ranking algorithm and API being as transparent as possible, so that isn’t a problem here, but each step of the process allows me to think through how I can continue to evangelize other algorithm owners to use APIs, to make their algorithms more observable and accountable. In my experience, most of the concerns around keeping algorithms secret stem from individual insecurities, and nothing actually technical, mathematical, or proprietary. The reasons for the insecurities are usually that the algorithm isn’t that mathematically sophisticated (I know mine isn’t), or maybe it is pretty flawed (I know mine is currently), and people just aren’t equipped to admit this (I know I am). I’ve worked for companies who venomously defend their algorithms and refuse to open them up, because in the end they know they aren’t defensible on many levels. The only value the algorithm possesses in these scenarios is secrecy, and...[<a href="/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/15_190_800_500_0_max_0_1_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi/">The Importance of the API Path Summary, Description, and Tags in an OpenAPI</a></h3>
			<p><em>15 Feb 2018</em></p>
			<p>I am creating a lot of OpenAPI definitions right now. Streamdata.io is investing in me pushing forward my API Stack work, where I profile API using OpenAPI, and index their operations using APIs.json. From the resulting indexes, we are building out the Streamdata.io API Gallery, which shows the possibilities of providing streaming APIs on top of existing web APIs available across the landscape. The OpenAPI definitions I’m creating aren’t 100% complete, but they are “good enough” for what we are needing to do with them, and are allowing me to catalog a variety of interesting APIs, and automate the proxying of them using Streamdata.io. I’m finding the most important part of doing this work is making sure there is a rich summary, description, and set of tags for each API. While the actual path, parameters, and security definitions are crucial to programmatically executing the API, the summary, description, and tags are essential so that I can understand what the API does, and make it discoverable. As I list out different areas of my API Stack research, like the financial market data APIs, it is critical that I have a title, and description for each provider, but the summary, description, and tags are what provides the heart of the index for what is possible with each API. When designing an API, as a developer, I tend to just fly through writing summary, descriptions, and tags for my APIs. I’m focused on the technical details, not this “fluff”. However, this represents one of the biggest disconnects in the API lifecycle, where the developer is so absorbed with the technical details, we forget, neglect, or just don’t are to articulate what we are doing to other humans. The summary, description, and tags are the outlines in the API contract we are providing. These details are much more than just the fluff for the API documentation. They actually describe the value being delivered, and allow this value to be...[<a href="/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/15/some-common-features-of-an-api-application-review-process/"><img src="https://s3.amazonaws.com/kinlane-productions2/kelly-taylor-app-approval-tweet.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/15/some-common-features-of-an-api-application-review-process/">Some Common Features Of An API Application Review Process</a></h3>
			<p><em>15 Feb 2018</em></p>
			<p>I received a tweet from my friend Kelly Taylor with USDS, asking for any information regarding establishing an “approve access to production data” for developers. He is working on an OAuth + FHIR implementation for the Centers for Medicare and Medicaid Services (CMS) Blue Button API. Establishing a standard approach for on-boarding developers into a production environment always makes sense, as you don’t want to give access to sensitive information without making sure the company, developer, and application has been thoroughly vetted. As I do with my work, I wanted to think through some of the approaches I’ve come across in my research, and share some tips and best practices. The Blue Button API team has a section published regarding how to get your application approved, but I wanted to see if I can expand on, while also helping share this information with other readers. This is a relevant use case that I see come up regularly in healthcare, financial, education, and other mainstream industries. Virtualization &amp; Sandbox The application approval conversation usually begins with ALL new developers being required to work with a sandboxed set of APIs, only providing production API access to approved developers. This requires having a complete set of virtualized APIs, mimicking exactly what would be used in production, but in a much safer, protected environment. One of the most important aspects of this virtualized environment is that there also needs to be robust sets of virtualized data, providing as much parity regarding what developers will experience when they enter the production environment. The sandbox environment needs to be as robust and reliable as the production, which is a mistake I see made over and over from providers, where the sandbox isn’t reliable, or as functional, and developers never are able to reach production status in a consistent and reliable way. Doing a Background Check Next, as reflected in the Blue Button teams approach, you should be profiling the company and...[<a href="/2018/02/15/some-common-features-of-an-api-application-review-process/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/15/how-big-or-small-is-an-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/31_156_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/15/how-big-or-small-is-an-api/">How Big Or Small Is An API?</a></h3>
			<p><em>15 Feb 2018</em></p>
			<p>I am working to build out the API Gallery for Streamdata.io, profiling a wide variety of APIs for inclusion in the directory, adding to the wealth of APIs that could be streamed using the service. As I work to build the index, I’m faced with the timeless question regarding, what is an API? Not technically what an API does, but what is an API in the context of helping people discover the API they are looking for. Is Twitter an API, or is the Twitter search/tweets path an API? My answer to this question always distills down to a specific API path, or as some call it an API endpoint. Targeting a specific implementation, use case, or value generated by a single API provider. Like most things in the API sector, words are used interchangeably, and depending on how much experience you have in the business, you will have much finer grained definitions about what something is, or isn’t. When I’m talking to the average business user, the Twitter API is the largest possible scope–the entire thing. In the context of API discovery, and helping someone find an API to stream or to solve a specific problem in their world, I’m going to resort to a very precise definition–in this case, it is the specific Twitter API path that will be needed. Depending on my audience, I will zoom out, or zoom in on what constitutes a unit of API. The only consistency I’m looking to deliver is regarding helping people understand, and find what they looking for–I’m not worried about always using the same scope in my definition of what an API is. You can see an example of this in action with the Alpha Vantage market data API I’m currently profiling, and adding to the gallery. Is Alpha Vantage is a single API, or 24 separate APIs? In the context of the Streamdata.io API Gallery, it will be 24 separate APIs. In the...[<a href="/2018/02/15/how-big-or-small-is-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/intrinio/intrinio-app-showcase.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/">A Really Nice API Application Showcase Over At The Intrinio Market Data API</a></h3>
			<p><em>15 Feb 2018</em></p>
			<p>I am profiling financial market data APIs currently, and as I’m doing my work profiling APIs, I’m always on the hunt for interesting elements of their API operations that I can showcase for my readers. While looking at the financial market data API from Intrinio, I found that I really, really like their application showcase, which providers a pretty attractive blueprint for how we can showcase what is being develop on top of our APIs.

The Intrinio application showcase is just clean looking, and has the bells and whistles you’d expect like categories, search, detail or list view, and detail pages providing you all the information you need about the application, and where you can find tutorials, code, and other relevant resources.



Another thing I really like is it isn’t just about web and mobile applications. They have spreadsheet integrations, and help walk you through how to “apply” each type of integration. This is what the application in API means to me. It isn’t always just about finished web, mobile, and device applications. It is about applying the resources available via the programmatic interfaces to some problem you have in your world.

Anyways, the Intrinio application showcase is totally worth profiling as part of my research. It is a great blueprint for other API providers to follow when crafting their own application showcases. This post give me a single URL that I can share with folks, and reference throughout my stories, white papers, guides, and talks. I’d love to see this become the standard for how API providers showcase their applications, keeping things simple, clean, and bringing value to their consumers.

[<a href="/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/14/your-microservices-effort-will-fail-because-you-will-never-decouple-your/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_deep_connections.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/14/your-microservices-effort-will-fail-because-you-will-never-decouple-your/">Your Microservices Effort Will Fail Because You Will Never Decouple Your</a></h3>
			<p><em>14 Feb 2018</em></p>
			<p>I’m regularly surprised by companies who are doing microservices which are failing to see the need the change organizational culture, and that microservices will be some magic voodoo to fix all their legacy technical debt. That simply decoupling and breaking down the technology, without any re-evaluation of the business and politics behind, will fix everything, and set the company, organizations, institution, or government agency on a more positive trajectory. In coming years, we will continue to hear stories about why microservices do not work, from endless waves or groups who were unable to do the hard work to decouple, and reorganize the operations behind the services they provide. The monolith legacy systems I’m seeing targeted are widely seen as purely technology, which is why it is often labeled as technical debt. What is missing from this targeting and labeling is any acknowledgement of the people and decisions behind the monolith. The years of business, political, and cultural investment into the monolith. How will we every unwind, or properly address the monolith, if we do not see the organizational, human, and business aspects of why it exists in the first place? Are we talking about the business decisions that went into creating and perpetuating the monolith? It is highly likely we will be making some of the same decisions with microservices, which could end up being worse than when we made them with a single system. Distributed mess, is often more painful than consolidated mess. I’m seeing endless waves of large organizations mandating that their teams invest in microservices, with no mandating for microteams, microbudgets, microdecisionmaking, or any of the other decoupling needed to make microservices truly work independently. I attach micro as a joke. I really don’t feel micro is the constant that needs applying when it comes to services, or the business and organizational mechanism behind them. However, it is the word du jour, and one that gets at some of the illnesses our...[<a href="/2018/02/14/your-microservices-effort-will-fail-because-you-will-never-decouple-your/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/14/the-growing-importance-of-github-topics-for-your-api-seo/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-topics-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/14/the-growing-importance-of-github-topics-for-your-api-seo/">The Growing Importance of Github Topics For Your API SEO</a></h3>
			<p><em>14 Feb 2018</em></p>
			<p>When you are operating an API, you are always looking for new ways to be discovered. I study this aspect of operating APIs from the flip-side–how do I find new APIs, and stay in tune with what APIs are to? Historically we find APIs using ProgrammableWeb, Google, and Twitter, but increasingly Github is where I find the newest, coolest APIs. I do a lot of searching via Github for API related topics, but increasingly Github topics themselves are becoming more valuable within search engine indexes, making them an easy way to uncover interesting APIs. I was profiling the market data API Alpha Vantage today, and one of the things I always do when I am profiling an API, is I conduct a Google, and then secondarily, a Github search for the APIs name. Interestingly, I found a list of Github Topics while Googling for Alpha Vantage API, uncovering some interesting SDKs, CLI, and other open source solutions that have been built on top of the financial data API. Showing the importance of operating your API on Github, but also working to define a set of standard Github Topic tags across all your projects, and helping encourage your API community to use the same set of tags, so that their projects will surface as well. I consider Github to be the most important tool in an API providers toolbox these days. I know as an API analyst, it is where I learn the most about what is really going on. It is where I find the most meaningful signals that allow me to cut through the noise that exists on Google, Twitter, and other channels. Github isn’t just for code. As I mention regularly, 100% of my work as API Evangelist lives within hundreds of separate Github repositories. Sadly, I don’t spend as much time as I should tagging, and organizing projects into meaningful topic areas, but it is something I’m going to be investing in...[<a href="/2018/02/14/the-growing-importance-of-github-topics-for-your-api-seo/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/14/code-generation-of-openapi-fka-swagger-still-the-prevailing-approach/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/gears-numbers-blue.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/14/code-generation-of-openapi-fka-swagger-still-the-prevailing-approach/">Code Generation Of OpenAPI (fka Swagger) Still The Prevailing Approach</a></h3>
			<p><em>14 Feb 2018</em></p>
			<p>Over 50% of the projects I consult on still generate OpenAPI (fka Swagger) from code, rather then the other way around. When I first begin working with any API development group as an advisor, strategist, or governance architect I always ask, “are you using OpenAPI?” Luckily the answer is almost always yes. The challenge is that most of the time they don’t understand the full scope of how to use OpenAPI, and are still opting for the more costly approach–writing code, then generating OpenAPI from annotations. It has been over five years since Jakub Nesetril(@jakubnesetril) of Apiary first decoupled this way of doing API design first, but clearly we still have a significant amount of work when it comes to API definition and design literacy amongst development groups. When you study where API services and tooling are headed it is clear that API deployment, and the actual writing of code is getting pushed further down in the life cycle. Services like Stoplight.io, and Postman are focusing on enabling a design, mock, document, test, and iterate approach, with API definitions (OpenAPI, Postman, etc) at the core. The actual deployment of API, either using open source frameworks, API gateways, or other method, is coming into the picture more downstream. Progressive API teams are hammering out exactly the API they need without ever writing any code, making sure the API design is dialed in before the more expensive, and often permanent code gets written and sent to production. You will see me hammering on this line of API design first messaging on API Evangelist over the next year. Many developers still see OpenAPI (fka Swagger) about generating API documentation, not as the central contract that is used across every stop along the API lifecycle. Most do not understand that you can mock instead of deploying, and even provide mock data, errors, and other scenarios, allowing you to prototype applications on top of API designs. It will take a...[<a href="/2018/02/14/code-generation-of-openapi-fka-swagger-still-the-prevailing-approach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/14/a-summary-of-aws-api-gateway-as-an-api-deployment-and-management-solution/"><img src="https://s3.amazonaws.com/kinlane-productions2/aws/aws-api-gateway-icon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/14/a-summary-of-aws-api-gateway-as-an-api-deployment-and-management-solution/">A Summary Of AWS API Gateway As An API Deployment and Management Solution</a></h3>
			<p><em>14 Feb 2018</em></p>
			<p>I was providing an overview of Kong, AWS API Gateway, and other solutions for a team I’m advising a couple weeks back. I was just looking to distill down some of the key features, and provide an overview to a large, distributed team. This work lends itself well to publishing here on the blog, so I published an overview of Kong yesterday, and today I wanted to publish the summary of the AWS API Gateway. The API gateway solution from AWS has some overlap with what Kong delivers, but I consider it to be more of an API deployment, as well as an API management gateway. The AWS API Gateway brings API deployment front and center, allowing you to define and deploy APIs that are wired up to your backend (AWS) infrastructure: API Endpoint - a host name of the API. the API endpoint can be edge-optimized or regional, depending on where the majority of your API traffic originates from. You choose a specific endpoint type when creating an API. Backend Endpoint - A backend endpoint is also referred to as an integration endpoint and can be a Lambda function, an HTTP webpage, or an AWS service action, or a mock interface. Swagger / OpenAPI - Using Swagger to import and export API configuration and definitions. Then the gateway brings a wealth of API management features, providing a look at how it has been baked into the cloud now: Accounts - Creation and management of Accounts. Keys - Creation and management of API Keys Certificates - Adding and management of certifications Documentation - Publishing of ApI documentation Domains - Mapping of domains Response - Custom Gateway responses. Models - Management of schema models. Validation - Validation of API requests SDK Generation - Generating of client SDKs Staging - Establishing of stages Tags - Tagging of resources Templates - Mapping template used to transform a payload. Plans - Establishing of different plans for API usage. VPC...[<a href="/2018/02/14/a-summary-of-aws-api-gateway-as-an-api-deployment-and-management-solution/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/13/streaming-and-eventdriven-architecture-represents-maturity-in-the-api-journey/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/downtheline_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/13/streaming-and-eventdriven-architecture-represents-maturity-in-the-api-journey/">Streaming And Event-Driven Architecture Represents Maturity In The API Journey</a></h3>
			<p><em>13 Feb 2018</em></p>
			<p>Working with Streamdata.io has forced a shift in how I see the API landscape. When I started working with their proxy I simply saw it about doing API in real time. I was hesitant because not every API had real time needs, so I viewed what they do as just a single tool in my API toolbox. While Server-Sent Events, and proxying JSON APIs is just one tool in my toolbox, like the rest of the tools in my toolbox it forces me to think through what an API does, and understand where it exists in the landscape, and where the API provider exists in their API journey. Something I’m hoping the API providers are also doing, but I enjoy doing from the outside-in as well. Taking any data, content, media, or algorithm and exposing as an API, is a journey. It is about understanding what that resource is, what it does, and what it means to the provider and the consumer. What this looks like day one, will be different from what it looks like day 365 (hopefully). If done right, you are engaging with consumers, and evolving your definition of the resource, and what is possible when you apply it programmatically through the interfaces you provide. API providers who do this right, are leveraging feedback loops in place with consumers, iterating on their APIs, as well as the resources they provide access to, and improving upon them. Just doing simple web APIs puts you on this journey. As you evolve along this road you will begin to also apply other tools. You might have the need for webhooks to start responding to meaningful events that are beginning to emerge across the API landscape, and start doing the work of defining your event-driven architecture, developing lists of most meaningful topics, and events that are occurring across your evolving API platform. Webhooks provide direct value by pushing data and content to your API consumers, but...[<a href="/2018/02/13/streaming-and-eventdriven-architecture-represents-maturity-in-the-api-journey/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/13/having-a-developer-yourdomain-is-clear-differentiator-in-the-api-game/"><img src="https://s3.amazonaws.com/kinlane-productions2/developer-you-com.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/13/having-a-developer-yourdomain-is-clear-differentiator-in-the-api-game/">Having A Developer.[YourDomain] Is Clear Differentiator In The API Game</a></h3>
			<p><em>13 Feb 2018</em></p>
			<p>I am profiling US, UK, French, and German banks as part of some research I am doing for Streamdata.io. I am profiling how far along in the API journey these banks are, and one clear differentiator for me is whether a bank has a developer.[bankdomain] subdomain setup for their APIs or not. The banks that have a dedicated subdomain for their API operations have a clear lead over those who do not. The domain doesn’t do much all by itself, but it is clear that when a bank can get this decision made, many of the other decisions that need to be made are also happening in tandem. This isn’t unique just to banking. This is something I’ve written about several times over the years, and remains constant after looking at thousands of APIs over the last eight years. When a company’s API presence exists within the help section of their website, the API is almost always secondary to the core business. When a company relies on a 3rd party service for their API and developer presence, it almost always goes dormant after a couple months, showing that APIs are just not a priority within the company. Having a dedicated subdomain, landing page, and set of resources dedicated to doing APIs goes a long way towards ensuring an API program gains the momentum it needs to be successful within an organization, and industry. I know that having a dedicated subdomain for API operations seems like a small thing to many folks. However, it is one of the top symptoms of a successful API in my experience. Making data, content, and algorithms available in a machine readable way for use in other applications by 3rd party via the web is something every company, organization, institution, and government agency should be doing in 2018. It is the next iteration of the web, and is not something that should be a side project. Having a dedicated subdomain demonstrates...[<a href="/2018/02/13/having-a-developer-yourdomain-is-clear-differentiator-in-the-api-game/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/13/aggregating-multiple-github-account-rss-feeds-into-single-json-api-feed/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/octocat-aggregate.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/13/aggregating-multiple-github-account-rss-feeds-into-single-json-api-feed/">Aggregating Multiple Github Account RSS Feeds Into Single JSON API Feed</a></h3>
			<p><em>13 Feb 2018</em></p>
			<p>Github is the number one signal in my API world. The activity that occurs via Github is more important than anything I find across Twitter, Facebook, LinkedIn, and other social channels. Commits to repositories and the other social activity that occurs around coding projects is infinitely more valuable, and telling regarding what a company is up to, than the deliberate social media signals blasted out via other channels is. I’m always working to dial in my monitoring of Github using the Github API, but also via the RSS feeds that are present on the public side of the platform. I feel RSS is often overlooked as an API data source, but I find that RSS is not only alive and well in 2018, it is something that is actively used on many platforms. The problem with RSS for me, is the XML isn’t always conducive to working with in many of my JavaScript enabled applications, and I also tend to want to aggregate, filter, and translate RSS feeds into more meaningful JSON. To help me accomplish this for Github, I crafted a simple PHP RSS aggregator and converter script which I can run in a variety of situations. I published the basic script to Github as a Gist, for easy reference. The simple PHP script just takes an array of Github users, loops through them, pulls their RSS feeds, and then aggregates them into a single array, sorts by date, and then outputs as JSON. It is a pretty crude JSON API, but it provides me with what I need to be able to use these RSS feeds in a variety of other applications. I’m going to be mining the feeds for a variety of signals, including repo and user information, which I can then use within other applications. The best part is this type of data mining doesn’t require a Github API key, and is publicly available, allowing me to scale up much further...[<a href="/2018/02/13/aggregating-multiple-github-account-rss-feeds-into-single-json-api-feed/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/13/a-summary-of-kong-as-an-api-management-solution/"><img src="https://s3.amazonaws.com/kinlane-productions2/kong/get-kong-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/13/a-summary-of-kong-as-an-api-management-solution/">A Summary Of Kong As An API Management Solution</a></h3>
			<p><em>13 Feb 2018</em></p>
			<p>I was breaking down what the API management solution Kong delivers for a customer of mine, and I figured I’d take what I shared via the team portal, and publish here on the blog. It is an easy way for me to create content, and make my consulting work more transparent here on the blog. I am using Kong as part of several healthcare and financial projects currently, and I am actively employing it to ensure customers are properly managing their APIs. I wasn’t the decision maker on any of these projects when it came to choosing the API management layer, I am just the person who is helping standardize how they are using API services and tooling across the API life cycle for these projects. First, Kong is an open source API management solution with an easy to install community edition, and enterprise level support when needed. They provide an admin interface, and developer portal for the API management proxy, but there is also a growing number of community editions like KongDash, and Konga emerging to make it a much more richer ecosystem. And of course, Kong has an API for managing the API management layer, as every API service and tooling provider should have. Now, let’s talk about what Kong does for helping in the deploying of your APIs: API Routing - The API object describes an API that’s being exposed by Kong. Kong needs to know how to retrieve the API when a consumer is calling it from the Proxy port. Each API object must specify some combination of hosts, uris, and methods Consumers - The Consumer object represents a consumer - or a user - of an API. You can either rely on Kong as the primary datastore, or you can map the consumer list with your database to keep consistency between Kong and your existing primary datastore. Certificates - A certificate object represents a public certificate/private key pair for an...[<a href="/2018/02/13/a-summary-of-kong-as-an-api-management-solution/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/12/more-outputs-are-better-when-it-comes-to-establishing-an-api-observability/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/ellisisland_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/12/more-outputs-are-better-when-it-comes-to-establishing-an-api-observability/">More Outputs Are Better When It Comes To Establishing An API Observability</a></h3>
			<p><em>12 Feb 2018</em></p>
			<p>I’ve been evolving an observability ranking for the APIs I track on for a couple years now. I’ve bene using the phrase to describe my API profiling and measurement approach since I first learned about the concept from Stripe. There are many perspectives floating around the space about what observability means in the context of technology, however mine is focused completely on APIs, and is more about communicating with external stakeholders, more than it is just about monitoring of systems. To recap, the Wikipedia definition for observability is: Formally, a system is said to be observable if, for any possible sequence of state and control vectors, the current state can be determined in finite time using only the outputs (this definition is slanted towards the state space representation). Less formally, this means that from the system’s outputs it is possible to determine the behavior of the entire system. If a system is not observable, this means the current values of some of its states cannot be determined through output sensors. Most of the conversations occurring in the tech sector are focused on monitoring operations, and while this is a component of my definition, I lean more heavily on the observing occurring beyond just internal groups, and observability being about helping keep partners, consumers, regulators, and other stakeholders be more aware regarding how complex systems work, or do not work. I feel that observability is critical to the future of algorithms, and making sense of how technology is impacting our world, and APIs will play a critical role in ensuring that the platforms have the external outputs required for delivering meaningful observability. When it comes to quantifying the observability of platforms and algorithms, the more outputs available the better. Everything should have APIs for determining the inputs and outputs of any algorithm, or other system, but there should also be operational level APIs that give insight into the underlying compute, storage, logging, DNS, and other layers...[<a href="/2018/02/12/more-outputs-are-better-when-it-comes-to-establishing-an-api-observability/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/12/labeling-your-high-usage-apis-and-externalizing-api-metrics-within-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/alpha-advantage/alpha-vantage-high-usage-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/12/labeling-your-high-usage-apis-and-externalizing-api-metrics-within-your-api/">Labeling Your High Usage APIs and Externalizing API Metrics Within Your API</a></h3>
			<p><em>12 Feb 2018</em></p>
			<p>I am profiling a number of market data APIs as part of my research with Streamdata.io. As I work my way through the process of profiling APIs I am always looking for other interesting ideas for stories on API Evangelist. One of the things I noticed while profiling Alpha Vantage, was that they highlighted their high usage APIs with prominent, very colorful labels. One of the things I’m working to determine in this round of profiling is how “real time” APIs are, or aren’t, and the high usage label adds another interesting dimension to this work. While reviewing API documentation it is nice to have labels that distinguish APIs from each other. Alpha Vantage has a fairly large number of APIs so it is nice to be able to focus on the ones that are used the most, and are more popular. For example, as part of my profiling I focused on the high usage technical indicator APIs, rather than profiling all of them. I need to be able to prioritize my work, and these labels helped me do that. Providing one example of the benefit that these types of labels can bring to the table. I’m guessing that there are many other time saving aspects of labeling popular APIs, beyond just saving me time. This type of labeling is an interesting way of externalizing API analytics in my opinion. Which is another interesting concept to think about across API operations. How can you take the most meaningful data points across your API management processes, and distill them down, externalize and share them so that your API consumers can benefit from valuable API metrics? In this context, I could see a whole range of labels that could be established, applied to interactive documentation using OpenAPI tags, and made available across API documentation, helping make APIs even more dynamic, and in sync with how they are actually being used, measured, and making an impact on operations....[<a href="/2018/02/12/labeling-your-high-usage-apis-and-externalizing-api-metrics-within-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/12/be-clear-about-your-api-pricing/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/green-gears-matrix.jp" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/12/be-clear-about-your-api-pricing/">Be Clear About Your API Pricing</a></h3>
			<p><em>12 Feb 2018</em></p>
			<p>I’m profiling a large number of APIs right now, and I am ranking APIs based upon how easy or difficult they are to access. Whether or not an API provide has a business model is part of the ranking, and how clearly articulated the access and pricing is around that model is a critical part of my profiling algorithm. The APIs that end up included in the API gallery I’m developing for Streamdata.io, and available as part of my wider API Stack research will all have to possess easy to articulate access levels. Not all of them will be free, but the ones that cost money will have straightforward pricing that can be articulate in a single sentence–something that seems to be elusive with many of the API providers I am profiling. I am regularly confused regarding the myriad of ways in which API providers obfuscate the pricing for their APIs. I’ve long been weary of API providers who don’t have a clear business model, but when they have a pricing page, but bu do not consistently apply it to APIs, I’m just left confounded. I can’t always tell if it is done maliciously, or they just haven’t approached their API through an external lens. If I find a pricing page, and the plans seem reasonable, and I’ve plugged my credit card in, but then I still don’t have access to some APIs, and there is no clear labeling of which APIs I have access to as part of my plan, I just can’t spend the afternoon testing and seeing which APIs return a 403 to understand the landscape. The API service composition, and pricing tiers needs to be coherent and front and center, otherwise I just have to move on. If I can’t communicate what is going on to others, it won’t be included in my work. I do not have a problem with different tiers of access, as long as they are communicated,...[<a href="/2018/02/12/be-clear-about-your-api-pricing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/12/a-dedicated-guest-blogger-program-for-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/intrinio/intrinio-guest-blogger-program.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/12/a-dedicated-guest-blogger-program-for-your-api/">A Dedicated Guest Blogger Program For Your API</a></h3>
			<p><em>12 Feb 2018</em></p>
			<p>I get endless waves of people wanting to “guest post” on API Evangelist. It isn’t something I’m interested in because of the nature of API Evangelist, and that it really is just my own stream of consciousness, and not about selling any particular product or service. However, if you are an API provider, looking for quality content for your blog, having a formal approach to managing guest bloggers might make sense. Sure, you don’t want to accept all the spammy requests that you will get, but with the right process, you could increase the drumbeat around your API, and build relationships with your partners and API consumers. There is an example of this in action at the financial data marketplace Intrinio, with their official blogger program. The blogging program for the platform has a set of established benchmarks defined by the Intrinio team, to establish quality for any post that is accepted as part of the program. What I find really interesting, is that they also offer three months of free access to data feeds for API consumers who publish a post via the platform. “Exceptional” participants in the program may have their free access extended, and ALL participants will receive discounts on paid data access subscriptions via the platforms APIs. This is the type of value exchange I like to see via API platforms. Too many APIs are simple one way streets, paying for GET access to data, content, media, and algorithms. API management shouldn’t be just about about metering one way access and charging for it. Sensible API management should measure value exchange around ALL platform resources, including blog and forum posts, and other activities API providers should be incentivizing via their platforms. This is one of the negative side effects of REST I feel–too much focus on resources, and not about the events that occur around these resources. Something we are beginning to move beyond in an event-driven API landscape. Next, I...[<a href="/2018/02/12/a-dedicated-guest-blogger-program-for-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/06/you-have-to-know-where-all-your-apis-are-before-you-can-deliver-on-api-governance/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/64_185_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/06/you-have-to-know-where-all-your-apis-are-before-you-can-deliver-on-api-governance/">You Have to Know Where All Your APIs Are Before You Can Deliver On API Governance</a></h3>
			<p><em>06 Feb 2018</em></p>
			<p>I wrote an earlier article that basic API design guidelines are your first step towards API governance, but I wanted to introduce another first step you should be taking even before basic API design guides–cataloging all of your APIs. I’m regularly surprised by the number of companies I’m talking with who don’t even know where all of their APIs are. Sometimes, but not always, there is some sort of API directory or catalog in place, but often times it is out of date, and people just aren’t registering their APIs, or following any common approach to delivering APIs within an organization–hence the need for API governance. My recommendation is that even before you start thinking about what your governance will look like, or even mention the word to anyone, you take inventory of what is already happening. Develop an org chart, and begin having conversations. Identify EVERYONE who is developing APIs, and start tracking on how they are doing what they do. Sure, you want to get an inventory of all the APIs each individual or team is developing or operating, but you should also be documenting all the tooling, services, and processes they employ as part of their workflow. Ideally, there is some sort of continuous deployment workflow in place, but this isn’t a reality in many of the organization I work with, so mapping out how things get done is often the first order of business. One of the biggest failures of API governance I see is that the strategy has no plan for how we get from where we are to where we ant to be, it simply focuses on where we want to be. This type of approach contributes significantly to pissing people off right out of the gate, making API governance a lot more difficult. Stop focusing on where you want to be for a moment, and focus on where you are. Build a map of where people are, tools,...[<a href="/2018/02/06/you-have-to-know-where-all-your-apis-are-before-you-can-deliver-on-api-governance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/06/riot-games-regional-api-endpoints/"><img src="https://s3.amazonaws.com/kinlane-productions2/riot-games/riot-games-developer-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/06/riot-games-regional-api-endpoints/">Riot Games Regional API Endpoints</a></h3>
			<p><em>06 Feb 2018</em></p>
			<p>I’m slowly categorizing all the APIs I find who are offering up some sort regional availability as part of their operations. With the easy of deployment using leading cloud services, it is something I am beginning to see more frequently. However, there is still a wide variety of reasons why an API provider will invest in this aspect of their operations, and I’m looking to understand more about what these motivations are. Sometimes it is because they are serving a global audience, and latency kills the experience, but other times I’m seeing it is more about the maturity of the API provider, and they’ve have such a large user base that they are getting more requests to deliver resources closer to home. The most recent API provider I have come across who is offering regional API endpoints is from Riot Games, the makers of League of Legends, who offers twelve separate regions for you to chose from, broken down using a variety of regional subdomains. The Riot Games API provides a wealth of meta data around their games, and while they don’t state their reasons for providing regional APIs, I’m guessing it is to make sure the meta data is localized to whichever country their customers are playing in. Reducing an latency across networks, making the overall gaming and supporting application experience as smooth and seamless as possible. Pretty standard reasons for doing regional APIs, and providing a simple example of how you do this at the DNS level. RIot Games also provides a regional breakdown of the availability of their regional endpoints on their API status page, adding another dimension to the regional API delivery conversation. If you are providing regional APIs, you should be monitoring them, and communicating this to your consumers. This is all pretty standard stuff, but I’m working to document every example of regional APIs I come across as part of my research. I’m considering adding a separate research area...[<a href="/2018/02/06/riot-games-regional-api-endpoints/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/06/keeping-api-schema-simple-for-wider-adoption/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-csv.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/06/keeping-api-schema-simple-for-wider-adoption/">Keeping API Schema Simple For Wider Adoption</a></h3>
			<p><em>06 Feb 2018</em></p>
			<p>One aspect of my talk at APIDays Paris this last week, included a slide about considering to allow API consumers to negotiate CSV responses from our API. Something that would probably NEVER occur to most API providers, and probably would make many even laugh at me. I’m used to it, and don’t care. While not something that every API provider should be considering, depending on the data you are serving up, and who your target API consumer ares, it is something that might make sense. Allowing for the negotiation of CSV responses represents lowering the bar for API consumption, and widening the audience who can put our APIs to work. I was doing more work around public data recently, and was introduced to an interesting look at some lessons from developing open data standards. I’m doing a deep dive into municipal data lately as part of my partnership with Streamdata.io, and I found the lessons they published interesting, and something that reflects my stance on API content negotiation. From the development and maintenance of the API, it quickly became clear that adjusting scripts after every election (and by-election) and website modification, was quickly becoming unsustainable. To address this issue, a simple CSV schema was developed to encourage standardisation of this data from the outset. The schema was designed to be as simple and easy to understand and implement as possible. Comprised of just 21 fields, 7 of which are recommended fields, the schema does not have hierarchical relationships between terms and can be implemented in a single CSV file. By making the standard this simple, we were able to get a number of adopters onboard and outputting their lists of elected representatives on their own open data portals. When it comes to APIs, simplicity rules. The simpler you can make your API, the more impact you will make. Allowing for the negotiation of CSV responses from your API when possible allows API consumers to go...[<a href="/2018/02/06/keeping-api-schema-simple-for-wider-adoption/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/06/consistency-in-branding-across-api-portals/"><img src="https://s3.amazonaws.com/kinlane-productions2/subway/london-underground.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/06/consistency-in-branding-across-api-portals/">Consistency in Branding Across API Portals</a></h3>
			<p><em>06 Feb 2018</em></p>
			<p>I recently watched a BBC documentary about the history of the branding used as part of the London Underground. I’m pretty absorbed lately with using public transit as an analogy for complex API implementations, and moving beyond just using subway maps, I thought the branding strategy for the London Underground provided other important lessons for API providers. The BBC documentary went into great detail regarding how much work was put into standardizing the font, branding, and presentation of information for each London Underground, to help reduce confusion, and help riders get where they needed, and making the city operate more efficiently. As I continue to study the world of API documentation, I think we have so much work ahead of us when it comes to standardizing how we present our API portals. Right now every API portal is different, even often times with multiple portals from the same company–see Amazon Web Services for example. I think we underestimate the damage this has to the overall API experience for consumers, and why we see API documentation like Swagger UI, Slate, and Read the Docs have such an impact. However this is just documentation, and we need this to occur as part of the wider API portal user experience. I’ve seen some standardized open source API portal solutions, and there are a handful of API portal services out there, but there really is no standard for how we deliver, brand, and operate the wider API experience. I have my minimum viable API portal definition, and have been tracking on the common building blocks of API operations for eight years now, but there are no “plug and play” solutions that users can implement, following any single approach. I have the data, and I even have a simple Twitter Bootstrap version of my definition (something I’m upgrading ASAP), but in my experience people get very, very, very hung up on the visual aspects of this conversation, want different visual...[<a href="/2018/02/06/consistency-in-branding-across-api-portals/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/05/the-more-we-know-about-you-the-more-api-access-you-get/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/65_144_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/05/the-more-we-know-about-you-the-more-api-access-you-get/">The More We Know About You The More API Access You Get</a></h3>
			<p><em>05 Feb 2018</em></p>
			<p>I’ve been trash talking APIs that identify me as part of some sort of sales funnel, and automate the decision around whether or not I get access to their API. My beef isn’t with API providers profiling me and making decisions about how much access I get, it is about them limiting profiles making it so I do not get access to their APIs at all. Their narrow definitions of the type of API consumers they are seeking does not include me, even though I have thousands of regular readers of my blog who do fit their profile. In the end, it is their loss, not mine, that they do not let me in, but the topic is still something I feel should be discussed out in the open, hopefully expanding the profile definitions for some API providers who may not have considered the bigger picture. I’ve highlighted the limiting profiling of API consumers that prevent access to APIs, but now I want to talk about how profiling can be sensibly used to limit access to API resources. Healthy API management always has an entry level tier, but what tiers are available after that often depend on a variety of other data points. One thing I see API providers regularly doing is requiring API consumers to provide more detail about who they are and what they are doing with an API. I don’t have any problem with API providers doing this, making educated and informed decisions regarding who an API consumer is or isn’t. As the API Evangelist I am happy to share more data points about me to get more access. I don’t necessarily want to do this to sign up for your entry level access tier, just so I can kick the tires, but if I’m needing deeper access, I am happy to fill our a fuller profile of myself, and what I am working on. Stay out of my way when it...[<a href="/2018/02/05/the-more-we-know-about-you-the-more-api-access-you-get/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/05/learning-about-the-headers-used-for-grpc-over-http2/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-grpc.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/05/learning-about-the-headers-used-for-grpc-over-http2/">Learning About The Headers Used for gRPC over HTTP/2</a></h3>
			<p><em>05 Feb 2018</em></p>
			<p>I am learning more about gRPC and HTTP/2, as part of the recent expansion of my API toolbox. I’m not a huge fan of Protocol Buffers, however I do get the performance gain they introduce, but I am very interested in learning more about how HTTP/2 is being used as a transport. While I’ve been studying how websockets, Kafka, MQTT, and other protocols have left the boundaries of HTTP and are embracing the performance gains available in the pure TCP realm, I’m more intrigued by the next generation of HTTP as a transport. Part of my learning process is all about understanding the headers available to us in the HTTP/2 realm. I’ve been learning more about the next generation HTTP headers from the gRPC Github repository which provides details on the request and response headers in play. HTTP/2 API Request Headers Request-Headers → Call-Definition *Custom-Metadata Call-Definition → Method Scheme Path TE [Authority] [Timeout] Content-Type [Message-Type] [Message-Encoding] [Message-Accept-Encoding] [User-Agent] Method → “:method POST” Scheme → “:scheme “ (“http” / “https”) Path → “:path” “/” Service-Name “/” {method name} # But see note below. Service-Name → {IDL-specific service name} Authority → “:authority” {virtual host name of authority} TE → “te” “trailers” # Used to detect incompatible proxies Timeout → “grpc-timeout” TimeoutValue TimeoutUnit TimeoutValue → {positive integer as ASCII string of at most 8 digits} TimeoutUnit → Hour / Minute / Second / Millisecond / Microsecond / Nanosecond Hour → “H” Minute → “M” Second → “S” Millisecond → “m” Microsecond → “u” Nanosecond → “n” Content-Type → “content-type” “application/grpc” [(“+proto” / “+json” / {custom})] Content-Coding → “identity” / “gzip” / “deflate” / “snappy” / {custom} Message-Encoding → “grpc-encoding” Content-Coding Message-Accept-Encoding → “grpc-accept-encoding” Content-Coding *(“,” Content-Coding) User-Agent → “user-agent” {structured user-agent string} Message-Type → “grpc-message-type” {type name for message schema} Custom-Metadata → Binary-Header / ASCII-Header Binary-Header → {Header-Name “-bin” } {base64 encoded value} ASCII-Header → Header-Name ASCII-Value Header-Name → 1*( %x30-39 / %x61-7A / “_” / “-“ /...[<a href="/2018/02/05/learning-about-the-headers-used-for-grpc-over-http2/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/05/i-appreciate-the-request-to-jump-on-phone-but-i-have-other-apis-to-test-drive/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/desertroad/clean_view/file-00_00_00_00.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/05/i-appreciate-the-request-to-jump-on-phone-but-i-have-other-apis-to-test-drive/">I Appreciate The Request To Jump On Phone But I Have Other APIs To Test Drive</a></h3>
			<p><em>05 Feb 2018</em></p>
			<p>Streamdata.io is investing in my API Stack work as we build out their API Gallery of valuable data streaming APIs. I’m powering through hundreds of APIs and using my approach to profiling APIs that I have been developing over the last eight years of operating API Evangelist. I have a large number of APIs to get through, so I don’t have a lot of time to spend on each API. I am quickly profiling and ranking them to quickly identify which one’s are worth my time. While there are many elements that get in the way of me actually being able to obtain an API key and begin using an API, one of the more frustrating elements when API providers require me to jump on the phone with them before I can test drive any APIs. I’ve encountered numerous APIs that require me talk to a sales person before I can do anything. I know that y’all think this is savvy. This is how business is done these days, but it just isn’t the way you start conversations with API consumers. Sure, there should be support channels available when I need them, but it SHOULD NOT be the way you begin a conversation with us API consumers. I’ve heard all the reasons possible for why companies feel like they need to do this, and I guarantee that all of them are based upon out of date perspectives around what APIs are all about. Often times they are a bi-product of not having a modern API management solution in place, and a team that lacks a wider awareness of the API sector and how API operations works. In 2018, I shouldn’t have to talk to you on the phone to understand what your API does, and how it fits into what I’m working on. Most of the time I do not even know what I’m working on. I’m just kicking the tires, seeing what is possible,...[<a href="/2018/02/05/i-appreciate-the-request-to-jump-on-phone-but-i-have-other-apis-to-test-drive/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/05/api-quota-api-webhooks-and-serversent-events-sse/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/goldsilverfalls/creativity/file-00_00_40_84.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/05/api-quota-api-webhooks-and-serversent-events-sse/">API Quota API, Webhooks, and Server-Sent Events (SSE)</a></h3>
			<p><em>05 Feb 2018</em></p>
			<p>I am profiling market data APIs as part of my partnership with Streamdata.io. It is a process I enjoy, because it provides me with a number of interesting stories I can tell here on API Evangelist. Many of the APIs I profile just frustrate me, but there are always the gems who are doing interesting things with their APIs, and understand providing APIs, as well as consuming APIs. One API that I’ve been profiling, and I am able to put to use in my work to build a gallery of real time data APIs, was 1Forge. 1Forge provides dead simple APIs for accessing market data, and surprise!! – you can sign up for a key, and begin making API calls within minutes. It might not sound like that big of a deal, but after going through 25+ APIs, I only have about 5 API keys. I’m working on an OpenAPI definition for 1Forge, so I can begin to poll, and stream the data they make available, including it in the Streamdata.io API gallery I’m building. However, as I was getting up and running with the API, I noticed their quota endpoint, which allows me to check my usage quote with the 1Forge API–something that I thought was story worthy. The idea of an endpoint to check my applications usage quota for an API seems like a pretty fundamental concept, but sadly it is something I do not see very often. It is something that should be default for ALL APIs, but additionally I’d like to see a webhook for, letting me know when my API consumption reaches different levels. Since I’m talking about Streamdata.io, it would also make sense to offer a Server-Sent Event (SSE) for the API quote endpoint, allowing me to bake the usage quota for all the APIs I depend on into a single API dashboard–streaming real time usage information across the APIs depend on, and maybe displaying things in RED when...[<a href="/2018/02/05/api-quota-api-webhooks-and-serversent-events-sse/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/03/api-is-not-just-rest/"><img src="https://s3.amazonaws.com/kinlane-productions2/talks/api-days-paris-2018/api-not-rest-1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/03/api-is-not-just-rest/">API Is Not Just REST</a></h3>
			<p><em>03 Feb 2018</em></p>
			<p>This is one of my talks from APIDays Paris 2018. Here is the abstract: The modern API toolbox includes a variety of standards and methodologies, which centers around REST, but also includes Hypermedia, GraphQL, real time streaming, event-driven architecture , and gRPC. API design has pushed beyond just basic access to data, and also can be about querying complex data structures, providing experience rich APIs, real-time data streams with Kafka and other standards, as well as also leveraging the latest algorithms and providing access to machine learning models. The biggest mistake any company, organization, or government agency can do is limit their API toolbox to be just about REST. Learn about a robust and modern API toolbox from the API Evangelist, Kin Lane. Diverse Toolbox After eight years of evangelizing APIs, when I participate in many API conversations, some people still assume I’m exclusively talking about REST as the API Evangelist–when in reality I am simply talking about APIs that leverage the web. Sure, REST is a dominant design pattern I shine a light on, and has enjoyed a significant amount of the spotlight over the last decade, but in reality on the ground at companies, organizations, institutions, and government agencies of all shapes and sizes, I find a much more robust API toolbox is required to get the job done. REST is just one tool in my robust and diverse toolbox, and I wanted to share with you what I am using in 2018. The toolbox I’m referring tool isn’t just about what is needed to equip an API architect to build out the perfect vision of the future. This is a toolbox that is equipped to get us from present day into the future, acknowledging all of the technical debt that exists within most organizations which many are looking to evolve as part of their larger digital transformation. My toolbox is increasingly pushing the boundaries of what I’ve historically defined as an API,...[<a href="/2018/02/03/api-is-not-just-rest/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/02/03/a-regulatory-subway-map-for-psd2/"><img src="https://s3.amazonaws.com/kinlane-productions2/transit/paris-metro.pn" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/02/03/a-regulatory-subway-map-for-psd2/">A Regulatory Subway Map For PSD2</a></h3>
			<p><em>03 Feb 2018</em></p>
			<p>This is one of my talks from APIDays Paris 2018. Here is the abstract: Understanding the PSD2 regulations unfolding for the banking industry is a daunting challenge for banks, aggregators, and regulators, let alone when you are an average user, journalist, or analyst trying to make sense of things. As the API Evangelist I have used a common approach to mapping out transit systems using the universally recognized subway map introduced by Harry Beck in London in the early 20th century. I’m taking this mapping technique and applying it to the PSD2 specification, helping provide a visual, and interactive way to navigating this new regulatory world unfolding in Europe. Join me for an exploration of API regulations in the banking industry, and how we can use a subway and transit map to help us quickly understand the complexities of PSD2, and learn what takes to get up and running with this new definition for the API economy. Using Transit As API Analogy I’ve been fascinated with transit systems and subway maps for most of my adult life. Recently I’ve begin thinking about the parallels between complex municipal transit systems, and complex API infrastructure. Beginning in 2015, I started working to see if I could apply the concept of a subway map to the world of APIs, not just as a visualization of complex API systems, but as a working interface to discover, learn about, and even govern hundreds, or even thousands of APIs working in concert. While very much a physical world thing, transit systems still share many common elements with API infrastructure, spanning multiple disparate systems, operated by disparate authorities, possessing legacy as well as modern infrastructure, and are used by many people for work and in their personal lives. While the transit system isn’t a perfect analogy for modern API infrastructure, there is enough there to keep me working to find a way to apply the concept, and specifically the transit map to...[<a href="/2018/02/03/a-regulatory-subway-map-for-psd2/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/26/shifting-gears-between-the-technology-and-politics-of-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/80_87_800_500_0_max_0_-1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/26/shifting-gears-between-the-technology-and-politics-of-apis/">Shifting Gears Between the Technology and Politics of APIs</a></h3>
			<p><em>26 Jan 2018</em></p>
			<p>I’ve been working on two talks for API Days in Paris next week. These talks are at two opposite ends of the API spectrum for me. The first one, API Is Not Just REST, is rooted in the technology of APIs, but then touches lightly on the business and politics of APIs. The second one, a regulatory subway map for PSD2, is all about the politics of APIs, but then touches lightly on the business and technology of APIs. I have the outlines for both talks done, and I’m working on the narrative and slides for each, along the way I’m really caught by how different each end of the spectrum are, and require me to use a different part of my brain–something I think really defines the yin and yang of APIs. When I’m down at the protocol level of APIs, thinking about the details of HTTP, TCP, and the nuance of how headers are used, and the messages we pass back and forth, the politics of APIs do not matter to me. My developer and architect brain is absorbed with the technical details, and the human or political consequences of my decisions really do not matter all that much. As long as things are technically correct, and my responses and requests are doing what is expected, I am good. It is easy for me to be railroaded within this technical silo, and I wouldn’t ever need to be concerned for the business and politics of it all, if my work as API Evangelist didn’t force me out of my comfort zone. Inversely, when I’m thinking about the politics of how this all works, and the intention and impact of regulatory guidance like PSD2 in Europe, the technical details of HTTP, headers, and what messages I’m using feel less important. Sure, they still matter to what I’m trying to do, but the strictness in which I define my protocols, headers, data formats, schema, and...[<a href="/2018/01/26/shifting-gears-between-the-technology-and-politics-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/24/where-am-i-in-the-sales-funnel-for-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/sand-hand_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/24/where-am-i-in-the-sales-funnel-for-your-api/">Where Am I In The Sales Funnel For Your API?</a></h3>
			<p><em>24 Jan 2018</em></p>
			<p>I’m signing up for a large number of new APIs lately as part of a project I am working on. It is pretty normal for me to sign up for a couple new APIs a week, or 10-20 within a month, but right now I’m signing up for hundreds, setting up an application and getting keys for each service. I’ll share more about what I’m working on in future stories, but I wanted to talk more about the on-boarding practices of some of these APIs. It is pretty clear from the on-boarding processes that I don’t rank very high in some of these API provider’s sales funnels, making me not deserving of self-service access, or even a sales call–which is a separate topic I will talk about in a future post. I’ve registered with a number of high value API providers who have more of an enterprise focus, but also have a seemingly self-service, public API available. After signing up for access, it becomes very clear that APIs are anything but self-service, and there is a sales funnel in play, and I’ve been ranked, tagged, and identified where I am in this sales funnel, and what value I bring as a potential small business–which is not much by usual measurements. I’m pretty well versed in how company’s set up their sales strategy, and have seen many companies think it is a good idea to translate these practies to their API operations. Only targeting the high value customers, and not really giving a shit about the rest of them. It just isn’t worth the resources to go after them, they don’t have the spending capacity of customers you want in your funnel. I get it. You are right. I don’t have a lot of money to buy your services, and will not become a high value customers. However, I have many readers who are high value customers, and trust my opinion about which services are worthy...[<a href="/2018/01/24/where-am-i-in-the-sales-funnel-for-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/24/i-want-to-be-able-to-post-and-put-and-receive-credits-on-my-api-bill/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/69_120_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/24/i-want-to-be-able-to-post-and-put-and-receive-credits-on-my-api-bill/">I Want to Be Able to POST and PUT and Receive Credits on My API Bill</a></h3>
			<p><em>24 Jan 2018</em></p>
			<p>I’ve been thinking about the potential for measuring value exchange at the API management level a lot more lately, and while I’m working on a project to profile banks that are needing to comply with the PSD2 regulations in Europe, I’m thinking about the missed opportunities for the API providers I’m using to fuel my research to leverage the value I’m generating. I’m using a variety of data enrichment APIs for helping add to the contact data, corporate profiles, images, documents, patents, and other valuable data to what I’m doing as part of my wider research into the API space. While these APIs have valuable services that I am paying for, all of the APIs are just using one HTTP verb–GET. On a regular basis I come across incorrect, or incomplete data, and as part of my work I dive in and correct the data, and continue to connect the dots. I often find better copies of logos, add in relevant business profile data, and I always provide very detailed information regarding a company’s API–which I find to be some of the most telling aspects of what a company does, or doesn’t do. I’m thankful for the services that the 3rd party APIs I utilize, but I think they are missing out on a pretty big opportunity for trusted partners like me to be able to POST or PUT the data I am gathering back to their systems. I would love to be able to POST and PUT back information to the APIs I am GETting my data from, and receive credits to my API bill for these contributions. It would benefit API providers by helping ensure the data they are providing is complete and acurate, and it would benefit me by helping me keep my API bills as low as possible. I understand that it would some work on the provider side to ensure I’m a trusted partner, and being able to verify the...[<a href="/2018/01/24/i-want-to-be-able-to-post-and-put-and-receive-credits-on-my-api-bill/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/24/helping-define-streamlinedata-io-as-more-than-just-real-time-streaming/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/24/helping-define-streamlinedata-io-as-more-than-just-real-time-streaming/">Helping Define Stream(Line)Data.io As More Than Just Real Time Streaming</a></h3>
			<p><em>24 Jan 2018</em></p>
			<p>One aspect of my partnership with Streamdata.io is about helping define what it is that Streamdata.io does–internally, and externally. When I use any API technology I always immerse myself in what it does, and understand every detail regarding the value it delivers, and I work to tell stories about this. This process helps me refine not just how I talk about the products and services, but also helps influence the road map for what the products and services deliver. As I get intimate with what Streamdata.io delivers, I’m beginning to push forward how I talk about the company. The first thoughts you have when you hear the name Streamdata.io, and learn about how you can proxy any existing JSON API, and begin delivering responses via Server-Sent Events (SSE) and JSON Patch, are all about streaming and real time. While streaming of data from existing APIs is the dominant feature of the service, I’m increasingly finding that the conversations I’m having with clients, and would be clients are more about efficiencies, caching, and streamlining how companies are delivering data. Many API providers I talk to tell me they don’t need real time streaming, but at the same time they have rate limits in place to keep their consumers from polling their APIs too much, increasing friction in API consumption, and not at all about streamlining it. These experiences are forcing me to shift how I open up conversations with API providers. Making real time and streaming secondary to streamlining how API providers are delivering data to their consumers. Real time streaming using Server-Sent Events (SSE) isn’t always about delivering financial and other data in real time. It is about delivering data using APIs efficiently, making sure only what what has been updated and needed is delivered when it is needed. The right time. This is why you’ll see me increasingly adding (line) to the Stream(line)data.io name, helping focus on the fact that we are helping streamline...[<a href="/2018/01/24/helping-define-streamlinedata-io-as-more-than-just-real-time-streaming/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/23/the-role-of-european-banking-authority-eba-when-it-comes-to-psd2/"><img src="https://s3.amazonaws.com/kinlane-productions2/psd2/220px-European_Banking_Authority_(EBA)_logo.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/23/the-role-of-european-banking-authority-eba-when-it-comes-to-psd2/">The Role of European Banking Authority (EBA) When It Comes To PSD2</a></h3>
			<p><em>23 Jan 2018</em></p>
			<p>As part of my continued effort to break down the Payment Services Directive 2 (PSD2) in Europe, and develop my awareness of how the regulations are intended, as well as the reality on the ground within the industry, I am working to map out all of the players involved. This post is about understanding the role of the European Banking Authority (EBA), and clearly understanding when and where they come into the conversation. First, what is the European Banking Authority (EBA)? They are the regulatory agency for the European Union, who is in charge of conducting stress tests on European banks and increasing transparency in the European financial system and identifying weaknesses in banks’ capital structures. When it comes to PSD2, their role is to: develop a publicly accessible central register of authorised payment institutions, which shall be kept up to date by the national authorities assist in resolving disputes between national authorities develop regulatory technical standards on strong customer authentication and secure communication channels with which all payment service providers must comply develop cooperation and information exchange between supervisory authorities The catalyst for this post was because I was looking for the “central register of authorized payment institutions”, and could not find it. Something that will be critical to this effort working, and evolving. I’m also on the hunt for more details regarding how they will be addressing authentication for all 3rd party API access, which will also be something that makes or breaks this effort. And, of course, as the API Evangelist I’m looking to help anyone in the position of helping “develop cooperation and information exchange”–it is what I do. When it comes to PSD2, I have gotten to know the API definition (OpenAPI), and I am making my way through the actual set of laws, but I’m still working to understand who all the players are. I’ll keep profiling every type participant in the PSD2 theater that is unfolding across Europe...[<a href="/2018/01/23/the-role-of-european-banking-authority-eba-when-it-comes-to-psd2/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/22/key-points-from-the-payment-services-directive-2-psd2/"><img src="https://s3.amazonaws.com/kinlane-productions2/psd2/psd2-eu.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/22/key-points-from-the-payment-services-directive-2-psd2/">Key Points From The Payment Services Directive 2 (PSD2)</a></h3>
			<p><em>22 Jan 2018</em></p>
			<p>
I’m immersed in studying the Payment Services Directive 2 (PSD2) in Europe, which includes an API definition to help enable the interoperability they are looking to achieve as part of the regulation. I’m working to break down the directive into bit such chunks to help be digest, and understand exactly what it does. The PSD2 laws seeks to improve the existing EU rules for electronic payments (hence the 2), and takes into account emerging approaches to payment services, such as Internet and mobile payments, with APIs at the hear.

The directive sets out rules concerning:


  strict security requirements for electronic payments and the protection of consumers’ financial data, guaranteeing safe authentication and reducing the risk of fraud
  the transparency of conditions and information requirements for payment services
  the rights and obligations of users and providers of payment services


Additionally, “the directive is complemented by Regulation (EU) 2015/751 which puts a cap on interchange fees charged between banks for card-based transactions. This is expected to drive down the costs for merchants in accepting consumer debit and credit cards.” Which can be one of the most frustrating aspects of banking today, where you have no expectations regarding the fees you can be charged around every turn, as you are just trying to make ends meet.

You will be seeing a lot more posts about PSD2 as I work to absorb the regulations, and the technical guidance set forth regarding banking APIs. I’m playing around with the OpenAPI definition for PSD2, and crafting a version of my API Transit subway map to represent the technical guidance present. I’m also working to understand the business, and political aspects of PSD2, which involves me breaking down the directive into this small, digestible stories, here on API Evangelist.

[<a href="/2018/01/22/key-points-from-the-payment-services-directive-2-psd2/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/19/developing-a-microservice-to-orchestrate-long-running-background-serversent/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/68_174_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/19/developing-a-microservice-to-orchestrate-long-running-background-serversent/">Developing a Microservice to Orchestrate Long Running Background Server-Sent</a></h3>
			<p><em>19 Jan 2018</em></p>
			<p>I am working to understand the value that Streamdata.io brings to the table, and one of the tools I am developing is a set of APIs to help me measure the difference in data received for normal API calls versus when they are proxied with Streamdata.io using Server-Sent Events (SSE) and JSON Patch. Creating an API to poll any 3rd party API I plug in is pretty easy and straightforward, but setting up a server setup to operate long running Server-Sent Events (SSE), managing for failure and keeping an eye on the results takes a little more consideration. Doing it browser side is easy, but server side removes the human aspect of the equation, which starts and stops the process. This post is just meant to just outline what I’m looking to build, and act as a set of project requirements for what I’m going to develop–it isn’t a guide to building it. This is just my way of working through my projects, while also getting content published on the blog ;-). I just need to work out the details of what I will need to run many different Server-Sent Events (SSE) jobs for long periods of time, or even continuously, and make sure nothing breaks, or at least minimize the breakages. Half of my project will be polling hundreds of APIs, while the other half of it will be proxy those same APIs, and making sure I’m receiving those updates continuously. I will need some basic APIs to operate each event stream I want to operate: Register - Register a new API URL I wish to run ongoing stream on. Start - Kick off a new stream for any single API I’m tracking on. Stop - Stop a stream from running for any single API I have streaming. Any API I deem worthy, and have successfully proxied with Streamdata.io will be registered, and operating as a long running background scripts via AWS EC2 instances...[<a href="/2018/01/19/developing-a-microservice-to-orchestrate-long-running-background-serversent/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/19/aws-api-gateway-openapi-vendor-extensions/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/api-gateway-extensions-swagger.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/19/aws-api-gateway-openapi-vendor-extensions/">AWS API Gateway OpenAPI Vendor Extensions</a></h3>
			<p><em>19 Jan 2018</em></p>
			<p>I was doing some work on the AWS API Gateway, and as I was going through their API documentation I found some of the OpenAPI vendor extensions they use as part of operations. These vendor extensions show up in the OpenAPI you export for any API, and reflect how AWS has extended the OpenAPI specification, making sure it does what they need it to do as part of AWS API Gateway operations. AWS has 20 separate OpenAPI vendor extensions as part of the OpenAPI specification for any API you manage using their gateway solution: x-amazon-apigateway-any-method - Specifies the Swagger Operation Object for the API Gateway catch-all ANY method in a Swagger Path Item Object. This object can exist alongside other Operation objects and will catch any HTTP method that was not explicitly declared. x-amazon-apigateway-api-key-source - Specify the source to receive an API key to throttle API methods that require a key. This API-level property is a String type. x-amazon-apigateway-authorizer - Defines a custom authorizer to be applied for authorization of method invocations in API Gateway. This object is an extended property of the Swagger Security Definitions object. x-amazon-apigateway-authtype - Specify an optional customer-defined information describing a custom authorizer. It is used for API Gateway API import and export without functional impact. x-amazon-apigateway-binary-media-types - Specifies the list of binary media types to be supported by API Gateway, such as application/octet-stream, image/jpeg, etc. This extension is a JSON Array. x-amazon-apigateway-documentation - Defines the documentation parts to be imported into API Gateway. This object is a JSON object containing an array of the DocumentationPart instances. x-amazon-apigateway-gateway-responses - Defines the gateway responses for an API as a string-to-GatewayResponse map of key-value pairs. x-amazon-apigateway-gateway-responses.gatewayResponse - Defines a gateway response of a given response type, including the status code, any applicable response parameters, or response templates. x-amazon-apigateway-gateway-responses.responseParameters - Defines a string-to-string map of key-value pairs to generate gateway response parameters from the incoming request parameters or using literal strings. x-amazon-apigateway-gateway-responses.responseTemplates -...[<a href="/2018/01/19/aws-api-gateway-openapi-vendor-extensions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/19/a-health-check-response-format-for-http-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/irakli/health-check-response-format-for-http-apis.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/19/a-health-check-response-format-for-http-apis/">A Health Check Response Format for HTTP APIs</a></h3>
			<p><em>19 Jan 2018</em></p>
			<p>My friend Irakli Nadareishvili, has published a new health check response format for HTTP APIs that I wanted to make sure was documented as part of my research. The way I do this is write a blog post, forever sealing this work in time, and adding it to the public record that is my API Evangelist brain. Since I use my blog as a reference when writing white papers, guides, blueprints, policies, and other aspects of my work, I need as many references to usable standards like this. I am going to just share the introduction from Irakli’s draft, as it says it all: The vast majority of modern APIs driving data to web and mobile applications use HTTP [RFC7230] as a transport protocol. The health and uptime of these APIs determine availability of the applications themselves. In distributed systems built with a number of APIs, understanding the health status of the APIs and making corresponding decisions, for failover or circuit-breaking, are essential for providing highly available solutions. There exists a wide variety of operational software that relies on the ability to read health check response of APIs. There is currently no standard for the health check output response, however, so most applications either rely on the basic level of information included in HTTP status codes [RFC7231] or use task-specific formats. Usage of task-specific or application-specific formats creates significant challenges, disallowing any meaningful interoperability across different implementations and between different tooling. Standardizing a format for health checks can provide any of a number of benefits, including: Flexible deployment - since operational tooling and API clients can rely on rich, uniform format, they can be safely combined and substituted as needed. Evolvability - new APIs, conforming to the standard, can safely be introduced in any environment and ecosystem that also conforms to the same standard, without costly coordination and testing requirements. This document defines a “health check” format using the JSON format [RFC7159] for APIs to...[<a href="/2018/01/19/a-health-check-response-format-for-http-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/18/docker-engine-api-has-openapi-download-at-top-of-their-api-docs/"><img src="https://s3.amazonaws.com/kinlane-productions2/docker/docker-engine-openapi-download.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/18/docker-engine-api-has-openapi-download-at-top-of-their-api-docs/">Docker Engine API Has OpenAPI Download At Top Of Their API Docs</a></h3>
			<p><em>18 Jan 2018</em></p>
			<p>I am a big fan API providers taking ownership of their OpenAPI definition, which enables API consumers to download a complete OpenAPI then import into any client tooling like Postman, using it to generate client SDKs, and getting up to speed regarding the surface area of an API. This is why I like to showcase API providers I come across who do this well, and occasionally shame API providers who don’t do it, and demonstrate to their consumers that they don’t really understand what OpenAPI definitions are all about. This week I am showcasing an API provider who does it well. I was on the hunt for an OpenAPI of the Docker Engine API, for use in a project I am consulting on, and was please to find that they have a button to download the OpenAPI for each version of the Docker Engine API right at the top of the page. Making it dead simple for me, as an API consumer, to get up and running with the Docker API in my tooling. OpenAPI is about much more than just the API documentation, and something that should be a first class companion to ALL API documentation for EVERY API provider out there–whether or not you are a devout OpenAPI (fka Swgger) believer. The Docker API team just saved me a significant amount of time in tracking down another OpenAPI, which most likely would be incomplete. Let alone the amount of work that would be required to hand-craft one for my project. I was able to take the existing OpenAPI and publish to the team Github Wiki for a project I’m advising on. The team will be able to use the OpenAPI to import into their Postman Client and begin to learn about the Docker API, which will be used to orchestrate the containers they are using to operate their own microservices. A subset of this team will also be crafting some APIs that proxy...[<a href="/2018/01/18/docker-engine-api-has-openapi-download-at-top-of-their-api-docs/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/17/five-apis-to-guide-you-on-your-way-to-the-data-dark-side/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/76_135_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/17/five-apis-to-guide-you-on-your-way-to-the-data-dark-side/">Five APIs to Guide You on Your Way to the Data Dark Side</a></h3>
			<p><em>17 Jan 2018</em></p>
			<p>I was integrating with the Clearbit API, doing some enrichment of the API providers I track on, and I found their API stack pretty interesting. I’m just using the enrichment API, which allows me to pass it a URL, and it gives me back a bunch of intelligence on the organization behind. I’ve added a bookmarklet to my browser, which allows me to push it, and the enriched data goes directly into my CRM system. Delivering what it the title says it does–enrichment. Next up, I’m going to be using the Clearbit Discovery API to find some potentially new companies who are doing APIs in specific industries. As I head over the to the docs for the API, I notice the other three APIs, and I feel like they reflect the five stages of transition to the data intelligence dark side. Enrichment API - The Enrichment API lets you look up person and company data based on an email or domain. For example, you could retrieve a person’s name, location and social handles from an email. Or you could lookup a company’s location, headcount or logo based on their domain name. Discovery API - The Discovery API lets you search for companies via specific criteria. For example, you could search for all companies with a specific funding, that use a certain technology, or that are similar to your existing customers. Prospector API - The Prospector API lets you fetch contacts and emails associated with a company, employment role, seniority, and job title. Risk API - The Risk API takes an email and IP and calculates an associated risk score. This is especially useful for figuring out whether incoming signups to your service are spam or legitimate, or whether a payment has a high chargeback risk. Reveal API - Reveal API takes an IP address, and returns the company associated with that IP. This is especially useful for de-anonymizing traffic on your website, analytics, and customizing...[<a href="/2018/01/17/five-apis-to-guide-you-on-your-way-to-the-data-dark-side/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/17/breaking-down-the-value-of-real-time-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80_168_800_500_0_max_0_1_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/17/breaking-down-the-value-of-real-time-apis/">Breaking Down The Value Of Real Time APIs</a></h3>
			<p><em>17 Jan 2018</em></p>
			<p>I am working to evolve an algorithm for Streamdata.io that helps measure the benefits of their streaming service. There are a couple layers to what they offer as a company, but as I dive into this algorithm, there are also multiple dimensions to what we all perceive as real time, and adding more complexity to the discussion, it is something that can significantly shift from industry to industry. The Streamdata.io team was working to productize this algorithm for quantifying the value their service delivers, but I wanted to take some time to break it down, lay it out on the workbench and think about it before they moved to far down this path. Ok. To help me get my brain going, I wanted to work my way through the dictionary sites, exploring what is real time? Real time often seems to describe a human rather than a machine sense of time. It is about communicating, showing, or presenting something at the time it actually happens, where there is no notable delay between the action and its effect or consequence. All of this is relative to the human receiving the real time event, as well as defining exactly when something truly happens / happened. Real time in banking is different than real time in stock trading, and will be different than media. All requiring their own perception of what is real time, and what the effects, consequences, and perceptions are. When it comes to the delivery or streaming of real time events, it isn’t just about the delivering of the event, message, or transaction. It is about doing it efficiently. The value of real time gets ruined pretty quickly when you have to wade through too much information, or you are given too many updates of events, messages, and transactions that are not relevant. Adding an efficient element to the concept of what is real time. Real time, streaming updates of EVERYTHING are not as meaningful...[<a href="/2018/01/17/breaking-down-the-value-of-real-time-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/17/api-transit-basics-deprecation/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-deprecation-2.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/17/api-transit-basics-deprecation/">API Transit Basics: Deprecation</a></h3>
			<p><em>17 Jan 2018</em></p>
			<p>This is a series of stories I’m doing as part of my API Transit work, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling API transit–leveraging the verb form of transit, to describe what every API should go through. This is a simple one. All APIs will eventually need to be deprecated. This is how you avoid legacy systems that have been up for over decades. Make sure the life span of each service is discussed as part of its conception, and put some details out about the expected timeline for its existence. Even if this becomes an unknown, at least you thought about it, and hopefully discussed it with others. Here are just a few of the common building blocks I’m seeing with API operations that respect their users enough to plan for API deprecation: Releases - Have a set release schedule, and think about what will be deprecated along with each release, allowing for future planning with push. Schedule - Have a deprecation schedule set for each API. You can always extend, or keep versions of your API beyond the date, but at least set a minimum schedule. Communication - Make sure you have a communication strategy around deprecations. Post to the blog, Tweet out notices, and send emails. The Sunset HTTP Header - This specification defines the Sunset HTTP response header field, which indicates that a URI is likely to become unresponsive at a specified point in the future. Another valuable concept this process will introduce is the possibility that APIs can be ephemeral and maybe only exist for days, weeks, or months. With CI/CD cycles allowing for daily, weekly, and monthly code pushes, there is no reason that APIs can evolve rapidly, and deprecate just as fast. Make sure...[<a href="/2018/01/17/api-transit-basics-deprecation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/17/api-management-and-the-measurement-of-value-exchanged/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/42_16_600_500_0_max_1_1_2-0.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/17/api-management-and-the-measurement-of-value-exchanged/">API Management and the Measurement of Value Exchanged</a></h3>
			<p><em>17 Jan 2018</em></p>
			<p>The concept of API management has been around for a decade now, and is something that is now part of the fabric of the cloud with services like AWS API Gateway. API management is about requiring all consumers of any API resource to sign up for any API access, obtain a set of keys that identify who they are, and pass these keys in with each API call they make from any application. Since every API call is logged, and every API call possesses these keys, it opens up the ability to understand exactly how your APIs are being used through reporting and analytics packages which come with all modern API management available today. This is the fundamentals of API management, allowing us to understand who is accessing our digital resources, and how they are putting them to use–in real time. The security of API management comes in with this balance of opening up access, being aware of who is accessing what, and being able to throttle or shut down the access of bad actors–more than it is ever about authentication, and requiring keys for all API calls. If you want access to any digital resources from a company, organization, institution, or government agency in a machine readable format, for use in any other web, mobile, or device application–you use the API. This allows ALL digital assets to be made available internally, to trusted partners, and even to the public, while still maintaining control over who has access to what, and what types of applications they are able to use them in. APIs introduce more control over our digital resources, not less–which is a persistent myth when it comes to web APIs. API management allows us to limit who has access to which APIs by putting each API into one or many “plans”. The concept of software as a service (SaaS) has dominated this discussion around plan access, establishing tiers of API access such as...[<a href="/2018/01/17/api-management-and-the-measurement-of-value-exchanged/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/16/transit-authorities-need-to-understand-that-api-management-means-more-control/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/32_161_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/16/transit-authorities-need-to-understand-that-api-management-means-more-control/">Transit Authorities Need to Understand that API Management Means More Control</a></h3>
			<p><em>16 Jan 2018</em></p>
			<p>As I look through the developer, data, and API portals of transit authorities in cities around the United States, one thing is clear–they are all underfunded, understaffed, and not a priority. This is something that will have to change if transit authorities are expected to survive, let alone thrive in a digital world. I’m working on a series of white papers as part of my partnership with Streamdata.io, on transit data, and the monetization of public data. I’ll be writing more on this subject as part of this work, but I needed to start working through my ideas, and begin crafting my narrative around how transit authorities can generate much needed revenue and compete in this digital world. I know that many transit authorities often see data as a byproduct, and something that is occasionally useful, and that their main objective is to keep the trains running. However, every tech company, and developer out there understands the value of the data being generated by the transit schedule, the trains, and most importantly the ridership. The Googles, Ubers, and data brokers are mining this data, enriching their big data warehouses, and actively working to generate revenue from transit authorities most valuable assets–their riders. Ticket sales, and people riding the trains seems like the direct value generation, but in the era of big data, where those people are going, what they are doing, reading, thinking, and who they are doing it with is equally or more valuable than the price of the ticket to ride. Most of the transit APIs I’m using require you to sign up for a key, so there is some sort of API management in place. However, there are still huge volumes of feeds and downloads that are not being managed, and there is no evidence that there is any sort of analysis, reporting, or other intelligence being gathered from API consumption. Demonstrating that API management is more about rate limiting, and keeping...[<a href="/2018/01/16/transit-authorities-need-to-understand-that-api-management-means-more-control/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/16/the-value-of-historical-transit-data-when-it-comes-to-machine-learning/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/52_175_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/16/the-value-of-historical-transit-data-when-it-comes-to-machine-learning/">The Value of Historical Transit Data When it Comes to Machine Learning</a></h3>
			<p><em>16 Jan 2018</em></p>
			<p>I’m working through the different ways that transit authorities can generate more revenue from their data using APIs as part of my work with Streamdata.io. Making data streaming and truly more real time is the obvious goal of this research, but Streamdata.io is invested in transit authorities take more control over their data resources, and use APIs to generate revenue at a time when they need all the revenue they can possible get their hands on. One overlap in the projects I’m working on with Streamdata.io is where transit data intersects with machine learning, and artificial intelligence. I’m not sure what transit authorities are doing with their historical data, but I know that it isn’t available via their APIs, and developer portals. I’m guessing they see historical data about schedules, vehicles, riderships, and other data points as a burden, and once they’ve generated the reports they need, don’t do anything else with it. This historical data is a goldmine of information when it comes to training machine learning models, which could then in turn be better used to understand ridership, make predictions, understand maintenance, scheduling, and other aspects of transit operations–let alone commerce, real estate, and other demographic data. There is a dizzying amount of investment going into machine learning and artificial intelligence right now, and is something that could be routed to transit authorities to help boost revenue. If all historical data on transit operations was digitized and available via APIs, then metered using modern API management approaches, it could be an entirely new revenue opportunity for transit authorities. Transit systems are the heartbeat of the cities they operate within, and historical data is the record of everything that occurs, which can be used to develop machine learning models for the transit industry, as well as real estate, commerce, and other sectors that transit systems feed into on a daily basis, and have for years. I do not know what data transit authorities possess....[<a href="/2018/01/16/the-value-of-historical-transit-data-when-it-comes-to-machine-learning/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/16/providing-a-guest-api-key/"><img src="https://s3.amazonaws.com/kinlane-productions2/washington-metro/wmata-guest-key.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/16/providing-a-guest-api-key/">Providing a Guest API Key</a></h3>
			<p><em>16 Jan 2018</em></p>
			<p>I’m spending time immersed in the world of transit data and APIs lately, and found a simple, yet useful approach to helping onboard developers over at the Washington Metropolitan Area Transit Authority (WMATA) API. When you you click on their products page (not sure why they use this name), you get a guest API key which allows you try out the API, and kick the tires. Of course, you can’t use the key in production applications, as it is rate limited and can change at any time, but the concept is simple, and provides an example which other API providers might want to consider. In my days as the API Evangelist I’ve seen API providers do this in a variety of ways, by providing sample API URLs complete with an API key, and by embedding a key in the OpenAPI definition, so that the interactive documentation picks up the key and will allow developers to make live, interactive API calls–to name a few. No matter what your approach, providing a guest key for users to play around without signing up makes a lot of sense. Of course, you want to rotate this key regularly, or at least be monitoring it to see what IP addresses it is being called from, and maybe understand how its being used–you never know, it might reveal some interesting use cases. Whatever your approach, get out of the way of your consumers. Don’t expect that everyone is going to want to sign up for an account so that they can learn about what your API does. Not everyone is interested in handing over their email address, and other information just so they can test drive. If you have API management in place (which you should), it really doesn’t take much to generate test users, applications, and keys, monitor them, and rotate them on a regular basis. I recommend using them like you would marketing tags, and strategically place them into...[<a href="/2018/01/16/providing-a-guest-api-key/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/16/moving-beyond-a-single-api-developer-portal/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/window_clean_view.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/16/moving-beyond-a-single-api-developer-portal/">Moving Beyond A Single API Developer Portal</a></h3>
			<p><em>16 Jan 2018</em></p>
			<p>I am working with a number of different groups who are using developer portals in some very different ways once they have moved beyond the concept that API developer portals are just for use in the public domain. With the introduction of the static site shift in the CMS landscape, and the introduction of Jekyll and Github Pages as part of the Github project workflow, the concept of the API developer portal is beginning to mean a lot of different things, depending on what the project objective is. An API portal is becoming something that can reflect a specific project, or group, and isn’t something that always has a public URL. Here are just a few of the ways in which I’m seeing portals being wielded as part of API operations. Individual Portals - Considering how developers and business users can be leverage portals to push forward conversations around the APIs they own and are moving forward. Team Portals - Thinking about how different groups and teams can have their own portals which aggregate APIs and other portals from across their project. Partner Portals - Leveraging a single, or even partner specific portals that are public or private for engaging in API projects with trusted partners. Public Portal - Begin the process of establishing a single Mutual of Omaha developer portal to provide a single point of entry for all public API efforts across the organization. Pipeline Integration - How can BitBucket be leverage for deploying of individual, team, partner, and even the public portal, making portals another aspect of the continuous deployment pipeline. One of the most interesting shifts that I am seeing is the deployment of portals as part of continuous deployment and integration pipelines. Since you can host a portal on Github, why not be deploying it, managing and evolving it as its own pipeline, or as part of individual projects, and partner integrations. This is something that static CMSs have have...[<a href="/2018/01/16/moving-beyond-a-single-api-developer-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/16/api-transit-basics-training/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-classroom-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/16/api-transit-basics-training/">API Transit Basics: Training</a></h3>
			<p><em>16 Jan 2018</em></p>
			<p>This is a series of stories I’m doing as part of my API Transit work, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling API transit–leveraging the verb form of transit, to describe what every API should go through. Think of support as a reactive area, while training will be the proactive area of the API life cycle. Ensuring there is a wealth of up to date material for API developers and consumers across all stops along the API life cycle. Investing in internal, and partner capacity when it comes to the fundamentals of APIs, as well as the finer details of each stop along the API life cycle, and CI/CD pipelines will pay off big time down the road. Every API should be included in training materials, workshops, and potentially part of conference talks given my product owners. If your API delivers value within your organization, to partners, and 3r party developers you should be training folks on putting this value to use. Here are some of the training areas I’m seeing emerge within successful API operations: Workshops - Conduct more of the workshops that I conducted with external consultants like me, as well as make sure they are conducted internally by each group. The first day of the our workshop was a great example of this in action. Curriculum - Establish common approaches to designing, developing, and evolving curriculum for teaching about the API lifecycle, as well as using each individual API. Provide forkable templates that developers can easily put to work as part of their work, and make support materials a pipeline asset that gets deployed along with documentation, and other assets. Conferences - Make sure you are sending team members to the latest conferences. In my conversations during the...[<a href="/2018/01/16/api-transit-basics-training/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/16/api-transit-basics-support/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-support.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/16/api-transit-basics-support/">API Transit Basics: Support</a></h3>
			<p><em>16 Jan 2018</em></p>
			<p>This is a series of stories I’m doing as part of my API Transit work, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling API transit–leveraging the verb form of transit, to describe what every API should go through. Beyond communication, make sure there is adequate support across API teams, and the services, tooling, and processes involved with operations. If an API goes unsupported, it might as well not exist at all, making standardized, comprehensive support practices essential. Every API should have an owner, with a contact information published as part of all API definitions. Ideally, every API owner has a backup point of contact to go to if someone should leave a company, or is out sick. Similar to the communications stop along this journey, there a handful of common support building blocks you see present within the leading API pioneers portals. These are the four I recommend baking in by default to all of your API portals, and anywhere API discovery and documentation exists. Email - Make sure there is support available via email channels, with a responsive individual on the other end–with accountability. Tickets - Consider a ticketing system for submitting and supporting requests around API operations. Dedicated - Identify one, or many individuals who can act as internal, partner, and public support when it makes sense. Office Hours - Consider one time a week where there is a human being available in person, or online to answer direct question. Every single API should have a support element present as part of its operations. Each API definition allows for the inclusion of responsible point of contact via email, and other channels–make use of it. Support should be baked into each API’s definition, making it accessible across the API lifecycle, in...[<a href="/2018/01/16/api-transit-basics-support/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/15/can-i-resell-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/docks_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/15/can-i-resell-your-api/">Can I Resell Your API?</a></h3>
			<p><em>15 Jan 2018</em></p>
			<p>Everyone wants their API to be used. We all suffer from “if we build it, they will come” syndrome in the world of APIs. If we craft a simple, useful API, developers will flock to it and integrate it into their applications. However, if you operate an API, you know that getting the attention of developers, and standing out amongst the growing number of APIs is easier said than done. Even if your API truly does bring the value you envision to the table, getting people to discover this value, and invest the time into integrating it into the platforms, products, and services takes a significant amount of work–requiring that you remove all possible obstacles and friction from any possible integration opportunity. One way we can remove obstacles for possible integrations is by allowing for ALL types of applications–even other APIs. If you think about it, APIs are just another type of application, and one that many API providers I’ve talked with either haven’t thought about at all, or haven’t thought about very deeply and restrict this use case, as they see it as directly competing with their interests. Why would you want to prevent someone from reselling your API, if it brings you traffic, sales, and the other value your API brings to your company, organization, institution, or government agency? If a potential API consumer has an audience, and wants to private label your API, how does that hurt your business? If you have proper API management in place, and have a partner agreement in place with them, how is it different than any other application? I’ve been profiling companies as part of my partnership with Streamdata.io, looking for opportunities to deliver real time streaming APIs on top of existing web APIs. Ideally, API providers become a Streamdata.io customer, but we are also looking to enable other businesses to step up and resell existing APIs as a streaming version. However, in some of the...[<a href="/2018/01/15/can-i-resell-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/15/api-transit-basics-security/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-padlock.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/15/api-transit-basics-security/">API Transit Basics: Security</a></h3>
			<p><em>15 Jan 2018</em></p>
			<p>This is a series of stories I’m doing as part of my API Transit work, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling API transit–leveraging the verb form of transit, to describe what every API should go through. Hopefully you already have your own security practices in place, with the ability to scan for vulnerabilities, and understand where security problems might exist. If you do, I’m guessing you probably already have procedures and protocols around reporting, and handling security problems across teams. Ideally, your API security practices are more about prevention than they are about responding to a crisis, but your overall strategy should have plans in place for addressing both ends of the spectrum. Unfortunately in the wider API space, much of the conversation around API security has been slowed by many people feeling like their API management solutions were doing everything that is needed. Luckily, in 2017 we began to see this thaw a bit and some API security focused solutions began to appear on the market, as well as some existing players began tuning into to address the specific concerns of API security, beyond the desktop, web, and other common areas of concern. Scanning APIs with OWASP Zap - OWASP is the top place for understanding security vulnerabilities of web applications, and they are expanding their focus to include APIs. 42 Crunch - A new, OpenAPI driven API security solution for helping deliver policies across API operations. OWASP REST Security Cheat Sheet - A checklist of considerations when it comes to API security out of OWASP. After crafting this stop along the API lifecycle I wanted to make sure and include API discovery in the conversation. API definitions like OpenAPI, and a solid API discovery strategy helps provide the...[<a href="/2018/01/15/api-transit-basics-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/15/api-transit-basics-communication/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-comment-bubbles.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/15/api-transit-basics-communication/">API Transit Basics: Communication</a></h3>
			<p><em>15 Jan 2018</em></p>
			<p>This is a series of stories I’m doing as part of my API Transit work, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling API transit–leveraging the verb form of transit, to describe what every API should go through. Moving further towards the human side of this API transit journey, I’d like to focus on one of the areas that I see cause the failure and stagnation of many API operations–basic communications. A lack of communication, and one way communication are the most common contributors to APIs not reaching their intended audience, and establishing much needed feedback loops that contribute to the API road map. This portion of the journey is not rocket science, it just take stepping back from the tech for a moment and thinking about the humans involved. When you look at Twitter, Twilio, Slack, Amazon, SalesForce, and the other leading API pioneers you see a handful of communication building blocks present across all of them. These are just a few of the communication elements that should be present in both internal, as well as external or publicly available API operations. Blogs - Make blogs a default part of ALL portals, whether partner, public, or internal. They don’t have to be grand storytelling vehicles, but can be used as part of communicating around updates within teams, groups, and for projects. Twitter - Not required for internally focused APIs, but definitely essential if you are running a publicly available API. Github - Github enables all types of communication around repos, issues, wikis, and other aspects of managing code, definitions, and content on the social coding platform. Slack - Leverage Slack for communicating around APIs throughout their life cycle, providing a history of what has occurred from start to finish. API Path...[<a href="/2018/01/15/api-transit-basics-communication/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/15/an-organized-approach-to-openapi-vendor-extensions-across-api-teams/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/15/an-organized-approach-to-openapi-vendor-extensions-across-api-teams/">An Organized Approach to OpenAPI Vendor Extensions Across API Teams</a></h3>
			<p><em>15 Jan 2018</em></p>
			<p>One of aspects of a project I’ve been assisting with recently involves helping define, implement, and organize the usage of OpenAPI vendor extensions across a distributed microservice development team. When I first began advising this group, I introduced them to the concept of extending the OpenAPI definition using x-extension format, expanding the teams approach to how they use the OpenAPI specification. They hadn’t heard that you can extend OpenAPI beyond what the specification brings to the table, allowing them to make it deliver exactly what they needed. Within this project each microservice exists in its own Github repository, with an OpenAPI definition available in the root, defining the surface area of the API. At the organizational level, I have a script that will loop through all Github repos, spider each OpenAPI looking for any x-extensions, and then it aggregates them into a single master list of OpenAPI vendor extensions that are used across all APIs. I then take the list, and add descriptions to each verified extension, and when I come across ones I haven’t seen before I reach out to microservice owners to understand what the vendor extension is used for, and ensure it is in alignment with overall API governance for the project. I am looking to encourage API designers, developers, and architects to extend OpenAPI. I am also looking to help them be responsible with this power, and make sure they are doing it for good reasons. I am also looking to organize, then educate across teams regarding how different groups are using OpenAPI vendor extensions, and incentivize the reuse and standardization of these extensions. I see vendor extensions as an area for potential innovation when it comes to defining what an API is capable of, as well as the relationship each microservice has with its supporting architecture. Acting for a relief valve for the often perceived constraints of the OpenAPI specification. This project reminded me that I need to make sure...[<a href="/2018/01/15/an-organized-approach-to-openapi-vendor-extensions-across-api-teams/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/12/orchestrating-api-integration-consumption-and-collaboration-with-the-postman/"><img src="https://s3.amazonaws.com/kinlane-productions2/postman/the-postman-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/12/orchestrating-api-integration-consumption-and-collaboration-with-the-postman/">Orchestrating API Integration, Consumption, and Collaboration with the Postman</a></h3>
			<p><em>12 Jan 2018</em></p>
			<p>You hear me say it all the time–if you are selling services and tooling to the API sector, you should have an API. In support of this way of thinking I like to highlight the API service providers I work with who follow this philosophy, and today’s example is from (Postman](https://postman.com). If you aren’t familiar with Postman, I recommend getting acquainted. It is an indispensable tool for integrating, consuming, and collaborating around the APIs you depend on, and are developing. Postman is essential to working with APIs in 2018, no matter whether you are developing them, or integrating with 3rd party APIs. Further amplifying the usefulness of Postman as a client tool, the Postman API reflects the heart of what Postman does as not just a client, but a complete life cycle tool. The Postman API provides five separate APIs, allowing you orchestration your API integration, consumption, and collaboration environment. Collections - The /collections endpoint returns a list of all collections that are accessible by you. The list includes your own collections and the collections that you have subscribed to. Environments - The /environments endpoint returns a list of all environments that belong to you. The response contains an array of environments’ information containing the name, id, owner and uid of each environment. Mocks - This endpoint fetches all the mocks that you have created. Monitors - The /monitors endpoint returns a list of all monitors that are accessible by you. The response contains an array of monitors information containing the name, id, owner and uid of each monitor. User - The /me endpoint allows you to fetch relevant information pertaining to the API Key being used. The user, collections, and environments APIs reflect the heart of the Postman API client, where mocks and monitors reflects its move to be a full API life cycle solution. This stack of APIs, and the Postman as a client tool reflects how API development, as well as API...[<a href="/2018/01/12/orchestrating-api-integration-consumption-and-collaboration-with-the-postman/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/12/aws-has-head-start-helping-navigate-regulatory-compliance-in-the-cloud/"><img src="https://s3.amazonaws.com/kinlane-productions2/aws/aws-cloud-compliance.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/12/aws-has-head-start-helping-navigate-regulatory-compliance-in-the-cloud/">AWS Has Head Start Helping Navigate Regulatory Compliance In The Cloud</a></h3>
			<p><em>12 Jan 2018</em></p>
			<p>I’m providing API guidance on a project being delivered to a government agency, as part of my Skylight partnership, and found myself spending more time looking around the AWS compliance department. You can find details on certifications, regulations, laws, and frameworks ranging from HIPPA and FERPA to FedRAMP, so that it can be used by federal government agencies in the United States, and other countries. You can find a list of services that are in scope, and track on their progress when it comes to compliance across this complex web of compliance rules. I’ve been primarily tracking on the progress of the AWS API Gateway which is currently in progress when it comes to FedRAMP compliance. When it comes to regulatory compliance, AWS has a significant leg up on its competitors, Google and Microsoft. Both of these cloud platforms have existing regulatory efforts, but they aren’t as organized, or as far along as AWS’s approach to delivering in this area. Delivering cloud solutions that are compliant gives AWS a pretty significant advantage when it comes to first impressions with government agencies, and enterprise organizations operating within heavily regulated industries. Once this impression is made, and these groups have gotten a taste of AWS, and migrated systems, and data to their cloud, it will be hard to change their behavior. As this whole Internet thing grows up, regulatory compliance is unavoidable. Many companies, organizations, institutions, and government agencies we are selling to are already needing to deliver when it comes to compliance, but even for the shiny new starts breaking new ground, at some point you will have to mature and deliver within regulatory constraints. Making AWS a pretty appealing place to be publishing databases, servers, and I’m hoping pretty soon, APIs using AWS API Gateway. If you are on the AWS API Gateway team, I’d love to get an update on the status, as I have a big government project I’d love to deploy using...[<a href="/2018/01/12/aws-has-head-start-helping-navigate-regulatory-compliance-in-the-cloud/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/12/api-transit-basics-sdkst/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-sdk.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/12/api-transit-basics-sdkst/">"API Transit Basics: SDKs\t"</a></h3>
			<p><em>12 Jan 2018</em></p>
			<p>Software Development Kits (SDKs), and code libraries in a variety of programming languages have always been a hallmark of API operations. Some API pundits feel that SDKs aren’t worth the effort to maintain, and keep in development alongside the rest of API operations, while others have done well delivering robust SDKs that span very valuable API stacks–consider the AWS JavaScript SDK as an example. Amidst this debate, SDKs continue to maintain their presence, and even have been evolving to support a more continuous integration (CI) and continuous deployment (CD) approach to delivering APIs and the applications that depend on them. Supporting SDKs in a variety of programming languages can be difficult for some API providers. Luckily there is tooling available that help auto-generate SDKs from API definitions, helping make the SDK part of the conversation a little smoother. Of course, it depends on the scope and complexity of your APIs, but increasingly auto-generated SDKs and code as part of a CI/CD process is becoming the normal way of getting things done, whether you are just making them available to your API consumers, or you are actually doing the consuming yourself. Swagger Codegen - The leading open source effort for generating SDKs from OpenAPI. APIMATIC - The leading service for generating SDKs from OpenAPI, and including as part of existing CI/CD efforts. RESTUnited - The easiest way to generate SDKs (REST API libraries): PHP, Python, Ruby, ActionScript (Flash), C#, Android, Objective-C, Scala, Java Depending on your versioning and build processes, SDK generation can be done alongside all the other stops along this life cycle. When you iterate on an API, you simply auto-generate documentation, tests, SDKs, and other aspects of supporting your services. Not all providers I talk with are easily able to jump into the aspect of producing code, as their build processes aren’t as streamline, and some of their APIs are too large to expect auto-generated code to perform as expected. However, it is...[<a href="/2018/01/12/api-transit-basics-sdkst/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/12/api-life-cycle-basics-testing/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-testing.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/12/api-life-cycle-basics-testing/">API Life Cycle Basics: Testing</a></h3>
			<p><em>12 Jan 2018</em></p>
			<p>Every API should be tested to ensure it delivers what is expected of it. All code being deployed should meet required unit and code tests, but increasingly API testing is adding another layer of assurance to existing build processes, even going so far as halting CI/CD workflows if tests fail. API testing is another area where API definitions are delivering, allowing tests to be built from existing artifacts, and allowing detailed assertions to be associated with tests to add to and evolve the existing definitions. API testing has grown over the last couple of years to include a variety of open source solutions, as well as cloud service providers. Most of the quality solutions allow you to import your OpenAPI, and automate the testing via APIs. Here are a few of the solutions I recommend considering as you think about how API testing can be introduced into your API operations. Runscope - An API testing service that uses OpenAPI for importing and exporting of API tests and assertions. Hippie-Swagger - An open source solution for testing your OpenAPI defined APIs. Spring Cloud Contract - Spring Cloud Contract is an umbrella project holding solutions that help users in successfully implementing the Consumer Driven Contracts approach. Postman Testing - With Postman you can write and run tests for each request using the JavaScript language. Frisby.js - Frisby is a REST API testing framework built on Node.js and Jasmine that makes testing API endpoints easy, fast, and fun. There are numerous ways to augment API testing on top of your existing testing strategy. More of these providers are integrating with Jenkins and other CI/CD solutions, allowing API testing to deeply integrate with existing pipelines. My recommendation is that the artifacts from these tests and assertions also live alongside OpenAPI and other artifacts and are used as part of the overall definition strategy, widening the meaning of “contract” to apply across all stops along the lifecycle–not just testing. While...[<a href="/2018/01/12/api-life-cycle-basics-testing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/11/working-with-general-transit-feed-specificationgtfs-realtime-data/"><img src="https://s3.amazonaws.com/kinlane-productions2/subway/subway-moving.gif" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/11/working-with-general-transit-feed-specificationgtfs-realtime-data/">Working With General Transit Feed Specification(GTFS) Realtime Data</a></h3>
			<p><em>11 Jan 2018</em></p>
			<p>I’ve been diving into the world of transit data, and learning more about GTFS and GTFS Realtime, two of the leading specifications for providing access to static and real time transit data. I’ve been able to take the static GTFS data and quickly render as APIs, using the zipped up CSV files provided. Next on my list I wanted to be able to work with GTFS Realtime data, as this is where the data is that changes much more often, and ultimately is more valuable in applications and to consumers. Google has developed a nice suite of GTFS Realtime bindings in a variety of programming languages, including .NET, Java, JavaScript / Node.js, PHP, Python, Ruby, and Golang. I went with the PHP bindings, which interestingly enough is the only one in its own Github repository. I’m using it because I still feel that PHP has the best opportunity for adoption within municipal organizations–something that is beginning to change, but still holds true in my experience. The GTFS-realtime data is encoded and decoded using Protocol Buffers, which provides a compact binary representation designed for fast and efficient processing of the data. Even with the usage of Protocol Buffers, which is also used by gRPC via HTTP/2, all of the GFTS Realtime data feeds I am consuming are being delivered via regular HTTP/1.1. I’m doing all this work to be able to make GTFS Realtime feeds more accessible for use by Streamdata.io, as the Protocol Buffers isn’t something the service currently supports. To make the data accessible for delivery via Server-Sent Events (SSE), and for partial updates to be delivered via JSON Patch, I need the Protocol Buffer format to be reduced to a simpler JSON format–which will be my next weeks worth of work on this project. I was able to pretty quickly bind to the MTA subway GTFS Realtime feed here in NYC using the PHP bindings, and get at up to date “vehicle” and...[<a href="/2018/01/11/working-with-general-transit-feed-specificationgtfs-realtime-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/11/the-metropolitan-transportation-authority-mta-bus-time-api-supports-service/"><img src="https://s3.amazonaws.com/kinlane-productions2/mta/mta-bus-time-siri.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/11/the-metropolitan-transportation-authority-mta-bus-time-api-supports-service/">The Metropolitan Transportation Authority (MTA) Bus Time API Supports Service</a></h3>
			<p><em>11 Jan 2018</em></p>
			<p>The General Transit Feed Specification (GTFS) format for providing access to transit data has dominated the landscape for most of the time I have been researching transit data and APIs over the last couple of weeks. A dominance led by Google and their Google Maps, who is the primary entity behind GTFS. However the tech team at Streamdata.io brought it to my attention the other day that the Metropolitan Transportation Authority (MTA) Bus Time API Supports Service Interface for Real Time Information (SIRI), another standard out of Europe. I think MTA’s introduction to Siri, and the story behind their decision tells a significant tale about how standards are viewed. According to the MTA, “SIRI (Service Interface for Real Time Information) is a standard covering a wide range of types of real-time information for public transportation. This standard has been adopted by the European standards-setting body CEN, and is not owned by any one vendor, public transportation agency or operator. It has been implemented in a growing number of different projects by vendors and agencies around Europe.” I feel like their thoughts about SIRI not being owned by any one vendor is an important thing to take note of. While GTFS is an open standard, it is clearly a Google-led effort, and I’d say their decision to use Protocol Buffers reflects the technology, business, and politics of Google’s vision for the transit sector. The MTA has evolved SIRI as part of their adoption, opting to deliver APIs as more of a RESTful interface as opposed to SOAP, and providing responses in JSON, which makes things much more accessible to a wider audience. While technologically sound decisions, I think using Protocol Buffers or even SOAP have political implications when you do not deeply consider your API consumers during the planning phases of your API. I feel like MTA has done this, and understands the need to lower the bar when it comes to the access of public...[<a href="/2018/01/11/the-metropolitan-transportation-authority-mta-bus-time-api-supports-service/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/11/openapi-will-help-you-get-your-api-house-in-order/"><img src="https://s3.amazonaws.com/kinlane-productions2/hashicorp/hashicorp-consul-api-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/11/openapi-will-help-you-get-your-api-house-in-order/">OpenAPI Will Help You Get Your API House In Order</a></h3>
			<p><em>11 Jan 2018</em></p>
			<p>I wrote a piece previously about Consul not supporting Swagger documentation at this time, and the API provider and consumer impact of this decision. I’m going to continue picking on Consul with another API definition story, because they are forcing me to hand-craft my own OpenAPI. If I just had to learn about the API, and load the OpenAPI (fka Swagger) definition into my postman, publish to the repo for the project, and import into other tooling I’m using, I wouldn’t be so critical. However, since I’m having to take their static, and often incomplete documentation, and generate an OpenAPI for my project, I’m going to vent some about their API and schema design, and they short-sighted view of OpenAPI (cough, cough fka Swagger). I just finished an OpenAPI for the Consul ACLs API path, and currently working on one for the Agent API path. I have already distilled down their static documentation into something that I can easily parse and translate into OpenAPI, I just need to finish doing the manual work. It is something I normally do with a scrape script, but the difficulties in consistently parsing their docs, combined with the scope of the docs, made me go with hand-crafting after distilling down the documentation for easier handling. I am familiar with the entire surface area of the Consul API, and now I’m getting to it’s intimate details, which also includes it’s intimate lack of details. The first area I begin stumbling on is with the design of the Consul API. While it is a web or HTTP API, it isn’t following most of the basics of REST, which would significantly help things be a little more intuitive. Simple examples of this in action would be with the LAN Coordinates for a node path, which is a PUT with the following path /coordinate/update – making for verb redundancy in the HTTP Verb and path. There also isn’t a consistent approach to ids...[<a href="/2018/01/11/openapi-will-help-you-get-your-api-house-in-order/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/11/api-life-cycle-basics-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-documentation-interactive.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/11/api-life-cycle-basics-documentation/">API Life Cycle Basics: Documentation</a></h3>
			<p><em>11 Jan 2018</em></p>
			<p>API documentation is the number one pain point for developers trying to understand what is going on with an API, as they work to get up and running consuming the resources they possess. From many discussions I’ve had with API providers it is also a pretty big pain point for many API developers when it comes to trying to keep up to date, and delivering value to consumers. Thankfully API documentation has been being driven by API definitions like OpenAPI for a while, helping keep things up date and in sync with changes going on behind the scenes. The challenge for many groups who are only doing OpenAPI to produce documentation, is that if the OpenAPI isn’t used across the API life cycle, it will often become forgotten, recreating that timeless challenge with API documentation. Thankfully in the last year or so I’m beginning to see more API documentation solutions emerge getting us beyond the Swagger UI age of docs. Don’t get me wrong, I’m thankful for what Swagger UI has done, but the I’m finding it to be very difficult to get people beyond the idea that OpenAPI (fka Swagger) isn’t the same thing as Swagger UI, and that the only reason you generate API definitions is to get documentation. There are a number of API documentation solutions to choose from in 2018, but Swagger UI still remains a viable choice for making sure your APIs are properly documented for your consumers. Swagger UI - Do not abandon Swagger UI, keep using it, but decouple it from existing code generation practices. Redoc - Another OpenAPI driven documentation solution. Read the Docs - Read the Docs hosts documentation, making it fully searchable and easy to find. You can import your docs using any major version control system, including Mercurial, Git, Subversion, and Bazaar. ReadMe.io - ReadMe is a developer hub for your startup or code. It’s a completely customizable and collaborative place for documentation, support,...[<a href="/2018/01/11/api-life-cycle-basics-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/11/api-life-cycle-basics-clients/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-client.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/11/api-life-cycle-basics-clients/">API Life Cycle Basics: Clients</a></h3>
			<p><em>11 Jan 2018</em></p>
			<p>I broke this area of my research into a separate stops a couple years back, as I saw several new types of service providers emerging to provide a new type of web-based API client. These new tools allowed you to consume, collaborate, and put APIs to use without writing any code. I knew that this shift was going to be significant, even though it hasn’t played out as I expected, with most of the providers disappearing, or being acquired, and leaving just a handful of solutions that we see today. These new web API clients allow for authentication, and the ability to quickly copy and paste API urls, or the importing of API definitions to begin making requests, and seeing responses for targeted aPIs. These clients were born out of earlier API explorers and interactive API documentation, but have matured into standalone services that are doing interesting things to how we consume APIs. Here are the three web API clients I recommend you consider as part of your API life cycle. Postman - A desktop and web client for working with and collaborating in a team environment around APIs. PAW - Paw is a full-featured HTTP client that lets you test and describe the APIs you build or consume. It has a beautiful native macOS interface to compose requests, inspect server responses, generate client code and export API definitions. -RESTFddle - An easy-to-use platform to work with APIs. It simplifies everything from API exposure to API consumption. RESTFiddle is an Enterprise-grade API Management Platform for teams. It helps you to design, develop, test and release APIs. Using web API clients allows for APIs to be easily defined, mocked, collaborated around, and leveraged as part of an API definition driven life cycle. The approach to integration saves significant cycles by allowing APIs to be designed, developed, and integrated with before any code gets written. Plus, the team and collaboration features that many of them posses can...[<a href="/2018/01/11/api-life-cycle-basics-clients/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/10/we-are-not-supporting-openapi-fka-swagger-as-we-already-published-our-docs/"><img src="https://s3.amazonaws.com/kinlane-productions2/hashicorp/hashicorp-consul-github-swagger-docs.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/10/we-are-not-supporting-openapi-fka-swagger-as-we-already-published-our-docs/">We Are Not Supporting OpenAPI (fka Swagger) As We Already Published Our Docs</a></h3>
			<p><em>10 Jan 2018</em></p>
			<p>I was looking for an OpenAPI for the Consul API to use in a project I’m working on. I have a few tricks for finding OpenAPI out in the wild, which always starts with looking over at APIs.guru, then secondarily Githubbing it (are we to verb status yet?). From a search on Github I came across an issue on the Github repo for Hashicorp’s Consul, which asked for “improved API documentation”, a Hashicorp employee ultimately responded with “we just finished a revamp of the API docs and we don’t have plans to support Swagger at this time.”. Highlighting the continued misconception of what is “OpenAPI”, what it is used for, and how important it can be to not just providing an API, but also consuming it. First things first. Swagger is now OpenAPI (has been for a while), an API specification format that is in the Open API Initiative (OAI), which is part of the Linux Foundation. Swagger is proprietary tooling for building with the OpenAPI specification. It’s an unfortunate and confusing situation that arose out of the move to the Open API Initiative, but it is one we need to move beyond, so you will find me correcting folks more often on this subject. Next, let’s look at the consumer question, asking for “improved API documentation”. OpenAPI (fka Swagger) is much more than documentation. I understand this position as much of the value it delivers to the API consumer is often the things we associate with documentation delivering. It teaches us about the surface area of an API, detailing the authentication, request, and response structure. However, OpenAPI does this in a machine readable way that allows us to take the definition with us, load it up in other tooling like Postman, as well as use to autogenerate code, tests, monitors, and many other time saving elements when we are working to integrate with an API. Lesson for the API consumers here is that OpenAPI...[<a href="/2018/01/10/we-are-not-supporting-openapi-fka-swagger-as-we-already-published-our-docs/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/10/some-of-the-thinking-behind-the-protocols-used-by-kafka/"><img src="https://s3.amazonaws.com/kinlane-productions2/kafka/kafka-protocol-guide.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/10/some-of-the-thinking-behind-the-protocols-used-by-kafka/">Some Of The Thinking Behind The Protocols Used By Kafka</a></h3>
			<p><em>10 Jan 2018</em></p>
			<p>I’ve been studying the overall Apache Stack a lot lately, with an emphasis on Kafka. I’m trying to understand what the future of APIs will hold, and where the leading edge of real time, event-driven architecture is at these days. I’m going through the protocol page for Kafka, learning about exactly how they move data around, and found their answers behind the decisions they’ve made along the way in deciding what protocols they chose to use were very interesting. All the way at the bottom of the Kafka protocol page you can find the following “Some Common Philosophical Questions”, providing some interesting backstory on the decisions behind the very popular platform. Some people have asked why we don’t use HTTP. There are a number of reasons, the best is that client implementors can make use of some of the more advanced TCP features–the ability to multiplex requests, the ability to simultaneously poll many connections, etc. We have also found HTTP libraries in many languages to be surprisingly shabby. Others have asked if maybe we shouldn’t support many different protocols. Prior experience with this was that it makes it very hard to add and test new features if they have to be ported across many protocol implementations. Our feeling is that most users don’t really see multiple protocols as a feature, they just want a good reliable client in the language of their choice. Another question is why we don’t adopt XMPP, STOMP, AMQP or an existing protocol. The answer to this varies by protocol, but in general the problem is that the protocol does determine large parts of the implementation and we couldn’t do what we are doing if we didn’t have control over the protocol. Our belief is that it is possible to do better than existing messaging systems have in providing a truly distributed messaging system, and to do this we need to build something that works differently. A final question is why...[<a href="/2018/01/10/some-of-the-thinking-behind-the-protocols-used-by-kafka/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/10/how-do-we-keep-teams-from-being-defensive-with-resources-when-doing-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/castle-on-hill-edinburgh_copper_circuit.JPG" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/10/how-do-we-keep-teams-from-being-defensive-with-resources-when-doing-apis/">How Do We Keep Teams From Being Defensive With Resources When Doing APIs?</a></h3>
			<p><em>10 Jan 2018</em></p>
			<p>I was talking with the IRS about their internal API strategy before Christmas, reviewing the teams current proposal before they pitched in across teams. One of the topics that came up, which I thought was interesting, was about how to prevent some teams from taking up a defensive stances around their resources when you are trying to level the playing field across groups using APIs and microservices. They had expressed concern that some groups just didn’t see APIs as a benefit, and in some cases perceived them as a threat to their current position within the agency. This is something I see at almost EVERY SINGLE organization I work with. Most technical groups who have established control over some valuable data, content, or other digital resource, have entrenched themselves, and become resistant to change. Often times these teams have a financial incentive to remain entrenched, and see API efforts as a threat to their budget and long term viability. This type of politics within large companies, organizations, institutions, and government agencies is the biggest threat to change than technology ever is. So, what can you do about it. Well, the most obvious thing is you can get leadership on your team, and get them to mandate change. Often times this will involve personnel change, and can get pretty ugly in the end. Alternately, I recommend trying to build bridges, by understanding the team in question, and find ways you can do API things that might benefit them. Maybe more revenue and budget opportunities. Reuse of code through open source, or reusable code and applications that might benefit their operations. I recommend mapping out the groups structure and needs, and put together a robust plan regarding how you can make inroads, build relationships, and potentially change behavior, instead of taking an adversarial tone. Another way forward is to ignore them. Focus on other teams. Find success. Demonstrate what APIs can do, and make the more entrenched...[<a href="/2018/01/10/how-do-we-keep-teams-from-being-defensive-with-resources-when-doing-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/10/building-an-api-partner-program-for-streamdata-io/"><img src="https://s3.amazonaws.com/kinlane-productions2/streamdata/streamdata-api-evangelist-partner.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/10/building-an-api-partner-program-for-streamdata-io/">Building An API Partner Program For Streamdata.io</a></h3>
			<p><em>10 Jan 2018</em></p>
			<p>I am working with Streamdata.io on a number of fronts when it comes to our partnership in 2018. One of the areas I’m helping them build, is the partner program around their API service. I’m taking what I’ve learned from studying the partner programs of other leading APIs, and I am pulling it together into coherent strategy that Streamdata.io can put to work over time. Like any other area of operations, we are going to start small, and move forward incrementally, making sure we are doing this in a sensible, and pragmatic way. First up, are the basics. What are we trying to accomplish with the Streamdata.io partner program. I want to have a simple and concise answer to what their partner program does, and is designed to accomplish. The Streamdata.io partner program is designed to encourage deeper engagement with companies, organizations, institutions, and government agencies that are putting Streamdata.io solutions to work, or are already operating within industries where Streamdata.io services will compliment what they are already doing. This partner program is meant to encourage continued participation by our customers, through offering them exposure, storytelling opportunities, referrals, and even new revenue opportunities. The Streamdata.io partner program is open to the public, just reach out and we’ll let you know if there is a fit between what our organizations are doing. It is a first draft, and not the official description, but it takes a crack at describing why we are doing it, and some about what it is. After looking through our existing partner list, and talking about the objectives of the Streamdata.io partner program we have settled in on a handful of types of partner opportunities available with the program. OEM - Our deeply integrated partners who offer a white label version Streamdata.io services to their customers. Mutual Lead Referral - Partners who we are looking to generate business for each other, sending leads back and forth to help drive growth. Co-Marketing -...[<a href="/2018/01/10/building-an-api-partner-program-for-streamdata-io/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/10/api-life-cycle-basics-portal/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-portal.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/10/api-life-cycle-basics-portal/">API Life Cycle Basics: Portal</a></h3>
			<p><em>10 Jan 2018</em></p>
			<p>A coherent strategy to delivering and operating API portals is something that gets lost in a number of the API operations I am asked to review. It is also one of the more interesting aspects of the successful strategies I track on, something that when done right, can become a vibrant source of information, and when done wrong, can make an API a ghost town, and something people back away from when finding. As part of my research I think a lot about how API portals can be used as part of each APIs lifecycle, as well as at the aggregate levels across teams, within groups, between partners, and the public. The most common form of the API portal is the classic public developer portal you find with Twitter, Twilio, Facebook, and other leading API pioneers. These portals provide a wealth of healthy patterns we can emulate, as well as some not so healthy ones. Beyond these public portals, I also se other patterns within the enterprise organizations I work with, that I think are worth sharing, showing how portals aren’t always just a single public destination, and can be much, much more. Individual Portals - Considering how developers and business users can be leverage portals to push forward conversations around the APIs they own and are moving forward. Team Portals - Thinking about how different groups and teams can have their own portals which aggregate APIs and other portals from across their project. Partner Portals - Leveraging a single, or even partner specific portals that are public or private for engaging in API projects with trusted partners. Public Portal - Begin the process of establishing a single Mutual of Omaha developer portal to provide a single point of entry for all public API efforts across the organization. Pipeline Integration - How can BitBucket be leverage for deploying of individual, team, partner, and even the public portal, making portals another aspect of the continuous deployment...[<a href="/2018/01/10/api-life-cycle-basics-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/10/api-life-cycle-basics-dns/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-dns.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/10/api-life-cycle-basics-dns/">API Life Cycle Basics: DNS</a></h3>
			<p><em>10 Jan 2018</em></p>
			<p>DNS is one of those shadowy things that tends to be managed by a select few wizards, and the rest of an organization doesn’t have much knowledge, awareness, or access at this level. APIs has shifted this reality for me, and is something I’m also seeing at organizations who are adopting a microservices, and devops approach to getting things done. DNS should be a first class citizen in the API toolbox, allowing for well planned deployments supporting a variety of services, but also allow for logging, orchestration, and most importantly security, at the frontline of our API operations. There are some basics I wanted to introduce to my readers when it comes to DNS for their API operations, but I also wanted to shine a light on where the DNS space is headed because of APIs. Some DNS and cloud providers are taking things to the next level, and APIs are central to that. Like most other stops along the API life cycle DNS is not just about doing DNS for your APIs, it is also about doing APIs for your DNS. Dedicated API DNS - I’m not in the business of telling you how to name the domain, or subdomain for your API, but you should have a plan, and be also considering having multiple subdomains, separating concerns across operations. API Control Over DNS - DNS is the frontline for your API infrastructure, even internally, and you should be able to programmatically configure, audit, orchestrate, and manage the DNS for your APIs using APIs. Regional Consideration - Begin thinking about how you name and manage your DNS with multiple zones and regions in operations–even if you aren’t quite ready, you should be thinking in this way. Amazon Route 53 Releases Auto Naming API for Service Name Management and Discovery - Thinking about how service addressing can be automated, as well as standardized as part of the life cycle. CloudFlare - You may not use...[<a href="/2018/01/10/api-life-cycle-basics-dns/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/09/the-data-behind-the-washington-post-story-on-police-shootings-in-2017/"><img src="https://s3.amazonaws.com/kinlane-productions2/washington-post/washington-post-fatal-force-story.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/09/the-data-behind-the-washington-post-story-on-police-shootings-in-2017/">The Data Behind The Washington Post Story On Police Shootings in 2017</a></h3>
			<p><em>09 Jan 2018</em></p>
			<p>I was getting ready to write my usual, “wish there was actual data behind this story about a database” story, while reading the Fatal Force story in the Washington Post, and then I saw the link! Fatal Force, 987 people have been shot and killed by police in 2017. Read about our methodology. Download the data. I am so very happy to see this. An actual prominent link to a machine readable version of the data, published on Github–this should be the default for ALL data journalism in this era. I see story after story reference the data behind, without providing any links to the data. As a database professional this practice drives me insane. Every single story that provides data driven visualizations, statistics, analysis, tables, or any other derivative from data journalism, should provide a link to the Github repository which contains at least CSV representations of the data, if not JSON. This is the minimum for ALL data journalism going forward. If you do not meet this bar, your work should be in question. Other analysts, researchers, and journalists should be able to come in behind your work and audit, verify, validate, and even build upon and augment your work, for it to be considered relevant in this time period. Github is free. Google Sheets is free. There is no excuse for you not to be publishing the data behind your work in a machine readable format. It makes me happy to see the Washington Post using Github like this, especially when they do not have an active API or developer program. I’m going to spend some time looking through the other repositories in their Github organization, and also begin tracking on which news agencies are actively using Github. Hopefully, in the near future, I can stop ranting about reputable news outlets not sharing their data behind stories in machine readable formats, because the rest of the industry will help police this, and...[<a href="/2018/01/09/the-data-behind-the-washington-post-story-on-police-shootings-in-2017/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/09/the-child-welfare-digital-services-cwds-certification-approval-and/"><img src="https://s3.amazonaws.com/kinlane-productions2/child-welfare-digital-services-california/child-welfare-digital-services.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/09/the-child-welfare-digital-services-cwds-certification-approval-and/">The Child Welfare Digital Services (CWDS) Certification, Approval, and</a></h3>
			<p><em>09 Jan 2018</em></p>
			<p>My partner Chris Cairns(@cscairns) over at Skylight sent me a link to the Child Welfare Digital Services (CWDS) Certification, Approval, and Licensing Services (CALS) API on Github the other day. The API isn’t your traditional public API, but shows what is possible when it comes to APIs at government agencies. The group behind the API has published their Digital Service Development Standards, and is actively using a Github Wiki to layout the API strategy for the organization. To give some backround, the Child Welfare Digital Services (CWDS) is for “state and county workers who ensure that safe and quality licensed facilities and approved homes are available for the children and nonminor dependents who need them, the CALS Digital Service Team will facilitate activities related to ensuring that licensed facilities, approved homes and associated adults meet and maintain required standards.” It makes me happy to see that they are investing so heavily in API, in support of such a worthy cause. Looking around their wiki I found a handful of APIs: Facility Facility Children Facility Complaint Facility Inspections You can also find more about their development process, data model, and approach to security on the Github Wiki for the organization. After looking around more at their Github organization, I found a handful of other operational APIs: Modules API Intake API Case Management API Document Management API Forms API Geo Services API There was also a Core API which they use as a base across all API projects, standardizing how they do things. Smart! I also found their testing strategy worthy of noting, just so I can add to my research. Integration Testing - To run Integration tests set property cals.api.url to point to environment host. Use gradle integrationTest task. In this case token will be generated for default test user, so it’s possible to test environment with Perry running in dev mode. Smoke Testing - Smoke test suite is part of integration tests. Set cals.api.url, use...[<a href="/2018/01/09/the-child-welfare-digital-services-cwds-certification-approval-and/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/09/generating-revenue-from-the-support-of-public-data-using-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/machine-road_blue_circuit_4.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/09/generating-revenue-from-the-support-of-public-data-using-apis/">Generating Revenue From The Support Of Public Data Using APIs</a></h3>
			<p><em>09 Jan 2018</em></p>
			<p>I’m exploring the different ways that public data access via APIs can be invested in, even with the opportunity for generating revenue, but without charging access to the data itself. I want public data to remain accessible, but in reality it costs money to provide access to public data, to refine, and evolve it. This is meant to be an exploration of how public data is invested in, not how you lock up public data, so please read everything before commenting, as every time I write about this subject, there are folks who blindly declare that ALL public data has to be free, no matter what. I agree (mostly), but there has to be commercial monetization opportunities around public data, otherwise it will never evolve, improve, be enriched, and in some cases available at all. I am looking for opportunities for public data stewards / owners to generated much needed revenue, as well as commercial interests to come in and augment, and build upon what is already available–going beyond what cash and resource strapped data stewards / owners might be able to do on their own. Here are a couple of areas I’m documenting a little more right now. Wholesale APIs - Deploying, managing, and providing access to public data via APIs that are designed specifically for individual consumers. This could be deploying an API on AWS, Google, Azure, or other infrastructure provider, and providing private, or even I guess public access to the data. Delivering a personalized, customized, and performant public data API experience. Real Time APIs - Providing a proxied stream of data from one or many public data sources, going beyond what the data steward / owner is capable of delivering. Charging for the technology, not access to the data itself. Cached - Delivering a cached experience, so that when the primary source goes down, all API consumers are still able to get at historical, or other relevant data without interruption. Transformation...[<a href="/2018/01/09/generating-revenue-from-the-support-of-public-data-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/09/api-life-cycle-basics-api-management/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-tools.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/09/api-life-cycle-basics-api-management/">API Life Cycle Basics: API Management</a></h3>
			<p><em>09 Jan 2018</em></p>
			<p>The need to manage APIs is one of the older aspects of doing business with web APIs. Beginning around 2006, then maturing, and being baked into the cloud and markets by 2016. Whether it is through an management gateway that proxies existing APIs, natively as part of the gateway that is used to deploy the APIs themselves, or as a connective layer within the code, API management is all about authenticating, metering, logging, analyzing, reporting, and even billing against API consumption. This landscape has significantly shifted lately, with the bottom end of the market becoming more competitive, but luckily there are enough open source and cloud solutions available to get the job done. Over the last decade API management providers have collectively defined some common approaches to getting business done using web APIs. While still very technical, API management is all about the business of APIs, and managing the value generated from providing access to data, content, algorithms, and other digital resources using the web. Here are the handful of common aspects of API management, which are being baked into the cloud, and made available across a number of open source solution providers catering to the API space: Authentication - Requiring all developers to register, obtain keys, and provide unique identification with API request they make. Service Composition - Allowing for the organizing and breaking down of APIs into meaningful lines of business, and allowing for different times of access to these products. Rate Limiting - Limiting, and protecting the value of digital resources, only allowing access to those who have been approved. Metering - Measuring each call that is made to APIs, and applying service composition, and pricing to all API traffic, quantifying the value of business being conducted. Reporting - Providing analysis and reporting on all activity, enabling API providers to develop awareness, and drill down regarding how resources are being used. Invoicing - Accounting for all API traffic and invoicing, charging, and...[<a href="/2018/01/09/api-life-cycle-basics-api-management/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/09/api-life-cycle-basics-api-logging/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-logging-2.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/09/api-life-cycle-basics-api-logging/">API Life Cycle Basics: API Logging</a></h3>
			<p><em>09 Jan 2018</em></p>
			<p>Logging has always been in the background of other stops along the API lifecycle, most notably the API management layer. However increasingly I am recommending pulling logging out of API management, and making it a first-class citizen, ensuring that the logging of all systems across the API lifecycle are aggregated, and accessible, allowing them to be accessed alongside other resources. Almost every stop in this basics of an API life cycle series will have its own logging layer, providing an opportunity to better understand each stop, but also side by side as part of the bigger picture. There are some clear leaders when it comes to logging, searching, and analyzing large volumes of data generated across API operations. This is one area you should not be reinventing the wheel in, and you need to be leveraging the experience of the open source tooling providers, as well as the cloud providers who have emerged across the landscape. Here is a snapshot of a few providers who will help you make logging a first class citizen in your API life cycle. Elastic Stack - Formerly known as the Elk Stack, the evolved approach to logging, search, and analysis out of Elastic. I recommend incorporating it into all aspects of operations, and deploying APIs to make them first class citizens. Logmatic - Whatever the language or stack, staging or production, front or back, Logmatic.io centralizes all your logs and metrics right into your browser. Nagio - Nagios Log Server greatly simplifies the process of searching your log data. Set up alerts to notify you when potential threats arise, or simply query your log data to quickly audit any system. Google Stackdriver - Google Stackdriver provides powerful monitoring, logging, and diagnostics. AWS CloudWatch - Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. I recommend cracking open logging from EVERY layer, and shipping them into a central system like Elastic for...[<a href="/2018/01/09/api-life-cycle-basics-api-logging/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/09/a-blueprint-for-an-augmented-transit-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/metrohero/metro-hero-dashboard.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/09/a-blueprint-for-an-augmented-transit-api/">A Blueprint For An Augmented Transit API</a></h3>
			<p><em>09 Jan 2018</em></p>
			<p>I’m working through research on the world of transit APIs as part of my partnership with Streamdata.io. From what I’ve gathered so far, the world of transit data and APIs is quite a mess, and there is a pretty significant opportunity to improve upon what already exists. In the course of my research, I stumbled across MetroHero, which is an application and API provider that operates on top of the Washington Metropolitan Area Transit Authority data and API feeds. I’m still working my way through their website, services, APIs, as well as talking with their team, but I’m fascinated with what they are doing, and wanted to think a little more about it before I talk with them this week. While their approach to improving upon WMATA applications is interesting, I think applying this way of thought to a government API is more interesting (surprise). The MetroHero API is what I’d consider to be an augmented API, operating on top of the WMATA API, and improving upon the data and services they make available about the Washington DC transit system. The MetroHero API, taken directly from their developer portal, “are available for free. In return, we require the following”: You must abide by WMATA’s Transit Data Terms of Use; by using our APIs, you agree to these terms of service. Any data returned by or derived from data returned by our APIs must be freely available to all users of your application. Any paywalled application that utilizes our APIs must also provide a free tier with access to the same data returned by or derived from our APIs. Any data returned by or derived from data returned by our APIs must be prominently credited back to MetroHero. For example, if this data is being displayed to users on a website or in an application, MetroHero must always be visually credited wherever and whenever the data appears or is used. The MetroHero API is not sanctioned...[<a href="/2018/01/09/a-blueprint-for-an-augmented-transit-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/08/understanding-events-across-your-api-platform-in-real-time/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripe-the-event-object.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/08/understanding-events-across-your-api-platform-in-real-time/">Understanding Events Across Your API Platform In Real Time</a></h3>
			<p><em>08 Jan 2018</em></p>
			<p>I spend a lot of time trying to understand and define what is API. With my new partnership with Streamdata.io I’m pushing that work into understanding APIs in real time, and as part of event-driven architecture. As the Streamdata.io team and I work to identify interesting APIs out there that would benefit from streaming using the service, a picture of the real time nature of API platforms begins to emerge. I’m beginning to see all of this as a maturity aspect of API platforms, and those who are further along in their journey, have a better understanding the meaningful events that are occurring via their operations. As part of this research I’ve been studying the Stripe API, looking for aspects of the platform that you could make more real time, and streaming. Immediately I come across the Stripe Events API, which is a “way of letting you know when something interesting happens in your account”. Using the Stripe Events API, “you can retrieve an individual event or a list of events from the API. We also have a separate system for sending the event objects directly to an endpoint on your server, called webhooks.” This is the heartbeat of the Stripe platform, and represents the “events” that API providers and consumers want to know about, and understand across platform usage. I think about the awareness API management has brought to the table in the form of metrics and analytics. Then I consider the blueprint the more mature platforms like Stripe have established when it comes to codifying this awareness in a way that can be accessed via API, and begin to make more real time, or at least asynchronous using webhooks. Then I think about what Streamdata.io provides with Server-Sent Events, and JSON Patch, providing a stream of these meaningful events in real time, as soon as these events happen–no polling necessary. This is what I find interesting about what they do, and why I’ve...[<a href="/2018/01/08/understanding-events-across-your-api-platform-in-real-time/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/08/seeing-reflections-from-the-past-rippling-in-the-api-pool-when-i-translated/"><img src="https://s3.amazonaws.com/kinlane-productions2/matrix-neo.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/08/seeing-reflections-from-the-past-rippling-in-the-api-pool-when-i-translated/">Seeing Reflections From The Past Rippling In The API Pool When I Translated</a></h3>
			<p><em>08 Jan 2018</em></p>
			<p>I was looking for the API definition and schema for the Service Interface for Real Time Information (SIRI) standard, but all they have were WSDL and XSDs. I am working with the Metropolitan Transit Authorities (MTA) SIRI feed, which returns JSON, and I wanted to have a JSON schema reflecting the responses I was working with. So I took the WSDLs and converted them to OpenAPI using the API Transformer, which kind of felt like that scene in the matrix where the world around him is beginning to turn to liquid as he pokes at it, right before he exits the matrix. I regularly get the emails and tweets from folks telling me “we’ve done all this before”, when I talk about OpenAPI. I’m fully aware that we have, and have even written a few stories about it. However, translating my first WSDL into an OpenAPI was a new experience for me, and made me think deeply about where we are at in 2018 a little bit more. I was reminded once again of how much we’ve left behind, and how much of this we are slowly recreating as part of the OpenAPI specification, and the other definitions and tooling we are developing. I don’t think all of this is bad, but I do think we’ve never been able to have an honest conversation about all of this. Over the last decade I feel like there has been two camps, those still committed to web services, and those that had invested in this new paradigm. Web service folks have mostly dug in their heals, and proclaimed, “we’ve already done all this”. While us web API folks moved forward with this new realm, somewhat in denial about where we’ve been. Most discussions have been pretty black or white, meaning it was all new, or all old, and we couldn’t really ever talk about things at a granular level, and exploring the grey layers in between. There...[<a href="/2018/01/08/seeing-reflections-from-the-past-rippling-in-the-api-pool-when-i-translated/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/01/08/looking-for-the-answer-instead-of-developing-an-understanding-of-good-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/status-berlin_matrix.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/01/08/looking-for-the-answer-instead-of-developing-an-understanding-of-good-api/">Looking For THE Answer Instead Of Developing An Understanding Of Good API</a></h3>
			<p><em>08 Jan 2018</em></p>
			<p>I recently worked with a large team on a microservices and API governance training a couple months back, where I saw a repeating pattern that I’ve experienced with other large enterprise groups. They seemed to want the right answers to doing APIs and microservices, instead of developing an understanding of good (or bad) API design, and determining the right way for themselves. The biggest challenge with this perception amongst development groups, is that there is no right answer, or one way of doing APIs and microservices–you need to find the right way forward for your team, and for each project. I get this a lot within large organizations who are just beginning their API journey–just show us the right way of doing it! To which I usually reply with, let’s roll up our sleeves and get to work on one of your services, and we will start showing you have to craft a simple, sensible, yet robust API that meets the needs of the project. I can’t show you the “right way”, until I understand what the particular need are, and zeroing in on what is the “right way”, takes work, refinement, and crafting f a robust definition for the service. API design isn’t easy. It takes time to understand what schema is involved, and what role that schema will play in the request and response of an API. It takes work to understand whether the complexity should be spread horizontally across each API paths, or vertically within a single path, using parameters, the body, and other aspects of the surface area of the API. Until I understand the client needs I can’t fully articulate whether we should stick with a simple JSON response, or a more sophisticated hypermedia media type, or possibly even go with something like GraphQL. Ultimately, I need you to go on this journey with me. I can help ask hard questions, and provide relevant answers about best practices I see...[<a href="/2018/01/08/looking-for-the-answer-instead-of-developing-an-understanding-of-good-api/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page6" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page8" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home Page</a></li>
    <li><a href="/blog/">The Blog</a></li>
    <li><a href="https://101.apievangelist.com/">API 101</a></li>
    <li><a href="http://history.apievangelist.com">History of APIs</a></li>
    <li><a href="https://women-in-tech.apievangelist.com/">Women in Technology</a></li>    
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
