<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>API Evangelist</title>
  <updated></updated>
  <link rel="self" href="https://apievangelist.com/atom.xml"/>
  <author><name>Kin Lane</name></author>
  <id>https://apievangelist.com/atom.xml</id>
	<entry>
    <title>The Challenges Of API Discovery Conversations Being Purely Technical</title>
    <link href="http://apievangelist.com/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/"/>
    <updated></updated>
    <content><![CDATA[
Ironically one of the biggest challenges facing API discovery on the web, as well as within the enterprise, is that most conversations focus purely on the technical, rather than the human and often business implications of finding and putting APIs to work. The biggest movement in the realm of API discovery in the last couple years has been part of the service mesh evolution of API infrastructure, and how your gateways “discover” and understand the health of APIs or microservices that provide vital services to applications and other systems. Don’t get me wrong, this is super critical, but it is more about satisfying a technical need, which is also being fueled by an investment wave-—it won’t contribute to much to the overall API discovery and search conversation because of it’s limited view of the landscape.

Runtime API discovery is critical, but there are so many other times we need API discovery to effectively operate the enterprise. Striving for technical precision at runtime is a great goal, but enabling all your groups, both technical and business to effectively find, understand, engage, and evolve with existing APIs should also be a priority. It can be exciting to focus on the latest technological trends, but doing the mundane profiling, documentation, and indexing of existing API infrastructure can have a much larger business impact. Defining the technical details of your API Infrastructure using OpenAPI, Postman, and other machine readable formats is just the beginning, ideally you are also working define the business side of things along the way.

I find that defining APIs using OpenAPI and JSON Schema to be grueling work. However, I find documenting the teams and owners behind APIs, the licensing, dependencies (both technical and business), pricing, and other business aspects of an API to be even more difficult. Over the last decade we’ve gotten to work standardizing how define the technical surface area of our APIs, but we’ve done very little work to standardize how we license, price, own, collaborate, and track on the other business implications of delivering APIs. This is one reason Steve Willmott and I created the APIs.json format, to help drive this discussion. Providing a machine readable API format to transcend the technical details of APIs, and allow us to better define the operational side of making sure APIs are discoverable.

APIs.json is about defining everything about your APIs that JSON Schema, OpenAPI, and AsyncAPI will not. Where your documentation is, how to find SDKs, what the terms and conditions are, or maybe the licensing behind your API. We designed the API specification to be flexible, and something that can be extended. There are a handful of default property types you can use when applying the format, but ultimately it is about pushing you to define your own using x- extensions. Helping API providers think through what the common building blocks of their API operations are, and provide them with a simple JSON or YAML format for indexing all of these elements for use in your API catalog, or publishing to the root of your developer portal. Helping augment what OpenAPI, JSON Schema, and AsyncAPI have done, but providing a single place for you to hang all of your API artifacts.

I’m working hard to continue refining my catalog of 3K+ APIs.json files. I’m working on better ways to validate or invalidate what I have indexed, and provide a single search interface for them. Once I’ve refreshed the catalog, and synced them with the evolution of the available APIs over at APIs.io, I will publish a fresh list of the companies I’m tracking on. I feel like one of the most critical business aspects of API discovery we consistently overlook, ignore, or are in denial of, is whether an API is still active, and anyone is home. This is a rampant illness in the cataloging of public APIs, but also something that you can find all over the enterprise. We need to do a better job of understand where are APIs are, but also be more honest about which APIs are used, do not have an owner, or are straight 404’ing and shouldn’t be listed in any active API catalog.
]]></content>
    <id>http://apievangelist.com/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/</id>
  </entry><entry>
    <title>Differences Between API Observability Over Monitoring, Testing, Reliability, and Performance</title>
    <link href="http://apievangelist.com/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/"/>
    <updated></updated>
    <content><![CDATA[
I’ve been watching the API observability coming out of Stripe, as well as Honeycomb for a couple years now. Then observability of systems is not a new concept, but it is one that progressive API providers have embraced to help articulate the overall health and reliability of their systems. In control theory, observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs. Everyone (except me) focuses only on the modern approaches for monitoring, testing, performance, status, and other common API infrastructure building blocks to define observability. I insist on adding the layers of transparency and communication, which I feel are the critical aspects of observability—-I mean if you aren’t transparent and communicative about your monitoring, testing, performance, and status, does it really matter?

I work to define observability as a handful of key API building blocks that every API provider should be investing in:


  Monitoring - Actively monitoring ALL of your APIs to ensure they are up and running.
  Testing - Performing tests to ensure APIs aren’t just up but also doing what they are intended to.
  Performance - Adding an understanding of how well your APIs are delivering to ensure they perform as expected.
  Security - Actively locking down, scanning, and ensuring all your API infrastructure is secure.


Many folks rely on the outputs from these areas to define observability, but there are a couple more ingredients needed to make it observable:


  Transparency - Sharing the practices and results from each of these areas is critical.
  Communication - If you aren’t talking about these things regularly they do not exist.
  Status - Providing real time status updates for al these areas is essential.


You can be actively observing the outputs from monitoring, testing, performance, and security operations, but if this data isn’t accessible to other people on your team, within or company, partners, and for the public as required, then things aren’t observable. Of course, I’m not talking about making ALL API activity public, but I’m saying, if you are a public API, and you aren’t providing transparency, communication, and status of your monitoring, testing, performance, and security—-then you aren’t observable.

I know many folks will disagree with me on this part, but that is ok. I am used to it. So far, I haven’t seen much embrace of the observability concept, with many providers either not understanding it, or not grasping the meaningful impact it will have on their operations. So I’m not holding my breath that folks will buy into my portion of it. However it is my self appointed role to make sure the bar is high, even if nobody adopts the same set of rules. In the end, API observability isn’t some new trendy buzzword, it is one of a handful of meaningful constructs that exist to help make us all better, but most likely will get lost in the shuffle of doing APIs each day. :-(
]]></content>
    <id>http://apievangelist.com/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/</id>
  </entry><entry>
    <title>Peer API Design Review Sessions</title>
    <link href="http://apievangelist.com/2019/07/30/peer-api-design-review-sessions/"/>
    <updated></updated>
    <content><![CDATA[
Public APIs have always benefitted from something that internal APIs do not always received—-feedback from other people. While the whole public API thing didn’t play out as I originally imagined, there is still a lot of benefit in letting other see, and provide open feedback on your API. It is painful for many developers to receive feedback on their API, and it is something that can be even more painful when it is done publicly. This is why so many of us shy away from establishing feedback loops around our APIs. It is hard work to properly design, develop, and then articulate our API vision to an external audience. It is something I understand well, but still suffer from when it comes to properly accessioning peer review and feedback on my API designs.

I prefer opening up to peer reviews of my API designs while they are still just mocks. I’m less invested in them at this point, and it is easier to receive feedback on them. It is way less painful to engage in an ongoing discussion fo what an API should (and shouldn’t) do early on, then it is to define the vision, deliver an API as code or within a gateway, and then have people comment on your baby that you have given birth to. It hurts to have people question your vision, and what you’ve put forth. Especially for us fragile white men who who aren’t often very good at accepting critical feedback, and want to just be left to our own devices. I’d much prefer just being a coder, but around 2008 through 2010 I saw the benefits to my own personal development when I opened up my work to my peers and let a little sunlight in. I am a better developer because of it.

One tool in my API toolbox that is growing in importance is the peer, and open API design review sessions. Taking an OpenAPI draft, loading it into Swagger Editor, firing up a Zoom or Google Hangout, and inviting others to openly share in the design of an API. I find it isn’t something everyone is equipped to do, but many are open to learning, or at least curious about how it works. Curious is good. It is a start. I think many folks aren’t fluent in the API design process, and are often afraid to appear like they don’t know what they are doing, and having an open discussion throughout the API design process helps them learn out in the open. Using a process that helps everyone involved learn together, and lower their guard a little bit when it comes to new ideas, new ways of doing things, and discussing the overall developer experience (DX) of delivering a quality API.

Peer API Design reviews is something I’d love to see more API design tooling support. If nothing else, more people just doing it with existing tools. You may not have fully embraced a complete API design first approach within your enterprise group, but openly discussing API design patterns is important. It is critical for any API developer to receive feedback on their design from other stakeholders, and other API development peers. It is important that we allow ourselves to open up to this feedback and sometimes criticism of our designs, based upon what others know, sharing potential views on how an API can reduce friction for consumers. Ideally, this process is also made accessible to non-developer stakeholders, and even business owners, but I’m thinking this is another post all by itself—-for right now, I just want to advocate for more peer API design review sessions.
]]></content>
    <id>http://apievangelist.com/2019/07/30/peer-api-design-review-sessions/</id>
  </entry><entry>
    <title>API For Processing Common Logging Formats And Generating OpenAPI Definitions</title>
    <link href="http://apievangelist.com/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/"/>
    <updated></updated>
    <content><![CDATA[
I’ve invested a lot of time in the last six months into various research, scripts, and tooling to help me with finding APIs within the enterprise. This work is not part my current role, but as a side project to help me get into the mindset of how to help the enterprise understand where their APIs are, and what APIs they are using. Almost every enterprise group I have consulted for has trouble keeping tabs on what APIs are being consumed across the enterprise, and I’m keen on helping understand what the best ways are to help them get their API houses in order.

While there are many ways to trace out how APIs are being consumed across the enterprise, I want to start with some of the basics, or the low hanging when it came to API logging within the enterprise. I’m sure there are a lot of common logging locations to tackle, but my list began with some of the common cloud platforms in use for logging of operations to begin my work—focusing on the following three cloud logging solutions:


  Amazon CloudFront - Beginning with the cloud leader, and looking at how the enterprise is centralizing their logs with CloudFront.
  Google StackDriver - Next, I found Google’s multi-platform approach interesting and worth evaluating as part of this work.
  Azure Logging - Of course, I have to include Azure in all of this as they are a fast growing competitor to Amazon in this space.


After establishing a short list of cloud platforms logging solutions, I began looking at which of the common web server formats I should be looking for within these aggregate logging locations, trying to map out how the enterprise is logging web traffic. Providing me with a short list of the three most common web server formats I should be looking at when it comes to mapping the enterprise API landscape—-providing artifacts of the APIs that enterprise groups are operating.


  Apache Log File - The most ubiquitous open source web server out there is the default for many API providers.
  NGINX Log File - The next most ubiquitous open source web server is definitely something I should be looking for.
  IIS Log File - Then of course, many Microsoft web server folks are still using IIS to serve up their API infrastructure.


These three web server logging formats represent a significant slice of the API logging pie. If I can identify these logging formats across common cloud logging locations, I feel that I can provide a pretty significant solution for finding the APIs that are in use across the enterprise. However, I didn’t just want to be looking a the web server logging for understanding what APIs are being served up, I also wanted to look at the exhaust from how APIs are being consume by looking at these two web browser and proxy traffic formats:


  HAR File - Allowing for the discovery of APIs that are used in web and browser applications across common use cases.
  Charles Proxy JSON Session - Using a common proxy application to reverse engineer web and mobile application API calls.


These cloud logging solutions, web server formats, as well as browser and proxy solutions give me a pretty interesting look at the API discovery pie. I have scripts to help identify these common formats, and then automatically produce OpenAPI definitions from them. It is pretty easy to run these scripts in a variety of ways to help automatically produce a catalog of OpenAPI definitions from them, automating the mapping of the API landscape within he enterprise. I have all of these scripts working for me in a variety of capacities, the next step is to further automate them, organize them into more of a usable suite of API tooling, then unleash them on a larger set of enterprise logs.

All of my scripts currently run as APIs, as I’m API-first, but I’m currently exploring ways in which I can better execute them at the command line, and as autonomous solutions that can be installed within the enterprise, without any external connections or dependencies. I have a list of ways in which I want to add more value on top of these API discovery solutions, allowing me to generate revenue from them. However right now, I am more interested in ensuring they help automate the API landscape across the majority of enterprise logging solutions. Once I dial this in, I will be looking for more ways to implement the existing functionality, as well as evolve to cover other platforms and formats. I’m just looking to deliver a basic solution for understanding where the hell all the APIs are in the enterprise, before I look to bake in more advanced features.
]]></content>
    <id>http://apievangelist.com/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/</id>
  </entry><entry>
    <title>API Storytelling Within The Enterprise</title>
    <link href="http://apievangelist.com/2019/07/28/api-storytelling-within-the-enterprise/"/>
    <updated></updated>
    <content><![CDATA[
Storytelling is important. Storytelling within the enterprise is hard. Reaching folks on the open web is hard work to, but there is usually an audience that will eventually tune in, and over time you can develop and cultivate that audience. The tools you have at your disposal within the enterprise are much more prescribed and often times dictated–even controlled. I also find that they aren’t always as effective as they are made out to be, with the perception being one thing, and the reach, engagement, and noise being then much harder realities you face when trying to get a message out.

Email might seem like a good idea, and is definitely a critical tool when reaching specific individuals or groups, but as a general company wide thing, it quickly becomes exponentially ineffective with each person you add on as CC. I’d say that you are better off creating a daily or weekly email newsletter if you are going to be sending across large groups of the enterprise rather than participating in the constant email barrage that occurs on a daily basis. Email is an effective tool when used properly, but I’d say I haven’t perfected the art of using email to reach my intended audience within the enterprise.

My preferred storytelling format is relatively muted within the enterprise — people rarely read blogs in this world. Blog reading is something you do out on the web apparently. This means I have to get pretty creative when it comes to getting your stories out. It doesn’t mean you shouldn’t be using this format of storytelling, but you just can’t count on folks to regularly consume a blog, or subscribe to an RSS feed. You can still have a blog, but you have to find other ways of slipping the links into existing conversations, documentation, and other avenues in which people consume information within the enterprise.

I would say this reality of reading within the enterprise is why I try to write more white papers and guides. I know that many folks across the enterprise prefer to consume their reading materials as a PDF on their laptop, desktop, or tablet. While this is definitely not my preferred way of consuming information, I have to remember that it is the primary way in which enterprise folks can cut through the noise, and find some quiet time to digest 6-8 pages of API blah blah blah during their busy day. While I will keep pumping out short form content on the blog, I will also be investing much more into creating longer form white papers and guides that have a greater opportunity of penetrating the enterprise.

I know that enterprise folks are caught up in the daily shit-storm and can’t always get to my blog, or spend too much time on Twitter. Making content more portable, and something they can email around, download and potentially consume later is important. As I work within the enterprise more I am realizing how critical this is for folks, including myself. I found myself firing back up my Pocket app on my iPad, so that I can queue things up for later. Reminding how difficult it is to tell stories within the enterprise and that I cannot discount tools like the PDF when it comes to reaching my intended audience. You really have to understand your audience, and work to meet them at their level, regardless of the tools you use to get information and be influenced by the deluge of storytelling we are inundated with on a daily basis.
]]></content>
    <id>http://apievangelist.com/2019/07/28/api-storytelling-within-the-enterprise/</id>
  </entry><entry>
    <title>APIs and Browser Bookmarklets</title>
    <link href="http://apievangelist.com/2019/07/24/apis-and-browser-bookmarklets/"/>
    <updated></updated>
    <content><![CDATA[
I have quite a few API driven bookmarklets I use to profile APIs. I recently quit using Google Chrome, so I needed to migrate all of them to Firefox. I saw this work as an opportunity to better define and organize them, as they had accumulated over the years without any sort of strategy. When I need some new functionality in my browser I would create a new API, and craft a bookmarklet that would accomplish whatever I needed. I wish I had more browser add-on development skills, something I regular try to invest in, but I find that bookmarklets are the next best thing when it comes to browser and API interactions.

There are a number of tasks I am looking to accomplish when I’m browsing the web pages of an API provider. The first thing I want to do is record their domain, then retrieve as much intelligence about the company behind the domain in a single click of the bookmarklet. This was the first bookmarklet and API I developed. Since then, I’ve made numerous others to record the pricing page, parse the terms of service, OpenAPI, and other valuable API artifacts from across the landscape. Bookmarklets are a great way to provide just a little more context combined with a URL pointer, for harvesting, processing, and possibly some human review. Allowing me to augment, enrich, and automate how I consume information as I’m roaming around the web, researching specific topics, and do what I do.

At this point I am actually glad I didn’t invest a lot of energy into developing Chrome browser extension, because it wouldn’t have easily translated to a Firefox world. Since I have been investing in APIs plus bookmarklets, I can easily import, or copy and paste my bookmarklets over. I”m spending the time to go through them, inventory them, and better organize them for optimal usage, so the migration is a little more work than just import and export. Another aspect of this work that I am thankful for is that I abstracted away is the usage of other 3rd party APIs. My very first bookmarklet which profiles the domain of the website I’m looking at has used several different business intelligence solutions, all of which I have been priced out of using, so I’ve resorted to other ways to obtain the profile information I need–the API continues to work despite the APIs I use under the hood.

Browsers are an area of my API research that is significantly deficient. I am working to invest a little more time here, focusing on the migration and evolution of my API driven bookmarklets, but also playing around with the Browser Reporting API, which is some pretty interesting HEADER voodoo. I can’t help but feel like the browsers will continue to play an increasingly important role when it comes to APIs. Not just because of browser APIs like the Reporting API, but also because of the hidden APIs web and mobile applications use, as well as the above the tables APIs we leverage within the browser—-like my bookmarklets. I find the browser a more interesting place to study how APIs are being put to work than with startups these days. I feel like it is where the “innovation” is occurring these days, and sadly, it isn’t the good kind of “innovation” everyone so passionately believes in—-it is the more exploitative ad-driven “innovation” that is pretty invasive in our lives.
]]></content>
    <id>http://apievangelist.com/2019/07/24/apis-and-browser-bookmarklets/</id>
  </entry><entry>
    <title>Absolutism Around API Tools Increases Friction And Failure</title>
    <link href="http://apievangelist.com/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/"/>
    <updated></updated>
    <content><![CDATA[
I know you believe your tools are the best. I mean, from your vantage point, they are. I also know that when you are building a new API tool, your investors want you to position your tooling as the best. The one solution to rule them all. They want you to work hard to make your tools the best, but also make sure and demonize other tooling as being inferior, out of date, and something the dinosaurs use. I know this absolute belief in your tooling feels good and right, but you are actually creating friction for your users, and potentially failure or at least conflict within their teams. Absolutism, along with divide and conquer strategies for evangelizing API tooling works for great short term financial strategies, but doesn’t do much to help us on the ground actually developing, managing, and sustaining APIs.

Ironically, there are many divers factors that contribute to why API tooling providers and their followers resort to absolutism when it comes to marketing and evangelizing their tools. Much of which has very little to do with the merits of the tools being discussed, and everything about those who are making the tools. I wanted to explore a few of them so they are available on the tip of my tongue while working within the enterprise.


  No Due Diligence On What Is Out There - Most startups who are developing API tooling do not spend the time understanding what already exists across the landscape, and get outside of the echo chamber to learn what real world companies are using to get the job done each day.
  No Learning Around Using Existing Tools - Even if startups are aware of existing tools, patterns, and processes, they rarely invest the time to actually understand what existing tools deliver—spending time to deeply understand how existing tools are being put to use by their would-be customers.
  Lack Of Awareness Around The Problem - There is a reason investors prefer young engineers when it comes to developing the next set of disruptive tooling, because they rarely understand the scope of problems being solved, and provide great fuel for short to mid-term growth strategies.
  Aggressive Male Dominated Environment - Young white men are perfect for this approach to delivering tooling that isn’t about the tool, but about a larger economic strategy, putting us passionate, privileged souls at the helm, and push them to do the disruptive bidding with very little awareness of the big picture.
  No Empathy For Others You Encounter - API tooling that takes an absolutist approach is rarely about empowering others, or understanding and providing solutions to their problems—lacking in empathy for other tooling providers, tooling consumers, or the companies left with each round of tech debt.
  Lack Of Diverse Experience In Industry - Entrepreneurs who ride each wave of API tooling absolutism and state their API tool is the one solution often lack experience in a variety of industries, and rarely have diverse experience outside of the - Silicon Valley echo chamber, and across multiple industries or geographic regions.
  VC Backed With Aggressive Growth - The aggressive absolutist approach of each wave of API tooling is almost always fueled by aggressive funding cycles, and have very little to do with the actual application of API tooling—operating the puppet strings which most API tooling providers and consumers on the front line do not see.


If you are in the business of tearing down someone else to deliver your tool, your tool will die by the same approach–someday. There is always a better funded, more aggressive solution to emerge on the market. Even if your tool has managed to achieve some level of market success, there will be a time when you let your guard down, and someone comes along to begin taking jabs at you. With each cycle of absolutism assault, the merits of the tooling mean very little. Perception always trumps reality, and there are always armies of developers waiting by in the wings to adopt what is new, and begin raising a pitchfork to attack what was. There is no allegiance and loyalty in this game.

I know. I know. This is just business. I just don’t get the game. Smart people have to make money! Yes, there are also many of us who are responsible for keeping the lights on. That aren’t as disloyal as you are, willing to jump from job to job, startup to startup. There are many of us who have been doing this a lot longer than you, and are willing to be responsible for the tech debt we incur along the way, and we do not mind doing the hard work to clean up your messes. I know that API tool absolutism makes you feel knowledgable and in control now, but just wait until you’ve ridden a few waves, and you’ve had many of your valuable tooling taken away from you because of this game. Then you will begin to see the other side of this, and better understand the toll of this business approach. Eventually you will grow weary of it, but fortunate for you, there will always be a fresh crop of recruits to wage this battle, and there is no rest for the wicked. #liveByDisruption #DieByDisruption
]]></content>
    <id>http://apievangelist.com/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/</id>
  </entry><entry>
    <title>The Higher Level Business Politics That I Am Not Good At Seeing In The API Space</title>
    <link href="http://apievangelist.com/2019/07/23/the-higher-level-business-poitics-that-i-am-not-good-at-seeing-in-the-api-space/"/>
    <updated></updated>
    <content><![CDATA[
I have built successful startups. I’m good at the technology of delivering new solutions. I am decent at understanding and delivering much of business side of bringing new technological solutions to market. What I’m not good at is the higher level business politics that occur. These are the invisible forces behind businesses that I’m good at seeing, playing in, and almost always catch me off guard, too late, or just simply piss me off that they are going on behind the scenes. Unfortunately it is in this realm where most of the money is to be made doing APIs, resulting in me being written out of numerous successful API efforts, because I’m not up to speed on what is going on.

Startups are great vehicles for higher level economic strategies. They are the playground of people with access to resources, and have economic visions that rarely jive with what is happening on the ground floors. Startup strategies count on a handful at the top understanding the vision, with most at the bottom levels not being able to see the vision, and small group of disposable stooges in the middle, ensuring that the top level vision is realized—at all costs. You can work full time at a startup, and even enjoy a higher level position, and still never see the political goings on that are actually motivating the investment in your startup. This is by design. The whole process depends on the lower levels working themselves to the bone, working on, marketing, and selling one vision, while there are actually many other things going on above, usually with a whole other set of numbers.

After 30 years of playing in this game I still stuck at seeing the higher level influences. I’ve seen shiny API tooling solution after shiny API tooling solution arrive on the market, and I still fall for the shine. Ooooohhh, look at that. It will solve X, or Y problem. I really like the vision of those team members. Their timing is perfect. They seem to have the right funding, and mindshare of developers. Then I begin to see some of the usual tell-tale signs of direction coming from up above. It will be subtle signals, like the change in pricing tiers, a quickness to support a standard on import, but very slow to support export. A shift in the marketing strategy. A public “pivot”. There are a diverse of signals you can tune into that will help predict where an API startup is headed, often times away from the original tooling vision, and the needs of the end-users.

With so much experience, you’d think I’d be better at this. I’m not good at it, because I hate playing these games. I like making money, but not in the way that follow the usual VC fueled playbook. To make money at scale you have to be willing to play by multiple playbooks, keeping one or more of them secret from your teams and end-users. This just isn’t me. I prefer being more transparent. I like building real businesses. I like developing real tools. This is what I’m good at. I’m not good at the higher level games required to build wealth for myself or others. It is this reality that leaves me so reluctant to share my knowledge with VCs, talk to and support new startups, and leaves me so cranky on a regular basis when I tell stories in the space. I know y’all think this is business as usual, and are looking to get your piece of the pie, but I operate at a different level, and refuse to go there.
]]></content>
    <id>http://apievangelist.com/2019/07/23/the-higher-level-business-poitics-that-i-am-not-good-at-seeing-in-the-api-space/</id>
  </entry><entry>
    <title>API Provider And Consumer Developer Portals</title>
    <link href="http://apievangelist.com/2019/07/23/api-provider-and-consumer-developer-portals/"/>
    <updated></updated>
    <content><![CDATA[
I’ve been studying API developer portals for almost a decade. I’ve visited the landing pages, portals, websites, and other incarnations from thousands of API providers. I have an intimate understanding of what is needed for API providers to attract, support, and empower API consumers. One area I’m deficient in, and I also think it reflects a wider deficiency in the API space, is regarding how to you make an API portal service both API providers and API consumers. Providing a single portal within the enterprise where everyone can come and understand how to deliver or consume an API.

There are plenty of examples out there now when it comes to publishing an API portal for your consumers, but only a few that show you how to publish an API. I’d say the most common example are API marketplaces that allow both API consumers and providers to coexist, but this model isn’t exactly what you want within the enterprise. One thing the model lacks is the on-boarding of new developers when it comes to actually developing an API. Suffering from many of the same same symptoms API management service providers have historically suffered from—-not providing true assistance when it comes to delivering a quality API.

When I envision an API portal that serves both providers and consumers, either publicly or privately, I envision just as much assistance when it comes to delivering a new API as we provide for new consumers of an API. Helping with API definition, design, deployment, management, testing, monitoring, documentation, and other critical stops along the API lifecycle. We need to see more examples of the split between API provider and consumers, equally helping both sides of the coin get up to speed, and be successful with what they are looking to achieve. I think we’ve spend almost 15 years investing in perfecting and monetizing the API portal with a focus not he consumer, and now we need to invest on helping make the portal easier for new API providers to step up and learn how to properly publish their API.

The modern API management solution is still tailored for the mystical API provider who knows how do to everything, where most do not understand the full API lifecycle. It would be an opportunity for an API management provider to go beyond just one or a handful of stops along the API lifecycle, and properly invest in on boarding new APIs. I think one reason why all of this suffers is that venture capitalists have never prioritized education and training for both API providers or consumers—-directing API service providers to only lightly invest when it comes to these API educational resources. Now that APIs have gone mainstream, we are going to need an industrial grade enterprise solution for delivering API portals that help onboard both API providers and consumers, and provide them both with what they need to navigate the entire lifecycle of the API solutions they are providing and applying.
]]></content>
    <id>http://apievangelist.com/2019/07/23/api-provider-and-consumer-developer-portals/</id>
  </entry><entry>
    <title>The Role Having Awareness Of Your API Traffic Plays In API Security</title>
    <link href="http://apievangelist.com/2019/07/22/the-role-having-awareness-of-your-api-traffic-plays-in-api-security/"/>
    <updated></updated>
    <content><![CDATA[
One of the biggest reasons we adopt new technology, and justify the development of new technology, is we do not want to do the heavy lifting when it comes to what we already have in place. A common illness when it comes to API security that I’ve been battling since 2015 is that you will have API security addressed once you adopted an API management solution. Your APIs require API keys, and thus they are secure. No further work necessary. The unwillingness or lack of knowledge regarding what is needed next, leaves a vacuum for new technology providers to come in and sell you the solution for what is next, when you should be doing more work to use the tools you already have.

When it comes to API management, most vendors sold it as purely a security solution, and when companies implement it they become secure. Missing the entire point for why we do API management-—to develop an awareness of our API usage and consumption. Having keys for your APIs is not enough. You actually have to understand how those API consumers are putting API resources to work, otherwise your API security will always be deficient. Some of the fundamentals of API management you should be employing as part of your API security are:


  Registration - Make all developers sign of for API usage, establishing the terms of use.
  API Keys - Require all developers internal or external to use API keys for every application.
  API Usage - Which APIs are being used by all API consumers putting them to use in applications.
  API Errors - Understanding what the errors being generated are, and who is responsible for them.
  Logging - The logging of all API traffic, reconciling against what you know as reported usage.
  Invoicing - Invoicing of all consumers for their usage, even if they aren’t paying you money.
  Reporting - Provide reports on API usage for all stakeholders, to regularly develop awareness.


These are the fundamentals of API management, however API keys and tokens seem to be the part that people feel is API security. Where API security is really all about actually developing a real-time awareness of who is using your API resources. Leaving your finger on the pulse so that when anything changes, or error rates are elevated, you already have a base level of awareness and can easily respond by shutting off keys, or limiting overall access to resources by offending applications.

There is much more that can be done in the name of API security. This is just a list of the elements of API management that contribute to API security, which are often neglected. Having API management does not equal API security. Properly applying API management contributes to API security, it is never API security by itself. If you aren’t doing API management properly, you are more likely to fall for the next generation of API security providers who are machine learning focused, promising to do the hard work of managing awareness for you, so you don’t have to. Your unwillingness to do the work in the first place, and properly understand the role that awareness of your traffic, makes you a ripe target for selling the next wave of API security solutionism. Good luck with that!
]]></content>
    <id>http://apievangelist.com/2019/07/22/the-role-having-awareness-of-your-api-traffic-plays-in-api-security/</id>
  </entry><entry>
    <title>Happy Path API Testing Bias</title>
    <link href="http://apievangelist.com/2019/07/22/happy-path-api-testing-bias/"/>
    <updated></updated>
    <content><![CDATA[
I see a lot of happy path bias when it comes to the development of APIs, but specifically when it comes to crafting testing to ensure APIs are delivering as expected. Happy path is a term used in testing to describe the desired outputs a developer and product owner is looking for. Making the not so happy path being about testing for outcomes that a developer and product owner is not wanting to occur. When it comes to API development most developers and product owners are only interested in the happy path, and will almost always cut corners, minimize the investment in, or completely lack an imagination when it comes to less than happy path API testing.

There are many reasons why someone will have a bias towards the happy path when developing an API. Every API provider is invested in achieving the happy path for delivering, providing, and consuming an API. This is what generates revenue. However, in this quest for revenue, we often become our own worst enemy. Shining a spotlight on the happy path, while being completely oblivious to what the not so happy paths will look like for end users. Why do we do this?


  Greed - We are so interested in getting an API up and running, used in our applications, and generating behavioral surplus, we are more than willing to ignore all other possible scenarios if we can easily meet our revenue goals by ignoring the unhappy path and there are no consequences.
  Tickets - Most development occurs using JIRA or other software development “tickets”, which tell developers what they are supposed to do to meet the requirements of their employment—tickets are written with the happy path in mind, and developers are rarely willing to do more.
  Imagination - While many of us technologists think we are imaginative creatures, most of us are pretty stuck in a computational way of thinking, and elaborating, iterating, and exploring beyond the initial happy path design of our API just does not exist.
  Use Software - Most of us developers do not actually use the platform we are developing, setting the stage for where we really don’t understand the problem being solve, further siloing us into seeing only the happy path that have been handed to us as part of initial product vision.
  White Male - The majority of us API developers are white men, or developers who report to white men, leaving entire shadows regarding how our APIs will be used and abused—when you are privileged, the happy path is always easier to see and walk on.
  Apathy - The majority of us are just doing our jobs, and we really do not have any excitement, passion, or interest in our jobs. We are just doing what we are told, and if our bosses do not specifically point out every single unhappy path, we don’t care.
  Velocity - Things move fast at almost any company delivering APIs, and it is easy to not have time to be able to step back and sufficiently think about what the happy paths might be when we are delivering APIs that deliver some functionality amidst a fast pace environment.
  Experience - Another reason for overlooking unhappy paths is we just do not have the experience to know about them. Startups and many technology focused companies like hiring young, low pay developers to get the job done, and they won’t always have the experience to see in the shadows.
  By Design - The product owners do not want the less than happy or unhappy paths patched, as they are there by design, and support the overall business model, which is usually advertising. Encouraging abuse, and exploitation of APIs, or at least ensuring they are much lower priorities.


There are few incentives to develop quality software these days. Revenue drives much of why we are delivering APIs, and incentivizing developers to think out of the box when it comes to API testing just doesn’t exist. Plus, it takes a lot of work to write first class tests alongside your code. Most developers are conditioned to see tests as secondary, and the thing you do only when you have the time. Making quality unhappy and less than happy path API testing always left on the cutting room floor, never making it into the final product.

You can see this bias playing out in the APIs behind Facebook, Twitter, and other advertising driven platforms. The abuse of APIs are often overlooked if it generates clicks, traffic, and increases the eyeballs. Secondarily I’d say that the consequences for when unhappy paths are identified for APIs is almost non-existent. There is no accountability for poorly designed APIs, or APIs that allow for uses beyond their intended purpose. In this environment, most API providers will never prioritize API testing, and incentivize developers to properly explore how an API can be misused, abused, or just not deliver the functionality promised. Ensuring that much of API usage exists on the unhappy path by design.
]]></content>
    <id>http://apievangelist.com/2019/07/22/happy-path-api-testing-bias/</id>
  </entry><entry>
    <title>What Is An Application?</title>
    <link href="http://apievangelist.com/2019/07/18/what-is-an-appication/"/>
    <updated></updated>
    <content><![CDATA[
I have struggled asking this question in many discussions I’ve had around the world, at technology conferences, on consulting projects, and in the back rooms of dimly lit bars. What is an application? You get ten different answers if you ask this question to ten different people. I’d say the most common response is to reference the applications on a mobile device. These are the relevant. Most accessible. The most active and visible form of application in our modern life. Older programmers see them as desktop applications, where younger programmers see them as web applications, with varying grades of server applications in between. If you operate at the network layer, you’ve undoubtedly bastardized the term to mean several different things. Personally, I’ve always tried to avoid these obvious and tangible answers to this question, looking beyond the technology.

My view of what an application is stems from a decade of studying the least visible, and least tangible aspect of an application, its programming interface. When talking to people about applications, the first question I ask folks is usually, “do you know what an API is”? If someone is API savvy I will move to asking, “when it comes to application programming interface (API), who or what is being programmed? Is it the platform? The application? Or, is it the end-user of the applications?” I’ve spent a decade thinking about this question, playing it over and over in my end, never quite being satisfied with what I find. Honestly, the more I scratch, the more concerned I get, and the more I’m unsure of exactly what an “application” is, and precisely who are what is actually being programmed. Let’s further this line of thinking by looking at the definitions of “application”:


  noun - The act of applying.
  noun - The act of putting something to a special use or purpose.
  noun - A specific use to which something is put.
  noun - The capacity of being usable; relevance.
  noun - Close attention; diligence.
  noun - A request, as for assistance, employment, or admission to a school.
  noun - The form or document on which such a request is made.
  noun - Computers A computer program designed for a specific task or use.
  adjective - Of or being a computer program designed for a specific task or use.


This list comes from my friends over at Wordnik (https://www.wordnik.com/words/application), who I adore, have an API (https://developer.wordnik.com/), and who have contributed to this discussion by playing a leading role in introducing the API sector to the OpenAPI (fka Swagger) specification to the API community. That is whole other layer of significance when it comes to the semantics and meaning behind the word application, which I will have to write more about in a future post. In short, words matter. Really, words are everything. The meaning behind them. The intent. The wider understanding and belief. This is why APIs, and web applications like Wording are important. Helping us make sense of the world around us. Anyways, back to what is an application?

I like the first entry on the list — the act of applying. But, each of these definitions resonate with my view of the landscape. Yet, I’d say that the most common answer to this question hover towards the second half of this list, not the first half of it. Most application developers would say they are programming the application—the interface is for them. If you are an API developer you believe that are the one programming the application, where higher up on the platform decision making chain, they are the ones programming the application, and applying their vision of the world. Something that isn’t always visible at the lower levels, by API developers delivering APIs, the application developers consuming them, or the end-users of the tangible applications being delivered. I see the end desktop, web, or mobile as an application. I see the API as an application. I see the network connecting the two as an application. I also see the wider ideology being applied across all these layers, even when it is out of view to the outer layers.

One of the biggest imbalances in my belief system around technology, a result of be operating at the lower levels of business, institutional, or government, is that I am the one “applying” and “programming”. I was developing and delivering the application. These interfaces served me. After studying the machine closer. Tracking on the cycles. Documenting the results over the course of many cycles. I began to realize that there is more to this “application” thing than what I”m seeing. Maybe I was too close to the gears and the noise of the machine to see the bigger picture. I’m in the role of application developer not because I’m good at what I do. I’m there because I conveniently think I’m in control of this supply chain. As I worked my way up the supply chain, and became an API developer I continue to believe that I was the one “applying” and “programming”. However after over a decade of doing that, I’m realizing that I am not the one calling the shots. I’m applying something for someone else, and that applications were much more than just an iPhone or Android application, or even a TCP, FTP, STMP, HTTP, HTTP/2, or other protocol application. How you answer this questions depends on where you operate within the machine.

After climbing my way up through the layers of the machine, and finding my way to hatch at the top, finally getting some air in my lungs—-I still do not find myself in a better position. With more knowledge, just comes more concerns. Sometimes I wish I could climb back down to the lower levels, and enjoy the warmth and comfort of the inner workings of the machine. However, no I can’t find comfort in the toil that used to comfort me online late into the night. I can’t ignore what we are applying when we work in the service of the application. We are doing the hard work to mine, develop, integrate, and extract value. We are doing the dirty work of applying the vision of others. We are doing the hard work of laying the digital railroad tracks for the Internet tycoons. We are imposing their vision on the world. The special use or purpose of APIs do not always serve us. The usability and relevance is only minimally focused on us. The close attention and diligence is centered on maximum extraction and value generation for the platform. All wrapped in a computer program, designed for a specific task or use. Obfuscating the true application, with digital eye candy that keeps us always connected, and always open to something new being applied or directed in our life.
]]></content>
    <id>http://apievangelist.com/2019/07/18/what-is-an-appication/</id>
  </entry><entry>
    <title>What Makes You Think Your GraphQL Consumers Will Want To Do The Work</title>
    <link href="http://apievangelist.com/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/"/>
    <updated></updated>
    <content><![CDATA[
Data work is grueling work. I’ve been working with databases since my first job developing student information databases in 1988 (don’t tell my wife). I’ve worked with Cobol, Foxpro, SQL Server, Filemaker, Access, MySQL, PostGres, and now Aurora databases over the last 30 years. I like data. I can even trick myself into doing massive data and schema refinement tasks on a regular basis. It is still grueling work that I rarely look forward to doing. Every company I’ve worked for has a big data problem. Data is not easy work, and the more data you accumulate, the more this work grows out of control. Getting teams of people to agree upon what needs to happen when it comes to schema and data storage, and actually execute upon the work in a timely, cost effective, and efficient way is almost always an impossible task. Which leaves me questioning (again), why GraphQL believers think they are going to be successfully in offshoring the cognitive and tangible work load to understand what data delivers, and then successfully apply it to a meaningful and monetizable business outcome.

Don’t get me wrong. I get the use cases where GraphQL makes sense. Where you have an almost rabid base of known consumers, who have a grasp on the data in play, and possesses an awareness of the schema behind. I’m have made the case for GraphQL as a key architectural component within a platform before. The difference in my approach over the majority of GraphQL believers, is that I’m acknowledging there is a specific group of savvy folks who need access to data, and understand the schema. I’m also being honest about who ends up doing the heavy data lifting here—-making sure this group wants it. However, I have an entirely separate group of users (the majority) who do not understand the schema, and have zero interest in doing the hard work to understand the schema, navigate relationships, and develop queries—-they just want access. Now. They don’t want to have to think about the big picture, they want one single bit of data, or a series of bits.

I’ve worked hard to engage in debate with GraphQL believers, and try to help provide them with advice on their approach. They aren’t interested. They operate within the echo chamber. They see a narrow slice of the API pie, and passionately believe their tool is the one solution. Like many API tooling peddlers who originate from within the echo chamber, they are hyper focused on the technology of managing our data. They think all data wranglers are like them. They think all data-driven companies are like theirs. They do not see the diverse types of API solutions that I see working across companies, institutions, organizations, and government agencies. They are not always aware of the business and political barriers that lie in between a belief in an API tool, and achieving a sustained implementation across the enterprise. They have that aggressive, tenacious startup way of penetrating operations, something that works well within the echo chamber, but it is an approach that will lose strength, and even begin to work against you outside the eco chamber within mainstream business operations.

I definitely see some interesting and useful tooling coming out of the GraphQL community. GraphQL is a tool in my API toolbox right along with REST, Hypermedia, Siren, HAL, JSON API, Alps, JSON Schema, Schema.org, OpenAPI, Postman Collections, Async API, Webhooks, Kafka, NATS, NGINX, Docker, and others. It has a purpose. It isn’t the silver bullet for me getting a handle on large amounts of data. It is one thing I consider when I’m trying to make sense of large data sets, and depending on the platform, the resources I have on staff, and the consumers, or the industry I am operating in–I MAY apply GraphQL. However, the times I will be able to successfully get it in the door, past legal, approved by leadership, accepted by internal developers, and then accepted and applied successfully by external developers, will be much fewer because of the aggressive echo chamber, investor-driven approach, which does not help sell the tool to my enterprise customers who have been investing in evolving their schema, and developing a suite of internal and external APIs over the last 20+ years. You might help me sell to a Silicon Valley savvy company, but you aren’t helping me in the mainstream enterprise.

This post will get the usual lineup of critical Tweets and comments. It’s fine. Once again the fact that I’m trying to help will be lost on believers. You will be more successful if you are honest about how the cognitive and  tangible workload is being shifted here. You are refusing to do the hard work to properly define and organize your schema, and provide meaningful imperative API capabilities that any developer can easily use, over providing a single declarative interface (and some tooling) to empower consumers to make sense of the schema, and access platform capabilities on their own. This works well with folks who are willing to take on the cognitive load of knowing the schema, knowing GraphQL, then are willing to accept the real work of crafting queries to get at what they desire. There are a whole lot of assumptions there that do not apply in all situations. I’d say about 12% of people I’ve worked with in my career would sign up for this. The rest of them, just want to get their task accomplished—get out of their way and give them an interface that is capable of accomplishing it for them.

Nobody wants to do data shit work. If you do, get help. It is a job that is only growing because our ability to generate, harvest, and store data has grown. There is a belief that data provides answers, without being really honest about what it takes to actually clean, refine, and organize the data, let alone any truthfulness regarding whether or not the answers are ever even there after we do invest in doing the dirty data work. Everyone is drowning in data. Everyone is chasing more data. Few are managing it well. This type of environment makes new data tooling very sexy. However, sexy tools rarely change the actual behavior on the ground within the enterprise. People still aren’t going to change behaviors overnight. GraphQL tooling will not solve all of our data problems. Ideally, we would see more interoperability of tooling between GraphQL and other API design, deployment, management, testing, monitoring, mocking, and client tooling. Like we are seeing with IBM Loopback, Postman, and others. Ideally, we would see less rhetoric around GraphQL being the one solution to rule them all. Ideally, we’d see less investor-driven rhetoric, and more real world solutions for applying through the mainstream business world. But, I’m guessing I’ll see the same response I’ve been getting on these posts since 2016. ;-(
]]></content>
    <id>http://apievangelist.com/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/</id>
  </entry><entry>
    <title>The Many Ways In Which APIs Are Taken Away</title>
    <link href="http://apievangelist.com/2019/07/17/the-many-ways-in-which-apis-are-taken-away/"/>
    <updated></updated>
    <content><![CDATA[
APIs are notorious for going away. There are so many APIs that disappear I really stopped tracking on it as a data point. I used keep track of APIs that were shuttered so that I could play a role in the waves of disgruntled pitchfork mobs rising up in their wake–it used to be a great way to build up your Hacker News points! But, after riding the wave a couple hundred waves of APIs shuttering, you begin to not really not give a shit anymore—-growing numb to it all. API deprecation grew so frequent, I wondered why anyone would make the claim that once you start an API you have to maintain it forever. Nope, you can shut down anytime. Clearly.

In the real world, APIs going away is a fact of life, but rarely a boolean value, or black and white. There are many high profile API disappearances and uprising, but there are also numerous ways in which some API providers giveth, and then taketh away from API consumers.:


  Deprecation - APIs disappear regularly both communicated, and not so communicated, leaving consumers scratching their heads.
  Disappear - Companies regularly disappear specific API endpoints acting like they were never there in the first place.
  Acquisition - This is probably one of the most common ways in which high profile, successful APIs disappear.
  Rate Limits - You can always rate limit away users make APIs inaccessible, or barely usable for users, essentially making it go away.
  Error Rates - Inject elevated error rates either intentionally or unintentionally can make an API unusable to everyone or select audience.
  Pricing Tiers - You can easily be priced out of access to an API making it something that acts just like deprecating for a specific group.
  Versions - With rapid versioning, usually comes rapid deprecation of earlier versions, moving faster than some consumers can handle.
  Enterprise - APIs moving from free or paid tier, into the magical enterprise, “call us” tier is a common ways in which APIs go away.
  Dumb - The API should not have existed in the first place and some people just take a while to realize it, and then shut down the API.


I’d say Facebook, Twitter, and Google shutting down, or playing games in any of these areas have been some of the highest profile. The sneaky shuttering of the Facebook Audience Insight API was one example, but didn’t get much attention. I’d say that Parse is one that Facebook handled pretty well. Google did it with Google+ and Google Translate, but then brought back it with a paid tier. LinkedIn regularly locks down and disappears its APIs. Twitter has also received a lot of flack for limiting, restricting, and playing games with their APIs. In the end, many other APIs shutter after waves of acquisitions, leaving us with as my ex-wife says–“nothing nice”!

Face.com, Netflix, 23andMe, Google Search, ESPN, and others have provided us with lots of good API deprecation stories, but in reality, most APIs go aware without much fanfare. You are more likely to get squeezed out by rate limits, errors, pricing, and other ways of consciously making an API unusable. If you really want to understand the scope of API deprecation visit the leading deprecated API directory ProgrammableWeb—-they have thousands of APIs listed that do not exist anymore. In the end it is very difficult to successfully operate an API, and most API providers really aren’t in it for the long haul. The reasons why APIs stay in existence are rarely a direct result of them being directly financially viable. Developers squawk pretty loudly when the free API they’ve been mooching off of disappears, but there are many, many, many other APIs that go away and nobody notices, or nobody talks about. In the world of APIs there are very few things you can count on being around for very long, and you should always have a plan B and C for every API you depend on.
]]></content>
    <id>http://apievangelist.com/2019/07/17/the-many-ways-in-which-apis-are-taken-away/</id>
  </entry><entry>
    <title>Paying for API Access</title>
    <link href="http://apievangelist.com/2019/07/17/paying-for-api-access/"/>
    <updated></updated>
    <content><![CDATA[
APIs that I can’t pay for more access grinds my gears. I am looking at you GitHub, Twitter, Facebook, and a few others. I spend $250.00 to $1500.00 a month on my Amazon bill, depending on what I’m looking to get done. I know I’m not the target audience for all of these platforms, but I’m guessing there is a lot more money on the table than is being acknowledged. I’m guessing that the reason companies don’t cater to this, is that there are larger buckets of money involved in what they are chasing behind the scenes. Regardless, there isn’t enough money coming my way to keep my mouth shut, so I will keep bitching about this one alongside the inaccessible pricing tiers startups like to employ as well. I’m going to keep kvetching about API pricing until we are all plugged into the matrix—-unless the right PayPal payment lands in my account, then I’ll shut up. ;-)

I know. I know. I’m not playing in all your reindeer startup games, and I don’t understand the masterful business strategy you’ve all crafted to get rich. I’m just trying to do something simple like publish data to GitHub, or do some basic research on an industry using Twitter. I know there are plenty of bad actors out there who want also access to your data, but it is all something that could be remedied with a little pay as you go pricing, applied to some base unit of cost applied to your resources. If I could pay for more Twitter and GitHub requests without having to be in the enterprise club, I’d be very appreciative. I know that Twitter has begun expanding into this area, but it is something that is priced out of my reach, and not the simple pay as you go pricing I prefer with AWS, Bing, and other APIs I happily spend money on.

If you can’t apply a unit of value to your API resources, and make them available to the masses in a straightforward way—-I immediately assume you are up to shady tings. Your business model is much more loftier than a mere mortal like me can grasp, let alone afford. I am just an insignificant raw material in your supply chain—-just be quiet! However, this isn’t rocket science. I can’t pay for Google Searches, but I can pay for Bing searches. I can’t pay for my GitHub API calls. I’m guessing at some point I’ll see Bing pricing go out of reach as Microsoft continues to realize the importance of investing at scale in the surveillance economy—-it is how you play in the big leagues. Or maybe they’ll be like Amazon, and realize they can still lead in the surveillance game while also selling things to the us lower level doozers who are actually building things. You can still mine data on what we are doing and establish your behavioral models for use in your road map, while still generating revenue by selling us lower level services.

The problem here ultimately isn’t these platforms. It is me. Why the hell do I keep insisting on using these platforms. I can always extricate myself from them. I just have to do it. I’d much rather just pay for my API calls, and still give up my behavioral data, over straight extraction and not getting what I need to run my business each day. I feel like the free model, with no access to pay for more API calls is a sign of a rookie operation. If you really want to operate at scale, you should be obfuscating your true intentions with a real paid service. If you are still hiding behind the free model, you are just getting going. The grownups are already selling services for a fair price as a front, while still exploiting us at levels we can’t see, so ultimately they can compete with all of us down the road. Ok, in all seriousness, why can’t we get on a common model for defining API access, and setting pricing. I’m really tired of all the games. It would really simplify my life if I could pay for what I use of any resource. I’m happy to pay a premium for this model, just make sure it is within my reach. Thanks.
]]></content>
    <id>http://apievangelist.com/2019/07/17/paying-for-api-access/</id>
  </entry><entry>
    <title>Imperative, Declarative, and Workflow APIs</title>
    <link href="http://apievangelist.com/2019/07/16/imperative-declarative-and-workflow-apis/"/>
    <updated></updated>
    <content><![CDATA[
At every turn in my API work I come across folks who claim that declarative APIs solutions are superior to imperative ones. They want comprehensive, single implementation, do it all their way approaches, over granular, multiple implementation API calls that are well defined by the platform. Declarative calls allow you to define a single JSON or YAML declaration that can then be consumed to accomplish many things, abstracting away the complexity of doing those many things, and just getting it done. Imperative API interfaces require many individual API calls to tweak each and every knob or dial on the system, but is something that is often viewed as more cumbersome from a seasoned integrator, but for newer, and lower level integrators a well designed imperative API can be an important lifeline.

Declarative APIs are almost always positioned against imperative APIs. Savvier, more experienced developers almost always want declarations. Where newer developers and those without a full view of the landscape, often prefer well designed imperative APIs that do one thing well. From my experience, I always try to frame the debate as imperative and declarative where the most vocal developers on the subject prefer to frame it as declarative vs imperative. I regularly have seasoned API developers “declare” that I am crazy for defining every knob and dial of an API resource, without any regard for use cases beyond what they see. They know the landscape, don’t want to be burdened them with having to pull every knob and dial, just give them one interface to declare everything they desire. A single endpoint with massive JSON or YAML post or abstract it all away with an SDK, Ansible, GraphQL, or a Terraform solution. Demanding that a platform meet their needs, without ever considering how more advanced solutions are delivered and executed, or the lower level folks who are on boarding with a platform, and may not have the same view of what is required to operate at that scale.

I am all for providing declarative API solutions for advanced users, however, not at the expense of the new developers, or the lower level ones who spend their day wiring together each individual knob or dial, so it can be used individually, or abstracted away as part of more declarative solution. I find myself regularly being the voice for these people, even though I myself, prefer a declarative solution. I see the need for both imperative and declarative, and understand the importance of good imperative API design to drive and orchestrate quality declarative API implementations, and even more flexible workflow solutions, which in my experience are often what processes, breaks down, and executes most declarations that are fed into a system. The challenge in these discussions are that the people in the know, who want the declarative solutions, are always the loudest and usually higher up on the food chain when it comes to getting things done. They can usually rock the boat, command the room, and dictate how things will get delivered, rarely ever taking into consideration the full scope of the landscape, especially what lower level people encounter in their daily work.

For me, the quality of an API always starts with the imperative design, and how well you think through the developer experience around every dial and knob, which will ultimately work to serve (or not), a more declarative and workflow approach. They all work together. If you dismiss the imperative, and bury it within an SDK, Ansible, or Terraform solution, you will end up with an inferior end solution. They all have to work in concert, and at some point you will have to be down in the weeds turning knobs and dials, understanding what is possible to orchestrate the overall solution we want. It is all about ensuring there is observability and awareness in how our API solutions work, while providing very granular approaches to putting them to work, while also ensuring there are simple, useable comprehensive approaches to moving mountains with hundreds or thousands of APIs. To do this properly, you can’t be dismissing the importance of imperative API design, in the service of your declarative—-if you do this, you will cannibalize the developer experience at the lower levels, and eventually your declarations will become incomplete, clunky, and do not deliver the correct “big picture” vision you will need of the landscape.

When talking API strategy with folks I can always tell how isolated someone is based upon whether they see it as declarative vs imperative, or declarative and imperative. If it is the former, they aren’t very concerned with with others needs. Something that will undoubtedly introduce friction as the API solutions being delivered, because they aren’t concerned with the finer details of API design, or the perspectives of the more junior level, or newer developers to the conversation. They see these workers in the same way they see imperative APIs, something that should be abstracted away, and is of no concern for them. Just make it work. Something that will get us to the same place our earlier command and control, waterfall, monolith software development practices have left us. With massive, immovable, monolith solutions that are comprehensive and known by a few, but ultimately cannot pivot, change, evolve, or be used in new ways because you have declared one or two ways in with the platform should be used. Rather than making things more modular, flexible, and orchestrate-able, where anyone can craft (and declare) a new way of stitching things together, painting an entirely new picture of the landscape with the same knobs and dials used before, but done so in a different way than was ever conceived by previous API architects.
]]></content>
    <id>http://apievangelist.com/2019/07/16/imperative-declarative-and-workflow-apis/</id>
  </entry><entry>
    <title>Hoping For More Investment In API Design Tooling</title>
    <link href="http://apievangelist.com/2019/07/16/hoping-for-more-investment-in-api-design-tooling/"/>
    <updated></updated>
    <content><![CDATA[
I was conducting an accounting of my API design toolbox, and realized it hasn’t changed much lately. It is still a very manual suite of tooling, and sometimes services, that help me craft my APIs. There are some areas I am actively investing in when it comes to API design, but honestly there really isn’t that much new out there to use. To help me remember how we got here, I wanted to take a quick walk through the history of API design, and check in on what is currently available when it comes to investing in your API design process.

API design has historically meant REST. Many folks still see it this way. While there has been plenty of books and articles on API design for over a decade, I attribute the birth of API design to Jakub and Z at Apiary (https://apiary.io). I believe they first cracked open the seed of API design, and the concept API design first. Which is one of the reasons I was so distraught when Oracle bought them. But we won’t go there. The scars run deep, and where has it got us? Hmm? Hmm?? Anyways, they set into motion an API design renaissance which has brought us a lot of interesting thinking on API design, a handful of tooling and services, some expansion on what API design means, but ultimately not a significant amount of movement overall.

Take a look at what AP+I Design (https://www.apidesign.com/) and API Plus (https://apiplus.com/) have done to architecture, API has done for the oil and gas industry (https://www.api.org/), and API4Design has done for the packaging industry (http://api4design.com/)—I am kidding. Don’t get me wrong, good things have happened. I am just saying we can do more. The brightest spot that represents the future for me is over at:


  Stoplight.io - They are not just moving forward API design, they are also investing in the full API lifecycle, including governance. I rarely plug startups, unless they are doing meaningful things, and Stoplight.io is.


After Stoplight.io, I’d say some of the open source tooling that still exists, and has been developed over the last five years, gives me the most hope that I will be able to efficiently design APIs at scale across teams:


  Swagger Editor - I still find myself using this tool for most of my quick API design work.
  API Stylebook - What Arnaud has done reflects what should be standard across all industries.
  OpenAPI GUI - A very useful and progressive OpenAPI GUI editor.
  Apicurio - Another useful GUI OpenAPI editor which shouldn’t be abandoned (cough cough).
  Web Concepts - The building blocks of every API design effort is located here.


I use these tools in my daily work, and think they reflect what I like to see when it comes to API design tooling investment. I would be neglectful if I didn’t give a shout out to a handful of companies doing good work in this area:


  Postman - While not coming from a pure API design vantage point, you can do some serious design work within Postman.
  APIMATIC - It takes good API design to deliver usable SDKs, and APIMATIC provides a nice set of design tooling for their services.
  Reprezen - They have invested heavily in their overall API design workflow and are a key player in the OpenAPI conversation.
  Restlet - My friends over at Restlet are still up to good things even thought they are part of Talend.


While I am not quite ready to showcase and highlight these companies because they don’t always reflect the positive API community influence I’d like to see, I don’t want to leave them out for what they bring to the table:


  SwaggerHub - They are doing interesting things, even if I’m still bummed over the whole Swagger -&amp;gt; OpenAPI bullshit, which I will never forget!
  Mulesoft - Their AnyPoint Designer is worth noting in this discussion. I will leave it there.


While writing this, and taking a fresh look at the search results for API design, editors, and tooling, and looking through my archives, I came across a couple players I have never seen before, either because I haven’t been tuned in, or because they are new:


  OpenAPI Designer - An interesting new player to the OpenAPI editor game.
  Visual Paradigm - Another interesting approach to delivering APIs – we may have to test drive.


I do not want to stop there. Maybe there are other API design toolbox forces at play, influencing, shifting, or directing the API design conversation in other ways, using different protocols and approaches:


  GraphQL - Maybe the GraphQL believers are right? Maybe it is the true solution to designing our APIs? What if? OMG
  Kafka - I think an event-driven approach has shifted the conversation for many, moving classic API design into the background.
  gRPC - Maybe we are moving towards a more HTTP/2 RPC way of delivering APIs and API design is becoming irrelevant.


It is possible that these solutions are siphoning off the conversation in new directions. Maybe the lack of investment is due to other influences that go well beyond the technology, or a specific approach to defining and designing the API problems. What might be some out of the box reasons the API design conversation hasn’t moved forward:


  Investment -  Most of the movement I’ve seen has occurred as well as stagnated because of the direction of venture capital.
  Hard - Maybe it is because it is hard, and nobody wants to do it, making it something that is difficult too monetize.
  Expertise - We need more training and the development of API design expertise to help lead the way, and show us how it is done.
  Bullshit - Maybe API design is bullshit, and we are delusional to think anything will ever become of API design in the first place.
  My Vision - Maybe my vision of API design tooling and services is too high of a bar, or unrealistic in some way.


Ultimately, I think API design is difficult. I think we need more investment in small open source API design tooling that do one thing well. I think other API paradigms will continue to distract us, but also potentially enrich us. I think my vision of API design is obtainable, but out of view of the current investment crowd. I think API design vision is either technical, or it is business. There is very little in between. This is why I highlight Stoplight.io. I feel they are critically thinking about not just API design, but also the rest of the lifecycle. I’d throw Postman into this mix, but they are more API lifecycle than pure design, but I do think they reflect more of the type of services and tooling I’d like to see.

I do not think resource centered web API design is going anywhere. From what I”m seeing, it is going mainstream. It is simple. Low cost. It gets the job done with minimal investment. I think we should invest more into open source API design solutions. I think we need continued investment in API design services like Stoplight.io, Postman, Reprezen, Restlet, and others. I think we need to shift the conversation to also include GraphQL, Kafka, and gRPC. I think investors can do a better job, but I”m not going to hold my breathe there. In the end, I go back to my hand-crafted, artisanal, API design workbench out back where I have cobbled together a few open source tools, on top of a Git foundation. Honestly, it is all I can afford, but I’ll keep playing with other tools I have access to, to see if something will shift my approach.
]]></content>
    <id>http://apievangelist.com/2019/07/16/hoping-for-more-investment-in-api-design-tooling/</id>
  </entry><entry>
    <title>What Is An API Contract?</title>
    <link href="http://apievangelist.com/2019/07/15/what-is-an-api-contract/"/>
    <updated></updated>
    <content><![CDATA[
I am big on regularly interrogating what I mean when I use certain phrases. I’ve caught myself repeating and reusing many hollow, empty, and meaningless phrases over my decade as the API Evangelist. One of these phrases is, “an API contract”. I use it a lot. I hear it a lot. What exactly do we mean by it? What is an API contract, and how is it different or similar to our beliefs and understanding around other types of contracts? Is it truth, or is just a way to convince people that what we are doing is just as legitimate as what came before? Maybe it is even more legitimate, like in a blockchain kind of way? It is an irreversible, unbreakable, digital contract think bro!

If I was to break down what I mean when I say API contract, I’d start with being able to establish to a shared articulation of what an API does. We have an OpenAPI definition which describes the surface area of the request and response of each individual API method being offered. It is available in a machine and human readable format for both of us to agree upon. It is something that both API provider and API consumer can agree upon, and get to work developing and delivering, and then integrating and consuming. An API contract is a shared understanding of what the capabilities of a digital interface are, allowing for applications to be programmed on top of.

After an API contract establishes a shared understanding, I’d say that an API contract helps mitigate change, or at leasts communicates it—-again, in a human and machine readable way. It is common practice to semantically version your API contracts, ensuring that you won’t remove or rename anything within a minor or patch release, committing to only changing things in a big way with each major release. Providing an OpenAPI of each version ahead of time, allowing consumers to review that new version of an API contract before they ever commit to integrating and moving to the next version. Helping reduce the amount of uncertainty that inevitably exists when an API changes, and consumers will have to respond with changes in their client API integrations.

Then I’d say an API contract moves into service level agreement (SLA) territory, and helps provide some guarantees around API reliability and stability. Moving beyond any single API, and also speaking to wider operations. An API contract represents a commitment to offering a reliable and stable service that is secure, observability, and the provider has consumers best interest in mind. A contract should reflect a balance between the provider and the consumer interests, and provide a machine and human readable agreement that reflects the shared understanding of what an API delivers—for an agreed upon price. Any API contract reflects the technical and business details of us doing business in this digital world.

Sadly, an API contract is often wielded in the name of all of these things, but there really is very little accountability or enforcement when it comes to API contracts. It is 100% up to the API provider to follow through and live up to the contract, with very little an API consumer can do if the contract isn’t met. Resulting in many badly behaved API providers, as well as monstrous API consumers. Right now, API contract is thrown around by executives, evangelists, analysts and pundits, more than they are ever actually used to govern what happens on the ground of API operations. Only time will tell if API contracts are just another buzzword that comes and goes, or if they become common place when it comes to doing business online in a digital world.
]]></content>
    <id>http://apievangelist.com/2019/07/15/what-is-an-api-contract/</id>
  </entry><entry>
    <title>My Primary API Search Engines</title>
    <link href="http://apievangelist.com/2019/07/12/my-primary-api-search-engines/"/>
    <updated></updated>
    <content><![CDATA[
I am building out several prototypes for the moving parts of an API search engine I want to build, pushing my usage of APIs.json and OpenAPI, but also trying to improve how I define, store, index, and retrieve valuable data about thousands of APIs through a simple search interface. I’m breaking out the actual indexing and search into their own areas, with rating system being another separate dimension, but even before I get there I have to actually develop the primary engines for my search prototypes, feeding the indexes with fresh signals of where APIs exist across the online landscape. There isn’t an adequate search engine out there, so I’m determined to jumpstart the conversation with an API search engine of my own. Something that is different from what web search engines do, and tailored to the unique mess we’ve created within the API industry.

My index of APIs.json and OpenAPI definitions, even with a slick search interface is just a catalog, directory, or static index of a small piece of the APIs that are out there. I see a true API search engine as three parts


  The Humans Searching for APIs - Providing humans with web application to search for new and familiar APIs.
  The Search Engine Searching For APIs - Ensuring that the search engine is regularly searching for new APIs.
  Other Systems Searching For APIs - Providing an API for other systems to search for new and familiar APIs.


Without the ability for the search engine to actually seek out new APIs, it isn’t a search engine in my opinion—-it is a search application. Without an API for searching for APIs, in my opinion, it isn’t an API search engine. It takes all three of these areas to make an application a true API search engine, otherwise we just have another catalog, directory, marketplace, or whatever you want to call it.

To help me put the engine into my API search engine, I’m starting with a handful of sources I’ve cultivated over the last five years studying the API industry. Providing me with some seriously rich sources of information when it comes to identifying new APIs:


  GitHub Code Search API - I have a vocabulary I use for uncovering artifacts that provide clues to where APIs exist. I can also expand this search to topics, repos, and other dimensions of GitHub search, but I’m going to make sure I’m exhausting and optimizing core search for all I can before I move on.  GitHub provides me with a handful of nuggets when it comes to finding APIs:
    
      Swagger - Machine readable JSON and YAML API definitions.
      OpenAPI - Machine readable JSON and YAML API definitions.
      Postman - Machine readable JSON API definitions.
      API Blueprint - Machine readable markdown definitions.
      RAML - Machine readable YAML definitions.
      HAR - Machine readable traffic snapshots.
      Domains - Domains doing interesting things with APIs.
      People - People doing interesting things with APIs.
    
  
  Bing Web Search API - I use my vocabulary to uncover domains and artifacts that provide clues to where APIs exist. Unlike GitHub, I have to pay for these API calls, so I’m being much more careful about how I spider, and coherently defining the vocabulary I use to uncover this landscape.  Bing provides me with a handful of nuggets when it comes to finding APIs:
    
      Domains - A look into many different domains who are talking APIs in specific verticals.
      GitHub - I find that Bing has some interesting indexes of GitHub — wondering how this will evolve.
    
  
  Twitter - I use my vocabulary to identify new domains where people are talking about APIs, and additional signs of APIs.
    
      Domains - A look into many different domains who are talking APIs in specific verticals.
      People - People doing interesting things with APIs.
    
  
  Domain - I have an exhaustive list of domains to spider for API artifacts, autogenerating OpenAPI index along the way.
    
      URLs - I look for a handful of valuable URLs for use as part of my index.
        
          Twitter - Their Twitter accounts.
          GitHub - Their GitHub accounts.
          LinkedIn - Their LinkedIn accounts.
          Feeds - Their Atom and RSS feeds.
          Definitions - Any API definitions I can find.
          Documentation - Where their documentation is.
          Other Links - I have a long list of other links I look for
        
      
    
  


These four areas represent the primary engines for my API search engine. I currently have these engines running on AWS, processing GitHub and Bing searches, and I’m currently refining my existing Twitter, and relevant domain harvesting engines. While Bing and GitHub are harvesting API signals, and indexing API artifacts like OpenAPI, Postman, and others, I’m going to overhaul my Twitter and domain approaches. Twitter has always been a treasure trove of API signals, but like on GitHub, it is getting harder to obtain these API signals at scale—-as their value increases, things are getting tighter with API access. Also, running a proper domain harvesting campaign across thousands of domains isn’t easy, and will require some refactoring to do at the scale I need for this type of effort.

While I have two of these up and running, indexing new APIs, I still have a significant amount of work to invest in each engine. What I have now is purely of prototype. It will take several cycles until I get each engine performing as desired, and then I’m expecting ongoing tweaks, adjustments, and refinements to be made daily, weekly, and monthly to get the results I’m looking for. There are many areas of deficiency in the API sector that bother me, but not having a simple way to search for new and existing APIs is one are I cannot tolerate any longer. I am happy that ProgrammableWeb has been around all these years, but they haven’t moved the needle in the right way. I also get why people do API marketplaces, but I’m afraid they aren’t moving the needle in a positive direction either. I’d say that APIs.Guru (https://apis.guru/openapi-directory/) is the most progressive vision when it comes to API search in the last decade–with all the innovation supposedly going on, that is just sad. I am guessing that venture capital does not always equal meaningful things we need will get built.
]]></content>
    <id>http://apievangelist.com/2019/07/12/my-primary-api-search-engines/</id>
  </entry><entry>
    <title>Taking A Fresh Look At The Nuance Of API Search</title>
    <link href="http://apievangelist.com/2019/07/11/taking-a-fresh-look-at-the-nuance-of-api-search/"/>
    <updated></updated>
    <content><![CDATA[
I have a mess of APIs.json and OpenAPI definitions I need to make sense of. Something that I could easily fire up an ElasticSearch instance, point at my API “data lake”, and begin defining facets and angles for making sense of what is in there. I’ve done this with other datasets, but I think this round I’m going to go a more manual route. Take my time to actually understand the nuance of API search over other types of search, take a fresh look at how I define and store API definitions, but also how I search across a large volume of data to find exactly the API I am looking for. I may end up going back to a more industrial grade solution in the end, but I am guessing I will at least learn a lot along the way.

I am using API standards as the core of my API index—APIs.json and OpenAPI. I’m importing other formats like API Blueprint, Postman, RAML, HAR, and others, but the core of my index will be APIs.json and OpenAPI. This is where I feel solutions like ElasticSearch might overlook some of the semantics of each standard, and I may not immediately be able to dial-in on the preciseness of the APIs.json schema when it comes to searching API operations, and OpenAPI schema when it comes to searching the granular details of what each individual API method delivers. While this process may not get me to my end solution, I feel like it will allow me to more intimately understand each data point within my API index in a way that helps me dial-in exactly the type of search I envision.

The first dimensions are of my API search index are derived from APIs.json schema properties I use to define every entity within my API search index:


  Name - The name of a company, organization, institution, or government agency.
  Description - The details of what a particular entity brings to the table.
  Tags - Specific tags applied to an entity, or even a collection of entities.
  Kin Rank - What Kin thinks of the entity being indexed with APIs.json.
  Alexa Rank - What Alex thinks of the entity being indexed with APIs.json.
  Common Properties - Using common properties like blog, Twitter, and GitHub.
  Included - Other related APIs that are included within the index.
  Maintainers - Details about who is the maintainer of the API definition.
  API Name - The name of specific API program or project that an entity possesses.
  API Description - The details of a specific API program or project that an entity possesses.
  API Tags - How the individual API program or project is tagged for organization.
  API Properties - The details of specific properties of an API like documentation, pricing, etc.


After indexing the 100K view with APIs.json, providing references to the different layers of API operations, I’m indexing the following OpenAPI schema properties:


  Title - The title of an individual API program or project that an entity possesses.
  Description - The description of an individual API program or project that an entity possesses.
  Domain - The subdomain, or top level domain that an API operates within.
  Version - The version of each individual API.
  Tags - The tags that are applied to a specific API program or project that an entity possesses.
  API Path - The actual path of each API.
  API Method Summary - The summary for an individual API method.
  API Method Description - The description for an individual API method.
  API Method Operation ID - The operation id for an individual API method.
  API Method Query Parameters - The query parameters for an individual API method.
  API Method Headers  - The headers for an individual API method.
  API Method Body - The body of an individual API method.
  API Method Tags - The tags applied to each individual API methods.
  Schema Object Name - The name of each of the schema objects.
  Schema Object Description - The description of each of the schema objects.
  Schema Properties - The properties of each of the schema objects.
  Schema Tags - The tags of each of the schema objects.


These details provide to be by the OpenAPI definition for each API provides me with the long tail of my search, going beyond just the names and description of each API, allowing me to turn on or turn off different facets of the OpenAPI specification when indexing, and delivering search results.  My biggest challenges in building this index center around:


  Completeness - I struggle with being able to invest the resources to properly complete the profile for each API.
  Inconsistency - Navigating the inconsistency of APIs, trying to nail down a single definition across thousands of the is hard.
  Performance - The performance of basic JavaScript search against such a large set of YAML / JSON documents isn’t optimal.
  Accuracy - The accuracy of API methods is difficult to ascertain without actually getting a key and firing up Postman, or other script.
  Up to Date - Understanding when information has become out of date, obsolete, or deprecated is a huge challenge with search.


Right now I have about 2K APIs defined with APIs.json, with a variety of OpenAPI artifacts to support. With more coming in each day through my search engine spiders, trolling GitHub and the open web for signs of API life. I’m working to refine my current index of APIs, making sure they are complete-enough for making available publicly. Then I want to be able to provide a basic keyword search tool, then slowly add each of these individual data points to some sort of advanced filter setting for this search tool. I’m not convinced I’ll end up with a usable solution in the end, but I convinced that I will flesh out more of the valuable data points that exist within an APIs.json and OpenAPI index.

This prototype will at least give me something to play with when it comes to crafting a JavaScript interface for the YAML API index I am publishing to GitHub. I feel like these API search knobs will help me better define my search index, and craft cleaner OpenAPI definitions for use in this API search index. As the index grows I can dial in the search filters, and look for the truly interesting patterns that exist across the API landscape. Then I’m hoping to add an API ratings layer to further help me cut through the noise, and identify the truly interesting APIs amidst the chaos and trash. Not all APIs are created equal and I will need a way to better index, rank, and then ultimately search for the APIs I need. While also helping me more easily discover entirely new types of APIs that I may not notice in my insanely busy world.
]]></content>
    <id>http://apievangelist.com/2019/07/11/taking-a-fresh-look-at-the-nuance-of-api-search/</id>
  </entry><entry>
    <title>Navigating API Rate Limit Differences Between Platforms</title>
    <link href="http://apievangelist.com/2019/07/10/navigating-api-rate-limit-diffs-between-platforms/"/>
    <updated></updated>
    <content><![CDATA[
I always find an API providers business model to be very telling about the company’s overall strategy when it comes to APIs. I’m currently navigating the difference between two big API providers, trying to balance my needs spread across very different approaches to offering up API resources. I’m working to evolve and refine my API search algorithms and I find myself having to do a significant amount of work due to the differences between GitHub and Microsoft Search. Ironically, they are both owned by the same company, but we all know their business models are seeking alignment as we speak, and I suspect my challenges with GitHub API is probably a result of this alignment.

The challenges with integrating with GitHub and Microsoft APIs are pretty straightforward, and something I find myself battling regularly when integrating with many different APIs. My use of each platform is pretty simple. I am looking for APIs. The solutions are pretty simple, and robust. I can search for code using the GitHub Search API, and I can search for websites using the Bing Search API. Both produce different types of results, but what both produce is of value to me. The challenge comes in when I can pay for each API call with Bing, and I do not have that same option with GitHub. I am also noticing much tighter restriction on how many calls I can make to the GitHub APIs. With Bing I can burst, depending on how much money I want to spend, but with GitHub I have no relief value—I can only make X calls a minute, per IP, per user.

This is a common disconnect in the world of APIs, and something I’ve written a lot about. GitHub (Microsoft) has a more “elevated” business model, with the APIs being just an enabler of that business model. Where Bing (Microsoft) is going with a much more straightforward API monetization strategy—pay for what you use. In this comparison my needs are pretty straightforward—-both providers have data I want, and I’m willing to pay for it. However, there is an additional challenge. I’m also using GitHub to manage the underlying application for my project. Meaning after I pull search results from GitHub and Bing, and run them through my super top secret, magical, and proprietary refinement algorithm, I publish the refined results to a GitHub repository, and manage the application in real time using Git, and GitHub APIs—which counts against my API usage.

I used to manage all my static sites and applications 100% on GitHub, using the APIs to orchestrate the data behind each Jekyll-driven site. For the last five years I’ve run API Evangelist, and waves of simple data-driven static applications on GitHub like this. It has been a good ride. A free ride. One I fear is coming to a close. I can no longer deploy static data-driven Jekyll apps on the platform, and confidently manage using the GitHub API anymore. It is something I do not expect to continue getting for free. I’d be happy to pay for my account on a per organization, per repo, and per API call basis. In the end, I’ll probably begin just relying on Git for bulk builds of each application I run on GitHub, and eventually begin migrating them to my own servers, running Jekyll on my own, and custom developing an API for managing the more granular changes across hundreds of micro applications that run on Jekyll using YAML data. It would be nice for GitHub to notice this type of application development as part of their business model, but I’m guessing it isn’t mainstream enough for folks to adopt, and GitHub to cater to.

Getting back to the search portion of this post. I am finding myself writing a scheduling algorithm that spread out my API calls across a 24 hour period. I guess I can also leverage the different GitHub accounts I have access to and maybe spread the harvesting across a couple EC2 instance, but I’d rather just do what Bing offers me, and put in my credit card. I am sure there are other ways I can find to circumvent the GitHub API rate limits, but why? I would rather just be above board and put in my credit card to be able to scale how I’m using the platform. One of the biggest challenges to API integration at scale in the future will be API providers who do not offer relief valves for their consumers. Significantly increasing the investment required to integrate with an API in a meaningful way, making it much more difficult to seamless use just a handful of APIs, let alone hundreds or thousands of them. This challenge is nothing new, and just one example of how the business of APIs can get in the way of the technology of APIs—-slowing things down along the way.
]]></content>
    <id>http://apievangelist.com/2019/07/10/navigating-api-rate-limit-diffs-between-platforms/</id>
  </entry><entry>
    <title>The JSON Schema Tooling In My Life</title>
    <link href="http://apievangelist.com/2019/07/10/the-json-schema-tooling-in-my-life/"/>
    <updated></updated>
    <content><![CDATA[
I am always pushing for more schema order in my life. I spend way too much time talking about APIs, when a significant portion of the API foundation is schema. I don’t have as many tools to help me make sense of my schema, and to improve them as definitions of meaningful objects. I don’t have the ability to properly manage and contain the growing number of schema objects that pop up in my world on a daily basis, and this is a problem. There is no reason I should be making schema objects available to other consumers if I do not have a full handle on what schema objects exist, let alone a full awareness of everything that has been defined when it comes to the role that each schema object plays in my operations.

To help me better understand the landscape when it comes to JSON Schema tooling, I wanted to take a moment and inventory the tools I have bookmarked and regularly use as part of my daily work with JSON Schema:


  JSON Schema Editor - https://json-schema-editor.tangramjs.com/ - An editor for JSON Schema.
  JSON Schema Generator - https://github.com/jackwootton/json-schema - Generates JSON Schema from JSON
  JSON Editor - https://json-editor.github.io/json-editor/ - Generates form and JSON from JSON Schema.
  JSON Editor Online -https://github.com/josdejong/jsoneditor/ - Allows me to work with JSON in a web interface.
  Another JSON Schema Validator (AJV) - https://github.com/epoberezkin/ajv - Validates my JSON using JSON Schema.


I am going to spend some time consolidating these tools into a single interface. They are all open source, and there is no reason I shouldn’t be localizing their operation, and maybe even evolving and contributing back. This helps me understand some of my existing needs and behavior when it comes to working with JSON Schema, which I’m going to use to seed a list of my JSON Schema needs, as drive a road map for things I’d like to see developed. Getting a little more structure regarding how I work with JSON Schema.


  Visual Editor - Being able to visual render and edit JSON Schema in browser.
  YAML / JSON Editor - Being able to edit JSON Schema in YAML or JSON.
  YAML to JSON Converter - Converting my YAML JSON Schema into JSON.
  JSON to YAML Converter - Converting my JSON JSON Schema into YAML.
  JSON to JSON Schema Generator - Generate JSON Schema from JSON object.
  JSON Schema to JSON Generator - Generate a JSON object from JSON Schema.
  JSON Validation Using JSON Schema - Validate my JSON using JSON Schema.
  Enumerators - Help me manage enumerators used across many objects.
  Search - Help me search across my JSON Schema objects, wherever they are.
  Guidance - Help me create better JSON Schema objects with standard guidelines.


This is a good start. If I can bring some clarity and coherence to these areas, I’m going to be able to step up my API design and development game. If I can’t, I’m afraid I’m going to be laying a poor foundation for any API I’m designing in this environment. I mean, how can I consciously provide access to any schema object that I don’t have properly defined, indexed, versioned, and managed? If I don’t fully grasp my schema objects, my API design is going to be off kilter, and most likely be causing friction with my consumers. Granted, I could be offloading the responsibility for making sense of my schema to my consumers using a GraphQL solution, but I’m more in the business of doing the heavy lifting in this area, as it pertains to my business—-I’m the one who should know what is going on with each and every object that passes through my business servers.

I wish there was a schema tool out there to help me do everything that I need. Unfortunately I haven’t seen it. The tooling that has rose up around the OpenAPI specification helps us better invest in schema objects when they are in the service of our API contracts, but nothing just for the sake of schema management. I will keep taking inventory of what tooling is available, as well as what I am needing when it comes to JSON Schema management. Who knows, something might pop up out there on the landscape. Or, more realistically I’m hoping little individual open source solutions keep popping up, allowing me to stitch them together and create the experience I’m looking for. I’m a big fan of this approach, rather than one service provider swooping in and providing the one tool to rule them all, only to get acquired and then be shut down–breaking my heart all over again.
]]></content>
    <id>http://apievangelist.com/2019/07/10/the-json-schema-tooling-in-my-life/</id>
  </entry><entry>
    <title>The Details Of My API Rating Formula</title>
    <link href="http://apievangelist.com/2019/07/09/The-details-of-my-api-rating-formula/"/>
    <updated></updated>
    <content><![CDATA[
Last week I put some thoughts down about the basics of my API rating system. This week I want to go through each of those basics, and try to flesh out the details of how I would gather the actual data needed to rank API providers. This is a task I’ve been through with several different companies, only to be abandoned, and then operated on my own for about three years, only to abandon once I ran low on resources. I’m working to invest more cycles into actually defining my API rating in a transparent and organic way, then applying it in a way that allows me to continue evolving, while also using to make sense of the APIs I am rapidly indexing.

First, I want to look at the API-centric elements I will be considering when looking at a company, organization, institution, government agency, or other entity, and trying to establish some sort of simple rating for how well they are doing APIs. I’ll be the first to admit that ratings systems are kind of bullshit, and are definitely biased and hold all kinds of opportunity for going, but I need something. I need a way to articulate in real time how good of an API citizen an API provider is. I need a way to rank the searches for the growing number of APIs in my API search index. I need a list of questions I an ask about an API in both a manual, or hopefully automated way:


  **Active / Inactive **- APIs that have no sign of life need a lower rating.
    
      HTTP Status Code - Do I get a positive HTTP status code back when I ping their URL(s)?
      Active Blog - Does their blog have regular activity on it, with relevant and engaging content?
      Active Twitter - Is there a GitHub account designated for the API, and is it playing an active role in its operations?
      Active GitHub - Is there a GitHub account designated for the API, and is it playing an active role in its operations?
      Manual Visit - There will always be a need for a regular visit to an API to make sure someone is still home.
    
  
  Free / Paid - What something costs impacts our decision to use or not.
    
      Manual Visit - There is no automated way to understand API pricing.
    
  
  Openness - Is an API available to everyone, or is a private thing.
    
      Manual Review - This will always be somewhat derived from a manual visit by an analyst to the API.
      Sentiment Analysis  - Some sentiment about the openness could be established from analyzing Twitter, Blogs, and Stack Exchange.
    
  
  Reliability - Can you depend on the API being up and available.
    
      Manual Review - Regularly check in on an API to see what the state of things are.
      Sentiment Analysis - Some sentiment about the reliability of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
      Monitoring Feed / Data - For some APIs, monitoring could be setup, but will cost resources to be able to do accurately.
    
  
  Fast Changing - Does the API change a lot, or remain relatively stable.
    
      Manual Review - Regularly check in on an API to see how often things have changed.
      Change Log Feed - Tune into a change log feed to see how often changes are Ade.
      Sentiment Analysis   - Some sentiment about the changes to an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
    
  
  Social Good - Does the API benefit a local, regional, or wider community.
    
      Manual Review - It will take the eye of an analyst to truly understand the social impact of an API.
    
  
  **Exploitative - Does the API exploit its users data, or allow others to do so.
    
      Manual Review - It will take a regular analyst review to understand whether an API has become exploitative.
      Sentiment Analysis - Some sentiment about the exploitative nature of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
    
  
  Secure - Does an API adequately secure its resources and those who use it.
    
      Manual Review - Regularly check in on an API to see how secure things are.
      Sentiment Analysis - Some sentiment about the security of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
      Monitoring Feed / Data - For some APIs, monitoring could be setup, but will cost resources to be able to do accurately, unless provided by provider.
    
  
  Privacy - Does an API respect privacy, and have a strategy for platform privacy.
    
      Manual Review - Regularly check in on an API to see how privacy is addressed, and what steps the platform has been taking to address.
      Sentiment Analysis - Some sentiment about the privacy of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
    
  
  Monitoring - Does a platform actively monitor its platform and allow others as well.
    
      **Manual Review **- Regularly check in on an API to see how secure things are.
      Sentiment Analysis - Some sentiment about the security of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
      Monitoring Feed / Data - For some APIs, monitoring could be setup, but will cost resources to be able to do accurately, unless provided by provider.
    
  
  Observability - Is there visibility into API platform operations, and its processes.
    
      Manual Review - It will always take an analyst to understand observability until there are feeds regarding every aspect of operations.
    
  
  Environment - What is the environment footprint or impact of API operations.
    
      Manual Review - This would take a significant amount of research into where APIs are hosted, and disclosure regarding the environment impact of data centers, and the regions they operate in.
    
  
  Popular - Is an API popular, and something that gets a large amount of attention.
    
      Manual Review - Analysts can easily provide a review of an API to better understand an APIs popularity.
      Sentiment Analysis - Some sentiment about the presence of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.
      Twitter Followers** - The number of Twitter followers for an account dedicated to an API provides some data.
      Twitter Mentions - Similarly the number of mentions of an API providers Twitter account provides additional data.
      GitHub Followers - The number of GitHub followers provides another dimension regarding how popular an API is.
      Stack Exchange Mentions - The question and answer site always provides some interesting insight into which APIs are being used.
      Blog Mentions - The number of blog posts on top tech blogs, as well as independent blogs provide some insight into popularity.
    
  
  Value - What value does an API bring to the table in generalized terms.
    
      Manual Review **- The only way to understand the value an API brings to the table is for an analyst to evaluate the resources made available. Maybe some day we’ll be able to do this with more precision, but currently we do not have the vocabulary for describing.
    
  


I am developing a manual questionnaire I can execute against while profiling every API. I have already done this for many APIs, but I’m looking to refine for 2019. I will also be automating wherever I can, leverage other APIs, feeds, and some machine learning to help me augment my heuristic analyst rank with some data driven elements. Some of these will only change when I, or hopefully another analyst reviews them, but some of this will be more dependent on data gathered each month. It will take some time for a ranking system based upon these elements to come into focus, but I’m guessing along the way I”m going to learn a lot, and this list will look very different in twelve months.

Next, I wanted to look at the elements of the rating system itself which I think are essential to the success of an API ranking system based upon the elements above. I’ve seen a number of efforts fail when it comes to indexing and ranking APIs. It is not easy. It is a whole lot of work, without an easy path to monetization like Google established with advertising. Many folks have tried and failed, and I feel like some of these elements will help keep things grounded, and provide more opportunity for success, if not at least sustainability.


  YAML Core - I would define the rating system in YAML.
    
      Rating Formula - The rating formula is machine readable and available as YAML, taking everything listed above and automating the application of it across APIs using a standard YAML definition.
      Rating Results - Publishing a YAML dump of the results of rating for each API provider, also providing a machine readable template for understanding how each API provider is being ranked.
    
  
  GitHub Base - Everything would be in a series of repositories.
    
      GitHub Repo - A GitHub repository is the unit of compute and storage for the rating.
      Git Management - I am using GitHub to apply the rating system across all APIs in my search index.
      GitHub API Management - I am automating the granular editing of the YAML core using the GitHub API.
    
  
  Observable - The entire algorithm, process, and results are open.
    
      Search Transparency - I will be tracking keyword searches, minus IP and user agent, then publishing the results to GitHub as YAML.
      Minimal Tracking - There will be minimal tracking of end-users searching and applying the ranking, with tracking being provider focused.
    
  
  Evolvable - It would be essential to evolve and adapt over time.
    
      Semantic Versioned - The search engine will be semantically versioned, providing a way of understanding it as it evolves.
      YAML - Everything is defined as YAML which is semantically versioned, so nothing is removed or changed until major releases.
    
  
  Weighted - Anyone can weight the questions that matters to them.
    
      Data Points - All data points will have a weight applied as a default, but ultimately will allow end-users to define the weights they desire.
      Slider Interface - Providing end-users with a sliding interface for defining the importance of each data point to them, and apply to the search.
    
  
  Completeness - Not all the profiles of APIs will be as complete as others.
    
      Data Points - The continual addition and evolution of data points, until we find optimal levels of ranking across industries, for sustained periods of time.
    
  
  Ephemeral - Understanding that nothing lasts forever in this world.
    
      Inactive - Making sure things that are inactive reflect this state.
      Deprecation - Always flag something as deprecated, reducing in rank.
      Archiving - Archive everything that has gone away, keeping indexes pure.
    
  
  Community - It should be a collaboration between key entities and individuals.
    
      GitHub - Operate the rating system out in the open on GitHub, leveraging the community for evolving.
      Merge Request - Allow for merge requests on the search index, as well as the ratings being applied.
      Forks - Allow for the workability of the API search, leveraging ranking as a key dimensions for how things can be forked.
      Contribution - Allow for community contribution to the index, and the ranking system, establishing partnerships along the way.
    
  
  Machine Readable - Able for machines to engage with seamlessly.
    
      YAML - Everything is published as YAML to keep things simple and machine readable.
      APIs.json - Follow a standard for indexing API operations and making them available.
      OpenAPI - Follow a standard for indexes the APIs, and making them available.
    
  
  Human Readable - Kept accessible to anyone wanting to understand.
    
      HTML - Provide a simple HTML application for end-users.
      CSS - Apply a minimalist approach to using CSS.
      JavaScript - Drive the search and engagement with client-side JavaScript, powered by APIs.
    
  


This provides me with my starter list of elements I think will set the tone for how this API search engine will perform. Ultimately there will be a commercial layer to how the API search and ranking works, but the goal is to be as transparent, observable, and collaborative around how it all works. A kind of observability that does not exist in web search, and definitely doesn’t in anything API search related. I’ll give it to DuckDuckGo, for being the good guys of web search, which I think provides an ethical model to follow, but I want to also be open with the rating system behind, to avoid some of the illness that commonly exists within rating agencies of any kind.

Next stop, will be about turning the rating elements into a YAML questionnaire that I can begin systematically applying to the almost 2,000 APIs I have in my index. With most of it being a manual process, I need to get the base rating details in place, begin asking them, and then version the questionnaire schema as I work my way through all of the APIs. I have enough experience with profiling APIs to know that what questions I ask, how I ask them, and what data I can gather about API will rapidly evolve once I begin trying to satisfy questions again real world APIs. How fast I can apply my API rating system to the APIs I have indexed, as well as quickly turn around and refresh over time will depend on how much time and resources I am able to manifest for this project. Something that will come and go, as this is just a side project for me, to keep me producing fresh content and awareness of the API space.
]]></content>
    <id>http://apievangelist.com/2019/07/09/The-details-of-my-api-rating-formula/</id>
  </entry><entry>
    <title>Thinking Differently When Approaching OpenAPI Diffs And Considering How To Layer Each Potential Change</title>
    <link href="http://apievangelist.com/2019/07/08/thinking-differently-when-approaching-openapi-diffs-and-considering-how-to-layer-each-potential-change/"/>
    <updated></updated>
    <content><![CDATA[
I have a lot of OpenAPI definitions, covering about 2,000 separate entities. For each entity, I often have multiple OpenAPIs, and I am finding more all the time. One significant challenge I have in all of this centers around establishing a master “truth” OpenAPI, or series of definitive OpenAPIs for each entity. I can never be sure that I have a complete definition of any given API, so I want to keep vacuuming up any OpenAPI, Swagger, Postman, or other artifact I can, and compare it with the “truth” copy” I have on indexed. Perpetually layering the additions and changes I come across while scouring the Internet for signs of API life. This perpetual update of API definitions in my index isn’t easy, and any tool that I develop to assist me will be in need constant refinement and evolution to be able to make sense of the API fragments I’m finding across the web.

There are many nuances of API design, as well as the nuances of how the OpenAPI specification is applied when quantifying the design of an API, making the process of doing a “diff” between two OpenAPI definitions very challenging. Rendering common “diff” tools baked into GitHub, and other solutions ineffective when it comes to understanding the differences between two API definitions that may represent a single API. These are some of the things I’m considering as I’m crafting my own OpenAPI “diff” tooling:


  Host - How the host is stored, defined, and applied across sandbox, production, and other implementations injects challenges.
  Base URL - How OpenAPI define their base url versus their host will immediately cause problems in how diffs are established.
  Path - Adding even more instability, many paths will often conflict with host and base URL, providing different fragments that show as differences.
  Verbs - Next I take account of the verbs available for any path, understanding what the differences are in methods applied.
  Summary - Summaries are difficult to diff, and almost always have to be evaluated and weighted by a human being.
  Description -  Descriptions are difficult to diff, and almost always have to be evaluated and weighted by a human being.
  Operation ID - These are usually autogenerated by tooling, and rarely reflect a provider defined standard, making them worthless in “diff”.
  Query Properties - Evaluating query parameters individually is essential to a granular level diff between OpenAPI definitions.
  Path Properties - Evaluating path parameters individually is essential to a granular level diff between OpenAPI definitions.
  Headers - Evaluating headers individually is essential to a granular level diff between OpenAPI definitions.
  Tags - Most providers do not tag their APIs, and they are often not included, and rarely provide much value when applying a “diff”.
  **Request Bodies - Request bodies provide a significant amount of friction for diffs depending on the complexity and design of an API.
  Responses - Responses often provide an incomplete view of an API, and rarely are robust enough to impact the “diff” view.
  Status Codes - Status codes should be evaluated on an individual basis, providing a variety of ways to articulate these statuses.
  Content Types - Content types these days are often application/json, but do provide some opportunities to define unique characteristics.
  Schema Objects - Schema is often not defined, and rarely used as part of a diff unless OpenAPIs are generated from log, HAR, and other files.
  Schema Properties - Schema properties are rarely present in OpenAPIs, making them not something that comes  up on the “diff” radar.
  Security Definitions - Security definitions are the holy grail of automating API indexing, but are rarely present in OpenAPI, and only in Postman Collections.
  References - The use of $ref, or absence of $ref and doing everything inline poses massive challenges to coherently considering “diff” results.
  Scope - The size of the OpenAPI snippet being applied as part of a “diff” helps narrow what needs to be considered by a human or machine.


This reflects the immediate concerns I have approaching the development of a custom “diff” tool for OpenAPI. First I am just trying to establish a strategy for stripping back the layers of OpenAPI definitions, and established a sort of layered user interface for me to manually accept or reject changes to an OpenAPI. An interface that will also allow me to define a sort of rules vocabulary for increasingly automating the decision making process. I’d love it if eventually the diff tool would show me just a single diff, present me with the change it thinks I should make, and allow me to just agree and move to the next “diff”. I have a lot of work to get things to this point.

Like API search, I feel like API diff is something I have to reduce to its basics, and then fumble my way towards finding an acceptable solution. I don’t feel there is a single “diff” tool for JSON or YAML that will have the eye that I demand for analyzing, presenting, and either manually or automatically merging a diff. Like the other layers of my API search engine, diff is something I need to think through, iterate upon, and repeat until I come up with something that helps me merge “diffs” efficiently across thousands of APIs, and hopefully eventually automates and abstract away the most common differences between the APIs that I am spidering and indexing. Like every other area it is something I’m only working on when I have time, but something I will eventually come out the other end with a usable OpenAPI diff tool, that can help me make sense of all the API definitions I’m bombarded with on a daily basis.
]]></content>
    <id>http://apievangelist.com/2019/07/08/thinking-differently-when-approaching-openapi-diffs-and-considering-how-to-layer-each-potential-change/</id>
  </entry><entry>
    <title>Why The Open Data Movement Has Not Delivered As Expected</title>
    <link href="http://apievangelist.com/2019/07/05/why-the-open-data-movement-has-not-delivered-as-expected/"/>
    <updated></updated>
    <content><![CDATA[
I was having a discussion with my friends working on API policy in Europe about API discovery, and the topic of failed open data portals came up. Something that is a regular recurring undercurrent I have to navigate in the world of APIs. Open data is a subset of the API movement, and something I have first-hand experience in, building many open data portals, contributing to city, county, state, and federal open data efforts, and most notably riding the open data wave into the White House and working on open data efforts for the Obama administration.

Today, there are plenty of open data portals. The growth in the number of portals hasn’t decreased, but I’d say the popularity, utility, and publicity around open data efforts has not lived up to the hype. Why is this? I think there are many dimensions to this discussion, and few clear answers when it comes to peeling back the layers of this onion, something that always makes me tear up.


  Nothing There To Begin With - Open data was never a thing, and never will be a thing. It was fabricated as part of an early wave of the web, and really never got traction because most people do not care about data, let alone it being open and freely available.
  It Was Just Meant To Be A Land Grab - The whole open data thing wasn’t about open data for all, it was meant to be open for business for a few, and they have managed to extract the value they needed, enrich their own datasets, and have moved on to greener pastures (AI / ML).
  No Investment In Data Providers - One f the inherent flaws of the libertarian led vision of web technology is that government is bad, so don’t support them with taxes. Of course, when they open up data sets that is goo for us, but supporting them in covering compute, storage, bandwidth, and data refinement or gathering is bad, resulting in many going away or stagnating.
  It Was All Just Hype From Tech Sector - The hype about open data outweighs the benefits and realities on the ground, and ultimately hurt the movement with unrealistic expectations, setting efforts back many years, and are now only beginning to recover now that the vulture capitalists are on to other things.
  Open Data Is Not Sexy - Open data is not easy to discover, define, refine, manage, and maintain as something valuable. Most government, institutions, and other organizations do have the resources to do properly, and only the most attractive of uses have the resources to pay people to do the work properly, incentivizing commercial offerings over the open, and underfunded offerings.
  Open Data Is Alive and Well - Open data is doing just fine, and is actually doing better, now that the spotlight is off of them. There will be many  efforts that go unnoticed, unfunded, and fall into disrepair, but there will also be many fruitful open data offerings out there that will benefit communities, and the public at large, along with many commercial offerings.
  Open Data Will Never Be VC Big - Maybe open data share the spotlight because it just doesn’t have the VC level revenue that investors and entrepreneurs are looking for. If it enriches their core data sets, and can be used to trying their machine learning models, it has value as a raw material, but as something worth shining a light on, open data just doesn’t rise to the scope needed to be a “product” all by itself.


My prognosis on why open data never has quite “made it”, is probably a combination of all of these things. There is a lot of value present in open data as a raw material, but a fundamental aspect of why data is “open”, is so that entrepreneurs can acquire it for free. They aren’t interested in supporting city, county, state, and federal data stewards, and helping them be successful. They just want it mandated that it is publicly available for harvesting as a raw material, for use in the technology supply chain. Open data primarily was about getting waves of open data enthusiasts to do the heavy lifting when it came to identifying where the most value raw data sources exist.

I feel pretty strong that we were all used to initiate a movement where government and institutions opened up their digital resources, right as this latest wave of information economy was peaking. Triggering institutions, organizations, and government agencies to bare fruit, that could be picked by technology companies, and used to enrich their proprietary datasets, and machine learning models. Open doesn’t mean democracy, it mostly means for business. This is the genius of the Internet evolution, is that it gets us all working in the service of opening things up for the “community”. Democratizing everything. Then once everything is on the table, companies grab what they want, and show very little interest in giving anything back to the movement. I know I have fallen for several waves of this ver the last decade.

I think open data has value. I think community-driven, standardized sets of data should continue to be invested in. I think we should get better at discovery mechanisms involving how we find data, and how we enable our data to be found. However, I think we should also recognize that there are plenty of capitalists who will see what we produce as a valuable raw resource, and something they want to get their hands on. Also, more importantly, that these capitalists are not in the businesses of ensuring this supply of raw resource continues to exist in the future. Like we’ve seen with the environment, these companies do not care about the impact their data mining has on the organizations, institutions, government agencies, and communities that produced them, or will be impacted when efforts go unfunded, and unsupported. Protecting our valuable community resources from these realities will not be easy as the endless march of technology continues.
]]></content>
    <id>http://apievangelist.com/2019/07/05/why-the-open-data-movement-has-not-delivered-as-expected/</id>
  </entry>
</feed>
