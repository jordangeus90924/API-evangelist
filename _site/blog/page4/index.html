<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-IMG_7162.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/09/10/the-api-reality-in-our-heads-versus-the-reality-on-the-ground/">The API Reality In Our Heads Versus The Reality On The Ground</a></h3>
			<p><em>10 Sep 2019</em></p>
			<p>I am spending some time grounding my views of the API landscape. Working my through all of my beliefs, and systematically blowing them to bits to see how they hold up against the stress of reality on the ground. This is something I&rsquo;ve become very good at when it comes to my personal beliefs in recent years, and something I&rsquo;ve been working to transfer to my professional world to help me keep a grip on what is going on. There are a number of reason why I fall prey to things that are not real in this game, and I&rsquo;m pretty aware of the shady things that occur in the business world, but when it comes to technology I find the stories it whispers in my ear prove to be particularly enchanting and seem to go from whisper to truth at a velocity I don&rsquo;t always understand.One of the things I need to develop a way of better evaluating in the moment is around the velocity at which things will happen. How fast adoption of APIs will occur within the mainstream. How quickly a company will adopt an API-first approach. And the time it will take a new tool to go from creation to adoption. Technology has this way of convincing me that everything is moving faster than ever before, and it is something that ends up as a residue on everything I touch, and is relative to how deeply I believe in this myth. As I approach a decade of doing this, I can say that API adoption and awareness has never played out in a timeline anywhere close to what I envisioned in the early days. At this point I&rsquo;d say that most things are at a 6X scale than I had imagined. Sure, there are exceptions, but when it comes to the normal pace of change, especially within the enterprise, it has taken about 6 times as long for things to take...[<a href="/2019/09/10/the-api-reality-in-our-heads-versus-the-reality-on-the-ground/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-new-80-140-800-500-0-max-0--5--5.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/09/10/discovering-the-confluent-schema-registry/">Discovering The Confluent Schema Registry</a></h3>
			<p><em>10 Sep 2019</em></p>
			<p>While spending time doing some research into schema management tooling I came across the Confluents Schema Registry. The schema management solutions is one of the first formal tools I&rsquo;ve come across that is specifically designed for helping folks get their schema house in order when it comes to APIs. I&rsquo;m sure there are others out there, but this was the first solution I've documented that addresses this in an API context as well as having an API, providing some of the critical features needed to make sense of the crazy schema mess enterprise organizations find themselves in.Here is the language from the Confluent website describing what the registry is all about: Confluent Schema Registry provides a RESTful interface for developers to define standard schemas for their events, share them across the organization and safely evolve them in a way that is backward compatible and future proof. The Confluence Schema Registry allows you to centralize your schema and provides a REST API to integrate, save, and retrieve schemas, and delivers functionality for automatically converting JSON messages to make your data human friendly. Providing a pretty fundamental schema management solution that other API service providers should be thinking about. Clearly this one is for use with your Kafka infrastructure, but the model applies across any API you are deploying, whether it is HTTP, TCP, MQTT, or otherwise&mdash;Confluent just provides us with one compelling model to follow.Now that I have schema catalog added to my monitoring system vocabulary it will be notifying me of other news, blogs, and other signals when it comes to how API providers are managing their schema, as well as any other API service providers like Confluent who are investing in this area of the API lifecycle. It is an area that I&rsquo;ve been beating the drum about for a while now, and something I&rsquo;d like to see more investment in. If companies are not able to get their schema house in order, they...[<a href="/2019/09/10/discovering-the-confluent-schema-registry/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-IMG_4564.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/09/09/continue-pushing-the-api-documentation-conversation-forward/">Continue Pushing The API Documentation Conversation Forward</a></h3>
			<p><em>09 Sep 2019</em></p>
			<p>I am been finally seeing the investment across the API sector I wanted to see when it comes to API documentation. There are multiple API definition driven API documentation offerings available on the market now. Both open source and high quality commercial services. A couple years after Swagger UI made it&rsquo;s splash I began lobbing for more investment in open source API documentation tooling, and after four years I&rsquo;m starting to see it beginning to happen. However, let&rsquo;s not rest on laurels and make sure we keep investing and building on the momentum that we have established, and continue making API documentation more valuable to developers, but also to business users who are interested in putting API resources to work.One of the major improvements in API documentation that I would like to see in coming years centers around visualizations. I&rsquo;d like to see interactive documentation be augmented and extended using D3.js, and other visualization components, rendering API responses in a more visually pleasing way. Helping make API responses more meaningful to developers, and potentially to businesses users who are trying to understand the value an API delivers. Visualizations have the potential to make API documentation something that introduces and educates developers to how to integrate with an API, but also demonstrate and illustrate how the data, content, and other resources can be valuable. Bonus points for any tooling provider if the visual results are actually embeddable and shareable across websites and social media, allowing anyone to take the results of each API response and quickly make available to a developer or business users network.Beyond just visualizations, I&rsquo;d like to see more interactive API documentation to make results savable, exportable, and shareable. Allowing any developer or business user to easily make an API request, then export the results as a JSON, CSV, or possibly as a spreadsheet. Empowering API consumers to quickly use API documentation to understand what an API delivers, get at the valuable at...[<a href="/2019/09/09/continue-pushing-the-api-documentation-conversation-forward/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/san-francisco-city-bridge-sf-city-bridge-copper-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/09/09/bridging-grand-visions-of-an-api-lifecycle-with-people-on-the-ground-being-successful-in-their-work/">Bridging Grand Visions of an API Lifecycle With People on the Ground Being Successful In Their Work</a></h3>
			<p><em>09 Sep 2019</em></p>
			<p>While my work as the API Evangelist can burn me out some of the time, I generally find it intellectually challenging. The work takes me from industry to industry, country to country, and to the highest levels of technology, and down to the work that goes on across development teams within companies. APIs are everywhere. Really, they are. They are impacting each one of our personal and professional lives, and this is one of the things that keeps me coming back doing what I do. The diversity of API implementations, and levels at which I can engage with the industry keeps me interested, continuously learning and producing.I enjoy thinking about the API space from the 250K level. It is interesting to study what is, what has been, and where things might be going when it comes to APIs. I find it compelling to learn about how API providers, service providers, and investors see things, then reconcile that with what actually happens on the ground. Looking at API change across business sectors has lifted my view from 100K to 250K, allowing me to not just understand how the tech echo chamber sees APIs, but also how mainstream businesses and government agencies see APIs. Always pushing me in new directions, helping me see beyond any single silo I might get trapped in along the way.Watching the API lifecycle come Into focus over the last decade has been interesting. Watching API management shift how we generate revenue from our software, witnessed API design come to life, as well as mocking, testing, and other stops along the API lifecycle have all been educational experiences. I like to think about API-first, and how progressive developers are delivering high quality APIs from end to end. Studying, listening and writing about these concepts, then repeating, repeating, and repeating, until I push my own understanding to new heights. This is what API Evangelist has been for me. It has been something that has...[<a href="/2019/09/09/bridging-grand-visions-of-an-api-lifecycle-with-people-on-the-ground-being-successful-in-their-work/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-DSC_0109.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/09/06/where-do-you-like-your-api-complexity/">Where Do You Like Your API Complexity?</a></h3>
			<p><em>06 Sep 2019</em></p>
			<p>I prefer my API complexity at the path, query, then schema levels of my API design&mdash;specifically in that order. I don&rsquo;t mind a huge number of individual API calls to get the job done because I just script away this aspect of API complexity. However, I do fully understand that many folks prefer their complexity at the query and schema levels over having lots of individual paths. I find that developers love to rant about imperative API complexity, and in my experience the folks who don&rsquo;t like path level API complexity are also some of the most vocal types of folks who are very confident that their way is the right way, when in reality, there is no right way&mdash;just the way YOUR consumers will want or need to access the API resources you are serving up.In my experience, how someone learned about the web, and then APIs, dictate much of where they like their API complexity. If developers prefer their complexity at the query parameter layer their API doorway was the web. If developers prefer their complexity in the schema, their doorway was likely mobile. If you understand the role that paths can play in managing complexity, you&rsquo;ve probably have embraced the overall concept of web APIs. If you understand the opportunity and necessity of sensibly spreading complexity across the path, query, and schema layers of your API, you probably don&rsquo;t just have experience providing APIs, you probably have a lot of experience supporting many different types of API consumers&mdash;the key that unlocks the door to seeing the bigger picture of managing API complexity.Not all API consumers are created equal. They have different views of what an API is, and how you use one. If you&rsquo;ve supported a wide audience of API consumers,&nbsp; you realize there is no single silver bullet when it comes to where you offload your API complexity. You have to make tradeoffs at every turn when designing your API, and...[<a href="/2019/09/06/where-do-you-like-your-api-complexity/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/docks-docks-graham-sutherland.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/09/06/api-management-should-not-just-limit-me-it-should-allow-me-to-scale/">API Management Should Not Just Limit Me, It Should Allow Me To Scale</a></h3>
			<p><em>06 Sep 2019</em></p>
			<p>I do a lot of thinking about API management. After almost a decade of contemplating how we manage our API infrastructure, I feel it is still the most important stop along the API lifecycle. I don&rsquo;t care how well designed, deployed, documented, and supported your APIs are, if you aren&rsquo;t tuned in using API management, you aren&rsquo;t going to be successful. API management provides you with the tools to you need to define and understand how your consumers will put your API resources to work. After almost 15 years of evolution, API management hasn&rsquo;t changed too much, but there is one core capability I&rsquo;d like to see evolve, expanding upon the often go to feature of API rate limiting.API rate limiting has been a staple of API management since the beginning, allowing you to limit how much fo any resource a group or individual consumer can get access to&mdash;limiting the rate at which they can make API calls. The reason for rate limiting will vary from provider to provider, but the most common reason is to conserve compute resources so that an API remains usable by all consumers. Next, I&rsquo;d say that pricing is the second most common reason for rate limiting, carving up API resources by access tier, and limiting the number of calls each API consumer can make per second, minute, day, or other time frame. While these concepts are still applicable to the business of APIs in 2019, I&rsquo;d like to see the concept evolve to keep up with how we deploy infrastructure in a containerized, Kubernetes, serverless cloudy landscape.Instead of capping what I can consume of your API, why not allow me to pay for more access, as well as more performance for a short period of time, or whatever duration I desire. You can still impose rate limits to measure everything I&rsquo;m consuming, but allow me to also give the OK and turn on the firehose. If I need to...[<a href="/2019/09/06/api-management-should-not-just-limit-me-it-should-allow-me-to-scale/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/old-gas-pumps-oldgaspumps-dark-dali.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/09/06/api-evangelist-does-not-run-on-github-anymore/">API Evangelist Does Not Run On GitHub Anymore</a></h3>
			<p><em>06 Sep 2019</em></p>
			<p>I migrated the main API Evangelist site off of GitHub the other day. The moved followed the migration of 100+ network sites of my API research a couple of weeks back. While I still have a handful of definitions and tooling published to GitHub, the migration of my main site signals a pretty huge shift in how I operate the site. I&rsquo;ve operated the site 100% on GitHub since 2014, using YAML as the backend data store, and Jekyll to publish the pages, blogs, and data-driven pages. I have always done this to keep the site as open and accessible as I possibly can, sharing all of the data behind what I was doing However, in 2019, due to increased GitHub API rate limits, Jekyll build timeout limits, and shifts in the purpose of API Evangelist, I don&rsquo;t see the value in me working to keep things open and available on GitHub anymore.To operate API Evangelist I am still going with a static approach, meaning all of the pages are published as static HTML, rather than making dynamic from a CMS or database--however, I won't be using Jekyll anymore. I will maintain all the content and data within my own home brew CMS and database, and I will publish things out on a schedule, and in response to specific events that occur. The move significantly reduces the complexity and workload on my part when it came to maintaining the many different repositories, schema, and increasinlgy complex publishing process. It is much easier to just publish HTML files to the file system of a Linux server than use Git and APIs to orchestrate changes across hundreds of repositories. It was something that was becoming untenable due to increased error rates with Jekyll builds when I committed a change, and impossible to do via the GitHub API with a shift in API rate limits recently.I&rsquo;m a little sad that it is all over. I enjoyed the performance...[<a href="/2019/09/06/api-evangelist-does-not-run-on-github-anymore/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-server-racks-clouds-copper-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/09/05/the-different-ways-api-providers-use-the-openapi-servers-collection/">The Different Ways API Providers Use The OpenAPI Servers Collection</a></h3>
			<p><em>05 Sep 2019</em></p>
			<p>I was looking through the OpenAPI definitions I have harvested via some automated scripts I have running, and I came across an API definition that had a variety of URLs available for their APIs, making this part of the definition something I want to study more, identifying the common patterns in use. I harvest a growing number of OpenAPI definitions and Postman Collections to help me stay in tune with who the interesting API providers are, and documenting what the common building blocks of APIs are, helping shine a light on the useful practices that exist across API providers within many different industries. The OpenAPI server collection is beeing used to help automate switching between a variety of locations, and is most commonly used to differentiate between the different stages of an API server, as see I this example: This is just the most common usage of the OpenAPI server collection out there. I&rsquo;d say the second most common example is publishing multiple regions in which an API is available&mdash;leveraging DNS to to make an API more available, performant, and meeting local and regional regulations. After harvesting and processing a couple thousand OpenAPI 30 definitions following doing the same with slightly more Swagger 2.0 files the importance of moving from a single host in Swagger 2.0 to multiple potential servers in OpenAPI 3.0 revealed itself. Signaling that APIs aren&rsquo;t just being deployed and made available in a single location or way. I will be regularly pulling the values for the server collection across all the OpenAPI definitions I index to develop a better understanding at how API providers are using it. It provides an interesting look at API providers roll out their infrastructure. I don&rsquo;t expect every API provider to be documenting their APIs this thoroughly, but since I&rsquo;m scanning GitHub for most of these API definitions, many of the API providers are publishing their OpenAPI definitions to GitHub because it is part of some...[<a href="/2019/09/05/the-different-ways-api-providers-use-the-openapi-servers-collection/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/desert-dragon-light-dali.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/09/03/controlling-the-conversation-around-your-mobile-application-apis/">Controlling The Conversation Around Your Mobile Application APIs</a></h3>
			<p><em>03 Sep 2019</em></p>
			<p>I have seen it play out over and over since I began monitoring the API conversation. Companies who launch APIs to power a mobile application but refuse to, or are unaware of how they should be controlling the conversation around public API infrastructure. The most common reason for this is that companies do not view the APIs behind their mobile applications as public APIs, and that somehow because they are buried within their mobile application, that they are safe from hackers. Completely clueless of the fact that anyone can run any mobile application through a proxy and reverse engineer the API infrastructure behind any mobile application.Mobile application platforms that do not control the conversation around their public APIs are the ones who end up having security incidents down the road. This is due to the face that these providers end up having a pretty significant blind spot stemming from their lack of awareness and control of the conversation around their APIs, and someone ends up paying closer attention to their APIs and eventually someone finds a vulnerability to exploit. If you have a publicly available mobile application, then you have publicly available APIs, and you should be treating your APIs like they are public. I&rsquo;m not saying you should offer the public free and unfettered access to your APIs, I am simply saying that you should be operating a public API program around your APIs, controlling who has access, and shaping the message around what your APIs do, or don&rsquo;t do.To see examples of companies who do not have a handle on their API conversation, search for TikTok API, or for Tinder API, and you&rsquo;ll see that hackers own the conversation when it comes to the APIs for these platforms. When you aren&rsquo;t dealing with this side of your operations, rogue API operators step up and dominate the conversation on GitHub, Stack Exchange, NPM, and other places that coders hang out. We&rsquo;ve already seen Tinder...[<a href="/2019/09/03/controlling-the-conversation-around-your-mobile-application-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-square-23-160-800-500-0-max-0--5--1-square.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/26/benefits-of-treating-my-api-infrastructure-as-apifirst/">Benefits Of Treating My API Infrastructure As API-First</a></h3>
			<p><em>26 Aug 2019</em></p>
			<p>Most API providers I speak with see the value of consistently delivering API infrastructure to power desktop, web, mobile, device, and network applications. Less than 10% of these providers see the API infrastructure that powers their APIs, and ultimately their applications as APIs. Meaning, these providers do not view every stop along the API lifecycle as a set of APIs, ensuring that your API definitions, design, mocking, deployment, management, monitoring, testing, orchestration, security, and documentation all have APIs, and are able to be governed programmatically. Mostly it is because they are just getting started on their API journey and / or they just don’t have the bandwidth to be able to step back and look holistically at what they are trying to accomplish. In this truly API-first world, every architectural decision begins with a README in a Git repo, and the business, architectural, and development teams working together to hammer out a simple name, description, and list of capabilities for each individual architectural component. Once in agreement, then they can then get to work hammering out a contract for the schema, and the interfaces that are available for potential consumers. Once again, working between business, architecture, and development teams to come up with the simplest, most useful, and durable contract possible. One that will work for all stakeholders, and best represent the architectural capabilities of each component you will need to successfully operate each stop along your API life cycle. Once there is an overview and contract in place for each architectural component, you can begin mocking, testing, and documenting it for wider potential consumption by other stops along the API life cycle. With all stakeholders in agreement over the capabilities of each architecture component, and how it will work, you can begin wiring up specific back-end capabilities which might be as simple as data and content storage, or as complex as API management, monitoring, and testing. Leveraging 3rd party services, open source solutions, or...[<a href="/2019/08/26/benefits-of-treating-my-api-infrastructure-as-apifirst/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-copper-servers.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/22/doing-a-diff-between-available-web-mobile-and-public-apis/">Doing A Diff Between Available Web, Mobile and Public APIs</a></h3>
			<p><em>22 Aug 2019</em></p>
			<p>I spend a lot of time running web and mobile applications through a proxy to reverse engineer their APIs. I generally use Charles Proxy for routing my desktop, web, and mobile traffic through, which then automatically saves a JSON dump of sessions every five minutes, and syncs with Dropbox via my shared folders. From there I have a schedule service that will look in the shared Dropbox folder every hour, sift through the Charles Proxy JSON dump, and look for JSON, XML, CSV, and other common machine readable formats—which are then converted into OpenAPI definitions. Allowing me to reverse engineer desktop, web, and mobile applications as I use them, and map the API surface area for these targeted applications. Recently I started playing with doing the same thing as part of my use of Postman. You can use the built-in proxy in the Postman native apps or use the Interceptor extension for the Postman app. Postman walks you through how to configure your laptop, mobile, and Postman application, and ultimately capture HTTP requests and save them to history or as a Postman Collection. Doing essentially the same thing I’m doing, but doing it with the Postman application, and leveraging collections instead of OpenAPI. I’d say there are pros and cons to both approaches, but Postman gives me the ability to manage environments, workspaces, and other essential concepts that would help take my API profiling work to the next level. One of the benefits of working with Postman collections is you get all the benefits of using the Postman app. Things like the built-in proxy for capturing traffic, but also the history, generate, fork, merge, and diff collections. My work profiling APIs is all about reverse engineering desktop, web, and mobile applications, as well as quickly translating API documentation in machine readable API definitions, like OpenAPI and Postman. When profiling an API, most of the time I have the API documentation open in one browser window,...[<a href="/2019/08/22/doing-a-diff-between-available-web-mobile-and-public-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/white-house-lawn-white-house-window-propaganda-leaflets.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/22/an-api-policy-domain-specialist-at-twitter/">An API Policy Domain Specialist At Twitter</a></h3>
			<p><em>22 Aug 2019</em></p>
			<p>
There are some jobs on the Internet I apply for no matter what my current situation is, and an API policy domain specialist at Twitter was one of them that popped up recently. I applied for the job within the first couple of hours after it came out, but haven’t heard from them. I can speculate on the reasons why, but I think a story about the job posting itself is actually more interesting, so I’ll focus there. It is the first time I’ve seen a job posting for this role, but I think it will eventually become a required role in the future for any company with a public API—-that is, if companies want avoid the trouble Twitter is going through right now, which again, is making Twitter the poster child for how to do APIs both right and wrong.

To highlight what this role is all about, I think Twitter’s own posting sums it up well, so let’s start by just reviewing what you’ll be doing if you get this job at Twitter:

[<a href="/2019/08/22/an-api-policy-domain-specialist-at-twitter/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-downtheline-dali-three.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/21/multiple-overlapping-api-life-cycles/">Multiple Overlapping API Life Cycle(s)</a></h3>
			<p><em>21 Aug 2019</em></p>
			<p>One of the toughest parts about teaching people about APIs is that there are many different views of what the API life cycle can be depending on who you are, and what your intentions are. As an advocate or evangelist for a single API you are speaking externally to the API consumption life cycle, but internally you are focused on the API delivery life cycle. As an API Evangelist for many APIs, targeting providers, consumers, and anyone else who comes along, I find it a constant challenge to properly speak to my intended audience. One problematic part of my storytelling I regularly see emerge is that I speak of a single API life cycle, where in reality there are many overlapping life cycles. So, to help me think through all of this I wanted to explore what these overlapping tracks might be—coming up with four distinct iterations of overlapping API building blocks. The API Delivery Life Cycle  The most common way we refer to the API life cycle is from the perspective of the API provider, where it is all about delivering an API. Referencing the stops along the life cycle that are most relevant to someone who is delivering a new API, or might be moving an API forward as part of the evolution of an existing resource. From my vantage point, I consider these to be the most common stops along the API delivery life cycle. Definitions - Defining what an API does, crafting the JSON schema, OpenAPI, AsyncAPI, and other machine readable definitions of what is potentially being delivered, initializing the contract that will guide an API through the life cycle. Design - Stepping back and considering the healthiest API Design practices to apply, working from a diverse API toolbox, and ensuring you have a good handle on who your audience is, and the design patterns they’ll respond to. Mocking - Generating a mock instance of an API using the contract to...[<a href="/2019/08/21/multiple-overlapping-api-life-cycles/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://kinlane-productions2.s3.amazonaws.com/api-conferences/the-postman-users-conference-2019.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/20/the-api-conferences-i-am-tracking-on-for-the-fall/">The API Conferences I Am Tracking On For The Fall</a></h3>
			<p><em>20 Aug 2019</em></p>
			<p>As we approach the fall it is time to begin thinking about the conference season, and what the most relevant API conferences are. I haven’t been doing any events this year, but staying in tune with the conference circuit has always been important to my work. Who knows, maybe I will be spend some more time investing in API related events after taking a break for six month. When it comes to API events and conferences, here is what I am tracking on.

[<a href="/2019/08/20/the-api-conferences-i-am-tracking-on-for-the-fall/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/abe-lincoln-one-yellow-collage-file-00-00-00-00.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/20/human-empathy-is-one-of-my-most-important-api-outreach-tools/">Human Empathy Is One Of My Most Important API Outreach Tools</a></h3>
			<p><em>20 Aug 2019</em></p>
			<p>I am an empathic human being. It is one of my top strengths, as well as one of my top weaknesses. It is also one of the most important tools in my API toolbox. Being able to understand the API experience from the position of different people throughout the world of APIs is a cornerstone of the API Evangelist brand. Personally, I find APIs themselves to be empathy triggering, and something that has regularly forced me out of my silos, then allowing me t put myself in the shoes of my consumers. Something that when realized in a perpetual fashion can become a pretty powerful force for dialing in the services you offer, and establish, maintain, and strengthen connections with other people within the community. Being able to listen to people in the hallways of conferences, and within the meeting rooms across enterprise, institutions, and government agencies, then internalize, process, and position my writing from what I learn from people is how I have written on API Evangelist for the last nine years. I rarely am positioning my narrative my own vantage point, or that of a company. Most of the time I am channeling someone I’ve met along the way, speaking from their perspective, and analyzing the world of APIs as they would see it. While I wish that the world always resembled my view of the API landscape, from experience I know better, and that there are many diverse ways of seeing the value or damage APIs are responsible for. While API design, and the overall user experience around API service and tooling goes a long way to speak to end-users, I still think the human touch, and positioning our messaging from the vantage point of our consumers will have the greatest impact. Making a person connection will last much longer than any single blog post, advertisement, Tweet, image, video, or other common unit of engagement. Of course, what you gather from putting...[<a href="/2019/08/20/human-empathy-is-one-of-my-most-important-api-outreach-tools/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-gears-numbers-blue.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/19/postman-collection-as-a-single-quantifiable-shareable-executable-unit-of-representation-for-any-digital-capability/">Postman Collection As A Single Quantifiable, Shareable, Executable Unit Of Representation For Any Digital Capability</a></h3>
			<p><em>19 Aug 2019</em></p>
			<p>In my world API definitions are more valuable than code. Code is regularly thrown away and rewritten. API definitions hold the persistent detail of what an API delivers, and contain all of the proprietary value when they are properly matured. OpenAPI has definitely risen to the top when it comes to which API definition formats you should be using, however, Postman Collections have one critical ingredient that makes them ultimately more usable, sharable, and meaningful to developers—-environmental context. This small but important difference is what makes Postman Collections so valuable as a single quantifiable, shareable, executable unit of representation for any digital capability. Like OpenAPI, Postman Collections describe the surface area of a web API, but they have that added layer to describe the environment you are running in, which makes it much more of a run-time and execute-time experience. This may seem like a minor detail, but developers who want instant gratification, a Postman Collection bundled with the Postman API lifecycle tooling, makes for a pretty powerful representation of a company’s, organization’s, institutions’s, or government agency’s digital capability. Allowing for API providers (or consumers) to describe what an API does in a machine readable format, bundle with it the environment context to actually execute the digital capability, and enable the unit of value to be realized within the Postman API development ecosystem. I can take any of my internal, or 3rd party public APIs I depend on, make successful calls to them, including authentication, tokens, and other environment variables, then export as a portable Postman Collection, and share with anyone I want using a simple URL, or by embedded the Run in Postman button within documentation, blog posts, and other resources. Then, any potential consumer can take that Postman Collection, load into their Postman client, and be able to realize the same digital capability I was using—-no documentation, on-boarding, or other friction required. You get instant gratification regarding putting the digital capability to work,...[<a href="/2019/08/19/postman-collection-as-a-single-quantifiable-shareable-executable-unit-of-representation-for-any-digital-capability/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-beach-rocks-currents-internet-numbers.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/19/a-second-wave-of-api-management-is-going-on/">A Second Wave of API Management is Going On</a></h3>
			<p><em>19 Aug 2019</em></p>
			<p>I fully surfed the first wave of API management. API Evangelist began by researching what Mashery, Apigee, and 3Scale had set into motion. API Evangelist continued to has exist through funding from 3Scale, Mulesoft, WSO2, and continues to exist because of the support of next generation providers like Tyk. I intimately understand what API management is, and why it is valuable to both API providers and consumers. API management is so relevant as infrastructure it is now baked into the AWS, Azure, and Google Clouds. However, if you listen to technological winds blowing out there, you will mostly hear that the age of API management is over with, but in reality it is just getting started. The folks telling these tales are purely seeing the landscape from an investment standpoint, and not from an actual boots on the ground within mainstream enterprise perspective—something that is going to burn them from an investment standpoint, because they are going to miss out on the second wave of API management that is going on. The basics of API haven’t changed from the first to the second wave, so let’s start with the fundamental building blocks of API management before I move into describing what the next wave will entail: Portal - A single URL to find out everything about an API, and get up and running working the resources that are available. On-Boarding - Think just about how you get a new developer to from landing on the home page of the portal to making their first API call, and then an application in production. Accounts - Allowing API consumers to sign up for an account, either for individual, or business access to API resources. Applications - Enable each account holder to register one or many applications which will be putting API resources to use. Authentication - Providing one, or multiple ways for API consumers to authenticate and get access to API resources. Services - Defining which services...[<a href="/2019/08/19/a-second-wave-of-api-management-is-going-on/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/border-crossing-bordercrossing-dali-three.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/16/seeing-api-consumers-as-just-the-other-ones/">Seeing API Consumers As Just The Other Ones</a></h3>
			<p><em>16 Aug 2019</em></p>
			<p>As API providers, it can be easy to find ourselves in a very distant position from the consumers of our APIs. In recent weeks I have been studying the impacts of behavioral approaches to putting technology to work, something that has led me to the work of Max Meyer, and his Psychology of the Other-One (1921). I haven’t read his book yet, but have finished other works citing his work on how to “properly” study how animals (including humans) behave. While the psychological impact of all of this interests me, I’m most interested in how this perspective has amplified and driven how we use technology, and specifically how APIs can be used to create or bridge the divide between us (API providers) and our (API consumers). While web and mobile technology is often portrayed as connecting and bringing people together, it also can be used to establish a separation between providers and consumers. We often get caught up in the scale and growth of delivering API infrastructure, and we forget that our API consumers are humans, and we can end up just seeing them as personas, humans, or just a demographic. Of course, as API providers, we can’t be expected to make a direct connection with every single consumer, but we also have to be wary of becoming so distant from their reality that we can’t make a connection with them at all. Leaving our products, services, and tooling something that doesn’t serve them in any way, and we fail to meet our own business objectives behind what we were building in the first place. There will aways be some distance between API provider and consumer. However, we have to regularly work to narrow this divide, otherwise negative forces can make their way in between us and consumers. If we simply see your API consumers and end-users as the “other ones”, it will make supporting, and investing in their success much more difficult. Trust with...[<a href="/2019/08/16/seeing-api-consumers-as-just-the-other-ones/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-new-van-gogh-starry-night-container-bridge-2.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/16/four-phases-of-internal-api-evangelism/">Four Phases Of Internal API Evangelism</a></h3>
			<p><em>16 Aug 2019</em></p>
			<p>General evangelism around what APIs are, as well as more precise advocacy around specific APIs or groups of API resources takes a lot of work, and repetition. Even as a seasoned API evangelist I can never assume my audience will receive and understand what it is that I am evangelizing, and I regularly find myself having to reassess the impact (or lack of) that I’m making, retool, refresh, and repeat my messaging to get the coverage and saturation I’m looking for. After a decade of doing this, I cannot tell which is more difficult, internal or external public evangelism, but I do find that after almost 10 years, I’m still learning from each audience I engage with—-proving to me that no single evangelism strategy will ever reliably work, and I need a robust, constantly evolving toolbox if I am going to be successful. I have many different tools in my internal evangelism toolbox, but I find that the most important aspect of what I do is repetition. I never assume that my audience understand what I’m saying after a single session, or simply by reading one wiki page, blog post, or white paper. Quality internal evangelism requires regular delivery and enforcement, and an acknowledgement that my first engagements with teams will not have the impact I desire. When it comes to internal evangelism, I tend to encounter the following phases when engaging with internal teams: Silence - The first time I present material to internal teams I almost always encounter silence. Teams will often listen dutifully, but rarely will engage with me, ask questions, and want to get to the details of what is going on. I can rarely assess the state of things, and find that the silence stems from a range of emotions, ranging from not caring at all, to being very distrustful of what I am presenting. I can never assume that teams will care, trust me, and that silence is always...[<a href="/2019/08/16/four-phases-of-internal-api-evangelism/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/35201856153_61bc075e4b-nazi-invasion.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/15/an-observable-regulatory-provider-or-industry-api-management-gateway/">An Observable Regulatory Provider Or Industry API Management Gateway</a></h3>
			<p><em>15 Aug 2019</em></p>
			<p>I wrote a separate piece on an API gateway specification standard recently, merging several areas of my research and riffing on a recent tweet from Sukanya Santhanam (@SukanyaSanthan1). I had all these building blocks laying around as part of my research on API gateways, but also from the other areas of the API lifecycle that I track on. Her tweet happened to coincide with other thoughts I had simmering, so I wanted to jump on the opportunity to publish some posts, and see if I could slow jam a conversation in this area. Now, after I defined what I’d consider to be a core API gateway set of building blocks, I wanted to take another crack at refining my vision for how we make it more observable and something that could be used as a tech sector regulatory framework. Copying and pasting from my API gateway core specification, here is what my v1 draft vision for an API gateway might be: Paths - Allowing many different API paths to exist. Schema - Allowing me to manage all of my schema. Integrations - Providing backend lego architecture. Resource - Allow for integration with other APIs. Database - Provide a stack of database integrations. Other - Define whole buffet of integration definitions. Requests - Define all of my HTTP 1.1 requests Methods - Providing me with my HTTP verbs. Path Parameters - Able to define path parameters. Query Parameters - Able to define query parameters. Bodies - Providing control over the request body. Headers - Full management of HTTP request headers. Encoding - Defining the media types in in use for requests. Validate - Providing validation for all incoming requests. Mappings - Allowing for mapping of requests to backend. Transformations - Transformation before sending to backend.. Examples - Ensuring there are samples for each request. Schema - Able to reference all schema used in requests. Tags - Being able to organize API requests using tags. Responses -...[<a href="/2019/08/15/an-observable-regulatory-provider-or-industry-api-management-gateway/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-algo-microservices.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/15/an-api-platform-reliability-engineer-at-stripe/">An API Platform Reliability Engineer At Stripe</a></h3>
			<p><em>15 Aug 2019</em></p>
			<p>I find that the most interesting and telling API building blocks come out of the companies who are furthest along in their API journey, and have made the conscious effort to heavily invest in their platforms. I try to regularly visit API platforms who are doing the most interesting work on a regular basis, because I am almost always able to find some gem of an approach that I can showcase here on the blog. This weeks gem is from API rockstar Stripe, and their posting for a reliability engineer for their API platform. Here is a summary from their job posting: As a Reliability Engineer, you’ll help lead an active area of high impact by defining and building proactive ways to further hone the reliability of our API. You’ll collaborate with team members across Engineering, as well as with our business, sales and operations teams to determine areas of greatest leverage. You Will: Work with engineers across the company to identify key areas for reliability improvement. Gather requirements and make thoughtful tradeoffs to ensure we are focusing our efforts on the most impactful projects. Work on services and tools to proactively improve the quality and reliability of our production API. Debug production issues across services and multiple levels of the stack. Improve operational standards, tooling, and processes. I’ve studied thousands of APIs over almost a decade, and seeing a company invest this heavily in API reliability is a rare thing. For me, this demonstrates two things, that Stripe takes their API seriously, and that it takes a huge amount of investment and resources to do APIs right. Something I don’t think many API providers realize as they try to do APIs as a side project, and wonder why they aren’t seeing the results they want. I find that the job postings for API providers is one of the most telling signals I can harvest to understand how committed a company is to their APIs....[<a href="/2019/08/15/an-api-platform-reliability-engineer-at-stripe/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-IMG_8312.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/05/some-of-the-api-gateway-building-blocks/">Some Of The API Gateway Building Blocks</a></h3>
			<p><em>05 Aug 2019</em></p>
			<p>The inspiration for this post wasn’t fully mine, I’m borrowing and building upon what Sukanya Santhanam (@SukanyaSanthan1) tweeted out the other day. It is a good idea, and something that should be open sourced and moved forward. I’ve been studying with API management since 2010, and using gateways for over 15 years. I’ve watched gateways evolve, and partnered regularly with API management and gateway providers (Shout out to Tyk). Modern API gateways aren’t your grandfather’s SOA tooling, they’ve definitely gone through several iterations. While I still prefer hand rolling and forging my APIs out back in my woodshed on an anvil, I find myself working with a lot of different API gateways lately. I’ve kept feeling like I needed to map out the layers of what I’d consider to be a modern API gateway, and begin providing links to the most relevant API gateways out there, and the most common building blocks for an API gateway. Now that you can find API gateways baked into the fabric of the cloud, it is time that we work to standardize the definition of what they can deliver. I’m not looking to change what already is. Actually, I’m looking to just document and build on what already is. As with every other stop along the API lifecycle I’m looking to just map out the common building blocks, and establish a blueprint going forward the might influence existing API gateway providers, as well as any newcomers. After going through my API gateway research for a while, I quickly sketched out these common building blocks for helping deploy, manage, monitor, and secure your APIs: Paths - Allowing many different API paths to exist. Schema - Allowing me to manage all of my schema. Integrations - Providing backend lego architecture. Resource - Allow for integration with other APIs. Database - Provide a stack of database integrations. Other - Define whole buffet of integration definitions. Requests - Define all of my HTTP...[<a href="/2019/08/05/some-of-the-api-gateway-building-blocks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-DSC_0035-2.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/05/a-look-at-behavioral-api-patents/">A Look At Behavioral API Patents</a></h3>
			<p><em>05 Aug 2019</em></p>
			<p>I have been studying uses of behavioral technology lately. Riffing off my partner in crimes work on the subject, but putting my own spin on it, and adding APIs (of course) into the mix. Applying on of my classic techniques, I figured I’d start with a patent search for “behavioral application programming interfaces”. I find patents to be a “wonderful” source of inspiration and understanding when it comes to what is going on with technology. Here are the top results for my patent search, with title, abstract, and a link to understand more about each patent. User-defined coverage of media-player devices on online social networks In one embodiment, a method includes detecting, by a media-player device including multiple antennas, a client system of a user is within a wireless communication range of the media-player device. In response to the detection, the media-player device broadcasts an authentication key for the user of the client system. The media-player device then registers the user to the media-player device based on the authentication key being verified by the client system. The media-player device further receives from the client system instructions to adjust a power level of each of the multiple antennas. The instructions are determined based on broadcast signals received at the client system and on a respective position of the client system associated with each received broadcast signal. The respective position of the client system is determined with respect to a position of the media-player device. Controlling use of vehicular Wi-Fi hotspots by a handheld wireless device A system and method of controlling use of vehicular Wi-Fi hotspots by a handheld wireless device includes: detecting that the handheld wireless device is communicating via a Wi-Fi hotspot; determining at the handheld wireless device that that the Wi-Fi hotspot is provided by a vehicle; and enabling one or more default prohibitions against transmitting low-priority data from the handheld wireless device via a cellular wireless carrier system while the handheld wireless...[<a href="/2019/08/05/a-look-at-behavioral-api-patents/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-square-97-193-800-500-0-max-0-1--1-square.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/02/reverse-engineering-mobile-apis-to-show-a-company-their-public-apis/">Reverse Engineering Mobile APIs To Show A Company Their Public APIs</a></h3>
			<p><em>02 Aug 2019</em></p>
			<p>One story I tell a lot when talking to folks about APIs, is how you can reverse engineer a mobile phone to map out the APIs being used. As the narrative goes, many companies that I talk with claim they do not have any public APIs when first engage with them. Then I ask them, “Do you have any mobile applications?”. To which the answer is almost always, “Yes!”. Having anticipated this regular conversation, if I am looking to engage with a company in the area of API consulting, I will have made the time to reverse engineer their application to produce a set of OpenAPI definitions that I can then share with them, showing that they indeed have public APIs. The process isn’t difficult, and I’ve written about this several times. To reverse engineer a mobile application, all you have to do is download the targeted application to your phone, configure your phone to use your laptop as a proxy, and be running Charles Proxy on your laptop. I’m not going to share a walkthrough of this, it is easy enough to Google and find the technical details of doing it, I’m more looking to just educate the average business person that this is possible. Once you have your mobile phone proxied through Charles, it will capture every call made home to the mother ship, logging the request and response structure of all APIs being used by the mobile application-—which uses public DNS for routing, making it a public API. I have an API that I developed which I can upload the resulting Charles Proxy output file, and convert all the API calls into an OpenAPI. Making quick work of documenting the APIs behind a mobile application. Which, when you share with someone who is under the belief that their mobile APIs are private APIs, it can make quite a splash. Usually you get a response, like “well, we don’t allow access to the...[<a href="/2019/08/02/reverse-engineering-mobile-apis-to-show-a-company-their-public-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/amusement-park-amusement-park-2-satan-red.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/02/didnt-we-already-do-that/">Didn’t We Already Do That?</a></h3>
			<p><em>02 Aug 2019</em></p>
			<p>When you are in the API game you hear this phrase a lot, “didn’t we already do that?”. It is a common belief system that because something was already done, that it means it will not work ever again. When you are operate solely in a computational world, you tend to see things as 1s and 0s, and if something was tried and “didn’t work”, there is no resetting of that boolean switch for some reason. We excel at believing things are done, without ever unpacking why something failed, or how the context may have changed. One of the things I’m going to do over the next couple months is go through the entire SOA toolbox and take accounting of everything we threw away, and evaluate what the possible reasoning were behind it—-good and bad. I strongly believe that many folks who abandoned SOA, willingly or unwillingly, and especially the people who enjoy saying, “didn’t we already do that”, haven never spent time unpacking why we did, why it didn’t work, let alone whether or not it might work today. I think this paradigm reflects one of the fundamental illnesses we encounter in the tech sector-—we have a dysfunctional and distorted relationship with the past (this is by design). I’ve written about this before, but I’ll say it again. Can you imagine saying, “didn’t we already do that” about other non-technical things. Fashion. Art. Music. Stories. Law. Why do we say it in technology? When it comes to SOA, there are many reasons why it didn’t work overall, and most of the reasons were not technical. So why would we not want to re-evaluate some of the technologies and practices to see if there would be a new opportunity to apply an old pattern or approach? This question isn’t just something I’ve heard a handful of times. There have been literally a hundred plus blog posts on API Evangelist where people have commented this—-especially when...[<a href="/2019/08/02/didnt-we-already-do-that/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/nazi-invasion-dark-hallway.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/01/the-future-of-apis-will-be-in-the-browser/">The Future Of APIs Will Be In The Browser</a></h3>
			<p><em>01 Aug 2019</em></p>
			<p>I have been playing with the new browser reporting API lately, and while it isn’t widely supported, it does work in Chrome, and soon Firefox. I won’t go into too much technical detail, but the API provides an interesting look at reporting on APIs usage in the browser. Offering a unique view into the shadows of what is happening behind the curtain in our browser when we are using common web applications each day. I have been proxying my web traffic for a long time to produce a snapshot at the domains who are operating beneath the covers, but it is interesting for browsers to begin baking in a look at the domains who are violating, generating errors, and other shenanigans. As I’m contemplating the API discovery universe I can’t help but think of the how “API innovation” is occurring within the browser. When I say “API innovation”, I don’t mean the kind that got us all excited from 2005 through 2010, or the golden days from 2010 through 2015-—I am talking the exploitative kind. Serving advertisers, trackers, and other exploitative practices. Most people would scoff at me calling these things APIs, but they are using the web to deliver machine readable information, so they are APIs. I’ve been tracking on the APIs I use behind the scene in my browser using Charles Proxy for a while now, but I’m feeling I should formalize my analysis. I’m thinking I’ll take a sampling of domain, maybe 250+, and automate the browsing of each page, while also running through Charles Proxy. Then aggregate all of the domains that are loaded, and categorize them by media type–just to give me a sampling of the APIs in operation behind the scenes of some common sites. I’m sure most are advertising or social related, but I’m guessing there are a lot of other surprises in there. While some of the APIs will be publicly showcased in some way, there are...[<a href="/2019/08/01/the-future-of-apis-will-be-in-the-browser/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/gears-4882162452-fa3126b38d-b-wols.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/08/01/about-giving-away-api-knowledge-for-free/">About Giving Away API Knowledge For Free</a></h3>
			<p><em>01 Aug 2019</em></p>
			<p>I’m in the business of providing access to the API knowledge accumulated over the last decade. Despite what many people claim, I do not know everything about APIs, but after a decade I have picked up a few things along the way. Historically, I have really enjoyed sharing my knowledge with people, but I’m increasingly becoming weary of sharing to openly because of the nature of how business gets conducted online. Specifically, that there are endless waves of folks who want to tap what I know without giving anything in return, who work at companies who enjoy a lot of resources. I know people have read the startup handbook, which tells them to reach out to people who know and get their view, but when everyone does this, and doesn’t give anything in return, it is, well…I guess business as usual? Right? ;-( Taking a sampling from the usual week in my inbox, I’ll get a handful of requests reflecting these personas: Analysts / Consultants - Other analysts reaching out to share information, and get my take on what I’m seeing. There is usually reciprocity here, so I’m usually happy to engage, especially if I know them personally, and have worked with them before. Startup Founders - I get a wide range of startup founders reaching out, many of which I do not know, wanting to get validation of their idea, and understand the marketplace they are targeting—usually if I know them, or they come with a reference I’ll engage. Venture Capitalists - There is a regular stream of VCs wanting to know what is happening, get my take on things, but they usually are just interested listin validating what they already know, and get introduced to some new concepts. Students - There is a growing number of students reaching out, and increasingly PHD students who are working on something API related as part of their studies. This represents the usual suspects. There are plenty...[<a href="/2019/08/01/about-giving-away-api-knowledge-for-free/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/containership-containership-blue-circuit-5.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/">The Challenges Of API Discovery Conversations Being Purely Technical</a></h3>
			<p><em>31 Jul 2019</em></p>
			<p>Ironically one of the biggest challenges facing API discovery on the web, as well as within the enterprise, is that most conversations focus purely on the technical, rather than the human and often business implications of finding and putting APIs to work. The biggest movement in the realm of API discovery in the last couple years has been part of the service mesh evolution of API infrastructure, and how your gateways “discover” and understand the health of APIs or microservices that provide vital services to applications and other systems. Don’t get me wrong, this is super critical, but it is more about satisfying a technical need, which is also being fueled by an investment wave-—it won’t contribute to much to the overall API discovery and search conversation because of it’s limited view of the landscape. Runtime API discovery is critical, but there are so many other times we need API discovery to effectively operate the enterprise. Striving for technical precision at runtime is a great goal, but enabling all your groups, both technical and business to effectively find, understand, engage, and evolve with existing APIs should also be a priority. It can be exciting to focus on the latest technological trends, but doing the mundane profiling, documentation, and indexing of existing API infrastructure can have a much larger business impact. Defining the technical details of your API Infrastructure using OpenAPI, Postman, and other machine readable formats is just the beginning, ideally you are also working define the business side of things along the way. I find that defining APIs using OpenAPI and JSON Schema to be grueling work. However, I find documenting the teams and owners behind APIs, the licensing, dependencies (both technical and business), pricing, and other business aspects of an API to be even more difficult. Over the last decade we’ve gotten to work standardizing how define the technical surface area of our APIs, but we’ve done very little work to standardize how...[<a href="/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/35201856153_61bc075e4b-udnie.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/">Differences Between API Observability Over Monitoring, Testing, Reliability, and Performance</a></h3>
			<p><em>31 Jul 2019</em></p>
			<p>I’ve been watching the API observability coming out of Stripe, as well as Honeycomb for a couple years now. Then observability of systems is not a new concept, but it is one that progressive API providers have embraced to help articulate the overall health and reliability of their systems. In control theory, observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs. Everyone (except me) focuses only on the modern approaches for monitoring, testing, performance, status, and other common API infrastructure building blocks to define observability. I insist on adding the layers of transparency and communication, which I feel are the critical aspects of observability—-I mean if you aren’t transparent and communicative about your monitoring, testing, performance, and status, does it really matter? I work to define observability as a handful of key API building blocks that every API provider should be investing in: Monitoring - Actively monitoring ALL of your APIs to ensure they are up and running. Testing - Performing tests to ensure APIs aren’t just up but also doing what they are intended to. Performance - Adding an understanding of how well your APIs are delivering to ensure they perform as expected. Security - Actively locking down, scanning, and ensuring all your API infrastructure is secure. Many folks rely on the outputs from these areas to define observability, but there are a couple more ingredients needed to make it observable: Transparency - Sharing the practices and results from each of these areas is critical. Communication - If you aren’t talking about these things regularly they do not exist. Status - Providing real time status updates for al these areas is essential. You can be actively observing the outputs from monitoring, testing, performance, and security operations, but if this data isn’t accessible to other people on your team, within or company, partners, and for the public as required, then things aren’t observable....[<a href="/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/abe-lincoln-one-smooth-ride-file-00-00-07-91.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/30/peer-api-design-review-sessions/">Peer API Design Review Sessions</a></h3>
			<p><em>30 Jul 2019</em></p>
			<p>Public APIs have always benefitted from something that internal APIs do not always received—-feedback from other people. While the whole public API thing didn’t play out as I originally imagined, there is still a lot of benefit in letting other see, and provide open feedback on your API. It is painful for many developers to receive feedback on their API, and it is something that can be even more painful when it is done publicly. This is why so many of us shy away from establishing feedback loops around our APIs. It is hard work to properly design, develop, and then articulate our API vision to an external audience. It is something I understand well, but still suffer from when it comes to properly accessioning peer review and feedback on my API designs. I prefer opening up to peer reviews of my API designs while they are still just mocks. I’m less invested in them at this point, and it is easier to receive feedback on them. It is way less painful to engage in an ongoing discussion fo what an API should (and shouldn’t) do early on, then it is to define the vision, deliver an API as code or within a gateway, and then have people comment on your baby that you have given birth to. It hurts to have people question your vision, and what you’ve put forth. Especially for us fragile white men who who aren’t often very good at accepting critical feedback, and want to just be left to our own devices. I’d much prefer just being a coder, but around 2008 through 2010 I saw the benefits to my own personal development when I opened up my work to my peers and let a little sunlight in. I am a better developer because of it. One tool in my API toolbox that is growing in importance is the peer, and open API design review sessions. Taking an OpenAPI draft,...[<a href="/2019/07/30/peer-api-design-review-sessions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/abandonedbuildings_blue_circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/">API For Processing Common Logging Formats And Generating OpenAPI Definitions</a></h3>
			<p><em>30 Jul 2019</em></p>
			<p>I’ve invested a lot of time in the last six months into various research, scripts, and tooling to help me with finding APIs within the enterprise. This work is not part my current role, but as a side project to help me get into the mindset of how to help the enterprise understand where their APIs are, and what APIs they are using. Almost every enterprise group I have consulted for has trouble keeping tabs on what APIs are being consumed across the enterprise, and I’m keen on helping understand what the best ways are to help them get their API houses in order. While there are many ways to trace out how APIs are being consumed across the enterprise, I want to start with some of the basics, or the low hanging when it came to API logging within the enterprise. I’m sure there are a lot of common logging locations to tackle, but my list began with some of the common cloud platforms in use for logging of operations to begin my work—focusing on the following three cloud logging solutions: Amazon CloudFront - Beginning with the cloud leader, and looking at how the enterprise is centralizing their logs with CloudFront. Google StackDriver - Next, I found Google’s multi-platform approach interesting and worth evaluating as part of this work. Azure Logging - Of course, I have to include Azure in all of this as they are a fast growing competitor to Amazon in this space. After establishing a short list of cloud platforms logging solutions, I began looking at which of the common web server formats I should be looking for within these aggregate logging locations, trying to map out how the enterprise is logging web traffic. Providing me with a short list of the three most common web server formats I should be looking at when it comes to mapping the enterprise API landscape—-providing artifacts of the APIs that enterprise groups are operating....[<a href="/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/long-factory-uncle-sam.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/28/api-storytelling-within-the-enterprise/">API Storytelling Within The Enterprise</a></h3>
			<p><em>28 Jul 2019</em></p>
			<p>Storytelling is important. Storytelling within the enterprise is hard. Reaching folks on the open web is hard work to, but there is usually an audience that will eventually tune in, and over time you can develop and cultivate that audience. The tools you have at your disposal within the enterprise are much more prescribed and often times dictated–even controlled. I also find that they aren’t always as effective as they are made out to be, with the perception being one thing, and the reach, engagement, and noise being then much harder realities you face when trying to get a message out. Email might seem like a good idea, and is definitely a critical tool when reaching specific individuals or groups, but as a general company wide thing, it quickly becomes exponentially ineffective with each person you add on as CC. I’d say that you are better off creating a daily or weekly email newsletter if you are going to be sending across large groups of the enterprise rather than participating in the constant email barrage that occurs on a daily basis. Email is an effective tool when used properly, but I’d say I haven’t perfected the art of using email to reach my intended audience within the enterprise. My preferred storytelling format is relatively muted within the enterprise — people rarely read blogs in this world. Blog reading is something you do out on the web apparently. This means I have to get pretty creative when it comes to getting your stories out. It doesn’t mean you shouldn’t be using this format of storytelling, but you just can’t count on folks to regularly consume a blog, or subscribe to an RSS feed. You can still have a blog, but you have to find other ways of slipping the links into existing conversations, documentation, and other avenues in which people consume information within the enterprise. I would say this reality of reading within the enterprise is...[<a href="/2019/07/28/api-storytelling-within-the-enterprise/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/gears-4882162452-fa3126b38d-b-blue-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/24/apis-and-browser-bookmarklets/">APIs and Browser Bookmarklets</a></h3>
			<p><em>24 Jul 2019</em></p>
			<p>I have quite a few API driven bookmarklets I use to profile APIs. I recently quit using Google Chrome, so I needed to migrate all of them to Firefox. I saw this work as an opportunity to better define and organize them, as they had accumulated over the years without any sort of strategy. When I need some new functionality in my browser I would create a new API, and craft a bookmarklet that would accomplish whatever I needed. I wish I had more browser add-on development skills, something I regular try to invest in, but I find that bookmarklets are the next best thing when it comes to browser and API interactions. There are a number of tasks I am looking to accomplish when I&rsquo;m browsing the web pages of an API provider. The first thing I want to do is record their domain, then retrieve as much intelligence about the company behind the domain in a single click of the bookmarklet. This was the first bookmarklet and API I developed. Since then, I&rsquo;ve made numerous others to record the pricing page, parse the terms of service, OpenAPI, and other valuable API artifacts from across the landscape. Bookmarklets are a great way to provide just a little more context combined with a URL pointer, for harvesting, processing, and possibly some human review. Allowing me to augment, enrich, and automate how I consume information as I&rsquo;m roaming around the web, researching specific topics, and do what I do. At this point I am actually glad I didn&rsquo;t invest a lot of energy into developing Chrome browser extension, because it wouldn&rsquo;t have easily translated to a Firefox world. Since I have been investing in APIs plus bookmarklets, I can easily import, or copy and paste my bookmarklets over. I&rdquo;m spending the time to go through them, inventory them, and better organize them for optimal usage, so the migration is a little more work than just import...[<a href="/2019/07/24/apis-and-browser-bookmarklets/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/legal-statue-legalstatue-smoking-cigarette.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/">Absolutism Around API Tools Increases Friction And Failure</a></h3>
			<p><em>24 Jul 2019</em></p>
			<p>I know you believe your tools are the best. I mean, from your vantage point, they are. I also know that when you are building a new API tool, your investors want you to position your tooling as the best. The one solution to rule them all. They want you to work hard to make your tools the best, but also make sure and demonize other tooling as being inferior, out of date, and something the dinosaurs use. I know this absolute belief in your tooling feels good and right, but you are actually creating friction for your users, and potentially failure or at least conflict within their teams. Absolutism, along with divide and conquer strategies for evangelizing API tooling works for great short term financial strategies, but doesn’t do much to help us on the ground actually developing, managing, and sustaining APIs. Ironically, there are many divers factors that contribute to why API tooling providers and their followers resort to absolutism when it comes to marketing and evangelizing their tools. Much of which has very little to do with the merits of the tools being discussed, and everything about those who are making the tools. I wanted to explore a few of them so they are available on the tip of my tongue while working within the enterprise. No Due Diligence On What Is Out There - Most startups who are developing API tooling do not spend the time understanding what already exists across the landscape, and get outside of the echo chamber to learn what real world companies are using to get the job done each day. No Learning Around Using Existing Tools - Even if startups are aware of existing tools, patterns, and processes, they rarely invest the time to actually understand what existing tools deliver—spending time to deeply understand how existing tools are being put to use by their would-be customers. Lack Of Awareness Around The Problem - There is a...[<a href="/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/los-angeles-from-observatory-losangeles-from-observatory-purp-paper.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/23/the-higher-level-business-politics-that-i-am-not-good-at-seeing-in-the-api-space/">The Higher Level Business Politics That I Am Not Good At Seeing In The API Space</a></h3>
			<p><em>23 Jul 2019</em></p>
			<p>I have built successful startups. I’m good at the technology of delivering new solutions. I am decent at understanding and delivering much of business side of bringing new technological solutions to market. What I’m not good at is the higher level business politics that occur. These are the invisible forces behind businesses that I’m good at seeing, playing in, and almost always catch me off guard, too late, or just simply piss me off that they are going on behind the scenes. Unfortunately it is in this realm where most of the money is to be made doing APIs, resulting in me being written out of numerous successful API efforts, because I’m not up to speed on what is going on. Startups are great vehicles for higher level economic strategies. They are the playground of people with access to resources, and have economic visions that rarely jive with what is happening on the ground floors. Startup strategies count on a handful at the top understanding the vision, with most at the bottom levels not being able to see the vision, and small group of disposable stooges in the middle, ensuring that the top level vision is realized—at all costs. You can work full time at a startup, and even enjoy a higher level position, and still never see the political goings on that are actually motivating the investment in your startup. This is by design. The whole process depends on the lower levels working themselves to the bone, working on, marketing, and selling one vision, while there are actually many other things going on above, usually with a whole other set of numbers. After 30 years of playing in this game I still stuck at seeing the higher level influences. I’ve seen shiny API tooling solution after shiny API tooling solution arrive on the market, and I still fall for the shine. Ooooohhh, look at that. It will solve X, or Y problem. I really...[<a href="/2019/07/23/the-higher-level-business-politics-that-i-am-not-good-at-seeing-in-the-api-space/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/server-cloud-server-racks-clouds-copper-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/23/api-provider-and-consumer-developer-portals/">API Provider And Consumer Developer Portals</a></h3>
			<p><em>23 Jul 2019</em></p>
			<p>I’ve been studying API developer portals for almost a decade. I’ve visited the landing pages, portals, websites, and other incarnations from thousands of API providers. I have an intimate understanding of what is needed for API providers to attract, support, and empower API consumers. One area I’m deficient in, and I also think it reflects a wider deficiency in the API space, is regarding how to you make an API portal service both API providers and API consumers. Providing a single portal within the enterprise where everyone can come and understand how to deliver or consume an API. There are plenty of examples out there now when it comes to publishing an API portal for your consumers, but only a few that show you how to publish an API. I’d say the most common example are API marketplaces that allow both API consumers and providers to coexist, but this model isn’t exactly what you want within the enterprise. One thing the model lacks is the on-boarding of new developers when it comes to actually developing an API. Suffering from many of the same same symptoms API management service providers have historically suffered from—-not providing true assistance when it comes to delivering a quality API. When I envision an API portal that serves both providers and consumers, either publicly or privately, I envision just as much assistance when it comes to delivering a new API as we provide for new consumers of an API. Helping with API definition, design, deployment, management, testing, monitoring, documentation, and other critical stops along the API lifecycle. We need to see more examples of the split between API provider and consumers, equally helping both sides of the coin get up to speed, and be successful with what they are looking to achieve. I think we’ve spend almost 15 years investing in perfecting and monetizing the API portal with a focus not he consumer, and now we need to invest on helping...[<a href="/2019/07/23/api-provider-and-consumer-developer-portals/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/IMG_4038_blue_circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/22/the-role-having-awareness-of-your-api-traffic-plays-in-api-security/">The Role Having Awareness Of Your API Traffic Plays In API Security</a></h3>
			<p><em>22 Jul 2019</em></p>
			<p>One of the biggest reasons we adopt new technology, and justify the development of new technology, is we do not want to do the heavy lifting when it comes to what we already have in place. A common illness when it comes to API security that I’ve been battling since 2015 is that you will have API security addressed once you adopted an API management solution. Your APIs require API keys, and thus they are secure. No further work necessary. The unwillingness or lack of knowledge regarding what is needed next, leaves a vacuum for new technology providers to come in and sell you the solution for what is next, when you should be doing more work to use the tools you already have. When it comes to API management, most vendors sold it as purely a security solution, and when companies implement it they become secure. Missing the entire point for why we do API management-—to develop an awareness of our API usage and consumption. Having keys for your APIs is not enough. You actually have to understand how those API consumers are putting API resources to work, otherwise your API security will always be deficient. Some of the fundamentals of API management you should be employing as part of your API security are: Registration - Make all developers sign of for API usage, establishing the terms of use. API Keys - Require all developers internal or external to use API keys for every application. API Usage - Which APIs are being used by all API consumers putting them to use in applications. API Errors - Understanding what the errors being generated are, and who is responsible for them. Logging - The logging of all API traffic, reconciling against what you know as reported usage. Invoicing - Invoicing of all consumers for their usage, even if they aren’t paying you money. Reporting - Provide reports on API usage for all stakeholders, to regularly develop...[<a href="/2019/07/22/the-role-having-awareness-of-your-api-traffic-plays-in-api-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-DSC-0084-dali-three.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/22/happy-path-api-testing-bias/">Happy Path API Testing Bias</a></h3>
			<p><em>22 Jul 2019</em></p>
			<p>I see a lot of happy path bias when it comes to the development of APIs, but specifically when it comes to crafting testing to ensure APIs are delivering as expected. Happy path is a term used in testing to describe the desired outputs a developer and product owner is looking for. Making the not so happy path being about testing for outcomes that a developer and product owner is not wanting to occur. When it comes to API development most developers and product owners are only interested in the happy path, and will almost always cut corners, minimize the investment in, or completely lack an imagination when it comes to less than happy path API testing. There are many reasons why someone will have a bias towards the happy path when developing an API. Every API provider is invested in achieving the happy path for delivering, providing, and consuming an API. This is what generates revenue. However, in this quest for revenue, we often become our own worst enemy. Shining a spotlight on the happy path, while being completely oblivious to what the not so happy paths will look like for end users. Why do we do this? Greed - We are so interested in getting an API up and running, used in our applications, and generating behavioral surplus, we are more than willing to ignore all other possible scenarios if we can easily meet our revenue goals by ignoring the unhappy path and there are no consequences. Tickets - Most development occurs using JIRA or other software development “tickets”, which tell developers what they are supposed to do to meet the requirements of their employment—tickets are written with the happy path in mind, and developers are rarely willing to do more. Imagination - While many of us technologists think we are imaginative creatures, most of us are pretty stuck in a computational way of thinking, and elaborating, iterating, and exploring beyond the initial...[<a href="/2019/07/22/happy-path-api-testing-bias/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-status-berlin-matrix.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/">What Makes You Think Your GraphQL Consumers Will Want To Do The Work</a></h3>
			<p><em>18 Jul 2019</em></p>
			<p>Data work is grueling work. I’ve been working with databases since my first job developing student information databases in 1988 (don’t tell my wife). I’ve worked with Cobol, Foxpro, SQL Server, Filemaker, Access, MySQL, PostGres, and now Aurora databases over the last 30 years. I like data. I can even trick myself into doing massive data and schema refinement tasks on a regular basis. It is still grueling work that I rarely look forward to doing. Every company I’ve worked for has a big data problem. Data is not easy work, and the more data you accumulate, the more this work grows out of control. Getting teams of people to agree upon what needs to happen when it comes to schema and data storage, and actually execute upon the work in a timely, cost effective, and efficient way is almost always an impossible task. Which leaves me questioning (again), why GraphQL believers think they are going to be successfully in offshoring the cognitive and tangible work load to understand what data delivers, and then successfully apply it to a meaningful and monetizable business outcome. Don’t get me wrong. I get the use cases where GraphQL makes sense. Where you have an almost rabid base of known consumers, who have a grasp on the data in play, and possesses an awareness of the schema behind. I’m have made the case for GraphQL as a key architectural component within a platform before. The difference in my approach over the majority of GraphQL believers, is that I’m acknowledging there is a specific group of savvy folks who need access to data, and understand the schema. I’m also being honest about who ends up doing the heavy data lifting here—-making sure this group wants it. However, I have an entirely separate group of users (the majority) who do not understand the schema, and have zero interest in doing the hard work to understand the schema, navigate relationships, and develop...[<a href="/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/lost-angeles-downtown-freeway-los-angeles-downtow-freeway-copper-circuit-2.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/18/what-is-an-application/">What Is An Application?</a></h3>
			<p><em>18 Jul 2019</em></p>
			<p>I have struggled asking this question in many discussions I&rsquo;ve had around the world, at technology conferences, on consulting projects, and in the back rooms of dimly lit bars. What is an application? You get ten different answers if you ask this question to ten different people. I&rsquo;d say the most common response is to reference the applications on a mobile device. These are the relevant. Most accessible. The most active and visible form of application in our modern life. Older programmers see them as desktop applications, where younger programmers see them as web applications, with varying grades of server applications in between. If you operate at the network layer, you&rsquo;ve undoubtedly bastardized the term to mean several different things. Personally, I&rsquo;ve always tried to avoid these obvious and tangible answers to this question, looking beyond the technology. My view of what an application is stems from a decade of studying the least visible, and least tangible aspect of an application, its programming interface. When talking to people about applications, the first question I ask folks is usually, &ldquo;do you know what an API is&rdquo;? If someone is API savvy I will move to asking, &ldquo;when it comes to application programming interface (API), who or what is being programmed? Is it the platform? The application? Or, is it the end-user of the applications?&rdquo; I&rsquo;ve spent a decade thinking about this question, playing it over and over in my end, never quite being satisfied with what I find. Honestly, the more I scratch, the more concerned I get, and the more I&rsquo;m unsure of exactly what an &ldquo;application&rdquo; is, and precisely who are what is actually being programmed. Let&rsquo;s further this line of thinking by looking at the definitions of &ldquo;application&rdquo;: noun - The act of applying. noun - The act of putting something to a special use or purpose. noun - A specific use to which something is put. noun - The capacity of being...[<a href="/2019/07/18/what-is-an-application/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/catacombs-catacombs-copper-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/17/the-many-ways-in-which-apis-are-taken-away/">The Many Ways In Which APIs Are Taken Away</a></h3>
			<p><em>17 Jul 2019</em></p>
			<p>APIs are notorious for going away. There are so many APIs that disappear I really stopped tracking on it as a data point. I used keep track of APIs that were shuttered so that I could play a role in the waves of disgruntled pitchfork mobs rising up in their wake–it used to be a great way to build up your Hacker News points! But, after riding the wave a couple hundred waves of APIs shuttering, you begin to not really not give a shit anymore—-growing numb to it all. API deprecation grew so frequent, I wondered why anyone would make the claim that once you start an API you have to maintain it forever. Nope, you can shut down anytime. Clearly. In the real world, APIs going away is a fact of life, but rarely a boolean value, or black and white. There are many high profile API disappearances and uprising, but there are also numerous ways in which some API providers giveth, and then taketh away from API consumers.: Deprecation - APIs disappear regularly both communicated, and not so communicated, leaving consumers scratching their heads. Disappear - Companies regularly disappear specific API endpoints acting like they were never there in the first place. Acquisition - This is probably one of the most common ways in which high profile, successful APIs disappear. Rate Limits - You can always rate limit away users make APIs inaccessible, or barely usable for users, essentially making it go away. Error Rates - Inject elevated error rates either intentionally or unintentionally can make an API unusable to everyone or select audience. Pricing Tiers - You can easily be priced out of access to an API making it something that acts just like deprecating for a specific group. Versions - With rapid versioning, usually comes rapid deprecation of earlier versions, moving faster than some consumers can handle. Enterprise - APIs moving from free or paid tier, into the magical enterprise,...[<a href="/2019/07/17/the-many-ways-in-which-apis-are-taken-away/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-old-door-lock-copper-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/17/paying-for-api-access/">Paying for API Access</a></h3>
			<p><em>17 Jul 2019</em></p>
			<p>APIs that I can’t pay for more access grinds my gears. I am looking at you GitHub, Twitter, Facebook, and a few others. I spend $250.00 to $1500.00 a month on my Amazon bill, depending on what I’m looking to get done. I know I’m not the target audience for all of these platforms, but I’m guessing there is a lot more money on the table than is being acknowledged. I’m guessing that the reason companies don’t cater to this, is that there are larger buckets of money involved in what they are chasing behind the scenes. Regardless, there isn’t enough money coming my way to keep my mouth shut, so I will keep bitching about this one alongside the inaccessible pricing tiers startups like to employ as well. I’m going to keep kvetching about API pricing until we are all plugged into the matrix—-unless the right PayPal payment lands in my account, then I’ll shut up. ;-) I know. I know. I’m not playing in all your reindeer startup games, and I don’t understand the masterful business strategy you’ve all crafted to get rich. I’m just trying to do something simple like publish data to GitHub, or do some basic research on an industry using Twitter. I know there are plenty of bad actors out there who want also access to your data, but it is all something that could be remedied with a little pay as you go pricing, applied to some base unit of cost applied to your resources. If I could pay for more Twitter and GitHub requests without having to be in the enterprise club, I’d be very appreciative. I know that Twitter has begun expanding into this area, but it is something that is priced out of my reach, and not the simple pay as you go pricing I prefer with AWS, Bing, and other APIs I happily spend money on. If you can’t apply a unit of value...[<a href="/2019/07/17/paying-for-api-access/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/server-cloud-server-racks-clouds-smoking-cigarette.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/16/imperative-declarative-and-workflow-apis/">Imperative, Declarative, and Workflow APIs</a></h3>
			<p><em>16 Jul 2019</em></p>
			<p>At every turn in my API work I come across folks who claim that declarative APIs solutions are superior to imperative ones. They want comprehensive, single implementation, do it all their way approaches, over granular, multiple implementation API calls that are well defined by the platform. Declarative calls allow you to define a single JSON or YAML declaration that can then be consumed to accomplish many things, abstracting away the complexity of doing those many things, and just getting it done. Imperative API interfaces require many individual API calls to tweak each and every knob or dial on the system, but is something that is often viewed as more cumbersome from a seasoned integrator, but for newer, and lower level integrators a well designed imperative API can be an important lifeline. Declarative APIs are almost always positioned against imperative APIs. Savvier, more experienced developers almost always want declarations. Where newer developers and those without a full view of the landscape, often prefer well designed imperative APIs that do one thing well. From my experience, I always try to frame the debate as imperative and declarative where the most vocal developers on the subject prefer to frame it as declarative vs imperative. I regularly have seasoned API developers “declare” that I am crazy for defining every knob and dial of an API resource, without any regard for use cases beyond what they see. They know the landscape, don’t want to be burdened them with having to pull every knob and dial, just give them one interface to declare everything they desire. A single endpoint with massive JSON or YAML post or abstract it all away with an SDK, Ansible, GraphQL, or a Terraform solution. Demanding that a platform meet their needs, without ever considering how more advanced solutions are delivered and executed, or the lower level folks who are on boarding with a platform, and may not have the same view of what is required to...[<a href="/2019/07/16/imperative-declarative-and-workflow-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-new-68-158-800-500-0-max-0-1--1.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/16/hoping-for-more-investment-in-api-design-tooling/">Hoping For More Investment In API Design Tooling</a></h3>
			<p><em>16 Jul 2019</em></p>
			<p>I was conducting an accounting of my API design toolbox, and realized it hasn’t changed much lately. It is still a very manual suite of tooling, and sometimes services, that help me craft my APIs. There are some areas I am actively investing in when it comes to API design, but honestly there really isn’t that much new out there to use. To help me remember how we got here, I wanted to take a quick walk through the history of API design, and check in on what is currently available when it comes to investing in your API design process. API design has historically meant REST. Many folks still see it this way. While there has been plenty of books and articles on API design for over a decade, I attribute the birth of API design to Jakub and Z at Apiary (https://apiary.io). I believe they first cracked open the seed of API design, and the concept API design first. Which is one of the reasons I was so distraught when Oracle bought them. But we won’t go there. The scars run deep, and where has it got us? Hmm? Hmm?? Anyways, they set into motion an API design renaissance which has brought us a lot of interesting thinking on API design, a handful of tooling and services, some expansion on what API design means, but ultimately not a significant amount of movement overall. Take a look at what AP+I Design (https://www.apidesign.com/) and API Plus (https://apiplus.com/) have done to architecture, API has done for the oil and gas industry (https://www.api.org/), and API4Design has done for the packaging industry (http://api4design.com/)—I am kidding. Don’t get me wrong, good things have happened. I am just saying we can do more. The brightest spot that represents the future for me is over at: Stoplight.io - They are not just moving forward API design, they are also investing in the full API lifecycle, including governance. I rarely plug startups,...[<a href="/2019/07/16/hoping-for-more-investment-in-api-design-tooling/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/adam-smith-adam-smith-purp-paper.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/15/what-is-an-api-contract/">What Is An API Contract?</a></h3>
			<p><em>15 Jul 2019</em></p>
			<p>I am big on regularly interrogating what I mean when I use certain phrases. I’ve caught myself repeating and reusing many hollow, empty, and meaningless phrases over my decade as the API Evangelist. One of these phrases is, “an API contract”. I use it a lot. I hear it a lot. What exactly do we mean by it? What is an API contract, and how is it different or similar to our beliefs and understanding around other types of contracts? Is it truth, or is just a way to convince people that what we are doing is just as legitimate as what came before? Maybe it is even more legitimate, like in a blockchain kind of way? It is an irreversible, unbreakable, digital contract think bro! If I was to break down what I mean when I say API contract, I’d start with being able to establish to a shared articulation of what an API does. We have an OpenAPI definition which describes the surface area of the request and response of each individual API method being offered. It is available in a machine and human readable format for both of us to agree upon. It is something that both API provider and API consumer can agree upon, and get to work developing and delivering, and then integrating and consuming. An API contract is a shared understanding of what the capabilities of a digital interface are, allowing for applications to be programmed on top of. After an API contract establishes a shared understanding, I’d say that an API contract helps mitigate change, or at leasts communicates it—-again, in a human and machine readable way. It is common practice to semantically version your API contracts, ensuring that you won’t remove or rename anything within a minor or patch release, committing to only changing things in a big way with each major release. Providing an OpenAPI of each version ahead of time, allowing consumers to review that...[<a href="/2019/07/15/what-is-an-api-contract/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-crypto-machine-bletchley-copper-circuit.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/12/my-primary-api-search-engines/">My Primary API Search Engines</a></h3>
			<p><em>12 Jul 2019</em></p>
			<p>I am building out several prototypes for the moving parts of an API search engine I want to build, pushing my usage of APIs.json and OpenAPI, but also trying to improve how I define, store, index, and retrieve valuable data about thousands of APIs through a simple search interface. I’m breaking out the actual indexing and search into their own areas, with rating system being another separate dimension, but even before I get there I have to actually develop the primary engines for my search prototypes, feeding the indexes with fresh signals of where APIs exist across the online landscape. There isn’t an adequate search engine out there, so I’m determined to jumpstart the conversation with an API search engine of my own. Something that is different from what web search engines do, and tailored to the unique mess we’ve created within the API industry. My index of APIs.json and OpenAPI definitions, even with a slick search interface is just a catalog, directory, or static index of a small piece of the APIs that are out there. I see a true API search engine as three parts The Humans Searching for APIs - Providing humans with web application to search for new and familiar APIs. The Search Engine Searching For APIs - Ensuring that the search engine is regularly searching for new APIs. Other Systems Searching For APIs - Providing an API for other systems to search for new and familiar APIs. Without the ability for the search engine to actually seek out new APIs, it isn’t a search engine in my opinion—-it is a search application. Without an API for searching for APIs, in my opinion, it isn’t an API search engine. It takes all three of these areas to make an application a true API search engine, otherwise we just have another catalog, directory, marketplace, or whatever you want to call it. To help me put the engine into my API search engine,...[<a href="/2019/07/12/my-primary-api-search-engines/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-beach-rocks-currents-internet-numbers.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/11/taking-a-fresh-look-at-the-nuance-of-api-search/">Taking A Fresh Look At The Nuance Of API Search</a></h3>
			<p><em>11 Jul 2019</em></p>
			<p>I have a mess of APIs.json and OpenAPI definitions I need to make sense of. Something that I could easily fire up an ElasticSearch instance, point at my API “data lake”, and begin defining facets and angles for making sense of what is in there. I’ve done this with other datasets, but I think this round I’m going to go a more manual route. Take my time to actually understand the nuance of API search over other types of search, take a fresh look at how I define and store API definitions, but also how I search across a large volume of data to find exactly the API I am looking for. I may end up going back to a more industrial grade solution in the end, but I am guessing I will at least learn a lot along the way. I am using API standards as the core of my API index—APIs.json and OpenAPI. I’m importing other formats like API Blueprint, Postman, RAML, HAR, and others, but the core of my index will be APIs.json and OpenAPI. This is where I feel solutions like ElasticSearch might overlook some of the semantics of each standard, and I may not immediately be able to dial-in on the preciseness of the APIs.json schema when it comes to searching API operations, and OpenAPI schema when it comes to searching the granular details of what each individual API method delivers. While this process may not get me to my end solution, I feel like it will allow me to more intimately understand each data point within my API index in a way that helps me dial-in exactly the type of search I envision. The first dimensions are of my API search index are derived from APIs.json schema properties I use to define every entity within my API search index: Name - The name of a company, organization, institution, or government agency. Description - The details of what a particular...[<a href="/2019/07/11/taking-a-fresh-look-at-the-nuance-of-api-search/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-DSC_0033.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/10/the-json-schema-tooling-in-my-life/">The JSON Schema Tooling In My Life</a></h3>
			<p><em>10 Jul 2019</em></p>
			<p>I am always pushing for more schema order in my life. I spend way too much time talking about APIs, when a significant portion of the API foundation is schema. I don’t have as many tools to help me make sense of my schema, and to improve them as definitions of meaningful objects. I don’t have the ability to properly manage and contain the growing number of schema objects that pop up in my world on a daily basis, and this is a problem. There is no reason I should be making schema objects available to other consumers if I do not have a full handle on what schema objects exist, let alone a full awareness of everything that has been defined when it comes to the role that each schema object plays in my operations. To help me better understand the landscape when it comes to JSON Schema tooling, I wanted to take a moment and inventory the tools I have bookmarked and regularly use as part of my daily work with JSON Schema: JSON Schema Editor - https://json-schema-editor.tangramjs.com/ - An editor for JSON Schema. JSON Schema Generator - https://github.com/jackwootton/json-schema - Generates JSON Schema from JSON JSON Editor - https://json-editor.github.io/json-editor/ - Generates form and JSON from JSON Schema. JSON Editor Online -https://github.com/josdejong/jsoneditor/ - Allows me to work with JSON in a web interface. Another JSON Schema Validator (AJV) - https://github.com/epoberezkin/ajv - Validates my JSON using JSON Schema. I am going to spend some time consolidating these tools into a single interface. They are all open source, and there is no reason I shouldn’t be localizing their operation, and maybe even evolving and contributing back. This helps me understand some of my existing needs and behavior when it comes to working with JSON Schema, which I’m going to use to seed a list of my JSON Schema needs, as drive a road map for things I’d like to see developed. Getting a little more structure...[<a href="/2019/07/10/the-json-schema-tooling-in-my-life/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-death-valley-national-park-dali-three-just-road.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/10/navigating-api-rate-limit-differences-between-platforms/">Navigating API Rate Limit Differences Between Platforms</a></h3>
			<p><em>10 Jul 2019</em></p>
			<p>I always find an API providers business model to be very telling about the company&rsquo;s overall strategy when it comes to APIs. I&rsquo;m currently navigating the difference between two big API providers, trying to balance my needs spread across very different approaches to offering up API resources. I&rsquo;m working to evolve and refine my API search algorithms and I find myself having to do a significant amount of work due to the differences between GitHub and Microsoft Search. Ironically, they are both owned by the same company, but we all know their business models are seeking alignment as we speak, and I suspect my challenges with GitHub API is probably a result of this alignment. The challenges with integrating with GitHub and Microsoft APIs are pretty straightforward, and something I find myself battling regularly when integrating with many different APIs. My use of each platform is pretty simple. I am looking for APIs. The solutions are pretty simple, and robust. I can search for code using the GitHub Search API, and I can search for websites using the Bing Search API. Both produce different types of results, but what both produce is of value to me. The challenge comes in when I can pay for each API call with Bing, and I do not have that same option with GitHub. I am also noticing much tighter restriction on how many calls I can make to the GitHub APIs. With Bing I can burst, depending on how much money I want to spend, but with GitHub I have no relief value&mdash;I can only make X calls a minute, per IP, per user. This is a common disconnect in the world of APIs, and something I&rsquo;ve written a lot about. GitHub (Microsoft) has a more &ldquo;elevated&rdquo; business model, with the APIs being just an enabler of that business model. Where Bing (Microsoft) is going with a much more straightforward API monetization strategy&mdash;pay for what you use. In...[<a href="/2019/07/10/navigating-api-rate-limit-differences-between-platforms/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-IMG_7559.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/09/the-details-of-my-api-rating-formula/">The Details Of My API Rating Formula</a></h3>
			<p><em>09 Jul 2019</em></p>
			<p>Last week I put some thoughts down about the basics of my API rating system. This week I want to go through each of those basics, and try to flesh out the details of how I would gather the actual data needed to rank API providers. This is a task I&rsquo;ve been through with several different companies, only to be abandoned, and then operated on my own for about three years, only to abandon once I ran low on resources. I&rsquo;m working to invest more cycles into actually defining my API rating in a transparent and organic way, then applying it in a way that allows me to continue evolving, while also using to make sense of the APIs I am rapidly indexing. First, I want to look at the API-centric elements I will be considering when looking at a company, organization, institution, government agency, or other entity, and trying to establish some sort of simple rating for how well they are doing APIs. I&rsquo;ll be the first to admit that ratings systems are kind of bullshit, and are definitely biased and hold all kinds of opportunity for going, but I need something. I need a way to articulate in real time how good of an API citizen an API provider is. I need a way to rank the searches for the growing number of APIs in my API search index. I need a list of questions I an ask about an API in both a manual, or hopefully automated way: Active / Inactive - APIs that have no sign of life need a lower rating. HTTP Status Code - Do I get a positive HTTP status code back when I ping their URL(s)? Active Blog - Does their blog have regular activity on it, with relevant and engaging content? Active Twitter - Is there a GitHub account designated for the API, and is it playing an active role in its operations? Active GitHub -...[<a href="/2019/07/09/the-details-of-my-api-rating-formula/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-server-cloud1-feed-people.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/08/thinking-differently-when-approaching-openapi-diffs-and-considering-how-to-layer-each-potential-change/">Thinking Differently When Approaching OpenAPI Diffs And Considering How To Layer Each Potential Change</a></h3>
			<p><em>08 Jul 2019</em></p>
			<p>I have a lot of OpenAPI definitions, covering about 2,000 separate entities. For each entity, I often have multiple OpenAPIs, and I am finding more all the time. One significant challenge I have in all of this centers around establishing a master “truth” OpenAPI, or series of definitive OpenAPIs for each entity. I can never be sure that I have a complete definition of any given API, so I want to keep vacuuming up any OpenAPI, Swagger, Postman, or other artifact I can, and compare it with the “truth” copy” I have on indexed. Perpetually layering the additions and changes I come across while scouring the Internet for signs of API life. This perpetual update of API definitions in my index isn’t easy, and any tool that I develop to assist me will be in need constant refinement and evolution to be able to make sense of the API fragments I’m finding across the web. There are many nuances of API design, as well as the nuances of how the OpenAPI specification is applied when quantifying the design of an API, making the process of doing a “diff” between two OpenAPI definitions very challenging. Rendering common “diff” tools baked into GitHub, and other solutions ineffective when it comes to understanding the differences between two API definitions that may represent a single API. These are some of the things I’m considering as I’m crafting my own OpenAPI “diff” tooling: Host - How the host is stored, defined, and applied across sandbox, production, and other implementations injects challenges. Base URL - How OpenAPI define their base url versus their host will immediately cause problems in how diffs are established. Path - Adding even more instability, many paths will often conflict with host and base URL, providing different fragments that show as differences. Verbs - Next I take account of the verbs available for any path, understanding what the differences are in methods applied. Summary - Summaries are...[<a href="/2019/07/08/thinking-differently-when-approaching-openapi-diffs-and-considering-how-to-layer-each-potential-change/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/fast-lights-freeway-redes-fast-flux-623x425-internet-numbers.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/05/why-the-open-data-movement-has-not-delivered-as-expected/">Why The Open Data Movement Has Not Delivered As Expected</a></h3>
			<p><em>05 Jul 2019</em></p>
			<p>I was having a discussion with my friends working on API policy in Europe about API discovery, and the topic of failed open data portals came up. Something that is a regular recurring undercurrent I have to navigate in the world of APIs. Open data is a subset of the API movement, and something I have first-hand experience in, building many open data portals, contributing to city, county, state, and federal open data efforts, and most notably riding the open data wave into the White House and working on open data efforts for the Obama administration. Today, there are plenty of open data portals. The growth in the number of portals hasn’t decreased, but I’d say the popularity, utility, and publicity around open data efforts has not lived up to the hype. Why is this? I think there are many dimensions to this discussion, and few clear answers when it comes to peeling back the layers of this onion, something that always makes me tear up. Nothing There To Begin With - Open data was never a thing, and never will be a thing. It was fabricated as part of an early wave of the web, and really never got traction because most people do not care about data, let alone it being open and freely available. It Was Just Meant To Be A Land Grab - The whole open data thing wasn’t about open data for all, it was meant to be open for business for a few, and they have managed to extract the value they needed, enrich their own datasets, and have moved on to greener pastures (AI / ML). No Investment In Data Providers - One f the inherent flaws of the libertarian led vision of web technology is that government is bad, so don’t support them with taxes. Of course, when they open up data sets that is goo for us, but supporting them in covering compute, storage, bandwidth, and...[<a href="/2019/07/05/why-the-open-data-movement-has-not-delivered-as-expected/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/san-francisco-city-bridge-sf-city-bridge-copper-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/03/api-interoperability-is-a-myth/">API Interoperability is a Myth</a></h3>
			<p><em>03 Jul 2019</em></p>
			<p>There are a number of concepts we cling to in the world of APIs. I’ve been guilting of inventing, popularizing, and spreading many myths in my almost decade as the API Evangelist. One of them that I’d like to debunk and be more realistic about is when it comes to API interoperability. When you are focused on just the technology of APIs, as well as maybe the low-level business of APIs, you are an API interoperability believer. Of course everyone wants API interoperability, and that all APIs should work seamlessly together. However, if you at all begin operating at the higher levels of the business of APIs, and spend any amount of time studying the politics of why and how we do APIs at scale, you will understand that API interoperability is a myth. This reality is one of the reasons us technologists who possess just a low-level understanding of how the business behind our tech operation, are such perfect tools for the higher level business thinkers, and people who successfully operate and manipulate at the higher levels of industries, or even at the policy level. We are willing to believe in API interoperability, and work to convince our peers that it is a thing, and we all work to expose, and open up the spigots across our companies, organizations, institutions, and government agencies. Standardized APIs and schema that play nicely with each other are valuable, but only within certain phases of a companies growth, or as part of a myth-information campaign to convince the markets that a company is a good corporate citizen. However, once a company achieves dominance, or the winds change around particular industry trends, most companies just want to ensure that all roads lead towards their profitability. Believing in API interoperability without a long term strategy means you are opening up your company to value extraction by your competitors. I don’t care how good your API management is, if your API...[<a href="/2019/07/03/api-interoperability-is-a-myth/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/machine-road-machine-road-blue-circuit-3.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/02/your-api-and-schema-is-that-complex-because-you-have-not-done-the-hard-work-to-simplify/">Your API and Schema Is That Complex Because You Have Not Done The Hard Work To Simplify</a></h3>
			<p><em>02 Jul 2019</em></p>
			<p>I find myself looking at a number of my more complex API designs, and saying to myself, “this isn’t complicated because it is hard, it is complicated because I did not spend the time required to simplify it appropriately”. There are many factors contributing to this reality, but I find that more often than not it is because I’m over-engineering something, and I am caught up in the moment focusing on a purely computation approach, and not considering the wider human, business, and other less binary aspects of delivering APIs. While I am definitely my own worst enemy in many API deliver scenarios, I’d say there are a wide range of factors that are influencing how well, or poorly that I design my API resources, with just a handful of them being: Domain - I just do not have the domain knowledge required to get the job done properly. Consumer - I just do not have the knowledge I need of my end consumers to do things right. Bandwidth - I just do not have the breathing room to properly sit down and make it happen. Narcissism - I am the best at this, I know what is needed, and I deem this complexity necessary. Lazy - I am just too damn lazy to actually dig in and get this done properly in the first place. Caring - I just do not give a shit enough to actually go the extra distance with API design. Dumb - This API is dumb, and I really should not be developing it in the first place. These are just a few of the reasons why I settle for complexity over simplicity in my API designs. It isn’t right. However, it seems to be a repeating pattern in some of my work. It is something that I should be exploring more. For me to understand why my work isn’t always of highest quality possible I need to explore each...[<a href="/2019/07/02/your-api-and-schema-is-that-complex-because-you-have-not-done-the-hard-work-to-simplify/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-containership-copper-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/01/the-complexity-of-api-discovery/">The Complexity of API Discovery</a></h3>
			<p><em>01 Jul 2019</em></p>
			<p>I can’t get API discovery out of my mind. Partly because I am investing significant cycles in this area at work, but it is also something have been thinking about for so long, that it is difficult to move on. It remains one of the most complex, challenging, and un-addressed aspects of the way the web is working (or not working) online today. I feel pretty strongly that there hasn’t been investment in the area of API discovery because most technology companies providing and consuming APIs prefer things be un-discoverable, for a variety of conscious and un-conscious reasons behind these belief systems.  What API Discovery Means? Depends On Who You Are… One of the reasons that API discovery does not evolve in any significant ways is because there is not any real clarity on what API discovery is. Depending on who you are, and what your role in the technology sector is, you’ll define API discovery in a variety of ways. There are a handful of key actors that contribute to the complexity of defining, and optimizing in the area of API discovery.   Provider - As an API provider, being able to discover your APIs, informs your operations regarding what your capabilities are, building a wider awareness regarding what a company, organization, institution, or government agency does, helping eliminate inefficiencies, and allows for more meaning decisions to be made at run-time across operations. Consumer - What you need as an internal consumer of APIs, or maybe a partner, or 3rd party developer will significantly shift the conversation around what API discovery means, and how it needs to be “solved”. There is another significant dimension to this discussion, separating human from other system consumption, further splintering the discussion around what API discovery is when you are a consumer of APIs. Analyst - As an analyst for specific industries, or technology in general, need to understand the industries they are watching, and how API tooling is...[<a href="/2019/07/01/the-complexity-of-api-discovery/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-square-supreme-court-judgement.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/07/01/the-basics-of-my-api-rating-formula/">The Basics of My API Rating Formula</a></h3>
			<p><em>01 Jul 2019</em></p>
			<p>I have been working on various approaches to rating APIs since about 2012. I have different types of algorithms, even having invested in operating one from about 2013 through 2016, which I used to rank my daily intake of API news. Helping me define what the cream on top of each industry being impacted by APIs, while also not losing site of interesting newcomers to the space. I have also had numerous companies and VCs approach me about establishing a formal API rating system—many of whom concluded they could do fairly easily and went off to try, then failed, and gave up. Rating the quality of APIs is subjective and very hard. When it comes to rating APIs I have a number of algorithms to help me, but I wanted to step back and think of it from a more simpler human vantage point, and after establishing a new overall relationship with the API industry. What elements do I think should exist within a rating system for APIs: Active / Inactive - APIs that have no sign of life need a lower rating. Free / Paid - What something costs impacts our decision to use or not. Openness - Is an API available to everyone, or is a private thing. Reliability - Can you depend on the API being up and available. Fast Changing - Does the API change a lot, or remain relatively stable. Social Good - Does the API benefit a local, regional, or wider community. Exploitative - Does the API exploit its users data, or allow others to do so. Secure - Does an API adequately secure its resources and those who use it. Privacy - Does an API respect privacy, and have a strategy for platform privacy. Monitoring - Does a platform actively monitor its platform and allow others as well. Observability - Is there visibility into API platform operations, and its processes. Environment - What is the environment footprint or...[<a href="/2019/07/01/the-basics-of-my-api-rating-formula/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/machine-road-machine-road-blue-circuit-3.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/06/25/why-schema-org-does-not-see-more-adoption-across-the-api-landscape/">Why Schema.org Does Not See More Adoption Across The API Landscape</a></h3>
			<p><em>25 Jun 2019</em></p>
			<p>I’m a big fan of Schema.org. A while back I generated an OpenAPI 2.0 (fka Swagger) definition for each one and published to GitHub. I’m currently cleaning up the project, publishing them as OpenAPI 3.0 files, and relaunching the site around it. As I was doing this work, I found myself thinking more about why Schema.org isn’t the goto schema solution for all API providers. It is a challenge that is multi-layered like an onion, and probably just as stinky, and will no doubt leave you crying. First, I think tooling makes a big difference when it comes to why API providers haven’t adopted Schema.org by default across their APIs. If more API design and development tooling would allow for the creation of new APIs using Schema.org defined schema, I think you’d see a serious uptick in the standardization of APIs that use Schema.org. In my experience, I have found that people are mostly lazy, and aren’t overly concerned with doing APIs right, they are just looking to get them delivered to meet specifications. If we provide them with tooling that gets the API delivered to specifications, but also in a standardized, they are doing to do it. Second, I think most API providers don’t have the time and bandwidth to think of the big picture like using standardized schema for all of their resources. Most people are under pressure to more with less, and standards is something that can be easily lost in the shuffle when you are just looking to satisfy the man. It takes extra effort and space to realize common standards as part of your overall API design. This is a luxury most companies, organizations, and government agencies can not afford, resulting in many ad hoc APIs defined in isolation. Third, I think some companies just do not care about interoperability. Resulting in them being lazy, or not making it a priority as stated in the first and second points. Most...[<a href="/2019/06/25/why-schema-org-does-not-see-more-adoption-across-the-api-landscape/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/meadowbutterfly-meadow-butterfly-internet-numbers.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/06/24/avoiding-complexity-and-just-deploying-yaml-json-and-csv-apis-using-github-or-gitlab/">Avoiding Complexity and Just Deploying YAML, JSON, and CSV APIs Using GitHub or GitLab</a></h3>
			<p><em>24 Jun 2019</em></p>
			<p>I find that a significant portion of I should be doing when defining, designing, developing, and delivering an API is all of avoiding complexity. Every step away along the API journey I am faced with opportunities to introduce complexity, forcing me to constantly question and say no to architectural design decisions. Even after crafting some pretty crafty APIs in my day, I keep coming back to JSON or YAML within Git, as the most sensible API architectural decision I can make. Git, with JSON and YAML stored within a repository, fronted with a Jekyll front-end does much of what I need. The challenge with selling this concept to others is that it is a static publish approach to APIs, instead of a dynamic pull of relevant data. This approach isn’t for every API solution, I’m not in the business selling one API solution to solve all of our problems. However, for many of the API uses I’m building for, a static Git-driven approach to publishing machine readable JSON or YAML data is a perfect low cost, low tech solution to delivering APIs. A Git repository hosted on GitHub or GitLab will store a significant amount of JSON, YAML, or CSV data. Something you can easily shard across multiple repositories within an organization / group, as well as across many repositories within many organization / groups. Both GitHub and GitLab offer free solutions, essentially letting you publish as many repositories as you want. As I said earlier, this is not a solution to all API needs, but when I’m looking to introduce some constraints to keep things low cost, simple, and easy to use and manage—a Git-driven API is definitely worth considering. However, going static for your API will force you to think about how you automate the lifecycle of your data, content, and other resources. The easiest way to manage JSON, CSV, or YAML data you have on GitHub or GitLab is to use the...[<a href="/2019/06/24/avoiding-complexity-and-just-deploying-yaml-json-and-csv-apis-using-github-or-gitlab/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-containership-copper-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/06/19/organizing-my-apis-using-openapi-tags/">Organizing My APIs Using OpenAPI Tags</a></h3>
			<p><em>19 Jun 2019</em></p>
			<p>I like my OpenAPI tags. Honestly, I like tags in general. Almost every API resource I design ends up having some sort of tagging layer. Too help me organize my world, I have a centralized tagging vocabulary that I use across my JSON Schema, OpenAPI, and AsyncAPI, to help me group, organize, filter, publish, and maintain my catalog of API and schema resources. The tag object for the OpenAPI specification is pretty basic, allowing you to add tags for an entire API contract, as well as apply them to each individual API method. Tooling, such as API documentation uses these tags to group your API resources, allowing you to break down your resources into logical bounded contexts. It is a pretty basic way of defining tags, that can go a long ways depending on how creative you want to get. I am extending tags with an OpenAPI vendor extension, but I also see that there is a issue submitted suggesting they move the specification forward by allowing for the nesting of tags&ndash;potentially taking OpenAPI tagging to the next level. I&rsquo;m allowing for a handful of extensions to the OpenAPI specification to accomplish the following: Tag Grouping - Help me nest, and build multiple tiers of tags for organization APIs. Tag Sorting - Allowing me to define a sort order that goes beyond an alphabetical list. I am building listing, reporting, and other management tools based up OpenAPI tagging to help me in the following areas: Tag Usage - Reporting how many resources are available for each tag, and tag group. Tag Cleanup - Helping me de-dupe, rename, deal with plural challenges, etc. Tag Translations - Translating old tags into new tags, helping keep things meaningful. Tag Clouds - Generating D3.js tag clouds from the tags applied to API resources. Packages - Deployment of NPM packages based upon different bounded contexts defined by tags. I am applying tags to the following specifications, stretching my OpenAPI tagging...[<a href="/2019/06/19/organizing-my-apis-using-openapi-tags/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/gears-4882162452-fa3126b38d-b-umberto-bocc.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/06/17/doing-the-hard-work-to-define-apis/">Doing The Hard Work To Define APIs</a></h3>
			<p><em>17 Jun 2019</em></p>
			<p>Two years later, I am still working to define the API driven marketplace that is my digital self. Understanding how I generate revenue from my brand (vomits in mouth a little bit), but also fight off the surveillance capitalists from mining and extracting value from my digital existence. It takes a lot of hard work to define the APIs you depend on to do business, and quantify the digital bits that you are transacting on the open web, amongst partners, and unknowingly with 3rd parties. As an individual, I find this to be a full time job, and within the enterprise, it is something that everyone will have to own a piece of, which in reality, is something that is easier said than done. Convincing enterprise leadership of the need to be aware of every enterprise capability being defined at the network, system, or application level is a challenge, but doable. Getting consensus on how to do this at scale, across the enterprise will be easier said than done. Identifying how the network, system, and applications across a large organization are being accessed, what schema, messages, and other properties are being exchanged is not a trivial task. It is a massive undertaking to reverse engineer operations, and identify the core capabilities being delivered, then define and document in a coherent way that can be shared with others, and included as part of an organization messaging campaign. Many will see work to define all enterprise API capabilities as a futile task–something that is impossible to deliver. Many others will not see the value of doing it in the first place, and unable to realize the big picture, they will see defining of APIs and underlying schema as meaningless busy work. Even when you do get folks on-board with the important, having the discipline to see the job through becomes a significant challenge. If moral is low within any organization group, and team members do not have...[<a href="/2019/06/17/doing-the-hard-work-to-define-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/rockingchair-face-2-atari-asteroids.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/06/16/there-is-no-single-right-way-to-do-apis/">There Is No Single Right Way To Do APIs</a></h3>
			<p><em>16 Jun 2019</em></p>
			<p>My time working in the API sector has been filled with a lot of lessons. I researched hard, paid attention, and found a number of surprising realities emerge across the API landscape. The majority of surprises have been in the shadows caused by my computational belief scaffolding I’ve been erecting since the early 1980s. A world where there has to be absolutes, known knowns, things are black and white, or more appropriately 1 or 0. If I studied all APIs, I’d find some sort of formula for doing APIs that is superior to everyone else’s approach to doing APIs. I was the API Evangelist man–I could nail down the one right way to do APIs. (Did I mention that I’m a white male autodidact?) I was wrong. There is no one right way to do APIs. Sure, there are many different processes, practices, and tools that can help you optimize your API operations, but despite popular belief, there is no single “right way” to do define, design, provide, or consume APIs. REST is not the one solution. Hypermedia is not the one solution. GraphQL is not the one solution. Publish / Subscribe is not the one solution. Event-driven is not the one solution. APIs in general are the the one solution. Anyone telling you there is one solution to doing APIs for all situations is selling you something. Understanding your needs, and what the pros and cons of different approaches are, is the only thing that will help you. If you are hyper focused on the technology, it is easy to believe in a single right way of doing things. Once you start having to deliver APIs in a variety of business sectors and domains, you will quickly begin to see your belief system in a single right way of doing things crumble. Different types of data require different types of approaches to API enablement. Different industries are knowledgable in different ways of API enablement,...[<a href="/2019/06/16/there-is-no-single-right-way-to-do-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/christianity-christianity-under-construction-copper-circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/06/12/api-definitions-are-important/">API Definitions Are Important</a></h3>
			<p><em>12 Jun 2019</em></p>
			<p>I found myself scrolling down the home page of API Evangelist and thinking about what topic(s) I thought were still the most relevant in my mind after not writing about APIs for the last six months. Hands down it is API definitions. These machine and human readable artifacts are the single most important thing for me when it comes to APIs I’m building, and putting to work for me. Having mature, machine readable API definitions for all API that you depend on, is essential. It also takes a lot of hard work to make happen. It is why I went API define first a long time ago, defining my APIs before I ever get to work designing, mocking, developing, and deploying my APIs. Right now, I’m heavily dependent on my: JSON Schema - Essential for defining all objects being used across API contracts. OpenAPI - Having OpenAPI contracts for al my web APIs is the default–they drive everything. AsyncAPI - Critical for defining all of my non HTTP 1.1 API contracts being provided or consumed. Postman Collections - Providing me with the essential API + environment definitions for run-time use. APIs.json - Helping me define all the other moving parts of API operations, indexing all my definitions. While there is plenty of other stops along the API lifecycle that are still relevant to me, my API definitions are hands down the most valuable intellectual property I possess. These four API specifications are essential to making my world work, but there are other more formalized specifications I’d love to be able to put to work: Orchestrations - I’d liked to see a more standardized, machine readable way for working with many API calls in a meaningful way. I know you can do this with Postman, and I’ve done with OpenAPI, and like Stoplight.io’s approach, but I want an open source solution. Licensing - I am not still actively using API Commons, but I’d like to invest...[<a href="/2019/06/12/api-definitions-are-important/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/open-nen.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2019/06/10/api-evangelist-is-open-for-business/">API Evangelist Is Open For Business</a></h3>
			<p><em>10 Jun 2019</em></p>
			<p>After six months of silence I've decided to fire API Evangelist back up again. I finally reached a place where I feel like I can separate out the things that caused me to step back in the first place. Mostly, I have a paycheck now, some health insurance, and I don't have to pretend I give a shit about APIs, startups, venture capital, and the enterprise. I'm being paid well to do an API job. I can pay my rent. I can go to the doctor when my health takes a hit. My basic needs are met. Beyond that, I'm really over having to care about building an API community, making change with APIs, and counteracting all of the negative effects of APIs in the wild. I can focus on exactly what interests me about technology, and contribute to the 3% of the API universe that isn't doing yawnworthy, half-assed, or straight up shady shit. I don't have to respond to all the emails in my inbox just because I need to eat, and have a roof over my head. I don't have to jump, just because you think your API thing is the next API thing. I can just do me, which really is the founding principle of API Evangelist. Third, I got a kid to put through college, and I'm going to make y'all pay for it. So, API Evangelist is open for business. I won't be producing the number of stories I used to. I'll only be writing about things I actually find interesting, and will explore other models for generating content, traffic, and revenue. So reach out, and pitch me. I'm looking for sponsors, and open to almost anything. Don't worry, I'll be my usual honest self and tell you whether I'm interested or not, and have strong opinions on what should be said, but pitch me. I'm open for business, I'll entertain any business offer keep API Evangelist in forward...[<a href="/2019/06/10/api-evangelist-is-open-for-business/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/kin-mountain_feed_people.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/11/27/asking-the-honest-questions-when-it-comes-to-your-api-journey/">Asking The Honest Questions When It Comes To Your API Journey</a></h3>
			<p><em>27 Nov 2018</em></p>
			<p>I engage with a lot of enterprise organizations in a variety of capacities. Some are more formal consulting and workshop engagements. While others are just emails, direct messages, and random conversation in the hallways and bars around API industry events. Many conversations are free flowing, and they trust me to share my thoughts about the API space, and provide honest answers to their questions regarding their API journey. Where others express apprehension, concern, and have trouble engaging with me because they are worried about what I might say about their approach to doing APIs within their enterprise organizations. Some have even told me that they’d like to formally bring me in for discussions, but they can’t get me pass legal or their bosses–stating I have a reputation for being outspoken. While in Vegas today, I had breakfast with Paolo Malinverno, analyst from Gartner, he mentioned the Oscar Wilde quote, “Whenever people agree with me I always feel I must be wrong.” Referring to the “yes” culture than can manifest itself around Gartner, but also within industries and the enterprise regarding what you should be investing in as a company. That people get caught up in up in culture, politics, and trends, and don’t always want to be asking, or be asked the hard questions. Which is the opposite of what any good API strategist, leader, and architect should be doing. You should be equipped and ready to be asked hard questions, and be searching out the hard questions. This stance is fundamental to API success, and you will never find what you are seeking when it comes to your API journey if you do not accept that many questions will be difficult. The reality that not all API service providers truly want to help enterprise organizations genuinely solve the business challenges they face, and that many enterprise technology leaders aren’t actually concerned with truly solving real world problems, has been one of the toughest pills...[<a href="/2018/11/27/asking-the-honest-questions-when-it-comes-to-your-api-journey/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/machine-road_atari_missle.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/11/25/a-diverse-api-toolbox-driving-hybrid-integrations-across-an-eventdriven-landscape/">A Diverse API Toolbox Driving Hybrid Integrations Across An Event-Driven Landscape</a></h3>
			<p><em>25 Nov 2018</em></p>
			<p>I’m heading to Vegas in the morning to spend two days in conversations with folks about APIs. I am not there for AWS re:Invent, or the Gartner thingy, but I guess in a way I am, because there are people there for those events, who want to talk to me about the API landscape. Folks looking to swap stories about enterprise API investment in possessing a diverse API toolbox for driving hybrid integrations in an event-driven landscape. I’m not giving any formal talks, but as with any engagement, I’m brushing up on the words I use to describe what I’m seeing across the space when it comes to the enterprise API lifecycle. The Core Is All About Doing Resource Based, Request And Response APIs Well I’m definitely biased, but I do not subscribe to popular notions that at some point REST, RESTful, web, and HTTP APIs will have to go away. We will be using web technology to provide simple, precise, useful access to data, content, and algorithms for some time to come, despite the API sectors continued evolution, and investment trends coming and going. Sorry, it is simple, low-cost, and something a wide audience gets from both a provider and consumer perspective. It gets the job done. Sure, there are many, many areas where web APIs fall short, but that won’t stop success continuing to be defined by enterprise organizations who can do microservices well at scale. Despite relentless assaults by each wave of entrepreneurs, simple HTTP APIs driving microservices will stick around for some time to come. API Discovery And Knowing Where All Of Your APIs Resources Actually Are API discovery means a lot of things to a lot of people, but when it comes to delivering APIs well at scale in a multi-cloud, event-driven world, I’m simply talking about knowing where all of your API resources are. Meaning, if I walked into your company tomorrow, could you should me a complete list...[<a href="/2018/11/25/a-diverse-api-toolbox-driving-hybrid-integrations-across-an-eventdriven-landscape/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/old-door-lock-2_marcel_duchamp.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/11/23/yaml-api-management-artifacts-from-aws-api-gateway/">YAML API Management Artifacts From AWS API Gateway</a></h3>
			<p><em>23 Nov 2018</em></p>
			<p>I’ve always been a big supporter of creating machine readable artifacts that help define the API lifecycle. While individual artifacts can originate and govern specific stops along the API lifecycle, they can also bring value when applied across other stops along the API lifecycle, and most importantly when it comes time to govern everything. The definition and evolution of individual API lifecycle artifacts is the central premise of my API discovery format APIs.json–which depends on there being machine readable elements within the index of each collection of APIs being documented, helping us map out the entire API lifecycle. OpenAPI provides us with machine readable details about the surface area of our API which can be used throughout the API lifecycle, but it lacks other details about the surface area of our API operations. So when I do come across interesting approaches to extending the OpenAPI specification which are also injecting a machine readable artifact into the OpenAPI that support other stops along the API lifecycle, I like to showcase what they are doing. I’ve become very fond of one within the OpenAPI export of any AWS API Gateway deployed API I’m working with, which provides some valuable details that can be used as part of both the deployment and management stops along the API lifecycle: x-amazon-apigateway-integration: uri: "http://example.com/path/to/the/code/behind/" responses: default: statusCode: "200" requestParameters: integration.request.querystring.id: "method.request.path.id" passthroughBehavior: "when_no_match" httpMethod: "GET" type: "http" This artifact is associated with each individual operation within my OpenAPI. It tells the AWS gateway how to deploy and manage my API. When I first import this OpenAPI into the gateway, it will deploy each individual path and operation, then it helps me manage it using the rest of the available gateway features. From this OpenAPI definition I can design, then autogenerate and deploy the code behind each individual operation, then deploy each individual path and operation to the AWS API Gateway and map them to the code behind. I can do this...[<a href="/2018/11/23/yaml-api-management-artifacts-from-aws-api-gateway/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/journey/journey-bridge.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/11/23/the-api-journey/">The API Journey</a></h3>
			<p><em>23 Nov 2018</em></p>
			<p>I’ve been researching the API space full time for the last eight years, and over that time I have developed a pretty robust view of what the API landscape looks like. You can find almost 100 stops along what I consider to be the API lifecycle on the home page of API Evangelist. While not every organization has the capacity to consider all 100 of these stops, they do provide us with a wealth of knowledge generated throughout my own journey. Where I’ve been documenting what the API pioneers have been doing with their API operations, how startups leverage simple web API infrastructure, as well as how the enterprise has been waking up to the API potential in the last couple of years. Over the years I’ve tapped this research for my storytelling on the blog, and for the white papers and guides I’ve produced. I use this research to drive my talks at conferences, meetups, and the workshops I do within the enterprise. I’ve long had a schema for managing my research, tracking on the APIs, companies, people, tools, repos, news, and other building blocks I track across the API universe. Now, after a year of working with them on the ground at enterprise organizations, I’m partnering with Streamdata.io (SDIO) to continue productizing my approach to the API lifecycle, which we are calling Journey, or specifically SDIO Journey. Our workshops are broken into four distinct areas of the lifecycle: Discovery (Goals, Definition, Data Sources, Discovery Sources, Discovery Formats, Dependencies, Catalog, Communication, Support, Evangelism) - Defining your digital resources are and what your enterprise capabilities are. Design (Definitions, Design, Versioning, Webhooks, Event-Driven, Protocols, Virtualization, Testing, Landing Page, Documentation, Support, Communication, Road Map, Discovery) - Going API first, as well as API design first when it comes to the delivery of all of your API resources. Development (Definitions, Discovery, Virtualization, Database, Storage, DNS, Deployment, Orchestration, Dependencies, Testing, Performance, Security, Communication, Support) - Considering what is needed...[<a href="/2018/11/23/the-api-journey/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-butterfly-vertical.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/11/21/what-does-the-next-chapter-of-storytelling-look-like-for-api-evangelist/">What Does The Next Chapter Of Storytelling Look Like For API Evangelist?</a></h3>
			<p><em>21 Nov 2018</em></p>
			<p>I find myself refactoring API Evangelist again this holiday season. Over the last eight years of doing API Evangelist I’ve had to regularly adjust what I do to keep it alive and moving forward. As I close up 2018, I’m finding the landscape shifting underneath me once again, pushing me to begin considering what the next chapter of API Evangelist will look like. Pushing me to adjust my presence to better reflect my own vision of the world, but hopefully also find balance with where things are headed out there in the real world. I started API Evangelist in July of 2010 to study the business of APIs. As I was researching things in 2010 and 2011 I first developed what I consider to be the voice of the API Evangelist, which continues to be the voice I use in my storytelling here in 2018. Of course, it is something that has evolved and matured over the years, but I feel I have managed to remain fairly consistent in how I speak about APIs throughout the journey. It is a voice I find very natural to speak, and is something that just flows on some days whether I want it to or not, but then also something I can’t seem to find at all on other days. Maintaining my voice over the last eight years has required me to constantly adjust and fine tune, perpetually finding the frequency required to keep things moving forward. First and foremost, API Evangelist is about my research. It is about me learning. It is about me crafting stories that help me distill down what I’m learning, in an attempt to articulate to some imaginary audience, which has become a real audience over the years. I don’t research stuff because I’m being paid (not always true), and I don’t tell stories about things I don’t actually find interesting (mostly true). API Evangelist is always about me pushing my skills forward...[<a href="/2018/11/21/what-does-the-next-chapter-of-storytelling-look-like-for-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109_201_800_500_0_max_0_-5_-5.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/11/16/the-ability-to-link-to-api-service-provider-features-in-my-workshops-and/">The Ability To Link To API Service Provider Features In My Workshops And</a></h3>
			<p><em>16 Nov 2018</em></p>
			<p>All of my API workshops are machine readable, driven from a central YAML file that provides all the content and relevant links I need to deliver what I need during a single, or multi-day API strategy workshop. One of the common elements of my workshops are links out to relevant resource, providing access to services, tools, and other insight that supports whatever I’m covering in my workshop. There are two parts to this equation, 1) me knowing to link to something, and 2) being able to link to something that exists. A number of API services and tooling I use don’t follow web practices and do not provide any easy way to link to a feature, or other way of demonstrating the functionality that exists. The web is built on this concept, but along the way within web and mobile applications, we’ve have seemed to lose our understanding for this fundamental concept. There are endless situations where I’m using a service or tool, and think that I should reference in one of my workshops, but I can’t actually find any way to reference as a simple URL. Value buried within a JavaScript nest, operating on the web, but not really behaving like you depend on the web. Sometimes I will take screenshots to illustrate the features of a tool or service I am using, but I’d rather have a clean URL and bookmark to a specific feature on a services page. I’d rather give my readers, and workshop attendees the ability to do what I’m talking about, not just hear me talk about it. In a perfect world, every feature of a web application would have a single URL to locate said feature. Allowing me to more easily incorporate features into my storytelling and workshops, but alas many UI / UX folks are purely thinking about usability and rarely thinking about instruct-ability, and being able to cite and reference a feature externally, using the fundamental...[<a href="/2018/11/16/the-ability-to-link-to-api-service-provider-features-in-my-workshops-and/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/flickr/361347580_2d9d02b83d_z.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/11/06/flickr-and-reconciling-my-history-of-apis-storytelling/">Flickr And Reconciling My History Of APIs Storytelling</a></h3>
			<p><em>06 Nov 2018</em></p>
			<p>Flickr was one of the first APIs that I profiled back in 2010 when I started API Evangelist. Using their API as a cornerstone of my research, resulting in their API making it into my history of APIs storytelling, continuing to be a story I’ve retold hundreds of times in the conversations I’ve had over the eight years of being the API Evangelist. Now, after the second (more because of Yahoo?) acquisition, Flickr users are facing significant changes regarding the number of images we can store on the platform, and what we will be charged for using the platform–forcing me to step back, and take another look at the platform that I feel has helped significantly shape the API space as we know it. When I step back and think about Flickr, it’s most important contribution to the world of APIs was all about the resources it made available. Flickr was the original image sharing API, powering the growing blogosphere at the beginning of this century. Flickr gave us a simple interface for humans in 2004, and an API for other applications just six months later, that provided us all with a place to upload the images we would be using across our storytelling on our blogs. Providing the API resources that we would be needed to power the next decade of storytelling via our blogs, but also set into the motion the social evolution of the web, demonstrating that images were an essential building block of doing business on the web, and in just a couple of years, on the new mobile devices that would become ubiquitous in our lives. Flickr was an important API resource, because it provided access to an important resource–our images. The API allowed you to share these meaningful resources on your blog, via Facebook and Twitter, and anywhere else you wanted. In 2005, this was huge. At the time, I was working to make a shift from being an...[<a href="/2018/11/06/flickr-and-reconciling-my-history-of-apis-storytelling/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/IMG_7598.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/11/01/the-impact-of-travel-on-being-the-api-evangelist/">The Impact Of Travel On Being The API Evangelist</a></h3>
			<p><em>01 Nov 2018</em></p>
			<p>Travel is an important part of what I do. It is essential to striking up new relationships, and reenforcing old ones. It is important for me to get out of my bubble, expose myself to different perspectives, and see the world in different ways. I am extremely grateful for the ability to travel around the US, and the world the way that I do. I am also extremely aware of the impact that travel has on me being the API Evangelist–the positive, the negative, and the general shift in my tone in storytelling after roaming the world. One of the most negative impact that traveling has on my world is on my ability to complete blog posts. If you follow my work, when I’m in the right frame of mind, I can produce 5-10 blog posts across the domains I write for, on a daily basis. The words just do not flow in the same way when I am on the road. I’m not in a storyteller frame of mind. At least in the written form. When I travel, I am existing in a more physical and verbal sense as the API Evangelist, something that doesn’t always get translated into words on my blog(s). This is something that is ok for short periods of time, but after extended periods of time on the road, it is something that will begin to take a toll on my overall digital presence. After the storytelling impact, the next area to suffer when I am on the road, is my actual project work. I find it very difficult to write code, or think at architectural levels while on the road. I can flesh out and move forward smaller aspects of the projects I’m working on, but because of poor Internet, packed schedules, and the logistics of being on the road, my technical mind always suffers. This is something that is also related to the impact on my overall storytelling....[<a href="/2018/11/01/the-impact-of-travel-on-being-the-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/machine-road_copper_circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/10/22/what-are-your-enterprise-api-capabilities/">What Are Your Enterprise API Capabilities?</a></h3>
			<p><em>22 Oct 2018</em></p>
			<p>I spend a lot of time helping enterprise organizations discover their APIs. All of the organizations I talk to have trouble knowing where all of their APIs are–even the most organized of them. Development and IT groups have just been moving too fast over the last decade to know where all of their web services, and APIs are. Resulting in large organizations not fully understanding what all of their capabilities are, even if it is something they actively operate, and may drive existing web or mobile applications. Each individual API within the enterprise represents a single capability. The ability to accomplish a specific enterprise tasks that is valuable to the business. While each individual engineer might be aware of the capabilities present on their team, without group wide, and comprehensive API discovery across an organization, the extent of the enterprise capabilities is rarely known. If architects, business leadership, and any other stakeholder can’t browse, list, search, and quickly get access to all of the APIs that exist, the knowledge of the enterprise capabilities will not be able to be quantified or articulated as part of regular business operations. In 2018, the capabilities of any individual API is articulated by it’s machine readable definition. Most likely OpenAPI, but could also be something like API Blueprint, RAML, or other specification. For these definitions to speak to not just the technical capabilities of each individual API, but also the business capabilities, they will have to be complete. Utilizing a higher level strategic set of tags that help label and organize each API into a meaningful set of business capabilities that best describes what each API delivers. Providing a sort of business capabilities taxonomy that can be applied to each API’s definition and used across the rest of the API lifecycle, but most importantly as part of API discovery, and the enterprise digital product catalog. One of the first things I ask any enterprise organization I’m working with upon...[<a href="/2018/10/22/what-are-your-enterprise-api-capabilities/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/events/paris-api-meetup/DqJd3bkJ.jpeg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/10/22/join-me-for-a-fireside-chat-at-the-paris-api-meetup-this-wednesday/">Join Me For A Fireside Chat At The Paris API Meetup This Wednesday</a></h3>
			<p><em>22 Oct 2018</em></p>
			<p>I am in Europe for most of October, and while I am in Paris we thought it would be a good idea to pull together a last minute API Meetup. Romain Simiand (@RomainSimiand), the API Evangelist at PeopleDoc was gracious enough to help pull things together, and the Streamdata.io team is stepping up to help with food and drink. Pulling together a last minute gathering at PeopleDoc in Paris, and bringing me on stage to talk about the technology, business, and politics of APIs, well as about some of my recent work on API discovery, and event-driven architecture. You can find more details on the Paris API Meetup site, with directions on how to find PeopleDoc. Make sure you RSVP so that we know you are coming, and of course, please help spread the word. We are over 30 people attending so far, but I think we can do better. I’m happy to get on stage and help drive the API discussion, but I’d prefer to have a healthy representation of the Paris API community asking questions, helping me understand what is happening across the area when it comes to APIs. I always have plenty of knowledge to share, but it becomes exponentially more valuable when people on the ground within communities are asking questions, and making it relevant to what is happening within the day to day operations of companies in the local area. While I enjoy doing conference keynotes and panels, my favorite format of event is the Meetup. Bringing together less than 100 people have a discussion about APIs. I always find that I learn the most in this environment, and able to actually engage with developers and business folks about what really matters when it comes to APIs. The larger the audience the more it is just about me broadcasting my message, and when it is a smaller and more intimate venue, I feel like I can better connect with people....[<a href="/2018/10/22/join-me-for-a-fireside-chat-at-the-paris-api-meetup-this-wednesday/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apis4dgov/DpyR9qrXoAAYo4r.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/10/22/i-participated-in-an-api-workshop-with-the-european-commission-last-week/">I Participated In An API Workshop With The European Commission Last Week</a></h3>
			<p><em>22 Oct 2018</em></p>
			<p>I was in Ispra, Italy last week for a two day workshop on APIs with the European Commission. The European Commission’s DG CONNECT together with the Joint Research Centre (JRC) launched a study with the purpose to gain further understanding of the current use of APIs in digital government and their added value for public services, and they invited me to participate. I was joined by Mehdi Medjaoui (@medjawii), David Berlind (@dberlind), and Mark Boyd (@mgboydcom), along with EU member states, and European cities, to help provide feedback and strategies for consideration by the commission. This European Commission study is looking at “innovative ways to improve interconnectivity of public services and reusability of public sector data, including dynamic data in real-time, safeguarding the data protection and privacy legislation in place.” Looking to: assess digital government APIs landscape and opportunities to support the digital transformation of public sector identify the added value for society and public administrations of digital government APIs (key enablers, drivers, barriers, potential risks and mitigates) define a basic Digital Government API EU framework and the next steps David Berlind from ProgrammableWeb gave a couple talks, with myself, Mehdi, and Mark following up. The rest of the time spent was hearing presentations from EU member states, and other municipal efforts–learning more about the successes and the challeges they face. What I heard reflected what I’ve experienced in federal government, as well as city, county, and state level API efforts I’ve participated in across the United States. &lt;p&gt;&lt;/p&gt;All groups were struggling to win over leaders and the public, modernize legacy system, build on top of open data efforts, and push forward the conversation using a modern approach to delivering web APIs. I am eager to see what comes out of the European Commission API project. While there are still interesting things happening in the United States, I feel like there is an opportunity for the EU to leap frog us when it comes to...[<a href="/2018/10/22/i-participated-in-an-api-workshop-with-the-european-commission-last-week/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/kin-lane-api-days-spain.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-discovery/">API Evangelist API Lifecycle Workshop on API Discovery</a></h3>
			<p><em>11 Oct 2018</em></p>
			<p>I’ve been doing more workshops on the API lifecycle within enterprise groups lately. Allowing me to refine my materials on the ground within enterprise groups, further flesh out the building blocks I recommend to API groups to help them craft their own API strategy. One of the first discussions I start with large enterprise groups is always in the area of API discovery, or commonly asked as, “do you know where all your APIs are?” EVERY group I’m working with these days is having challenges when it comes to easy discovery across all the digital resources they possess, and put to use on a daily basis. I’m working with a variety of companies, organizations, institutions, and government agencies when it comes to the API discovery of their digital self: Low Hanging Fruit (outline) - Understanding what resources are already on the public websites, and applications, by spidering existing domains looking for data assets that should be delivered as API resources. Discovery (outline) - Actively looking for web services and APIs that exist across an organization, industry, or any other defined landscape. Documenting, aggregating, and evolving what is available about each API, while also publishing back out and making available relevant teams. Communication (outline) - Having a strategy for reaching out to teams and engaging with them around API discovery, helping the remember to register and define their APIs as part of wider strategy. Definitions (outline) - Work to make ensure that all definitions are being aggregated as part of the process so that they can be evolved and moved forward into design, development and production–investing in all of the artifacts that will be needed down the road. Dependencies (outline) - Defining any dependencies that are in play, and will play a role in operations. Auditing the stack behind any service as it is being discovered and documented as part of the overall effort. Support (outline) - Ensure that all teams have support when it comes...[<a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-discovery/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/43043431_10156747264069813_2487933138479611904_n.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-design/">API Evangelist API Lifecycle Workshop on API Design</a></h3>
			<p><em>11 Oct 2018</em></p>
			<p>I’ve been doing more workshops on the API lifecycle within enterprise groups lately. Allowing me to refine my materials on the ground within enterprise groups, further flesh out the building blocks I recommend to API groups to help them craft their own API strategy. One area of the API lifecycle I find more groups working on these days, centers around a design-first approach to the API lifecycle. While not many groups I work with achieved a design-first approach doing APIs, almost all of them I talk to express interest in making this a reality at least within some groups, or projects. The appeal of being able to define, design, mock, and iterate upon an API contract before code gets written is very appealing to enterprise API groups, and I’m looking to help them think through this part of their API lifecycle, and work towards making API design first a reality at their organization. Definition (outline) - Using definitions as the center of the API design process, developing an OpenAPI contract for moving things through the design phase, iterating, evolving, and making sure the definitions drive the business goals behind each service. Design (outline) - Considering the overall approach to design for all APIs, executing upon design patterns that are in use to consistently deliver services across teams. Leveraging a common set of patterns that can be used across services, beginning with REST, but also evetually allowing the leveraging of hypermedia, GraphQL, and other patterns when it comes to the deliver of services. Versioning (outline) - Managing the definition of each API contract being defined as part of the API design stop for this area of the lifecycle, and having a coherent approach to laying out next steps. Virtualization (outline) - Providing mocked, sandbox, and virtualized instances of APIs and other data for understanding what an API does, helping provide an instance of an API that reflects exactly how it should behave in a production environment. Testing (outline) - Going beyond...[<a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-design/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://gist.github.com/kinlane/5e52d6063a0744d711795beb6e60365f.js" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/10/09/the-layers-of-completeness-for-an-openapi-definition/">The Layers Of Completeness For An OpenAPI Definition</a></h3>
			<p><em>09 Oct 2018</em></p>
			<p>Everyone wants their OpenAPIs to be complete, but what that really means will depend on who you are, what your knowledge of OpenAPI is, as well as being driven by your motivation for having an OpenAPI in the first place. I wanted to take a crack at articulating a complete(enough) definition for OpenAPIs I create, based upon what I’m needing them to do. Info &amp; Base - Give the basic information I need to understand who is behind, and where I can access the API. Paths - Provide an entry for every path that is available for an API, and should be included in this definition. Parameters - Provide a complete list of all path, query, and header parameters that can be used as part of an API. Descriptions - Flesh out descriptions for all the path and parameter descriptions, helping describe an API does. Enums - Publish a list of all the enumerated values that are possible for each parameter used as part of an API. Definitions - Document the underlying schema being returned by creating a JSON schema definition for the API. Responses - Associate the definition for the API with the path using a response reference, connecting the dots regarding what will be returned. Tags - Tag each path with a meaningful set of tags, describing what resources are available in short, concise terms and phrases. Contacts - Provide contact information for whoever can answer questions about an API, and provide a URL to any support resources. Create Security Definitions - Define the security for accessing the API, providing details on how each API request will be authenticated. Apply Security Definitions - Apply the security definition to each individual path, associating common security definitions across all paths. Complete(enough) - That should give us a complete (enough) API description. Obviously there is more we can do to make an OpenAPI even more complete and precise as a business contract, hopefully speaking to both...[<a href="/2018/10/09/the-layers-of-completeness-for-an-openapi-definition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-definition-stories/xignite-api-url.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/10/09/a-quick-manual-way-to-create-an-openapi-from-a-get-api-request/">A Quick Manual Way To Create An OpenAPI From A GET API Request</a></h3>
			<p><em>09 Oct 2018</em></p>
			<p>I have numerous tools that help me create OpenAPIs from the APIs I stumble across each day. Ideally I’m crawling, scraping, harvesting, and auto-generating OpenAPIs, but inevitably the process gets a little manual. To help reduce friction in these manual processes, I try to have a variety of services, tools, and scripts I can use to make my life easier, when it comes to create a machine readable definition of an API I am using–in this scenario it is the xignite CloudAlerts API. One way I’ll create an OpenAPI from a simple GET API request, providing me with a machine readable definition of the surface area of that API, is using Postman. When you have the URL copied onto your clipboard, open up your Postman, and paste the URL with all the query parameters present. You’ll have to save your API request, and add it to a collection, but then you can choose to share the collection, and retrieve the URL to this specific requests Postman Collection. This gives you a machine readable definition of the surface area of this particular API, defining the host, baseURL, path, and parameters, but it doesn’t give me more detail about the underlying schema being returned. To begin crafting the schema for the underlying definition of the API, and connect it to the response for my API definition, I’ll need an OpenAPI–which I can create from my Postman Collection using API Transformer from APIMATIC. After pasting the URL for the Postman Collection into the API transformer form, you can generate an OpenAPI from the definition. Now you have an OpenAPI, except it is missing the underlying schema, which I will just grab the response from my last request, and convert it into JSON schema using JSONSchema.net. I’ll just grab the properties section of these, as the bottom definitions portion of the OpenAPI specification is just JSON Schema. I can merge my JSON schema with my OpenAPI, adding it to...[<a href="/2018/10/09/a-quick-manual-way-to-create-an-openapi-from-a-get-api-request/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/kin-lane-api-days-spain.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/10/08/api-evangelist-and-streamdata-io-api-lifecycle-workshops/">API Evangelist And Streamdata.io API Lifecycle Workshops</a></h3>
			<p><em>08 Oct 2018</em></p>
			<p>I have been partnering with Streamdata.io to evolve how I work with enterprise groups on their API lifecycle strategy. After working closely with the Streadata.io sales team, it became clear that many enterprise organizations weren’t quite ready for the event-driven infrastructure Streamdata.io provides. Most groups were in desperate need of stepping back and developing their own formal strategy for delivering APIs across the enterprise, before they could every scale their operations and take advantage of things being more event-driven and real time. In response, I set out to evolve my own API lifecycle research, gathered over the last eight years of studying the API space, and make it more accessible to the enterprise, as self-service short form and long form content, in-person workshops, and forkable blueprints that any enterprise can set in motion on their own. The results is a series of evolvable API projects, that we are using to drive our ongoing workshop engagements with enterprise API groups, focusing in on six critical areas of the API lifecycle: Discovery (demo) - Knowing where all of your APIs and services are across groups. Design (demo) - Focus in on an a design and virtualized API lifecycle before deployment. Development (demo) - Understanding the many ways in which APIs can be developed &amp; deployed. Production (demo) - Thinking critically about properly operating API infrastructure. Governance (demo) - Understanding how to measure, report, and evolve API operations. Not all of our workshops will cover all of these areas. Depending on the time we have available, the scope of the team participating in a workshop(s), and how far along teams are in their overall API journey, the workshops might head in different directions, and expand or contract the depth in which we dive into each of these area (ie. not everyone is ready for governance). After several workshops this year, we have found these areas of the API lifecycle to be the most meaningful ways to organize a...[<a href="/2018/10/08/api-evangelist-and-streamdata-io-api-lifecycle-workshops/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109_214_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/28/api-developer-outreach-research-for-the-department-of-veterans-affairs/">API Developer Outreach Research For The Department of Veterans Affairs</a></h3>
			<p><em>28 Sep 2018</em></p>
			<p>This is a write-up for research I conducted with my partner Skylight Digital. The team conducted a series of interviews with leading public and private sector API platforms regarding how they approached developer outreach, and then I wrote it up as a formal report, which the Skylight Digital team then edited and polished. We are looking to provide as much information as possible regarding how the VA, and other federal agencies should consider crafting their API outreach efforts. This is Skylight’s report for the U.S. Department of Veterans Affairs (VA) microconsulting project, “Developer Outreach.” The VA undertook this project in order to better understand how private- and public-sector organizations approach Application Programming Interface (API) developer outreach. In preparing this report, we drew on nearly a decade’s worth of our own API developer outreach expertise, as well as information obtained through interviews with seven different organizations. For each interview, we followed an interview script (see Appendix A) and took notes. The Centers for Medicare &amp; Medicaid Services (CMS) Blue Button API program (see Appendix B), the Census Bureau (see Appendix C), the OpenFEC program (see Appendix D), and Salesforce (see Appendix E) all agreed to releasing our notes publicly. The other three organizations (a large social networking site, a government digital services delivery group, and a cloud communications platform) preferred to keep them private. We structured this report to largely reflect the interview conversations that we held with people who are involved in developer outreach programs and activities. These conversations focused around the following questions: What is the purpose and scope of your API developer outreach program? What does success look like for you? What are the key elements or practices (e.g, documentation, demos, webinars, conferences, blog posts) that you are using to drive and sustain effective adoption of your API(s)? Do you make use of an API developer sandbox to drive and sustain adoption? If so, please describe how you’ve designed that environment to be...[<a href="/2018/09/28/api-developer-outreach-research-for-the-department-of-veterans-affairs/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/97_193_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/20/having-the-dedication-to-lead-an-api-effort-forward-within-a-large-enterprise/">Having The Dedication To Lead An API Effort Forward Within A Large Enterprise</a></h3>
			<p><em>20 Sep 2018</em></p>
			<p>I work with a lot of folks who work in large enterprise organizations, institutions, and government agencies who are moving the API conversation forward within their groups. I’m all too familiar with what it takes to move forward the API conversation within large, well established enterprise organizations. However, I am the first to admit that while I have a deep understanding of what it involves, I do not have the fortitude to actually lead an effort for the sustained amount of time it takes to actually make change. I just do not have the patience and the personality for it, and I’m eternally grateful for those that do. There are regular streams of emails in my inbox from people embedded within enterprise organizations, looking for guidance, counseling, and assistance in moving forward the API conversation at their organizations. I am happy to provide assistance in an advisory capacity, and consulting with groups to help them develop their strategies. A significant portion of my income comes from conducting 1-3 day workshops within the enterprise, helping teams work through what they need to. There is one thing I cannot contribute to any of these teams, the dedication and perseverance it will need to actually make it happen. It takes a huge amount of organization knowledge to move things forward at a large organization. You have to know who the decision makers are, and who are the gatekeepers for all of the important resources–this is knowledge you have to acquire by being embedded, and working within an organization for a very long time. You just can’t walk in the door and be able to make sense of things within days, or weeks. You have to be able to work around schedules, and personalities–getting to know people, and truly begin to understand their motivations, and willingness to contribute, or whether they’ll actually decide to work against you. The culture of any enterprise organization will be the most important area...[<a href="/2018/09/20/having-the-dedication-to-lead-an-api-effort-forward-within-a-large-enterprise/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/kong/kong-summit-2018.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/18/understanding-the-eventdriven-api-infrastructure-opportunity-that-exists/">Understanding The Event-Driven API Infrastructure Opportunity That Exists</a></h3>
			<p><em>18 Sep 2018</em></p>
			<p>I am at the Kong Summit in San Francisco all day tomorrow. I’m going to be speaking about research into the event-driven architectural layers I’ve been mapping out across the API space. Looking for the opportunity to augment existing APIs with push technology like webhooks, and streaming technology like SSE, as well as pipe data in an out of Kafka, fill data lakes, and train machine learning models. I’ll be sharing what I’m finding from some of the more mature API providers when it comes to their investment in event-driven infrastructure, focusing in on Twilio, SendGrid, Stripe, Slack, and GitHub. As I am profiling APIs for inclusion in my API Stack research, and in the API Gallery, I create an APIs.json, OpenAPI, Postman Collection(s), and sometimes an AsyncAPI definition for each API. All of my API catalogs, and API discovery collections use APIs.json + OpenAPI by default. One of the things I profile in each of my APIs.json, is the usage of webhooks as part of API operations. You can see collections of them that I’ve published to the API Gallery, aggregating many different approaches in what I consider to be the 101 of event-driven architecture, built on top of existing request and response HTTP API infrastructure. Allowing me to better understand how people are doing webhooks, and beginning to sketch out plans for a more event-driven approach to delivering resources, and managing activity on any platform that is scaling. While studying APIs at this level you begin to see patterns across how providers are doing what they are doing, even amidst a lack of standards for things like webhooks. API providers emulate each other, it is how much of the API space has evolved in the last decade. You see patterns like how leading API providers are defining their event types. Naming, describing, and allowing API consumers to subscribe to a variety of events, and receive webhook pings or pushes of data, as well...[<a href="/2018/09/18/understanding-the-eventdriven-api-infrastructure-opportunity-that-exists/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/blue-button/blue-button-api-docs.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/18/talking-healthcare-apis-with-the-cms-blue-button-api-team-at-apistrat-in/">Talking Healthcare APIs With The CMS Blue Button API Team At #APIStrat In</a></h3>
			<p><em>18 Sep 2018</em></p>
			<p>We have the API evangelist from one of the most significant APIs out there today at #APIStrat in Nashville next week. Mark Scrimshire (@ekivemark), Blue Button Innovator and Developer Evangelist from NewWave Telecoms and Technologies will be on the main stage next Tuesday, September 25th 2018. Mark will be bringing his experience helping stand up the Blue Button API with the Centers for Medicare and Medicaid Services (CMS), and sharing the stories from the trenches while delivering this critical piece of health API infrastructure within the United States. I consider the Blue Button API to be one of the most significant APIs out there right now for several key factors: API Reach - An API that has potential to reach 44 million Medicare beneficiaries, which is 15 percent of the U.S. population–that is a pretty significant audience to reach when it comes to the overall API conversation. Fast Healthcare Interoperability Resources (FHIR) - The Blue Button API supports Hl7 / FHIR, pushing the specification forward in the overall healthcare API interoperability discussion, making it extremely relevant to APIStrat and the OpenAPI Initiative (OAI). Government API Blueprint - The way in which the Blue Button API team at CMS and USDS is delivering the API is providing a potential blueprint that other federal and stage level agencies can follow when rolling out their own Medicare related APIs, but also any other critical infrastructure that this country depends on. This is why I am always happy to support the Blue Button API team in any way I can, and I am very stoked to have them at APIStrat in Nashville next week. I’ve spent a lot of time working with, and studying what the Blue Button API team is up to, and I spoke at their developer conference hosted at the White House last month. They have some serious wisdom to share when it comes to delivering public APIs at this scale, making the keynote with Mark...[<a href="/2018/09/18/talking-healthcare-apis-with-the-cms-blue-button-api-team-at-apistrat-in/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/stack-exchange/stack-exchange-api.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/17/sadly-stack-exchange-blocks-api-calls-being-made-from-any-of-amazons-ip-block/">"Sadly Stack Exchange Blocks API Calls Being Made From Any Of Amazons IP Block"</a></h3>
			<p><em>17 Sep 2018</em></p>
			<p>I am developing an authentication and access layer for the API Gallery that I am building for Streamdata.io, while also federating it for usage as part of my API Stack research. In addition to building out these catalogs for API discovery purposes, I’m also developing a suite of tools that allow users to subscribe to different topics from popular sources like GitHub, Reddit, and Stack Overflow (Exchange). I’ve been busy adding one or two providers to my OAuth broker each week, until the other day I hit a snag with the Stack Exchange API. I thought my Stack Exchange API OAuth flow had been working, it’s been up for a few months, and I seem to remember authenticating against it before, but this weekend I began getting an error that my IP address was blocked. I was looking at log files trying to understand if I was making too many calls, or some other potential violation, but I couldn’t find anything. Eventually I emailed Stack Exchange to see what their guidance once, to which I got a prompt reply: “Yes, we block all of Amazon’s AWS IP addresses due to the large amount of abuse that comes from their services. Unfortunately we cannot unblock those addresses at this time.” Ok then. I guess that is that. I really don’t feel like setting up another server with another provider just so I can run an OAuth server from there. Or, maybe I guess I might have to if I expect to offer a service that provides OAuth integration with Stack Exchange. It’s a pretty unfortunate situation that doesn’t make a whole lot of sense. I can understand adding another layer of white listing for developers, pushing them to add their IP address to their Stack Exchange API application, and push us to justify that our app should have access, but blacklisting an entire cloud provider from accessing your API is just dumb. I am going to...[<a href="/2018/09/17/sadly-stack-exchange-blocks-api-calls-being-made-from-any-of-amazons-ip-block/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/32_39_600_400_0_avg_1_1_1.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/17/justifying-my-existence-in-your-api-sales-and-marketing-funnel/">Justifying My Existence In Your API Sales And Marketing Funnel</a></h3>
			<p><em>17 Sep 2018</em></p>
			<p>I feel like I’m regularly having to advocate for my existence, and the existence of developers who are like me, within the sales and marketing funnel for many APIs. I sign up for a lot of APIs, and have the pleasure of enjoy a wide variety of on-boarding processes for APIs. Many APIs I have no problem signing up, on-boarding, and beginning to make calls, while others I have to just my existence within their API sales and marketing funnel. Don’t get me wrong, I’m not saying that I shouldn’t be expected to justify my existence, it is just that many API providers are setup to immediately discourage, create friction for, and dismiss my class of API integrator–that doesn’t fit neatly into the shiny big money integration you have defined at the bottom of your funnel. I get that we all need to make money. I have to. I’m actually in the business of helping you make money. I’m just saying that you are missing out on a significant amount of opportunity if you only focus on what comes out the other side of your funnel, and discount the nutrients developers like me can bring to your funnel ecosystem. I’m guessing that my little domain apievangelist.com does return the deal size scope you are looking for, but I think you are putting too much trust into the numbers provided to you by your business intelligence provider. I get that you are hyper focused on making the big deals, but you might be leaving a big deal on the table by shutting out small fish, who might have oversized influence within their organization, government agency, or within an industry. Your business intelligence is focusing on the knowns, and doesn’t seem very open to considering the unknowns. As the API Evangelist I have an audience. I’ve been in business since 2010, so I’ve built up an audience of enterprise folks who read what I write, and listen...[<a href="/2018/09/17/justifying-my-existence-in-your-api-sales-and-marketing-funnel/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/capital-battle_blue_circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/17/i-am-needing-some-evidence-of-how-apis-can-make-an-impact-in-government/">I Am Needing Some Evidence Of How APIs Can Make An Impact In Government</a></h3>
			<p><em>17 Sep 2018</em></p>
			<p>Eric Horesnyi (@EricHoresnyi), the CEO of Streamata.io and I were on a call with a group of people who are moving forward the API conversation across Europe, with the assistance of the EU. The project has asked us to assist them in the discovery of more data and evidence of how APIs are making an impact in how government operates within the European Union, but also elsewhere in the world. Aggregating as much evidence as possible to help influence the EU API strategy, and learn from what is already being done. I’m heading to Italy next month to present to the group, and participate in conversations with other API practitioners and evangelists, so I wanted to start my usual amount of storytelling here on the blog to solicit contributions from my audience about what they are seeing. I am looking for some help from my readers who work at city, county, state, and federal agencies, or at the private entities who help them with their API efforts. I am looking for official, validated, on the record examples of APIs making a positive impact on how government serves its constituents. Quantifiable examples of how a government agency have published a private, partner, or public API, and it helped the agency better meet its mission. I’m looking for anything mundane, as well as the unique and interesting, with tangible evidence to back it all up. Like number of developers, partners, apps, cost saving, efficiencies, or any other positive effect. Demonstrating that APIs when done right can move the conversation forward at a government agency. For this round, I’m going to need first hand accounts, because I will need to help organize the data, and work with this group to submit it to the European Union as part of their wider effort. This is something I’ve been doing loosely since 2012, but I need to start getting more official about how I gather the stories, and pull together...[<a href="/2018/09/17/i-am-needing-some-evidence-of-how-apis-can-make-an-impact-in-government/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_blue_circuit_5.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/17/being-open-is-more-about-being-open-so-someone-can-extract-value-than-open/">Being Open Is More About Being Open So Someone Can Extract Value Than Open</a></h3>
			<p><em>17 Sep 2018</em></p>
			<p>One of the most important lessons I’ve learned in the last eight years, is that when people are insistent about things being open, in both accessibility, and cost, it is often more about things remaining open for them to freely (license-free) to extract value from it, that it is ever about any shared or reciprocal value being generated. I’ve fought many a battle on the front lines of “open”, leaving me pretty skeptical when anyone is advocating for open, and forcing me to be even critical of my own positions as the API Evangelist, and the bullshit I peddle. In my opinion, ANYONE wielding the term open should be scrutinized for insights into their motivations–me included. I’ve spend eight years operating on the front line of both the open data, and the open API movements, and unless you are coming at it from the position of a government entity, or from a social justice frame of mind, you are probably wanting open so that you can extract value from whatever is being opened. With many different shades of intent existing when it comes to actually contributing any value back, and supporting the ecosystem around whatever is actually being opened. I ran with the open data dogs from 2008 through 2015 (still howl and bark), pushing for city, county, state, and federal government open up data. I’ve witnessed how everyone wants it opened, sustained, maintained, and supported, but do not want to give anything back. Google doesn’t care about the health of local transit, as long as the data gets updated in Google Maps. Almost every open data activist, and data focused startup I’ve worked with has a high expectation for what government should be required to do, and want very low expectations regarding what should be expected of them when it comes to pay for commercial access, sharing enhancements and enrichments, providing access to usage analytics, and be observable and open to sharing access to...[<a href="/2018/09/17/being-open-is-more-about-being-open-so-someone-can-extract-value-than-open/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-definitions-support.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/13/providing-minimum-viable-api-documentation-blueprints-to-help-guide-your-api/">Providing Minimum Viable API Documentation Blueprints To Help Guide Your API</a></h3>
			<p><em>13 Sep 2018</em></p>
			<p>I was taking a look at the Department of Veterans Affairs (VA) API documentation for the VA Facilities API, and intending on providing some feedback on the API implementation. The API itself is pretty sound, and I don’t have any feedback without having actually integrated it into an application, but following on the heals of my previous story about how we get API developers to follow minimum viable API documentation guidance, I had lots of feedback on the overall deliver of the documentation for the VA Facilities API, helping improve on what they have there. Provide A Working Example of Minimum Viable API Documentation One of the ways that you help incentivize your API developers to deliver minimum viable API documentation across their API implementations is you do as much of the work for them as you can, and provide them with a forkable, downloadable, clonable API documentation that meets the minimum viable requirements. To help illustrate what I’m talking about I created a base GitHub blueprint for what I’d suggest as a minimum viable API documentation at the VA. Providing something the VA can consider, and borrow from as they are developing their own strategy for ensuring all APIs are consistently documented. Covering The Bare Essentials That Should Exist For All APIs I wanted to make sure each API had the bare essentials, so I took what the VA has already done over at developer.va.gov, and republished it as a static single page application that runs 100% on GitHub pages, and hosted in a GitHub repository–providing the following essential building blocks for APIs at the VA: Landing Page - Giving any API a single landing page that contains everything you need to know about working with an API. The landing page can be hosted as its own repo, and subdomain, and the linked up with other APIs using a facade page, or it could be published with many other APIs in a single repository....[<a href="/2018/09/13/providing-minimum-viable-api-documentation-blueprints-to-help-guide-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/mosaic-face_blue_circuit.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/13/please-refer-the-engineer-from-your-api-team-to-this-story/">Please Refer The Engineer From Your API Team To This Story</a></h3>
			<p><em>13 Sep 2018</em></p>
			<p>I reach out to API providers on a regular basis, asking them if they have an OpenAPI or Postman Collection available behind the scenes. I am adding these machine readable API definitions to my index of APIs that I monitor, while also publishing them out to my API Stack research, the API Gallery, APIs.io, work to get them published in the Postman Network, and syndicated as part of my wider work as an OpenAPI member. However, even beyond my own personal needs for API providers to have a machine readable definition of their API, and helping them get more syndication and exposure for their API, having an definition present significantly reduces friction when on-boarding with their APIs at almost every stop along a developer’s API integration journey. One of the API providers I reached out to recently responded with this, “I spoke with one of our engineers and he asked me to refer you to https://developer.[company].com/”. Ok. First, I spend over 30 minutes there just the other day. Learning about what you do, reading through documentation, and thinking about what was possible–which I referenced in my email. At this point I’m guessing that the engineer in question doesn’t know what an OpenAPI or Postman Collection is, they do not understand the impact these specifications are having on the wider API ecosystem, and lastly, I’m guessing they don’t have any idea who I am(ego taking control). All of which provides me with the signals I need to make an assessment of where any API is in their overall journey. Demonstrating to me that they have a long ways to go when it comes to understanding the wider API landscape in which they are operating in, and they are too busy to really come out of their engineering box and help their API consumers truly be successful in integrating with their platform. I see this a lot. It isn’t that I expect everyone to understand what OpenAPI...[<a href="/2018/09/13/please-refer-the-engineer-from-your-api-team-to-this-story/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/stack-exchange/stack-exchange-access-tokens-api.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/10/stack-exchange-has-an-api-that-returns-the-details-for-all-of-your-access/">Stack Exchange Has An API That Returns The Details For All Of Your Access</a></h3>
			<p><em>10 Sep 2018</em></p>
			<p>I’m a big fan of helpful authentication features, where API providers make it easier to manage our increasingly hellish environment, application, token, and other management duties of the average API integrator. To help me better manage my API apps, and the OAuth tokens I have in play, I am trying to document all the sensible approaches I come across while putting different APIs to work, and scouring the API landscape for stories. One example of this in action is out of the Stack Exchange API, where you can find an API endpoint for accessing the details of your OAuth tokens, and invalidate, and de-authorize them. A pretty useful API endpoint when you are integrating with APIs, and find yourself having to manage many tokens across many APIs, apps, and users. Helping you check in on the overall health and activity of your tokens, revoking, renewing, and making sure they work when you need them the most. It is helpful for me to write about the helpful authentication practices I come across while using APIs. It helps me aggregate them into a nice list of features API providers should consider supporting. If I don’t write about it here on the blog, then it doesn’t exist in my research, and my future storytelling. My goal is to help spread the knowledge about what is working across the sector, so that more API providers will adopt along the way. You know what is better than Stack Exchange providing an API to manage your access tokens? All API providers providing you with an API to manage your access tokens! These stories, and any other relevant links I’ve curated will be published to my API authentication research. Eventually I’ll roll all the features I’ve aggregated into either a long form blog post, or white paper I’ll publish and put out with the assistance of one of my partners. I’m interested in the authentication portion of this, but also I’m looking...[<a href="/2018/09/10/stack-exchange-has-an-api-that-returns-the-details-for-all-of-your-access/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/priorities/university-of-api.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/10/some-ideas-for-api-discovery-collections-that-students-can-use/">Some Ideas For API Discovery Collections That Students Can Use</a></h3>
			<p><em>10 Sep 2018</em></p>
			<p>This is a topic I’ve wanted to set in motion for some time now. I had a new university professor city my work again as part of one of their courses recently, something that floated this concept to the top of the pile again–API discovery collections meant for just for students. Helping k-12, community college, and university students quickly understand where to find the most relevant APIs to whatever they are working on. Providing human, but also machine readable collections that can help jumpstart their API education. I use the API discovery format APIs.json to profile individual, as well as collections of APIs. I’m going to kickstart a couple of project repos, helping me flesh out a handful of interesting collections that might help students better understand the world of APIs: Social - The popular social APIs like Twitter, Facebook, Instagram, and others. Messaging - The main messaging APIs like Slack, Facebook, Twitter, Telegram, and others. Rock Star - The cool APIs like Twitter, Stripe, Twilio, YouTube, and others. Amazon Stack - The core AWS Stack like EC2, S3, RDS, DynamoDB, Lambda, and others. Backend Stack - The essential App stack like AWS S3, Twilio, Flickr, YouTube, and others. I am going to start there. I am trying to provide some simple, usable collections or relevant APIs for students are just getting started If there are any other categories, or stacks of APIs you think would be relevant for students to learn from I’d love to hear your thoughts. I’ve done a lot of writing about educational and university based APIs, but I’ve only lightly touched upon what APIs should students be learning about in the classroom. Providing ready to go API collections will be an important aspect of the implementation of any API training and curriculum effort. Having the technical details of the API readily available, as well as the less technical aspects like signing up, pricing, terms of service, privacy policies, and other...[<a href="/2018/09/10/some-ideas-for-api-discovery-collections-that-students-can-use/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/los-angeles-downtow-freeway_blue_circuit_5.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/07/the-path-to-production-for-department-of-veteran-affairs-va-api-applications/">The Path To Production For Department of Veteran Affairs (VA) API Applications</a></h3>
			<p><em>07 Sep 2018</em></p>
			<p>This post is part of my ongoing review of the Department of Veteran Affairs (VA) developer portal and API presence, moving on to where I take a closer look at their path to production process, and provide some feedback on how the agency can continue to refine the information they provide to their new developers. Helping map out the on-boarding process for any new developer, ensuring they are fully informed about what it will take to develop an application on top of VA APIs, and move those application(s) from a developer state to a production environment, and actually serving veterans. Beginning with the VA’s base path to production template on GitHub, then pulling in some elements I found across the other APIs they have published to developer.va.gov, and finishing off with some ideas of my own, I shifted the outline for the path to production to look something like this: Background - Keeping the background of the VA API program. [API Overview] - Any information relevant to specific API(s). Applications - One right now, but eventually several applications, SDK, and samples. Documentation - The link, or embedded access to the API documentation, OpenAPI definition, and Postman Collection. Authentication - Overview of how to authenticate with VA APIs. Development Access - Provide an overview of signing up for development access. Developer Review - What is needed to become a developer. Individual - Name, email, and phone. Business - Name, URL. Location - In country, city, and state. Application Review - What is needed to have an application(s). Terms of Service - In alignment with platform TOS. Privacy Policy - In alignment with platform TOS. Rate Limits - Aware of the rate limits that are imposed. Production Access - What happens once you have production access. Support &amp; Engagement - Using support, and expected levels of engagement. Service Level Agreement - Platform working to meet an SLA governing engagement. Monthly Review - Providing monthly reviews of access...[<a href="/2018/09/07/the-path-to-production-for-department-of-veteran-affairs-va-api-applications/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-value-generation-funnel.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/07/an-api-value-generation-funnel-with-metrics/">An API Value Generation Funnel With Metrics</a></h3>
			<p><em>07 Sep 2018</em></p>
			<p>I’ve had several folks asking me to articulate my vision of an API centric “sales” funnel, which technically is out of my wheelhouse in the sales and marketing area, but since I do have lots opinions on what a funnel should look like for an API platform, I thought I’d take a crack at it. To help articulate what is in my head I wanted to craft a narrative, as well as a visual to accompany how I like to imagine a value generation funnel for any API platform. I envision a API-driven value generation funnel that can be tipped upside down, over and over, like an hour glass, generating value is it repeatedly pushes API activity through center, driven by a healthy ecosystem of developers, applications, and end-users putting applications to work / use. Providing a way to generate awareness and engagement with any API platform, while also ensuring a safe, reliable, and secure ecosystem of applications that encourage end-user adoption, engagement, and loyalty–further expanding on the potential for developers to continue developing new applications, and enhancing their applications to better serve end-users. I am seeing things in ten separate layers right now, something I’ll keep shifting and adjusting in future iterations, but I just wanted to get a draft funnel out the door: Layer One - Getting folks in the top of the funnel. Awareness - Making people aware of the APIs that are available. Engagement - Getting people engaged with the platform in some way. Conversation - Encouraging folks to be part of the conversation. Participation - Getting developers participating on regular basis. Layer Two Developers - Getting developers signing up and creating accounts. Applications - Getting developers signing up and creating applications. Layer Three Sandbox Activity - Developers being active within the sandbox environment. Layer Four Certifed Developers - Certifying developers in some way to know who they are. Certified Application - Certifying applications in some way to ensure quality. Layer...[<a href="/2018/09/07/an-api-value-generation-funnel-with-metrics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/141-Post+Con+2018-Speakers.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/06/my-api-storytelling-depends-on-the-momentum-from-regular-exercise-and-practice/">My API Storytelling Depends On The Momentum From Regular Exercise And Practice</a></h3>
			<p><em>06 Sep 2018</em></p>
			<p>I’ve been falling short of my normal storytelling quotas recently. I like to have at least 3 posts on API Evangelist, and two posts on Streamdata.io each day. I have been letting it slip because it was summer, but I will be getting back to my regular levels as we head into the fall. Whenever I put more coal in the writing furnace, I’m reminded of just how much momentum all of this takes, as well as the regular exercise and practice involved, allowing me to keep pace in the storytelling marathon across my blog(s). The more stories I tell, the more stories I can tell. After eight years of doing this, I’m still surprised abut what it takes to pick things back up, and regain my normal levels of storytelling. If you make storytelling a default aspect of doing work each day, finding a way to narrate your regular work with it, it is possible to achieve high volumes of storytelling going out the door, generating search engine and social media traffic. Also, if you root your storytelling in the regular work you are already doing each day, the chances it will be meaningful enough for people to tune in only increases. My storytelling on API Evangelist is important because it helps me think through what I’m working on. It helps me become publicly accessible by generating more attention to my work, firing up new conversations, and reenforces the existing ones I’m already having. When the storytelling slows, it means I’m either doing a unhealthy amount of coding or other work, or my productivity levels are suffering overall. This makes my API storytelling a heartbeat of my operations, and a regular stream of storytelling reflects how healthy my heartbeat is from regular exercise, and usage of my words (instead of code). I know plenty of individuals, and API related operations that have trouble finding their storytelling voice. Expressing that they just don’t have the...[<a href="/2018/09/06/my-api-storytelling-depends-on-the-momentum-from-regular-exercise-and-practice/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-personal-access-token.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/06/allowing-users-to-get-their-own-oauth-tokens-for-accessing-an-api-without-the/">Allowing Users To Get Their Own OAuth Tokens For Accessing An API Without The</a></h3>
			<p><em>06 Sep 2018</em></p>
			<p>I run a lot of different applications that depend on GitHub, and use GitHub authentication as the identity and access management layer for these apps. One of the things I like the most about GitHub and how I feel it handles it’s OAuth more thoroughly than most other platforms, is how they let you get you own OAuth token under your settings &gt; developer settings &gt;personal access tokens. You don’t need to setup an application, and do the whole OAuth dance, you just get a token that you can use to pass along with each API call. I operate my own OAuth server which allows me to authenticate using OAuth with many leading APIs, so generating an OAuth token, and setting up a new provider isn’t too hard. However, it is always much easier to go under my account settings, create a new personal access token for a specific purpose, and get to work playing with an API. I wish that ALL API providers did this. At first glance, it looks like GitLab, Harvest, TypeForm, and ContentFul all provide personal access tokens as a first option for on-boarding with their APIs. Demonstrating this is more of a pattern, than just a GitHub feature. One of these days I’m going to have to do another story documenting the entire GitHub OAuth system, because they have a lot of interesting bells and whistles that make using their platform much more secure, and just a more frictionless experience than other API providers I use on a regular basis. GitHub has ground down a lot of the sharp corners on the whole authentication experience when it comes to OAuth. It would make a nice blueprint to share, and work to convince other API providers it is a pattern worth following. Reducing the cognitive load around OAuth management for any API integration, and standardizing how API providers support their API consumers, and end-users. I have 3 separate Twitter Apps setup...[<a href="/2018/09/06/allowing-users-to-get-their-own-oauth-tokens-for-accessing-an-api-without-the/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/streamdata/streamdata-api-partners-philosophy.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/04/what-have-you-done-for-us-lately-api-partner-edition/">What Have You Done For Us Lately (API Partner Edition)</a></h3>
			<p><em>04 Sep 2018</em></p>
			<p>I’ve been working on developing and evolving the Streamdata.io partner program, trying to move forward conversations with other service providers in the space that have existed long before I started working on things, as well as other newer relationships that I’ve helped bring in. I’m fascinated by how partner programs work, or do not work, and have invested a lot of time trying to optimize and improve how I do my own operations, and assist my partners and clients in evolving and delivering on their own partner vision. It is difficult to establish, and continue meaningful and balanced partnerships between technology service and tooling providers. Sometimes providers have enough compatibility and synergy, that they are able to hit the ground running with meaningful activities that strengthen, and build partnership momentum. We are trying to establish a meaningful, yet effective way of measuring partner activity, and understanding the value that is being generated, and where reciprocity exists. Looking at the following activities produced by Streamdata.io and it’s partners: Partner Page - Being published to both of our partner pages. Testimonials - Providing quotes for each other about our services. Blog Posts - Publishing blog posts about partnership and each others services. White Papers - Publishing white papers or guides about partnership and each others services. Press Releases - Working on join press releases about partnership and each others services. Integrations - Publishing open source repositories demonstrating integration and usage of each others services. Workshops - Conduct workshops for each others customers, helping deliver each others services within our ecosystems. Business - Actually provide business referrals from our customers, and conversations occurring across both companies. There are other activities we like to see happening, but these eight areas represent the most common exchanges we encourage amongst our partners. The trick is always pushing for reciprocity across all these areas, help deliver on a balanced partnership, and make sure there is equal value being generated for both sides...[<a href="/2018/09/04/what-have-you-done-for-us-lately-api-partner-edition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/federal-goverment-portals.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/04/the-federal-agencies-who-use-their-developer-domain-gov-subdomain/">The Federal Agencies Who Use Their developer.[domain].gov Subdomain</a></h3>
			<p><em>04 Sep 2018</em></p>
			<p>I was reviewing the new developer portal for the Department of Veterans Affairs (VA), and one of things I took notice of, was their use of the developer.va.gov subdomain. In my experience, the API efforts that invest in a dedicated subdomain, and specifically a developer dot subdomain, tend to more invested in what they are doing than efforts that publish to a subfolder, or subsection of their website. As I was writing this post, I had a question in arise in my mind, regarding how many other federal agencies use a dedicated subdomain for their developer programs–something I wanted to pick up later, and understand the landscape a little more. I took a list of current federal agency domains from the GSA and wrote a little script to append developer. to each of the domains, and conduct an HTTP status code check to see whether or not these pages existed. Here are the dedicated developer areas I found for the US federal government: Department of Veterans Affairs (VA) - https://developer.va.gov/ Department of Labor - https://developer.dol.gov International Trade Administration (ITA) - https://developer.trade.gov &amp; https://developer.export.gov United States Patent and Trademark Office - https://developer.uspto.gov National Renewable Energy Laboratory - https://developer.nrel.gov Centers for Medicare &amp; Medicaid Services - https://developer.cms.gov The Advanced Distributed Learning Initiative - http://developer.adlnet.gov &amp; http://developers.adlnet.gov United States Environmental Protection Agency - http://developer.epa.gov USA Jobs - http://developer.usajobs.gov These nine agencies have decided to invest in a subdomain for their developer portals. I have to recognize two others who provide these subdomains, but then redirect to a subsection of their websites: National Park Service - http://developer.nps.gov redirect to https://www.nps.gov/subjects/developer/index.htm Data.gov - http://developer.data.gov redirects to https://www.data.gov/developers/ Additionally, there is a single domain I noticed that used the plural version of the subdomain: Code.gov - https://developers.code.gov (plural) Along the way, I also noticed that many agencies would redirect their subdomain, and I assume all subdomains to the root of their agency’s domain. Ideally, all federal agencies would have a Github...[<a href="/2018/09/04/the-federal-agencies-who-use-their-developer-domain-gov-subdomain/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-github-issue-production-api-access-request.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/04/the-basics-of-the-va-api-feedback-loop/">The Basics Of The VA API Feedback Loop</a></h3>
			<p><em>04 Sep 2018</em></p>
			<p>I’m working to break down the moving parts of API efforts over at the VA, and work to provide as much relevant feedback as I possibly can. One of the components I’m wanting to think about more is the feedback loop for the VA API efforts. The feedback loop is one of the most essential aspects of doing an API, and is quickly can become one of the most debilitating, paralyzing, and nutrient starving aspects of operating an API platform if done wrong, or non-existent. However, the feedback loop is also one of the most valuable reasons for wanting to do APIs in the first place, providing the essential feedback you will need from consumers, and the entire API ecosystem to move the API forward in a meaningful way. Current Seeds Of The VA API Feedback Loop Current the VA is supporting the VA API developer portal using GitHub Issues and email. I mentioned in my previous review of the VA API developer portal that the personal email addresses provided for email support should be generalized, sharing the load when it comes to email support for the platform. Next, I’d like to address the usage of GitHub issues for support, along with email, and step back to look at how this contributes to, or could possibly take away from the overall feedback loop for the VAPI API effort. Defining what the basics of an API feedback loop for the VA might be. Expanding Upon The VA Usage Of GitHub Issues I support the usage of GitHub issues for public support of any API related project. It provides a simple, observable way for anyone to get support around the VA APIs. While I’m guessing it was accidental, I like the specialization of the current repo, and usage of GitHub issues, and that it being dedicated to VA API clients and their needs. I’d encourage this type of continued focus when it comes to establishing additional feedback...[<a href="/2018/09/04/the-basics-of-the-va-api-feedback-loop/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/reduce+everything+to+a+transaction_tr.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/09/04/remembering-that-apis-are-used-to-reduce-everything-down-to-a-transaction/">Remembering That APIs Are Used To Reduce Everything Down To A Transaction</a></h3>
			<p><em>04 Sep 2018</em></p>
			<p>This is our regular reminder that APIs are not good, nor bad, nor neutral. They are simply a tool in our technological toolbox, and something that is often used for very dark reasons, and occasionally for good. One of the most dangerous things I’ve found about APIs is just the general thought process that goes along with them, regarding how all roads lead to reducing, and distilling things down to a single transaction. APIs, REST, microservices, and other design patterns are all about taking something from our physical world, and redefining it as something that can be transmitted back and forth using the low cost request and response infrastructure of the web. No matter what you are designing your API for, your mission is to reduce everything to a simple transaction that can be exchanged between your server, and any other system, web, mobile, device, or network application. This digital resource could be a photo of your kids, a message to your mother, the balance of your bank account, your personal thoughts going into your notebook, the latest song you listened to, your DNA, your test results for cancer, or any other piece of relevant data, content, media, object, or other resource that is being sent or received online. APIs are all about reducing all of our meaningful digital bits to the smallest possible transaction, and then daisy chaining them together to produce some desired set of results. This API-ification of everything can be a good thing. It can make our lives better, but one of the negative side effects of this reducing of everything to a transaction, is that now that transaction can be bought and sold. The digitization of everything in our lives is rarely ever about making our lives better and whatever the reasons we are told up front, and almost always are about reducing that little piece of our lives to a transaction that can be quantified, have a value place...[<a href="/2018/09/04/remembering-that-apis-are-used-to-reduce-everything-down-to-a-transaction/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-developer-portal.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2018/08/30/why-i-feel-the-department-of-veterans-affairs-api-effort-is-so-significant/">Why I Feel The Department Of Veterans Affairs API Effort Is So Significant</a></h3>
			<p><em>30 Aug 2018</em></p>
			<p>I have been working on API and open data efforts at the Department of Veterans Affairs (VA) for five years now. I’m passionate about pushing forward the API conversation at the federal agency because I want to see the agency deliver on its mission to take care of veterans. My father, and my step-father were both vets, and I lived through the fallout from my step-fathers two tours in Vietnam, exposure to the VA healthcare and benefits bureaucracy, and ultimately his passing away from cancer which he acquired from to his exposure to Agent Orange. I truly want to see the VA streamline as many of its veteran facing programs as they possibly can. I’ve been evangelizing for API change and leadership at the VA since I worked there in 2013. I’m regularly investing unpaid time to craft stories that help influence people I know who are working at the VA, and who are potentially reading my work. Resulting in posts like my response to the VA’s RFI for the Lighthouse API management platform, which included a round two response a few months later. Influence through storytelling is the most powerful tool I got in my API evangelist toolbox. This Is An Amazon Web Services Opportunity The most popular story on my blog is, “The Secret to Amazon’s Success–Internal APIs”. Which tells a story of the mythical transition of Amazon from an online commerce website to the cloud giant, who is now powering a significant portion of the web. The story is mostly fiction, but continues to be the top performing story on my blog six years later. I’ve heard endless conference talks about this subject, I’ve seen my own story framed on the wall in enterprise organizations in Europe and Australia, and as a feature link on internal IT portals. This is one of the most important stories we have in the API sector, and what is happening at the VA right now will...[<a href="/2018/08/30/why-i-feel-the-department-of-veterans-affairs-api-effort-is-so-significant/">Read More</a>]</p>
			<p><hr /></p>
	  

		<!-- Pagination links -->
		<table width="100%">
			<tr>
				<td align="left" width="33%">
				  
				    <a href="/blog/page3" class="previous">
				      &#8592; Previous
				    </a>
				  
			</td>
			<td align="center" width="33%">
			  <span class="page_number ">
			    Page: 4 of 37
			  </span>
			</td>
			<td align="right" width="33%">
		  
		    <a href="/blog/page5" class="next">Next &#8594;</a>
		  
			</tr>
			</tr>
		</table>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
	<hr>
	<p align="center">
		relevant work:
		<a href="http://apievangelist.com">apievangelist.com</a> |
		<a href="http://adopta.agency">adopta.agency</a>
	</p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
