<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/14/an-openapi-vendor-extension-for-defining-your-api-audience/">An OpenAPI Vendor Extension For Defining Your API Audience</a></h3>
        <span class="post-date">14 Mar 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/97_193_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>The clothing marketplace Zalando has an interesting approach to classifying their APIs based upon who is consuming them. It isn’t just about APIs being published publicly, or privately, they actually have standardized their definition, and have established an OpenAPI vendor extension, so that the definition is machine readable and available via their OpenAPI.</p>

<p><a href="http://zalando.github.io/restful-api-guidelines/index.html#219">According to the Zalando API design guide</a>, “<em>each API must be classified with respect to the intended target audience supposed to consume the API, to facilitate differentiated standards on APIs for discoverability, changeability, quality of design and documentation, as well as permission granting. We differentiate the following API audience groups with clear organisational and legal boundaries.</em>”</p>

<ul>
  <li><strong>component-internal</strong> - The API consumers with this audience are restricted to applications of the same functional component (internal link). All services of a functional component are owned by specific dedicated owner and engineering team. Typical examples are APIs being used by internal helper and worker services or that support service operation.</li>
  <li><strong>business-unit-internal</strong> - The API consumers with this audience are restricted to applications of a specific product portfolio owned by the same business unit.</li>
  <li><strong>company-internal</strong> - The API consumers with this audience are restricted to applications owned by the business units of the same the company (e.g. Zalando company with Zalando SE, Zalando Payments SE &amp; Co. KG. etc.)</li>
  <li><strong>external-partner</strong> - The API consumers with this audience are restricted to applications of business partners of the company owning the API and the company itself.</li>
  <li><strong>external-public</strong> - APIs with this audience can be accessed by anyone with Internet access.</li>
</ul>

<p><em><strong>Note:</strong> a smaller audience group is intentionally included in the wider group and thus does not need to be declared additionally. The API audience is provided as API meta information in the info-block of the Open API specification and must conform to the following specification</em>:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#/info/x-audience:
  type: string
  x-extensible-enum:
    - component-internal
    - business-unit-internal
    - company-internal
    - external-partner
    - external-public
  description: |
    Intended target audience of the API. Relevant for standards around
    quality of design and documentation, reviews, discoverability,
    changeability, and permission granting.
</code></pre></div></div>
<p><em><strong>Note:</strong> Exactly one audience per API specification is allowed. For this reason a smaller audience group is intentionally included in the wider group and thus does not need to be declared additionally. If parts of your API have a different target audience, we recommend to split API specifications along the target audience — even if this creates redundancies (rationale).</em></p>

<p>Here is an example of the OpenAPI vendor extension in action, as part of the info block:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>swagger: '2.0'
info:
  x-audience: company-internal
  title: Parcel Helper Service API
  description: API for &lt;...&gt;
  version: 1.2.4
</code></pre></div></div>
<p>Providing a pretty interesting way of establishing the scope and reach of each API in a way that makes each API owner think deeply about who they are / should be targeting with the service. Done in a way that makes the audience focus machine readable, and available as part of it’s OpenAPI definition which can be then used across discovery, documentation, and through API governance and security.</p>

<p>I like the multiple views of who the audience could be, going beyond just public and private APIs. I like that it is an OpenAPI vendor extension. I like that they even have a schema crafted for the vendor extension–another interesting concept I’d like to see more of. Overall, making for a pretty compelling approach to define the reach of our APIs, and quantifying the audience we are looking to reach with each API we publish.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/14/an-openapi-vendor-extension-for-defining-your-api-audience/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/14/rules-for-extending-your-api-with-each-version/">Rules for Extending Your API With Each Version</a></h3>
        <span class="post-date">14 Mar 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/23_19_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m spending time learning from the API design guides of other leading API providers, absorbing their healthy practices, and assimilating them into my own consulting and storytelling. One API design guide I am learning a lot from is out of <a href="https://adidas-group.gitbooks.io/api-guidelines/">the Adidas Group</a>, which contains a wealth of wisdom regarding not just the design of your API, but also the deployment and evolution of the API resources we are publishing.</p>

<p><a href="https://adidas-group.gitbooks.io/api-guidelines/content/core-principles/rules-for-extending.html">One particularly interesting piece of advice I found within Adidas API design guidance were their rules for extending an API</a>, which I think is pretty healthy advice for an API developer to think about.</p>

<p><em>Any modification to an existing API MUST avoid breaking changes and MUST maintain backward compatibility.</em></p>

<p><em>In particular, any change to an API MUST follow the following Rules for Extending:</em></p>

<ul>
  <li><em>You MUST NOT take anything away (related: Minimal Surface Principle , Robustness Principle)</em></li>
  <li><em>You MUST NOT change processing rules</em></li>
  <li><em>You MUST NOT make optional things required</em></li>
  <li><em>Anything you add MUST be optional (related <a href="https://en.wikipedia.org/wiki/Robustness_principle">Robustness Principle</a>)</em></li>
</ul>

<p><em>NOTE: These rules cover also renaming and change to identifiers (URIs). Names and identifiers should be stable over the time including their semantics.</em></p>

<p>First of all, I think many API developers aren’t even thinking about what constitutes a breaking change most of the time. So having any guidance that makes them pause and think about this topic is a good thing. Second, I think we should be sharing more stories about when things break, helping folks think more about these elements–the problem is that many folks are embarrassed they introduced a breaking changes, and would rather not talk about it, let alone make it publicly known.</p>

<p>I am working my way through <a href="http://apistylebook.com/">the API Stylebook</a>, learning from all the API design guides it has aggregated. There is a wealth of knowledge in there to learn from, and contains topics that make for great stories here on the blog. I wish more API providers would actively publish their API governance strategy, so that we can keep aggregating, and learning from each other. Making the wider API space more consistent, and hopefully more reliable along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/14/rules-for-extending-your-api-with-each-version/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/13/seeing-messy-api-design-practices-as-an-opportunity/">Seeing Messy API Design Practices As An Opportunity</a></h3>
        <span class="post-date">13 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/13439120_10154285627219813_3054276594176638940_n_copper_circuit_2.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m generating OpenAPI definitions for a wide variety of APIs currently, and I’m regularly stumbling on the messiness of the API design practices being deployed. When you are exposed to a large number of different APIs it is easy to get frustrated, begin ranting, and bitching about how ignorant people are of healthy, sensible API practices. This is the easy route. Making sense of it all, finding the interesting signals and patterns, and extracting where the opportunity are takes a significant amount of effort (so does biting tongue).</p>

<p>After profiling almost 500 API providers, I have almost 25K separate API paths indexed using OpenAPI, and APIs.json. I’ve tagged each API path using its OpenAPI definition. Pulling words from the path name, and any summary and description provided within the API documentation. Doing my best to describe the value contained within each API resource. Then I started grouping by these tags, to see what it produced. Sometimes the API paths it lists makes total sense, but most of the time it makes no sense at all. Then, I started noticing interesting patterns in how people describe their resources. Grouping things like “favorites” across all types of APIs, revealing some pretty interesting perspectives, and honest views of the resources being exposed.</p>

<p>Something as simple as “activities” can mean 30 or 40 different things when applied across CRM, storage, DNS, travel or sports APIs. At first, I’m like this shit is broken. The more time I spend with the mess, the more I’m starting to think there is more to this mess than meets the eye. I could be wrong. I often am. It is likely just my contrarian view of things, and my unique view of the API landscape in action. <a href="http://apievangelist.com/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/">Where many people see duplicative work, I see common patterns</a>. I just see the landscape differently than people who are just looking to get their work done, sell a product or service, and find an exit for their startup. I’m not looking for any solution, doorway, or exit. I just see all of this as a journey, and I’m fascinated by how people view their API resources, then describe, tag and bag them.</p>

<p>I’m not sure where all of this API indexing and tagging will lead me. I don’t feel compelled to sort out the mess, and covert any of these API providers to a more sensible API religion. I’m just looking to understand more about the motivations behind why they did what they did. Provide commentary on where they fit into the bigger picture, and if they are delivering interesting and valuable API resources, help make them more discoverable, and executable when it matters. As I have been doing for the las eight years, I’m just looking to learn from others, and stay in tune with where things are headed, no matter how messy it might be. I see APIs as a kind of controlled chaos, which are increasingly driving our already chaotic lives.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/13/seeing-messy-api-design-practices-as-an-opportunity/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/13/google-releases-a-protocol-buffer-implementation-of-the-fast-healthcare-interoperability-resources-fhir-standard/">Google Releases a Protocol Buffer Implementation of the Fast Healthcare Interoperability Resources (FHIR) Standard</a></h3>
        <span class="post-date">13 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/fhir/FHIR_logo-1080x675.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>Google is renewing its interest in the healthcare space by <a href="https://research.googleblog.com/2018/03/making-healthcare-data-work-better-with.html">releasing a protocol buffer implementation of the fast healthcare interoperability resources (FHIR) standard</a>. Protocol buffers are “Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler”. Its the core of the next generation of APIs at Google, often using HTTP/2 as a transport, while also living side by side with RESTful APIs, which use OpenAPI as the definition, in parallel to what protocol buffers deliver.</p>

<p>It’s a smart move by Google. Providing a gateway for healthcare data to find its way to their data platform products like Google Cloud BigQuery, and their machine learning solutions built on Tensorflow. They want to empower healthcare providers with powerful solutions that help onboard their data, and be able to connect the dots, and make sense of it at scale. However, I wouldn’t stop with protocol buffers. I would also make sure they also invest in API infrastructure on the RESTful side of the equation, developing OpenAPI specs alongside the protocol buffers, and providing translation between, and tooling for both realms.</p>

<p>While I am a big supporter of gRPC, and protocol buffers, I’m skeptical of the complexity it brings, in exchange for higher performance. Part of making sense of health care data will require not just technical folks being able to make sense of what is going on, but also business folks, and protocol buffers, and gRPC solutions will be out of reach of these users. Web APIs, combined with YAML OpenAPI has begun to make the API contracts involved in all of this much more accessible to business users, putting everything within their reach. In our technological push forward, let’s not forget the simplicity of web APIs, and exclude business healthcare users as IT has done historically.</p>

<p>I’m happy to see more FHIR-compliant APIs emerging on the landscape. PSD2 for banking, and FHIR for healthcare are the two best examples we have of industry specific API standards. So it is important that the definitions proliferate, and the services and tooling emerge and evolve. I’m hoping we see even more movement on this front in 2018, but I have to say I’m skeptical of Google’s role, as they’ve come and gone within this arena before, and are exceptional at making sure all roads lead to their solutions, without many roads leading back to enrich the data owners, and stewards. If we can keep API definitions, simple, accessible, and usable by everyone, not just developers and IT folks, we can help empower healthcare data practitioners, and not limit, or restrict them, when it is most important.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/13/google-releases-a-protocol-buffer-implementation-of-the-fast-healthcare-interoperability-resources-fhir-standard/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/13/multi-lingual-machine-learning-api-deployments/">Multi-Lingual Machine Learning API Deployments</a></h3>
        <span class="post-date">13 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/paralleldots/multi-lingual-website-676x507.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p><a href="https://hackernoon.com/launching-paralleldots-ai-apis-in-multiple-languages-2bed9eeff664">Machine learning API ParallelDots has a story on launching their APIs in multiple languages</a>. Allowing them to “serve a truly global customer base with following language options for our key APIs (Sentiment Analysis, Emotion Analysis, and Keyword generator)”. Something that I think more APIs providers are going to have to think about in coming years, as the need for API resources expands around the globe.</p>

<p>I’m tracking on the localization efforts of different API providers, and I’d say that having service availability in different regions, and multi-lingual support are the top two areas I’m seeing movement. I see two driving forces behind this, 1) the customers are demanding localization for their businesses, and 2) the governments in those countries are imposing regulations and other laws that dictate where resources can be stored and operate. I guess, something that can be seen as markets working things out, if you also believe in the value of regulations.</p>

<p>I also see a negative in all of this, specifically in this case, the imperialistic aspects of artificial intelligence and machine learning. Meaning, the models are trained here in western countries, but then being applied, injected, and imposed upon people in other countries. Selling the service as some sort of truth, watermark, and organic solution to sentiment, emotion, and intelligence without actually localizing the models. I’m not making any assumptions around ParralelDots motivations, just pointing out that this will be a problem, and should be ignored–if it makes you mad, then you are probably part of the problem.</p>

<p>As I conduct my API research, I will keep tracking on localization efforts like this. When I have the bandwidth I will dive in deeper and better understand how providers are defining these localization layers to their APIs in their API design, deployment, documentation, and other elements. I’ll be keeping an eye on which industries are moving fastest when it comes to API localization, and try to understand where it is deemed the most important by providers, consumers, and regulators. Lots to keep an eye on, and understand in 2018 when it comes to the expansion of the world of APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/13/multi-lingual-machine-learning-api-deployments/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/13/api-as-a-product-principles-at-zalando/">API As A Product Principles From Zalando</a></h3>
        <span class="post-date">13 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/zalando/zalando-api-guidelines.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>As I’m working through the API design guides from API leaders, looking for useful practices that I can include in my own API guidance, I’m finding <a href="zalando.github.io/restful-api-guidelines/">electronic commerce company Zalando’s API design guide</a> full of some pretty interesting advice. I wanted to showcase the section about their API as a product principles, which I think reflects what I hear many companies striving for when they do APIs.</p>

<p>From the Zalando API design guide principles:</p>

<p><em>Zalando is transforming from an online shop into an expansive fashion platform comprising a rich set of products following a Software as a Platform (SaaP) model for our business partners. As a company we want to deliver products to our (internal and external) customers which can be consumed like a service.</em></p>

<p><em>Platform products provide their functionality via (public) APIs; hence, the design of our APIs should be based on the API as a Product principle:</em></p>

<ul>
  <li><em>Treat your API as product and act like a product owner</em></li>
  <li><em>Put yourself into the place of your customers; be an advocate for their needs</em></li>
  <li><em>Emphasize simplicity, comprehensibility, and usability of APIs to make them irresistible for client engineers</em></li>
  <li><em>Actively improve and maintain API consistency over the long term</em></li>
  <li><em>Make use of customer feedback and provide service level support</em></li>
</ul>

<p><em>RESTful API as a Product makes the difference between enterprise integration business and agile, innovative product service business built on a platform of APIs.</em></p>

<p><em>Based on your concrete customer use cases, you should carefully check the trade-offs of API design variants and avoid short-term server side implementation optimizations at the expense of unnecessary client side obligations and have a high attention on API quality and client developer experience.</em></p>

<p><em>API as a Product is closely related to our API First principle which is more focused on how we engineer high quality APIs.</em></p>

<p>Zalando provides a pretty coherent vision for how we all should be doing APIs. I like this guidance because it helps quantify something we hear a lot–APIs as a product. However, it also focuses in on what is expected of the product owners. It also gets at why companies should be doing APIs in the first place, talking about the benefits they bring to the table.</p>

<p>I’m enjoying the principles section of Zalando’s API design guide. It goes well beyond just API design, and reflects what I consider to be principles for wider API governance. Many companies are still considering this API design guidance, but I find that companies who are publishing these documents publicly are often maturing and moving beyond just thinking deeply about design–providing a wealth of other wisdom when it comes to doing APIs right.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/13/api-as-a-product-principles-at-zalando/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/12/the-importance-of-tags-in-openapi-definitions-for-machine-learning-apis/">The Importance Of Tags In OpenAPI Definitions For Machine Learning APIs</a></h3>
        <span class="post-date">12 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/contrafabulists/machine+learning.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I am profiling APIs as part of my partnership with <a href="http://streamdata.io">Streamdata.io</a>, and my continued <a href="http://theapistack.com">API Stack</a> work. As part of my work, I am creating OpenAPI, Postman Collections, and APIs.json indexes for APIs in a variety of business sectors, and as I’m finishing up the profile for <a href="https://docs.paralleldots.com/">ParallelDots machine learning APIs</a>, I am struck (again) by the importance of tags within OpenAPI definitions when it comes to defining what any API does, and something that will have significant effects on the growing machine learning, and artificial intelligence space.</p>

<p>While profiling ParallelDots, I had to generate the OpenAPI definition from the Postman Collection they provide, which was void of any tags. I went through the handful of API paths, manually adding tags for each of the machine learning resources. I’m adding tags like sentiment, emotions, semantics, taxonomy, and classification, to each path. Trying to capture what resources were available, allowing for the discovery, filtering, and execution of each individual machine learning model being exposed using a simple web API. While the summary and description explain what each API does to developers, the tags are really the precise meaning in a machine readable context.</p>

<p>In the fast moving, abstract realm of machine learning, and artificial intelligence it can be difficult to truly understand what each API does, or doesn’t do. I struggle with it, and I’m pretty well versed in what is possible and not possible with machine learning. Precise tagging provide us with a single definition of what each machine learning API does, setting a baseline of understanding for putting ML APIs to work. Something that if I consistently apply across all of the machine learning APIs I’m profiling, I can can begin honing and dialing in my catalog of valuable API resources, and begin creating a coherent map of what is possible with machine learning APIs–helping ground us in this often hyped realm.</p>

<p>Once I have a map of the machine learning landscape established, I want to continue evolving my API ranking strategy to apply specifically to machine learning models being exposed via APIs. Not just understanding what they do, but also the quality of what they do. Are the machine algorithms delivering as advertised? Which APIs have a consistent track record in not just the reliability of the APIs, but also the reliability of the responses. Further bringing clarity to the fast moving, venture capital fueled, magical, mystical realm of artificial intelligence. Helping average business consumers better understand which APIs they can depend on, and which ones they might want to avoid.</p>

<p>I’m hoping my API Stack profiling will encourage more API providers to begin doing the heavy lifting of creating OpenAPI definitions, complete with tags themselves. We are always going to need the participation of the community to help make sure they are complete, and as meaningful as they can, but API providers will need to step up and invest in the process whenever possible. As the machine learning and artificial intelligence realms mature, we are going to need a meaningful vocabulary for describing what it is they do, and a ranking system for sorting out the good, the mediocre, and the bad. We’ll need all of this to be contained within the machine readable definitions we are using for discovery, and at runtime, allowing us to automate more efficiently using machine learning APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/12/the-importance-of-tags-in-openapi-definitions-for-machine-learning-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/12/some-common-api-data-types-to-put-to-use/">Some Common API Data Types To Put To Use</a></h3>
        <span class="post-date">12 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/carryload_dali_three.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m continuing my exploration of the API design guidance published by leading API providers. This time, I am taking a look at the API design guide publish by Adidas, <a href="https://adidas-group.gitbooks.io/api-guidelines/content/application/common-data-types.html">specifically the portion of it addressing what some of the common API types should be</a>. Providing some API design essentials, that all of us API providers should be baking into our APIs by default.</p>

<p>Here is the short list of <a href="https://adidas-group.gitbooks.io/api-guidelines/">default standards Adidas</a> is supporting across their API operations:</p>

<ul>
  <li><strong>Date and Time Format</strong> - Date and Time MUST always conform to the <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601 format</a> e.g.: 2017-06-21T14:07:17Z (date time) or 2017-06-21 (date), it MUST use the UTC (without time offsets).</li>
  <li><strong>Duration Format</strong> - Duration format MUST conform to the <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601 format</a> standard e.g.: P3Y6M4DT12H30M5S (three years, six months, four days, twelve hours, thirty minutes, and five seconds).</li>
  <li><strong>Time Interval Format</strong> - Time Interval format MUST conform to the <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601 format</a> standard e.g.: 2007-03-01T13:00:00Z/2008-05-11T15:30:00Z.</li>
  <li><strong>Standard Time Stamps</strong> - Where applicable, a resource representation SHOULD contain the standard timestamps: createdAt, updatedAt, and finishedAt, using the <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601 format</a>.</li>
  <li><strong>Language Code Format</strong> - Language codes MUST conform to <a href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes">the ISO 639</a> e.g.: en for English.</li>
  <li><strong>Country Code Format</strong> - Country codes MUST conform to <a href="https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2">the ISO 3166-1</a> alpha-2 e.g.: DE for Germany.</li>
  <li><strong>Currency Format</strong> - Currency codes MUST conform to <a href="https://en.wikipedia.org/wiki/ISO_4217">the ISO 4217</a> e.g.: EUR for Euro.</li>
</ul>

<p>These are the common data formats we are all using across our APIs. We should be working to support standard formats wherever possible, embracing the wider web, and investing in the longevity and usability of our API designs. Making our APIs sure that they speak to as wide of audience as they possibly can, and operate in a consistent way, no matter which API you are integrating with.</p>

<p>I want to also give a shout out to the <a href="http://apistylebook.com/">API Stylebook</a>, where I’m easily finding these API design guides that I’m using for these stories, and including in my API design guidance in my consulting work. Also, I’d like to encourage other API providers to reuse the patterns present in these guides, as well as publishing their own API design governance guidance whenever possible. It is critical that we all share these stories, so that we can learn from each other, and emulate the healthiest patterns already in use across the space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/12/some-common-api-data-types-to-put-to-use/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/12/thinking-about-delete-and-destroyting-entities/">Thinking About API Status Codes For Destroying Entities Using DELETE</a></h3>
        <span class="post-date">12 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/deliveroo/deliveroo-new-visual-branding-logo.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I am pulling together some API design guidance for some projects I’m consulting on, so I’m spending time reviewing the API design guides the leading API providers who have published them publicly. Learning from what they are doing across their own companies, organizations, institutions, and government agencies when it comes to sensible API governance.</p>

<p>Today, I am learning from the British food delivery company, Deliveroo, and documenting <a href="https://deliveroo.engineering/guidelines/api-design/#external-facing">their guidance for which HTTP status codes should be returned for any API methods that use DELETE</a>:</p>

<p>If it exists, it should return status:</p>

<ul>
  <li>204 No Content if the entity was successfully destroyed,</li>
  <li>404 Not Found if the entity does not exist</li>
  <li>410 Gone if the entity is known to have existed but no longer does.</li>
</ul>

<p>Additionally 4xx response codes may be used:</p>

<ul>
  <li>412 Precondition Failed</li>
  <li>415 Unsupported if using versioning and the server doesn’t support the specified version.</li>
</ul>

<p>I have to admit, I haven’t been properly publishing status codes any DELETE methods I provide. I’ve just been returning a 200 if successful, and 404 if it didn’t exist. I hadn’t put any further thought into the proper way of handling it. I just haven’t had the time, or the knowledge within reach to be able to handle properly.</p>

<p>I know that the RESTafarians enjoy debating these finer details of API design, and using HTTP, but for the rest of us, we just need some guidance from y’all. Which is why I spend time learning from existing API leaders who publish their API design guidance, and sharing as individual stories here on the blog, as well as including in my official project consulting guidance.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/12/thinking-about-delete-and-destroyting-entities/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/12/a-machine-readable-baseline-for-api-providers-in-my-api-stack-work/">A Machine Readable Baseline For API Providers In My API Stack Work</a></h3>
        <span class="post-date">12 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-definitions-api-stack.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’m rebooting <a href="http://theapistack.com/">my API Stack</a> work as part of my partnership with <a href="http://streamdata.io">Streamdata.io</a>. I’m spending a significant portion of my day profiling API providers, documenting what it is they bring to the table. Historically I’ve published the resulting APIs.json and OpenAPI definition(s) to a single Github repository driving theapistack.com. The primary folder was already getting too big, and since I’m looking at adding at least a thousand more API providers to the stack, I am going to need to shard things out a bit.</p>

<p>To help accommodate the increased scale, I’m breaking up the API Stack into two separate Github organizations. One for individual API providers called <a href="https://github.com/api-stack-providers">api-stack-providers</a>, and a second for individual topics called <a href="https://github.com/api-stack-topics">api-stack-topics</a>. I’ve began publishing individual repositories for each API provider that I am monitoring, establishing a self-contained, continuously deployable, and integratabtle set of definitions for each API. I’m needing each API provider to be independent from each other, and I’m even publishing individual API definitions for each API path, distilling things down to the smallest possible unit of compute possible.</p>

<p>Each repository contains three main API definitions, using <a href="https://github.com/api-stack-providers/paralleldots">the ParallelDots machine learning API</a> as an example:</p>

<ul>
  <li><a href="https://github.com/api-stack-providers/paralleldots/blob/master/apis.yaml">APIs.json</a> - The entire index of the API provider’s operation.</li>
  <li><a href="https://github.com/api-stack-providers/paralleldots/blob/master/openapi/complete.yaml">OpenAPI</a> - The definition of the surface area of entire API.</li>
  <li><a href="https://github.com/api-stack-providers/paralleldots/blob/master/postmancollections/complete.json">Postman Collection</a> - An execute-time ready definition for the Postman client.</li>
</ul>

<p>I’m also publishing an individual OpenAPI for each API path within a listings folder, allowing me to provide the detailed execute-time definition I’m needing to work with each API. I’m also considering how I can do this with the Postman Collections. Next, I’m looking to understand how I can include the multiple source definitions within the repository, providing original copies of API definitions that I’m pulling from sources like <a href="http://apis.guru">APIs.guru</a>, <a href="https://any-api.com/">Any API</a>, and directly from API providers. Sharing the entire history behind how each set of API definitions has come together, and hopefully encouraging others to submit pull requests with fresh copies.</p>

<p>Ideally, each API provider is maintaining their own set of definitions in their own Github repository, like <a href="https://apievangelist.com/2017/05/22/box-goes-all-in-on-openapi/">Box</a>, <a href="https://apievangelist.com/2017/03/01/new-york-times-manages-their-openapi-using-github/">NY Times</a>, and <a href="http://apievangelist.com/2017/11/06/an-example-of-how-every-api-provider-should-be-using-openapi-out-of-the-slack-platform/">Slack</a> do, which I’ll just keep in sync with. However, for the rest, I’m looking to maintain an independent Github repository for each API providers, providing a single repo, URL, and set of issues for discussing the evolution of the OpenAPI, Postman, and APIs.json indexes. Each project will also have <a href="http://paralleldots.stack.network/">its own portal landing page</a>, with generated API docs. It’s a lot to maintain, but it is something I’m already doing behind the scenes in a database, and I might as well continue the work out in the open where I can solicit the help of the community, and build on the hard work of existing projects.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/12/a-machine-readable-baseline-for-api-providers-in-my-api-stack-work/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/09/defining-the-smallest-unit-possible-for-use-at-api-runtime/">Defining The Smallest Unit Possible For Use At API Runtime</a></h3>
        <span class="post-date">09 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/peachtree/peachtree-email-validation.png" width="45%" align="right" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">I’m thinking a lot about what is needed at API runtime lately</a>. How we document and provide machine readable definitions for APIs, and how we provide authentication, pricing, and even terms of service to help reduce friction. As Mike Amundsen (<a href="https://twitter.com/mamund">@mamund</a>) puts it, to enable “find and bind”. This goes beyond simple API on-boarding, and getting started pages, and looks to make APIs executable within a single click, allowing us to put them to use as soon as we find them.</p>

<p>The most real world example of this in action can be found with <a href="https://www.getpostman.com/docs/v6/postman_for_publishers/run_button/creating_run_button">the Run in Postman button</a>, which won’t always deal with the business and politics of APIs at runtime, but will deal with just about everything else. Allowing API providers to publish Run in Postman Buttons, defined using a Postman Collection, which include authentication environment details, that API consumers can use to quickly fire up an API in a single click. One characteristic I’ve come across that contributes to Postman Collections being truly executable is that they reflect the small unit possible for use at API runtime.</p>

<p><a href="https://developer.peachtreedata.com/Documentation#overview">You can see an example of this in action over at Peachtree Data</a>, who like many other API providers have crafted Run in Postman buttons, but instead of doing this for the entire surface area of their API, they have done it for a single API path. Making the Run in Postman button much more precise, and executable. Taking it beyond just documentation, to actually being more of a API runtime executable artifact. This is a simple shift in how Postman Collections can be used, but a pretty significant one. Now instead of wading through all of Peachtree’s APIs in my Postman, I can just do an address cleanse, zip code lookup, or email validation–getting down to business in a single click.</p>

<p>This is an important aspect of on-boarding developers. I may not care about wading through and learning about all your APIs right now. I’m just looking for the API solution I need to a particular problem. Why clutter up my journey with a whole bunch of other resources? Just give me what I need, and get out of my way. Most other API providers I have looked at in <a href="https://www.getpostman.com/api-network/">Postman’s API Network</a> have provided a single Run in Postman button for all of their APIs, where Peachtree has opted to provide many Run in Postman buttons for each of their APIs. Distinguishing themselves, and the value of each of their API resources in a pretty significant way.</p>

<p>I asked the questions the other week, regarding <a href="http://apievangelist.com/2018/02/15/how-big-or-small-is-an-api/">how big or how small is an API</a>? I’m struggling with this question in my API stack work, as part of an investment by Streamdata.io to develop an API gallery. Do people want to find Amazon Web Services APIs? Amazon EC2 APIs? Or the single path for firing up an instance of EC2? What is the small unit of compute we should be documenting, generating OpenAPI and Postman Collections for? I feel like this is an important API discovery conversation to be having. I think depending on the circumstances, the answer will be different. It is a question I’ll keep asking in different scenarios, to help me better understand how I can document, publish, and make APIs not just more discoverable, but usable at runtime.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/09/defining-the-smallest-unit-possible-for-use-at-api-runtime/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/09/the-postman-api-network/">The Postman API Network</a></h3>
        <span class="post-date">09 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/postman/postman-api-network.png" width="45%" align="right" style="padding: 15px;" /></p>
<p><a href="https://www.getpostman.com/api-network/">The Postman API Network</a> is one of the recent movements in the API discovery space I’ve been working to get around to covering. As Postman continues its expansion from being just an API client, to a full lifecycle API development solution, they’ve added a network for discovering existing APIs that you can begin using within Postman in a single click. Postman Collections make it ridiculously easy to get up and running with an API. So easy, I’m confounded why ALL APIs aren’t publishing Postman Collections with Run in Postman Buttons published in their API docs.</p>

<p><a href="https://www.getpostman.com/api-network/">The Postman API Network</a> provides a catalog of APIs in over ten categories, with links to each API’s documentation. All of the APIs in the network have a Run in Postman button available as part of their documentation, which includes them in the Postman API Network. It is a pretty sensible approach to building a network of valuable APIs, who all have invested in there being a runtime-ready, machine readable Postman Collection for their APIs. One of the more interesting approaches I’ve seen introduced to help solve the API discovery problem in the eight years I’ve been doing API Evangelist.</p>

<p>I’ve been talking to Abhinav Asthana (<a href="https://twitter.com/a85">@a85</a>) about the Postman API Network, and working to understand how I can contribute, and help grow the catalog as part of my work as the API Evangelist. I’m a fan of Postman, and an advocate of it as an API lifecycle development solution, but I’m also really keen on bringing comprehensive API discovery solutions to the table. With the Postman API Network, and other API discovery solutions I’m seeing emerge recently, I’m finding renewed energy for this area of my work. Something I’ll be brainstorming and writing about more frequently in coming months.</p>

<p><a href="http://apis.how/streamdata">Streamdata.io</a> has been investing in me moving forward the API discovery conversation, to build out their vision of a Streamdata.io API Gallery, but also to contribute to the overall API discovery conversation. I’m in the middle of understanding how this aligns with my existing API Stack work, APIs.json and APIs.io effort, as well as with APIs.guru, AnyAPI, and the wider OpenAPI Initiative. If you have thoughts you’d like to share, feel free to ping me, and I’m happy to talk more about the API discovery, network, and run-time work I’m contributing to, and better understand how your work fits into the picture.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/09/the-postman-api-network/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/09/keeping-track-of-federal-government-open-source-projects-using-the-code.gov-api/">Keeping Track Of Federal Government Open Source Projects Using The Code.gov API</a></h3>
        <span class="post-date">09 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/code-gov/code-gov-screenshot.png" width="45%" align="right" style="padding: 15px;" /></p>
<p>Open source software is increasingly driving the federal government, <a href="https://fcw.com/articles/2017/10/04/oracle-rips-18f-usds.aspx">despite the wishes of companies like Oracle</a>. I’ve been watching an interesting project grow within the federal government to help quantify open source across the federal government called Code.gov, which “leverages the power of code sharing and collaboration to help the US Government cut down on duplicative software development and save millions of taxpayer dollars for the American people.” Something I think we can all get behind, when it comes to the collision of technology and government we are seeing play out.</p>

<p>Code.gov allows you to <a href="https://code.gov/#/explore-code">browse open source projects by government agency</a>, see the details for the project, visit the repository, and contact the project owners. The project even has <a href="https://code.gov/#/help-wanted">a help wanted section where you can roll up your sleeves and actually contribute to specific projects</a>, and help agencies push forward their project. You can follow up on what is happening using <a href="https://code.gov/#/roadmap">the Code.gov roadmap</a>, and stay in tune with whats next for the very important project. However, the aspect I’m most interested in, is the evolution of the Code.gov API.</p>

<p>There is <a href="https://github.com/GSA/code-gov-api">an open source API behind the Code.gov project</a>, providing programmatic access to government agencies repositories, the code, types of programming languages used, status, and other critical details for projects. There is <a href="https://api.code.gov/docs/">an OpenAPI-driven, Swagger UI documentation for the project</a>, providing a nice look at the programmatic surface for the project. You can use the API they have provided, or <a href="https://github.com/GSA/code-gov-api">you can fork the API and operate yourself</a>, providing an interesting look at delivering open source APIs that span multiple federal agencies–a blueprint that should be considered for other types of projects.</p>

<p>Code.gov reflects what I want to see come out of the federal government when it comes to technology. Open source. API-driven. Doing on thing, and doing it well. I can see spinoffs of the project emerge, focusing on just the environment, transportation, healthcare, and other verticals, helping shine a light on important projects going on across the federal government, attract and orchestrate talent and partnerships around those projects. Then build valuable dashboards, visualizations, and other applications on top of the APIs behind the project(s). I’ll be continuing to document Code.gov’s approach, and the API behind it, so that I can have the blueprint ready as part of my federal government API toolbox.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/09/keeping-track-of-federal-government-open-source-projects-using-the-code.gov-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/08/a-mock-only-api-development-reality/">Imagine A Mock Only API Development Reality</a></h3>
        <span class="post-date">08 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/gears_smoking_cigarette.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>Imagine if we didn’t actually write code when we developed our APIs initially. What if the bar for putting an API into production was so high, that many APIs never actually made it to that level. I spend a lot of time mocking and prototyping APIs, but I don’t have a strict maturity model determining when I actually bring an API to life using code. I’d love it if I never actually write code for many of my API prototypes and just mocked them, and iterated upon the mocks instead of thinking they needed actually have a database backend, and code or gateway front-end.</p>

<p>What if I only executed these stops along the API lifecycle, instead of actually writing any code:</p>

<ul>
  <li><strong>Define</strong> - Define what I want my API to do, and what types of resources will be involved.</li>
  <li><strong>Design</strong> - Actually design my API and underlying schema using an OpenAPI definition.</li>
  <li><strong>Mock</strong> - Publish a mock representation of my API, complete with virtualized data behind responses.</li>
  <li><strong>Document</strong> - Make sure and publish documentation to a simple developer portal.</li>
  <li><strong>Plan</strong> - A overview of what it will cost, and the model for managing consumption.</li>
  <li><strong>Testing</strong> - Set up tests to ensure each API is up and doing what it was intended to do.</li>
  <li><strong>Communicate</strong> - Actually write stories, and communicate around what an API does.</li>
  <li><strong>Support</strong> - Establish and maintain feedback loops around the API to listen to consumers.</li>
  <li><strong>Road Map</strong> - Gather feedback into a road map, and plan next version, complete with a change log.</li>
</ul>

<p>In this life cycle I would never actually write any code. I would never actually setup a database backend. Everything would be mocked and virtualized, but seem as real as I possibly can. Even maybe providing different types of virtualized datasets and responses, so people can experience different types of responses or scenarios. I’d work hard to make sure each API was as complete, and function like a real APIs, as I possibly could.</p>

<p>Now imagine within a company, if I had to do all of this, and before I would ever be allowed to begin developing my API for real, I had to sell my API. Show its value, and get some consumers to buy into my overall API design, and even develop some mock integrations showing the potential. Before my API would be considered for inclusion in the production road map, I’d have to prove that my approach was mature enough in each of the areas outlined above:</p>

<ul>
  <li><strong>Define</strong> - Robust OpenAPI and JSON schema present.</li>
  <li><strong>Design</strong> - A complete design meeting all governance guidelines.</li>
  <li><strong>Document</strong> - Complete, up to date API document present.</li>
  <li><strong>Plan</strong> - A clear definition of costs, and how consumption will be measured.</li>
  <li><strong>Testing</strong> - A full suite of tests available with 100% coverage.</li>
  <li><strong>Communicate</strong> - Demonstrated ability to communicate around its operations.</li>
  <li><strong>Support</strong> - Demonstrated ability to support the APIs operation.</li>
  <li><strong>Road Map</strong> - Clearly articulated road map, and change log for the API.</li>
</ul>

<p>As an API developer, once I’ve build the case for my microservice, presented it, and demonstrated its value–then I would be approved to actually begin developing it. Maybe in some cases I would be required to iterate upon it for another version, and engage with a larger audience before actually given the license and budget to do it for real. Going well beyond just an API design first reality, and enforcing a maturity model around how we define and communicate around our services, and raising the bar for what gets let into production.</p>

<p>Many of the companies, organizations, institutions, and government agencies I’m working with are having a hard time adopting a define and design first strategy, let alone a reality such as this. Imagine how different our production worlds would look if you had all this experience before you were ever given the license to write code. Think about how much worthless code gets written, when we should just be iterating on our ideas. How much of our budget gets wasted on writing code that should never have existed to begin with. It’s a nice thought. However, it is just a fantasy.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/08/a-mock-only-api-development-reality/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/08/your-obbsessive-focus-on-the-api-resource-is-hindering-meaningful-events-from-happening/">Your Obsessive Focus On The API Resource Is Hindering Meaningful Events From Happening</a></h3>
        <span class="post-date">08 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/berlin_wall_graham_sutherland.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve been profiling a number of market data APIs as part of my work with <a href="http://apis.how/streamdata">Streamdata.io</a> to identify valuable sources of data that could be streamed using their service. A significant portion of the APIs I come across are making it difficult for me to get at the data they have because of their views around the value of the data, intellectual property, and maintaining control over it in an API-driven world. These APIs don’t end up on the list of APIs I’m including in the profiling work, the gallery / directory, and don’t get included in any of the stories I’m telling, as a result of this tight control.</p>

<p>The side effect of this is I end up getting repeated sales emails and phone calls asking if I am still interested in their data. If there was just one or two of these, I’d jump on phones and explain, but because I’m dealing with 50+ of them, I just don’t have the bandwidth, and I have to move on. The thing is, I’m personally not interested in their data. I’m interested in other people being interested in their data, and being an enabler to helping them to get at it. However, since I can’t actually profile the APIs, create OpenAPI definitions for the request and response structure for inclusion in the API gallery / directory I’m building, I really don’t need their APIs in my work.</p>

<p>I know these platforms are protective of their data because it is valuable. They should be. However modern API management allows for them to open up the sampling of everything they have to offer, without giving away the farm. This allows enablers, analysts, and storytellers like me to test drive things, profile what they have to offer, and include within our applications. Then my users find what they want, head over to the source of the data, sign up for API keys, talk to their sales staff about what data sets they are interested in. I’m just a middle man, a broker, someone who is looking to enable engagements with their data. I’m only interested in the data, because I understand it is valuable to others, not because I am personally interested in doing anything with it.</p>

<p>This reality is common amongst data brokers who live in the pre-API era. They don’t understand API management, and they don’t understand how innovation using APIs work. They still rely on a pretty closed, tight-gripped approach to selling data. API enablers like me don’t have the time to mess around in these worlds. There are too many APIs out there to waste our time. I’m looking to profile the best quality market data source, that are frictionless to get up and running with. I’m not looking for free data, I understand it costs to get at. I just want to send leads their way, but I need to be able to profile what you have to offer in detailed, in a machine readable way. In the end, it doesn’t matter, because these providers won’t be around for long. Other, more API-savvy data providers will emerge, and run them out of business–it is the circle of API life.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/08/your-obbsessive-focus-on-the-api-resource-is-hindering-meaningful-events-from-happening/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/08/axway-asking-for-an-openapi-of-a-streamdata-io-api-so-they-can-screenshot-it/">Axway Asking for an OpenAPI of The Streamdata.io API So They Can Screenshot It</a></h3>
        <span class="post-date">08 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/beachclouds_clean_view.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>We are working closely with <a href="https://www.axway.com/en">Axway</a> on a number of projects over here at <a href="http://apis.how/streamdata">Streamdata.io</a>. After we got out of a meeting with their team the other day we received an email from them asking if we had an OpenAPI definition for a demo Streamdata.io market data API. They were wanting to include it in some marketing materials, and needed a screenshot of it. To be able to generate the visual they desired, they needed an OpenAPI to make it tangible enough for capturing in a screenshot and presenting as part of a larger story.</p>

<p>This may sound like a pretty banal thing, but when you step back and realize the importance of OPenAPI when it comes to communication, and making something very abstract a tangible, visual thing, it becomes more significant. You can tell someone there is a market data API, but taking a screenshot of documentation generated via an OpenAPI which displays the market data paths, a couple of parameters like stocker ticker symbol and maybe date range, and then plug in some actual values like the ticker symbol for AAPL, and show the JSON response takes things to a new level. This is OpenAPI empowered storytelling, marketing and communications in my book. Elevating what OpenAPI brings to the table to new stops along the API life cycle.</p>

<p>This isn’t just about documentation. This is about making an abstract API concept more visual, more meaningful, and able to be captured in an image. Axway is trying to demonstrate the value of their API solutions, coupled potentially with Streamdata.io services, in a single image–providing a lot more rich context, and visualizations that amplify their marketing materials. This isn’t just documenting what is going on so that developers know what to do with an API, this is telling stories so that business users understanding what is possible with an API–using a machine readable format like OpenAPI to help deliver the 1000 words the image will be worth.</p>

<p>Using OpenAPI like this reflects where I’d like to see API documentation go. Sure, we still need dynamic API documentation driven by OpenAPI definitions for developers to understand what is going on, but we need more snippets, visualization, and emotion driving solutions to exist. Things that marketers, bloggers, and other storytellers can use in their materials. We need OpenAPI-driven tools that help them plug in a relevant API definition, and generate a meaningful visual that they can use in a slide deck, blog post, or other material. We need our API documentation to speak beyond the developer community and become something that anyone can put to work in their API storytelling efforts–no coding required.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/08/axway-asking-for-an-openapi-of-a-streamdata-io-api-so-they-can-screenshot-it/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/07/where-do-we-start-the-api-conversation-at-our-large-organization/">Where Do We Start The API Conversation At Our Large Organization?</a></h3>
        <span class="post-date">07 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/cityscape_dali_three.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>It can be tough to know where to start with APIs at your large organization. Everyone is already in motion, in a variety of groups, geographic regions, serving different lines of business. The API conversation is already occurring across your organization, it is just happening in a distributed and fragmented way, without any coherent orchestration, and strategy moving it forward in unison. I’d like to sit down with your team(s), and understand what APIs look like across your organization, and help work through a first draft of your API strategy, and define how we can better orchestrate existing efforts, and begin establishing an API governance framework for bringing the API vision into focus.</p>

<p>Let’s put together a schedule for me to sit down with a variety of teams in a partial day, full day, or multi-day series of workshops. Where we can work to define a more comprehensive, and coherent strategy for APIs. One that is rooted in what is already happening, but works to brig together the disparate voices from each team into a single conversation about how APIs can make change across your organization in a more consistent, and meaningful way. Let’s sit down and begin to define what APIs mean at your organization and map of what is already occurring, and develop framework that will allow us to begin to move things forward.</p>

<p><strong>Define Objectives</strong><br />
I’d like to learn more about the objectives behind your investment in an organization-wide API strategy, and understand more around the motivations, vision behind what you are looking to achieve. I’d like to cut through the hype, and get to the real reasons you want to be doing APIs.</p>

<ul>
  <li><strong>Internal</strong> - What benefits should APIs bring to internal teams within the organization?</li>
  <li><strong>Partner</strong> - How will APIs be used to further partner engagements, and maximize their deliverables?</li>
  <li><strong>Public</strong> - What can be achieved by performing APIs in the public sphere, and engaging 3rd party developers, and media?</li>
</ul>

<p>I’d like everyone to bring their stories of how APIs are currently being done within their groups, and how each team sees the future of ther API programs. As well as learning from leadership about what they’d like to achieve by doing APIs in a more coordinated fashion across the organizations and lines of business.</p>

<p><strong>Define Organization</strong><br />
To help move forward the conversation, I’d like to learn more about your organizational structure, and understand the cultural makeup of all groups, individuals, the leadership structures in place–all of which will impact how APIs are realized (or not).</p>

<ul>
  <li><strong>Groups</strong> - What groups will be involved with defining, deploying, managing, and consuming APIs?</li>
  <li><strong>Actors</strong> - Who are the different types of actors involved in shaping what APIs look like within your organization.</li>
  <li><strong>Partners</strong> - What external partners will immediately impact how APIs are done, and the value they can bring to the table?</li>
</ul>

<p>Your organizational map will have everything to do with why APIs are working or not working. Your organizational DNA will help define the impact APIs can have internally, as well as externally with partners and the wider public.</p>

<p><strong>Define Architecture</strong><br />
Next, I’d like to get a handle on the architecture that is currently in place across your organization. Understand the current technological makeup of how your organization gets business done on a day to day basis. Learning about how technology is being leveraged to deliver services today, and will continue as part of a wider API strategy.</p>

<ul>
  <li><strong>Platforms</strong> - Which of the major cloud platforms are you operating on?</li>
  <li><strong>Systems</strong> - What major systems are in place that drive services today?</li>
  <li><strong>Services</strong> - What external services do you currently put to use across operations?</li>
  <li><strong>Tools</strong> - What are the open source tools in use across your organization?</li>
</ul>

<p>I’m looking to understand how web services, APIs, and applications are delivered (and consumed) today. I’d like to learn more about the technical makeup of your organization so that I can better understand the current state of things, and how existing technology can be leveraged to deliver the next generation of APIs.</p>

<p><strong>Define Services</strong><br />
Then, I’d like to paint a landscape picture of APIs across your organization, starting with the past, understanding how you have gotten to where you are today. Identify existing, successful APIs that are in place, while also getting to work on what the future will bring.</p>

<ul>
  <li><strong>Legacy</strong> - What web services are already in place to make things work across organizations?</li>
  <li><strong>Existing</strong> - What are the existing APIs already in place which we can use as a blueprint for how APIs should be done?</li>
  <li><strong>Future</strong> - I’d like to learn about what the future will hold when it comes to APIs across the organization.</li>
</ul>

<p>This process can be as high level, or as technical as needed, depending on the different actors we assemble at the table. I’d like to learn about the services in terms of business leadership, as well as the finer technical views–of course, being respectful of the time we have.</p>

<p><strong>Define Lifecycle</strong><br />
With an understanding of the organization, and the existing architecture and services in play, let’s start defining the lifecycle(s) that exists for moving each individual APIs forward, understanding all the stops they’ll pass through from design to deprecation.</p>

<ul>
  <li><strong>Defining</strong> - How are APIs currently defined, from interface to schema?</li>
  <li><strong>Designing</strong> - What does API design look like across the organization?</li>
  <li><strong>Mocking</strong> - Is virtualization of APIs and data a common practice?</li>
  <li><strong>Documenting</strong> - How are APIs documented and detailed for wider usage?</li>
  <li><strong>Clients</strong> - What interfaces, SDKs, IDEs, and client tooling are used for API development?</li>
  <li><strong>Deploying</strong> - What does the current API build and deploy process look like?</li>
  <li><strong>Managing</strong> - What management and gateway layers exist as part of API operations?</li>
  <li><strong>Planning</strong> - How are costs and value generation quantified and measured involving APIs?</li>
  <li><strong>Testing</strong> - What are the current API monitoring, testing, and performance practices look like?</li>
  <li><strong>Security</strong> - How is security defined, evaluated, and responded to across API operations?</li>
  <li><strong>Legal</strong> - How do terms of service, privacy policies, licensing, and other regulations get addressed?</li>
  <li><strong>Discovery</strong> - How are APIs and services catalogued and discovered throughout operations?</li>
  <li><strong>Support</strong> - What does support look like when it comes to the API lifecycle?</li>
  <li><strong>Communications</strong> - What is expected around communications across teams for each API?</li>
  <li><strong>Evangelism</strong> - How are APIs evangelized internally, and externally amongst partners?</li>
</ul>

<p>I’m looking to develop an understanding of how the API lifecycle is currently approached, while also discussing what the first draft of the future organizational wide API lifecycle will look like. Something we can put put into action immediately, and begin measuring as part of an overall governance strategy.</p>

<p><strong>Define Governance</strong><br />
We should now have enough to begin talking about how a comprehensive API strategy can be put into place. How individual APIs will be developed and managed, as well as the organizations and actors who will be responsible for their existence. I’d like to talk about how we’ll establish a wider understanding of API operations, measure what is happening, and begin orchestrating its movement in a organized way:</p>

<ul>
  <li><strong>Mapping</strong> - Let’s take the map of the organization, architecture, and lifecycle, and talk about how we’ll turn it into a living map of API operations.</li>
  <li><strong>Measuring</strong> - What metrics will we use to measure our progress, and understand how APIs are moving forward throughout the lifecycle in a consistent way.</li>
  <li><strong>Maturity</strong> - How do we define maturity throughout API operations, understanding not all APIs will be production ready, and always allowing for innovation.</li>
  <li><strong>Reporting</strong> - Let’s discuss how we’ll be reporting on API operations, and communicating with leadership, and across teams about the progress being made.</li>
  <li><strong>Incentives</strong> - What are the incentives we can use to help make sure everyone is a team player, and working towards a common goal across the organization?</li>
  <li><strong>Coaching</strong> - I’d like to better understand how we can implement an evangelism and coaching strategy as part of the wider API governance execution.</li>
  <li><strong>Auditing</strong> - How will we be auditing the API governance strategy, as well as the wider API operations, and understanding where the gaps are?</li>
  <li><strong>Refining</strong> - Let’s make sure everyone understands that API governance will require constance refining, and evolving to make it work as expected.</li>
</ul>

<p>I’d like to understand what API governance means to your organization. Is it more carrot, or more stick. I’d like to understand how a draft governance plan can be put into place immediately, but fully knowing that it will be a living, evolving, and maturing approach to ensuring quality and consistency of delivering APIs at scale across your organization.</p>

<p><strong>Initial, As Well As Ongoing Engagement</strong><br />
Let’s get to work setting up an initial engagement. I’ll leave to you to decide whether it should be a partial, full, or multi day affair. This work to define an API strategy can be invested in at multiple scopes (high level, or in detail), and should continue to live on, expand, and evolve with each engagement. Ideally, it is is something that is worked through with as many teams possible, then repeated as part of a recurring schedule, as time allows. As the map of the organization, the existing architecture and service landscape, lifecycle and governance comes into focus, these working sessions will become even more important, and productive.</p>

<p>I’d like to ask anyone joining the conversation to bring any answers, thoughts, concerns, and questions they have regarding this outline with them. They are welcome to be as high level, or down in the technical weeds as they like–I’ll absorb it all. Ideally, there is a mix of business and technical folks present, with varying levels of leadership in attendance reflecting how API decision making occurs, or will occur across your organization. I’m looking forward to learning more about APIs across your organization, and learning from everyone at the table, while also sharing my knowledge and experience from studying the API sector for the last eight years.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/07/where-do-we-start-the-api-conversation-at-our-large-organization/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/07/us-companies-getting-ahead-of-eu-regulations/">US Companies Getting Ahead Of EU Regulations</a></h3>
        <span class="post-date">07 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/gargoyle-nd-paris_atari_missle.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I find it to be a telling sign of the culture at US companies when it comes to their response, or lack of response to EU regulations. I’ve been curating stories from the API providers that I track on when it comes to GDPR or PSD2, keeping a notebook or research that is ready when I get around to diving in deeper. The companies who are openly talking about these regulations and being proactive about responding to them, are usually the API providers who have a strong stance in the US market, and are poised to, or already expanding this reach to Europe. Companies who haven’t made any noise are probably not concerned with the European market, or just hoping the regulations fizzle out I guess?</p>

<p>After diving into my curation notebook the first company to stand out is <a href="https://auth0.com/">Auth0</a>, with a variety of blog posts, and resources on navigating both PSD2 and GDPR. Auth0 is in a good position to provide critical authentication and user information management APIs to other companies who are working to comply with the regulations, so it makes sense that they would be getting ahead of all of this. I fully grasp that many companies are simply issuing their press releases stating they can help with GDPR or PSD2, but you can quickly cut through the fluff by looking at how much they’ve invested in their response, materials, and services. <a href="https://auth0.com/docs/compliance/gdpr">Auth0 has a pretty extensive knowledge-base on GDPR</a>, <a href="https://auth0.com/blog/open-auth-standards-psd2/">providing PSD2 guidance on their blog</a>, as well as <a href="https://auth0.com/blog/auth0-europe-launches/">investing in their EU region for a couple of years</a>–demonstrating it is more than just a press release.</p>

<p>I’m working my way through the list of US API providers, and service providers who are being active on the subject of EU regulations. I feel it is an important sign of the strength of the company, and demonstrates a healthy understanding of how regulations aren’t always bad, and that they can actually help industries thrive. I’m actively working on projects involving GDPR and PSD2 in Europe, and I’m eager to develop my understanding of how these regulations are changing the face of the technology industry in Europe, but also how this will impact US companies. I’m hoping that it will begin to shift and evolve the culture around data ownership, privacy, and APIs as industry standards in the US. I feel that as Internet technology matures, we are going to need to view data a little different, otherwise things won’t be sustainable. I’m hopeful that EU regulations can help set this into motion–we’ll see, maybe I’m naive.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/07/us-companies-getting-ahead-of-eu-regulations/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/06/explaining-api-security-to-organizational-leadership/">Explaining API Security To Organizational Leadership</a></h3>
        <span class="post-date">06 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/castle-on-hill-edinburgh_propaganda_leaflets.JPG" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been tasked with helping explain API security to senior leadership, and wanted to work through my ideas here on the blog. For this audience, I’m not going to get down into the weeds regarding the technical specification behind OAuth, and other approaches, and try to keep things high level, introducing folks to the art that is API security. The phrase API security represents a balance of concepts because APIs are by nature about providing access, while security is about controlling and sometimes limiting access, resulting in a new way of getting business done on the open web.</p>

<p><strong>First, What Are APIs?</strong><br />
APIs are not the latest trend, or vendor solution, they are the next evolution in the web. Web sites and applications return HTML via a URL, and meant to display information to humans in a browser, while APIs return JSON or XML of the same information, but meant to be used in other applications and systems. API security is designed to allow access to our digital resources using the web, while also securing it in a way to ensure only the intended audience is able to obtain  access. APIs are designed to securely provide access to data, content, media, and algorithms using the same web that us humans use to access information online via our browsers.</p>

<p><strong>Access Using Secure URLs</strong><br />
APIs use web URLs get read and write data, content, media, and to allow engagement with algorithms. If you want a list of press releases, you visit https://api.example.com/press/. If you want a list of contacts from the CRM, you visit https://api.example.com/contacts/. The URL for all API resources should be encrypted by default, protecting all requests and responses in transit. Providing the first layer of security for APIs, ensuring only approved consumers can view data, content, media, and valuable algorithms being transmitted online.</p>

<p><strong>Registration Always Required For APIs</strong><br />
A common misconception about web APIs is that they are all like Twitter, and are publicly available on the web. However, almost all web APIs actually require that you register before you get any access to any resources. This process is the beginning of what is called API management, where developers have to sign up for an account, and in some cases be approved before they get access to APIs. Most of the time there is self-service, automatic registration, but developers only get limited access, which once they’ve been approved, proven their identity, or put in a credit card–will be able to obtain higher levels of access to resources.</p>

<p><strong>Application Keys For Each API Call</strong><br />
Once developers have been approved for access, they can begin making API calls. However, each API will require that API keys, and required credentials are present with each call. Providing identification of every API consumer, and exactly what they are consuming. API keys are often seen as all that is needed to properly securing APIs, but in reality, they are much more about identifying and tracking what API consumers are doing. Going beyond just focusing on securing the digital resources, and really developing an awareness of who is accessing what, which will prove to be more valuable than just requiring registration to access APIs alone.</p>

<p><strong>Suite Of Authentication Options</strong><br />
There are a handful of approaches in use when it comes to requiring developers to authenticate and pass along their API keys with each response. Each approach has pros and cons, but the industry has widely settled in on four main ways to require API developers to authenticate themselves when using APIs:</p>

<ul>
  <li><strong>Basic Auth</strong> - Usage of the basic authentication format that is part of the standard HTTP operations, employing a username and password as credentials for accessing API resources.</li>
  <li><strong>Key Access</strong> - Providing simple tokens, often called API key as a common way to access to APIs, issuing one to each developer and per application they register.</li>
  <li><strong>JSON Web Token</strong> - JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.</li>
  <li><strong>oAuth</strong> - Providing an oAuth layer to API operations, securing high value APIs, while also opening up a conversation between an API platform, developers, and end-users regarding the access of their content and data.</li>
</ul>

<p>Depending on the security requirements of the resource, and whether or not users generated data and content is involved, you may select a different path for how you require API developers to authenticate. API keys via the URL or headers, as well as Basic Auth are the most common. with JSON Web Tokens, and OAuth being put to work in higher security environments, and where users data, and permission might be required.</p>

<p><strong>Logging Of All API Activity</strong><br />
A essential aspect of securing APIs involves the logging of ALL API calls, no matter who the consumer is, what application it is serving, and whether it is for internal, or external consumption. All API calls get logged equally, and when all API developers are required to authenticate and pass their keys with each API call, all the evidence needed to understand API activity and consumption is present. When you combine the API level logging with backend database logs, and front-end DNS logs, you can define a set of perimeters that will help ensure the security of your API resources.</p>

<p><strong>Modern API Management Solutions</strong><br />
API management combines the authentication and logging described above with API plans, rate limiting, and analytics, to achieve a heightened awareness regarding who is access what API resources, and how they are putting them to use. Modern API management allows for the monitoring of API registrations, authentication, and consumption in real time, with controls for limiting or shutting off access whenever terms of service and security violations are identified. Providing API providers with the tools they need to monitor and respond to any security concerns, while staying in tune with exactly how resources are being accessed via APIs.</p>

<p><strong>Defining Access To APIs Using Service Composition</strong><br />
API management also allows API providers to develop different levels of API access plans, which govern the APIs that developers will have access to, and how much they are entitled to consume. API service composition is all about organizing different APIs into different plans, and setting rate limits that govern how much a user can use per second, minute, days, or by the month. New users are often placed into plans with stricter limitations, while more trusted, partner, and internal consumers enjoy higher rate limits, and less restrictive plans. Service composition helps minimize the damage that occurs whenever there are bad actors present, or security incidents occur, keeping the breaches limited to a small subset of low value APIs, and limited amount of resources accessed.</p>

<p><strong>Monitoring All API Availability</strong><br />
Beyond API management, and the analysis of API consumption, it is common for API providers to setup external monitors that keep an eye on whether APIs are up or down, and what their overall availability are. Providing a status dashboard showing whether APIs are available, which also often shows historical availability over time. The health and availability of an API is usually a barometer of the security of an API. Insecure, and compromised APIs often times have an unreliable availability and track record. Making monitoring critical to the overall security of API operations.</p>

<p><strong>Granular Testing of All APIs</strong><br />
Augmenting API management and monitoring, the most mature and secure API providers out there also run recurring tests on APIs, going beyond just seeing if they are up and available, and actually making sure they respond, and deliver the data and content that is expected. These tests will sometimes go further and test for the ability to publish bad data to APIs, input incorrect or additional information, and push the boundaries of what an API will respond to. Mimicking some of the behaviors in which malicious users and applications will perform, and testing the quality of the surface area of API.</p>

<p><strong>Security Scans For All APIs</strong><br />
Beyond granular tests for all APIs, more general security scans are regularly performed, looking beyond the potentially known security problems, and finding the more unknown issues. Scanning additional URLS, parameters, headers, and checking for holes, gaps, and other areas of the surface area for APIs which may have been overlooked or forgotten. Security scanning reflects the scanning that already occurs on most web and mobile applications, but will also consider many of the API specific vulnerabilities that we’ve seen behind breaches of the past, and those that are unique to API integration scenarios.</p>

<p><strong>API Security Is More About Building An Awareness</strong><br />
While API security centers around establishing a defensive perimeter around API resources, policing and enforcing rules along this perimeter, and encrypting all traffic, most of API security is realized through an awareness around how APIs are being accessed. API management, logging, monitoring, testing, and analytics provide an approach to understanding how data, content, media, and algorithms are being used, or not being used. Providing an evolved level of awareness that goes well beyond legacy web services and database connectivity.</p>

<p>API security should center around encryption, and common approaches to authenticating APIs. However, if they are not being properly monitored, tested, analyzed, and audited, a strong security perimeter, encryption, and strong authentication will not mean much. It is important for all key stakeholders involved in API operations to understand that API security is a balance between allowing for access and consumption, while also locking down, encrypting, and defending the perimeter. The API providers who find the most success with their API operations tend to strike a balance between these two opposing personalities of API security, making sure everything is secure, while also making sure providers, developers, and end-users feel secure while also being able to get access the resource they need to do their job.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/06/explaining-api-security-to-organizational-leadership/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/">Capital One DevExchange Provides An Important Banking API Blueprint</a></h3>
        <span class="post-date">06 Mar 2018</span>
        <p><a href="https://developer.capitalone.com"><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-banking-home-page.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>When you take a look at the banking API landscape in the United States, there is one clear leader in the game–Capital One. Their <a href="https://developer.capitalone.com/">DevExchange</a> program is miles ahead of every one of their competitors, giving them a significant head start when it comes to the banking API economy. Their approach to delivering APIs meets all of my minimum requirements for any successful API platform, and even exceeds it, providing what I’d consider to be a leading example blueprint that all banking API providers should be following.</p>

<p>The Capital One DevExchange begins as any API operation should, with a dedicated portal located at developer.[domain]:</p>

<ul>
  <li><a href="https://developer.capitalone.com/">developer.capitalone.com</a></li>
</ul>

<p>After landing on the home page for the Capital One DevExchange you get everything you need to get up and running with the APIs they have:</p>

<ul>
  <li><a href="https://developer.capitalone.com/platform-documentation/getting-started/">Getting started</a></li>
  <li><a href="https://developer.capitalone.com/platform-documentation/authorization-with-oauth-20/">Authentication</a></li>
  <li><a href="https://developer.capitalone.com/platform-documentation/">Documentation</a></li>
  <li><a href="https://developer.capitalone.com/platform-documentation/errors/">Errors</a></li>
  <li><a href="https://developer.capitalone.com/sign-in/">Login</a></li>
  <li><a href="https://developer.capitalone.com/sign-up">Registration</a></li>
</ul>

<p>The Capital One DevExchange provides four main groups of public APIs currently, started with access to account information in the following areas:</p>

<ul>
  <li>Retrieve account products - /deposits/account-products (GET)</li>
  <li>Retrieve account product details - /deposits/account-products/{productId} (GET)</li>
  <li>Create new account application - /deposits/account-applications (POST)</li>
  <li>Retrieve out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (GET)</li>
  <li>Answer out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (PUT)</li>
  <li>Retrieve account application details - /deposits/account-applications/{applicationId} (GET)</li>
</ul>

<p>As well as some credit card offers, showcasing the products they have available:</p>

<ul>
  <li>Retrieve product listings - /credit-offers/products (GET)</li>
  <li>Retrieve card products - /credit-offers/products/cards (GET)</li>
  <li>Retrieve card products - /credit-offers/products/cards (GET)</li>
  <li>Retrieve card products by type - /credit-offers/products/cards/{cardType} (GET)</li>
  <li>Retrieve card product details - /credit-offers/products/cards/{cardType}/{productId} (GET)</li>
</ul>

<p>Which you can actually sign up for and do a pre-qualification via APIs:</p>

<ul>
  <li>Create prequalification check - /credit-offers/prequalifications (POST)</li>
  <li>Create prequalification acknowledgment - /credit-offers/prequalifications/{prequalificationId} (POST)</li>
  <li>Create applicant key - /credit-offers/applicant-details POST</li>
</ul>

<p>Get access to Capital One rewards via APIs:</p>

<ul>
  <li>Retrieve rewards accounts - /rewards/accounts (GET)</li>
  <li>Retrieve rewards account details - /rewards/accounts/{rewardsAccountReferenceId} (GET)</li>
</ul>

<p>And details about merchants involved in transactions:</p>

<ul>
  <li>Retrieve merchant data - /merchant-insights/merchants (GET)</li>
  <li>Refresh merchant details - /merchant-insights/merchants/{merchantId} (GET)</li>
</ul>

<p>This version of the API is available in a sandbox and production environments:</p>

<ul>
  <li><a href="https://developer.capitalone.com/platform-documentation/using-the-sandbox/">Sandbox</a></li>
</ul>
<p><a href="http://api.reimaginebanking.com/"><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-hackathon-api.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>You can tell Capital One is moving cautiously with their public APIs, but they are definitely further along than other banks. Beyond what is publicly available in their sandbox and production environment they have another exploratory set of APIs coming from a project they call Nessie, <a href="http://api.reimaginebanking.com/">Capital One’s Hackathon API</a> that gives you access to a multitude of real public-facing data, such as ATM and bank branch locations, complete with mock customer account data. This set of APIs was used as part of a hackathon put on by the bank, but are worth showcasing as an example of where the bank is headed with their API road map.</p>

<p>By providing a pretty robust stack of API paths for working with accounts:</p>

<ul>
  <li>Get all accounts - /accounts (GET)</li>
  <li>Delete a specific existing account - /accounts/{id} (DELETE)</li>
  <li>Get account by id - /accounts/{id} (GET)</li>
  <li>Update a specific existing account - /accounts/{id} (PUT)</li>
  <li>Get customer that owns the specified account - /accounts/{id}/customer (GET)</li>
  <li>Get accounts by customer id - /customers/{id}/accounts (GET)</li>
  <li>Create an account - /customers/{id}/accounts (POST)</li>
</ul>

<p>As well as bills that are associated with accounts:</p>

<ul>
  <li>Get all bills for a specific account - /accounts/{id}/bills (GET)</li>
  <li>Create a bill - /accounts/{id}/bills (POST)</li>
  <li>Delete a specific existing bill - /bills/{billId} (DELETE)</li>
  <li>Get bill by id - /bills/{billId} (GET)</li>
  <li>Update a specific existing bill - /bills/{billId} (PUT)</li>
  <li>Get bills by customer id - /customers/{id}/bills (GET)</li>
</ul>

<p>Then API paths for managing deposits:</p>

<ul>
  <li>Get all deposits - /accounts/{id}/deposits (GET)</li>
  <li>Create a deposit - /accounts/{id}/deposits (POST)</li>
  <li>Delete a specific existing deposit - /deposits/{id} (DELETE)</li>
  <li>Get deposit by id - /deposits/{id} (GET)</li>
  <li>Update a specific existing deposit - /deposits/{id} (PUT)</li>
</ul>

<p>And for any details on loans:</p>

<ul>
  <li>Get all loans - /accounts/{id}/loans (GET)</li>
  <li>Create a loan - /accounts/{id}/loans (POST)</li>
  <li>Delete a specific existing loan - /loans/{id} (DELETE)</li>
  <li>Get loan by id - /loans/{id} (GET)</li>
  <li>Update a specific existing loan - /loans/{id} (PUT)</li>
</ul>

<p>Then providing insight into all purchases:</p>

<ul>
  <li>Get all purchases - /accounts/{id}/purchases (GET)</li>
  <li>Create a purchase - /accounts/{id}/purchases (POST)</li>
  <li>Get all purchases by account and merchant - /merchants/{id}/accounts/{accountId}/purchases (GET)</li>
  <li>Get all purchases by merchant - /merchants/{id}/purchases (GET)</li>
  <li>Delete a specific existing purchase - /purchases/{id} (DELETE)</li>
  <li>Get purchase by id - /purchases/{id} (GET)</li>
  <li>Update a specific existing purchase - /purchases/{id} (PUT)</li>
</ul>

<p>As well as bank account transfers:</p>

<ul>
  <li>Get all transfers - /accounts/{id}/transfers (GET)</li>
  <li>Create a transfer - /accounts/{id}/transfers (POST)</li>
  <li>Delete a specific existing transfer - /transfers/{transferId} (DELETE)</li>
  <li>Get transfer by id - /transfers/{transferId} (GET)</li>
  <li>Update a specific existing transfer - /transfers/{transferId} (PUT)</li>
</ul>

<p>And withdrawals:</p>

<ul>
  <li>Get all withdrawals - /accounts/{id}/withdrawals (GET)</li>
  <li>Create a withdrawal - /accounts/{id}/withdrawals (POST)</li>
  <li>Delete a specific existing withdrawal - /withdrawals/{id} (DELETE)</li>
  <li>Get withdrawal by id - /withdrawals/{id} (GET)</li>
  <li>Update a specific existing withdrawal - /withdrawals/{id} (PUT)</li>
</ul>

<p>Details about banking customers:</p>

<ul>
  <li>Get all customers - /customers (GET)</li>
  <li>Create a customer - /customers (POST)</li>
  <li>Get customer by id - /customers/{id} (GET)</li>
  <li>Update a specific existing customer - /customers/{id} (PUT)</li>
</ul>

<p>As well as available merchants:</p>

<ul>
  <li>Get all merchants - /merchants (GET)</li>
  <li>Create a merchant - /merchants (POST)</li>
  <li>Get merchant by id - /merchants/{id} (GET)</li>
  <li>Update a specific existing merchant - /merchants/{id} (PUT)</li>
</ul>

<p>Available ATMS:</p>

<ul>
  <li>Get all ATMs - /atms (GET)</li>
  <li>Get ATM by id - /atms/{id} (GET)</li>
</ul>

<p>And Bank Branches:</p>

<ul>
  <li>Get all branches - /branches (GET)</li>
  <li>Get branch by id - /branches/{id} (GET)</li>
</ul>

<p>I am not sure what Capital One’s intentions are with introducing these APIs to the main stack of public APIs, but they are still available within a separate Hackathon sandbox environment:</p>

<ul>
  <li><a href="http://api.reimaginebanking.com/">Hackathon API</a></li>
</ul>

<p>Beyond the sandbox, hackathon, and production APIs available, Capital One provides all the expected support and communication channels that you expect out of an active API program:</p>

<ul>
  <li><a href="https://developer.capitalone.com/support/">Support</a></li>
  <li><a href="https://developer-support.capitalone.com/apex/DXSearchArticles">Knowledgebase</a></li>
  <li><a href="https://developer.capitalone.com/blogs/">Blog</a></li>
  <li><a href="https://twitter.com/CapitalOneDevEx">Twitter</a></li>
</ul>

<p>As well as not forgetting the required elements present from the legal department, helping set the tone for API engagements:</p>

<ul>
  <li><a href="https://www.capitalone.com/identity-protection/privacy/statement">Privacy policy</a></li>
  <li><a href="https://developer.capitalone.com/single/terms-and-conditions/">Terms of service</a></li>
</ul>

<p><a href="https://developer.capitalone.com/open-source/"><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-open-source.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>Where the Capital One API program really begins to impress is with their usage of Github, and providing of open source solutions:</p>

<ul>
  <li><a href="https://github.com/capitalone">Capital One Github</a></li>
  <li><a href="https://github.com/CapitalOne-DevExchange">Capital One Devexchange Github</a></li>
  <li><a href="https://developer.capitalone.com/open-source/">Open Source</a></li>
</ul>

<p>It isn’t easy doing Open Source in a highly regulated industry, and Capital One not only is doing it, <a href="https://developer.capitalone.com/blog-post/open-source-in-a-regulated-environment-lessons-learned-on-our-open-source-journey-at-capital-one/">they actually share some of the story behind their struggle</a>. The open source projects they have been releasing, and the stories they’ve told about them on the blog tell just as important of a story as each of their APIs do. Further externalizing how the bank delivers technology at the bank, as well as with partners and 3rd party developers.</p>

<p>The Capital One DevExchange is what ALL US banks should be emulating. This isn’t just about publicly available APIs. This is about being able to deliver APIs consistently, in ways that are in sync with your lines of business. The more you can deliver this publicly, tell the story of it, and release open source code that demonstrates the value it brings, the more you will be able to do this internally, across groups, and amongst your partners. The biggest mistake any bank can be making in 2018 is thinking doing APIs is purely about retail banking. It is really about how agile and effective your bank will be leverage web technology internally, and externally within your company.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and-constraints/">Serverless, Like Microservices Is About Understanding Our Dependencies And Constraints</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/death-valley-national-park_dali_three_just_road.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am not buying into all the hype around the recent serverless evolution in compute. However, like most trends I study, I am seeing some interesting aspects of how people are doing it, and some positive outcomes for teams who are doing it sensibly. I am not going all in on serverless when it comes to deploying or integrating with APIs, but I am using it for some projects, when it makes sense. I find AWS Lambda to be a great way to get in between the AWS API Gateway and backend resources, in order to conduct a little transformation. Keeping serverless as just one of many tools in my API deployment and integration toolbox, yet never going overboard with any single solution.</p>

<p>I put serverless into the same section of my toolbox as I do microservices. Serverless is all about decoupling how I deploy my APIs, helping me keep things doing small, meaningful things behind each of my API paths. Serverless forces me to think through how I am decoupling my backend, and pushes me to identify dependencies, acknowledge constraints, and get more creative in how I write code in this environment. To do one thing, and do it well, I need an intimate understanding of where my data and other backend resources are, and how I can distill a unit of compute down to the smallest unit as I possibly can. Identifying, reducing, and being honest about what my dependencies are is essential to serverless working or not working for me.</p>

<p>The challenge I’m having with serverless at the moment is that it can be easy to ignore many of the benefits I get from dependencies, while just assuming all dependencies are bad. Specifically around the frameworks I deploy as part of my API solutions. When coupled with AWS API Gateway this isn’t much of a problem, but all by itself, AWS Lambda doesn’t have all fo the other HTTP, transformation, header, request, and response goodness I enjoy from the API frameworks I’ve historically depend on. Showing me that understanding our dependencies is important, both good and bad. Not all dependencies are bad, as long as I am aware of them, have a knowledge of what they bring to the table, and it is a stable, healthy, and regularly evaluated relationships. This is a challenge I also come across regularly in microservices as well as serverless journeys, that we just throw things out for the sake of size, without thinking deeply about why we are doing something, and whether or not it is worth it.</p>

<p>I can see plenty of scenarios where I will be thinking that a serverless approach is good, but after careful evaluation of dependencies and constraints, I abandon this path as an option. I’m guessing we’ll see a goldilocks style of serverless emerge that helps us find this sweet spot. Similar to what we are finding with the microservices evolution, in that micro isn’t always about small–it is often just about finding the sweet spot between big and small, hot and cold, dependent and independent. Being sensible about how we use the tools in our API toolbox, and realizing most of these approaches are more about the journey, than they are about the actual tool.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and-constraints/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/an-openapi-rules-engine/">An OpenAPI-Driven, API Governance Rules Engine</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/rules/9968073905_95ce575233_z.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Phil Sturgeon (<a href="https://twitter.com/philsturgeon">@philsturgeon</a>) alerted me to a pretty cool project he is cooking up, called <a href="https://github.com/wework/speccy">Speccy</a>. Which provides a rules engine for validating your OpenAPI definitions. “Taking off from where <a href="https://twitter.com/PermittedSoc/">Mike Ralphson</a> started with linting in <a href="https://github.com/Mermade/swagger2openapi/">swagger2openapi</a>, Speccy aims to become the <a href="https://github.com/bbatsov/rubocop">rubocop</a> or <a href="https://eslint.org/">eslint</a> of OpenAPI”, and to “sniff your files for potentially bad things. “Bad” is objective, but you’ll see validation errors, along with special rules for making your APIs better.” Helping make sure your API definitions are as consistent as they possibly can be, and deliver on your API governance strategy (you have one right?)</p>

<p>With Speccy, there are a default set of rules, things like ensuring you have a summary or a description for each API path:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "operation-summary-or-description",
	"object": "operation",
	"enabled": true,
	"description": "operation should have summary or description",
	"or": ["summary", "description"]
}
</code></pre></div></div>

<p>Or making sure you add descriptions to your parameters:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "parameter-description",
	"object": "parameter",
	"enabled": true,
	"description": "parameter objects should have a description",
	"truthy": "description"
}
</code></pre></div></div>

<p>Or making sure you include tags for each aPI path:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "operation-tags",
	"object": "operation",
	"enabled": true,
	"description": "operation should have non-empty tags array",
	"truthy": "tags",
	"skip": "isCallback"
}
</code></pre></div></div>

<p>Then you can get more strict by requiring contact information:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "contact-properties",
	"object": "contact",
	"enabled": true,
	"description": "contact object should have name, url and email",
	"truthy": [ "name", "url", "email" ]&lt;br /&gt;
}
</code></pre></div></div>

<p>And make sure youi have a license applied to your API:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "license-url",
	"object": "license",
	"enabled": true,
	"description": "license object should include url",
	"truthy": "url"
}
</code></pre></div></div>

<p>Speccy is available <a href="https://www.npmjs.com/package/speccy">as a Node package</a>, which you can easily run at the command line. Speccy is definitely what is needed out there right now, helping us validate the growing number of OpenAPI definitions in our life. As many companies are thinking about how they can apply API governance across their operations, they should be looking at contributing to <a href="https://github.com/wework/speccy">Speccy</a>. It is something I’ve been talking with API service providers about for some time, but haven’t seen an open source answer emerge, that can help us develop rules for what we expect of our OpenAPI definitions.</p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/openapi-logo.png" align="right" width="25%" style="padding: 15px;" /></p>
<p>My only feedback right now, is that we need lots of people using it, and helping contribute rules. Oh, and wrap it in an API, and make it available as an easy to use, and deploy containerized microservice. Then lets get to work on the Github Gist-driven marketplace of rules, where I can publish the rules I develop across the projects I’m working on, and of the clients I consult with. Let’s get to work making sure there are a wealth of rules, broken down into different categories for API providers to choose from. Then let’s get API tooling and service providers to begin baking a Speccy rules engine into their solutions, and allow for the import and management of open source rules.</p>

<p>Speccy only works with OpenAPI 3.0, which makes sense if we are going to be moving forward with this conversation. Spreccy is how we will validate that banking APIs are PSD2 compliant. It is how we will ensure healthcare APIs support the FHIR specification. I have other suggestions for the CLI and API usage of Speccy, but I’d rather see investment in the available rules, before I make too many functional suggestions. I think the rules are where we will begin to define what we are looking for in an OpenAPI rules engine, and that should drive the Speccy features which end up on the road map.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/an-openapi-rules-engine/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks/">People Seem to Want Lego Kits and Not a Bucket of Lego Blocks When It Comes To Doing APIs</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/legos/lego-millenium-falcon-instructions.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>While I wish everyone saw the modular potential of APIs and microservices like I do, I’ve come to the realization that most people are just interested in ready to go kits that walk them through every detail of doing APIs, rather than actually playing, learning, evolving, and learning to be productive with a big bucket of APIs. I’m not just focusing on business users here, I am talking about a significant portion of the developers I come across, who really don’t seem that interested in learning to apply API concepts, and understanding when and where to use them, they just want a set of instructions that walk them through each step of deploying an API.</p>

<p>I actually am a proponent of there being more boxed, lego style API kits that teach you how to build the product API, task API, press release API, and other common implementations. Robust, detailed, ready-to-go API implementations that would walk people through each and every step of defining, designing, deploying, managing, monitoring, testing, and documenting their API. I feel like this would significantly help folks think through what are healthy API practices, and be introduced to different ways of thinking around APIs. However, I do not want people to become reliant on only being able to operate within this paradigm, and not actually be able to fix, evolve, and deliver their own custom API solutions, using the healthy practices they are being introduced to.</p>

<p>People seem to just want shortcuts, and things done for them. People want the solutions packaged and delivered to their doorstep. They seem unable to be able to find the solutions on their own, or even be able to absorb a lesson delivered via a packaged solution. I’m not sure what the cure for this condition is. It is hard to tell whether it is vendor induced, or (would be) API provider induced. Have people been conditioned by vendors to just be spoon fed solutions? Or are people just lazy, and not interested in truly learning, truly tackling their technical debt, and finding a new path forward? IDK. I’m feeling like it is probably a little of both, feeding off of each other.</p>

<p>I get why the pre-built, ready-to-go Lego API kit is appealing. Everyone wants to have a full-blown millennium falcon when they are done working on a project, and not some blocky mcsquare flying machine. However, it takes time before you are able to deliver at that scope. You need to practice building smaller implementations, and yes playing with other pre-built kits, doing some reverse engineering, in addition to some forward engineering. I’m guessing this all comes down to if you truly want to know and understand, or if you are just looking for solutions. If you are just looking for solutions, I’m guessing in 5 years you’ll be eagerly buying the next solution for the API mess you are putting into place currently. It is the natural evolution of how technology gets bought and sold.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud-providers/">The Need for Standardized API Plans and Pricing to Compete with Cloud Providers</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/docks_copper_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://cloudplatform.googleblog.com/2018/02/introducing-Cloud-Billing-Catalog-API-GCP-pricing-in-real-time.html">Google launched their Cloud Billing Catalog API, providing access to thee pricing for their cloud API catalog</a> the other day. <a href="https://docs.microsoft.com/en-us/azure/billing/">Azure has their billing API</a>, and <a href="https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/Welcome.html">AWS has their cost explorer API service</a>, showing that programmatic access to what API resources cost, as well as management of usage, billing, invoicing, and other aspects of doing business with APIs is becoming the normal mode of operating an API platform.</p>

<p>I’ve long used AWS, Google, and increasingly Azure as a blueprint for what us smaller API providers should be doing. They are full of positive and negative lessons for any API provider. However, I’m starting to see what they are doing as not just a blueprint, but potentially something that will force many of us API providers out of businesses if we cannot emulate what they are doing at scale. The tractor beam that is the cloud providers is strong. They bring a lot of benefits to the table. So much so, it is getting harder and harder for independent API providers to compete. Offering benefits to consumers that will become deal breakers with using other 3rd party API providers services, pushing API consumers to stay within their chosen cloud platform walled garden.</p>

<p>As a developer, if I can programmatically manage the plans, pricing, and billing for ALL the APIs I use via AWS, Google, and Azure, but I have to manually manage this across many different 3rd party providers, I am going to be hesitant when it comes to adopting any new services that aren’t within my cloud domain. As I depend on more APIs, the benefits of being able to programmatically manage the business of my API consumption is becoming increasingly critical. If the individual 3rd party API providers I use don’t begin to offer APIs for managing the business of my API consumption, and adopting a standardized interface across all the APIs I depend on, I’m going to favor my cloud native API solutions over the 3rd party, and custom ones–giving cloud providers a significant advantage.</p>

<p>It is something that smaller API providers are going to have to start thinking about, and stop being special little snowflakes, and consider how they can start standardizing and being interoperable. Otherwise the cloud API providers are going to continue to gain marketshare, and everyone will just use the APIs available to use in the cloud. Reducing the competition, diversity, and utility that the API sector has become known for. We will just use Amazon, Google, and Azure APIs, and if they don’t have it, it probably won’t be done. Innovation will only occur within the cloud marketplaces, and be way less vibrant than the API sector we’ve enjoyed over the last decade.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud-providers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/">Thoughts On The Schema.Org WebAPI Type Extension</a></h3>
        <span class="post-date">02 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/schema-org/schema-org.png" width="30%" align="right" style="padding: 15px;" /></p>
<p>I’m putting some thought into <a href="https://webapi-discovery.github.io/rfcs/rfc0001.html#content-types">the Schema.Org WebAPI Type Extension proposal</a> by Mike Ralphson (Mermade Software) and Ivan Goncharov (APIs.guru), to “facilitate better automatic discovery of WebAPIs and associated machine and human-readable documentation”. It’s an interesting evolution in how we define APIs, in terms of API discovery, but I would also add potentially at “execute time”.</p>

<p>Here is what a base WebAPI type schema could look like:</p>

<p>```{</p>

<p>“@context”: “http://schema.org/”,</p>

<p>“@type”: “WebAPI”,</p>

<p>“name”: “Google Knowledge Graph Search API”,</p>

<p>“description”: “The Knowledge Graph Search API lets you find entities in the Google Knowledge Graph. The API uses standard schema.org types and is compliant with the JSON-LD specification.”,</p>

<p>“documentation”: “https://developers.google.com/knowledge-graph/”,</p>

<p>“termsOfService”: “https://developers.google.com/knowledge-graph/terms”,</p>

<p>“provider”: {</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"@type": "Organization",

"name": "Google Inc."   } }```
</code></pre></div></div>

<p>Then the proposed extensions could include the following:</p>

<ul>
  <li><strong>versions</strong> (OPTIONAL array of <a href="https://schema.org/Thing">thing</a> -&gt; <a href="http://meta.schema.org/Property">Property</a> -&gt; <a href="http://schema.org/softwareVersion">softwareVersion</a>). It is RECOMMENDED that APIs be versioned using [semver]</li>
  <li><strong>entryPoints</strong> (OPTIONAL array of <a href="https://schema.org/Thing">Thing</a> -&gt; <a href="https://schema.org/Intangible">Intangible</a> -&gt; <a href="https://schema.org/EntryPoint">EntryPoint</a>)</li>
  <li><strong>license</strong> (OPTIONAL, <a href="http://schema.org/CreativeWork">CreativeWork</a> or <a href="http://schema.org/URL">URL</a>) - the license for the design/signature of the API</li>
  <li><strong>transport</strong> (enumerated <a href="http://schema.org/Text">Text</a>: HTTP, HTTPS, SMTP,  MQTT, WS, WSS etc)&lt;/p&gt;</li>
  <li><strong>apiProtocol</strong> (OPTIONAL, enumerated <a href="http://schema.org/Text">Text</a>:  SOAP, GraphQL, gRPC, Hydra, JSON API, XML-RPC, JSON-RPC etc)</li>
  <li><strong>webApiDefinitions</strong> (OPTIONAL array of <a href="http://schema.org/EntryPoint">EntryPoints</a>) containing links to <a data-link-type="dfn" href="#machine-readable-api-definition" id="ref-for-machine-readable-api-definition-1">machine-readable API definition</a>s</li>
  <li><strong>webApiActions</strong> (OPTIONAL array of potential <a href="http://schema.org/Action">Actions</a>)</li>
</ul>

<p>The webApiDefinitions (EntryPoint) contentType property contains a reference to one of the following conten types:</p>

<ul>
  <li><strong>OpenAPI / Swagger in JSON</strong> - application/openapi+json or application/x-openapi+json</li>
  <li><strong>OpenAPI / Swagger in YAML</strong> - application/openapi</li>
  <li><strong>RAML</strong> - application/raml+yaml</li>
  <li><strong>API Blueprint in markdown</strong> - text/vnd.apiblueprint</li>
  <li><strong>API Blueprint parsed in JSON</strong> - application/vnd.refract.parse-result+json</li>
  <li><strong>API Blueprint parsed in YAML</strong> - application/vnd.refract.parse-result+yaml</li>
</ul>

<p>Then the webApiActions property brings a handful of actions to the table, with the following being suggested:</p>

<ul>
  <li><strong>apiAuthentication</strong> - Links to a resource detailing authentication requirements. Note this is a human-readable resource, not an authentication endpoint</li>
  <li><strong>apiClientRegistration</strong> - Links to a resource where a client may register to use the API</li>
  <li><strong>apiConsole</strong> - Links to an interactive console where API calls may be tested</li>
  <li><strong>apiPayment</strong> - Links to a resource detailing pricing details of the API</li>
</ul>

<p>I fully support extending the Schema.org WebAPI vocabulary in this way. It adds all the bindings needed to make the WebAPI type executable at runtime, as well as it states at discovery time. I like the transport and protocol additions, helping ensure the WebAPI vocabulary is as robust as it possibly can. webApiDefinitions provides all the technical details regarding the surface area of the API we need to actually engage with it at runtime, and webApiActions begins to get at some of the business of APIs friction that exists at runtime. Making for an interesting vocabulary that can be used to describe web APIs, which also becomes more actionable by providing everything you need to get up and running.</p>

<p>The suggestions are well thought out and complete. If I was to add any elements, I’d say it also needs a support link. There will be contact information embedded within the API definitions, but having a direct link along with registration, documentation, terms of service, authentication, and payment would help out significantly. I would say that the content type to transport and protocol coverage is deficient a bit. Meaning you have SOAP, but not referencing WSDL. I know that there isn’t a direct definition covering every transport and protocol, but eventually it should be as comprehensive as it can. (ie. adding AsyncAPI, etc. in the future)</p>

<p>The WebAPI type extensions reflect what we have been trying to push forward with our <a href="http://apisjson.org">APIs.json</a> work, but comes at it from a different direction. I feel there are significant benefits to having all these details as part of the Schema.org vocabulary, expanding on what you can describe in a common way. Which can then also be used as part of each APIs requests, responses, and messages. I don’t see APIs.json as part of a formal vocabulary like this–I see it more as the agile format for indexing APIs that exist, and building versatile collections of APIs which could also contain a WebAPI reference.</p>

<p>I wish I had more constructive criticism or feedback, but I think it is a great first draft of suggestions for evolving the WebAPI type. There are other webApiActions properties I’d like to see based upon my APIs.json work, but I think this represents an excellent first step. There will be some fuzziness between documentation and apiConsole, as well as gaps in actionability between apiAuthentication, and apiClientRegistration–thinks like application creation (to get keys), and opportunities to have Github, Twitter, and other OpenID/OAuth authentication, but these things can be worked out down the road. Sadly there isn’t much standardization at this layer currently, and I see this extension as a first start towards making this happen. As I said, this is a good start, and we have lots of work ahead as we see more adoption.</p>

<p>Nice work Mike and Ivan! Let me know how I can continue to amplify and get the word out. We need to help make sure folks are describing their APIs using Schema.org. I’d love to be able to automate the discovery of APIs, using existing search engines and tooling–I know that you two would like to see this as well. API discovery is a huge problem, which there hasn’t been much movement on in the last decade, and having a common vocabulary that API providers can use to describe their APIs, which search engines can tune into would help move us further down the road when it comes to having more robust API discovery.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/">An Observable Industry Level Directory Of API Providers And Consumers</a></h3>
        <span class="post-date">01 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/vancouver_diego_rivera1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve been breaking down the work on banking APIs coming out of <a href="http://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/">Open Banking in the UK</a> lately. I recently took all their <a href="http://open.banking.blueprint.apievangelist.com/">OpenAPI definitions and published as a demo API developer portal</a>. Bringing the definitions out of the shadows a little bit, and showing was is possible with the specification. Pushing the project forward some more today I published <a href="http://open.banking.blueprint.apievangelist.com/#Open Banking - Directory APIs">the Open Banking API Directory specification to the project</a>, showing the surface area of the very interesting, and important component of open banking APIs in the UK.</p>

<p>The Open Banking Directory provides a pretty complete, albeit rough and technical approach to delivering observability for <a href="http://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/">the UK banking industry API ecosystem actor layer</a>. Everyone involved in the banking API ecosystem in UK has to be registered in the directory. It provides profiles of the banks, as well as any third party players. It really provides an unprecedented, industry level look at how you can make API ecosystems more transparent and observable. This thing doesn’t exist at the startup level because nobody wants to be open with the number of developers, or much else regarding the operation of their APIs. Making any single, or industry level API ecosystem, operate as black boxes–even if they claim to be an “open API”.</p>

<p>Could you imagine if API providers didn’t handle their own API management layer, and an industry level organization would handle the registration, certification, directory, and dispute resolution between API providers and API consumers? Could you imagine if we could see the entire directory of Facebook and Twitter developers, understand what businesses and individuals were behind the bots and other applications? Imagine if API providers couldn’t lie about the number of active developers, and we knew how many different APIs each application developers used? And it was all public data? An entirely different API landscape would exist, with entirely different incentive models around providing and consuming gAPIs.</p>

<p>The Open Banking Directory is an interesting precedent. It’s not just an observable API authentication and management layer. It also is an API. Making the whole thing something that can be baked into the industry level, as well as each individual application. I’m going to have to simmer on this concept some more. I’ve thought a lot about collective API developer and client solutions, but never anything like this. I’m curious to see how this plays out in a heavily regulated country and industry, but also eager to think about how something like this might work (or not) in government API circles, or even in the private sector, within smaller, less regulated industries.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/28/an-open-banking-blueprint-api-portal/">An Open Banking API Portal Blueprint</a></h3>
        <span class="post-date">28 Feb 2018</span>
        <p>I have been learning all about <a href="https://www.openbanking.org.uk/">the banking API efforts out of Open Banking in the UK lately</a>. They are evolving a set of read / write account and transaction API, as well as public data APIs for some of the common information 3rd party developers are looking to get their hands on. I'm intrigued with the traction the organization has gotten, and I want to be able to fully understand what they are developing, as well help contribute where I can.</p>
<p>To help me understand the API specification, as well as hopefully contribute to the conversation, I am publishing <a href="http://open.banking.blueprint.apievangelist.com/">an blueprint API portal for the API</a>. It is a demo portal, running on Github, which uses the API Evangelist graphical look, but I am also publishing documentation for v1.1.1 of the account and payments API, as well as v2.1 of the public data APIs. I'm looking to publish the API specification like any bank would, but it won't actually be a live API--yet. I'd like to turn it into a mock API, with some virtualized data to demonstrate what is possible.</p>
<p align="center"><a href="http://open.banking.blueprint.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/open-banking/open-banking-accounts-documentation-screenshot.png" width="90%" align="center" style="padding: 15px;" /></a></p>
<p>I've only had time to publish the overview of the project, and the documentation for each current version. I have a todo list of things I would like to invest in when I have more time. Eventually, I want it to be a complete, forkable Open Banking API portal that any bank in the UK could publish. Then I'm looking to create country specific versions to help push French, German, and other banks to push a portal. It doesn't have to be my solution that the banks use, but hopefully they'll at least use what I have provided as a blueprint. The goal isn't just to get them to use the portal, it is to get them implementing their bank's API developer portal in a standardized way--similar to the API specification from Open Banking, but this is the portal specification.</p>
<p>I will spend time on the portal over the next couple of weeks. If there is something you'd like to see accomplished, or something I'm missing entirely, feel free to <a href="https://github.com/european-banking-apis/open-banking-blueprint/issues">submit a Github issue for the project</a>. The project repository is a little messy right now as it is in full development, so if you fork, be careful--you might want to wait.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/28/an-open-banking-blueprint-api-portal/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">What We Need To Be Machine Readable At API Run Time</a></h3>
        <span class="post-date">28 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/4882162452_fa3126b38d_b_spagetti_accident.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I had breakfast with Mike Amundsen (<a href="https://twitter.com/mamund">@mamund</a>) and Matt McLarty (<a href="https://twitter.com/MattMcLartyBC">@MattMcLartyBC</a>) of the CA API Academy team this morning in midtown this morning. As we were sharing stories of what each other was working on, the topic of what is needed to execute an API call came up. Not the time consuming find an API, sign up for an account, figure out the terms of service and pricing version, but all of this condensed into something that can happen in a split second within applications and systems.</p>

<p>How do we distill down the essential ingredients of API consumption into a single, machine readable unit that can be automated into what Mike Amundsen calls, “find and bind”. This is something I’ve been thinking a lot about lately as I work on my API discovery research, and there are a handful of elements that need to be present:</p>

<ul>
  <li><strong>Authentication</strong> - Having keys to be able to authentication.</li>
  <li><strong>Surface Area</strong> - What is the host, base url, path, headers, and parameters for a request.</li>
  <li><strong>Terms of Service</strong> - What are the legal terms of service for consumption.</li>
  <li><strong>Pricing</strong> - How much does each API request cost me?</li>
</ul>

<p>We need these elements to be machine readable and easily accessible at discover and runtime. Currently the surface area of the API can be described using OpenAPI, that isn’t a problem. The authentication details can be included in this, but it means you already have to have an application setup, with keys. It doesn’t include new users into the equation, meaning, discovering, registering, and obtaining keys. I have a draft specification I call “API plans” for the pricing portion of it, but it is something that still needs a lot of work. So, in short, we are nowhere near having this layer ready for automation–which we will need to scale all of this API stuff.</p>

<p>This is all stuff I’ve been beating a drum about for years, and I anticipate it is a drum I’ll be beating for a number of more years before we see come into focus. I’m eager to see Mike’s prototype on “find and bind”, because it is the only automated, runtime, discovery, registration, and execute research I’ve come across that isn’t some proprietary magic. I’m going to be investing more cycles into my API plans research, as well as the terms of service stuff I started way back when alongside my API Commons project. Hopefully, moving all this forward another inch or two, and flesh out more of the machine readable components we’ll need at this layer.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/28/the-business-of-running-government-as-a-microservices-platform/">The Business of Running Government As A Microservices Platform</a></h3>
        <span class="post-date">28 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/capital-battle_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I recently <a href="http://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">wrote a response to a recent Department of Veterans Affairs RFI which contained a section about the business of operating government as a microservices platform</a>.I know that many folks wouldn’t make it that far in the 10K word response, so I wanted to break it out into its own post. I feel pretty strongly about the potential of decoupling how we deliver technology across government, but for this to be successful we are also going to have to decouple the business and politics of it all as well. This post reflects <a href="http://public.data.api.management.apievangelist.com/">my current research and thinking about the business of APIs in government, and is part of some ongoing work I am doing around API management, public data, and how we begin to think differently about how government engages with the public in a digital age</a>.</p>

<p>There are many interpretations of what is a microservice, but for the purposes of this post, it is a simple set of APIs that meet one precise set of government services. The API definition, database, back-end code, management layer, documentation, support and all other essential elements are self-contained, and usually stored in a single Github, or Bitbucket repository, when delivering microservices. Each microservice possesses its own technical, business, and political contract, outline how the service will be delivered, managed, supported, communicated, and versioned. These contracts can be realized individually, or grouped together as a larger, aggregate contract that can be submitted, while still allowing each individual service within that contract to operate independently.</p>

<p><strong>Decoupling The Business Of Delivering Government Digital Services</strong>
The microservices approach isn’t just about the technical components. It is about making the business of delivering vital government services more modular, portable, and scalable. Something that will also decouple and shift the politics of delivering critical services to veterans. Breaking things down into much more manageable chunks that can move forward independently at the contract level. Helping both simplify, and streamline the deliver of services for both the provider, vendor, as well as any other stakeholders involved in the software lifecycle. Microservices isn’t just about decoupling the technology, it is about decoupling the business of delivering digital services:</p>

<ul>
  <li><strong>Micro Procurement</strong> - One of the benefits of breaking down services into small chunks, is that the money needed to deliver the service can become much smaller, potentially allowing for a much smaller, more liquid and flowing procurement cycle. Each service has a micro definition of the monetization involved with the service, which can be aggregated by groups of services and projects.</li>
  <li><strong>Micro Payments</strong> - Payments for service deliver can be baked into the operations and life cycle of the service. API management excels at measuring how much a service is accessed, and testing, monitoring, logging, security, and other stops along the API life cycle can all be measured, and payments can be delivered depend on quality of service, as well as volume of service.</li>
</ul>

<p>Amazon Web Services already has the model for defining, measuring, and billing for API consumption in this way. This is the bread and butter of the Amazon Web Services platform, and the cornerstone of what we know as the cloud. This approach to delivering, scaling, and ultimately billing or payment for the operation and consumption of resources, just needs to be realized within each agency, and the rest of the federal government. We have seen a shift in how government views the delivery and operation of technical resources using the cloud over the last five years, we just need to see the same shift for the business of APIs over the next five years.</p>

<p><strong>Changing The Way Government Does Business</strong>
API management is where you begin changing the way government does business. API management has been used for a decade to measure, limit, and quantify the value being exchanged at the API level. Now that API management has been baked into the cloud, we are starting to see the approach being scaled to deliver at a marketplace level. With over ten years of experience with delivering, quantifying, metering and billing at the API level, Amazon is the best example of this monetization approach in action, with two distinct ways of quantifying the business of APIs.</p>

<ul>
  <li><strong>AWS Marketplace Metering Service</strong> - SaaS style billing model which provides a consumption monetization model in which customers are charged only for the number of resources they use–the best known cloud model.</li>
  <li><strong>AWS Contract Service</strong> - Billing customers in advance for the use of software, providing an entitlement monetization model in which customers pay in advance for a certain amount of usage, which could be used to deliver certain amount of storage per month for a year, or a certain amount of end-user licenses for some amount of time.</li>
</ul>

<p>This provides a framework for thinking about how the business of microservices can be delivered. Within these buckets, AWS provides a handful of common dimensions for thinking through the nuts and bolts of these approaches, quantifying how APIs can be monetized, in nine distinct areas:</p>

<ul>
  <li><strong>Users</strong> – One AWS customer can represent an organization with many internal users. Your SaaS application can meter for the number of users signed in or provisioned at a given hour. This category is appropriate for software in which a customer’s users connect to the software directly (for example, with customer-relationship management or business intelligence reporting).</li>
  <li><strong>Hosts</strong> – Any server, node, instance, endpoint, or other part of a computing system. This category is appropriate for software that monitors or scans many customer-owned instances (for example, with performance or security monitoring). Your application can meter for the number of hosts scanned or provisioned in a given hour.</li>
  <li><strong>Data</strong> – Storage or information, measured in MB, GB, or TB. This category is appropriate for software that manages stored data or processes data in batches. Your application can meter for the amount of data processed in a given hour or how much data is stored in a given hour.</li>
  <li><strong>Bandwidth</strong> – Your application can bill customers for an allocation of bandwidth that your application provides, measured in Mbps or Gbps. This category is appropriate for content distribution or network interfaces. Your application can meter for the amount of bandwidth provisioned for a given hour or the highest amount of bandwidth consumed in a given hour.</li>
  <li><strong>Request</strong> – Your application can bill customers for the number of requests they make. This category is appropriate for query-based or API-based solutions. Your application can meter for the number of requests made in a given hour.</li>
  <li><strong>Tiers</strong> – Your application can bill customers for a bundle of features or for providing a suite of dimensions below a certain threshold. This is sometimes referred to as a feature pack. For example, you can bundle multiple features into a single tier of service, such as up to 30 days of data retention, 100 GB of storage, and 50 users. Any usage below this threshold is assigned a lower price as the standard tier. Any usage above this threshold is charged a higher price as the professional tier. Tier is always represented as an amount of time within the tier. This category is appropriate for products with multiple dimensions or support components. Your application should meter for the current quantity of usage in the given tier. This could be a single metering record (1) for the currently selected tier or feature pack.</li>
  <li><strong>Units</strong> – Whereas each of the above is designed to be specific, the dimension of Unit is intended to be generic to permit greater flexibility in how you price your software. For example, an IoT product which integrates with device sensors can interpret dimension “Units” as “sensors”. Your application can also use units to make multiple dimensions available in a single product. For example, you could price by data and by hosts using Units as your dimension. With dimensions, any software product priced through the use of the Metering Service must specify either a single dimension or define up to eight dimensions, each with their own price.</li>
</ul>

<p>These dimensions reflect the majority of API services being sold out there today, we don’t find ourselves in a rut with measuring value, like just paying per API call. Allowing government API plans to possess one or more dimensions, beyond any single use case.</p>

<ul>
  <li><strong>Single Dimension</strong> - This is the simplest pricing option. Customers pay a single price per resource unit per hour, regardless of size or volume (for example, $0.014 per user per hour, or $0.070 per host per hour).</li>
  <li><strong>Multiple Dimensions</strong> – Use this pricing option for resources that vary by size or capacity. For example, for host monitoring, a different price could be set depending on the size of the host. Or, for user-based pricing, a different price could be set based on the type of user (admin, power user, and read-only user). Your service can be priced on up to eight dimensions. If you are using tier-based pricing, you should use one dimension for each tier.</li>
</ul>

<p>This provides a business framework that government can provide for vendors and 3rd party developers, allowing them to operate their services within a variety of business models. Derived from many of the hard costs they face, and providing additional volume based revenue, based upon how may API calls of any particular service receives.</p>

<p>Beyond this basic monetization framework, I’d also recommend adding in an incentive framework that would dovetail with the business models proposed, but then provide different pricing levels depending on how well the services perform, and deliver on the agreed upon API contract. There are a handful of bullets I’d consider here.</p>

<ul>
  <li><strong>Design</strong> - How well does a service meet API design guidelines set forth in governance guidance.</li>
  <li><strong>Monitoring</strong> - Has a service consistently met its monitoring goals, delivering against an agreed upon service level agreement (SLA).</li>
  <li><strong>Testing</strong> - Beyond monitoring, are APIs meeting granular interface testing, along a regular testing &amp; monitoring schedule.</li>
  <li><strong>Communication</strong> - Are service owners meeting expectations around communication around a service operations.</li>
  <li><strong>Support</strong> - Does a service meet required support metrics, making sure it is responsive and helpful.</li>
  <li><strong>Ratings</strong> - Provide a basic set of metrics, with accompanying ratings for each service.</li>
  <li><strong>Certification</strong> - Allowing service providers to get certified, receiving better access, revenue, and priority.</li>
</ul>

<p>All of the incentive framework is defined and enforced via the API governance strategy for the platform. Making sure all microservices, and their owners meet a base set of expectations. When you take the results and apply weekly, monthly, and quarterly against the business framework, you can quickly begin to see some different pricing levels, and revenue opportunities around all microservices emerge. You deliver consistent, reliable, highly ranked microservices, you get paid higher percentages, enjoy greater access to resources, and prioritization in different ways via the platform–if you don’t, you get paid less, and operate fewer services.</p>

<p>This model is already visible on the AWS platform. All the pieces are there to make it happen for any platform, operating on top of the AWS platform. The marketplace, billing, and AWS API Gateway connection to API plans exists. When you combine the authentication and service composition available at the AWS API Gateway layer, with the IAM policy solutions available via AWS, an enterprise grade solution for delivering this model securely at scale, comes into focus.</p>

<p><strong>Incentivizing Government API Vendors and Contractors</strong>
Keep everything small, and well defined. Measure, reported upon, and priced using the cloud model, connecting to a clear set of API governance guidance and expectations. The following areas can support paying and incentivizing contractors based upon not just usage, but also meeting the API contract.</p>

<ul>
  <li><strong>Management</strong> - API management puts all microservices into plans, then log, meter, and track on value exchanged at this level.</li>
  <li><strong>Marketplace</strong> - Turning the platform into a marketplace that can be occupied by a variety of internal, pattern, vendor, 3rd party, and public actors.</li>
  <li><strong>Monetization</strong> - Granular understanding of all the resources it takes to deliver each individual service, and understand the costs associated with operating at scale.</li>
  <li><strong>Plans</strong> - A wealth of API plans in place at the API gateway level, something that is tied to IAM policies, and in alignment with API governance expectations.</li>
  <li><strong>Governance</strong> - Providing a map, and supporting guidance around government platform API governance. Understanding, measuring, and enforcing consistency across the API lifecycle–platform wide.</li>
  <li><strong>Value Exchange</strong> - Using the cloud model, which is essentially the original API management, marketplace, and economy model. Not just measuring consumption, but used to maximize and generate revenue from the value exchanged across the platform.</li>
</ul>

<p>When you operate APIs on AWS and Azure, the platform as a service layer can utilize and benefit from the underlying infrastructure as a service monetization framework. Meaning, you can use AWS’s business model for managing the measuring, paying, and incentivizing of microservice operators. All the gears are there, they just need to be set in motion to support the management of a government API marketplace platform.</p>

<p>I have been studying Amazon full time for almost eight years. I’ve been watching Azure play catch up for the last three years. I run my infrastructure, and a handful of clients on AWS. I understand the API landscape of both providers, and how they can be woven into this vision for the business of government APIs. I see the AWS API stack, and the Azure API stack, as a starter set of services that can be built upon to deliver a government microservices platform. All the components are there. It just need the first set of services to be defined, delivering the essential building blocks any platform needs–things like compute, storage, dns, messaging, etc. The progress to other more outward, application, and system integration services.</p>

<p>My objective with this approach is to enable government services to be delivered as individual, self-contained units, that can be used as part of a larger orchestration of government services. Open up government and letting some sunlight in. Think about what Amazon has been able to achieve by delivering its own internal operations as services, and remaking not just retail, but also touching almost every other industry with Amazon Web Services. The Amazon Web Services myth story provides a powerful and compelling narrative for any company, organizations, institution, or government agency to emulate.</p>

<p>My proposal is not meant to be a utopian vision for how government works. However it is meant to shine a light on existing ways of delivering services via the cloud, with APIs at the center. Helping guide each service in its own individual journey, while also serving the overall mission of the platform–to help the veteran be successful in their own personal journey.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/28/the-business-of-running-government-as-a-microservices-platform/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/27/what-it-the-streamdata-io-api-gallery/">What Is The Streamdata.io API Gallery?</a></h3>
        <span class="post-date">27 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation.</p>

<p><strong>What is the Streamdata.io API Gallery?</strong><br />
It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas.</p>

<p><strong>What are we looking to accomplish with the Streamdata.io API Gallery?</strong><br />
Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible.</p>

<p><strong>What is the opportunity around the Streamdata.io API Gallery?</strong><br />
Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow.</p>

<p><strong>Why is this relevant to Streamdata.io, and their road map?</strong><br />
It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which potential customers might possibly find interesting, bringing them new API consumers, while also opening their eyes up to the event-driven opportunity that exists out there.</p>

<p><strong>Distilling Things Down A Bit More</strong><br />
Ok, that answers the general questions about what the Streamdata.io API Gallery is, and why we are building it. Now I want to distill down a little bit more to help me articulate the gallery as part of a series of presentations, existing as just a handful of bullet points. Helping get the point across in hopefully 60 seconds or less.</p>

<ul>
  <li>What is the Streamdata.io API Gallery?
    <ul>
      <li>API directory, for finding individual units of compute within specific topics.</li>
      <li>OpenAPI (fka Swagger) driven, making each unit of value usable at run-time.</li>
      <li>APIs.json indexed, making the collections of resources easy to search and use.</li>
      <li>Github hosted, making it forkable and continuously deployable and integrate(able).</li>
    </ul>
  </li>
  <li>Why is the Streamdata.io Gallery relevant?
    <ul>
      <li>It maps out the API universe with an emphasis on the value each individual API path possesses.</li>
      <li>Categories, tags, and indexes APIs into collections which are published to Github.</li>
      <li>Provides a human and machine friendly view of the existing publish and subscribe landscape.</li>
      <li>Begins to organize the API universe in context of a real time event-driven messaging world.</li>
    </ul>
  </li>
  <li>What is the opportunity around the Streamdata.io API Gallery?
    <ul>
      <li>Redefining the API landscape from an event-driven perspective.</li>
      <li>Quantify, qualify, and rank APIs to understand what is the most interesting and highest quality.</li>
      <li>Help API providers realize events occurring via their existing platforms.</li>
      <li>Begin moving beyond a request and response model to an event-driven reality.</li>
    </ul>
  </li>
</ul>

<p>There is definitely a lot more going on within the Streamdata.io API Gallery, but I think this captures the essence of what we are trying to achieve. A lot of what we’ve done is building upon my existing API Stack work, where I have worked to profile and index public APIs using OpenAPI and APIs.json, but this round of work is taking things to a new level. With API Stack I ended up with lists of companies and organizations, each possessing a list of APIs. The Streamdata.io API Gallery is a list of API resources, broken down by the unit of value they bring to the table, which is further defined by whether it is a GET, POST, or PUT–essentially a publish or subscribe opportunity.</p>

<p>Additionally, I am finally finding traction with the API rating system(s) I have been developing for the last five years. Profiling and measuring the companies behind the APIs I’m profiling, and making this knowledge available not just at discover time, but potentially at event and run time. Basically being able to understand the value of an event when it happens in real time, and be able to make programmatic decisions regarding whether we care about the particular event or not. Eventually, allowing us to subscribe only to the events that truly matter to us, and are of the highest value–then tuning out the rest. Delivering API ratings in an increasingly crowded and noisy event-driven API landscape.</p>

<p>We have the prototype for the Streamdata.io API Gallery ready to go. We are still adding APIs, and refining how they are tagged and organized. The rating system is very basic right now, but we will be lighting up different dimensions of the rating(s) algorithm, and hopefully delivering on different angles of how we quantify the value of events that occurring. I’m guessing we will be doing a soft launch in the next couple of weeks to little fanfare, and it will be something that builds, and evolves over time as the API index gets refined and used more heavily.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/27/what-it-the-streamdata-io-api-gallery/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting-session/">Three Areas I Would Like To Cover When We Sit Down For An API Consulting Session</a></h3>
        <span class="post-date">27 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_146_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m putting together some presentations for a handful of upcoming engagements, where I’m wanting to help my audience understand what an initial engagement will look like. While I am looking to have just a handful of bullets that can live on a single, or handful of slides, I also want a richer narrative to go along with it. To achieve this I rely on my blog, which helps me work my way through the details of what I do, and distill things down into something that I can deliver on the ground within the companies, organizations, institutions, and government agencies I am conducting business with.</p>

<p>When I am sitting down with a new audience, and working to help them understand how I can help them begin, jumpstart, revive, and move forward with their API journey, I’m usually breaking things into three main areas:</p>

<ul>
  <li><strong>Landscape Mapping</strong> - Establish a map of what currently is within an organization.
    <ul>
      <li><strong>Internal Resources</strong> - What existing web services, APIs, teams, and resources exist?</li>
      <li><strong>External Objectives</strong> - What are the external objectives of doing APIs?</li>
    </ul>
  </li>
  <li><strong>Strategy Development</strong> - Craft a coherent strategy for moving forward with APIs.
    <ul>
      <li><strong>API Lifecycle</strong> - Lay out a step by step list of stops along a modern API life cycle.</li>
      <li><strong>API Support</strong> - Identify how the strategy and operations will be supported within an organization.</li>
      <li><strong>API Evangelism</strong> - Consider how the message around API operations will spread internally, and externally.</li>
    </ul>
  </li>
  <li><strong>Execution</strong> - Identify a clear set of next steps regarding how APIs will evolve.
    <ul>
      <li><strong>Infrastructure</strong> - What services, tooling, and other API infrastructure is needed?</li>
      <li><strong>Resources</strong> - What resources have been identified for moving the API conversation forward?</li>
      <li><strong>Governance</strong> - What is the governance strategy for measuring, reporting upon, and enforcing the deliver of APIs across the API lifecycle presented.</li>
    </ul>
  </li>
</ul>

<p>When I present to a new group of people within an organization, this is the outline I am looking to flesh out. I have to understand what is already occurring (or not) on the ground, which is why I need the landscape map. Then, borrowing from my existing API research I can help develop a a detailed strategy, which includes the critical elements of how we will be supporting and evangelizing the effort–which without, API efforts will always struggle. After that, I want to quickly get to work on how we will be executing on this vision, even if it just involves more investment in the landscape map, and overall strategy.</p>

<p>I am working on more detailed materials to hand out prior to, and at the time I sit down with new clients, but I wanted to articulate in a single page, and using a simple set of bullets what I am looking to accomplish with any new consulting relationship. With a map in hand, and an strategy in mind, I’m confident that I can help folks I talk with move forward with their API journey in a more meaningful way. Something not everyone I talk with is confident in doing on their own, but with a little assistance, I’m pretty sure they will be able to get to work defining what the API journey will look like for their organization.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting-session/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/27/mapping-out-the-api-landscape/">Mapping Out The API Landscape</a></h3>
        <span class="post-date">27 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation.</p>

<p><strong>What is the Streamdata.io API Gallery?</strong>
It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas.</p>

<p><strong>What are we looking to accomplish with the Streamdata.io API Gallery?</strong>
Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible.</p>

<p><strong>What is the opportunity around the Streamdata.io API Gallery?</strong>
Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow.</p>

<p><strong>Why is this relevant to Streamdata.io, and their road map?</strong>
It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which potential customers might possibly find interesting, bringing them new API consumers, while also opening their eyes up to the event-driven opportunity that exists out there.</p>

<p><strong>Distilling Things Down A Bit More</strong>
Ok, that answers the general questions about what the Streamdata.io API Gallery is, and why we are building it. Now I want to distill down a little bit more to help me articulate the gallery as part of a series of presentations, existing as just a handful of bullet points. Helping get the point across in hopefully 60 seconds or less.</p>

<ul>
  <li>What is the Streamdata.io API Gallery?
    <ul>
      <li>API directory, for finding individual units of compute within specific topics.</li>
      <li>OpenAPI (fka Swagger) driven, making each unit of value usable at run-time.</li>
      <li>APIs.json indexed, making the collections of resources easy to search and use.</li>
      <li>Github hosted, making it forkable and continuously deployable and integrate(able).</li>
    </ul>
  </li>
  <li>Why is the Streamdata.io Gallery relevant?
    <ul>
      <li>It maps out the API universe with an emphasis on the value each individual API path possesses.</li>
      <li>Categories, tags, and indexes APIs into collections which are published to Github.</li>
      <li>Provides a human and machine friendly view of the existing publish and subscribe landscape.</li>
      <li>Begins to organize the API universe in context of a real time event-driven messaging world.</li>
    </ul>
  </li>
  <li>What is the opportunity around the Streamdata.io API Gallery?
    <ul>
      <li>Redefining the API landscape from an event-driven perspective.</li>
      <li>Quantify, qualify, and rank APIs to understand what is the most interesting and highest quality.</li>
      <li>Help API providers realize events occurring via their existing platforms.</li>
      <li>Begin moving beyond a request and response model to an event-driven reality.</li>
    </ul>
  </li>
</ul>

<p>There is definitely a lot more going on within the Streamdata.io API Gallery, but I think this captures the essence of what we are trying to achieve. A lot of what we’ve done is building upon my existing API Stack work, where I have worked to profile and index public APIs using OpenAPI and APIs.json, but this round of work is taking things to a new level. With API Stack I ended up with lists of companies and organizations, each possessing a list of APIs. The Streamdata.io API Gallery is a list of API resources, broken down by the unit of value they bring to the table, which is further defined by whether it is a GET, POST, or PUT–essentially a publish or subscribe opportunity.</p>

<p>Additionally, I am finally finding traction with the API rating system(s) I have been developing for the last five years. Profiling and measuring the companies behind the APIs I’m profiling, and making this knowledge available not just at discover time, but potentially at event and run time. Basically being able to understand the value of an event when it happens in real time, and be able to make programmatic decisions regarding whether we care about the particular event or not. Eventually, allowing us to subscribe only to the events that truly matter to us, and are of the highest value–then tuning out the rest. Delivering API ratings in an increasingly crowded and noisy event-driven API landscape.</p>

<p>We have the prototype for the Streamdata.io API Gallery ready to go. We are still adding APIs, and refining how they are tagged and organized. The rating system is very basic right now, but we will be lighting up different dimensions of the rating(s) algorithm, and hopefully delivering on different angles of how we quantify the value of events that occurring. I’m guessing we will be doing a soft launch in the next couple of weeks to little fanfare, and it will be something that builds, and evolves over time as the API index gets refined and used more heavily.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/27/mapping-out-the-api-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/">One Of The Best API Getting Started I Have Come Across</a></h3>
        <span class="post-date">26 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/starling/starling-home-page.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m working my way through banking and Fintech companies in the UK, and I stumbled across the Starling banking API. I began doing my usual clicking around as I do with any API, looking at the documentation, the getting started, and other primary links. After landing on <a href="https://developer.starlingbank.com/get-started">the Starling getting started page</a>, I have to say that it is the single best example of a getting started page I have ever come across in my time as API Evangelist. It is robust, informative, well laid out, and has everything you need to well, get started.</p>

<p>The Starling getting started page is broken up into six separate sections:</p>

<p>1) Register Your Application<br />
2) Setup Starter Kit<br />
3) Play in the Sandbox<br />
4) Personal Access<br />
5) Going Live<br />
6) Contact Us<br /></p>

<p>Each getting started section has a simple, concise description with relevant visuals and code samples, as well as possession simple action buttons, like sign, login, register application, and the other meaningful things you need to get started. The Starling getting started is going to become my go to example of how to create an API getting started page. You can really tell whoever put it together spent a lot of time refining it, and walking through it until it was 100% complete.</p>

<p>Starling even has a sandbox, marketplace, and a join Slack button. I can’t rave about their approach enough. I’m going to turn it into a case study regarding how to create a getting started page, and showcase on the home page of the site. I wish every API put as much energy into their getting started page as Starling has. It would take the friction out of on-boarding APis, and make it a much more pleasant experience.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/">The Banking API Actors In The UK</a></h3>
        <span class="post-date">26 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/shakespeare/shakespeare.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been profiling <a href="https://www.openbanking.org.uk/">the work of the Open Banking Implementation Entity when nit comes to banking API standards in the UK</a>. As part of my getting up to speed on the banking ecosystem in the UK, and Europe, I’ve been posting a series of small blog posts, outlining different aspects of how things work, and who the players are. While going through the Open Banking documentation, I came across a great list of the “actors” int he Open Banking API ecosystem, which taught me a lot about who is involved, and was worth reposting here as a list.</p>

<p>The Open Banking eco-system consists of a number of actors, which may be a natural person or an entity:</p>

<ul>
  <li><strong>Payment Service User (PSU)</strong> - Person - Payment Services User is a natural or legal person making use of a payment service as a payee, payer or both</li>
  <li><strong>Payment Service Provider (PSP)</strong> - Legal Entity - A legal entity (and some natural persons) that provide payment services as defined by PSD2 Article 4(11)</li>
  <li><strong>Account Servicing Payment Service Provider (ASPSP)</strong> - Legal Entity - Account Servicing Payment Service Providers provide and maintain a payment account for a payer as defined by the PSRs and, in the context of the Open Banking Ecosystem are entities that publish Read/Write APIs to permit, with customer consent, payments initiated by third party providers and/or make their customers’ account transaction data available to third party providers via their API end points.</li>
  <li><strong>Third Party Providers / Trusted Third Parties (TPP)</strong> - Legal Entity - Third Party Providers are organisations or natural persons that use APIs developed to Standards to access customer’s accounts, in order to provide account information services and/or to initiate payments. Third Party Providers are either/both Payment Initiation Service Providers (PISPs) and/or Account Information Service Providers (AISPs).</li>
  <li><strong>Payment Initiation Service Provider (PISP)</strong> - Legal Entity - A Payment Initiation Services Provider provides an online service to initiate a payment order at the request of the payment service user with respect to a payment account held at another payment service provider.</li>
  <li><strong>Account Information Service Provider (AISP)</strong> - Legal Entity - An Account Information Service provides account information services as an online service to provide consolidated information on one or more payment accounts held by a payment service user with one or more payment service provider(s).</li>
  <li><strong>TPP Primary Technical Contact (TPP-PTC)</strong> - Person - A Primary Technical Contact is an individual nominated by a TPP to have access to the Directory and will be able to nominate other Directory technical users. This should be a main point of contact on technical configuration and a senior member of staff with responsibility for the management of the Open Banking digital identity.</li>
  <li><strong>TPP Secondory Technical Contact (TPP-STC)</strong> - Person - A person that carries out technical operations on behalf of a TPP. A TPP-STC has the same permissions as a TPP-PTC except for the ability to nominate other Directory technical users.</li>
  <li><strong>ASPSP Primary Technical Contact (ASPSP-PTC)</strong> - Person - A Primary Technical Contact is an individual nominated by the ASPSP to have access to the Directory and will be able to nominate other Directory technical users. This should be a main point of contact on technical configuration and a senior member of staff with responsibility for the management of the Open Banking digital identity.</li>
  <li><strong>ASPSP Secondory Technical Contact (ASPSP-STC)</strong> - Person - A person that carries out technical operations on behalf of an ASPSP. An ASPSP-STC has the same permissions as a ASPSP-PTC except for the ability to nominate other Directory technical users.</li>
  <li><strong>Regulatory Bodies</strong> - Legal Entity - Government or industry bodies that have a regulatory role in the payments industry. This includes, but is not limited to, the UK Competition &amp; Markets Authority (CMA), HM Treasury (HMT), EBA, etc.	No</li>
  <li><strong>Member State Competent Authorities (MSCA)</strong> - Legal Entity - Regulatory Body	The regulatory body (or bodies) in each of the EU member states that is responsible for maintaining a register of payment institutions.</li>
  <li><strong>Financial Conduct Authority (FCA)</strong> - Legal Entity - The Financial Conduct Authority is the competent authority for the UK	No</li>
  <li><strong>Open Banking Limited (OB)</strong> - Legal Entity - The Open Banking Implementation Entity is the delivery organization working with the CMA9 and other stakeholders to define and develop the required APIs, security and messaging standards that underpin Open Banking.</li>
  <li><strong>OB Directory Administrator</strong> - Person - A person working for OB that is responsible for executing various technical processes related to the Directory on behalf of OB.</li>
</ul>

<p>I wish I could find breakdowns of the actors within every industry I work in like this. I’ve been studying up on how the banking industry works, but my knowledge moved forward quite a bit after studying this. It is the best explanation of the acronyms I’ve been coming across lately, and is the best breakdown of everyone involved in an API ecosystem I have ever come across in my eight years of studying the space.</p>

<p>Open Banking is doing some really interesting work on the banking API front. This is just one of many artifacts I’m coming across that demonstrate they really have their act together when it comes to developing banking API specifications in the UK.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">Round Two Of The Department of Veterans Affairs Lighthouse Platform RFI</a></h3>
        <span class="post-date">24 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/23_113_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m spending some more time thinking about APIs at the Department of Veterans Affairs (VA), in response to round two of their request for information (RFI). <a href="http://apievangelist.com/2017/10/26/my-response-on-the-department-of-veterans-affairs-rfi-for-the-lighthouse-api-management-platform/">A couple months back I had responded to an earlier RFI</a>, providing as much information as I could think of, for consideration as part of their API journey. As a former VA employee, and son of two Vietnam Vets (yes two), you can say I’m always willing to invest some in APIs over at the VA.</p>

<p>To provide a response, I have taken the main questions they asked, broken them out here, and provided answers to the best of my ability. In my style, the answers are just free form rants, based upon my knowledge of the VA, and the wider API space. It is up to the VA, to decide what is relevant to them, and should be included in their agency API strategy.</p>

<p><strong><em>2. Current Scope<br />
While the acquisition strategy for Lighthouse has not yet been formalized, VA envisions that the program will consist of multiple contracts.  For example, a contract for recommending policy and standards to form governance would likely be separate from an API build team.  The key high level activities below are anticipated to be included within these contracts, and VA is requesting feedback from industry on how these activities should be aligned between multiple contracts.  The list below is not inclusive of all tasks required to support this program.  Additionally, VA intends to provide the IAM solution and the provisioning of necessary cloud resources to host the proposed technology stack.  VA’s current enterprise cloud providers are Microsoft Azure and Amazon Web Services.</em></strong></p>

<p><strong>Microservice Focused Operational &amp; Implementation</strong><br />
Lighthouse should embrace a microservices way of doing things, so that the platform can avoid legacy trappings when it comes to delivering software at the VA, which have resulted in large, monolithic systems, possessing enormous budgets, and entrenched teams, that are able to develop a resistance to change and evolution. This microservices way of doing things should be adopted internally, as well as externally, then applied to the technology, business, and politics of delivering ALL Lighthouse infrastructure.</p>

<p>All contracts should be defined and executed in a modular way, with the only distinction between  projects being operational, or for specific project implementations. Everything should be delivered as microservices, no matter whether it is in support of operating the Lighthouse platform, or delivering services to Lighthouse-driven applications. The technology and business of each service should be self-contained, modular, and focusing on doing one thing, and doing it well. Ensuring all services executed as part of Lighthouse operations are decoupled, working independently, allowing for easily defining, delivering, managing, evolving, and deprecating of every operational and implementational service that makes Lighthouse work.</p>

<p>Operational services will be the first projects delivered via the platform, and will be used to establish and mature the Lighthouse project deliver workflow, but then going forward, every additional operational, as well as specific implementation focused service will utilize the same workflow and life cycle.</p>

<ul>
  <li><strong>Definitions</strong> - Everything begins as a set of definitions. Leveraging OpenAPI, JSON Schema, Dockerfiles, and other common definitions to provide a human, and machine readable definition of every project, which is ultimately delivered as a microservice.</li>
  <li><strong>Github</strong> - Each microservice begins as either a public or private Github repository, with a README index of the definition of what a service will deliver. Providing a self-contained, continuously deployed and integration blueprint of what a service does.</li>
  <li><strong>Architecture</strong> - Always providing a comprehensive outline all backend architecture used to support a specific service, including the technical, as well as the business, and security policy elements of what it takes to deliver the required service.</li>
  <li><strong>Tooling</strong> - Always providing a comprehensive outline of any tools used as part of delivering a service, to provide what is needed from a front-end delivery and execution vantage point.</li>
  <li><strong>Lifecycle</strong> - Establish a lifecycle, that each service will need to pass through, ensuring consistent delivery, and management of services that adhere to governance standards.
    <ul>
      <li><strong>define</strong> - What definitions are required for services?</li>
      <li><strong>design</strong> - What is the API design guidance involved?</li>
      <li><strong>mock</strong> - How are APIs and data virtualized as part of development?</li>
      <li><strong>portal</strong> - Which portals are service published to, or will possess?</li>
      <li><strong>document</strong> - What documentation is required and delivered?</li>
      <li><strong>test</strong> - Where are the code, as well as interface level tests?</li>
      <li><strong>clients</strong> - What client environment are in use for design, development, and testing?</li>
      <li><strong>**</strong>* - Pause there, and repeat until the desired service is realized…</li>
      <li><strong>deploy</strong> - How are services delivered as part of a containerized, continuous deployment pipeline?</li>
      <li><strong>dns</strong> - What DNS is needed to address and route traffic to services?</li>
      <li><strong>manage</strong> - What API management level services are in place to secure, log, limit, and report of API and service consumption?</li>
      <li><strong>logging</strong> - What is the logging stack, how is it shipped, analyzed, and reported upon?</li>
      <li><strong>monitor</strong> - What monitors are required and in place for each service?</li>
      <li><strong>performance</strong> - How is performance measured and reported upon?</li>
      <li><strong>sdk</strong> - What client libraries, SDKs, and samples in place for service integration?</li>
      <li><strong>depenencies</strong> - What internal service, and external API dependencies are in play?</li>
      <li><strong>licensing</strong> - What is the data, code, interface, and other licensing that apply?</li>
      <li><strong>privacy</strong> - Are privacy policies in place, and considered for the platform, partners, developers, and end-users.</li>
      <li><strong>terms</strong> - Are terms of service in place, and independently considered for each service?</li>
      <li><strong>monetization</strong> - What are the operating costs, and other monetization considerations?</li>
      <li><strong>plans</strong> - What API consumption plans, rate limits, and policies in place to govern service usage?</li>
      <li><strong>support</strong> - What support mechanisms are in place, with relevant point of contacts?</li>
      <li><strong>communication</strong> - What communication channels are in place, such as blogs, social, and messaging channels?</li>
      <li><strong>observability</strong> - What is the observability of each service, from open source to monitoring, and CI/CD workflows, ensuring it can be audited?</li>
      <li><strong>discovery</strong> - What is required to register, route, and discover an API as part of overall operations?</li>
      <li><strong>evangelism</strong> - What is the plan for making sure a service is known, used, and evangelized amongst target audience?</li>
    </ul>
  </li>
  <li><strong>Governance</strong> - How is each step along the life cycle measured, reported upon, and audited as part of governance, to understand how a service is meeting platform requirements, and evolving along a maturity path–allowing for innovation to occur, and newer ideas to flourish, but also allow more hardened, secure, and mature services to rise to the top.</li>
</ul>

<p>The OpenAPI, JSON Schema, and other definitions for each microservice will ultimately be the contract for each project. Of course, to deliver the first set of operational platform services (compute, storage, DNS, pipeline, logging, etc.) these independent contracts might need to be grouped into a single, initial contract. Something that will also occur around different groups of services being delivered at any point in the future, but each individual service should be self-contained, with its own contract definition existing in it’s Github repository core.</p>

<p><em><strong>Question: API Roadmap Development (Backlog, Future)</strong></em><br />
Each service being delivered via Lighthouse will possess its own self-contained road map as part of its definition. Providing a standardized, yet scalable way to address what is being planned, is being delivered, operated, and when anything will ultimately be deprecated.</p>

<ul>
  <li><strong>Github Issues</strong> - Each Github repository has it’s own issues for managing all conversations around the service road map. Tags and milestones can be used to designate the past, present, future, and other relevant segmentation of the road map.</li>
  <li><strong>Micro / Macro</strong> - Each services posses micro level detail about the road map, which is available via Github APIs, in a machine readable way for inclusion at the macro level, serving governance, reporting, and higher level road map considerations.</li>
  <li><strong>Communication</strong> - Each service owner is responsible for road map related communication, support, and management providing their piece of the overall road map puzzle.</li>
</ul>

<p>The Lighthouse platform road map should work like an orchestra, with each participant bringing their contribution, but platform operators and conductors defining the overall direction the platforms is headed. At scale, Lighthouse will be thousands of smaller units, organized by hundreds of service owners and stewards, serving millions of end-users, with feedback loops in service through the stack.</p>

<p><em><strong>Question: Outreach (Internal &amp; External Parties)</strong></em><br />
Outreach is essential to the viability of any platform, and represents the business and political challenges that lie ahead for the VA, or any government agency looking to work seamlessly with public and private sector partners, as well as the public at large. There will be many services involved with Lighthouse operations that will need to be private, but the default should always be public, allowing for as much transparency and observability as possible, which will feed platform outreach in a positive way.</p>

<ul>
  <li><strong>Github Project Pages</strong> - Each Github repository can have a public facing Github Pages static site portal and landing page. Allowing for individual service, or group portals to exist, providing a destination for all stakeholders to get involved.</li>
  <li><strong>Github Social Framework</strong> - Github provides a wealth of outreach and communication solutions from organization and repository search, to issues and wikis, and tagging services with individual topics. All of which can be used as part of outreach and engagement in a private or public setting.</li>
  <li><strong>Twitter</strong> - Microblogging provides a great way to publish regular updates, and provide communication around platform operations.</li>
  <li><strong>Linkedin</strong> - Enterprise development groups, especially those in service of the government tend to use Github for establishing their profile, and maintaining their presence, which can be incorporated into all outreach efforts.</li>
  <li><strong>Blogs</strong> - The platform should possess its own public and / or private blogs, as well as potentially more topically, service, or project based blogs that expand outreach to the long tail of platform operations.</li>
</ul>

<p>This type of outreach around platform operations is something that scares the hell out of government folks, and the majority of government APIs operation are critically deficient in the area of outreach. This has to change. If there is no feedback loop in place, and outreach doesn’t occur regularly and consistently, the platform will not succeed. This is how the API world operates.</p>

<p><em><strong>Question: Management of API Request Process (Internal (VA)/External (Non-VA))</strong></em><br />
New services will always be needed. Operational and implementation related requests should all be treated the same. Obviously there will be different prioritization mechanisms in place, but API requests should just be the birth of any new service, allowing it to begin its journey, and transit through the API lifecycle described above. Not all requests will reach deployment, and not all deployments will reach maturity, but all API requests should be treated equally.</p>

<ul>
  <li><strong>Definitions</strong> - Each API request begins with a definition. A simple description of what a service will do.</li>
  <li><strong>Github</strong> - Each API request begins its journey as a Github repository, with a README containing its basic definition, and conversation around its viability within Github issues.</li>
  <li><strong>JSON Schema</strong> - As part of each request, all data that will be leverage as part of service operations should be defined as JSON Schema, and included in the Github repository.</li>
  <li><strong>OpenAPI</strong> - Additionally, the access to the service, and its underlying data and resource should be defined using a machine readable OpenaPI definition, outlining the contract of the service.</li>
  <li><strong>Certification</strong> - Some stakeholders will have submitted API requests before, and better understand the process, and be certified owners of existing services, working as part of trusted organizations, expediting and prioritizing the request process.</li>
  <li><strong>Template(s)</strong> - The most common service patterns to emerge should be defined as template, providing seeds and starter projects to help expedite and streamline the API request process, ensuring all the moving parts are there to make a decision, in a forkable, replicable package.</li>
</ul>

<p>New API requests should be encourage. Anyone should be able to submit a new service, replicate, or augment an existing service, or respond to a platform API RFP. The life cycle described above should be open to everyone looking to submit an API request. Allowing them to define, design, mock, and iterate their submission. Even providing a nearly usable representation of a service, even before the idea or service is accepted. Forcing everyone to flesh out their service, deliver a viable proof of concept, that will streamline the API acceptance process.</p>

<p><em><strong>Question: Propose, Implement and Manage the PaaS (technology stack)</strong></em><br />
As mentioned before, this aspect of Lighthouse should be delivered as microservices, alongside every other service being delivered via the platform. It just so happens that this portion of the stack will be the first to be delivered, and be iterated upon, evolved, and deprecated just like any other service. To put this in perspective, I will outline the AWS, and Azure infrastructure need to support management of the platform later on in this post, while considering the fact that AWS and Azure have been on the same journey that the VA is on with Lighthouse, something that has been playing out for the last decade.</p>

<p>The VA wants to be the Amazon of serving veterans. They want internal groups, vendors, contractors, veteran health and service organizations, and independent developers to come build their solutions on the Lighthouse platform. The VA should uses its own services for internal service delivery, as well as supporting external projects. The operational side of Lighthouse platform should be all microservice projects, with the underlying infrastructure being Azure or AWS solutions, providing a common platform as a service stack that can be leveraged, no matter where the actual service is deployed.</p>

<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>

<p><em><strong>Question: DevOps Continuous Integration and Continuous Delivery (CI/CD) of APIs</strong></em><br />
Every service in support of operations or implementations via the Lighthouse platform will exist as a self-contained Github repository, with all the artifacts needed to be included in any application pipeline. The basic DNA blueprint for each service should be crafted to support any single CI/CD service, or ideally even multiple types of CI/CD and orchestration solutions like AWS and Azure both support.</p>

<ul>
  <li><strong>Microservices</strong> - Lighthouse CI/CD will be all about microservice orchestration, and using a variety of pipelines to deliver and evolve initially hundreds, and eventually thousands of services in concert.</li>
  <li><strong>Github</strong> - Github will the cellular component driving the Lighthouse CI/CD workflow, providing individual service “legos” that can be composed, assembled, disassembled, and delivered in any way.</li>
  <li><strong>Definitions</strong> - Each microservice will contain all the artifacts needed for supporting the entire life cycle listed above, driven by a variety of CI/CD pipelines. Leveraging dockerfiles, build packages, OpenAPI definitions, schema, and other definitions to continuously deliver and integrate across platform operations.</li>
</ul>

<p>Both AWS and Azure provide CI/CD workflows, which can be used to satisfy the portion of the RFI. I will list out all the AWS and Azure services I think should be considered below. Additionally, Jenkins, CircleCI, or other 3rd party CI/CD could easily be brought in to deliver on this aspect of platform delivery. The microservices core can be used as part of any pipeline delivery model.</p>

<p><em><strong>Question: Environment Operations &amp; Maintenance (O&amp;M)</strong></em><br />
Again, everything operates as microservices, and gets delivered independently as services that can be configured and maintained as part of overall platform operations and maintenance, or in service of individual services, and groups of services supporting specific implementations.</p>

<ul>
  <li><strong>Microservices</strong> - Everything is available as microservices, allowing the underlying environment operations and maintainenace to be orchestration, and optimized in real time.</li>
</ul>

<p>Each of the AWS and Azure services listed below are APIs. They allow for the configuration and management of each service via API or CLI, allowing the architecture to be seamlessly managed as part of the overall API stack, as well as the CI/CD pipeline. Making environment operations and maintenance, just part of the continuous delivery cycle(s).</p>

<p><em><strong>Question: Release Management</strong></em><br />
Release occurs at the granular service level. With Github and CI/CD as the vehicle for moving release forward daily, versioning, defining, and communicating all the way. With the proper code and API level testing in place, release management can happen safely at scale.</p>

<ul>
  <li><strong>Github</strong> - Github version control, branches, and release management should  be used as part of the overall release management strategy.</li>
  <li><strong>Versioning</strong> - Establishment of a service versioning strategy for minor and major code, and interface releases, allowing independent release management that can occur at the higher orchestration level</li>
  <li><strong>CI/CD Pipelines</strong> - Everything should be a pipeline, broken down by logical operational, organization, and project boundaries, operating on a continuous release cycle.</li>
  <li><strong>Microservices</strong> - Everything is operated independently, and released independently via containers, with approach dependency management as part of each release.</li>
  <li><strong>Definitions</strong> - OpenAPI and JSON Schema are versioned and use to act as the contract for each release.</li>
  <li><strong>Communications</strong> - Along with each release, comes a standard approach to notification, communication, and support.</li>
</ul>

<p>Release management will horizontally take a significant amount of time to wrap your head around. Moving forward hundreds, and thousands of services in concert won’t be easy. However it will be more resilient, and forgiving than moving forward a single monolith.</p>

<p><em><strong>Question: API Analytics</strong></em><br />
Awareness should be baked in by default to the Lighthouse platform, measuring everything, and reporting on it consistently, providing observability across all aspects of operations in alignment with security policies. Analysis should be its own set of operational services, that span the entire length of the Lighthouse platform.</p>

<ul>
  <li><strong>Log Shipping</strong> - The database, container, web server, management, and DNS logs for ALL services should be shipped, and centralized, for complete access and analysis.</li>
  <li><strong>APIs</strong> - Centralized logs should be its own service, with programmatic access to logs for all platform services.</li>
  <li><strong>Modular</strong> - Analytics should be modular, bit-size API-driven elements that can be mixed, composed, published, and visualized in reusable ways.</li>
  <li><strong>Embeddables</strong> - Modular, embeddable UI elements should be developed as applications on top of platform analytics APIs, allowing for portable dashboard that can be remixed, reused, and evolved.</li>
  <li><strong>Search</strong> - The logging and reporting layer of the platform should have a core search element, allowing all logs to searched, as well as the logs for how API consumers are analyzing logs (mind blown).</li>
  <li><strong>Continouous</strong> - As with all other services, analytics, reporting, and visualizations should be continuous, and ever evolving and deployed on a day to day, week to week basis.</li>
</ul>

<p>A standard logging strategy across all services is how we achieve a higher level of API analytics, going beyond just database or web server statics, and even API management analytics, providing end to end, comprehensive platform service measurement, analysis, reporting, and visualization. Allowing platform operators, consumers, and auditors to access and understand how all service are being used, or not being used.</p>

<p><em><strong>Question: Approval to Operate (ATO) Support for Environments</strong></em><br />
Every service introduced as part of the Lighthouse platform should have all the information required to support ATO, with it baked into the governance and maturity life cycle for any service. It actually lends itself well to the maturity elements of the lifecycle above, ensuring there is ATO before anything is deployed.</p>

<ul>
  <li><strong>Definitions</strong> - All definitions are present for satisfying ATO.</li>
  <li><strong>Github</strong> - Everything is self-contained within a single place for submission.</li>
  <li><strong>Governance</strong> - ATO is part of the governance process, while still allowing for innovation.</li>
  <li><strong>Micro / Macro</strong> - ATO for each individual service can be considered, as well as at the project, group levels, understanding where services fit in at macro level.</li>
</ul>

<p>ATO can be built into the templated API request and submission process discussed earlier, allowing for already approved architecture, tooling, and patterns to be used over and over, streamlining the ATO cycle. Helping service developers enjoy more certainty around the ATO process, while still allowing for innovation to occur, pushing the ATO definition and process when it makes sense.</p>

<p><em><strong>Question: Build APIs including system level APIs that connect into backend VA systems</strong></em><br />
Everything is a microservice, and there are plenty of approaches to ensure that legacy backend systems can enjoy continued use and evolution through evolved APIs. The API life cycle allows for the evolution of existing backend systems that operate in the cloud and on-premise in small, bit-size service implementations.</p>

<ul>
  <li><strong>Gateway</strong> - AWS API Gateway and Azure API management makes it easy to publish newer APIs on top of legacy backend systems.</li>
  <li><strong>Facades</strong> - Establishing facade patterns for modernizing, and evolving legacy systems, allowing them to take on a new interface, while still maintaining existing system.</li>
  <li><strong>OpenAPI</strong> - Map out newer APIs using OpenAPI, then importing into gateways and wiring up with backend systems.</li>
  <li><strong>Schema</strong>- Mapping out the schema transformations from backend systems to front-end API requests and responses using JSON Path, and JSON Schema.</li>
  <li><strong>Microservices</strong> - Delivering newer APIs on top of legacy systems in smaller, more evolvable services.</li>
</ul>

<p>From the frontend, you shouldn’t be able to tell whether a legacy VA system is in use, or newer cloud infrastructure. All applications should be using APIs, and all APIs should be delivered as individual or groups of microservices, that do one thing and does it well. As APIs evolve, the backend systems should be decoupled and evolved as well, but until that becomes possible, all consumption of data, content, and other resources will be routed through the Lighthouse API stack.</p>

<p><em><strong>Question: API key management or managing third party access (authorization, throttling, etc.)</strong></em><br />
Both theAWS API Gateway, and Azure API Management allow for the delivery of modern API management infrastructure that can be used to govern internal, partner, and 3rd party access to resources. All applications should be using APIs, and ALL APIs should be using a standardized API management approach, no matter whether the consumption is internal or external. Ensuring consistent authorization, throttling, logging, and other aspects of doing business with APIs.</p>

<ul>
  <li><strong>IAM</strong> - Leverage API keys, JWT, and OAuth in conjunction with IAM policies governing which backend resources are available to API consumers.</li>
  <li><strong>Gateway</strong> - All API traffic is routed through the AWS API Gateway and Azure API management layers, allowing for consistent and comprehensive management across all API consumption.</li>
  <li><strong>Management</strong> - Apply consistent logging, rate limiting, transformations, error and security at the API management level, ensuring all services behave in the same way.</li>
  <li><strong>Plans</strong> - Establishing of a variety of API plans that dictate API levels of access, which services are accessible to different API key levels, that are in sync with backend IAM policies.</li>
  <li><strong>Logging</strong> - Every API call is logged, and contains user and application keys, allow ALL API consumption to be audited and reported upon, and responded to in real time.</li>
  <li><strong>Security</strong> - Providing a single point of entry, and the ability to shut down access, striking the balance between access and security which is the hallmark of doing APIs.</li>
</ul>

<p>API management is baked into the cloud. It is a discipline that has been evolving for over a decade, and is available on both the AWS and Azure platforms. The tools are there, Lighthouse just needs to establish a coherent strategy for authentication, service composition, logging, reporting, and responding to API consumption at scale in real time. Staying out of the way of consumers, while also ensuring that they only have access to the data, content, and other resources they are allowed to, in alignment with overall governance.</p>

<p><em><strong>Management of API lifecycle in cloud, hybrid, and/or on premise environments</strong></em><br />
All operational aspects of the Lighthouse platform should be developed as independent microservices, with a common API–no matter what the underlying architecture is. The DNS service API should be the same, regardless of whether it is managing AWS or Azure DNS, or possibly any other on-premise or 3rd party service–allowing for platform orchestration using a common API stack.</p>

<ul>
  <li><strong>Microservices</strong> - Each operational service is a microservice, with possibly multiple versions, depending on the backend architecture in use.</li>
  <li><strong>Containers</strong> - Every operational service is operated as a container, allowing it to run in any cloud environment.</li>
  <li><strong>Github</strong> - All services live as a Github repository, allowing it to be checked out and forked via any cloud platform.</li>
</ul>

<p>The modular, containerized, microservice approach to delivering the Lighthouse platform will allow for the deployment, scaling, and redundant implementation of services in any cloud environment, as well as on-premise, or hybrid scenarios. All services operate using the same microservice footprint, using containers, and a consistent API surface area, allowing for the entire platform stack to be orchestrated against no matter where the actual service resides.</p>

<p>_**Question: 3. Use Case<br />
To better provide insight into aligning activities to contracts, VA has provided the use case below. Please walk through this use case discussing each activity and the contract it would be executed under.</p>

<p><strong>Veteran Verification Sample Use Case: VA has a need for a Veteran Verification API to verify a Veteran status from a number of VA backend systems to be shared internally and externally as an authoritative data source.  These backend systems potentially have conflicting data, various system owners, and varying degrees of system uptime._</strong></p>

<p>This is a common problem within large organizations, institutions, and government agencies. This is why we work to decouple, modularize, and scale not just the technology of building applications on backend systems, but also the business, and politics of it all. Introducing a competitive element when it comes to data management access, and building in redundancy, resilience, and a healthier incentive model into how we provide access to critical data, content, and other resources.</p>

<p>I have personal experience with this particular use case. One of the things I did while working at the VA, was conduct public data inventory, and move forward the conversation around a set of veteran benefit web services, which included asking the question–who had the authoritative record for a veteran? Many groups felt they were the authority, but in my experience, nobody actually did entirely. The incentives in this environment weren’t about actually delivering a meaningful record on a veteran, it was all about getting a significant portion of the budget. I recommend decoupling the technology, business, and politics of providing access to veterans data using a microservices approach.</p>

<ul>
  <li><strong>Microservices</strong> - Break the veterans record into separate, meaningful services.</li>
  <li><strong>Definitions</strong> - Ensure the definitions for the schema and API are open and accessible.</li>
  <li><strong>Discovery</strong> - Make sure that the Veteran Verification API is full discoverable.</li>
  <li><strong>Testing</strong> - Make sure the Verification API is fully tested on a regular basis.</li>
  <li><strong>Monitoring</strong> - Ensure that there are regular monitors for the Verification API.</li>
  <li><strong>Redundancy</strong> - Encourage multiple implementations of the same API to be delivered and owned by separate groups in separate regions, with circuit breaker behavior in APIs and applications.</li>
  <li><strong>Balancing</strong> - Load balance between services and regions, allowing for auto-scaled APIs.</li>
  <li><strong>Aggregation</strong> - Encourage the development of aggregate APIs that bridge multiple source, providing aggregate versions of the veteran’s record, encouraging new service owners to improve on existing services.</li>
  <li><strong>Reliability</strong> - Incentivize and reward reliability with Verification API owners, through revenue and priority access.</li>
</ul>

<p>There should be no single owner of any critical VA service. Each service should have redundant versions of the service, available in different regions, and managed by separate owners. Competition should be encouraged, with facade and aggregate introduced, putting pressure on core service providers to deliver quality, or their service(s) will be de-prioritized, and newer services will be given traffic and revenue priority. The same backend database can be exposed via many different APIs, with a variety of owners and incentives in place to encourage the quality of service.</p>

<p>APIs, coupled with the proper terms of service in place can eliminate an environment where defensive data positions are established. If other API owners can get access to the same data, and offer a better quality API, then evangelize and gain traction with application owners, entrenched API providers will no longer flourish. Aggregate and facade APIs allow for the evolution of existing APIs, even if the API owners are unwilling to move and evolve. Shifting the definition of what is authoritative, making it much more liquid, allowing it to shift and evolve, rather than just be diluted and meaningless, as it is often seen in the current environment.</p>

<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/69_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>

<p>_<strong>Question: 4. Response<br />
In addition to providing the requested content above, VA asks for vendors to respond to the following questions:</strong></p>

<p><strong>Describe how you would align the aforementioned activities between contracts, and the recommended price structure for contracts?_</strong></p>

<p>Each microservice would have its own technical, business, and political contract, outline how the service will be delivered, managed, supported, communicated, and versioned. These contracts can be realized individually, or grouped together as a larger, aggregate contract that can be submitted, while still allowing each individual service within that contract to operate independently.</p>

<p>As mentioned before, the microservices approach isn’t just about the technical components. It is about making the business of delivering vital VA services more modular, portable, and scalable. Something that will also decouple and shift the politics of delivering critical services to veterans. Breaking things down into much more manageable chunks that can move forward independently at the contract level.</p>

<ul>
  <li><strong>Micro Procurement</strong> - One of the benefits of breaking down services into small chunks, is that the money needed to deliver the service can become much smaller, potentially allowing for a much smaller, more liquid and flowing procurement cycle. Each service has a micro definition of the monetization involved with the service, which can be aggregated by groups of services and projects.</li>
  <li><strong>Micro Payments</strong> - Payments for service deliver can be baked into the operations and life cycle of the service. API management excels at measuring how much a service is accessed, and testing, monitoring, logging, security, and other stops along the API life cycle can all be measured, and payments can be delivered depend on quality of service, as well as volume of service.</li>
</ul>

<p>Amazon Web Services already has the model for defining, measuring, and billing for API consumption in this way. This is the bread and butter of the Amazon Web Services platform, and the cornerstone of what we know as the cloud. This approach to delivering, scaling, and ultimately billing or payment for the operation and consumption of resources, just needs to be realized by the VA, and the rest of the federal government. We have seen a shift in how government views the delivery and operation of technical resources using the cloud over the last five years, we just need to see the same shift for the business of APIs over the next five years.</p>

<p>_<strong>Question: The Government envisions a managed service (ie: vendor responsible for all aspects including licenses, scaling, provisioning users, etc.) model for the entire technology stack.  How could this be priced to allow for scaling as more APIs are used?  For example, would it be priced by users, API calls, etc.?_</strong></p>

<p>API management is where you start this conversation. It has been used for a decade to measure, limit, and quantify the value being exchanged at the API level. Now that API management has been baked into the cloud, we are starting to see the approach being scaled to deliver at a marketplace level. With over ten years of experience with delivering, quantifying, metering and billing at the API level, Amazon is the best example of this monetization approach in action, with two distinct ways of quantifying the business of APIs.</p>

<ul>
  <li><strong>AWS Marketplace Metering Service</strong> - SaaS style billing model which provides a consumption monetization model in which customers are charged only for the number of resources they use–the best known cloud model.</li>
  <li><strong>AWS Contract Service</strong> - Billing customers in advance for the use of software, providing an entitlement monetization model in which customers pay in advance for a certain amount of usage, which could be used to deliver certain amount of storage per month for a year, or a certain amount of end-user licenses for some amount of time.</li>
</ul>

<p>This provides a framework for thinking about how the business of microservices can be delivered. Within these buckets, AWS provides a handful of common dimensions for thinking through the nuts and bolts of these approaches, quantifying how APIs can be monetized, in nine distinct areas:</p>

<ul>
  <li><strong>Users</strong> – One AWS customer can represent an organization with many internal users. Your SaaS application can meter for the number of users signed in or provisioned at a given hour. This category is appropriate for software in which a customer’s users connect to the software directly (for example, with customer-relationship management or business intelligence reporting).</li>
  <li><strong>Hosts</strong> – Any server, node, instance, endpoint, or other part of a computing system. This category is appropriate for software that monitors or scans many customer-owned instances (for example, with performance or security monitoring). Your application can meter for the number of hosts scanned or provisioned in a given hour.</li>
  <li><strong>Data</strong> – Storage or information, measured in MB, GB, or TB. This category is appropriate for software that manages stored data or processes data in batches. Your application can meter for the amount of data processed in a given hour or how much data is stored in a given hour.</li>
  <li><strong>Bandwidth</strong> – Your application can bill customers for an allocation of bandwidth that your application provides, measured in Mbps or Gbps. This category is appropriate for content distribution or network interfaces. Your application can meter for the amount of bandwidth provisioned for a given hour or the highest amount of bandwidth consumed in a given hour.</li>
  <li><strong>Request</strong> – Your application can bill customers for the number of requests they make. This category is appropriate for query-based or API-based solutions. Your application can meter for the number of requests made in a given hour.</li>
  <li><strong>Tiers</strong> – Your application can bill customers for a bundle of features or for providing a suite of dimensions below a certain threshold. This is sometimes referred to as a feature pack. For example, you can bundle multiple features into a single tier of service, such as up to 30 days of data retention, 100 GB of storage, and 50 users. Any usage below this threshold is assigned a lower price as the standard tier. Any usage above this threshold is charged a higher price as the professional tier. Tier is always represented as an amount of time within the tier. This category is appropriate for products with multiple dimensions or support components. Your application should meter for the current quantity of usage in the given tier. This could be a single metering record (1) for the currently selected tier or feature pack.</li>
  <li><strong>Units</strong> – Whereas each of the above is designed to be specific, the dimension of Unit is intended to be generic to permit greater flexibility in how you price your software. For example, an IoT product which integrates with device sensors can interpret dimension “Units” as “sensors”. Your application can also use units to make multiple dimensions available in a single product. For example, you could price by data and by hosts using Units as your dimension. With dimensions, any software product priced through the use of the Metering Service must specify either a single dimension or define up to eight dimensions, each with their own price.</li>
</ul>

<p>These dimensions reflect the majority of API services being sold out there today, we don’t find ourselves in a rut with measuring value, like just paying per API call. Allowing Lighthouse API plans to possess one or more dimensions, beyond any single use case.</p>

<ul>
  <li><strong>Single Dimension</strong> - This is the simplest pricing option. Customers pay a single price per resource unit per hour, regardless of size or volume (for example, $0.014 per user per hour, or $0.070 per host per hour).</li>
  <li><strong>Multiple Dimensions</strong> – Use this pricing option for resources that vary by size or capacity. For example, for host monitoring, a different price could be set depending on the size of the host. Or, for user-based pricing, a different price could be set based on the type of user (admin, power user, and read-only user). Your service can be priced on up to eight dimensions. If you are using tier-based pricing, you should use one dimension for each tier.</li>
</ul>

<p>This provides a framework that Lighthouse can provide to 3rd party developers, allowing them to operate their services within a variety of business models. Derived from many of the hard costs they face, and providing additional volume based revenue, based upon how may API calls of any particular service receives.</p>

<p>Beyond this basic monetization framework, I’d add in an incentive framework that would dovetail with the business models proposed, but then provide different pricing levels depending on how well the services perform, and deliver on the agreed upon API contract. There are a handful of bullets I’d consider here.</p>

<ul>
  <li><strong>Design</strong> - How well does a service meet API design guidelines set forth in governance guidance.</li>
  <li><strong>Monitoring</strong> - Has a service consistently met its monitoring goals, delivering against an agreed upon service level agreement (SLA).</li>
  <li><strong>Testing</strong> - Beyond monitoring, are APIs meeting granular interface testing, along a regular testing &amp; monitoring schedule.</li>
  <li><strong>Communication</strong> - Are service owners meeting expectations around communication around a service operations.</li>
  <li><strong>Support</strong> - Does a service meet required support metrics, making sure it is responsive and helpful.</li>
  <li><strong>Ratings</strong> - Provide a basic set of metrics, with accompanying ratings for each service.</li>
  <li><strong>Certification</strong> - Allowing service providers to get certified, receiving better access, revenue, and priority.</li>
</ul>

<p>All of the incentive framework is defined and enforced via the API governance strategy for the platform. Making sure all microservices, and their owners meet a base set of expectations. When you take the results and apply weekly, monthly, and quarterly against the business framework, you can quickly begin to see some different pricing levels, and revenue opportunities around all microservices emerge. You deliver consistent, reliable, highly ranked microservices, you get paid higher percentages, enjoy greater access to resources, and prioritization in different ways via the platform–if you don’t, you get paid less, and operate fewer services.</p>

<p>This model is already visible on the AWS platform. All the pieces are there to make it happen for any platform, operating on top of the AWS platform. The marketplace, billing, and AWS API Gateway connection to API plans exists. When you combine the authentication and service composition available at the AWS API Gateway layer, with the IAM policy solutions available via AWS, an enterprise grade solution for delivering this model securely at scale, comes into focus.</p>

<p><em><strong>Question: Is there a method of paying or incentivizing the contractor based on API usage?</strong></em><br />
I think I hit on this with the above answer(s). Keep payments small, and well defined. Measured, reported upon, and priced using the cloud model, connecting to a clear set of API governance guidance and expectations. The following areas can support paying and incentivizing contractors based upon not just usage, but also meeting the API contract.</p>

<ul>
  <li><strong>Management</strong> - API management puts all microservices into plans, then log, meter, and track on value exchanged at this level.</li>
  <li><strong>Marketplace</strong> - Turning the platform into a marketplace that can be occupied by a variety of internal, pattern, vendor, 3rd party, and public actors.</li>
  <li><strong>Monetization</strong> - Granular understanding of all the resources it takes to deliver each individual service, and understand the costs associated with operating at scale.</li>
  <li><strong>Plans</strong> - A wealth of API plans in place at the API gateway level, something that is tied to IAM policies, and in alignment with API governance expectations.</li>
  <li><strong>Governance</strong> - Providing a map, and supporting guidance around the Lighthouse platform API governance. Understanding, measuring, and enforcing consistency across the API lifecycle–platform wide.</li>
  <li><strong>Value Exchange</strong> - Using the cloud model, which is essentially the original API management, marketplace, and economy model. Not just measuring consumption, but used to maximize and generate revenue from the value exchanged across the platform.</li>
</ul>

<p>When you operate APIs on AWS and Azure, the platform as a service layer can utilize and benefit from the underlying infrastructure as a service monetization framework. Meaning, you can use AWS’s business model for managing the measuring, paying, and incentivizing of microservice owners. All the gears are there, they just need to be set in motion to support the management of a government API marketplace platform.</p>

<p><em><strong>Based on the information provided, please discuss your possible technology stack and detail your experience supporting these technologies.</strong></em><br />
Both Amazon Web Services and Azure provide the building blocks of what you need to execute the above. Each cloud platform has its own approach to delivering infrastructure at scale. Providing an interesting mix of API driven resources you can jumpstart any project.</p>
<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/27_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><strong>AWS</strong>
First, let’s take a look at what is relevant to this vision from the Amazon Web Services side of things. These are all the core AWS solutions on the table, with dashboard, API, and command line access to get the job done.</p>

<p><strong>Compute</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/ec2/?hp=tile&amp;so-exp=below">Amazon EC2</a> - Virtual Servers in the Cloud</li>
  <li><a href="https://aws.amazon.com/ec2/autoscaling/?hp=tile&amp;so-exp=below">Amazon EC2 Auto Scaling</a> - Scale Compute Capacity to Meet Demand</li>
  <li><a href="https://aws.amazon.com/ecs/?hp=tile&amp;so-exp=below">Amazon Elastic Container Service</a> - Run and Manage Docker Containers</li>
  <li><a href="https://aws.amazon.com/eks/?hp=tile&amp;so-exp=below">Amazon Elastic Container Service for Kubernetes</a> - Run Managed Kubernetes on AWS</li>
  <li><a href="https://aws.amazon.com/ecr/?hp=tile&amp;so-exp=below">Amazon Elastic Container Registry</a> - Store and Retrieve Docker Images</li>
  <li><a href="https://aws.amazon.com/lightsail/?hp=tile&amp;so-exp=below">Amazon Lightsail</a> - Launch and Manage Virtual Private Servers</li>
  <li><a href="https://aws.amazon.com/fargate/?hp=tile&amp;so-exp=below">AWS Fargate</a> - Run Containers without Managing Servers or Clusters</li>
  <li><a href="https://aws.amazon.com/batch/?hp=tile&amp;so-exp=below">AWS Batch</a> - Run Batch Jobs at Any Scale</li>
  <li><a href="https://aws.amazon.com/lambda/?hp=tile&amp;so-exp=below">AWS Lambda</a> - Run your Code in Response to Events</li>
  <li><a href="https://aws.amazon.com/serverlessrepo/?hp=tile&amp;so-exp=below">AWS Serverless Application Repository</a> - Discover, Deploy, and Publish Serverless Applications Auto Scaling</li>
</ul>

<p><strong>Storage</strong>
<a href="https://aws.amazon.com/s3/?hp=tile&amp;so-exp=below">Amazon S3</a> - Scalable Storage in the Cloud
<a href="https://aws.amazon.com/ebs/?hp=tile&amp;so-exp=below">Amazon EBS</a> - Block Storage for EC2
<a href="https://aws.amazon.com/efs/?hp=tile&amp;so-exp=below">Amazon Elastic File System</a> - Managed File Storage for EC2
<a href="https://aws.amazon.com/glacier1/?hp=tile&amp;so-exp=below">Amazon Glacier</a> - Low-cost Archive Storage in the Cloud
<a href="https://aws.amazon.com/storagegateway/?hp=tile&amp;so-exp=below">AWS Storage Gateway</a> - Hybrid Storage Integration</p>

<p><strong>Database</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/rds/aurora/?hp=tile&amp;so-exp=below">Amazon Aurora</a> - High Performance Managed Relational Database</li>
  <li><a href="https://aws.amazon.com/rds/?hp=tile&amp;so-exp=below">Amazon RDS</a> - Managed Relational Database Service for MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB</li>
  <li><a href="https://aws.amazon.com/dynamodb/?hp=tile&amp;so-exp=below">Amazon DynamoDB</a> - Managed NoSQL Database</li>
  <li><a href="https://aws.amazon.com/elasticache/?hp=tile&amp;so-exp=below">Amazon ElastiCache</a> - In-memory Caching System</li>
  <li><a href="https://aws.amazon.com/redshift/?hp=tile&amp;so-exp=below">Amazon Redshift</a> - Fast, Simple, Cost-effective Data Warehousing</li>
</ul>

<p><strong>Authentication</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/iam/?hp=tile&amp;so-exp=below">AWS Identity &amp; Access Management</a> - Manage User Access and Encryption Keys</li>
  <li><a href="https://aws.amazon.com/cognito/?hp=tile&amp;so-exp=below">Amazon Cognito</a> - Identity Management for your Apps</li>
  <li><a href="https://aws.amazon.com/single-sign-on/?hp=tile&amp;so-exp=below">AWS Single Sign-On</a> - Cloud Single Sign-On (SSO) Service</li>
  <li>a href=”https://aws.amazon.com/cloudhsm/?hp=tile&amp;so-exp=below”&gt;AWS CloudHSM&lt;/a&gt; - Hardware-based Key Storage for Regulatory Compliance</li>
</ul>

<p><strong>Management</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/api-gateway/?hp=tile&amp;so-exp=below">Amazon API Gateway</a> - Build, Deploy, and Manage APIs</li>
  <li><a href="https://aws.amazon.com/autoscaling/?hp=tile&amp;so-exp=below">AWS Auto Scaling</a> - Scale Multiple Resources to Meet Demand</li>
  <li><a href="https://aws.amazon.com/cloudformation/?hp=tile&amp;so-exp=below">AWS CloudFormation</a> - Create and Manage Resources with Templates</li>
</ul>

<p><strong>Logging</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/cloudwatch/?hp=tile&amp;so-exp=below">Amazon CloudWatch</a> - Monitor Resources and Applications</li>
  <li><a href="https://aws.amazon.com/cloudtrail/?hp=tile&amp;so-exp=below">AWS CloudTrail</a> - Track User Activity and API Usage</li>
</ul>

<p><strong>Network</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/vpc/?hp=tile&amp;so-exp=below">Amazon VPC</a> - Isolated Cloud Resources</li>
  <li><a href="https://aws.amazon.com/cloudfront/?hp=tile&amp;so-exp=below">Amazon CloudFront</a> - Global Content Delivery Network</li>
  <li><a href="https://aws.amazon.com/route53/?hp=tile&amp;so-exp=below">Amazon Route 53</a> - Scalable Domain Name System</li>
</ul>

<p><strong>Discovery</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/application-discovery/?hp=tile&amp;so-exp=below">AWS Application Discovery Service</a> - Discover On-Premises Applications to Streamline Migration</li>
  <li><a href="https://aws.amazon.com/servicecatalog/?hp=tile&amp;so-exp=below">AWS Service Catalog</a> - Create and Use Standardized Products</li>
</ul>

<p>Migration</p>
<ul>
  <li><a href="https://aws.amazon.com/dms/?hp=tile&amp;so-exp=below">AWS Database Migration Service</a> - Migrate Databases with Minimal Downtime</li>
  <li><a href="https://aws.amazon.com/server-migration-service/?hp=tile&amp;so-exp=below">AWS Server Migration Service</a> - Migrate On-Premises Servers to AWS</li>
</ul>

<p><strong>Orchestration</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/codedeploy/?hp=tile&amp;so-exp=below">AWS CodeDeploy</a> - Automate Code Deployment</li>
  <li><a href="https://aws.amazon.com/codepipeline/?hp=tile&amp;so-exp=below">AWS CodePipeline</a> - Release Software using Continuous Delivery</li>
  <li><a href="https://aws.amazon.com/config/?hp=tile&amp;so-exp=below">AWS Config</a> - Track Resource Inventory and Changes</li>
  <li><a href="https://aws.amazon.com/systems-manager/?hp=tile&amp;so-exp=below">AWS Systems Manager</a> - Gain Operational Insights and Take Action</li>
</ul>

<p><strong>Monitoring</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/trustedadvisor/?hp=tile&amp;so-exp=below">AWS Trusted Advisor</a> - Optimize Performance and Security</li>
  <li><a href="https://aws.amazon.com/premiumsupport/phd/?hp=tile&amp;so-exp=below">AWS Personal Health Dashboard</a> - Personalized View of AWS Service Health</li>
</ul>

<p><strong>Security</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/guardduty/?hp=tile&amp;so-exp=below">Amazon GuardDuty</a> - Managed Threat Detection Service</li>
  <li><a href="https://aws.amazon.com/certificate-manager/?hp=tile&amp;so-exp=below">AWS Certificate Manager</a> - Provision, Manage, and Deploy SSL/TLS Certificates</li>
  <li><a href="https://aws.amazon.com/shield/?hp=tile&amp;so-exp=below">AWS Shield</a> - DDoS Protection</li>
  <li><a href="https://aws.amazon.com/waf/?hp=tile&amp;so-exp=below">AWS WAF</a> - Filter Malicious Web Traffic</li>
</ul>

<p><strong>Analytics</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/athena/?hp=tile&amp;so-exp=below">Amazon Athena</a> - Query Data in S3 using SQL</li>
  <li><a href="https://aws.amazon.com/cloudsearch/?hp=tile&amp;so-exp=below">Amazon CloudSearch</a> - Managed Search Service</li>
  <li><a href="https://aws.amazon.com/redshift/?hp=tile&amp;so-exp=below">Amazon Redshift</a> - Fast, Simple, Cost-effective Data Warehousing</li>
</ul>

<p><strong>Integration</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/step-functions/?hp=tile&amp;so-exp=below">AWS Step Functions</a> - Coordinate Distributed Applications</li>
  <li><a href="https://aws.amazon.com/sqs/?hp=tile&amp;so-exp=below">Amazon Simple Queue Service (SQS)</a> - Managed Message Queues</li>
  <li><a href="https://aws.amazon.com/sns/?hp=tile&amp;so-exp=below">Amazon Simple Notification Service (SNS)</a> - Pub/Sub, Mobile Push and SMS</li>
  <li><a href="https://aws.amazon.com/amazon-mq/?hp=tile&amp;so-exp=below">Amazon MQ</a> - Managed Message Broker for ActiveMQ</li>
</ul>

<p>I’m a big fan of the AWS approach. Their marketplace, and AWS API gateway provide unprecedented access to backend cloud, and on-premise resources, which can be secured using AWS IAM. Amazon Web Services products a robust infrastructure as a services, adequate enough to deliver any platform as a services solutions.</p>

<p><strong>Azure</strong></p>

<p>Next, let’s look at the Azure stack to see what they bring to the table. There is definitely some overlap with the AWS list of resources, but Microsoft has a different view of the landscape than Amazon does. However, similar to Amazon, most of the building blocks are here to deliver on the proposal above.</p>

<p><strong>Compute</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/virtual-machines/">Virtual Machines</a> - Provision Windows and Linux virtual machines in seconds</li>
  <li><a href="https://azure.microsoft.com/en-us/services/app-service/">App Service</a> - Quickly create powerful cloud apps for web and mobile</li>
  <li><a href="https://azure.microsoft.com/en-us/services/functions/">Functions</a> - Process events with serverless code</li>
  <li><a href="https://azure.microsoft.com/en-us/services/batch/">Batch</a> - Cloud-scale job scheduling and compute management</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-instances/">Container Instances</a> - Easily run containers with a single command</li>
  <li><a href="https://azure.microsoft.com/en-us/services/service-fabric/">Service Fabric</a> - Develop microservices and orchestrate containers on Windows or Linux</li>
  <li>a href=”https://azure.microsoft.com/en-us/services/virtual-machine-scale-sets/”&gt;Virtual Machine Scale Sets&lt;/a&gt; - Manage and scale up to thousands of Linux and Windows virtual machines</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-service/">Azure Container Service (AKS)</a> - Simplify the deployment, management, and operations of Kubernetes</li>
  <li><a href="https://azure.microsoft.com/en-us/services/cloud-services/">Cloud Services</a> - Create highly-available, infinitely-scalable cloud applications and APIs</li>
  <li><a href="https://azure.microsoft.com/en-us/services/virtual-machines/linux-and-open/">Linux Virtual Machines</a> - Provision virtual machines for Ubuntu, Red Hat, and more</li>
  <li><a href="https://azure.microsoft.com/en-us/services/">Windows Virtual Machines</a> - Provision virtual machines for SQL Server, SharePoint, and more</li>
</ul>

<p><strong>Storage</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/">Storage</a> - Durable, highly available, and massively scalable cloud storage</li>
  <li><a href="https://azure.microsoft.com/en-us/services/backup/">Backup</a> - Simple and reliable server backup to the cloud</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storsimple/">StorSimple</a> - Lower costs with an enterprise hybrid cloud storage solution</li>
  <li><a href="https://azure.microsoft.com/en-us/services/site-recovery/">Site Recovery</a> - Orchestrate protection and recovery of private clouds</li>
  <li><a href="https://azure.microsoft.com/en-us/services/data-lake-store/">Data Lake Store</a> - Hyperscale repository for big data analytics workloads</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/blobs/">Blob Storage</a> - REST-based object storage for unstructured data</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/unmanaged-disks/">Disk Storage</a> - Persistent, secured disk options supporting virtual machines</li>
  <li><a href="https://azure.microsoft.com/en-us/services/managed-disks/">Managed Disks</a> - Persistent, secured disk storage for Azure virtual machines</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/queues/">Queue Storage</a> - Effectively scale apps according to traffic</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/files/">File Storage</a> - File shares that use the standard SMB 3.0 protocol</li>
</ul>
<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/27_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><strong>Deployment</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/app-service/api/">API Apps</a> - Easily build and consume Cloud APIs</li>
</ul>

<p><strong>Containers</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/app-service/">App Service</a> - Quickly create powerful cloud apps for web and mobile</li>
  <li><a href="https://azure.microsoft.com/en-us/services/batch/">Batch</a> - Cloud-scale job scheduling and compute management</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-registry/">Container Registry</a> - Store and manage container images across all types of Azure deployments</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-instances/">Container Instances</a> - Easily run containers with a single command</li>
  <li><a href="https://azure.microsoft.com/en-us/services/service-fabric/">Service Fabric</a> - Develop microservices and orchestrate containers on Windows or Linux</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-service/">Azure Container Service (AKS)</a> - Simplify the deployment, management, and operations of Kubernetes</li>
</ul>

<p><strong>Databases</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-database/">SQL Database</a> - Managed relational SQL Database as a service</li>
  <li><a href="https://azure.microsoft.com/en-us/services/cosmos-db/">Azure Cosmos DB</a> - Globally distributed, multi-model database for any scale</li>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-data-warehouse/">SQL Data Warehouse</a> - Elastic data warehouse as a service with enterprise-class features</li>
  <li><a href="https://azure.microsoft.com/en-us/services/cache/">Redis Cache</a> - Power applications with high-throughput, low-latency data access</li>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-server-stretch-database/">SQL Server Stretch Database</a> - Dynamically stretch on-premises SQL Server databases to Azure</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/tables/">Table Storage</a> - NoSQL key-value store using semi-structured datasets</li>
  <li><a href="https://azure.microsoft.com/en-us/services/postgresql/">Azure Database for PostgreSQL</a> - Managed PostgreSQL database service for app developers</li>
  <li><a href="https://azure.microsoft.com/en-us/services/mysql/">Azure Database for MySQL</a> - Managed MySQL database service for app developers</li>
  <li><a href="https://azure.microsoft.com/en-us/services/database-migration/">Azure Database Migration Service</a> - Simplify on-premises database migration to the cloud</li>
</ul>

<p><strong>Authentication</strong>
<a href="https://azure.microsoft.com/en-us/services/active-directory/">Azure Active Directory</a> - Synchronize on-premises directories and enable single sign-on
<a href="https://azure.microsoft.com/en-us/services/multi-factor-authentication/">Multi-Factor Authentication</a> - Add security for your data and apps without adding hassles for users
<a href="https://azure.microsoft.com/en-us/services/key-vault/">Key Vault</a> - Safeguard and maintain control of keys and other secrets
<a href="https://azure.microsoft.com/en-us/services/active-directory-b2c/">Azure Active Directory B2C</a> - Consumer identity and access management in the cloud</p>

<p><strong>Management</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/api-management/">API Management</a> - Publish APIs to developers, partners, and employees securely and at scale</li>
</ul>

<p><strong>Logging</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/log-analytics/">Log Analytics</a> - Collect, search, and visualize machine data from on-premises and cloud</li>
  <li><a href="https://azure.microsoft.com/en-us/services/traffic-manager/">Traffic Manager</a> - Route incoming traffic for high performance and availability</li>
</ul>

<p><strong>Monitoring</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/monitor/">Azure Monitor</a> - Highly granular and real-time monitoring data for any Azure resource</li>
  <li><a href="https://azure.microsoft.com/en-us/features/azure-portal/">Microsoft Azure portal</a> - Build, manage, and monitor all Azure products in a single, unified console</li>
</ul>

<p><strong>Analytics</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/hdinsight/">HDInsight</a> - Provision cloud Hadoop, Spark, R Server, HBase, and Storm clusters</li>
  <li><a href="https://azure.microsoft.com/en-us/services/hdinsight/apache-spark/">Apache Spark for Azure HDInsight</a> - Apache Spark in the cloud for mission critical deployments</li>
  <li><a href="https://azure.microsoft.com/en-us/services/hdinsight/apache-storm/">Apache Storm for HDInsight</a> - Real-time stream processing made easy for big data</li>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-data-warehouse/">SQL Data Warehouse</a> - Elastic data warehouse as a service with enterprise-class features</li>
  <li><a href="https://azure.microsoft.com/en-us/services/log-analytics/">Log Analytics</a> - Collect, search, and visualize machine data from on-premises and cloud</li>
  <li><a href="https://azure.microsoft.com/en-us/services/data-lake-store/">Data Lake Store</a> - Hyperscale repository for big data analytics workloads</li>
  <li><a href="https://azure.microsoft.com/en-us/services/data-lake-analytics/">Data Lake Analytics</a> - Distributed analytics service that makes big data easy</li>
  <li><a href="https://azure.microsoft.com/en-us/services/analysis-services/">Azure Analysis Services</a> - Enterprise grade analytics engine as a service</li>
  <li><a href="https://azure.microsoft.com/en-us/services/databricks/">Azure Databricks</a> - Fast, easy, and collaborative Apache Spark-based analytics platform</li>
</ul>

<p><strong>Network</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/cdn/">Content Delivery Network</a> - Ensure secure, reliable content delivery with broad global reach</li>
  <li><a href="https://azure.microsoft.com/en-us/services/dns/">Azure DNS</a> - Host your DNS domain in Azure</li>
  <li><a href="https://azure.microsoft.com/en-us/services/virtual-network/">Virtual Network</a> - Provision private networks, optionally connect to on-premises datacenters</li>
  <li><a href="https://azure.microsoft.com/en-us/services/traffic-manager/">Traffic Manager</a> - Route incoming traffic for high performance and availability</li>
  <li><a href="https://azure.microsoft.com/en-us/services/load-balancer/">Load Balancer</a> - Deliver high availability and network performance to your applications</li>
  <li><a href="https://azure.microsoft.com/en-us/services/network-watcher/">Network Watcher</a> - Network performance monitoring and diagnostics solution</li>
</ul>

<p><strong>Orchestration</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/scheduler/">Scheduler</a> - Run your jobs on simple or complex recurring schedules</li>
  <li><a href="https://azure.microsoft.com/en-us/services/automation/">Automation</a> - Simplify cloud management with process automation</li>
  <li><a href="https://azure.microsoft.com/en-us/services/automation-control/">Automation &amp; Control</a> - Centrally manage all automation and configuration assets</li>
</ul>

<p><strong>Integration</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/data-factory/">Data Factory</a> - Orchestrate and manage data transformation and movement</li>
  <li><a href="https://azure.microsoft.com/en-us/services/logic-apps/">Logic Apps</a> - Automate the access and use of data across clouds without writing code</li>
  <li><a href="https://azure.microsoft.com/en-us/services/event-grid/">Event Grid</a> - Get reliable event delivery at massive scale</li>
</ul>

<p><strong>Search</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/search/">Azure Search</a> - Fully-managed search-as-a-service</li>
</ul>

<p><strong>Discovery</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/active-directory-ds/">Azure Active Directory Domain Services</a> - Join Azure virtual machines to a domain without domain controllers</li>
</ul>

<p><strong>Security</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/security-center/">Security Center</a> - Unify security management and enable advanced threat protection across hybrid cloud workloads</li>
  <li><a href="https://azure.microsoft.com/en-us/services/security-compliance/">Security &amp; Compliance</a> - Enable threat detection and prevention through advanced cloud security</li>
  <li><a href="https://azure.microsoft.com/en-us/services/ddos-protection/">Azure DDoS Protection</a> - Protect your applications from Distributed Denial of Service (DDoS) attacks</li>
</ul>

<p><strong>Governance</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/azure-policy/">Azure Policy</a> - Implement corporate governance and standards at scale for Azure resources</li>
</ul>

<p><strong>Monetization</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/cost-management/">Cost Management</a> - Optimize what you spend on the cloud, while maximizing cloud potential</li>
</ul>

<p><strong>Experience</strong><br />
I have been studying Amazon full time for almost eight years. I’ve been watching Azure play catch up for the last three years. I run my infrastructure, and a handful of clients on AWS. I understand the API landscape of both providers, and how they can be woven into vision proposed so far.</p>

<p>I see the AWS API stack, and the Azure API stack, as a starter set of services that can be built upon to deliver the base Lighthouse implementation. All the components are there. It just need the first set of Lighthouse services to be defined, delivering the essential building blocks  any platform needs, things like compute, storage, dns, messaging, etc. I recommend that the VA Lighthouse team take the AWS API stack, and mold it into v1 of the Lighthouse API stack. Take the momentum from AWS’s own API journey, build upon it, and set into motion the VA Lighthouse API journey.</p>

<p>Enable VA services to be delivered as individual, self-contained units, that can be used as part of a larger VA orchestration of veteran services. Open up the VA and let some sunlight in. Think about what Amazon has been able to achieve by delivering its own internal operations as services, and remaking not just retail, but also touching almost every other industry with Amazon Web Services. <a href="https://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/">The Amazon Web Services myth story</a> provides a powerful and compelling narrative for any company, organizations, institution, or government agency like the VA to emulate.</p>

<p>This proposal is not meant to be a utopian vision for the VA. However it is meant to, as the name of the project reflects, shine a light on existing ways of delivering services via the cloud. Helping guide each service in its own individual journey, while also serving the overall mission of the platform–to help the veteran be successful in their own personal journey.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/pano-lighthouse_copper_circuit.jpg" width="100%" align="center" /></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/22/aws-iam-like-policies-for-aws-api-gateway-and-marketplace-billing/">AWS IAM-Like Policies For AWS API Gateway And Marketplace Billing</a></h3>
        <span class="post-date">22 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/server-cloud1_internet_numbers.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>The primary reason I’ve been adopting more AWS solutions as part of my API stack, and using tools I have historically felt lock me into the AWS ecosystem, is the available of AWS identity and access management (IAM). I just cannot deliver secure at this level as a small business owner, and their robust solution lets me dial in exactly what I need when it comes to defining who has access to what across my API infrastructure. I can define different policies, and apply them at the API management layer using both AWS Lambda and AWS API Gateway. Keeping everything separated, yet with a single API stack as the point of entry, for all consumers and applications.</p>

<p>I want all of this security goodness, but for the business of my APIs. Similar to the engine that drives the relationship between me as an AWS Marketplace user and AWS, I want a framework for applying business policies at the plan level within AWS API Gateway. I want to determine who has access to which resources, as well as what they can use, but I want to be able to meter this usage, and charge different rates. Compute, storage, and bandwidth for my partners is different than for retail API consumers, with a mix of resource and API call based metrics.</p>

<p><a href="https://plans.apievangelist.com/2017/10/23/api-monetization-framework-introduced-by-aws-marketplace/">The AWS monetization policies would reflect the AWS Marketplace framework</a>, giving me a mix of metering and contract based billing, reflecting single or multi-dimensional usage across the eight areas of consumption they support currently. I want to be able to establish common monetization policies across all my microservices, and allow product managers to implement them consistently at scale using AWS API Gateway. Like security, these API product managers shouldn’t be experts in the economics of the services being offered, they should just be able to apply from a common pool of business policies, and provide feedback on how to evolve, when appropriate.</p>

<p>This concept is very much in the realm of traditional API management service composition, but would possess a machine readable policy format just like IAM policies. API monetization policies could be reported upon, providing breakdown of consumption of resources at the backend system, or front-end API path level, helping translate the monetization side of our API strategy, into actual API plans that can be executed at run-time. Providing a standardized, scalable, quantifiable way to measure the value exchange that occurs at the API level. Done in a way that could be applied internally, or external with partners, and 3rd party developers. Making the business of my APIs more consistent, modular, and reusable–just like the rest of my API infrastructure.</p>

<p>I think AWS has a significant advantage in this area. They have the advanced resource management infrastructure, as well as the business side of all of this from managing their own APIs, but also from slowly rolling it out as part of the AWS Marketplace. AWS API Gateway has the plan, and marketplace key, providing the beginning of the implementation. All we need is the standardized policies based upon their existing pricing framework, and the ability to measure and report upon at the AWS API Gateway plan level. The working parts are there, it just needs to be brought together. It might also be something someone could piece together from logging, and other existing outputs on the AWS platform, and create an external reporting and billing solution. IDK. Just brainstorming, what I’d like to see, and getting it here on the blog before the thought passes.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/22/aws-iam-like-policies-for-aws-api-gateway-and-marketplace-billing/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/">Provisioning A Default App And Keys For Your API Consumers On Signup</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/sabre/sabre-travel-signup-keys.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I sign up for a lot of APIs. I love anything that reduces friction when on-boarding, and allows me to begin making an API call in 1-3 clicks. I’m a big fan of API providers that allow me to signup using my Github OAuth, preventing me from having to sign up for yet another account. I’m also a big fan of providers who automatically provision an application for me as part of the signup, and have my API keys waiting for me as soon as I’ve registered.</p>

<p><a href="https://developer.sabre.com/member/register">While signing up for the Sabre travel API I saw that they provisioned my application as part of the API sign up process in a way that was worth showcasing</a>. Saving me the time and hassle of having to add a new application after I’ve signed up. Stuff like this might seem like a pretty small detail when developing an API on-boarding process, but when you are signing up for many different APIs, and trying to manage your time–these little details add up to be a significant time saver.</p>

<p>Ideally, API providers would auto-provision a default application along with the signup, but I like the idea of also giving me the option to name my application while registering. When crafting your API registration flow, make sure you spend time signing up multiple times, and try to put yourself in your API consumers shoes. I even recommend signing up for an account each week, repeatedly experiencing what your consumers will be exposed to. I also recommend spending time signing up for other APIs on a regular basis, to experience what they offer–you will always surprised by what I find.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/open-banking-in-the-uk-openapi-template/">An Open Banking in the UK OpenAPI Template</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p>After learning more about <a href="https://www.openbanking.org.uk/">what Open Banking is doing for APIs in the UK</a>, I realized that I needed an OpenAPI template for the industry specification. There are six distinct schema available as part of the project, and I wanted a complete OpenAPI to describe which paths were available, as well as the underlying response schema. I got work crafting one from the responses that were available within <a href="https://www.openbanking.org.uk/open-data-apis/">the Open Banking documentation</a>.</p>

<p>Open Banking had schema available for their API definitions, but OpenAPI is the leading API and data specification out there today, so it makes sense that there should be an OpenAPI available, helping all participating banking API providers take advantage of all the tooling available within the OpenAPI community. To help support, I have published my Open Banking OpenAPI definition as a Github Gist:</p>

<script src="https://gist.github.com/kinlane/57c720c18e4d0ad370ad92c0ab9613f7.js"></script>

<p>I’ve applied this OpenAPI definition to the 17 banks they have listed, and will be including them in the next publishing of <a href="http://theapistack.com">my API Stack project</a>. Open Banking provides a common definition that can be used across many banks, and an OpenAPI template allows me to quickly apply the common template to each individual bank. Generating bank specific documentation, SDK and code samples, monitoring, tests, and other client tooling. Helping me put the valuable data being made available via each API to work.</p>

<p>I’d like to see more organizations like Open Banking emerge. I’d also like to help ensure they all make OpenAPI templates available for any API and schema specifications they establish. The API lifecycle is increasingly OpenAPI defined, and when you make your guidance available in the OpenAPI format, you are enabling actors within any industry to quickly get up and running with designing, deploying, managing, testing, monitoring, and almost every other stop along a modern API lifecycle. Increasing the chances of adoption for any API standards you are putting out there.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/open-banking-in-the-uk-openapi-template/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/">An Opportunity Around Providing A Common OpenAPI Enum Catalog</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/enums/bitcoin-pools.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m down in the details of the OpenAPI specification lately, working my way through hundreds of OpenAPI definitions, trying to once again make sense of the API landscape at scale. I’m working to prepare as many API path definitions as I possibly can to be runnable within one or two clicks. OpenAPI definitions, and Postman Collections are essential to making this happen, both of which require complete details on the request surface area for an API. I need to know everything about the path, as well as any headers, path, or query parameters that need to included. A significant aspect of this definition being complete includes default, and enum values being present.</p>

<p>If I can’t quickly choose from a list of values, or run with a default value, when executing an API, the time to seeing a live response grows significantly. If I have to travel back to the HTML documentation, or worse, do some Googling before I can make an API call, I just went from seconds to potentially minutes or hours before I can see a real world API response. Additionally, if there are many potential values available for each API parameter, enums become critical building blocks to helping me understand all the dimensions of an API’s surface area. Something that should have been considered as part of the API’s design, but often just gets left as part of API documentation.</p>

<p>When playing with a Bitcoin API with the following path /blocks/{pool_name}, I need to the list of pools I can choose from. When looking to get a stock market quote from an API with the following path, /stock/{symbol}/quote, I need a list of all the ticker symbols. Having, or not having these enum values at documentation, and execute time, are essential. Many of these lists of values are so common, developers take them for granted. Assuming that API consumers just have them laying around, and really aren’t worth including in documentation. You’d think we all have lists of states, countries, stock tickers, Bitcoin pools, and other data just laying around, but even as the API Evangelist, I often find myself coming up short.</p>

<p>All of this demonstrates a pretty significant opportunity for someone to create a Github hosted, searchable, forkable list of common OpenAPI enum lists. Providing an easy place for API providers, and API consumers to discover simple, or complex lists of values that should be present in API documentation, and included as part of all OpenAPIs. I recommend just publishing each enum JSON or YAML list as a Github Gist, and then publishing as a catalog via a simple Github Pages website. If I don’t see something pop up in the next couple of months, I’ll probably begin publishing something myself. However, I need another API related project like I need a hole in the head, so I’m holding off in hopes another hero or champion steps up and owns the enum portion of the growing OpenAPI conversation.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/">What Is Open Banking In The UK?</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/open-banking/open-banking-screenshot.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am profiling banks in the UK as part of an effort move forward my <a href="http://theapistack.com">API Stack</a> work, and populate the <a href="https://streamdata.io/developers/api-gallery/">Streamdata.io API Gallery</a>. One significant advantage that banks in the UK have over other countries in the EU, and even in the US, is the help of <a href="https://www.openbanking.org.uk">Open Banking</a>. To help profile the organization, I’ll just borrow from their website to define who they are and what they do.</p>

<p><em>The Open Banking Implementation Entity was created by the UK’s Competition and Markets Authority to create software standards and industry guidelines that drive competition and innovation in UK retail banking.</em></p>

<p>In 2016, The Competition and Markets Authority (CMA) published a report on the UK’s retail banking market which stated that older, larger banks do not have to compete hard enough for customers’ business, and smaller and newer banks were finding it difficult to grow and access the UK banking market. To solve this problem, they proposed a number of remedies including Open Banking, which defines API standards that are intended to help level that playing field.</p>

<p>The role of Open Banking is to:</p>

<ul>
  <li>Design the specifications for the Application Programming Interfaces (APIs) that banks and building societies use to securely provide Open Banking</li>
  <li>Support regulated third party providers and banks and building societies to use the Open Banking standards</li>
  <li>Create security and messaging standards</li>
  <li>Manage the Open Banking Directory which allows regulated participants like banks, building societies and third party providers to enroll in Open Banking</li>
  <li>Produce guidelines for participants in the Open Banking ecosystem</li>
  <li>Set out the process for managing disputes and complaints</li>
</ul>

<p>This approach to standardizing API definitions is the type of leadership that is needed to move API conversation forward in ALL industries. I know in the US, many enjoy viewing regulations as always bad, but this type of organizational designation can go a long way towards moving an industry forward in a concerted fashion. Doing the hard work to establish a common API definition, and play a central role in helping ensure each actor within an industry is implementing the definition as expected.</p>

<p>I’d like to see more organizations emerge that reflect Open Banking’s mission, in a variety of industries. Many companies do not have the time, expertise, or desire to do the homework and understand what needs to occur on the API front. Speaking from experience, there is’t a lot of vendor-free funding to do this kind of work, and it is something that will require public sector investment. In my opinion, this doesn’t always have to be government led, but there should be industry neutral funding available to move forward the conversation in a way that benefits everyone involved, without a focus on any single product or service.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/20/relationship-between-openapi-path-summary-tags-and-asyncapi-topics/">Relationship Between OpenAPI Path, Summary, Tags and AysncAPI Topics</a></h3>
        <span class="post-date">20 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/23_160_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m working my way through several hundred OpenAPI definitions that I have forked from <a href="https://apis.guru/">APIs.guru</a>, <a href="https://any-api.com/">Any API</a>, and have automagically generated from API documentation scrape scripts I have developed over time. Anytime I evolve a new OpenAPI definition, I first make sure the summary, description, and tags are as meaningful as they possibly can. Sadly this work is also constrained by how much time I have to spend with each API, as well as how well designed their API is in the first place. I have a number of APIs that help me enrich this automatically, by mining the API path, applying regular expressions, but often times it takes a manual review to add tags, polish summaries, and make the OpenAPI details as meaningful as I possibly can, in regards to what an API does.</p>

<p>As I’m taking a break from this work, I’m studying up on <a href="https://www.asyncapi.com/">AsyncAPI</a>, trying to get my head around how I can be crafting API definitions for the message-based, event-driven, streaming APIs I’m profiling alongside my regular API research. One of the areas the AsyncAPI team is pushing forward is around the concept of a topic–_“to create a definition that suites most use cases and establish the foundation for community tooling and better interoperability between products when using AsyncAPI.”_ or to elaborate further, <em>“a topic is a string representing where an AsyncAPI can publish or subscribe. For the sake of comparison they are like URLs in a REST API.”</em> Now I’m thinking about the relationships between the API design elements I’m wrestling with in my API definitions, and how the path, summary, and tags reflect what Async is trying to articulate with their topics discussion.</p>

<p><strong>{organization}.{group}.{version}.{type}.{resources}.{event}</strong></p>

<ul>
  <li><strong>organization</strong> - the name of the organization or company.</li>
  <li><strong>group</strong> - the service, team or department in charge of managing the message..</li>
  <li><strong>version</strong> - the version of the message for the given service. This version number should remain the same unless changes in the messages are NOT backward compatible.</li>
  <li><strong>type</strong> - the type of the message, e.g., is it a command or an event?. This value should always be event unless you’re trying to explicitly execute a command in another service, i.e., when using RPC.</li>
  <li><strong>resources</strong> - resources and sub-resources, in a word (or words) describing the resource the message refers to. For instance, if you’re sending a message to notify a user has just signed up, the resource should be user. But, if you want to send a message to notify a user has just changed her full name, you could name it as user.full_name.</li>
  <li><strong>event</strong> - an event or command name, in case message type is event, this should be a verb in past tense describing what happened to the resource, and in case message type is command, this should be a verb in infinitive form describing what operation you want to perform.</li>
</ul>

<p><strong>Example(s):</strong></p>

<ul>
  <li>hitch.accounts.1.event.user.signedup</li>
  <li>hitch.email.1.command.user.welcome.send</li>
</ul>

<p>As I’m crafting OpenAPI definitions, and publishing them to Github, I’m using Jekyll to give me access to the large numbers of OpenAPI definitions I’ve published, and indexed using APIs.json, as Liquid objects. For each site. I can references APIs path using a dotted notation, such as site.twilio.send-sms-get. I haven’t polished my naming conventions, and simply taking the path, stripping out everything but the alpha, numeric characters for the file names, but it got me thinking about how I might want to get more structured in how I name the individual units of compute I’m publishing using OpenAPI, and often times as Postman Collections.</p>

<p>As I publish these API definitions to Github, as part of my API profiling for inclusion in the Streamdata.io API, I’m looking to establish a map of the surface area, that I can potentially turn into webhooks, streamings, and other approaches to real time message delivery. This is why I’m looking to understand AsyncAPI, to help quantify the result of this work. After I map out the surface area of the APIs, and quantify the topics at play, and obtain an API key, I need a way to then map out the real time streams of messages that will get passed around. To do this, I will need a way to turn each potential API response and its resulting request into a topic definition into a well defined, measurable input or output–AsyncAPI is going to help me do this.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/20/relationship-between-openapi-path-summary-tags-and-asyncapi-topics/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/">People Who Provide Enum For Their OpenAPI Definitions Are Good People</a></h3>
        <span class="post-date">20 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/64_116_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m processing a significant amount of OpenAPI definitions currently, as well as crafting a number of them from scraped API documentation. After you work with a lot of OpenAPI definitions, aiming to achieve a specific objective, you really get to know which aspects of the OpenAPI are the most meaningful, and helpful when they are complete. <a href="http://apievangelist.com/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi-definition/">I talked about the importance of summary, description, and tags last week</a>, and this week I’d like to highlight how helpful it is when the stewards of OpenAPI definitions include enum values for their parameters, and I think they are just good people. ;-)</p>

<p>Enums are simply just a list of potential values for each of the parameters you outline as part of your API definition. So if you have state as a parameter for use in the request of your API, you have a list of the 50 US states as the enum. If you the parameter is color, you have just the color black, because we all know it is the only color(all the colors). ;-) If you provide a parameter that will accept a standard set of inputs, you should consider providing an enum list to help your consumers understand the potential for that parameter. Outlining the dimensions of the parameter in a simple JSON or YAML array of every single possible value.</p>

<p>I can’t articulate how many times I have to go looking for a list of values. Sometimes it is present within the description for the OpenAPI, but often times I have to go back to the portal for the API, and follow a link to a page that lists out the values. That is, if an API provider decides to provide this information at all. The thoughtful ones do, the even more thoughtful ones put it in their OpenAPI definitions as enum values. Anytime I come across a list of enums that I can quickly build an array, select, and other common aspects of doing business with APIs, I’m a happy camper.</p>

<p>Which is why you find me writing up enums. Boring. Boring. Boring. However, it is something that makes me happy, potentially multiple times in a single day, and imagine that multiplied by the number of developers you have, or maybe “had”, depending on how frustrating it is to find the values that can be used in your API’s parameters. In my opinion, enums add rich dimensions to what your API does, and can be as important as the overall design of your API. Depending on how you’ve designed your API, you may have invested heavily in design, or may be leaning on your API parameters to do the heavy lifting of helping you–making them even more important when it comes to documenting them as part of your API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using-apis/">Insecurity Around Providing Algorithmic Transparency And Observability Using APIs</a></h3>
        <span class="post-date">16 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/68_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://streamdata.io/blog/benchmark-quantifying-api-performance/">I’m working on a ranking API for my partner Streamdata.io to help quantify the efficiencies they bring to the table when you proxy an existing JSON web API using their service</a>. I’m evolving an algorithm they have been using for a while, wrapping it in a new API, and applying it across the APIs I’m profiling as part of my <a href="http://theapistack.com">API Stack</a>, and the <a href="https://streamdata.io/developers/api-gallery/">Streamdata.io API Gallery</a> work. I can pass the ranking API any OpenAPI definition, and it will poll and stream the API for 24 hours, and return a set of scores regarding how real time the API is, and what the efficiency gains are when you use Streamdata.io as a proxy for the API.</p>

<p>As I do this work, I find myself thinking more deeply about the role that APIs can play in helping make algorithms more transparent, observable, and accountable. My API ranking algorithm is pretty crude, but honestly it isn’t much different than many other algorithms I’ve seen companies defend as intellectual property and their secret sauce. Streamdata.io is invested in the ranking algorithm and API being as transparent as possible, so that isn’t a problem here, but each step of the process allows me to think through how I can continue to evangelize other algorithm owners to use APIs, to make their algorithms more observable and accountable.</p>

<p>In my experience, most of the concerns around keeping algorithms secret stem from individual insecurities, and nothing actually technical, mathematical, or proprietary. The reasons for the insecurities are usually that the algorithm isn’t that mathematically sophisticated (I know mine isn’t), or maybe it is pretty flawed (I know mine is currently), and people just aren’t equipped to admit this (I know I am). I’ve worked for companies who venomously defend their algorithms and refuse to open them up, because in the end they know they aren’t defensible on many levels. The only value the algorithm possesses in these scenarios is secrecy, and the perception that there is magic going on behind the scenes. When in reality, it is a flawed, crude, simple algorithm that could actually be improved upon if it was opened up.</p>

<p>I’m not insecure about my lack of mathematical skills, or the limitations of my algorithm. I want people to point out its flaws, and improve upon my math. I want the limitations of the algorithm to be point out. I want API providers and consumers to use the algorithm via the API (when I publish) to validate, or challenge the algorithmic assumptions being put forth. I’m not in the business of selling smoke and mirrors, or voodoo algorithmics. I’m in the business of helping people understand how inefficient their API responses are, and how they can possibly improve upon them. I’m looking to develop my own understanding of how can make APIs more event-driven, real time, and responsive. I’m not insecure about providing transparency and observability around the algorithms I develop, using APIs–all algorithm developers should be as open and confident in their own work.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/16/using-jekyll-and-openapi-to-evolve-api-documentation-and-storytelling/">Using Jekyll And OpenAPI To Evolve My API Documentation And Storytelling</a></h3>
        <span class="post-date">16 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/68_158_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m reworking my API Stack work as independent sets of <a href="https://jekyllrb.com/docs/collections/">Jekyll collections</a>. Historically I just dumped all <a href="http://apisjson.org/">APIs.json</a>, and OpenAPIs into the central data folder, and grouped them into folders by company name. Now I am breaking them out into tag based collections, using a similar structure. Further evolving how I document and tell stories using each API. I have been published a single OpenAPI for each platform, but now I’m publishing a separate OpenAPI for each API path–we will see where this goes, it might ultimately end up biting me in the ass. I’m doing this because I want to be able to talk about a single API path, and provide a definition that can be viewed, interpreted, and executed against, independent of the other paths–Jekyll+OpenAPI is helping me accomplish this.</p>

<p>With each API provider possessing its own APIs.json index, and each API path having its own OpenAPI definition, I’m able to mix up how I document and tell stories around these APIs. I can list them by API provider, or by individual API path. I can filter based upon tags, and provide execute-time links that reference each individual unit of API. I have separate JavaScript functions that can be referenced if the API path is GET, POST, or PUT. I can even inherit other relevant links like API sign up or terms of service as part of its documentation. I can reference all of this as part of larger documentation, or within blog posts, and other pages throughout the website–which will be refreshed whenever I update the OpenAPI definition.</p>

<p>If you aren’t familiar with how Jekyll works. It is a static content solution, that allows you do develop collections. You can put CSV, JSON, or YAML into these collections (folders), and they become objects you can reference using Liquid syntax. So if I put Twitter’s APIs.json, and OpenAPI into a folder within my social collection, I can reference as site.social.twitter which is the APIs.json for Twitter’s entire API operations, and I can reference individual APIs as site.social.twitter.search for the individual OpenAPI defining the Twitter search API path. This decouples API documentation for me, and allows me to not just document APIs, but tell stories with  API definitions, making my API portals much more interactive, and hopefully engaging.</p>

<p>I just got my API stack approach refreshed using this new format. Now I just need to go through all my APIs and rebuild the underlying Github repository. I have thousands of APIs that I track on, and I’m curious how this approach holds up at scale. While <a href="http://theapistack.com">API Stack</a> is a single repository, I can essentially publish any collection of APIs I desire to any of the hundreds of repositories that make up the API Evangelist network. Allowing me to seamless tell stories using the technical details of API operations, and the individual API resources they serve up. Further evolving how I tell stories around the APIs I’m tracking on. While my API documentation has always been interactive, I think this newer, more modular approach, reflects the value each unit of value an API brings to the table, rather than just looking to document all the APIs a provider possesses.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/16/using-jekyll-and-openapi-to-evolve-api-documentation-and-storytelling/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi-definition/">The Importance of the API Path Summary, Description, and Tags in an OpenAPI Definition</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/15_190_800_500_0_max_0_1_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am creating a lot of OpenAPI definitions right now. <a href="http://apis.how/streamdata.io">Streamdata.io</a> is investing in me pushing forward my <a href="http://theapistack.com">API Stack</a> work, where I profile API using OpenAPI, and index their operations using APIs.json. From the resulting indexes, we are building out the <a href="https://streamdata.io/developers/api-gallery/">Streamdata.io API Gallery</a>, which shows the possibilities of providing streaming APIs on top of existing web APIs available across the landscape. The OpenAPI definitions I’m creating aren’t 100% complete, but they are “good enough” for what we are needing to do with them, and are allowing me to catalog a variety of interesting APIs, and automate the proxying of them using Streamdata.io.</p>

<p>I’m finding the most important part of doing this work is making sure there is a rich summary, description, and set of tags for each API. While the actual path, parameters, and security definitions are crucial to programmatically executing the API, the summary, description, and tags are essential so that I can understand what the API does, and make it discoverable. As I list out different areas of my API Stack research, like <a href="http://market.data.apievangelist.com/">the financial market data APIs</a>, it is critical that I have a title, and description for each provider, but the summary, description, and tags are what provides the heart of the index for what is possible with each API.</p>

<p>When designing an API, as a developer, I tend to just fly through writing summary, descriptions, and tags for my APIs. I’m focused on the technical details, not this “fluff”. However, this represents one of the biggest disconnects in the API lifecycle, where the developer is so absorbed with the technical details, we forget, neglect, or just don’t are to articulate what we are doing to other humans. The summary, description, and tags are the outlines in the API contract we are providing. These details are much more than just the fluff for the API documentation. They actually describe the value being delivered, and allow this value to be communicated, and discovered throughout the life of an API–they are extremely important.</p>

<p>As I’m doing this work, I realize just how important these descriptions and tags are to the future of these APIs. Whenever it makes sense I’m translating these APIs into streaming APIs, and I’m taking the tags I’ve created and using them to define the events, topics, and messages that are being transacted via the API I’m profiling. I’m quantifying how real time these APIs are, and mapping out the meaningful events that are occurring. This represents the event-driven shift we are seeing emerge across the API landscape in 2018. However, I’m doing this on top of API providers who may not be aware of this shift in how the business of APIs is getting done, and are just working hard on their current request / response API strategy. These summaries, descriptions, and tags, represent how we are going to begin mapping out the future that is happening around them, and begin to craft a road map that they can use to understand how they can keep evolving, and remain competitive.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi-definition/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/">A Really Nice API Application Showcase Over At The Intrinio Market Data API</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p>I am profiling financial market data APIs currently, and as I’m doing my work profiling APIs, I’m always on the hunt for interesting elements of their API operations that I can showcase for my readers. While looking at the financial market data API from Intrinio, I found that <a href="https://intrinio.com/marketplace/apps">I really, really like their application showcase</a>, which providers a pretty attractive blueprint for how we can showcase what is being develop on top of our APIs.</p>

<p>The Intrinio application showcase is just clean looking, and has the bells and whistles you’d expect like categories, search, detail or list view, and detail pages providing you all the information you need about the application, and where you can find tutorials, code, and other relevant resources.</p>

<p align="center"><a href="https://intrinio.com/marketplace/apps"><img src="https://s3.amazonaws.com/kinlane-productions/intrinio/intrinio-app-showcase.png" /></a></p>

<p>Another thing I really like is it isn’t just about web and mobile applications. They have spreadsheet integrations, and help walk you through how to “apply” each type of integration. This is what the application in API means to me. It isn’t always just about finished web, mobile, and device applications. It is about applying the resources available via the programmatic interfaces to some problem you have in your world.</p>

<p>Anyways, the Intrinio application showcase is totally worth profiling as part of my research. It is a great blueprint for other API providers to follow when crafting their own application showcases. This post give me a single URL that I can share with folks, and reference throughout my stories, white papers, guides, and talks. I’d love to see this become the standard for how API providers showcase their applications, keeping things simple, clean, and bringing value to their consumers.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/how-big-or-small-is-an-api/">How Big Or Small Is An API?</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/31_156_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working to build out <a href="https://streamdata.io/developers/api-gallery/">the API Gallery for Streamdata.io</a>, profiling a wide variety of APIs for inclusion in the directory, adding to the wealth of APIs that could be streamed using the service. As I work to build the index, I’m faced with the timeless question regarding, what is an API? Not technically what an API does, but what is an API in the context of helping people discover the API they are looking for. Is Twitter an API, or is the Twitter search/tweets path an API? My answer to this question always distills down to a specific API path, or as some call it an API endpoint. Targeting a specific implementation, use case, or value generated by a single API provider.</p>

<p>Like most things in the API sector, words are used interchangeably, and depending on how much experience you have in the business, you will have much finer grained definitions about what something is, or isn’t. When I’m talking to the average business user, the Twitter API is the largest possible scope–the entire thing. In the context of API discovery, and helping someone find an API to stream or to solve a specific problem in their world, I’m going to resort to a very precise definition–in this case, it is the specific Twitter API path that will be needed. Depending on my audience, I will zoom out, or zoom in on what constitutes a unit of API. The only consistency I’m looking to deliver is regarding helping people understand, and find what they looking for–I’m not worried about always using the same scope in my definition of what an API is.</p>

<p><a href="https://streamdata.io/blog/robust-market-data-apis-alphavantage/">You can see an example of this in action with the Alpha Vantage market data API I’m currently profiling</a>, and adding to the gallery. Is Alpha Vantage is a single API, or 24 separate APIs? In the context of the Streamdata.io API Gallery, it will be 24 separate APIs. In the context of telling the story on the blog, there is a single Alpha Vantage API, with many paths available. I don’t want someone searching specifically for a currency API to have to wade through all 24 Alpha Vantage paths, I want them to find specifically the path for their currency API. When it comes to API storytelling, I am fine with widening the scope of my definition, but when it comes to API discovery I prefer to narrow the scope down to a more granular unit of value.</p>

<p>For me, it all comes down the definition of what an API is. It is all about applying a programmatic interface. If I’m applying in a story that targets a business user, I can speak in general terms. If I’m applying to solve a specific business problem, I’m going to need to get more precise. This precision can spin out of control if you are dealing with developers who tend to get dogmatic about programming languages, frameworks, platforms, and the other things that make their worlds go round. I’m not in the business of being “right”. I’m in the business of helping people understand, and solve the problems they have. Which gives me a wider license when it comes to defining how big or small an API can be. It is a good place to be.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/how-big-or-small-is-an-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/some-common-features-of-an-api-application-review-process/">Some Common Features Of An API Application Review Process</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p><a href="https://twitter.com/ktinboulder/status/961601920887607296"><img src="https://s3.amazonaws.com/kinlane-productions/kelly-taylor-app-approval-tweet.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="https://twitter.com/ktinboulder/status/961601920887607296">I received a tweet from my friend Kelly Taylor with USDS</a>, asking for any information regarding establishing an “approve access to production data” for developers. <a href="https://bluebutton.cms.gov/developers/">He is working on an OAuth + FHIR implementation for the Centers for Medicare and Medicaid Services (CMS) Blue Button API</a>. Establishing a standard approach for on-boarding developers into a production environment always makes sense, as you don’t want to give access to sensitive information without making sure the company, developer, and application has been thoroughly vetted.</p>

<p>As I do with my work, I wanted to think through some of the approaches I’ve come across in my research, and share some tips and best practices. <a href="https://bluebutton.cms.gov/developers/#production-api-access">The Blue Button API team has a section published regarding how to get your application approved</a>, but I wanted to see if I can expand on, while also helping share this information with other readers. This is a relevant use case that I see come up regularly in healthcare, financial, education, and other mainstream industries.</p>

<p><strong>Virtualization &amp; Sandbox</strong><br />
The application approval conversation usually begins with ALL new developers being required to work with a sandboxed set of APIs, only providing production API access to approved developers. This requires having a complete set of virtualized APIs, mimicking exactly what would be used in production, but in a much safer, protected environment. One of the most important aspects of this virtualized environment is that there also needs to be robust sets of virtualized data, providing as much parity regarding what developers will experience when they enter the production environment. The sandbox environment needs to be as robust and reliable as the production, which is a mistake I see made over and over from providers, where the sandbox isn’t reliable, or as functional, and developers never are able to reach production status in a consistent and reliable way.</p>

<p><strong>Doing a Background Check</strong><br />
Next, as reflected in the Blue Button teams approach, you should be profiling the company and organization, as well as the individual behind each application. <a href="http://apievangelist.com/2016/03/30/best-buy-will-not-issue-api-keys-to-free-email-accounts-and-wants-to-get-to-know-your-company/">You see company’s like Best Buy refusing any API signup that doesn’t have an official company domain that can be verified</a>. In addition to requiring developers provide a thorough amount of information about who they are, and who they work for, many API providers are using background and profiling services like <a href="https://clearbit.com/">Clearbit</a> to obtain more details about a user based upon their email, IP address, and company domain. Enabling different types of access to API resources depending on the level of scrutiny a developer is put under. I’ve seen this level of scrutiny go all the way up to requiring the scanning of drivers license, and providing corporate documents before production access is approved.</p>

<p><strong>Purpose of Application</strong><br />
One of the most common filtering approaches I’ve seen centers around asking developer about the purpose of their application. The more detail the better. As we’ve seen from companies like Twitter, the API provider holds a lot of power when it comes to deciding what types of applications will get built, and it is up to the developer to pitch the platform, and convince them that their application will serve the mission of the organization, as well as any stakeholders, and end-users who will be leveraging the application. This process can really be a great filter for making sure developers think through what they are building, requiring them to put forth a coherent proposal, otherwise they will not be able to get full access to resources. This part of the process should be conducted early on in the application submission process, reducing frustrations for developers if their application is denied.</p>

<p><strong>Syncing The Legal Department</strong><br />
Also reflected in the Blue Button team’s approach is the syncing of the legal aspects of operating an API platform, and it’s applications. Making sure the application’s terms of service, privacy, security, cookie, branding, and other policies are in alignment with the platform. One good way of doing this is offering a white label edition of the platforms legal documents for use by the each application. Doing the heavy legal work for the application developers, while also making sure they are in sync when it comes to the legal details. Providing legal develop kits (LDK) will grow in prominence in the future, just like providing software development kits (SDK), helping streamline the legalities of operating a safe and secure API platform, with a wealth of applications in its service.</p>

<p><strong>Live or Virtual Presentation</strong><br />
Beyond the initial pitch selling an API provider on the concept of an application, I’ve seen many providers require an in-person, or virtual demo of the working application before it can be added to a production environment, and included in the application gallery. It can be tough for platform providers to test drive each application, so making the application owners do the hard work of demonstrating what an application does, and walking through all of its features is pretty common. I’ve participated on several judging panels that operate quarterly application reviews, as well as part of specific events, hackathons, and application challenges. Making demos a regular part of the application lifecycle is easier to do when you have dedicated resources in place, with a process to govern how it will all work in recurring batches, or on a set schedule.</p>

<p><strong>Getting Into The Code</strong><br />
As part of the application review process many API providers require that you actually submit your code for review via Github. Providing details on ALL dependencies, and performing code, dependency, and security scans before an application can be approved. I’ve also see this go as far as requiring the use of specific SDKs, frameworks, or include proxies within the client layer, and requiring all HTTP calls be logged as part of production applications. This process can be extended to include all cloud and SaaS solutions involved, limiting where compute, storage, and other resources can be operated. Requiring all 3rd party APIs in use be approved, or already on a white list of API providers before they can be put to use. This is obviously the most costly part of the application review process, but depending on how high the bar is being set, it is one that many providers will decide to invest in, ensuring the quality of all applications that run in a production environment.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/apple-app-review.png" align="right" width="40%" style="padding: 15px;" /></p>

<p><strong>Regular Review &amp; Reporting</strong><br />
One important thing about the application review process is that it isn’t a one time process. Even once an application is accepted an added into the production environment, this process will need to be repeated for each version release of the application, along with the changes to the API. Of course the renewal process might be shorter than the initial approval workflow, but auditing and regular check-in should be common, and not forgotten. This touches on the client level SDK, and API management logging needs of the platform, and that regular reporting upon application usage and activity should be available in real time, as well as part of each application renewal. API operations is always about taking advantage the real time awareness introduced at the API consumption layer, and staying in tune with the healthy, and not so healthy patterns that emerge from logging everything an application is doing.</p>

<p><strong>Business Model</strong><br />
It is common to ask application developers about their business model. The absence of a business model almost always reflects the underlying exploitation and sale of data being access or generated as part of application’s operation. Asking developers how they will make money and sustain their operations, along with regular checkins to make sure it is truly in effect, is an easy to ensure that applications are protecting the interests of the platform, its partners, and the applications end-users.</p>

<p>There are many other approaches I’ve seen API providers require before accepting an application into production. However, I think we should also be working hard to keep the process simple, and meaningful. Of course, we want a high bar for quality, but as with everything in the API world, there will always be compromises in how we deliver on the ground. Depending on the industry you are operating the bar will be made higher, or possibly lowered a little to allow for more innovation. I’ve included a list of some of the application review process I found across my research–showing a wide range of approaches across API providers we are all familiar with. Hopefully that helps you think through the application review process a little more. It is something I’ll write about again in the future as I push forward my research, and distill down more of the common building blocks I’m seeing across the API landscape.</p>

<p><strong>Some Leading Application Review Processes</strong><br /></p>

<ul>
  <li><a href="https://www.instagram.com/developer/review/">Instagram</a></li>
  <li><a href="https://developer.concur.com/manage-apps/app-certification.html">SAP Concur</a></li>
  <li><a href="https://developer.paypal.com/docs/classic/lifecycle/goingLive/">Paypal</a></li>
  <li><a href="https://developers.google.com/adsense/host/review_main">Adsense</a></li>
  <li><a href="https://help.shopify.com/api/listing-in-the-app-store/app-requirements-and-success-criteria/app-review-checklist">Shopify</a></li>
  <li><a href="https://developer.riotgames.com/application-process.html">Riot Games</a></li>
  <li><a href="https://api.slack.com/security-review">Slack</a></li>
  <li><a href="https://www.docusign.com/blog/dsdev-docusign-go-live-process-now-automated/">Docusign</a></li>
  <li><a href="http://dev.splunk.com/view/app-cert/SP-CAAAE8P">Splunk</a></li>
  <li><a href="https://go.developer.ebay.com/compatible-application-check-and-checklist-going-live">Ebay</a></li>
  <li><a href="https://developer.apple.com/app-store/review/">Apple</a></li>
</ul>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/some-common-features-of-an-api-application-review-process/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/code-generating-openapi-still-prevailing-approach/">Code Generation Of OpenAPI (fka Swagger) Still The Prevailing Approach</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/gears-numbers-blue.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>Over 50% of the projects I consult on still generate OpenAPI (fka Swagger) from code, rather then the other way around. When I first begin working with any API development group as an advisor, strategist, or governance architect I always ask, “are you using OpenAPI?” Luckily the answer is almost always yes. The challenge is that most of the time they don’t understand the full scope of how to use OpenAPI, and are still opting for the more costly approach–writing code, then generating OpenAPI from annotations. It has been over five years since Jakub Nesetril(@jakubnesetril) of Apiary first decoupled this way of doing API design first, but clearly we still have a significant amount of work when it comes to API definition and design literacy amongst development groups.</p>

<p>When you study where API services and tooling are headed it is clear that API deployment, and the actual writing of code is getting pushed further down in the life cycle. Services like Stoplight.io, and Postman are focusing on enabling a design, mock, document, test, and iterate approach, with API definitions (OpenAPI, Postman, etc) at the core. The actual deployment of API, either using open source frameworks, API gateways, or other method, is coming into the picture more downstream. Progressive API teams are hammering out exactly the API they need without ever writing any code, making sure the API design is dialed in before the more expensive, and often permanent code gets written and sent to production.</p>

<p>You will see me hammering on this line of API design first messaging on API Evangelist over the next year. Many developers still see OpenAPI (fka Swagger) about generating API documentation, not as the central contract that is used across every stop along the API lifecycle. Most do not understand that you can mock instead of deploying, and even provide mock data, errors, and other scenarios, allowing you to prototype applications on top of API designs. It will take a lot of education, and awareness building to get API developers up to speed that this is all possible, and begin the long process of changing behavior on the ground. Teams just are used to this way of thinking, but once they understand what is possible, they’ll realize what they have been missing.</p>

<p>I need to come up with some good analogies for generating API definitions from code. It really is an inefficient, and a very costly way to get the job done. Another problem is that this approach tends to be programming language focused, which always leaves its mark on the API design. I’m going to be working with both Stoplight.io and Postman to help amplify this aspect of delivering APIs, and how their services and tooling helps streamline how we develop our APIs. I’m going to be working with banks, insurance, health care, and other companies to improve how they deliver APIs, shifting things towards a design-first way of doing business. You’ll hear the continued drumbeat around all of this on API Evangelist in coming months, as I try to get the attention of folks down in the trenches, and slowly shift the behavior towards a better way of getting things done.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/code-generating-openapi-still-prevailing-approach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/the-growing-importance-of-github-topics-for-your-api-seo/">The Growing Importance of Github Topics For Your API SEO</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-topics-screenshot.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>When you are operating an API, you are always looking for new ways to be discovered. I study this aspect of operating APIs from the flip-side–how do I find new APIs, and stay in tune with what APIs are to? Historically we find APIs using ProgrammableWeb, Google, and Twitter, but increasingly Github is where I find the newest, coolest APIs. I do a lot of searching via Github for API related topics, but increasingly Github topics themselves are becoming more valuable within search engine indexes, making them an easy way to uncover interesting APIs.</p>

<p><a href="https://streamdata.io/blog/robust-market-data-apis-alphavantage/">I was profiling the market data API Alpha Vantage today</a>, and one of the things I always do when I am profiling an API, is I conduct a Google, and then secondarily, a Github search for the APIs name. Interestingly, <a href="https://github.com/topics/alpha-vantage">I found a list of Github Topics while Googling for Alpha Vantage API</a>, uncovering some interesting SDKs, CLI, and other open source solutions that have been built on top of the financial data API. Showing the importance of operating your API on Github, but also working to define a set of standard Github Topic tags across all your projects, and helping encourage your API community to use the same set of tags, so that their projects will surface as well.</p>

<p>I consider Github to be the most important tool in an API providers toolbox these days. I know as an API analyst, it is where I learn the most about what is really going on. It is where I find the most meaningful signals that allow me to cut through the noise that exists on Google, Twitter, and other channels. Github isn’t just for code. As I mention regularly, 100% of my work as API Evangelist lives within hundreds of separate Github repositories. Sadly, I don’t spend as much time as I should tagging, and organizing projects into meaningful topic areas, but it is something I’m going to be investing in more. Conveniently, I’m doing a lot of profiling of APIs for my partner Streamdata.io, which involves establishing meaningful tags for use in defining real time data stream topics that consumers can subscribe to–making me think a little more about the role Github topics can play.</p>

<p>One of these days I will do a fresh roundup of the many ways in which Github can be used as part of API operations. I’m trying to curate and write stories about everything I come across while doing my work. The problem is there isn’t a single place I can send my readers to when it comes to applying this wealth of knowledge to their operations. The first step is probably to publish Github as its own research area on Github (mind blown), as I do with my other projects. It has definitely risen up in importance, and can stand on its own feet alongside the other areas of my work. Github plays a central role in almost every stop along the API life cycle, and deserves its own landing page when it comes to my API research, and priority when it comes to helping API providers understanding what they should be doing on the platform to help make their API operations more successful.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/the-growing-importance-of-github-topics-for-your-api-seo/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

	<table width="100%" border="1" style="background-color:#FFF; border: 0px #FFF;">
		<tr style="background-color:#FFF; border: 0px #FFF;">
			<td align="left">
				<a href="/blog/page4" class="button"><< Prev</a></li>
			</td>
			<td></td>
			<td align="right">
				<a href="/blog/page6" class="button">Next >></a>
			</td>
		</tr>
	</table>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/" target="_blank"><img src="https://apievangelist.com/images/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/" class="image"><img src="https://apievangelist.com/images/postman-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
