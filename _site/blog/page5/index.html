<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }
    
    .container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
    }
    .video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }    

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/11/21/what-does-the-next-chapter-of-storytelling-look-like-for-api-evangelist/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-butterfly-vertical.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/11/21/what-does-the-next-chapter-of-storytelling-look-like-for-api-evangelist/">What Does The Next Chapter Of Storytelling Look Like For API Evangelist?</a></h3>
			<p><em>21 Nov 2018</em></p>
			<p>I find myself refactoring API Evangelist again this holiday season. Over the last eight years of doing API Evangelist I’ve had to regularly adjust what I do to keep it alive and moving forward. As I close up 2018, I’m finding the landscape shifting underneath me once again, pushing me to begin considering what the next chapter of API Evangelist will look like. Pushing me to adjust my presence to better reflect my own vision of the world, but hopefully also find balance with where things are headed out there in the real world. I started API Evangelist in July of 2010 to study the business of APIs. As I was researching things in 2010 and 2011 I first developed what I consider to be the voice of the API Evangelist, which continues to be the voice I use in my storytelling here in 2018. Of course, it is something that has evolved and matured over the years, but I feel I have managed to remain fairly consistent in how I speak about APIs throughout the journey. It is a voice I find very natural to speak, and is something that just flows on some days whether I want it to or not, but then also something I can’t seem to find at all on other days. Maintaining my voice over the last eight years has required me to constantly adjust and fine tune, perpetually finding the frequency required to keep things moving forward. First and foremost, API Evangelist is about my research. It is about me learning. It is about me crafting stories that help me distill down what I’m learning, in an attempt to articulate to some imaginary audience, which has become a real audience over the years. I don’t research stuff because I’m being paid (not always true), and I don’t tell stories about things I don’t actually find interesting (mostly true). API Evangelist is always about me pushing my skills forward...[<a href="/2018/11/21/what-does-the-next-chapter-of-storytelling-look-like-for-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/11/16/the-ability-to-link-to-api-service-provider-features-in-my-workshops-and/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109_201_800_500_0_max_0_-5_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/11/16/the-ability-to-link-to-api-service-provider-features-in-my-workshops-and/">The Ability To Link To API Service Provider Features In My Workshops And</a></h3>
			<p><em>16 Nov 2018</em></p>
			<p>All of my API workshops are machine readable, driven from a central YAML file that provides all the content and relevant links I need to deliver what I need during a single, or multi-day API strategy workshop. One of the common elements of my workshops are links out to relevant resource, providing access to services, tools, and other insight that supports whatever I’m covering in my workshop. There are two parts to this equation, 1) me knowing to link to something, and 2) being able to link to something that exists. A number of API services and tooling I use don’t follow web practices and do not provide any easy way to link to a feature, or other way of demonstrating the functionality that exists. The web is built on this concept, but along the way within web and mobile applications, we’ve have seemed to lose our understanding for this fundamental concept. There are endless situations where I’m using a service or tool, and think that I should reference in one of my workshops, but I can’t actually find any way to reference as a simple URL. Value buried within a JavaScript nest, operating on the web, but not really behaving like you depend on the web. Sometimes I will take screenshots to illustrate the features of a tool or service I am using, but I’d rather have a clean URL and bookmark to a specific feature on a services page. I’d rather give my readers, and workshop attendees the ability to do what I’m talking about, not just hear me talk about it. In a perfect world, every feature of a web application would have a single URL to locate said feature. Allowing me to more easily incorporate features into my storytelling and workshops, but alas many UI / UX folks are purely thinking about usability and rarely thinking about instruct-ability, and being able to cite and reference a feature externally, using the fundamental...[<a href="/2018/11/16/the-ability-to-link-to-api-service-provider-features-in-my-workshops-and/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/11/06/flickr-and-reconciling-my-history-of-apis-storytelling/"><img src="https://s3.amazonaws.com/kinlane-productions2/flickr/361347580_2d9d02b83d_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/11/06/flickr-and-reconciling-my-history-of-apis-storytelling/">Flickr And Reconciling My History Of APIs Storytelling</a></h3>
			<p><em>06 Nov 2018</em></p>
			<p>Flickr was one of the first APIs that I profiled back in 2010 when I started API Evangelist. Using their API as a cornerstone of my research, resulting in their API making it into my history of APIs storytelling, continuing to be a story I’ve retold hundreds of times in the conversations I’ve had over the eight years of being the API Evangelist. Now, after the second (more because of Yahoo?) acquisition, Flickr users are facing significant changes regarding the number of images we can store on the platform, and what we will be charged for using the platform–forcing me to step back, and take another look at the platform that I feel has helped significantly shape the API space as we know it. When I step back and think about Flickr, it’s most important contribution to the world of APIs was all about the resources it made available. Flickr was the original image sharing API, powering the growing blogosphere at the beginning of this century. Flickr gave us a simple interface for humans in 2004, and an API for other applications just six months later, that provided us all with a place to upload the images we would be using across our storytelling on our blogs. Providing the API resources that we would be needed to power the next decade of storytelling via our blogs, but also set into the motion the social evolution of the web, demonstrating that images were an essential building block of doing business on the web, and in just a couple of years, on the new mobile devices that would become ubiquitous in our lives. Flickr was an important API resource, because it provided access to an important resource–our images. The API allowed you to share these meaningful resources on your blog, via Facebook and Twitter, and anywhere else you wanted. In 2005, this was huge. At the time, I was working to make a shift from being an...[<a href="/2018/11/06/flickr-and-reconciling-my-history-of-apis-storytelling/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/11/01/the-impact-of-travel-on-being-the-api-evangelist/"><img src="https://s3.amazonaws.com/kinlane-productions2/IMG_7598.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/11/01/the-impact-of-travel-on-being-the-api-evangelist/">The Impact Of Travel On Being The API Evangelist</a></h3>
			<p><em>01 Nov 2018</em></p>
			<p>Travel is an important part of what I do. It is essential to striking up new relationships, and reenforcing old ones. It is important for me to get out of my bubble, expose myself to different perspectives, and see the world in different ways. I am extremely grateful for the ability to travel around the US, and the world the way that I do. I am also extremely aware of the impact that travel has on me being the API Evangelist–the positive, the negative, and the general shift in my tone in storytelling after roaming the world. One of the most negative impact that traveling has on my world is on my ability to complete blog posts. If you follow my work, when I’m in the right frame of mind, I can produce 5-10 blog posts across the domains I write for, on a daily basis. The words just do not flow in the same way when I am on the road. I’m not in a storyteller frame of mind. At least in the written form. When I travel, I am existing in a more physical and verbal sense as the API Evangelist, something that doesn’t always get translated into words on my blog(s). This is something that is ok for short periods of time, but after extended periods of time on the road, it is something that will begin to take a toll on my overall digital presence. After the storytelling impact, the next area to suffer when I am on the road, is my actual project work. I find it very difficult to write code, or think at architectural levels while on the road. I can flesh out and move forward smaller aspects of the projects I’m working on, but because of poor Internet, packed schedules, and the logistics of being on the road, my technical mind always suffers. This is something that is also related to the impact on my overall storytelling....[<a href="/2018/11/01/the-impact-of-travel-on-being-the-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/10/22/what-are-your-enterprise-api-capabilities/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/machine-road_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/10/22/what-are-your-enterprise-api-capabilities/">What Are Your Enterprise API Capabilities?</a></h3>
			<p><em>22 Oct 2018</em></p>
			<p>I spend a lot of time helping enterprise organizations discover their APIs. All of the organizations I talk to have trouble knowing where all of their APIs are–even the most organized of them. Development and IT groups have just been moving too fast over the last decade to know where all of their web services, and APIs are. Resulting in large organizations not fully understanding what all of their capabilities are, even if it is something they actively operate, and may drive existing web or mobile applications. Each individual API within the enterprise represents a single capability. The ability to accomplish a specific enterprise tasks that is valuable to the business. While each individual engineer might be aware of the capabilities present on their team, without group wide, and comprehensive API discovery across an organization, the extent of the enterprise capabilities is rarely known. If architects, business leadership, and any other stakeholder can’t browse, list, search, and quickly get access to all of the APIs that exist, the knowledge of the enterprise capabilities will not be able to be quantified or articulated as part of regular business operations. In 2018, the capabilities of any individual API is articulated by it’s machine readable definition. Most likely OpenAPI, but could also be something like API Blueprint, RAML, or other specification. For these definitions to speak to not just the technical capabilities of each individual API, but also the business capabilities, they will have to be complete. Utilizing a higher level strategic set of tags that help label and organize each API into a meaningful set of business capabilities that best describes what each API delivers. Providing a sort of business capabilities taxonomy that can be applied to each API’s definition and used across the rest of the API lifecycle, but most importantly as part of API discovery, and the enterprise digital product catalog. One of the first things I ask any enterprise organization I’m working with upon...[<a href="/2018/10/22/what-are-your-enterprise-api-capabilities/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/10/22/join-me-for-a-fireside-chat-at-the-paris-api-meetup-this-wednesday/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/paris-api-meetup/DqJd3bkJ.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/10/22/join-me-for-a-fireside-chat-at-the-paris-api-meetup-this-wednesday/">Join Me For A Fireside Chat At The Paris API Meetup This Wednesday</a></h3>
			<p><em>22 Oct 2018</em></p>
			<p>I am in Europe for most of October, and while I am in Paris we thought it would be a good idea to pull together a last minute API Meetup. Romain Simiand (@RomainSimiand), the API Evangelist at PeopleDoc was gracious enough to help pull things together, and the Streamdata.io team is stepping up to help with food and drink. Pulling together a last minute gathering at PeopleDoc in Paris, and bringing me on stage to talk about the technology, business, and politics of APIs, well as about some of my recent work on API discovery, and event-driven architecture. You can find more details on the Paris API Meetup site, with directions on how to find PeopleDoc. Make sure you RSVP so that we know you are coming, and of course, please help spread the word. We are over 30 people attending so far, but I think we can do better. I’m happy to get on stage and help drive the API discussion, but I’d prefer to have a healthy representation of the Paris API community asking questions, helping me understand what is happening across the area when it comes to APIs. I always have plenty of knowledge to share, but it becomes exponentially more valuable when people on the ground within communities are asking questions, and making it relevant to what is happening within the day to day operations of companies in the local area. While I enjoy doing conference keynotes and panels, my favorite format of event is the Meetup. Bringing together less than 100 people have a discussion about APIs. I always find that I learn the most in this environment, and able to actually engage with developers and business folks about what really matters when it comes to APIs. The larger the audience the more it is just about me broadcasting my message, and when it is a smaller and more intimate venue, I feel like I can better connect with people....[<a href="/2018/10/22/join-me-for-a-fireside-chat-at-the-paris-api-meetup-this-wednesday/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/10/22/i-participated-in-an-api-workshop-with-the-european-commission-last-week/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apis4dgov/DpyR9qrXoAAYo4r.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/10/22/i-participated-in-an-api-workshop-with-the-european-commission-last-week/">I Participated In An API Workshop With The European Commission Last Week</a></h3>
			<p><em>22 Oct 2018</em></p>
			<p>I was in Ispra, Italy last week for a two day workshop on APIs with the European Commission. The European Commission’s DG CONNECT together with the Joint Research Centre (JRC) launched a study with the purpose to gain further understanding of the current use of APIs in digital government and their added value for public services, and they invited me to participate. I was joined by Mehdi Medjaoui (@medjawii), David Berlind (@dberlind), and Mark Boyd (@mgboydcom), along with EU member states, and European cities, to help provide feedback and strategies for consideration by the commission. This European Commission study is looking at “innovative ways to improve interconnectivity of public services and reusability of public sector data, including dynamic data in real-time, safeguarding the data protection and privacy legislation in place.” Looking to: assess digital government APIs landscape and opportunities to support the digital transformation of public sector identify the added value for society and public administrations of digital government APIs (key enablers, drivers, barriers, potential risks and mitigates) define a basic Digital Government API EU framework and the next steps David Berlind from ProgrammableWeb gave a couple talks, with myself, Mehdi, and Mark following up. The rest of the time spent was hearing presentations from EU member states, and other municipal efforts–learning more about the successes and the challeges they face. What I heard reflected what I’ve experienced in federal government, as well as city, county, and state level API efforts I’ve participated in across the United States. &lt;p&gt;&lt;/p&gt;All groups were struggling to win over leaders and the public, modernize legacy system, build on top of open data efforts, and push forward the conversation using a modern approach to delivering web APIs. I am eager to see what comes out of the European Commission API project. While there are still interesting things happening in the United States, I feel like there is an opportunity for the EU to leap frog us when it comes to...[<a href="/2018/10/22/i-participated-in-an-api-workshop-with-the-european-commission-last-week/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-discovery/"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/kin-lane-api-days-spain.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-discovery/">API Evangelist API Lifecycle Workshop on API Discovery</a></h3>
			<p><em>11 Oct 2018</em></p>
			<p>I’ve been doing more workshops on the API lifecycle within enterprise groups lately. Allowing me to refine my materials on the ground within enterprise groups, further flesh out the building blocks I recommend to API groups to help them craft their own API strategy. One of the first discussions I start with large enterprise groups is always in the area of API discovery, or commonly asked as, “do you know where all your APIs are?” EVERY group I’m working with these days is having challenges when it comes to easy discovery across all the digital resources they possess, and put to use on a daily basis. I’m working with a variety of companies, organizations, institutions, and government agencies when it comes to the API discovery of their digital self: Low Hanging Fruit (outline) - Understanding what resources are already on the public websites, and applications, by spidering existing domains looking for data assets that should be delivered as API resources. Discovery (outline) - Actively looking for web services and APIs that exist across an organization, industry, or any other defined landscape. Documenting, aggregating, and evolving what is available about each API, while also publishing back out and making available relevant teams. Communication (outline) - Having a strategy for reaching out to teams and engaging with them around API discovery, helping the remember to register and define their APIs as part of wider strategy. Definitions (outline) - Work to make ensure that all definitions are being aggregated as part of the process so that they can be evolved and moved forward into design, development and production–investing in all of the artifacts that will be needed down the road. Dependencies (outline) - Defining any dependencies that are in play, and will play a role in operations. Auditing the stack behind any service as it is being discovered and documented as part of the overall effort. Support (outline) - Ensure that all teams have support when it comes...[<a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-discovery/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-design/"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/43043431_10156747264069813_2487933138479611904_n.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-design/">API Evangelist API Lifecycle Workshop on API Design</a></h3>
			<p><em>11 Oct 2018</em></p>
			<p>I’ve been doing more workshops on the API lifecycle within enterprise groups lately. Allowing me to refine my materials on the ground within enterprise groups, further flesh out the building blocks I recommend to API groups to help them craft their own API strategy. One area of the API lifecycle I find more groups working on these days, centers around a design-first approach to the API lifecycle. While not many groups I work with achieved a design-first approach doing APIs, almost all of them I talk to express interest in making this a reality at least within some groups, or projects. The appeal of being able to define, design, mock, and iterate upon an API contract before code gets written is very appealing to enterprise API groups, and I’m looking to help them think through this part of their API lifecycle, and work towards making API design first a reality at their organization. Definition (outline) - Using definitions as the center of the API design process, developing an OpenAPI contract for moving things through the design phase, iterating, evolving, and making sure the definitions drive the business goals behind each service. Design (outline) - Considering the overall approach to design for all APIs, executing upon design patterns that are in use to consistently deliver services across teams. Leveraging a common set of patterns that can be used across services, beginning with REST, but also evetually allowing the leveraging of hypermedia, GraphQL, and other patterns when it comes to the deliver of services. Versioning (outline) - Managing the definition of each API contract being defined as part of the API design stop for this area of the lifecycle, and having a coherent approach to laying out next steps. Virtualization (outline) - Providing mocked, sandbox, and virtualized instances of APIs and other data for understanding what an API does, helping provide an instance of an API that reflects exactly how it should behave in a production environment. Testing (outline) - Going beyond...[<a href="/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-design/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/10/09/the-layers-of-completeness-for-an-openapi-definition/"><img src="https://gist.github.com/kinlane/5e52d6063a0744d711795beb6e60365f.js" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/10/09/the-layers-of-completeness-for-an-openapi-definition/">The Layers Of Completeness For An OpenAPI Definition</a></h3>
			<p><em>09 Oct 2018</em></p>
			<p>Everyone wants their OpenAPIs to be complete, but what that really means will depend on who you are, what your knowledge of OpenAPI is, as well as being driven by your motivation for having an OpenAPI in the first place. I wanted to take a crack at articulating a complete(enough) definition for OpenAPIs I create, based upon what I’m needing them to do. Info &amp; Base - Give the basic information I need to understand who is behind, and where I can access the API. Paths - Provide an entry for every path that is available for an API, and should be included in this definition. Parameters - Provide a complete list of all path, query, and header parameters that can be used as part of an API. Descriptions - Flesh out descriptions for all the path and parameter descriptions, helping describe an API does. Enums - Publish a list of all the enumerated values that are possible for each parameter used as part of an API. Definitions - Document the underlying schema being returned by creating a JSON schema definition for the API. Responses - Associate the definition for the API with the path using a response reference, connecting the dots regarding what will be returned. Tags - Tag each path with a meaningful set of tags, describing what resources are available in short, concise terms and phrases. Contacts - Provide contact information for whoever can answer questions about an API, and provide a URL to any support resources. Create Security Definitions - Define the security for accessing the API, providing details on how each API request will be authenticated. Apply Security Definitions - Apply the security definition to each individual path, associating common security definitions across all paths. Complete(enough) - That should give us a complete (enough) API description. Obviously there is more we can do to make an OpenAPI even more complete and precise as a business contract, hopefully speaking to both...[<a href="/2018/10/09/the-layers-of-completeness-for-an-openapi-definition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/10/09/a-quick-manual-way-to-create-an-openapi-from-a-get-api-request/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-definition-stories/xignite-api-url.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/10/09/a-quick-manual-way-to-create-an-openapi-from-a-get-api-request/">A Quick Manual Way To Create An OpenAPI From A GET API Request</a></h3>
			<p><em>09 Oct 2018</em></p>
			<p>I have numerous tools that help me create OpenAPIs from the APIs I stumble across each day. Ideally I’m crawling, scraping, harvesting, and auto-generating OpenAPIs, but inevitably the process gets a little manual. To help reduce friction in these manual processes, I try to have a variety of services, tools, and scripts I can use to make my life easier, when it comes to create a machine readable definition of an API I am using–in this scenario it is the xignite CloudAlerts API. One way I’ll create an OpenAPI from a simple GET API request, providing me with a machine readable definition of the surface area of that API, is using Postman. When you have the URL copied onto your clipboard, open up your Postman, and paste the URL with all the query parameters present. You’ll have to save your API request, and add it to a collection, but then you can choose to share the collection, and retrieve the URL to this specific requests Postman Collection. This gives you a machine readable definition of the surface area of this particular API, defining the host, baseURL, path, and parameters, but it doesn’t give me more detail about the underlying schema being returned. To begin crafting the schema for the underlying definition of the API, and connect it to the response for my API definition, I’ll need an OpenAPI–which I can create from my Postman Collection using API Transformer from APIMATIC. After pasting the URL for the Postman Collection into the API transformer form, you can generate an OpenAPI from the definition. Now you have an OpenAPI, except it is missing the underlying schema, which I will just grab the response from my last request, and convert it into JSON schema using JSONSchema.net. I’ll just grab the properties section of these, as the bottom definitions portion of the OpenAPI specification is just JSON Schema. I can merge my JSON schema with my OpenAPI, adding it to...[<a href="/2018/10/09/a-quick-manual-way-to-create-an-openapi-from-a-get-api-request/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/10/08/api-evangelist-and-streamdata-io-api-lifecycle-workshops/"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/kin-lane-api-days-spain.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/10/08/api-evangelist-and-streamdata-io-api-lifecycle-workshops/">API Evangelist And Streamdata.io API Lifecycle Workshops</a></h3>
			<p><em>08 Oct 2018</em></p>
			<p>I have been partnering with Streamdata.io to evolve how I work with enterprise groups on their API lifecycle strategy. After working closely with the Streadata.io sales team, it became clear that many enterprise organizations weren’t quite ready for the event-driven infrastructure Streamdata.io provides. Most groups were in desperate need of stepping back and developing their own formal strategy for delivering APIs across the enterprise, before they could every scale their operations and take advantage of things being more event-driven and real time. In response, I set out to evolve my own API lifecycle research, gathered over the last eight years of studying the API space, and make it more accessible to the enterprise, as self-service short form and long form content, in-person workshops, and forkable blueprints that any enterprise can set in motion on their own. The results is a series of evolvable API projects, that we are using to drive our ongoing workshop engagements with enterprise API groups, focusing in on six critical areas of the API lifecycle: Discovery (demo) - Knowing where all of your APIs and services are across groups. Design (demo) - Focus in on an a design and virtualized API lifecycle before deployment. Development (demo) - Understanding the many ways in which APIs can be developed &amp; deployed. Production (demo) - Thinking critically about properly operating API infrastructure. Governance (demo) - Understanding how to measure, report, and evolve API operations. Not all of our workshops will cover all of these areas. Depending on the time we have available, the scope of the team participating in a workshop(s), and how far along teams are in their overall API journey, the workshops might head in different directions, and expand or contract the depth in which we dive into each of these area (ie. not everyone is ready for governance). After several workshops this year, we have found these areas of the API lifecycle to be the most meaningful ways to organize a...[<a href="/2018/10/08/api-evangelist-and-streamdata-io-api-lifecycle-workshops/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/28/api-developer-outreach-research-for-the-department-of-veterans-affairs/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109_214_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/28/api-developer-outreach-research-for-the-department-of-veterans-affairs/">API Developer Outreach Research For The Department of Veterans Affairs</a></h3>
			<p><em>28 Sep 2018</em></p>
			<p>This is a write-up for research I conducted with my partner Skylight Digital. The team conducted a series of interviews with leading public and private sector API platforms regarding how they approached developer outreach, and then I wrote it up as a formal report, which the Skylight Digital team then edited and polished. We are looking to provide as much information as possible regarding how the VA, and other federal agencies should consider crafting their API outreach efforts. This is Skylight’s report for the U.S. Department of Veterans Affairs (VA) microconsulting project, “Developer Outreach.” The VA undertook this project in order to better understand how private- and public-sector organizations approach Application Programming Interface (API) developer outreach. In preparing this report, we drew on nearly a decade’s worth of our own API developer outreach expertise, as well as information obtained through interviews with seven different organizations. For each interview, we followed an interview script (see Appendix A) and took notes. The Centers for Medicare &amp; Medicaid Services (CMS) Blue Button API program (see Appendix B), the Census Bureau (see Appendix C), the OpenFEC program (see Appendix D), and Salesforce (see Appendix E) all agreed to releasing our notes publicly. The other three organizations (a large social networking site, a government digital services delivery group, and a cloud communications platform) preferred to keep them private. We structured this report to largely reflect the interview conversations that we held with people who are involved in developer outreach programs and activities. These conversations focused around the following questions: What is the purpose and scope of your API developer outreach program? What does success look like for you? What are the key elements or practices (e.g, documentation, demos, webinars, conferences, blog posts) that you are using to drive and sustain effective adoption of your API(s)? Do you make use of an API developer sandbox to drive and sustain adoption? If so, please describe how you’ve designed that environment to be...[<a href="/2018/09/28/api-developer-outreach-research-for-the-department-of-veterans-affairs/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/20/having-the-dedication-to-lead-an-api-effort-forward-within-a-large-enterprise/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/97_193_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/20/having-the-dedication-to-lead-an-api-effort-forward-within-a-large-enterprise/">Having The Dedication To Lead An API Effort Forward Within A Large Enterprise</a></h3>
			<p><em>20 Sep 2018</em></p>
			<p>I work with a lot of folks who work in large enterprise organizations, institutions, and government agencies who are moving the API conversation forward within their groups. I’m all too familiar with what it takes to move forward the API conversation within large, well established enterprise organizations. However, I am the first to admit that while I have a deep understanding of what it involves, I do not have the fortitude to actually lead an effort for the sustained amount of time it takes to actually make change. I just do not have the patience and the personality for it, and I’m eternally grateful for those that do. There are regular streams of emails in my inbox from people embedded within enterprise organizations, looking for guidance, counseling, and assistance in moving forward the API conversation at their organizations. I am happy to provide assistance in an advisory capacity, and consulting with groups to help them develop their strategies. A significant portion of my income comes from conducting 1-3 day workshops within the enterprise, helping teams work through what they need to. There is one thing I cannot contribute to any of these teams, the dedication and perseverance it will need to actually make it happen. It takes a huge amount of organization knowledge to move things forward at a large organization. You have to know who the decision makers are, and who are the gatekeepers for all of the important resources–this is knowledge you have to acquire by being embedded, and working within an organization for a very long time. You just can’t walk in the door and be able to make sense of things within days, or weeks. You have to be able to work around schedules, and personalities–getting to know people, and truly begin to understand their motivations, and willingness to contribute, or whether they’ll actually decide to work against you. The culture of any enterprise organization will be the most important area...[<a href="/2018/09/20/having-the-dedication-to-lead-an-api-effort-forward-within-a-large-enterprise/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/18/understanding-the-eventdriven-api-infrastructure-opportunity-that-exists/"><img src="https://s3.amazonaws.com/kinlane-productions2/kong/kong-summit-2018.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/18/understanding-the-eventdriven-api-infrastructure-opportunity-that-exists/">Understanding The Event-Driven API Infrastructure Opportunity That Exists</a></h3>
			<p><em>18 Sep 2018</em></p>
			<p>I am at the Kong Summit in San Francisco all day tomorrow. I’m going to be speaking about research into the event-driven architectural layers I’ve been mapping out across the API space. Looking for the opportunity to augment existing APIs with push technology like webhooks, and streaming technology like SSE, as well as pipe data in an out of Kafka, fill data lakes, and train machine learning models. I’ll be sharing what I’m finding from some of the more mature API providers when it comes to their investment in event-driven infrastructure, focusing in on Twilio, SendGrid, Stripe, Slack, and GitHub. As I am profiling APIs for inclusion in my API Stack research, and in the API Gallery, I create an APIs.json, OpenAPI, Postman Collection(s), and sometimes an AsyncAPI definition for each API. All of my API catalogs, and API discovery collections use APIs.json + OpenAPI by default. One of the things I profile in each of my APIs.json, is the usage of webhooks as part of API operations. You can see collections of them that I’ve published to the API Gallery, aggregating many different approaches in what I consider to be the 101 of event-driven architecture, built on top of existing request and response HTTP API infrastructure. Allowing me to better understand how people are doing webhooks, and beginning to sketch out plans for a more event-driven approach to delivering resources, and managing activity on any platform that is scaling. While studying APIs at this level you begin to see patterns across how providers are doing what they are doing, even amidst a lack of standards for things like webhooks. API providers emulate each other, it is how much of the API space has evolved in the last decade. You see patterns like how leading API providers are defining their event types. Naming, describing, and allowing API consumers to subscribe to a variety of events, and receive webhook pings or pushes of data, as well...[<a href="/2018/09/18/understanding-the-eventdriven-api-infrastructure-opportunity-that-exists/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/18/talking-healthcare-apis-with-the-cms-blue-button-api-team-at-apistrat-in/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/blue-button/blue-button-api-docs.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/18/talking-healthcare-apis-with-the-cms-blue-button-api-team-at-apistrat-in/">Talking Healthcare APIs With The CMS Blue Button API Team At #APIStrat In</a></h3>
			<p><em>18 Sep 2018</em></p>
			<p>We have the API evangelist from one of the most significant APIs out there today at #APIStrat in Nashville next week. Mark Scrimshire (@ekivemark), Blue Button Innovator and Developer Evangelist from NewWave Telecoms and Technologies will be on the main stage next Tuesday, September 25th 2018. Mark will be bringing his experience helping stand up the Blue Button API with the Centers for Medicare and Medicaid Services (CMS), and sharing the stories from the trenches while delivering this critical piece of health API infrastructure within the United States. I consider the Blue Button API to be one of the most significant APIs out there right now for several key factors: API Reach - An API that has potential to reach 44 million Medicare beneficiaries, which is 15 percent of the U.S. population–that is a pretty significant audience to reach when it comes to the overall API conversation. Fast Healthcare Interoperability Resources (FHIR) - The Blue Button API supports Hl7 / FHIR, pushing the specification forward in the overall healthcare API interoperability discussion, making it extremely relevant to APIStrat and the OpenAPI Initiative (OAI). Government API Blueprint - The way in which the Blue Button API team at CMS and USDS is delivering the API is providing a potential blueprint that other federal and stage level agencies can follow when rolling out their own Medicare related APIs, but also any other critical infrastructure that this country depends on. This is why I am always happy to support the Blue Button API team in any way I can, and I am very stoked to have them at APIStrat in Nashville next week. I’ve spent a lot of time working with, and studying what the Blue Button API team is up to, and I spoke at their developer conference hosted at the White House last month. They have some serious wisdom to share when it comes to delivering public APIs at this scale, making the keynote with Mark...[<a href="/2018/09/18/talking-healthcare-apis-with-the-cms-blue-button-api-team-at-apistrat-in/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/17/sadly-stack-exchange-blocks-api-calls-being-made-from-any-of-amazons-ip-block/"><img src="https://s3.amazonaws.com/kinlane-productions2/stack-exchange/stack-exchange-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/17/sadly-stack-exchange-blocks-api-calls-being-made-from-any-of-amazons-ip-block/">"Sadly Stack Exchange Blocks API Calls Being Made From Any Of Amazons IP Block"</a></h3>
			<p><em>17 Sep 2018</em></p>
			<p>I am developing an authentication and access layer for the API Gallery that I am building for Streamdata.io, while also federating it for usage as part of my API Stack research. In addition to building out these catalogs for API discovery purposes, I’m also developing a suite of tools that allow users to subscribe to different topics from popular sources like GitHub, Reddit, and Stack Overflow (Exchange). I’ve been busy adding one or two providers to my OAuth broker each week, until the other day I hit a snag with the Stack Exchange API. I thought my Stack Exchange API OAuth flow had been working, it’s been up for a few months, and I seem to remember authenticating against it before, but this weekend I began getting an error that my IP address was blocked. I was looking at log files trying to understand if I was making too many calls, or some other potential violation, but I couldn’t find anything. Eventually I emailed Stack Exchange to see what their guidance once, to which I got a prompt reply: “Yes, we block all of Amazon’s AWS IP addresses due to the large amount of abuse that comes from their services. Unfortunately we cannot unblock those addresses at this time.” Ok then. I guess that is that. I really don’t feel like setting up another server with another provider just so I can run an OAuth server from there. Or, maybe I guess I might have to if I expect to offer a service that provides OAuth integration with Stack Exchange. It’s a pretty unfortunate situation that doesn’t make a whole lot of sense. I can understand adding another layer of white listing for developers, pushing them to add their IP address to their Stack Exchange API application, and push us to justify that our app should have access, but blacklisting an entire cloud provider from accessing your API is just dumb. I am going to...[<a href="/2018/09/17/sadly-stack-exchange-blocks-api-calls-being-made-from-any-of-amazons-ip-block/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/17/justifying-my-existence-in-your-api-sales-and-marketing-funnel/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/32_39_600_400_0_avg_1_1_1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/17/justifying-my-existence-in-your-api-sales-and-marketing-funnel/">Justifying My Existence In Your API Sales And Marketing Funnel</a></h3>
			<p><em>17 Sep 2018</em></p>
			<p>I feel like I’m regularly having to advocate for my existence, and the existence of developers who are like me, within the sales and marketing funnel for many APIs. I sign up for a lot of APIs, and have the pleasure of enjoy a wide variety of on-boarding processes for APIs. Many APIs I have no problem signing up, on-boarding, and beginning to make calls, while others I have to just my existence within their API sales and marketing funnel. Don’t get me wrong, I’m not saying that I shouldn’t be expected to justify my existence, it is just that many API providers are setup to immediately discourage, create friction for, and dismiss my class of API integrator–that doesn’t fit neatly into the shiny big money integration you have defined at the bottom of your funnel. I get that we all need to make money. I have to. I’m actually in the business of helping you make money. I’m just saying that you are missing out on a significant amount of opportunity if you only focus on what comes out the other side of your funnel, and discount the nutrients developers like me can bring to your funnel ecosystem. I’m guessing that my little domain apievangelist.com does return the deal size scope you are looking for, but I think you are putting too much trust into the numbers provided to you by your business intelligence provider. I get that you are hyper focused on making the big deals, but you might be leaving a big deal on the table by shutting out small fish, who might have oversized influence within their organization, government agency, or within an industry. Your business intelligence is focusing on the knowns, and doesn’t seem very open to considering the unknowns. As the API Evangelist I have an audience. I’ve been in business since 2010, so I’ve built up an audience of enterprise folks who read what I write, and listen...[<a href="/2018/09/17/justifying-my-existence-in-your-api-sales-and-marketing-funnel/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/17/i-am-needing-some-evidence-of-how-apis-can-make-an-impact-in-government/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/capital-battle_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/17/i-am-needing-some-evidence-of-how-apis-can-make-an-impact-in-government/">I Am Needing Some Evidence Of How APIs Can Make An Impact In Government</a></h3>
			<p><em>17 Sep 2018</em></p>
			<p>Eric Horesnyi (@EricHoresnyi), the CEO of Streamata.io and I were on a call with a group of people who are moving forward the API conversation across Europe, with the assistance of the EU. The project has asked us to assist them in the discovery of more data and evidence of how APIs are making an impact in how government operates within the European Union, but also elsewhere in the world. Aggregating as much evidence as possible to help influence the EU API strategy, and learn from what is already being done. I’m heading to Italy next month to present to the group, and participate in conversations with other API practitioners and evangelists, so I wanted to start my usual amount of storytelling here on the blog to solicit contributions from my audience about what they are seeing. I am looking for some help from my readers who work at city, county, state, and federal agencies, or at the private entities who help them with their API efforts. I am looking for official, validated, on the record examples of APIs making a positive impact on how government serves its constituents. Quantifiable examples of how a government agency have published a private, partner, or public API, and it helped the agency better meet its mission. I’m looking for anything mundane, as well as the unique and interesting, with tangible evidence to back it all up. Like number of developers, partners, apps, cost saving, efficiencies, or any other positive effect. Demonstrating that APIs when done right can move the conversation forward at a government agency. For this round, I’m going to need first hand accounts, because I will need to help organize the data, and work with this group to submit it to the European Union as part of their wider effort. This is something I’ve been doing loosely since 2012, but I need to start getting more official about how I gather the stories, and pull together...[<a href="/2018/09/17/i-am-needing-some-evidence-of-how-apis-can-make-an-impact-in-government/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/17/being-open-is-more-about-being-open-so-someone-can-extract-value-than-open/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_blue_circuit_5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/17/being-open-is-more-about-being-open-so-someone-can-extract-value-than-open/">Being Open Is More About Being Open So Someone Can Extract Value Than Open</a></h3>
			<p><em>17 Sep 2018</em></p>
			<p>One of the most important lessons I’ve learned in the last eight years, is that when people are insistent about things being open, in both accessibility, and cost, it is often more about things remaining open for them to freely (license-free) to extract value from it, that it is ever about any shared or reciprocal value being generated. I’ve fought many a battle on the front lines of “open”, leaving me pretty skeptical when anyone is advocating for open, and forcing me to be even critical of my own positions as the API Evangelist, and the bullshit I peddle. In my opinion, ANYONE wielding the term open should be scrutinized for insights into their motivations–me included. I’ve spend eight years operating on the front line of both the open data, and the open API movements, and unless you are coming at it from the position of a government entity, or from a social justice frame of mind, you are probably wanting open so that you can extract value from whatever is being opened. With many different shades of intent existing when it comes to actually contributing any value back, and supporting the ecosystem around whatever is actually being opened. I ran with the open data dogs from 2008 through 2015 (still howl and bark), pushing for city, county, state, and federal government open up data. I’ve witnessed how everyone wants it opened, sustained, maintained, and supported, but do not want to give anything back. Google doesn’t care about the health of local transit, as long as the data gets updated in Google Maps. Almost every open data activist, and data focused startup I’ve worked with has a high expectation for what government should be required to do, and want very low expectations regarding what should be expected of them when it comes to pay for commercial access, sharing enhancements and enrichments, providing access to usage analytics, and be observable and open to sharing access to...[<a href="/2018/09/17/being-open-is-more-about-being-open-so-someone-can-extract-value-than-open/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/13/providing-minimum-viable-api-documentation-blueprints-to-help-guide-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-definitions-support.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/13/providing-minimum-viable-api-documentation-blueprints-to-help-guide-your-api/">Providing Minimum Viable API Documentation Blueprints To Help Guide Your API</a></h3>
			<p><em>13 Sep 2018</em></p>
			<p>I was taking a look at the Department of Veterans Affairs (VA) API documentation for the VA Facilities API, and intending on providing some feedback on the API implementation. The API itself is pretty sound, and I don’t have any feedback without having actually integrated it into an application, but following on the heals of my previous story about how we get API developers to follow minimum viable API documentation guidance, I had lots of feedback on the overall deliver of the documentation for the VA Facilities API, helping improve on what they have there. Provide A Working Example of Minimum Viable API Documentation One of the ways that you help incentivize your API developers to deliver minimum viable API documentation across their API implementations is you do as much of the work for them as you can, and provide them with a forkable, downloadable, clonable API documentation that meets the minimum viable requirements. To help illustrate what I’m talking about I created a base GitHub blueprint for what I’d suggest as a minimum viable API documentation at the VA. Providing something the VA can consider, and borrow from as they are developing their own strategy for ensuring all APIs are consistently documented. Covering The Bare Essentials That Should Exist For All APIs I wanted to make sure each API had the bare essentials, so I took what the VA has already done over at developer.va.gov, and republished it as a static single page application that runs 100% on GitHub pages, and hosted in a GitHub repository–providing the following essential building blocks for APIs at the VA: Landing Page - Giving any API a single landing page that contains everything you need to know about working with an API. The landing page can be hosted as its own repo, and subdomain, and the linked up with other APIs using a facade page, or it could be published with many other APIs in a single repository....[<a href="/2018/09/13/providing-minimum-viable-api-documentation-blueprints-to-help-guide-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/13/please-refer-the-engineer-from-your-api-team-to-this-story/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/mosaic-face_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/13/please-refer-the-engineer-from-your-api-team-to-this-story/">Please Refer The Engineer From Your API Team To This Story</a></h3>
			<p><em>13 Sep 2018</em></p>
			<p>I reach out to API providers on a regular basis, asking them if they have an OpenAPI or Postman Collection available behind the scenes. I am adding these machine readable API definitions to my index of APIs that I monitor, while also publishing them out to my API Stack research, the API Gallery, APIs.io, work to get them published in the Postman Network, and syndicated as part of my wider work as an OpenAPI member. However, even beyond my own personal needs for API providers to have a machine readable definition of their API, and helping them get more syndication and exposure for their API, having an definition present significantly reduces friction when on-boarding with their APIs at almost every stop along a developer’s API integration journey. One of the API providers I reached out to recently responded with this, “I spoke with one of our engineers and he asked me to refer you to https://developer.[company].com/”. Ok. First, I spend over 30 minutes there just the other day. Learning about what you do, reading through documentation, and thinking about what was possible–which I referenced in my email. At this point I’m guessing that the engineer in question doesn’t know what an OpenAPI or Postman Collection is, they do not understand the impact these specifications are having on the wider API ecosystem, and lastly, I’m guessing they don’t have any idea who I am(ego taking control). All of which provides me with the signals I need to make an assessment of where any API is in their overall journey. Demonstrating to me that they have a long ways to go when it comes to understanding the wider API landscape in which they are operating in, and they are too busy to really come out of their engineering box and help their API consumers truly be successful in integrating with their platform. I see this a lot. It isn’t that I expect everyone to understand what OpenAPI...[<a href="/2018/09/13/please-refer-the-engineer-from-your-api-team-to-this-story/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/10/stack-exchange-has-an-api-that-returns-the-details-for-all-of-your-access/"><img src="https://s3.amazonaws.com/kinlane-productions2/stack-exchange/stack-exchange-access-tokens-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/10/stack-exchange-has-an-api-that-returns-the-details-for-all-of-your-access/">Stack Exchange Has An API That Returns The Details For All Of Your Access</a></h3>
			<p><em>10 Sep 2018</em></p>
			<p>I’m a big fan of helpful authentication features, where API providers make it easier to manage our increasingly hellish environment, application, token, and other management duties of the average API integrator. To help me better manage my API apps, and the OAuth tokens I have in play, I am trying to document all the sensible approaches I come across while putting different APIs to work, and scouring the API landscape for stories. One example of this in action is out of the Stack Exchange API, where you can find an API endpoint for accessing the details of your OAuth tokens, and invalidate, and de-authorize them. A pretty useful API endpoint when you are integrating with APIs, and find yourself having to manage many tokens across many APIs, apps, and users. Helping you check in on the overall health and activity of your tokens, revoking, renewing, and making sure they work when you need them the most. It is helpful for me to write about the helpful authentication practices I come across while using APIs. It helps me aggregate them into a nice list of features API providers should consider supporting. If I don’t write about it here on the blog, then it doesn’t exist in my research, and my future storytelling. My goal is to help spread the knowledge about what is working across the sector, so that more API providers will adopt along the way. You know what is better than Stack Exchange providing an API to manage your access tokens? All API providers providing you with an API to manage your access tokens! These stories, and any other relevant links I’ve curated will be published to my API authentication research. Eventually I’ll roll all the features I’ve aggregated into either a long form blog post, or white paper I’ll publish and put out with the assistance of one of my partners. I’m interested in the authentication portion of this, but also I’m looking...[<a href="/2018/09/10/stack-exchange-has-an-api-that-returns-the-details-for-all-of-your-access/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/10/some-ideas-for-api-discovery-collections-that-students-can-use/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/priorities/university-of-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/10/some-ideas-for-api-discovery-collections-that-students-can-use/">Some Ideas For API Discovery Collections That Students Can Use</a></h3>
			<p><em>10 Sep 2018</em></p>
			<p>This is a topic I’ve wanted to set in motion for some time now. I had a new university professor city my work again as part of one of their courses recently, something that floated this concept to the top of the pile again–API discovery collections meant for just for students. Helping k-12, community college, and university students quickly understand where to find the most relevant APIs to whatever they are working on. Providing human, but also machine readable collections that can help jumpstart their API education. I use the API discovery format APIs.json to profile individual, as well as collections of APIs. I’m going to kickstart a couple of project repos, helping me flesh out a handful of interesting collections that might help students better understand the world of APIs: Social - The popular social APIs like Twitter, Facebook, Instagram, and others. Messaging - The main messaging APIs like Slack, Facebook, Twitter, Telegram, and others. Rock Star - The cool APIs like Twitter, Stripe, Twilio, YouTube, and others. Amazon Stack - The core AWS Stack like EC2, S3, RDS, DynamoDB, Lambda, and others. Backend Stack - The essential App stack like AWS S3, Twilio, Flickr, YouTube, and others. I am going to start there. I am trying to provide some simple, usable collections or relevant APIs for students are just getting started If there are any other categories, or stacks of APIs you think would be relevant for students to learn from I’d love to hear your thoughts. I’ve done a lot of writing about educational and university based APIs, but I’ve only lightly touched upon what APIs should students be learning about in the classroom. Providing ready to go API collections will be an important aspect of the implementation of any API training and curriculum effort. Having the technical details of the API readily available, as well as the less technical aspects like signing up, pricing, terms of service, privacy policies, and other...[<a href="/2018/09/10/some-ideas-for-api-discovery-collections-that-students-can-use/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/07/the-path-to-production-for-department-of-veteran-affairs-va-api-applications/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/los-angeles-downtow-freeway_blue_circuit_5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/07/the-path-to-production-for-department-of-veteran-affairs-va-api-applications/">The Path To Production For Department of Veteran Affairs (VA) API Applications</a></h3>
			<p><em>07 Sep 2018</em></p>
			<p>This post is part of my ongoing review of the Department of Veteran Affairs (VA) developer portal and API presence, moving on to where I take a closer look at their path to production process, and provide some feedback on how the agency can continue to refine the information they provide to their new developers. Helping map out the on-boarding process for any new developer, ensuring they are fully informed about what it will take to develop an application on top of VA APIs, and move those application(s) from a developer state to a production environment, and actually serving veterans. Beginning with the VA’s base path to production template on GitHub, then pulling in some elements I found across the other APIs they have published to developer.va.gov, and finishing off with some ideas of my own, I shifted the outline for the path to production to look something like this: Background - Keeping the background of the VA API program. [API Overview] - Any information relevant to specific API(s). Applications - One right now, but eventually several applications, SDK, and samples. Documentation - The link, or embedded access to the API documentation, OpenAPI definition, and Postman Collection. Authentication - Overview of how to authenticate with VA APIs. Development Access - Provide an overview of signing up for development access. Developer Review - What is needed to become a developer. Individual - Name, email, and phone. Business - Name, URL. Location - In country, city, and state. Application Review - What is needed to have an application(s). Terms of Service - In alignment with platform TOS. Privacy Policy - In alignment with platform TOS. Rate Limits - Aware of the rate limits that are imposed. Production Access - What happens once you have production access. Support &amp; Engagement - Using support, and expected levels of engagement. Service Level Agreement - Platform working to meet an SLA governing engagement. Monthly Review - Providing monthly reviews of access...[<a href="/2018/09/07/the-path-to-production-for-department-of-veteran-affairs-va-api-applications/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/07/an-api-value-generation-funnel-with-metrics/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-value-generation-funnel.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/07/an-api-value-generation-funnel-with-metrics/">An API Value Generation Funnel With Metrics</a></h3>
			<p><em>07 Sep 2018</em></p>
			<p>I’ve had several folks asking me to articulate my vision of an API centric “sales” funnel, which technically is out of my wheelhouse in the sales and marketing area, but since I do have lots opinions on what a funnel should look like for an API platform, I thought I’d take a crack at it. To help articulate what is in my head I wanted to craft a narrative, as well as a visual to accompany how I like to imagine a value generation funnel for any API platform. I envision a API-driven value generation funnel that can be tipped upside down, over and over, like an hour glass, generating value is it repeatedly pushes API activity through center, driven by a healthy ecosystem of developers, applications, and end-users putting applications to work / use. Providing a way to generate awareness and engagement with any API platform, while also ensuring a safe, reliable, and secure ecosystem of applications that encourage end-user adoption, engagement, and loyalty–further expanding on the potential for developers to continue developing new applications, and enhancing their applications to better serve end-users. I am seeing things in ten separate layers right now, something I’ll keep shifting and adjusting in future iterations, but I just wanted to get a draft funnel out the door: Layer One - Getting folks in the top of the funnel. Awareness - Making people aware of the APIs that are available. Engagement - Getting people engaged with the platform in some way. Conversation - Encouraging folks to be part of the conversation. Participation - Getting developers participating on regular basis. Layer Two Developers - Getting developers signing up and creating accounts. Applications - Getting developers signing up and creating applications. Layer Three Sandbox Activity - Developers being active within the sandbox environment. Layer Four Certifed Developers - Certifying developers in some way to know who they are. Certified Application - Certifying applications in some way to ensure quality. Layer...[<a href="/2018/09/07/an-api-value-generation-funnel-with-metrics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/06/my-api-storytelling-depends-on-the-momentum-from-regular-exercise-and-practice/"><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/141-Post+Con+2018-Speakers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/06/my-api-storytelling-depends-on-the-momentum-from-regular-exercise-and-practice/">My API Storytelling Depends On The Momentum From Regular Exercise And Practice</a></h3>
			<p><em>06 Sep 2018</em></p>
			<p>I’ve been falling short of my normal storytelling quotas recently. I like to have at least 3 posts on API Evangelist, and two posts on Streamdata.io each day. I have been letting it slip because it was summer, but I will be getting back to my regular levels as we head into the fall. Whenever I put more coal in the writing furnace, I’m reminded of just how much momentum all of this takes, as well as the regular exercise and practice involved, allowing me to keep pace in the storytelling marathon across my blog(s). The more stories I tell, the more stories I can tell. After eight years of doing this, I’m still surprised abut what it takes to pick things back up, and regain my normal levels of storytelling. If you make storytelling a default aspect of doing work each day, finding a way to narrate your regular work with it, it is possible to achieve high volumes of storytelling going out the door, generating search engine and social media traffic. Also, if you root your storytelling in the regular work you are already doing each day, the chances it will be meaningful enough for people to tune in only increases. My storytelling on API Evangelist is important because it helps me think through what I’m working on. It helps me become publicly accessible by generating more attention to my work, firing up new conversations, and reenforces the existing ones I’m already having. When the storytelling slows, it means I’m either doing a unhealthy amount of coding or other work, or my productivity levels are suffering overall. This makes my API storytelling a heartbeat of my operations, and a regular stream of storytelling reflects how healthy my heartbeat is from regular exercise, and usage of my words (instead of code). I know plenty of individuals, and API related operations that have trouble finding their storytelling voice. Expressing that they just don’t have the...[<a href="/2018/09/06/my-api-storytelling-depends-on-the-momentum-from-regular-exercise-and-practice/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/06/allowing-users-to-get-their-own-oauth-tokens-for-accessing-an-api-without-the/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-personal-access-token.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/06/allowing-users-to-get-their-own-oauth-tokens-for-accessing-an-api-without-the/">Allowing Users To Get Their Own OAuth Tokens For Accessing An API Without The</a></h3>
			<p><em>06 Sep 2018</em></p>
			<p>I run a lot of different applications that depend on GitHub, and use GitHub authentication as the identity and access management layer for these apps. One of the things I like the most about GitHub and how I feel it handles it’s OAuth more thoroughly than most other platforms, is how they let you get you own OAuth token under your settings &gt; developer settings &gt;personal access tokens. You don’t need to setup an application, and do the whole OAuth dance, you just get a token that you can use to pass along with each API call. I operate my own OAuth server which allows me to authenticate using OAuth with many leading APIs, so generating an OAuth token, and setting up a new provider isn’t too hard. However, it is always much easier to go under my account settings, create a new personal access token for a specific purpose, and get to work playing with an API. I wish that ALL API providers did this. At first glance, it looks like GitLab, Harvest, TypeForm, and ContentFul all provide personal access tokens as a first option for on-boarding with their APIs. Demonstrating this is more of a pattern, than just a GitHub feature. One of these days I’m going to have to do another story documenting the entire GitHub OAuth system, because they have a lot of interesting bells and whistles that make using their platform much more secure, and just a more frictionless experience than other API providers I use on a regular basis. GitHub has ground down a lot of the sharp corners on the whole authentication experience when it comes to OAuth. It would make a nice blueprint to share, and work to convince other API providers it is a pattern worth following. Reducing the cognitive load around OAuth management for any API integration, and standardizing how API providers support their API consumers, and end-users. I have 3 separate Twitter Apps setup...[<a href="/2018/09/06/allowing-users-to-get-their-own-oauth-tokens-for-accessing-an-api-without-the/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/04/what-have-you-done-for-us-lately-api-partner-edition/"><img src="https://s3.amazonaws.com/kinlane-productions2/streamdata/streamdata-api-partners-philosophy.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/04/what-have-you-done-for-us-lately-api-partner-edition/">What Have You Done For Us Lately (API Partner Edition)</a></h3>
			<p><em>04 Sep 2018</em></p>
			<p>I’ve been working on developing and evolving the Streamdata.io partner program, trying to move forward conversations with other service providers in the space that have existed long before I started working on things, as well as other newer relationships that I’ve helped bring in. I’m fascinated by how partner programs work, or do not work, and have invested a lot of time trying to optimize and improve how I do my own operations, and assist my partners and clients in evolving and delivering on their own partner vision. It is difficult to establish, and continue meaningful and balanced partnerships between technology service and tooling providers. Sometimes providers have enough compatibility and synergy, that they are able to hit the ground running with meaningful activities that strengthen, and build partnership momentum. We are trying to establish a meaningful, yet effective way of measuring partner activity, and understanding the value that is being generated, and where reciprocity exists. Looking at the following activities produced by Streamdata.io and it’s partners: Partner Page - Being published to both of our partner pages. Testimonials - Providing quotes for each other about our services. Blog Posts - Publishing blog posts about partnership and each others services. White Papers - Publishing white papers or guides about partnership and each others services. Press Releases - Working on join press releases about partnership and each others services. Integrations - Publishing open source repositories demonstrating integration and usage of each others services. Workshops - Conduct workshops for each others customers, helping deliver each others services within our ecosystems. Business - Actually provide business referrals from our customers, and conversations occurring across both companies. There are other activities we like to see happening, but these eight areas represent the most common exchanges we encourage amongst our partners. The trick is always pushing for reciprocity across all these areas, help deliver on a balanced partnership, and make sure there is equal value being generated for both sides...[<a href="/2018/09/04/what-have-you-done-for-us-lately-api-partner-edition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/04/the-federal-agencies-who-use-their-developer-domain-gov-subdomain/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/federal-goverment-portals.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/04/the-federal-agencies-who-use-their-developer-domain-gov-subdomain/">The Federal Agencies Who Use Their developer.[domain].gov Subdomain</a></h3>
			<p><em>04 Sep 2018</em></p>
			<p>I was reviewing the new developer portal for the Department of Veterans Affairs (VA), and one of things I took notice of, was their use of the developer.va.gov subdomain. In my experience, the API efforts that invest in a dedicated subdomain, and specifically a developer dot subdomain, tend to more invested in what they are doing than efforts that publish to a subfolder, or subsection of their website. As I was writing this post, I had a question in arise in my mind, regarding how many other federal agencies use a dedicated subdomain for their developer programs–something I wanted to pick up later, and understand the landscape a little more. I took a list of current federal agency domains from the GSA and wrote a little script to append developer. to each of the domains, and conduct an HTTP status code check to see whether or not these pages existed. Here are the dedicated developer areas I found for the US federal government: Department of Veterans Affairs (VA) - https://developer.va.gov/ Department of Labor - https://developer.dol.gov International Trade Administration (ITA) - https://developer.trade.gov &amp; https://developer.export.gov United States Patent and Trademark Office - https://developer.uspto.gov National Renewable Energy Laboratory - https://developer.nrel.gov Centers for Medicare &amp; Medicaid Services - https://developer.cms.gov The Advanced Distributed Learning Initiative - http://developer.adlnet.gov &amp; http://developers.adlnet.gov United States Environmental Protection Agency - http://developer.epa.gov USA Jobs - http://developer.usajobs.gov These nine agencies have decided to invest in a subdomain for their developer portals. I have to recognize two others who provide these subdomains, but then redirect to a subsection of their websites: National Park Service - http://developer.nps.gov redirect to https://www.nps.gov/subjects/developer/index.htm Data.gov - http://developer.data.gov redirects to https://www.data.gov/developers/ Additionally, there is a single domain I noticed that used the plural version of the subdomain: Code.gov - https://developers.code.gov (plural) Along the way, I also noticed that many agencies would redirect their subdomain, and I assume all subdomains to the root of their agency’s domain. Ideally, all federal agencies would have a Github...[<a href="/2018/09/04/the-federal-agencies-who-use-their-developer-domain-gov-subdomain/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/04/the-basics-of-the-va-api-feedback-loop/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-github-issue-production-api-access-request.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/04/the-basics-of-the-va-api-feedback-loop/">The Basics Of The VA API Feedback Loop</a></h3>
			<p><em>04 Sep 2018</em></p>
			<p>I’m working to break down the moving parts of API efforts over at the VA, and work to provide as much relevant feedback as I possibly can. One of the components I’m wanting to think about more is the feedback loop for the VA API efforts. The feedback loop is one of the most essential aspects of doing an API, and is quickly can become one of the most debilitating, paralyzing, and nutrient starving aspects of operating an API platform if done wrong, or non-existent. However, the feedback loop is also one of the most valuable reasons for wanting to do APIs in the first place, providing the essential feedback you will need from consumers, and the entire API ecosystem to move the API forward in a meaningful way. Current Seeds Of The VA API Feedback Loop Current the VA is supporting the VA API developer portal using GitHub Issues and email. I mentioned in my previous review of the VA API developer portal that the personal email addresses provided for email support should be generalized, sharing the load when it comes to email support for the platform. Next, I’d like to address the usage of GitHub issues for support, along with email, and step back to look at how this contributes to, or could possibly take away from the overall feedback loop for the VAPI API effort. Defining what the basics of an API feedback loop for the VA might be. Expanding Upon The VA Usage Of GitHub Issues I support the usage of GitHub issues for public support of any API related project. It provides a simple, observable way for anyone to get support around the VA APIs. While I’m guessing it was accidental, I like the specialization of the current repo, and usage of GitHub issues, and that it being dedicated to VA API clients and their needs. I’d encourage this type of continued focus when it comes to establishing additional feedback...[<a href="/2018/09/04/the-basics-of-the-va-api-feedback-loop/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/09/04/remembering-that-apis-are-used-to-reduce-everything-down-to-a-transaction/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/reduce+everything+to+a+transaction_tr.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/09/04/remembering-that-apis-are-used-to-reduce-everything-down-to-a-transaction/">Remembering That APIs Are Used To Reduce Everything Down To A Transaction</a></h3>
			<p><em>04 Sep 2018</em></p>
			<p>This is our regular reminder that APIs are not good, nor bad, nor neutral. They are simply a tool in our technological toolbox, and something that is often used for very dark reasons, and occasionally for good. One of the most dangerous things I’ve found about APIs is just the general thought process that goes along with them, regarding how all roads lead to reducing, and distilling things down to a single transaction. APIs, REST, microservices, and other design patterns are all about taking something from our physical world, and redefining it as something that can be transmitted back and forth using the low cost request and response infrastructure of the web. No matter what you are designing your API for, your mission is to reduce everything to a simple transaction that can be exchanged between your server, and any other system, web, mobile, device, or network application. This digital resource could be a photo of your kids, a message to your mother, the balance of your bank account, your personal thoughts going into your notebook, the latest song you listened to, your DNA, your test results for cancer, or any other piece of relevant data, content, media, object, or other resource that is being sent or received online. APIs are all about reducing all of our meaningful digital bits to the smallest possible transaction, and then daisy chaining them together to produce some desired set of results. This API-ification of everything can be a good thing. It can make our lives better, but one of the negative side effects of this reducing of everything to a transaction, is that now that transaction can be bought and sold. The digitization of everything in our lives is rarely ever about making our lives better and whatever the reasons we are told up front, and almost always are about reducing that little piece of our lives to a transaction that can be quantified, have a value place...[<a href="/2018/09/04/remembering-that-apis-are-used-to-reduce-everything-down-to-a-transaction/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/30/why-i-feel-the-department-of-veterans-affairs-api-effort-is-so-significant/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-developer-portal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/30/why-i-feel-the-department-of-veterans-affairs-api-effort-is-so-significant/">Why I Feel The Department Of Veterans Affairs API Effort Is So Significant</a></h3>
			<p><em>30 Aug 2018</em></p>
			<p>I have been working on API and open data efforts at the Department of Veterans Affairs (VA) for five years now. I’m passionate about pushing forward the API conversation at the federal agency because I want to see the agency deliver on its mission to take care of veterans. My father, and my step-father were both vets, and I lived through the fallout from my step-fathers two tours in Vietnam, exposure to the VA healthcare and benefits bureaucracy, and ultimately his passing away from cancer which he acquired from to his exposure to Agent Orange. I truly want to see the VA streamline as many of its veteran facing programs as they possibly can. I’ve been evangelizing for API change and leadership at the VA since I worked there in 2013. I’m regularly investing unpaid time to craft stories that help influence people I know who are working at the VA, and who are potentially reading my work. Resulting in posts like my response to the VA’s RFI for the Lighthouse API management platform, which included a round two response a few months later. Influence through storytelling is the most powerful tool I got in my API evangelist toolbox. This Is An Amazon Web Services Opportunity The most popular story on my blog is, “The Secret to Amazon’s Success–Internal APIs”. Which tells a story of the mythical transition of Amazon from an online commerce website to the cloud giant, who is now powering a significant portion of the web. The story is mostly fiction, but continues to be the top performing story on my blog six years later. I’ve heard endless conference talks about this subject, I’ve seen my own story framed on the wall in enterprise organizations in Europe and Australia, and as a feature link on internal IT portals. This is one of the most important stories we have in the API sector, and what is happening at the VA right now will...[<a href="/2018/08/30/why-i-feel-the-department-of-veterans-affairs-api-effort-is-so-significant/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/29/understanding-where-folks-are-coming-from-when-they-say-api-management-is/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_19_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/29/understanding-where-folks-are-coming-from-when-they-say-api-management-is/">Understanding Where Folks Are Coming From When They Say API Management Is</a></h3>
			<p><em>29 Aug 2018</em></p>
			<p>I am always fascinated when I get push back from people about API management, the authentication, service composition, logging, analysis, and billing layer on the world of APIs. I seem to be find more people who are skeptical that it is even necessary anymore, and that it is a relic of the past. When I first started coming across the people making these claims earlier this year I was confused, but as I’ve pondered on the subject more, I’m convinced their position is more about the world of venture capital, and investment in APIs, that it is about APIs. People who feel like you do not need to measure the value being exchanged at the API layer aren’t considering the technology or business of delivering APIs. They are simply focused on the investment cycles that are occurring, and see API management as something that has been done, it is baked into the cloud, and really isn’t central to API-driven businesses. They perceive that the age of API management as being over, it is something the cloud giants are doing now, thus it isn’t needed. I feel like this is a symptom of tech startup culture being so closely aligned with investment cycles, and the road map being about investment size and opportunity, and rarely the actual delivery of the thing that brings value to companies, organizations, institutions, and government agencies. I feel this perception is primarily rooted in the influence of investors, but it is also based upon a limited understanding of API management, and seeing APIs being a about delivering public APIs, maybe with a complimenting a SaaS offering, and a free, pro, and enterprise tiers of access. When in reality API management is about measuring, quantifying, reporting upon, and in some cases billing for the value that is exchanged at the system integration, web, mobile, device, and network application levels. However, to think API operators shouldn’t be measuring, quantifying, reporting, and generating revenue...[<a href="/2018/08/29/understanding-where-folks-are-coming-from-when-they-say-api-management-is/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/29/api-portals-designed-for-api-provider-and-api-consumers/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/66_189_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/29/api-portals-designed-for-api-provider-and-api-consumers/">API Portals Designed For API Provider And API Consumers</a></h3>
			<p><em>29 Aug 2018</em></p>
			<p>I’ve been working a couple organizations who are struggling with providing information within their API developer portal intended for API publishers, pushing their API portal beyond just being for their API consumers. Some of the folks I’ve been working with haven’t thought about their API developer portals being for both API publishers and consumers, and asked me to weigh in on the pros and cons of doing this. Helping them understand how they can continue their journey towards not just being an API platform, but also an API marketplace. Some of the conversations we were having about providing API lifecycle materials to API developers, helping them deliver APIs consistently across all stops along lifecycle, focused on creating a separate portal for API publishers, decoupling them from where the APIs would be discovered and consumed. After some discussion, and consideration, it feels like it would be an unnecessary disconnect, to have API publishers going to a different location than where their APIs would end up being discovered, and integrated with. That having them actively involved in the publishing, presentation, and even support of, and engagement with consumers would benefit everyone involved. Think of it being like Rapid API, but a large company, organization, institution, or government agency. You can find APIs, and integrate with existing APIs, or you can also become an API publisher, and be someone who helps publish and manage APIs as well. You will have one account, but you can find documentation, usage information and other resources for the APIs you consume, but then you will also access information, and usage data on the APIs you’ve published. Pushing API developers within an organization to actively think about both sides of the API coin, and learn how to be both provider and consumer. Helping add to the catalog of APIs, but also helping evolve and grow the army of API people across an organization. We still have a lot of work ahead of us...[<a href="/2018/08/29/api-portals-designed-for-api-provider-and-api-consumers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/28/trying-to-define-an-algorithm-for-my-aws-api-cost-calculations-across-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/4882162452_fa3126b38d_b_aqua.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/28/trying-to-define-an-algorithm-for-my-aws-api-cost-calculations-across-api/">Trying To Define An Algorithm For My AWS API Cost Calculations Across API</a></h3>
			<p><em>28 Aug 2018</em></p>
			<p>I am trying to develop a base base API pricing formula for determining what my hard costs are for each API I’m publishing using Amazon RDS, EC2, and AWS API Gateway. I also have some APIs I am deploying using Amazon RDS, Lambda, and AWS API Gateway, but for now I want to get a default base for determining what operating my APIs will cost me, so I can mark up and reliably generate profit on top of the APIs I’m making available to my partners. AWS has all the data for me to figure out my hard costs, I just need a formula that helps me accurately determine what my AWS bill will be per API. Math isn’t one of my strengths, so I’m going to have to break this down, and simmer on things for a while, before I will be able to come up with some sort of working formula. Here are my hard costs for what my AWS resources will cost me, for three APIs I have running currently in this stack: AWS RDS - I am running a db.r3.large instance which costs me $0.29 per hour or 211.70 per month, with the bandwidth free to my Amazon EC2 instances in the same availability zone. I do not have any public access, so I don’t have any incoming or outgoing traffic, except from the EC2 instance. AWS EC2 - I am running a t2.large instance which costs me $0.0928 per hour or $67.74 per month with bandwidth out costing me $0.155 per GB. I’m curious if they have an Amazon EC2 to AWS API Gateway data consideration? I couldn’t find anything currently. AWS API Gateway - Overall using AWS API Gateway costs me $3.50 per million API calls received, with the first 1GB costing me $0.00/GB, and costing me $0.09/GB for the next 9.999 TB. Across these three APIs, I am seeing an average of 5KB per responses, which is an...[<a href="/2018/08/28/trying-to-define-an-algorithm-for-my-aws-api-cost-calculations-across-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/28/reviewing-the-department-of-veterans-affairs-new-developer-portal/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-developer-portal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/28/reviewing-the-department-of-veterans-affairs-new-developer-portal/">Reviewing The Department Of Veterans Affairs New Developer Portal</a></h3>
			<p><em>28 Aug 2018</em></p>
			<p>I wanted to take a moment and review the Department Of Veterans Affairs (VA) new developer portal. Spending some time considering at how far they’ve come, what they published so far, and brainstorm on what the future might hold. Let me open by saying that I am working directly and indirectly with the VA on a variety of paid projects, but I’m not being paid to write about their API effort–that is something I’ve done since I worked there in 2013. I will craft a disclosure to this effect that I put at the bottom of each story I write about the VA, but I wanted to put out there in general, as I work through my thoughts on what is happening over at the VA. The VA Has Come A Long Way I wanted to open up this review with a nod towards how far the VA has come. There have been other publicly available APIs at the VA, as well as a variety of open data efforts, but the VA is clearly going all in on APIs with this wave. The temperature at VA in 2013 when it came to APIs was lukewarm at best. With the activity I’m seeing at the moment, I can tell that the temperature of the water has gone way up. Sure, the agency still has a long way to go, but from what I can tell, the leadership is committed to the agencies API journey–something I have not seen before. Developer.VA.Gov Sends The Right Signal It may not seem like much, but providing a public API portal at developer.va.gov sends a strong signal that the VA is seriously investing in their API effort. I see a lot of API programs, and companies who have a dedicated domain, or subdomain for their API operations are always more committed than people who make it just a subsection of their existing website, or existing as a help entry in a...[<a href="/2018/08/28/reviewing-the-department-of-veterans-affairs-new-developer-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/28/not-all-companies-are-interested-in-the-interoperability-that-apis-bring/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/fence_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/28/not-all-companies-are-interested-in-the-interoperability-that-apis-bring/">Not All Companies Are Interested In The Interoperability That APIs Bring</a></h3>
			<p><em>28 Aug 2018</em></p>
			<p>I’ve learned a lot in eight years of operating API Evangelist. One of the most important things I’ve learned to do is separate my personal belief and interest in technology from the realities of the business world. I’ve learned that not all businesses are ready for the change that doing APIs bring, and that many businesses really aren’t interested in the interoperability, portability, and observability that APIs bring to the table. Despite what I may believe, APIs in the real world often have a very different outcome. I see the potential of having a public API developer portal where you publish all the digital resources your company offers. Providing self-service registration to access these digital resources at a fair, transparent, and pay for what you use pricing model. I get what this can do for companies when it comes to attracting developer talent to help deliver the applications that are needed for any platform to thrive. I’ve seen the benefits to the end-users of these applications when it comes to giving them control over their data, the ability to leverage 3rd party applications, while also better understanding, managing, and ultimately owning the digital resources they generate each day. I also regularly see how this all can be a serious threat to how some businesses operate, and work to reveal the illnesses that exist within many businesses, and the shady things the occur behind the firewall each day. I regularly see businesses pay lip service to the concept of APIs, but in reality, are more about locking things up, and slowing things down to their benefit, instead of opening up access, and streamlining anything. I’m not saying that businesses do this by default, and are always being led from the top down to behave this way, I am saying it gets baked into the fabric of how teams, groups, and individuals cells in the overall organizational organism. These cells learn to resist, fight back, appear like...[<a href="/2018/08/28/not-all-companies-are-interested-in-the-interoperability-that-apis-bring/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/28/helping-the-federal-government-get-in-tune-with-their-api-uptime-and/"><img src="https://s3.amazonaws.com/kinlane-productions2/apimetrics/api-metrics-federal-government-1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/28/helping-the-federal-government-get-in-tune-with-their-api-uptime-and/">Helping The Federal Government Get In Tune With Their API Uptime And</a></h3>
			<p><em>28 Aug 2018</em></p>
			<p>Nobody likes to be told that their APIs are unreliable, and unavailable on a regular basis. However, it is one of those pills that ALL APIs have to swallow, and EVERY API provider should be paying for an external monitoring service to tell us when are APIs are up or down. Having a monitoring service to tell us when our APIs are having problems, complete with a status dashboard, and history of our API's availability are essential building blocks of any API provider. If you expect consumers to use your API, and bake it into their systems and applications, you should committed to a certain level of availability, and offering a service level agreement if possible. My friends over at APImetrics monitor APIs across multiple industries, but we've been partnering to keep an eye on federal government APIs, in support of my work in DC. They've recently [shared an informative dashboard tracking on the performance of federal government APIs](https://apimetrics.io/us-government-api-performance-dashboard/), providing an interesting view of the government API landscape, and the overall reliability of APIs they provide. They continue by breaking down the performance of federal government APIs, including how the APIs perform across multiple North American regions across four of the leading cloud providers: Helping us visualize the availability of federal government APIs for the last seven days, by applying their APImetrics CASC score to each of the federal government APIs, and ranking their overall uptime and availability: I know it sucks being labeled as one of the worst performing APIs, but you also have the opportunity to be named one the best performing APIs. ;-) This is a subject that many private sector companies struggle with, and the federal government has an extremely poor track record for monitoring their APIs, let alone sharing the information publicly. Facing up to this stuff sucks, and you are forced to answer some difficult questions about your operations, but it is also something can't be ignored away when...[<a href="/2018/08/28/helping-the-federal-government-get-in-tune-with-their-api-uptime-and/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/27/provide-your-api-developers-with-a-forkable-example-of-api-documentation-in/"><img src="https://s3.amazonaws.com/kinlane-productions2/va-working/va-demo-swagger-ui-documentation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/27/provide-your-api-developers-with-a-forkable-example-of-api-documentation-in/">Provide Your API Developers With A Forkable Example of API Documentation In</a></h3>
			<p><em>27 Aug 2018</em></p>
			<p>I responded about how teams should be documenting their APIs when they have both legacy and new APIs the other day. I wanted to keep the conversation thread going with an example of one possible API documentation implementation. The best way to deliver API documentation guidance in any organization is to provide a forkable, downloadable example of whatever you are talking about. To help illustrate what I am talking about, I wanted to take one documentation solution, and publish it as a GitHub repository. I chose to go with a simple OpenAPI 3.0 defined API contract, driving a Swagger UI driven API documentation, hosted using GitHub Pages, and managed as a GitHub repository. In my story about how teams should be documenting their APIs, I provided several API definition formations, and API documentation options–for this walk-through I wanted to narrow it down to a single combination, providing the minimum(alist) viable options possible using OpenAPI 3.0 and SwaggerUI. Of course, any federal agency implementing such a solution should wrap the documentation with their own branding, similar to the City Pairs API prototype out of GSA, which originated over at CFPB. I used the VA Facilities API definition from the developer.va.gov portal for this sample. Mostly because it was ready to go, and relevant to the VA efforts, but also because it was using OpenAPI 3.0–I think it is worth making sure all API documentation moving forward supports is supporting the latest version of OpenAPI. The API documentation is here, the OpenAPI definition is here, and the Github repository is here, showing what is possible. There are plenty of other things I’d like to see in a baseline API documentation template, but this provides a good first draft for a true minimum viable definition. The goal with this project is to provide a basic seed that any team could use. Next, I will add in some other building blocks, and implementation a ReDoc, DapperDox, or WSDLDoc version....[<a href="/2018/08/27/provide-your-api-developers-with-a-forkable-example-of-api-documentation-in/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/27/how-do-we-get-api-developers-to-follow-the-minimum-viable-api-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions2/va-working/api-documentation-guidance-1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/27/how-do-we-get-api-developers-to-follow-the-minimum-viable-api-documentation/">How Do We Get API Developers To Follow The Minimum Viable API Documentation</a></h3>
			<p><em>27 Aug 2018</em></p>
			<p>After providing some guidance the other day on how teams should be documenting their APIs, one of the follow up comments was: “Now we just have to figure out how to get the developers to follow the guidance!” Something that any API leadership and governance team is going to face as they work to implement new policies across their organization. You can craft the best possible guidance for API design, deployment, management, and documentation, but it doesn’t mean anyone is actually going to follow your guidance. Moving forward API governance within any organization represents the cultural frontline of API operations. Getting teams to learn about, understand, and implement sensible API practices is always easier said than done. You may think your vision of the organizations API future is the right one, but getting other internal groups to buy into that vision will take a significant amount of work. It is something that will take time, resources, and be something that will always be shifting and evolving over time. Lead By Example The best way to get developers to follow the minimum viable API documentation guidance being set forth is to do the work for them. Provide templates and blueprints of what you want them to do. Develop, provide, and evolve forkable and downloadable API documentation examples, with simple README checklists of what is expected of them. I’ve published a simple example using the VA Facilities API definition published as OpenAPI 3.0 and Swagger UI to Github Pages, with the entire thing forkable via the Github repository. It is very bare bones example of providing API documentation guidance is a package that can be reused, providing API developers with a working example of what is expected of them. Make It A Group Effort To help get API developers on board with the minimum viable API documentation guidance being set forth, I recommend making it a group effort. Recruit help from developers to improve upon API documentation...[<a href="/2018/08/27/how-do-we-get-api-developers-to-follow-the-minimum-viable-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/27/do-not-miss-internal-developer-portals-developer-engagement-behind-the/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/Kristof-Van-Tomme.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/27/do-not-miss-internal-developer-portals-developer-engagement-behind-the/">Do Not Miss Internal Developer Portals: Developer Engagement Behind the</a></h3>
			<p><em>27 Aug 2018</em></p>
			<p>
We are getting closer to the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th. The schedule for the conference is up, along with the first lineup of keynote speakers, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is “Do Not Miss Internal Developer Portals: Developer Engagement Behind the Firewall” by Kristof Van Tomme (@kvantomme), Pronovix (@pronovix) on September 25th.

Here is Kristof’s abstract for the API session:

While there are a lot of talks and blogposts about APIs and the importance of an APIs Developer eXperience, most are about public API products. And while a lot of the best practices for API products are also applicable to private APIs, there are significant differences in the circumstances and trade-offs they need to make.
The most important difference is probably in their budgets: as potential profit centers, API products can afford to invest a lot more money in documentation and UX driven developer portal improvements. Internal APIs rarely have that luxury.

In this talk I will explain the differences between public and private APIs, introduce upstream DX, and explain how it can improve downstream DX. Introduce experience design (a.k.a. gamification) and Innersourcing (open sourcing practices behind the firewall) and describe how they could be used on internal developer portals.

Kristof is an expert in delivering developer portals and API documentation, making his talk a must attend session. You can register for the event here, and there are still sponsorship opportunities available. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

I am looking forward to seeing you all in Nashville next month!

[<a href="/2018/08/27/do-not-miss-internal-developer-portals-developer-engagement-behind-the/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/24/may-contain-nuts-the-case-for-api-labeling-by-erik-wilde-dret-api-academy/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/erik-wilde.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/24/may-contain-nuts-the-case-for-api-labeling-by-erik-wilde-dret-api-academy/">May Contain Nuts: The Case for API Labeling by Erik Wilde (@dret), API Academy</a></h3>
			<p><em>24 Aug 2018</em></p>
			<p>
We are getting closer to the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th. The schedule for the conference is up, along with the first lineup of keynote speakers, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is “May Contain Nuts: The Case for API Labeling“ by Erik Wilde (@dret), API Academy (@apiacademy) on September 25th.

I’ll let Erik’s bio set the stage for what he’ll be talking about at APIStrat:

Erik is a frequent speaker at both industry and academia events. In his current role at the API Academy, his work revolves around API strategy, design, and management, and how to help organizations with their digital transformation. Based on his extensive background in Web architecture and technologies, Erik combines deep expertise in protocols and representations with insights into API practices at today’s organizations.

Before joining API Academy and working in the API space full-time, Erik spent time at Siemens and EMC, in both cases working at ways how APIs could be used for their internal service ecosystems, as well as for better ways for customers to use services and products. Before that, Erik spent most of his life in academia, working at UC Berkeley and ETH Zürich. Erik received his Ph.D. in computer science from ETH Zürich, and his diploma in computer science from TU Berlin.

Erik nows his stuff, and can be found on the road with the CA API Academy, making this stop in Nashville, TN a pretty special opportunity. You can register for the event here, and there are still sponsorship opportunities available. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

I am looking forward to seeing you all in Nashville next month!

[<a href="/2018/08/24/may-contain-nuts-the-case-for-api-labeling-by-erik-wilde-dret-api-academy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/24/living-in-a-post-facebook-twitter-and-instagram-api-world/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_19_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/24/living-in-a-post-facebook-twitter-and-instagram-api-world/">Living In A Post Facebook, Twitter, and Instagram API World</a></h3>
			<p><em>24 Aug 2018</em></p>
			<p>While Facebook, Twitter, and Instagram will always have a place in my history of APIs, I feel like we are entering a post Facebook, Twitter, and Instagram API world. All three platforms are going through serious evolutions, which includes tightening down the controls on API access, and shuttering of many APIs. These platforms are tightening things down for a variety of reasons, which are more about their business goals, than it is about the community. I’m not saying these APIs will go away entirely, but the era where where these API platforms ruled is coming to a close. The other day I articulated that these platform only needed us for a while, and now that they’ve grown to a certain size do not need us anymore. While this is true, I know there is more to the story of why we are moving on in the Facebook, Twitter, and Instagram story. We can’t understand the transformation that is occurring without considering that these platform’s business models are playing out, and they (we) are reaping what they’ve sown with their free and open platform business models. It isn’t so much that they are looking to screw over their developers, they are just making another decision, in a long line of decisions to keep generating revenue from their user generated realities, and advertising fueled perception. I don’t fault Twitter, Facebook, and Instagram for fully opening their APIs, then closing them off over time. I fault us consumers for falling for it. I do fault Twitter, Facebook, and Instagram a little for not managing their APIs better along the way, but when your business model is out of alignment with proper API management, it is only natural that you look the other way when bad things are happening, or you are just distracted with other priorities. This is ultimately why you should avoid platforms who don’t have an API, or a clear business model for their platform. There...[<a href="/2018/08/24/living-in-a-post-facebook-twitter-and-instagram-api-world/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/24/how-should-teams-be-documenting-their-apis-when-you-have-both-legacy-and-new-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109_214_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/24/how-should-teams-be-documenting-their-apis-when-you-have-both-legacy-and-new-apis/">How Should Teams Be Documenting Their APIs When You Have Both Legacy And New APIs?</a></h3>
			<p><em>24 Aug 2018</em></p>
			<p>I’m continuing my work to help the Department of Veterans Affairs (VA) move forward their API strategy. One area I’m happy to help the federal agency with, is just being available to answer questions, which I also find make for great stories here on the blog–helping other federal agencies also learn along the way. One question I got from the agency recently, is regarding how the teams should be documenting their APIs, taking into consideration that many of them are supporting legacy services like SOAP. From my vantage point, minimum viable API documentation should always include a machine readable definition, and some autogenerated documentation within a portal at a known location. If it is a SOAP service, WSDL is the format. If it is REST, OpenAPI (fka Swagger) is the format. If its XML RPC, you can bend OpenAPI to work. If it is GraphQL, it should come with its own definitions. All of these machine readable definitions should exist within a known location, and used as the central definition for the documentation user interface. Documentation should not be hand generated anymore with the wealth of open source API documentation available. Each service should have its own GitHub/BitBucket/GitLab repository with the following: README - Providing a concise title and description for the service, as well as links to all documentation, definitions, and other resources. Definitions - Machine readable API definitions for the APIs underlying schema, and the surface area of the API. Documentation - Autogenerated documentation for the API, driven by its machine readable definition. Depending on the type of API being deployed and managed, there should be one or more of these definition formats in place: Web Services Description Language (WSDL) - The XML-based interface definition used for describing the functionality offered by the service. OpenAPI - The YAML or JSON based OpenAPI specification format managed by the OpenAPI Initiative as part of the Linux Foundation. JSON Schema - The vocabulary that allows for...[<a href="/2018/08/24/how-should-teams-be-documenting-their-apis-when-you-have-both-legacy-and-new-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/23/the-importance-of-postman-api-environment-files/"><img src="https://s3.amazonaws.com/kinlane-productions2/postman/postman-environments.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/23/the-importance-of-postman-api-environment-files/">The Importance Of Postman API Environment Files</a></h3>
			<p><em>23 Aug 2018</em></p>
			<p>I’m a big fan of Postman, and the power of their development environment, as well as their Postman Collection format. I think their approach to not just integrating with APIs, but also enabling the development and delivery of APIs has shifted the conversation around APIs in the last couple of years–not too many API service providers accomplish this in my experience. There are several dimensions to what Postman does that I think are pushing the API conversation forward, but one that has been capturing my attention lately are Postman Environment Files. Using Postman, you can manage many different environments used for working with APIs, and if you are a pro or enterprise customer, you can export a file that represents an environment, making each of these API definitions more portable and collaborative. Managing the variety of environments for the hundreds of APIs I use is one of the biggest pain points I have. Postman has significantly helped me get a handle on the tokens and keys I use across the internal, as well as partner and public APIs that I depend on each day to operate API Evangelist. Postman environments allows me to define environments within the Postman application, and then share them as part of the pro / enterprise team experience. You can also manage your environments through the Postman API, if you need to more deeply integrate with your operations. The Postman Environment File makes all of this portable, sharable, and used across environments. It is one of the reasons that makes Postman Collections more valuable to some users, in specific contexts, because it has that run time aspect to what it does. Postman let’s you communicate effectively around the APIs you are deploying and integrating with, and solves relevant pain points like API environment management, that can stand in the way of integration. There aren’t many features of API service providers I get very excited about, but the potential of Postman as...[<a href="/2018/08/23/the-importance-of-postman-api-environment-files/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/23/getting-email-updates-from-the-api-providers-i-care-about-is-one-way-to-stay/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_158_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/23/getting-email-updates-from-the-api-providers-i-care-about-is-one-way-to-stay/">Getting Email Updates From The API Providers I Care About Is One Way To Stay</a></h3>
			<p><em>23 Aug 2018</em></p>
			<p>I do not like email. I do not have a good relationship with my inbox. However, it is one of those ubiquitous tools I have to use, and understand the value it can bring to my world. The goal is to not let my inbox control me too much, as my it is is often a task list that other people think they can control. With all that said, I’m finding renewed value in email newsletters, on several fronts. While I’d prefer to get updates via Atom, I’m warming up to receiving updates from API providers, and API service providers in my inbox. I am an active subscriber to the REST API Notes Newsletter, API Developer Weekly, and other relative newsletters. I’m also finding myself opening up more emails from the API providers, and service providers I’m registered with. Historically, I often see email as a nuisance, but I’m beginning to see emails from the companies I’m paying attention to as a healthy signal. Increasingly, it is a signal that I’m using to understand the overall health of a platform, and yet another signal that will go silent when a platform isn’t supporting their user base, and potentially running out of funding for their operations. I recently wrote a script that harvests emails from my inbox, and tracks on the communications occurring with API providers and service providers I am monitoring. At it’s most basic, it is a heartbeat that I can use to tell when an API provider or service provider is still alive. After that, I’m looking at harvest URLs, and other data, and use the signals to float an API provider or service provider up on my list. It can be tough to remember to tune into what is going on across hundreds and thousands of APIs, and any signal I can harvest to help companies float up is a positive thing. Hopefully it is something that will also incentivize API provider...[<a href="/2018/08/23/getting-email-updates-from-the-api-providers-i-care-about-is-one-way-to-stay/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/23/any-way-you-want-it-extending-swagger-ui-for-fun-and-profit-by-kyle-shockey/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/kyle-shocky-smarbear.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/23/any-way-you-want-it-extending-swagger-ui-for-fun-and-profit-by-kyle-shockey/">Any Way You Want It: Extending Swagger UI for Fun and Profit by Kyle Shockey</a></h3>
			<p><em>23 Aug 2018</em></p>
			<p>
We are getting closer to the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th. The schedule for the conference is up, along with the first lineup of keynote speakers, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is “Any Way You Want It: Extending Swagger UI for Fun and Profit” by Kyle Shockey (@kyshoc) of SmartBear Software (@SmartBear) on September 25th.

Here is Kyle’s abstract for the session:

Your APIs are tailored to your needs - shouldn’t your tools be as well? In this talk, we’ll explore how Swagger UI 3 makes it easier than ever to create custom functionality, and common use cases for the power that the UI’s plugin system provides.

Learn how to:

- Create plugins that extend existing features and define new functionality
- Integrate Swagger UI seamlessly by defining a custom layout
- Package and share plugins that can be reused by the community (or your organization)

Swagger UI has changed the conversation around how we document our APIs, and being able to extend the interface is an important part of keeping the API documentation conversation evolving, and APIStrat is where this type of discussion is happening. You can register for the event here, and there are still sponsorship opportunities available. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

I am looking forward to seeing you all in Nashville next month!

[<a href="/2018/08/23/any-way-you-want-it-extending-swagger-ui-for-fun-and-profit-by-kyle-shockey/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/22/testing-and-meaningful-mocks-in-a-microservice-system-by-laura-medalia/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/laura-medalia.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/22/testing-and-meaningful-mocks-in-a-microservice-system-by-laura-medalia/">Testing and Meaningful Mocks in a Microservice System by Laura Medalia</a></h3>
			<p><em>22 Aug 2018</em></p>
			<p>
We are getting closer to the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th. The schedule for the conference is up, along with the first lineup of keynote speakers, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is “Testing and Meaningful Mocks in a Microservice System”  by Laura Medalia (@codergirl__) of Care/Of on September 25th.

Here is Laura’s abstract for the session:

Laura will be talking about tooling for mocking microservice endpoints in a meaningful way using Open API specifications. She will cover how to set up microservice deployments processes so that with each versioned microservice deployed a mock of the service with up to date contracts will also be deployed. Laura will also show how to use tooling she and her team built to consume these lightweight mocks in unit tests and either get default mock responses or mock out custom responses for different test cases.

In an era where many API development groups are working to move to a design and mock first approach, this session will be key to our journey, and APIStrat is where we all need to be. You can register for the event here, and there are still sponsorship opportunities available. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

I am looking forward to seeing you all in Nashville next month!

Photo Credit: Laura Medalia on Pintaram.

[<a href="/2018/08/22/testing-and-meaningful-mocks-in-a-microservice-system-by-laura-medalia/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/22/searching-for-apis-that-possess-relevant-company-information/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/cityscape_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/22/searching-for-apis-that-possess-relevant-company-information/">Searching For APIs That Possess Relevant Company Information</a></h3>
			<p><em>22 Aug 2018</em></p>
			<p>I’m evolving the search for the Streamdata.io API Gallery I’ve been working on lately. I’m looking to move the basic keywords search that searches the API name and description, as well as the API path, summary, and description using a key word or phrase, to also be about searching parameters in a meaningful way. Each of the APIs in the Streamdata.io API have an OpenAPI definition. It is how I render each of the individual API paths using Jekyll and Github Pages. These parameters give me another dimension of data in which I can index, and use as a facet in my API gallery search. I am developing different sets of vocabulary to help me search against the parameters used across APIs, with one of them being focused on company related information. I’m trying to find APIs that provide the ability to add, update, and search against company related data, content, and execute algorithms that help make sense of company resources. There is no perfect way to search for API parameters that touch on company resources, but right now I’m looking for a handful of fields: company, organization, business, enterprise, agency, ticker, corporate, and employer. Returning APIs that have a parameter with any of those words in the path or summary, and weighting differently if it is in the description or tags for each API path. Next, I’m also tagging each API path that has a URL field, because this will allow me to connect the dot to a company, organization, or other entity via the domain. This is all I’m trying to do, is connect the dots using the parameter structure of an API. I find that there is an important story being told at the API design layer, and API search and discovery is how we are going to bring this story out. Connecting the dots at the corporate level is just one of many interesting stories out there, just waiting to be...[<a href="/2018/08/22/searching-for-apis-that-possess-relevant-company-information/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/22/it-is-hard-to-go-api-define-first/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/69_120_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/22/it-is-hard-to-go-api-define-first/">It Is Hard To Go API Define First</a></h3>
			<p><em>22 Aug 2018</em></p>
			<p>Last year I started saying API define first, instead of API design first. In response to many of the conversations out there about designing, then mocking, and eventually deploying your APIs into a production environment. I agree that you should design and iterate before writing code, but I feel like we should be defining our APIs even before we get to the API design phase. Without the proper definitions on the table, our design phase is going to be a lot more deficient in standards, common patterns, goals, and objectives, making it important to invest some energy in defining what is happening first–then iterate on the API definitions throughout the API lifecycle, not just design. I prefer to have a handful of API definitions drafted, before I move onto to the API design phase: Title - A simple, concise title for my API. Description - A simple, concise description for my API. JSON Schema - A set of JSON schema for my APIs OpenAPI - An OpenAPI for the surface area of my API. Assertions - A list of what my API should be delivering. Standards - What standards are being applied with this API. Patterns - What common web patterns will be used with this API. Goals - What are the goals for this particular API. I like having all of this in a GitHub repository before I get to work, actually designing my APIs. It provides me with the base set of definitions I need to go to be as effective as I can in my API design phase. Of course, each of these definitions will be iterated, added to, and evolved as part of the API design phase, and beyond. The goal is to just get a base set of building blocks on the workbench, properly setting the tone for what my API will be doing. Grounding my API work early on in the API lifecycle, in a consistent way that I...[<a href="/2018/08/22/it-is-hard-to-go-api-define-first/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/21/identifying-the-different-types-of-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/97_193_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/21/identifying-the-different-types-of-apis/">Identifying The Different Types Of APIs</a></h3>
			<p><em>21 Aug 2018</em></p>
			<p>APIs come in many shapes and sizes. Even when APIs may share a common resource, the likelihood that they are similar in functional, will be slim. Even after eight years of studying APIs, I still struggle with understanding the differences, and putting APIs into common buckets. Think of the differences between two image APIs like Flickr and Instagram, but then also think about the difference between Twitter and Twilio–the differences are many, and a challenge to articulate. I’m pushing forward my API Stack, and API Gallery work, and I’m needing to better organize APIs into meaningful groups that I can add to the search functionality for each of API discovery services. To help me establish a handful of new buckets, I’m thinking more critically about the different types of API functionality I’m coming across, establishing seven new buckets: General Data - You can get at data across the platform, users, and resources. Relative Data - You can get at data that is relative to a user, company, or specific account. Static Data - The data doesn’t change too often, and will always remain fairly constant. Evolving Data - The data changes on a regular basis, providing a reason to come back often. Historical Data - Provides access to historical data, going back an X number of. years. Service - The API is offered as a service, or is provided to extend a specific service. Algorithmic - The API provides some sort of algorithmic functionality like ML, or otherwise. Understanding the type of data an API provides is important to the work I’m doing. Streamdata.io caters to the needs of financial organizations, and they are looking for data to help them with their investment portfolio, but also have very particular opinions around the type of data they want. This first version of my API type list is heavily weighted towards data, but as I evolve in my thinking, I’m guessing the service and algorithmic buckets will...[<a href="/2018/08/21/identifying-the-different-types-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/21/describing-your-api-with-openapi-3-0-by-anthony-eden-aeden-dnsimple/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/anthony-eden-dns-simple.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/21/describing-your-api-with-openapi-3-0-by-anthony-eden-aeden-dnsimple/">Describing Your API with OpenAPI 3.0 by Anthony Eden (@aeden), DNSimple</a></h3>
			<p><em>21 Aug 2018</em></p>
			<p>
We are getting closer to the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th. The schedule for the conference is up, along with the first lineup of keynote speakers, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is “Describing Your API with OpenAPI 3.0” by Anthony Eden (@aeden), DNSimple (@dnsimple) on September 25th.

Here is Christian’s abstract for the session:

For the last 10 years, DNSimple has operated a comprehensive web API for buying, connecting, and operating domain names. After hearing about OpenAPI at APIStrat 2017, we decided to describe the DNSimple API using the OpenAPI v3 specification - this is the story of why we did it, how we did it, and where we are today.

By the end of this presentation you will have the tools you’ll need to evaluate your own API and decide if implementing OpenAPI makes sense for you, and if so, how you can get started. You’ll have a better understanding of the tools available to you to help write your OpenAPI 3 definition, as well the basics on how to write your own definition for your APIs.

We are all still working to make the switch from OpenAPI 2.0 to 3.0, and with APIStrat being owned and operated by the OpenAPI Initiative, it will definitely be the place to have face to face discussions that influence the road map for the API specification. You can register for the event here, and there are still sponsorship opportunities available. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

I am looking forward to seeing you all in Nashville next month!

[<a href="/2018/08/21/describing-your-api-with-openapi-3-0-by-anthony-eden-aeden-dnsimple/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/21/algolia-kindly-provides-a-hacker-news-search-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algolia/hacker-news-algolia-search-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/21/algolia-kindly-provides-a-hacker-news-search-api/">Algolia Kindly Provides A Hacker News Search API</a></h3>
			<p><em>21 Aug 2018</em></p>
			<p>I was working on a serverless app for Streamdata.io that takes posts to Hacker News and streams them into an Amazon S3 data lake, and I came across the Algolia powered Hacker News search API. After being somewhat frustrated with the simplicity of the official Hacker News API, I was pleased to find the search kindly provided by Algolia. There is no search API available for the core Hacker News API, and the design leaves a lot to be desired, so the simplicity of Algolia’s API solution was refreshing. There is a lot of data flowing into Hacker News on a regular day, so providing a search API is pretty critical. Additionally, Algolia’s ability to deliver such a simple, usable, yet powerful API on top of a relevant data source like Hacker News demonstrates the utility of what Algolia offers as a search solution–something I wanted to take a moment to point out here on the blog. I consider search to be an essential ingredient for any API. Every API should have a search element to their stack, allowing the indexing and searching of all API resources through a single path. Making Algolia a relevant API service provider in this area, enabling API providers to outsource the indexing and searching of their resources, and the delivery of a dead simple API for your consumers to tap into. This path forward is probably not for every API, as many weave specialized search throughout their API design, but for teams who are lacking in resources, and can afford to outsource this element–Algolia makes sense. Seeing Algolia in action, for a specific API I was integrating with helped bring their service front and center for me. I tend to showcase Elastic for deploying API search solutions, but it is a good to receive a regular reminder that Algolia does the same thing as a service. Their work on the Hacker News Search API provides a good example of...[<a href="/2018/08/21/algolia-kindly-provides-a-hacker-news-search-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/20/we-need-you-api-developers-until-we-have-grown-to-a-certain-size/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/castle-on-hill-edinburgh_feed_people.JPG" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/20/we-need-you-api-developers-until-we-have-grown-to-a-certain-size/">We Need You API Developers Until We Have Grown To A Certain Size</a></h3>
			<p><em>20 Aug 2018</em></p>
			<p>I’m watching several fronts along the API landscape evolve right now, with large API providers shifting, shutting down, and changing how they do business with their APIs–now that they don’t need their ecosystem of developers as much. It is easy to point the finger at Facebook, Twitter, Google, and the other giants, but it really is a wider systemic, business of APIs illness that will continue to negatively impact the API universe. While this behavior is very prominent with the leading API providers right now, it is something we’ll see various waves of it’s influence on the tone of the entire API sector further on down the road. How API providers treat their consumers vary from industry to industry, and is different depending on the types of resources being made available. However, the issue of API providers treating their developers differently when they are just getting started versus once they grow to a certain size, is something that will continue to plague all areas of the API space. Companies change as they mature, and their priorities will no doubt evolve, but almost all will feel compelled to exploit developers early on so that they can grow their numbers–taking advantage of the goodwill and curiosity of the developer personality. The polarization of the API management layer is difficult to predict from the outside. I want to help new API providers out early on, but after eight years of doing this, and seeing many API providers and service providers evolve, and go away–I am left very skeptical of EVERYONE. I think many developers in the API space are feeling the same, and are weary of kicking the tires on new APIs, unless they have a specific project and purpose. I think the days of developers working for free within an API ecosystem are over. It is a concept that will still exist in some form, but along with each wave of new API startups taking advantage of...[<a href="/2018/08/20/we-need-you-api-developers-until-we-have-grown-to-a-certain-size/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/20/the-redirect-url-to-confirm-selling-your-api-in-aws-marketplace-provides-us-with-a-positive/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/aws-marketplace-aws-saas-seller-integration-guide.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/20/the-redirect-url-to-confirm-selling-your-api-in-aws-marketplace-provides-us-with-a-positive/">The Redirect URL To Confirm Selling Your API In AWS Marketplace Provides Us With A Positive</a></h3>
			<p><em>20 Aug 2018</em></p>
			<p>I am setting up different APIs using the AWS API Gateway and then publishing them to the AWS Marketplace, as part of my work with Streamdata.io. I’m getting a feel for what the process is all about, and how small I can distill an API product to be, as part of the AWS Marketplace process. My goal is to be able to quickly define APIs using OpenAPI, then publish them to AWS API Gateway, and leverage the gateway to help me manage the entire business of the service from signup to discovery. As I was adding one of my first couple of APIs to the AWS Marketplace, and I found the instructions regarding the redirect URL for each API to be a good template. Each individual API service I’m offering will have its own subscription confirmation URL with the AWS API marketplace, with the relevant variables present I will need to scale the technical and business of delivering my APIs: AWS Marketplace Confirmation Redirect URL: https://YOUR_DEVELOPER_PORTAL_API_ID.execute-api.[REGION].amazonaws.com/prod/marketplace-confirm/[USAGE_PLAN_ID] This URL is what my retail customers will be given after they click to subscribe to one of my APIs. Each API has its specific portal (present in URL), as well as being associated with a specific API usage plan within AWS API Gateway. Also notice that they have a variable for region, allowing me to deliver services by region, and scale up the technical side of delivering the various APIs I’m deploying–another important consideration when delivering reliable and performant services. Pointing out the URL for a signup process might seem like a small thing. However, because of AWS’s first mover advantage in the cloud, and their experience as an API pioneer, I feel like they are in a unique position to be defining the business layer of delivering APIs at scale in the cloud. The business opportunities available at the API management and marketplace layers within the AWS ecosystem are untapped, and represent the next generation of API...[<a href="/2018/08/20/the-redirect-url-to-confirm-selling-your-api-in-aws-marketplace-provides-us-with-a-positive/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/20/microserviceing-like-a-unicorn-with-envoy-istio-kubernetes-with-christian/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/the-hardest-part-about-microserv.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/20/microserviceing-like-a-unicorn-with-envoy-istio-kubernetes-with-christian/">Microserviceing Like a Unicorn With Envoy, Istio & Kubernetes With Christian</a></h3>
			<p><em>20 Aug 2018</em></p>
			<p>
We are getting closer to the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th. The schedule for the conference is up, along with the first lineup of keynote speakers, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is “Microservice’ing Like a Unicorn With Envoy, Istio and Kubernetes” by Christian Posta (@christianposta), Red Hat @RedHat on September 25th.

Here is Christian’s abstract for the session:

The exciting parts of APIs, unfortunately, happen when services actually try communicating and working together to accomplish some business function. The service-mesh approach has emerged to help make service communication boring. In particular, a project named Istio.io has garnered attention in the open-source community as a way of implementing the service mesh capabilities. These capabilities include pushing application-networking concerns down into the infrastructure: things like retries, load balancing, timeouts, deadlines, circuit breaking, mutual TLS, service discovery, distributed tracing and others.

As Istio becomes more popular and widely used, we’re going to see a lot of people put it into production for their API use cases. This talk will walk attendees through the Istio architecture, and more importantly, help them understand how it all works.

API delivery, integration, orchestration, and development of mesh networks all represent the next generation of doing APIs, and APIStrat is where we are having these discussions. You can register for the event here, and there are still sponsorship opportunities available. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

I am looking forward to seeing you all in Nashville next month!

[<a href="/2018/08/20/microserviceing-like-a-unicorn-with-envoy-istio-kubernetes-with-christian/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/20/having-an-api-deprecation-page-like-evrythng-does/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/catacombs_blue_circuit_3.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/20/having-an-api-deprecation-page-like-evrythng-does/">Having An API Deprecation Page Like EVRYTHNG Does</a></h3>
			<p><em>20 Aug 2018</em></p>
			<p>The API providers I talk to regularly are rarely proactive when it comes to addressing API deprecation. Most API providers aren’t thinking about shutting down any service they deliver until they’ve actually encountered the need down the road. Many just begin their API journey, assuming their APIs will be a success, and they will have to support every version forever. However, once they encounter what it will take to support older versions, they begin to change their tune, something that often comes as an unexpected surprise to consumers. Shutting down old APIs will never be easy, but the process can be made easier with a little proactive communication. One example of this practice in action can be found over at the Internet of Things provider Evrythng, with their API deprecation page. Which provides a pretty simple layout for an API deprecation page, beyond just a title and description of what the future might hold. API Status The following status labels are applicable to APIs, features, or SDK versions depending on their current support status: _- Preview - May change at any time. Stable - Fully released and stable. Will not change at short notice. Deprecated - No longer supported (and may have been replaced), and may be removed in the future at an announced date. Use not encouraged. Removed - Removed, and no longer supported or available._ Communication When a deprecation is announced, the details and any relevant migration information will be available on the following channels: _- The Developer Blog. The @evrythngdev Twitter account. The relevant feature page on the Developer Hub. Enterprise customers may receive information by email to their specified EVRYTHNG contact, if applicable._ Customers using one our SLAs can read the General section for more information. Evrythng provides us with some important building blocks for using as part of our overall API deprecation strategy. Something that EVERY API provide should be considering as they prepare to launch a new API. API...[<a href="/2018/08/20/having-an-api-deprecation-page-like-evrythng-does/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/13/it-isnt-just-that-you-have-a-pdf-for-your-api-docs-it-is-because-it-demonstrates-that-you-do-not-use-other-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/pdf-everywhere.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/13/it-isnt-just-that-you-have-a-pdf-for-your-api-docs-it-is-because-it-demonstrates-that-you-do-not-use-other-apis/">"It Isnt Just That You Have A PDF For Your API Docs, It Is Because It Demonstrates That You Do Not Use Other APIs"</a></h3>
			<p><em>13 Aug 2018</em></p>
			<p>I look at a lot of APIs. I can tell a lot about a company, and the people behind an API from looking at their developer portal, documentation, and other building blocks of their presence. One of the more egregious sins I feel an API provider can make when operating their API is publishing their API documentation as a PDF. This is something that was acceptable up until about 2006, but over a decade after it shows that the organization behind an API hasn’t done their homework. The crime really isn’t the fact that an API provider is using a PDF for their documentation. I’m fine with API providers publishing a PDF version of their API, to provide a portable version of it. Where a PDF version of the documentation becomes a problem is when it is the primary version of the documentation, which demonstrates that the creators don’t get out much and haven’t used many other APIs. If an API team has done their homework, actually put other 3rd party APIs to work, they would know that PDF documentation for APIs is not the norm out in the real world. One of the strongest characteristics an API provider can possess, is an awareness of what other API providers are doing. The leading API providers demonstrate that they’ve used other APIs, and are aware of what API consumers in the mainstream are used to. Most mainstream API consumers will simply close the tab when they encounter an API that has a PDF document for their API. Unless you have some sort of mandate to use that particular API, you are going to look elsewhere. If an API provider isn’t up to speed on what the norms are for API documentation, and are outwardly facing, the chance they’ll actively support their API is always diminished. PDF API documentation may not seem like too big of a mistake to many enterprise, institutional, and government API providers, but...[<a href="/2018/08/13/it-isnt-just-that-you-have-a-pdf-for-your-api-docs-it-is-because-it-demonstrates-that-you-do-not-use-other-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/13/bringing-discovery-within-data-api-marketplaces-out-into-the-open/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/granaryfield_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/13/bringing-discovery-within-data-api-marketplaces-out-into-the-open/">Bringing Discovery Within Data API Marketplaces Out Into The Open</a></h3>
			<p><em>13 Aug 2018</em></p>
			<p>I spend time reviewing each wave of data API marketplaces as they emerge on the landscape every couple of years. There are a number of reasons why these data marketplaces exist, ranging from supporting government agencies, NGOs, or for commercial purposes. One of the most common elements of API-driven data marketplaces that frustrates me is when they don’t do the hard work to expose the meta data around the databases, datasets, spreadsheets, and the raw data they are providing access to–making it very difficult to actually discover anything of interest. You can see a couple examples of this with mLab, World Health Organization, Data.World, and others. While these platforms provide (sometimes) impressive ability to manage data stores, but they don’t always do a good job exposing the meta data of their catalogs as part of the available APIs. Dynamically generating API endpoints, documentation, and other resources based upon the data that is being published to their platforms. Leaving developers to do the digging, and making the investment to understand what is available on a platform. Some of the platforms I encounter obfuscate their data metadata on purpose, requiring developers to qualified before they get access to valuable resources. Most I think, just do not put themselves in the position of an API consumer who lands on their developer page, and doesn’t know anything about an API. They understand the database, and the API, so it all makes sense to them, and they don’t have any empathy for anyone else who isn’t in the know. Which is a common trait of database centered people who speak in acronyms, and schema that they assume other people know, and do not spend much time thinking outside of that bubble. I could make a career out of deploying APIs on top of other data marketplace APIs. Autogenerating a more accessible, indexable, intuitive layer on top of what they’ve already deployed. I regularly find a wealth of data that is...[<a href="/2018/08/13/bringing-discovery-within-data-api-marketplaces-out-into-the-open/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/07/working-with-my-openapi-definitions-in-an-api-editor-helps-stabilize-them/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/swagger-editor-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/07/working-with-my-openapi-definitions-in-an-api-editor-helps-stabilize-them/">Working With My OpenAPI Definitions In An API Editor Helps Stabilize Them</a></h3>
			<p><em>07 Aug 2018</em></p>
			<p>I’m deploying three new APIs right now, using a new experimental serverless approach I’m evolving. One is a location API, another providing API access to companies, and the third involves working with patents. I will be evolving these three simple web APIs to meet the specific needs of some applications I’m building, but then I will also be selling retail and wholesale access to each API once they’ve matured enough. With all three APIs of these APIs, I began with a simple JSON schema from the data source, which I used to generate three rough OpenAPI definitions that will acts the contract seed for my three services. Once I had three separate OpenAPI contracts for the services I was delivering, I wanted to spend some time hand designing each of the APIs before I imported into AWS API Gateway, generating Lambda functions, loading in Postman, and used to support other stops along the API lifecycle. I still use a localized version of Swagger Editor for my OpenAPI design space, but I’m working to migrate to OpenAPI-GUI as soon as I can. I still very much enjoy the side by side design experience in Swagger Editor, but I want to push forward the GUI side of the conversation, while still retaining quick access to the RAW OpenAPI for editing. One of the reasons why I still use Swagger Editor is because of the schema validation it does behind the scenes. Which is one of the reasons I need to learn more about Speccy, as it is going to help me decouple validation from my editor, and all me to use it as part of my wider governance strategy, not just at design time. However, for now I am highly dependent on my OpenAPI editor helping me standardize and stabilize my OpenAPI definitions, before I use them along other stops along the API lifecycle. These three APIs I’m developing are going straight to deployment, because they are...[<a href="/2018/08/07/working-with-my-openapi-definitions-in-an-api-editor-helps-stabilize-them/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/07/twice-the-dose-of-vanick-digital-at-apistrat-in-nashville-tn-next-month/"><img src="https://s3.amazonaws.com/kinlane-productions2/vanick-digital/apistrat-2018-vanick-digital.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/07/twice-the-dose-of-vanick-digital-at-apistrat-in-nashville-tn-next-month/">Twice The Dose Of Vanick Digital At APIStrat in Nashville, TN Next Month</a></h3>
			<p><em>07 Aug 2018</em></p>
			<p>We are kicking it into overdrive now that the schedule is up for APIStrat in Nashville, TN this September 24th through 26th. From now until the event at the end of September you are going to hear me talk about all the amazing speakers we have, the companies they work for, and the interesting things they are all doing with APIs. One of the perks of being a speaker or a sponsor at APIStrat–you get coverage on API Evangelist, a become part of the buzz around the 9th edition of the API Strategy &amp; Practice Conference (APIStrat), now operated by the OpenAPI Initiative (OAI) and the Linux Foundation. Today’s post is about my friends over at the digital solutions and API management agency Vanick Digital. With APIStrat coming to their backyard, and their ability to capture the attention of the APIStrat program committee, Vanick Digital has two separate talks this year: Securing the Full API Stack by Patrick Chipman - APIs open up new channels for sharing and consuming data, but whenever you open a new channel, new security risks emerge. Additionally, APIs often involve a variety of new components, such as API gateways, in-memory databases, edge caches, facade layers, and microservice-aligned data stores that can complicate the security landscape. How and where do you apply the right controls to ensure your API and your data are secure? In this session, we’ll answer that question by identifying the different components commonly used in the delivery of API products. For each layer, we’ll discuss the security risks that can and should be mitigated there, along with best practice approaches (including ABAC, OAuth2, and more) to implement those mitigations. What Do You Mean By “API as a Product”? by Lou Powell - You may have heard the term “API Product.” But what does it mean? In this talk I will introduce the concept and explain the benefits and challenges of transforming your organization to view your APIs...[<a href="/2018/08/07/twice-the-dose-of-vanick-digital-at-apistrat-in-nashville-tn-next-month/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/07/i-am-speaking-in-washington-d-c-at-the-blue-button-2-0-developer-conference/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/blue-button/blue-button-api-docs.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/07/i-am-speaking-in-washington-d-c-at-the-blue-button-2-0-developer-conference/">I Am Speaking In Washington D.C. At The Blue Button 2.0 Developer Conference</a></h3>
			<p><em>07 Aug 2018</em></p>
			<p>I’m heading to Washington D.C. this Monday to speak on the API life cycle as part of the Blue Button 2.0 Developer Conference. We’ll be coming together in the Eisenhower Executive Office Building, within the west wing complex of the White House, to better understand how we can, “bring together developers to learn and share insights on how we can leverage claims data to serve the Medicare population.” The gathering will hear from CMS Administrator Seema Verma and other Administrator Leadership about Blue Button 2.0 and the MyHealthEData initiative, while also hosting a series of break sessions, which I’m part of: Blue Button 2.0 and FHIR (where it’s all heading) with Mark Scrimshire and Cat Greim MyHealthEData and Interoperability with Alex Mugge and Joy Day Overview of Medicare Claims Data with Karl Davis Medicare Beneficiary User Research with Allyssa Allen Sync for Science with Josh Mandel and Andrew Bjonnes API Design with Kin Lane Registration for the gathering is now closed, but if you are a federal govy, I’m sure you can find someone to get you in. I’m looking forward to seeing the CMS, HHS, and USDS folks again, as they are doing some amazing stuff with the Blue Button API, as well as hang out with some of the VA people I know will be there. The Blue Button API is one of the more important API blueprints we have out there in the healthcare space, as well as the federal government. I’ve been a champion on Blue Button since I contributed to the project back when I worked in DC back in 2013, and will continue to invest in its success in coming years. In my session I will be covering my API lifecycle and governance research as the API Evangelist, but I’m eager to talk with more folks involved with the Blue Button API about what is next, and better understand where HL7 FHIR is headed, while also developing my awareness...[<a href="/2018/08/07/i-am-speaking-in-washington-d-c-at-the-blue-button-2-0-developer-conference/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/06/sap-and-being-late-to-the-api-game/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/SAP/P5160110.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/06/sap-and-being-late-to-the-api-game/">SAP And Being Late To The API Game</a></h3>
			<p><em>06 Aug 2018</em></p>
			<p>I’ve been having regular meetings with the SAPI API team lately, talking through their presence in the API space, and throwing out ideas for what the future might hold. This isn’t a paid engagement, it is just something I’m interested in investing in on my own, but like many other API service providers in the space, we are exploring what partnership opportunities might there might be. Last week, I wrote about their API Business Hub, which I’ll keep exploring, but first I wanted to address perceptions in the industry that SAP is a little late to the game, when it comes to going all in on APIs. For me, the SAP API journey begins in 2010, when I left my role as VP of Technology at WebEvents Global, who runs all of SAP Events. By 2010, I had found success in my role by scaling up infrastructure using the AWS cloud, which was orchestrated using APIs, and I was beginning to get a taste of where things were going when it came to delivering digital resources to mobile applications using APIs. I helped lead the technology around SAPPHIRE, SAP’s flagship conference, as well as many other lesser events, and meetings. I had also gotten in trouble by SAP IT leadership for using the cloud, which they labeled a “hobby toy”, and told me I shouldn’t be using. However, I was delivering applications more quickly and cost effectively, so my leadership really couldn’t dismiss my usage of the cloud, and more specifically web APIs. Convincing me that web APIs were going to be more successful than the current web services strategy I was working under. After seeing the success I was having using APIs to deliver SAP events, and better understanding the potential for delivering global infrastructure in the cloud, and via the increasingly ubiquitous mobile devices we had in our pockets, I left my position and started API Evangelist–eight years later, I’m still doing it....[<a href="/2018/08/06/sap-and-being-late-to-the-api-game/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/06/making-sure-my-api-dependencies-include-data-provenance/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_19_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/06/making-sure-my-api-dependencies-include-data-provenance/">Making Sure My API Dependencies Include Data Provenance</a></h3>
			<p><em>06 Aug 2018</em></p>
			<p>I am publishing a new API for locations. I am tired of needing some of the same location based resources across projects, and not having a simple, standardized API I can depend on. So I got to work finding the most accurate and complete data set I could find of cities, regions, and countries. I settled on using the complete, and easy to use countries-regions-cities project by David Graham–providing a straightforward SQL script I can use as the seed for my locations API database. After crafting an API for this database using AWS API Gateway and Lambda, and working my way down my API checklist, it occurred to me that I wanted to include David Graham’s work as one of the project dependencies. Giving him attribution, while honestly acknowledging my project’s dependency on the data he provided. I’m working hard to include all dependencies within each of the microservices that I’m publishing, being mindful of every data, code, and human dependency that exists behind each service I deliver. Even if I don’t rely on regular updates from them, I still want to acknowledge their contribution, and consider attribution as one layer of my API dependency discussion. Having a dependency section of my API checklist has helped me evolve how I think about defining the dependencies my services have. I initially began tracking all other services that my microservices were dependent on, but then I quickly began adding details about the other software, data, and people the service depends on as well. I’m also pulling together a machine readable definition for tracking on my microservice dependencies. It will be something I include in the API discovery (APIs.json) document for each service, alongside the OpenAPI, and other specifications. Allowing me to track on the dependencies (and attribution) for all of my APIs, and API related artifacts that I am producing on a regular basis. Providing data provenance for each of my services, documenting the origins of all...[<a href="/2018/08/06/making-sure-my-api-dependencies-include-data-provenance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/08/06/automating-inequality-and-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/apistrat-virginia.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/08/06/automating-inequality-and-apis/">Automating Inequality (and APIs)</a></h3>
			<p><em>06 Aug 2018</em></p>
			<p>As we prepare for APIStrat in Nashville, TN this September 24th through 26th, I asked my partner in crime Audrey Watters (@audreywatters) to write a post on the significance of Virginia Eubanks, the author of Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor keynoting the conference–she shared this story, of why her work is so significant, and why it is important for the API community to tune in. Repeatable tasks can and should be automated – that’s an assertion that you’ll hear all the time in computing. Sometimes the rationale is efficiency – it’s cheaper, faster, “labor-saving.” Automation will free up time; it will make our lives easier. Or so we’re told. Sometimes automation is encouraged in order to eliminate human error or bias. Increasingly, automation is eliminating human decision-making altogether. And in doing so, let’s be clear, neither bias nor error are removed; rather they are often re-inscribed. Automation – algorithmic decision-making – can obscure error; it can obscure bias. This push for more automated decision-making works hand-in-hand with the push for more data collection, itself a process that is already shaped by precedent and by politics. And all this, of course, is facilitated by APIs. APIs are commonly referred to as a “glue” of sorts – the implication, more often than not, is that APIs are simply a neutral technology holding larger technical systems together. But none of this is neutral – not the APIs and not the algorithms and not the databases. These technologies are never neutral in their design, development, or implementation. The systems that technologies exist in – organizationally, economically, politically, culturally – are never neutral either. It seems imperative that those building digital technologies begin to think much more critically about the implications of their work, recognizing that the existing inequalities in the analog systems are readily being ported to the digital sphere. This makes the work of one of the keynote speakers at this fall’s...[<a href="/2018/08/06/automating-inequality-and-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/31/we-need-your-help-moving-the-asyncapi-specification-forward/"><img src="https://s3.amazonaws.com/kinlane-productions2/asyncapi/asyncapi-define-your-message-driven-apis.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/31/we-need-your-help-moving-the-asyncapi-specification-forward/">We Need Your Help Moving The AsyncAPI Specification Forward</a></h3>
			<p><em>31 Jul 2018</em></p>
			<p>We need your help moving the AsyncAPI specification forward. Ok, first, what is the AsyncAPI specification? “The AsyncAPI Specification is a project used to describe and document Asynchronous APIs. The AsyncAPI Specification defines a set of files required to describe such an API. These files can then be used to create utilities, such as documentation, integration and/or testing tools.” AsyncAPI is a sister specification to OpenAPI, but instead of describing the request and response HTTP API landscape, AsyncAPI is describing the message, topic, event, and streaming API landscape across the HTTP and TCP landscape. It is how we are going to continue to ensure there is machine readable descriptions of this portion of the API landscape, for use in tooling and services. My friend Fran Mendez (@fmvilas) is the creator and maintainer of the specification, and he is doing way too much of the work on this important specification and he needs our help. Here is Fran’s request for our help to contribute: AsyncAPI is an open source project that’s currently maintained by me, with no company or funds behind. More and more companies are using AsyncAPI and the work needed is becoming too much work for a single person working in his spare time. E.g., for each release of the specification, tooling and documentation should be updated. One could argue that I should be dedicating full time to the project, but it’s in this point where it’s too much for spare time and very little to get enough money to live. I want to keep everything for free, because I firmly believe that engineering must be democratized. Also, don’t get me wrong, this is not a complaint. I’m going to continue running the project either with or without contributors, because I love it. This is just a call-out to you, the AsyncAPI lover. I’d be very grateful if you could lend a hand, or even raise your hand and become a co-maintainer. Up to...[<a href="/2018/07/31/we-need-your-help-moving-the-asyncapi-specification-forward/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/31/practical-secdevops-for-apis-from-42crunch-at-apistrat-in-nashville-this-fall/"><img src="https://s3.amazonaws.com/kinlane-productions2/42crunch/42-crunch-the-api-security-platform-for-the-enterprise.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/31/practical-secdevops-for-apis-from-42crunch-at-apistrat-in-nashville-this-fall/">Practical SecDevOps for APIs From @42Crunch At APIStrat In Nashville This Fall</a></h3>
			<p><em>31 Jul 2018</em></p>
			<p>We are gearing up for the next edition of APIStrat in Nashville, TN this September 24th through 26th. With the conference less than two months away, and the schedule up, I’m building momentum with my usual drumbeat about the speakers, and companies involved. So you’ll be reading a lot of stories related to APIStrat in coming weeks, where I’m looking to build awareness and attendance of the conference, but more importantly showcasing the individuals and companies who are supporting it and helping making the 9th edition of APIStrat amazing. One of innovative startups I’m partnering with right now, and who you will find speaking and sponsoring APIStrat is 42Crunch. Full disclosure, I’m regularly talking with 42Crunch regarding their road map, and I consider them an API Evangelist partner, however, this is because I find them to be one of the more progressive, and important API startups out there right now. 42Crunch is important in my opinion, because they are focusing on API security, a critical stop along the API lifecycle, and also because of the OpenAPI-driven, awareness building approach to delivering API security. 42Crunch isn’t just bringing API security solutions to the table for you to purchase as a service, they bring API security solutions to the table that help you invest in your internal API security practices–which is the most critical aspect of what they do in my opinion. 42Crunch is focused on API security, but they are what I consider to be a full API lifecycle solution. Meaning they play nicely as one of the tools in your API lifecycle toolbox. Which begins with being OpenAPI-driven, and treating your API’s definition as a contract, but with 42Crunch it is about using this contract to empower your API team to make API security a first-class citizen across all stops along the API lifecycle. Not just at the API management layer, or as an after thought later on when you scan your infrastructure–42Crunch is baking...[<a href="/2018/07/31/practical-secdevops-for-apis-from-42crunch-at-apistrat-in-nashville-this-fall/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/27/the-service-level-agreement-sla-definition-for-the-openapi-specification/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/legalstatue_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/27/the-service-level-agreement-sla-definition-for-the-openapi-specification/">The Service Level Agreement (SLA) Definition For The OpenAPI Specification</a></h3>
			<p><em>27 Jul 2018</em></p>
			<p>I’m currently learning more about SLA4OAI, an open source standard for describing SLA in APIs, which is based on the standards proposed by the OAI, adding an optional profile for defining SLA (Service Level Agreements) for APIs. “This SLA definition in a neutral vendor flavor will allow to foster innovation in the area where APIs expose and documents its SLA, API Management tools can import and measure such key metrics and composed SLAs for composed services aggregated way in a standard way.” Providing not just a needed standard for the API sector, but more importantly one that is built on top of an existing standard. SLA4OAI, provides an interesting way to define the SLA for any API, providing a set of objects that augment and can be paired with an OpenAPI definition using an x-sla vendor extension: Context - Holds the main information of the SLA context. Infrastructure - Required Provides information about tooling used for SLA storage, calculation, governance, etc. Pricing - Global pricing data. Metrics - A list of metrics to use in the context of the SLA. Plans - A set of plans to define different service levels per plan. Quotas - Global quotas, these are the default quotas, but they could be overridden by each plan later. Rates - Global rates, these are the default rates, but they could be overridden by each plan later. Guarantees - Global guarantees, these are the default guarantees, but they could be overridden by each plan later. Configuration - Define the default configurations, later each plan can be override it. These objects provide all the details you will need to quantify the SLA for any OpenAPI defined API. In order to validate each of the SLAs, a separate Basic SLA Management Services is provided to implement the services that control, manage, report and track SLAs. Providing the reporting output you will need to understand whether or not each individual API is meeting its SLA. Providing...[<a href="/2018/07/27/the-service-level-agreement-sla-definition-for-the-openapi-specification/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/26/thinking-deeply-about-other-people-using-your-api-is-the-most-valuable-lesson/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/uspto/uspto-peds-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/26/thinking-deeply-about-other-people-using-your-api-is-the-most-valuable-lesson/">Thinking Deeply About Other People Using Your API Is The Most Valuable Lesson</a></h3>
			<p><em>26 Jul 2018</em></p>
			<p>I am deploying a patent review API for a client, using data from the Patent Examination Data System (PEDS). You can download complete JSON or XML data from the United States Patent Office (USPTO), and they even have an API. So, why would I be launching yet another API? Well, because what they have is so cryptic, complex, and lacking in any schema or API design, there is value in me pushing the API conversation forward a bit by thinking deeply about how other people will potentially be using these resources–something the USPTO clearly hasn’t done. The USPTO PEDS API (that is more acronyms than you can shake a stick at) is a great example of how much database people, and developers take for granted as they operate within their little bubbles, without much concern for how the rest of the world views their work–take a look at the screenshot of thee USPTO PEDS API. There is only one telling sign on this page regarding what this API does–the email address for the contact, which has a uspto.gov address. Beyond that there is not a single sign of the resources available within this API, or the value they bring to the table. Even if you can extrapolate that this is a patent API, there is nothing to tell you that you can’t actually get patent data from this, you can only get meta data about the patents, reviewers, inventors, and the activity around the patent. For me, the API reflects many of the challenges developers and database people face when it comes to thinking out of their box, and effectively communicating with external consumers–which is the whole reason we do web APIs. I’m pretty well versed consuming patent data, and it took me several hours to get up to speed with this set of resources. I opted to not deal with the API, which is just an ElasticSearch index on top of a patent file...[<a href="/2018/07/26/thinking-deeply-about-other-people-using-your-api-is-the-most-valuable-lesson/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/26/monolithic-serverless-wtf/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/statue-face-open-mouth_blue_circuit_5.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/26/monolithic-serverless-wtf/">Monolithic Serverless? WTF?</a></h3>
			<p><em>26 Jul 2018</em></p>
			<p>While writing about the discussions I’ve been having with folks around using monorepos to manage microservices, I came across this post about whether or not people should be using a single monolithic Lambda function or multiple lambda functions with the AWS API Gateway. Again, surprising me about how lazy people are, and how difficult it is for people to think about things in a decoupled way. Which I think is the reason many people will go back to doing monolithic applications, and fail at microservices, not because it technically won’t work, it is just because it will be perceived as more work, and with a lack of imagination around how to work in a distributed way, people will give up. First, I do not think microservices is a good idea for all applications. Second, I don’t always subscribe to microservices meaning small or micro. I think a service mindset is good, and it is healthy to decouple, and reduce the surface area of your services, minimizing dependencies, but there are many situations where a super small microservice will be a bad idea. However, if you are going to do serverless microservices with Lambda and AWS API Gateway, I do not understand why you’d want a single monolithic function behind many different API paths. I’m guessing that people who think you should do monolithic serverless haven’t thought about sensible organization of their functions, and orchestration of them using the AWS CLI or API. They are managing them through the AWS dashboard and are thinking, “man this is a lot of work, let’s just do a single function, with the routing built in.” Similar to folks thinking a monorepo is a good idea over many different repos, without ever thinking about organizations using Github organizations, and orchestration using Git and the Github API, people aren’t getting creative with their Lamdba functions. People seem to be in love with brainstorming and dreaming about decoupled approaches to doing APIs,...[<a href="/2018/07/26/monolithic-serverless-wtf/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/26/kicking-the-tires-on-the-sap-api-business-hub/"><img src="https://s3.amazonaws.com/kinlane-productions2/sap/sap-api-business-hub.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/26/kicking-the-tires-on-the-sap-api-business-hub/">Kicking The Tires On The SAP API Business Hub</a></h3>
			<p><em>26 Jul 2018</em></p>
			<p>I told the folks over at SAP that I would take a look at their API Business Hub. It isn’t paid work, just helping provide feedback on another addition to the API discovery front, something I’m pretty committed to helping push forward in any way that I can. They’ve pulled together a pretty clean, OpenAPI driven catalog of useful APIs for the enterprise, so I wanted to make sure I kick the tires and size it up alongside the other API discovery work I am doing. The SAP API Business Hub is a pretty simple and clean catalog for searching and browsing applications, integrations, as well as APIs–I am going to focus in on the API section. Which at first glance looks to have about 70 separate APIs, but then you notice each of them are just umbrellas for each API platform, and some of them contain many different API endpoints. Some of the APIs are simple language translation and text extraction resources, while others provide robust access to the SAP S/4HANA Cloud, SAP Ariba, and other SAP systems. You see a lot of SAP focused solutions, but then you also see a handful of partner solutions added via their platform partner program. I see the beginnings of a useful API catalog getting going on over at the SAP API Business Hub. Each API is well documented, and provides an OpenAPI definition for each API, complete with interactive documentation you can play within a sandbox environment. More than most API catalogs, marketplaces, and directories I profile have available. Allowing you to kick the tires and see what is going on, before working with the production version. They also provide you with a Java SDK to download for each API, something that could easily be expanded to support many different platforms, programming languages, and continuous integration cycles with solutions like APIMATIC. Making it more of a discovery, as well as integration marketplace. Like any API marketplace...[<a href="/2018/07/26/kicking-the-tires-on-the-sap-api-business-hub/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/26/avoid-being-captain-obvious-when-documenting-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/captain_small.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/26/avoid-being-captain-obvious-when-documenting-your-api/">Avoid Being Captain Obvious When Documenting Your API</a></h3>
			<p><em>26 Jul 2018</em></p>
			<p>I read a lot of API documentation, and help review API portals for clients, and one of the most common rookie mistakes I see made, is people pointing out the obvious, and writing a bunch of fluffy, meaningless content that gets in the way of people actually using an API. When the obvious API industry stuff is combined with the assumed elements of what a company does, you end up with a meaningless set of obstacles that slow API integration down. Here is the most common thing I read when entering an API portal: “This is an API for querying data from the [Company X] platform, to get access to JSON from our system which allows you to get data from our system into yours using the web. You will need to write code to make calls to our APIs documented here on the page below. Our API uses REST to accept request and provide responses in a JSON format.” I’ve read API after API that never tells you what the API does. It just assumes you know what the company does, and then goes into verbose explanations of what API, REST, JSON, and other things that should be intuitive if an API is well designed, and immediately accessible via an API. People tend to make to many assumptions about API consumers already knowing what a company does, while also assuming they known absolutely nothing about APIs, and burying actual API documentation behind a bunch of API blah blah blah, instead of just doing and being the API. It is another side effect of developers, database, and IT folk not being very good at thinking outside of their bubble. It goes beyond techies not having social skills, and is more about them not having to think about other people at all. They just don’t have the ability to put themselves in the shoes of someone landing on the home page of their developer portal, and...[<a href="/2018/07/26/avoid-being-captain-obvious-when-documenting-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/19/discover-profile-quantify-rank-and-publish-new-apis-to-the-streamdata-io/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_blue_circuit_3.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/19/discover-profile-quantify-rank-and-publish-new-apis-to-the-streamdata-io/">Discover, Profile, Quantify, Rank, And Publish New APIs To The Streamdata.io</a></h3>
			<p><em>19 Jul 2018</em></p>
			<p>About 60% of my work these days is building upon the last five years of my API Stack research, with a focus on building out the Streamdata.io API Gallery. We are fine tuning our approach for discovering new API-driven resources from across the landscape, while also profiling, quantifying, ranking, and publishing to the Streamdata.io API Gallery, The API Stack, and potentially other locations like the Postman Network, APIs.Guru, and other API discovery destinations I am working with. Helping us make sense of the increasingly noisy API landscape, while identifying the most valuable resources, and then profiling them to help reduce friction when it comes to potentially on-boarding and streaming data from each resource. Discover New API-Driven Resources Finding new APIs isn’t too difficult, you just have to Google for them. Finding new APIs in an automated way, with minimum human interaction becomes a little more difficult, but there are some proven ways to get the job done. There is no single place to go find new APIs, so I’ve refined a list of common place I use to discover new APIs: Search Engines - Using search engine APIs to look for APIs based upon the vocabulary we’ve developed. Github - Github provides a wealth of signals when it comes to APIs, and we use the Github API to discover interesting sources using our vocabulary. Stack Overflow - Using the Stack Exchange API, we are able to keep an eye out for developers talking about different types of interesting APIs. Twitter - The social network still provides some interesting signals when it comes to discussions about interesting APis. Reddit - There are many developers who still use Reddit to discuss technical topics, and ask questions about the APIs they are using. Using the topic and entity vocabulary we’ve been developing, we can automate the discovery of new APIs across these sources using their APIs. Helping track on signals for the existing APIs we are keeping an...[<a href="/2018/07/19/discover-profile-quantify-rank-and-publish-new-apis-to-the-streamdata-io/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/19/api-governance-models-in-the-public-and-private-sector/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_113_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/19/api-governance-models-in-the-public-and-private-sector/">API Governance Models In The Public and Private Sector</a></h3>
			<p><em>19 Jul 2018</em></p>
			<p>This is a report for the Department of Veterans Affairs microconsulting project, “Governance Models in Public and Private Sector”. Providing an overview of API governance to help the VA, “understand, with the intention to adopt, best practices from the private and public sector, specifically for prioritizing APIs to build, standards to which to build APIs, and making the APIs usable by external consumers.” Pulling together several years of research conducted by industry analyst API Evangelist, as well as phone interviews with API practitioners from large enterprise organizations who are implementing API governance on the ground across the public and private sector, conducted by Skylight Digital. We’ve assembled this report to reflect the interview conversations we had with leaders from the space, helping provide a walk through of the types or roles and software architecture being employed to implement governance at large organizations. Then we walk through governance as it pertains to identifying possible APIs, developing standards around the delivery of APIs, how organizations are moving APIs into production, as well as presenting them to their consumers. Wrapping up with an overview of formal API governance details, as well as an acknowledgement that most API governance is rarely ever a fully formed initiative at this point in time. Providing a narrative for API governance, with a wealth of bulleted elements that can be considered, and assembled in the service of helping govern the API efforts across any large enterprise. Roles Within An Organization There are many roles being used by organizations who are leading the conversation around the delivery of high quality, industry changing APIs. Defining the personalities that are needed to make change across large organizations when it comes to delivering APIs consistently at scale. While there may be many names for the specific roles leading the charge it is clear that these people are bringing a unique blend of skills to an organization, with an emphasis in a couple of key areas: Leadership -...[<a href="/2018/07/19/api-governance-models-in-the-public-and-private-sector/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/16/tvmaze-uses-hal-for-their-api-media-type/"><img src="https://s3.amazonaws.com/kinlane-productions2/tv-maze/tvm_api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/16/tvmaze-uses-hal-for-their-api-media-type/">TVMaze Uses HAL For Their API Media Type</a></h3>
			<p><em>16 Jul 2018</em></p>
			<p>
One of the layers of the API universe where I come across an increased number Hypermedia APIs is in the movie, television, and entertainment space. Where having a more flowing API experience makes a lot of sense, and the extra investment in link relations will pay off. One example of this I recently came across was over at TVMaze, who has a pretty robust hypermedia API, where they opted for using HAL as their media type.

Like any good hypermedia should, TVMaze begins with its root URL: http://api.tvmaze.com, and provides a robust set of endpoints from there:

Search

  Show Search
  Show single search
  Show Lookup
  People search


Schedule

  Full Schedule


Shows

  Show main information
  Show episode list
  Episode by number
  Episodes by date
  Show seasons
  Season episodes
  Show cast
  Show crew
  Show AKA’s
  Show index


People

  Person main information
  Person cast credits
  Person crew credits


Updates

  Show updates


The TVMaze API isn’t an overly complex hypermedia API. I think it is simple, elegant, and shows how you can use link relations to establish a more meaningful experience for API consumers. Allowing you to navigate the large, ever-changing catalog of television shows, allowing the API client to do the heavy lifting of navigating the shows, schedules, and people involved with each production.

There hasn’t been enough showcasing of the hypermedia APIs available out there. Usually once a year I remember to give the subject some attention, or when I come across interesting ones like TVMaze. Hypermedia isn’t just an academic idea anymore, and is something that has gotten traction in a number of sectors, and I keep seeing signs of growth and adoption. I don’t think it will be the API solution most hypermedia believers envisioned it, but I do think it is a viable tool in our API toolbox, and for the right projects it makes a lot of sense.

[<a href="/2018/07/16/tvmaze-uses-hal-for-their-api-media-type/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/16/if-a-search-for-swagger-or-openapi-doesnt-yield-results-i-try-for-a-postman/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80_86_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/16/if-a-search-for-swagger-or-openapi-doesnt-yield-results-i-try-for-a-postman/">If A Search For Swagger or OpenAPI Doesnt Yield Results I Try For A Postman</a></h3>
			<p><em>16 Jul 2018</em></p>
			<p>While profiling any company, a couple of the Google searches I will execute right away are for “[Company Name] Swagger” and “[Company Name] OpenAPI”, hoping that a provide is progressive enough to have published an OpenAPI definition–saving me hours of work understanding what their API does. I’ve added a third search to my toolbox, if these other two searches do not yield results, searching for “[Company Name] Postman”, revealing whether or not a company has published a Postman Collection for their API–another sign of a progressive, outward thinking API provider in my book. A machine readable definition for an API tells me more about what a company, organization, institution, or government agency does, than anything else I can dig up on their website, or social media profiles. An OpenAPI definition or Postman Collection is a much more honest view of what an organization does, than the marketing blah blah that is often available on a website. Making machine readable definitions something I look for almost immediately, and prioritize profiling, reviewing, and understanding the entities I come across with a machine readable definition, over those that do not. I only have so much time in a day, and I will prioritize an entity with an OpenAPI or Postman, over those who do not. The presence of an OpenAPI and / or Postman Collection isn’t just about believing in the tooling benefits these definitions provide. It is about API providers thinking externally about their API consumers. I’ve met a lot of API providers who are dismissive of these machine readable definitions as trends, which demonstrates they aren’t paying attention to the wider API space, and aren’t thinking about how they can make their API consumers lives easier–they are focused on doing what they do. In my experience these API programs tend to not grow as fast, focus on the needs of their integrators and consumers, and often get shut down after they don’t get the results they...[<a href="/2018/07/16/if-a-search-for-swagger-or-openapi-doesnt-yield-results-i-try-for-a-postman/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/16/for-every-competitor-you-keep-out-of-your-api-docs-you-are-keeping-twenty-new/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/old-door-lock_smoking_cigarette.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/16/for-every-competitor-you-keep-out-of-your-api-docs-you-are-keeping-twenty-new/">For Every Competitor You Keep Out Of Your API Docs You Are Keeping Twenty New</a></h3>
			<p><em>16 Jul 2018</em></p>
			<p>It is interesting for me to still regularly come across so many API providers who have a public API portals, but insist on keeping most of their documentation behind a login. Stating that they are concerned with competitors getting access to the design of their API and the underlying schema. Revealing some indefensible API business models, and general paranoia around doing business on the web. Something that usually is a sign for me of a business that is working really hard to maintain a competitive grip within an industry, without actually having to do the hard work of innovating and moving the conversation forward. Confident API providers know that you can put your API documentation out in the open, complete with schema, without giving away the farm. If your competition can take your API design, and underlying schema, and recreate your business–you should probably go back to the drawing board, and come up with a new business idea. Your API and schema definition is not your business. I’ve used this comparison may times–your API docs are like a restaurant menu. Can you imagine restaurants that kept them hidden until they were sure you are going to be a customer? If you think that your competition can read your menu and recreate all your dishes, then you won’t be in business very long, because your dishes probably weren’t that special to begin with. For every competitor you keep out of your API documentation, you are keeping twenty new customers out as well. I’m guessing that your savvy competitors are going to be able to get in anyways with a fake account, or otherwise. Don’t waste your time on hiding your API and keeping it out of the view of your potential customers–invest your energy in making sure your APIs kick ass. To use the restaurant analogy again, make sure ingredients are the best, and your processes, and your service are top notch. Don’t make your menu...[<a href="/2018/07/16/for-every-competitor-you-keep-out-of-your-api-docs-you-are-keeping-twenty-new/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/16/concern-around-working-with-many-github-repositories/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/114_69_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/16/concern-around-working-with-many-github-repositories/">Concern Around Working With Many Github Repositories</a></h3>
			<p><em>16 Jul 2018</em></p>
			<p>I’m regularly fascinated with API development teams I work with expressing their concern with working with many Github repositories. With all of the complexity I watch teams embrace when it comes to frameworks, scaffolding, continuous integration, deployment, and orchestration solutions, I’m lost on why many Github repositories suddenly become such a challenge. A Github organization is a folder, and a Github repository is a folder, that you can check out locally, on your server, or work with via an API, or Git. It is a distributed file store, that you can orchestrate with programmatically, and can be as logical or illogical as you design it to be. I work really hard to keep technical complexity to a minimum in my world, but this means limiting unnecessary vendor lock-in, and tech just because it is the latest trend. For me, Git is just a distributed file system, with version control built in, with Github providing a nice API and network effect layer that makes it compelling. Referencing a Github folder is just a matter of using its org and repo name, checking it out, and working with the standardized layout of information I have published there. Allowing me to work with, and orchestrate thousands of separate folders (repos), across almost 50 organizations (folders), in a consistent way, as a one person team. Something that has taken me about four years to setup, and fine tune, but has become essential to what I do on a daily basis. I’m not saying working with hundreds or thousands of individual Github repositories can’t be complex, or doesn’t take a significant amount of work. I’m just saying I’m intrigued by how technologists who manage large systems, adopt complex frameworks for delivering simple web solutions, and regularly make other complex technological investments, draw the line here. I see Github as a robust file store, with two doorways, 1) Git, and 2) API. I have a standardized structure to what I store...[<a href="/2018/07/16/concern-around-working-with-many-github-repositories/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/12/my-api-lifecycle-checklist-and-scorecard/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-blue-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/12/my-api-lifecycle-checklist-and-scorecard/">My API Lifecycle Checklist And Scorecard</a></h3>
			<p><em>12 Jul 2018</em></p>
			<p>I am working on delivering a handful of new APIs, which I will be turning into products. I will be prototyping, developing, and operating them in production environments for myself, and for a handful of customers. To help guide my workflow, I’ve crafted this API lifecycle definition to help direct my efforts in an ongoing lifecycle approach. Define - Define the problem and solution in a human and machine readable way. Repository - Establish a Github repository. README - Craft a README for the repository. Title - Provide title for the service. Description - Provide concise description for the service. Goals - Establish goals for the service. Schema - Organize loose, and JSON schema for the service. OpenAPI - Establish an OpenAPI for the service. Assertions - Craft a set of assertions for the service. Team - Define the team behind the service. Design - Establish a base set of design practices for the service. Versioning - Determine how the code, schema, and the API be versioned. Base Path - Set the base path for the service. Path(s) - Define a set of resource paths for the service. Verb(s) - Define which HTTP, and other verbs will be used for the service. Parameters - Define a list of query parameters in use to work with service. Headers - Define the HTTP Headers that will be used to work with the service. Response(s) - Provide a resulting message and associated schema definition for the service. Media Types - Define whether service will return CSV, JSON, and / or XML responses. Status Codes - Define the available status code for each responses. Pagination - Define how pagination will be handled for requests and responses. Sorting - Define how sorting will be handled for requests and responses. Database - Establish the base for how data will be managed behind the service. Platform - Define which platform is in use to manage the database. Schema - Establish the schema...[<a href="/2018/07/12/my-api-lifecycle-checklist-and-scorecard/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/11/ping-identity-acquires-elasticbeam-to-establish-new-api-security-solution/"><img src="https://s3.amazonaws.com/kinlane-productions2/elastic-beam/image.ping.480.medium.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/11/ping-identity-acquires-elasticbeam-to-establish-new-api-security-solution/">Ping Identity Acquires ElasticBeam To Establish New API Security Solution</a></h3>
			<p><em>11 Jul 2018</em></p>
			<p>You don’t usually find me writing about API acquisitions unless I have a relationship with the company, or there are other interesting aspects of the acquisition that makes it noteworthy. This acquisition of Elastic Beam by Ping Identity has a little of both for me, as I’ve been working with the Elastic Beam team for over a year now, and I’ve been interested in what Ping Identity is up to because of some research I am doing around open banking in the UK, and the concept of an industry level API identity and access management, as well as API management layer. All of which makes for an interesting enough mix for me to want to quantify here on the blog and load up in my brain, and share with my readers. From the press release, “Ping Identity, the leader in Identity Defined Security, today announced the acquisition of API cybersecurity provider Elastic Beam and the launch of PingIntelligence for APIs.” Which I think reflects some of the evolution of API security I’ve been seeing in the space, moving being just API management, and also being about security from the outside-in. The newly combined security solution, PingIntelligence for APIs, focuses in on automated API discovery, threat detection &amp; blocking, API deception &amp; honeypot, traffic visibility &amp; reporting, and self-learning–merging the IAM, API management, and API security realms for me, into a single approach to addressing security that is focused on the world of APIs. While I find this an interesting intersection for the world of APIs in general, where I’m really intrigued by the potential is when it comes to the pioneering open banking API efforts coming out of the UK, and the role Ping Identity has played. “Ping’s IAM solution suite, the Ping Identity Platform, will provide the hub for Open Banking, where all UK banks and financial services organizations, and third-party providers (TPPs) wanting to participate in the open banking ecosystem, will need to...[<a href="/2018/07/11/ping-identity-acquires-elasticbeam-to-establish-new-api-security-solution/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/11/monetizing-your-device-location-data-with-lotadata/"><img src="https://s3.amazonaws.com/kinlane-productions2/lotadata/lotadata-platform.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/11/monetizing-your-device-location-data-with-lotadata/">Monetizing Your Device Location Data With LotaData</a></h3>
			<p><em>11 Jul 2018</em></p>
			<p>There are a lot of people making money off of the acquisition, organization, and providing access to data in our digital world. While I quietly tune into what the data monetization trends are, I am also actively looking for interesting approaches to generating revenue from data, but specifically with an eye on revenue sharing opportunities for the owners or stewards of that data. You know as opposed to just the exploitation of people’s data, and generating of revenue without them knowing, or including them in the conversation. To help counteract this negative aspect of the data economy, I’m always looking to highlight (potentially) more positive outcomes when it comes to making money from data. I was recently profiling the API of the people intelligence platform LotaData, and I came across their data monetization program, which provides an interesting look at how platforms can help data stewards generate revenue from data, but in a way that makes it accessible to individuals looking to monetize their own data as well. “LotaData’s AI platform transforms raw location signals into ‘People Intelligence’ for monetization, usually based upon the follow key attributes: latitude, longitude, timestamp, deviceID, and accuracy.” Representing activity at a location and/or point in time, allowing LotaData’s to understand what is happening at specific places at scale, and develop meaningful insights and behavioral segments that other companies and government agencies will want to buy into. Some of the examples they provide are: Commuting daily on CalTrain from Palo Alto to San Francisco Mid-week date night at Nopa on the way back from work Sweating it out at Soul Cycle on Saturday mornings Taking the dog out for a walk on Sunday afternoons Season ticket holder for Warriors games at the Oakland Arena LotaData’s location-based insights and segments are entirely inferred from raw location signals, emphasizing that they do not access or collect any personally identifiable information (PII) from mobile phones–stating that they “do not and never will collect...[<a href="/2018/07/11/monetizing-your-device-location-data-with-lotadata/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/11/i-am-sorry-but-your-company-is-too-big-for-me-to-talk-to/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/104_198_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/11/i-am-sorry-but-your-company-is-too-big-for-me-to-talk-to/">I Am Sorry, But Your Company Is Too Big For Me To Talk To</a></h3>
			<p><em>11 Jul 2018</em></p>
			<p>It is funny work with companies, organizations, institutions, and government agencies of all shapes and sizes, and learn all the weird practices they have, and the strange belief systems they’ve established. One day I will be talking to a 3 person startup, the next day I’ll be talking with a large bank, and after that I’ll be working with a group at a massive government agency. I have to be mindful of my time, make sure I’m meeting my mission, having an impact, as well as paying my bills, but for the most part I don’t have any entrenched rules about who I will talk to, or who I will share my knowledge with. One thing I chuckle at regularly is when I come across large organizations who will gladly talk with me, and tap my knowledge, but won’t work with some of the startups I work with, or the conferences I produce, because they are “too small”. They can’t waste their time working with small startups because it won’t bring the scope of revenue they need to justify the relationships, but they’ll gladly talk to me and welcome the exposure and knowledge I might bring. Sometimes I feel like telling organizations, “sorry you are just to large to work with, you are almost guaranteed to fail at this whole API thing, why should I bother?” I think I’ll say it sometimes jokingly, but not really interested in truly being a dick at that level. Most large organizations can’t figure out how to work with me in any long term anyway, because they are too bureaucratic and slow moving. Other large organizations have no problem figuring out how to get me past legal, and getting me paid, but some just can’t figure it out. I had one large enterprise group who follows my work, wanted to get me in really badly, but their on-boarding team needed proof that I was the API Evangelist going back...[<a href="/2018/07/11/i-am-sorry-but-your-company-is-too-big-for-me-to-talk-to/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/10/using-openapi-and-json-patch-to-articulate-changes-for-your-api-road-map/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/downtheline_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/10/using-openapi-and-json-patch-to-articulate-changes-for-your-api-road-map/">Using OpenAPI And JSON PATCH To Articulate Changes For Your API Road Map</a></h3>
			<p><em>10 Jul 2018</em></p>
			<p>I’m doing a lot of thinking regarding how JSON PATCH can be applied because of my work with Streamdata.io. When you proxy an existing JSON API with Streamdata.io, after the initial response, every update sent over the wire is articulated as a JSON PATCH update, showing only what has changed. It is an efficient, and useful way to show what has changed with any JSON API response, while being very efficient about what you transmit with each API response, reducing polling, and taking advantage of HTTP caching. As I’m writing an OpenAPI diff solution, helping understand the differences between OpenAPI definitions I’m importing, and allowing me to understand what has changed over time, I can’t help but think that JSON PATCH would be a great way to articulate change of the surface area of an API over time–that is, if everyone loyally used OpenAPI as their API contract. Providing an OpenAPI diff using JSON PATCH would be a great way to articulate an API road map, and tooling could be developed around it to help API providers publish their road map to their portal, and push out communications with API consumers. Helping everyone understand exactly what is changing in way that could be integrated into existing services, tooling, and systems–making change management a more real time, “pipelinable” (making this word up) affair. I feel like this could help API providers better understand and articulate what might be breaking changes. There could be tooling and services that help quantify the scope of changes during the road map planning process, and teams could submit OpenAPI definitions before they ever get to work writing code, helping them better see how changes to the API contract will impact the road map. Then the same tooling and services could be used to articulate the road map to consumers, as the road map becomes approved, developed, and ultimately rolled out. With each OpenAPI JSON PATCH moving from road map to change...[<a href="/2018/07/10/using-openapi-and-json-patch-to-articulate-changes-for-your-api-road-map/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/10/people-do-not-use-tags-in-their-openapi-definitions/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_158_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/10/people-do-not-use-tags-in-their-openapi-definitions/">People Do Not Use Tags In Their OpenAPI Definitions</a></h3>
			<p><em>10 Jul 2018</em></p>
			<p>I import and work with a number of OpenAPI definitions that I come across in the wild. When I come across a version 1.2, 2.0, 3.0 OpenAPI, I import them into my API monitoring system for publishing as part of my research. After the initial import of any OpenAPI definition, the first thing I look for is the consistent in the naming of paths, the availability of summary, descriptions, as well as tags. The naming conventions used is paths is all over the place, some are cleaner than others. Most have a summary, with fewer having descriptions, but I’d say about 80% of them do not have any tags available for each API path. Tags for each API path are essential to labeling the value a resource delivers. I’m surprised that API providers don’t see the need for applying these tags. I’m guessing it is because they don’t have to work with many external APIs, and really haven’t put much thought into other people working with their OpenAPI definition beyond it just driving their own documentation. Many people still see OpenAPI as simply a driver of API documentation on their portal, and not as an API discovery, or complete lifecycle solution that is portable beyond their platform. Not considering how tags applied to each API resource will help others index, categorize, and organize APIs based upon the value in delivers. I have a couple of algorithms that help me parse the path, summary, and description to generate tags for each path, but it is something I’d love for API providers to think more deeply about. It goes beyond just the resources available via each path, and the tags should reflect the overall value an API delivers. If it is a product, event, messaging, or other resource, I can extract a tag from the path, but the path doesn’t always provide a full picture, and I regularly find myself adding more tags to each API(if I...[<a href="/2018/07/10/people-do-not-use-tags-in-their-openapi-definitions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/10/my-moving-towards-a-modern-api-lifecycle-from-postcon-2018/"><img src="https://s3.amazonaws.com/kinlane-productions2/talks/postcon/moving-towards-a-modern-api-lifecycle-postcon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/10/my-moving-towards-a-modern-api-lifecycle-from-postcon-2018/">My Moving Towards a Modern API Lifecycle From POST/CON 2018</a></h3>
			<p><em>10 Jul 2018</em></p>
			<p>I gave a talk early in in June at POST/CON 2018 in San Francisco. The conference was a great mix of discussions reflecting the Postman community. You can find all the talks on Google, including mine about moving towards a modern AP lifecycle.



You can find all the stop along what I consider to be a modern API lifecycle on the home page of API Evangelist, with links to any of my research, services, tooling, and other storytelling I’ve done in each area.

Thanks again to Postman for having me out!

[<a href="/2018/07/10/my-moving-towards-a-modern-api-lifecycle-from-postcon-2018/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/10/long-running-api-requests-and-differential-api-responses/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80_140_800_500_0_max_0_-5_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/10/long-running-api-requests-and-differential-api-responses/">Long Running API Requests And Differential API Responses</a></h3>
			<p><em>10 Jul 2018</em></p>
			<p>I am shifting my long running API operations from a PHP / EC2 based implementation to a more efficient Node.js / Lambda based solution, and I promised James Higginbotham (@launchany) that I’d share a breakdown of my process with him a month or so back. I’m running 100+, to bursts of 1000+ long running API requests for a variety of purposes, and it helps me to tell the narrative behind my code, introducing some coherence into the why and how of what I’m doing, while also sharing with others along the way. I had covered my earlier process a little bit in a story a few months ago, but as I was migrating the process, I wanted to further flesh out, and make sure I wasn’t mad. The base building block of each long running API request I am making is HTTP. The only difference between these API requests, and any others I am making on a daily basis, is that they are long running–I am keeping them alive for seconds, minutes, and historically hours. My previous version of this work ran as long running server side jobs using PHP, which I monitored and kept alive as long as I possibly could. My next generation scripts will have a limit of 5 minutes per API request, because of constraints imposed by Lambda, but I am actually find this to be a positive constraint, and something that will be helping me orchestrate my long running API requests more efficiently–making them work on a schedule, and respond to events. Ok, so why am I running these API calls? A variety of reasons. I’m monitoring a Github repository, waiting for changes. I’m monitoring someone’s Twitter account, or a specific Tweet, looking for a change, like a follow, favorite, or retweet. Maybe I’m wanting to know when someone asks a new question about Kafka on Stack Overflow, or Reddit. Maybe I’m wanting to understand the change schedule for a...[<a href="/2018/07/10/long-running-api-requests-and-differential-api-responses/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/09/using-plain-language-in-your-api-paths/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/power_colorful_blocks.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/09/using-plain-language-in-your-api-paths/">Using Plain Language In Your API Paths</a></h3>
			<p><em>09 Jul 2018</em></p>
			<p>It is tough to help developers think outside of the world they operate within. Most software is still developed and managed within silos, knowing it’s inner workings will never be seen by anyone outside of the team. This mode of operation is a rich environment for poor code quality, and teams with port communication. This is one of the reasons I’ve embraced web APIs, after running software development teams since the 1990s, I’ve been put in charge of some pretty dysfunctional teams, and some pretty unwieldy legacy codebases, so once I started working out in the open using web APIs, I did’t want to go back. Web APIs aren’t the cure for all of our technology problems, but it does begin to let some sunlight in on some messed up ways of doing things. One common illness I still see trickling out of API operations are developers not using plain language. Speaking in acronyms, code, and other cryptic ways of articulating the resources they are exposing. I came across a set of API resources for managing a DEG the other day. You could add, updated, delete and get DEGs. You can also pull analytics, history, and other elements of a DEG. I spent about 10-15 minutes looking around their developer portal, documentation, and even Googling, but never could figure out what a DEG was. Nowhere in their documentation did they ever tell consumers what a DEG was, you just had to be in the know I guess. The API designer (if that occurred) and developer had never stopped to consider that maybe someone would stumble across their very public API and not know what a DEG was. Demonstrating how us developers have trouble thinking outside our silos, and thinking about what others will need. There is no reason that your API paths shouldn’t be plain language, using common words. I’m not even talking about good RESTful resource design, I’m simply talking about looking at the...[<a href="/2018/07/09/using-plain-language-in-your-api-paths/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/09/operating-your-api-in-the-cloud-kill-zone/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/beachclouds_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/09/operating-your-api-in-the-cloud-kill-zone/">Operating Your API In The Cloud Kill Zone</a></h3>
			<p><em>09 Jul 2018</em></p>
			<p>When you operate your application within the API ecosystem of a large platform, depending on the platform, you might have to worry about the platform operator copying, and emulating what you do. Twitter has long been accused of sharecropping within their ecosystem, and other larger platforms have come out with similar features to what you can find within their API communities. Not all providers take the ideas, it is also very common for API platforms to acquire talent, features, and applications from their ecosystems–something that Twitter has done regularly. Either way, API ecosystems are the R&amp;D, and innovation labs for many platforms, where the latest features get proven. As the technology playing field has consolidated across three major cloud providers, AWS, Azure, and Google, this R&amp;D and innovation zone, has become more of a cloud kill zone for API providers. Where the cloud giants can see the traction you are getting, and decide whether or not they want to launch a competing solution behind the scenes. Investors are tuning into this new cloud kill zone, and in many cases opting not to invest in startups who operate on a cloud platform, afraid that the cloud giant will just come along and copy a service, and begin directly competing with companies operating within their own ecosystem. Making it a kill zone for API providers, who can easily be assimilated into the AWS, Azure, or Google stack, and left helpless do anything but wither on the vine, and die. Much like other API ecosystems, AWS, Azure, and Google all have the stats on who is performing across their platforms, and they know which solutions developers are demanding. Factoring in the latest growth trends into their own road maps, and making the calculations around whether they will be investing in their own solutions, or working to partner, and eventually acquire a company operating with this new kill zone. The 1000 lb cloud gorillas carry a lot of weight...[<a href="/2018/07/09/operating-your-api-in-the-cloud-kill-zone/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/09/i-love-the-api-enthusiasm-predix-but-please-publish-an-api-style-guide-for/"><img src="https://s3.amazonaws.com/kinlane-productions2/predix/predix-diagram.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/09/i-love-the-api-enthusiasm-predix-but-please-publish-an-api-style-guide-for/">I Love The API Enthusiasm Predix, But Please Publish An API Style Guide For</a></h3>
			<p><em>09 Jul 2018</em></p>
			<p>I was profiling the volume of API from the Internet of Things platform Predix this last week. Luckily they have OpenAPI definitions for each of the APIs, something that makes my life a lot easier. As, they have a wealth of APIs available, doing an amazing amount of work when it comes to connecting devices to the Internet–I love their enthusiasm for putting out APIs. My only critical feedback for them after working my way through their API definitions, is they should invest some time to develop an API design guide, and distribute across their teams. The wild variances in definition and design of their APIs made me stumble a number of times while learning about what they do. While looking through the definitions for the Predix APIs, I found many inconsistent patterns between them, and you could tell that they had different teams (or individuals) working across the suite of APIs. The inconsistencies ranged from the naming, description, and how the meta data was provided for each API, all the way to acronyms used in API paths, and other things that prevented me from understand what an API did all together. While I am stoked they provide OpenAPI definitions for all of their APIs, I still struggled to understand what was possible with many of their APIs. It kind of feels like they need an external editor to review each API definition before it leaves the door, as well as some sort of automated validation using JSON schema, that would work against a common set of API design standards. I can tell that Predix has an extremely powerful stack of Internet of Things API resources. They have insight, predictive, and event-driven layers, and a wealth of resources for device operators to put to work. They just need another layer of API design polish on their APIs, as well as ensuring their API documentation reflects this design polish, helping bring it all home. If they...[<a href="/2018/07/09/i-love-the-api-enthusiasm-predix-but-please-publish-an-api-style-guide-for/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/07/09/concerns-around-managing-many-microservice-repositories-and-going-with-a-mono/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/lacloudydaypano_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/07/09/concerns-around-managing-many-microservice-repositories-and-going-with-a-mono/">Concerns Around Managing Many Microservice Repositories And Going With A Mono</a></h3>
			<p><em>09 Jul 2018</em></p>
			<p>About half of the teams I work with on microservices strategy are beginning to freak out about the number repositories they have, and someone is regularly bringing up the subject of having a mono repo. Which is usually a sign for me that a group is not ready for doing the hard work involved with microservices, but also shows a lack of ability to think, act, and respond to things in a distributed way. It can be a challenge to manage many different repositories, but with a decoupled awareness of the sprawl that can exist, and some adjustments and aggregation to your strategy it can be doable, even for a small team. The most import part of sticking to multiple repositories is for the sake of the code. Keeping services decoupled in reality, not just name is extremely important. Allowing the code behind each service to have its own repository, and build a pipeline that keeps things more efficient, nimble, and fast. Each service you layer into a mono repo will be one more chunk of time needed when it comes to builds, and understanding what is going on with the codebase. I know there are a growing number of strategies for managing mono repos efficiently, but it is something that will begin to work against your overall decoupling efforts, and you are better off having a distributed strategy in place, because code is only the first place you’ll have to battle centralization, in the name of a more distributed reality. Github, Gitlab, and Bitbucket all have an API, which makes all of your repositories accessible in a programmatic way. If you are building microservices, and working towards a distributed way of doing things, it seems like you should be using APIs to aggregate and automate your reality. It is pretty easy to setup an organization for each grouping of microservices, and setup a single master or control repository where you can aggregate information, and...[<a href="/2018/07/09/concerns-around-managing-many-microservice-repositories-and-going-with-a-mono/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/06/29/mayors-governors-and-lawmakers-tech-companies-are-getting-rich-mining-your-constituents-data/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64_99_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/06/29/mayors-governors-and-lawmakers-tech-companies-are-getting-rich-mining-your-constituents-data/">Mayors, Governors, And Lawmakers: Tech Companies Are Getting Rich Mining Your Constituents Data</a></h3>
			<p><em>29 Jun 2018</em></p>
			<p>It has been a fascinating and eye opening experience sitting at the intersection of tech startups and the web, mobile, and device applications they’ve built over the last decade. In 2010 I was captivated by the power of APIs to deliver resources to developers, and end-users. In 2018, I’m captivated by the power of APIs to mine end-users like they are just a resource, with the assistance of the developer class. A dominant white male class of people who are more than willing to look the other way when exploitation occurs, and make for the perfect “tools” to be exploited by the wealthy investor class. While I do not have much hope for diversity efforts in tech, or the bro culture waking up, I do have hope for city and state/provincial lawmakers to wake up to the exploitation that is going on. I’ve seen hints of cities waking up to the mining that has been occurring by Facebook and Google over the last decade. The open exploitation and monetization of a city’s and state’s most precious resources–their constituents. While some cities are still swooning over having Amazon set up shop, or Facebook to build a data center, these company’s web, mobile, and device applications have infiltrated their districts been probing, mining, extracting, and shipping value back to offshore corporate headquarters. You can see this occurring with Google Maps, which has long been a darling of the API community. We were all amazed at the power of this new mapping resource, something us developers could never have built on our own. We all integrated it into our websites, and embedded it into our mobile applications. We could use it to navigate and find where we were going, completely unaware of the power of the application to mine data from our local transit authorities, businesses, as well as track the location of all of us at each moment. Google Maps was the perfect trojan horse to invade...[<a href="/2018/06/29/mayors-governors-and-lawmakers-tech-companies-are-getting-rich-mining-your-constituents-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/06/29/looking-for-sponsors-for-apistrat-2018-in-nashville-tn-this-september/"><img src="https://s3.amazonaws.com/kinlane-productions2/apistrat/apistrat-conference-sponsorship-prospectus.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/06/29/looking-for-sponsors-for-apistrat-2018-in-nashville-tn-this-september/">Looking For Sponsors For APIStrat 2018 In Nashville, TN This September</a></h3>
			<p><em>29 Jun 2018</em></p>
			<p>We are building up to the 9th edition of API Strategy &amp; Practice (APIStrat) happening in Nashville, Tennessee this September 24th through 26th. As part of the build up we are looking for sponsors to help make the event happen, bringing the API community together once again to share stories from the trenches, and discuss healthy practices that are allowing companies, organizations, institutions, and government agencies make an impact when it comes to their API operations. The 2017 edition of APIStrat in Portland, OR was a huge success, and help complete the transition of APIStrat to be part of the OpenAPI Initiative (OAI). After seven editions, and four years of operation exclusively by 3Scale and API Evangelist, the event has matured and will continue growing under the guidance of the OAI, and the community that has evolved around the OpenAPI specification. Presenting an opportunity for other API providers, and API service providers to get involved by joining as an OAI member and / or sponsoring APIStrat, and joining the conversation that has been going on in the community since early 2013. You can download the APIStrat conference prospectus from the Linux Foundation / OAI event website, and there is a form to submit to learn more about sponsoring. You can also email [email&#160;protected] if you’d like to get plugged in. Feel free to also reach out to me as well, as I’m in charge of trying to drum up sponsors, and expand our base beyond just the OAI membership, and the companies who stepped up last year. Helping API providers and service providers understand what a community event APIStrat is, and help it differentiate from the other API, and tech-focused conferences that are happening. I’m definitely biased, as I help start and grow the conference, but after running tech events for over a decade, it was important to me that APIStrat grow into a community event about ideas, and less about vendors and product pitches....[<a href="/2018/06/29/looking-for-sponsors-for-apistrat-2018-in-nashville-tn-this-september/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/06/29/graphql-and-rest-differences-explained-with-burgers/"><img src="https://s3.amazonaws.com/kinlane-productions2/graphql/DgsXLk_X4AEKiJJ.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/06/29/graphql-and-rest-differences-explained-with-burgers/">GraphQL And REST Differences Explained With Burgers</a></h3>
			<p><em>29 Jun 2018</em></p>
			<p>GraphQL folks keep on with the GraphQL vs REST narratives, rather than a REST and / or GraphQL narrative lately with a recent burger meme/narrative. Continuing to demonstrate their narrow view of the landscape, and revealing the short lived power of an adversarial approach to community building. I get why people do this, because they feel they are being clever, and that the click response from the echo chamber re-enforces it, but ultimately it is something that won’t move the conversation forward, but it does get them kudos within their community–which is what many of them live for. I’ll start with the usual disclaimer. I actually like GraphQL, and prescribe it as part of my API toolbox. However, rather than a REST vs GraphQL approach, I sell it as REST and GraphQL, depending on the developer audience we are trying to reach with our efforts. Whether or not you use GraphQL on your platform is completely based upon knowing your developers, and working with a group that understands the resources before offered–something the GraphQL community continues their failure to see. Also their adversarial marketing tactics has lost me several GraphQL projects in government because it comes off as being a trend, and not something that will be around very long. With that said, I think this meme tells a great story about GraphQL, and demonstrates the illnesses of not the technology, but the ideology and beliefs of the community. I had a couple of thoughts after seeing the Tweet, and reviewing the replies: 1) I thought it was an anti-GraphQL meme at first. Demonstrating that you can build a horrible burger with some very well known ingredients. Spoofing on the burger emoji drama that has been going on in recent years.. I mean, is the lettuce the plate in the GraphQL burger? 2) Like GraphQL, the food choices demonstrate that GraphQL works well in very controlled environments. Where there are known ingredients, and your clients/customers/developers...[<a href="/2018/06/29/graphql-and-rest-differences-explained-with-burgers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/06/26/why-is-api-versioning-in-the-path-still-the-dominant-pattern/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/los-angeles-downtow-freeway_blue_circuit_5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/06/26/why-is-api-versioning-in-the-path-still-the-dominant-pattern/">Why Is API Versioning In The Path Still The Dominant Pattern?</a></h3>
			<p><em>26 Jun 2018</em></p>
			<p>API versioning is almost always one of the top attended discussions at conferences I help organize, and one of the first questions I get in the QA sessions at workshops I conduct. People want to understand the “right way” to version, when it my experience there is rarely ever a “right way” to version your APIs. There are commonly held practices regarding sensible ways to version your APIs, as well as dominant patterns for how you version APIs, but there isn’t any 100% solid answer to the question, despite what many folks might say. In my experience, the most commonly held approach to properly versioning your APIs (if you are going to), is to put the major and minor version in your header and / or combine it with content-type negotiation via your header. However, even with this knowledge being widely held, the most dominant pattern for versioning your APIs is sticking it in the URL of your API. I know many API providers who put the version in the header, despite many on their team fully being aware that it is something that should be put in the header. So, why is this? Why do people still do it the “wrong way”, even though then know how to do it the “right way”? I feel like this phenomenon reflects the wider API space, and how upside down many API belief systems are. People put the version in the URL because it is easier for them, and it is easier for their developers to understand. While headers are a native aspect of developing using the web, they are still very foreign and unknown to most developers. While this shows the lack of web literacy that is rampant amongst developers, it also demonstrates why simple web APIs have dominated the landscape–they are easy for a wide segment of developers to understand. An aspect of why this whole API thing has worked that many technologists overlook, and...[<a href="/2018/06/26/why-is-api-versioning-in-the-path-still-the-dominant-pattern/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/06/26/not-liking-openapi-fka-swagger-when-you-have-no-idea-what-it-does/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64_181_800_500_0_max_0_1_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/06/26/not-liking-openapi-fka-swagger-when-you-have-no-idea-what-it-does/">Not Liking OpenAPI (fka Swagger) When You Have No Idea What It Does</a></h3>
			<p><em>26 Jun 2018</em></p>
			<p>People love to hate in the API space. Ok, I guess its not exclusive to the API space, but it is a significant aspect of the community. I receive a regular amount of people hating on my work, for no reason at all. I also see people doing it to others in the API space on a regular basis. It always makes me sad to see, and have always worked to try to be as nice as I can to counteract the male negativity and competitive tone that often exists. While I feel bad for the people on the receiving end of all of this, I often times feel bad for the people on the giving end of things, as they are often not the most informed and up to speed folks, who seem to enjoy opening their mouth before they understand what is happening. One thing I notice regularly, is that these same people like to bash on is OpenAPI (fka Swagger). I regularly see people (still) say how bad of an idea it is, and how it has done nothing for the API space. One common thread I see with these folks, which prevents me from saying anything to them, is that it is clear they really don’t have an informed view of what OpenAPI is. Most people spend a few minutes looking it, maybe read a few blog posts, and then establish their opinions about what it is, or what it isn’t. I regularly find people who are using it as part of their work, and don’t actually understand the scope of the specification and tooling, so when someone is being vocal about it and doesn’t use actually it, it is usually pretty clear pretty quickly how uninformed they are about the specification, tooling, and scope of the community. I’ve been tracking on it since 2011, and I still have trouble finding OpenAPI specifications, and grasping all of the ways it is...[<a href="/2018/06/26/not-liking-openapi-fka-swagger-when-you-have-no-idea-what-it-does/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/06/26/a-public-selfservice-api-platform-as-a-competitive-advantage/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-gas-pumps.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/06/26/a-public-selfservice-api-platform-as-a-competitive-advantage/">A Public Self-Service API Platform as a Competitive Advantage</a></h3>
			<p><em>26 Jun 2018</em></p>
			<p>When it comes to providing data, content, and even ML and AI models via APIs, having a public platform will become a competitive advantage. I know that many companies see it as giving away something, especially when your resources and business model are not defensible, but in reality having a publicly available, 24/7 operational, self-service solution will give you an edge over your more proprietary approaches to making resources available on the web. Sure, your competition will be able to often get in there without friction, but so will your customers–how many customers vs. competitors do you have? I know many companies believe in the power of a sales team to be able to squeeze every last penny out of would be customers, but a sales only approach leaves a significant amount of self-service revenue on the table. Throughout the course of our busy days, many IT decision makers just do not have the time for the phone calls and lunches involved with the traditional sales process. Sure, there are some IT decision makers who fill their schedule with these types of conversations, but there are a growing number who depend on self-service, SaaS approaches to getting business done on a daily basis–look at the growth of Amazon Web Services over the last decade if you need a reference point. If you think a public API platform involves giving away your intellectual property in 2018, you are severely behind the times on where the sector has been headed for about a decade. Far enough behind that you may not be able to play catch up at the speed in which things are shifting. A public portal, documentation, and other resources does not mean you are giving anything away. Even having a free tier doesn’t mean that you are giving away the farm. Modern API management solutions allow you to generate leads, let developers kick the tires, while also still being able to charge what the...[<a href="/2018/06/26/a-public-selfservice-api-platform-as-a-competitive-advantage/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/06/22/staying-informed-of-api-changes-using-streamdata-io/"><img src="https://s3.amazonaws.com/kinlane-productions2/twitter/twitter-automation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/06/22/staying-informed-of-api-changes-using-streamdata-io/">Staying Informed of API Changes Using Streamdata.io</a></h3>
			<p><em>22 Jun 2018</em></p>
			<p>My friend James Higginbotham (@launchany) was sharing his frustration with being able to stay in tune with changes to a variety of APIs. Like me, James works to stay in tune with a variety of signals available via platforms like Twitter, Github, and other commonly used services. These platforms don’t always properly signal when things are updated, changed, or advanced, making it difficult to understand the granular changes that occur like likes, votes, edits, and other common events that occur via highly active platforms. This challenge is why the evolution towards a more event-driven approach to operating an API platform is not just more efficient, it gives users what they need. Using event-driven architectural approaches like Webhooks, and real times streams. This is one of the reasons I’m interested in what Streamdata.io does, beyond them helping support me financially, is that they allow me to focus on the event-driven shift that is occurring with many leading API providers, and needs to be brought to the attention of other platforms. Helping API providers be more efficient in what they are doing, while also meeting the needs of the most demanding customers like James and myself. It is easy to think Streamdata.io is just about streaming real time data. This is definitely a large aspect of what the SaaS solution does, but the approach to using Server-Sent Events (SSE), with incremental updates using JSON Patch adds another useful dimension when it comes to understanding what has changed. You can proxy an existing HTTP API that returns a JSON response using Streamdata.io, and the first response will look just like any other, but every pushed response after that will be a JSON Patch of just what has changed. Doing the heavy lifting of figuring out what has changed in each API response and only sending you the difference, and allowing you to focus only on what has changed, and not having to rely on timestamps, and other signals...[<a href="/2018/06/22/staying-informed-of-api-changes-using-streamdata-io/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/06/21/working-to-keep-programming-language-dogma-at-edges-of-the-api-conversation/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/28_68_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/06/21/working-to-keep-programming-language-dogma-at-edges-of-the-api-conversation/">Working To Keep Programming Language Dogma At Edges Of The API Conversation</a></h3>
			<p><em>21 Jun 2018</em></p>
			<p>I’m fascinated by the dominating power of programming languages. There are many ideological forces at play in the technology sector, but the dogma that exists within each programming language community continues to amaze me. The potential absence of programming language dogma within the world of APIs is one of the reasons I feel it has been successful, but alas, other forms of dogma tends to creep in around specific API approaches and philosophies, making API evangelism and adoption always a challenge. The absence of programming languages in the API design, management, and testing discussion is why they have been so successful. People in these disciplines have ben focused on the language agnostic aspects of just doing business with APIs. It is also one of the reasons the API deployment conversation still is still so fragmented, with so many ways of getting things done. When it comes to API deployment, everyone likes to bring their programming language beliefs to the table, and let it affect how we actually deliver this API, and in my opinion, why API gateways have the potential to make a comeback, and even excel when it comes to playing the role of API intermediary, proxy, and gateway. Programming language dogma is why many groups have so much trouble going API first. They see APIs as code, and have trouble transcending the constraints of their development environment. I’ve seen many web or HTTP APIs called Java API, Python APIs, or reflect a specific language style. It is hard for developers to transcend their primary programming language, and learn multiple languages, or think in a language agnostic way. It is not easy for us to think out of our boxes, and consider external views, and empathize with people who operate within other programming or platform dimensions. It is just easier to see the world through our lenses, making the world of APIs either illogical, or something we need to bend to our way of...[<a href="/2018/06/21/working-to-keep-programming-language-dogma-at-edges-of-the-api-conversation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2018/06/21/do-not-try-to-service-all-the-stops-along-the-api-lifecycle-as-an-api-service/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/66_189_800_500_0_max_0_-5_-1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2018/06/21/do-not-try-to-service-all-the-stops-along-the-api-lifecycle-as-an-api-service/">Do Not Try To Service All The Stops Along The API Lifecycle As An API Service</a></h3>
			<p><em>21 Jun 2018</em></p>
			<p>One thing I see a lot from API service providers who are selling their services to the API sector, is that once they find success servicing one stop along the API lifecycle, they often want to service other additional stops. I don’t have a problem with API service providers delivering across multiple stops along the API lifecycle, however I do caution of trying to expand across too many stops, and potentially doing any of them poorly, rather than partnering with other more specialized API service providers to help you focus on what you do best. I’m a big advocate for encouraging API providers to service one to five stops along the API lifecycle well, and then partner for helping deliver the rest of the stops. I know that all your investors are encouraging to take as many pieces of the puzzle as you possibly can, but there is more money in doing a handful of things really well, over doing many things poorly. Try to be an expert in a handful of specialized areas, over being a generalist. Then make sure your platform is as interoperable as possible, while investing in your partner program to attract the best of breed API service providers to your platform. This balance between focusing on a handful of stops and partnering is why I emphasize and study common approaches to delivering plugins. All platforms should invest in plugin infrastructure, to allow for extending their reach beyond the stops that a platform services. Feature creep, and platform bloat is a real challenge, especially when you have investors whispering in your ear to keep building, and a very vocal, but often long-tail group of users demanding solutions to their unique problems. Plugin and connector architecture is how you help manage this reality, and provide a relief valve for delivering too many features as part of your platform, while also bringing in potential partners who can help extend what your platforms in...[<a href="/2018/06/21/do-not-try-to-service-all-the-stops-along-the-api-lifecycle-as-an-api-service/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page4" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page6" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home Page</a></li>
    <li><a href="/blog/">The Blog</a></li>
    <li><a href="https://101.apievangelist.com/">API 101</a></li>
    <li><a href="http://history.apievangelist.com">History of APIs</a></li>
    <li><a href="https://women-in-tech.apievangelist.com/">Women in Technology</a></li>    
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
