<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/">Capital One DevExchange Provides An Important Banking API Blueprint</a></h3>
        <span class="post-date">06 Mar 2018</span>
        <p><a href="https://developer.capitalone.com"><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-banking-home-page.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>When you take a look at the banking API landscape in the United States, there is one clear leader in the game–Capital One. Their <a href="https://developer.capitalone.com/">DevExchange</a> program is miles ahead of every one of their competitors, giving them a significant head start when it comes to the banking API economy. Their approach to delivering APIs meets all of my minimum requirements for any successful API platform, and even exceeds it, providing what I’d consider to be a leading example blueprint that all banking API providers should be following.</p>

<p>The Capital One DevExchange begins as any API operation should, with a dedicated portal located at developer.[domain]:</p>

<ul>
  <li><a href="https://developer.capitalone.com/">developer.capitalone.com</a></li>
</ul>

<p>After landing on the home page for the Capital One DevExchange you get everything you need to get up and running with the APIs they have:</p>

<ul>
  <li><a href="https://developer.capitalone.com/platform-documentation/getting-started/">Getting started</a></li>
  <li><a href="https://developer.capitalone.com/platform-documentation/authorization-with-oauth-20/">Authentication</a></li>
  <li><a href="https://developer.capitalone.com/platform-documentation/">Documentation</a></li>
  <li><a href="https://developer.capitalone.com/platform-documentation/errors/">Errors</a></li>
  <li><a href="https://developer.capitalone.com/sign-in/">Login</a></li>
  <li><a href="https://developer.capitalone.com/sign-up">Registration</a></li>
</ul>

<p>The Capital One DevExchange provides four main groups of public APIs currently, started with access to account information in the following areas:</p>

<ul>
  <li>Retrieve account products - /deposits/account-products (GET)</li>
  <li>Retrieve account product details - /deposits/account-products/{productId} (GET)</li>
  <li>Create new account application - /deposits/account-applications (POST)</li>
  <li>Retrieve out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (GET)</li>
  <li>Answer out of wallet questions - /deposits/account-applications/{applicationId}/out-of-wallet (PUT)</li>
  <li>Retrieve account application details - /deposits/account-applications/{applicationId} (GET)</li>
</ul>

<p>As well as some credit card offers, showcasing the products they have available:</p>

<ul>
  <li>Retrieve product listings - /credit-offers/products (GET)</li>
  <li>Retrieve card products - /credit-offers/products/cards (GET)</li>
  <li>Retrieve card products - /credit-offers/products/cards (GET)</li>
  <li>Retrieve card products by type - /credit-offers/products/cards/{cardType} (GET)</li>
  <li>Retrieve card product details - /credit-offers/products/cards/{cardType}/{productId} (GET)</li>
</ul>

<p>Which you can actually sign up for and do a pre-qualification via APIs:</p>

<ul>
  <li>Create prequalification check - /credit-offers/prequalifications (POST)</li>
  <li>Create prequalification acknowledgment - /credit-offers/prequalifications/{prequalificationId} (POST)</li>
  <li>Create applicant key - /credit-offers/applicant-details POST</li>
</ul>

<p>Get access to Capital One rewards via APIs:</p>

<ul>
  <li>Retrieve rewards accounts - /rewards/accounts (GET)</li>
  <li>Retrieve rewards account details - /rewards/accounts/{rewardsAccountReferenceId} (GET)</li>
</ul>

<p>And details about merchants involved in transactions:</p>

<ul>
  <li>Retrieve merchant data - /merchant-insights/merchants (GET)</li>
  <li>Refresh merchant details - /merchant-insights/merchants/{merchantId} (GET)</li>
</ul>

<p>This version of the API is available in a sandbox and production environments:</p>

<ul>
  <li><a href="https://developer.capitalone.com/platform-documentation/using-the-sandbox/">Sandbox</a></li>
</ul>
<p><a href="http://api.reimaginebanking.com/"><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-hackathon-api.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>You can tell Capital One is moving cautiously with their public APIs, but they are definitely further along than other banks. Beyond what is publicly available in their sandbox and production environment they have another exploratory set of APIs coming from a project they call Nessie, <a href="http://api.reimaginebanking.com/">Capital One’s Hackathon API</a> that gives you access to a multitude of real public-facing data, such as ATM and bank branch locations, complete with mock customer account data. This set of APIs was used as part of a hackathon put on by the bank, but are worth showcasing as an example of where the bank is headed with their API road map.</p>

<p>By providing a pretty robust stack of API paths for working with accounts:</p>

<ul>
  <li>Get all accounts - /accounts (GET)</li>
  <li>Delete a specific existing account - /accounts/{id} (DELETE)</li>
  <li>Get account by id - /accounts/{id} (GET)</li>
  <li>Update a specific existing account - /accounts/{id} (PUT)</li>
  <li>Get customer that owns the specified account - /accounts/{id}/customer (GET)</li>
  <li>Get accounts by customer id - /customers/{id}/accounts (GET)</li>
  <li>Create an account - /customers/{id}/accounts (POST)</li>
</ul>

<p>As well as bills that are associated with accounts:</p>

<ul>
  <li>Get all bills for a specific account - /accounts/{id}/bills (GET)</li>
  <li>Create a bill - /accounts/{id}/bills (POST)</li>
  <li>Delete a specific existing bill - /bills/{billId} (DELETE)</li>
  <li>Get bill by id - /bills/{billId} (GET)</li>
  <li>Update a specific existing bill - /bills/{billId} (PUT)</li>
  <li>Get bills by customer id - /customers/{id}/bills (GET)</li>
</ul>

<p>Then API paths for managing deposits:</p>

<ul>
  <li>Get all deposits - /accounts/{id}/deposits (GET)</li>
  <li>Create a deposit - /accounts/{id}/deposits (POST)</li>
  <li>Delete a specific existing deposit - /deposits/{id} (DELETE)</li>
  <li>Get deposit by id - /deposits/{id} (GET)</li>
  <li>Update a specific existing deposit - /deposits/{id} (PUT)</li>
</ul>

<p>And for any details on loans:</p>

<ul>
  <li>Get all loans - /accounts/{id}/loans (GET)</li>
  <li>Create a loan - /accounts/{id}/loans (POST)</li>
  <li>Delete a specific existing loan - /loans/{id} (DELETE)</li>
  <li>Get loan by id - /loans/{id} (GET)</li>
  <li>Update a specific existing loan - /loans/{id} (PUT)</li>
</ul>

<p>Then providing insight into all purchases:</p>

<ul>
  <li>Get all purchases - /accounts/{id}/purchases (GET)</li>
  <li>Create a purchase - /accounts/{id}/purchases (POST)</li>
  <li>Get all purchases by account and merchant - /merchants/{id}/accounts/{accountId}/purchases (GET)</li>
  <li>Get all purchases by merchant - /merchants/{id}/purchases (GET)</li>
  <li>Delete a specific existing purchase - /purchases/{id} (DELETE)</li>
  <li>Get purchase by id - /purchases/{id} (GET)</li>
  <li>Update a specific existing purchase - /purchases/{id} (PUT)</li>
</ul>

<p>As well as bank account transfers:</p>

<ul>
  <li>Get all transfers - /accounts/{id}/transfers (GET)</li>
  <li>Create a transfer - /accounts/{id}/transfers (POST)</li>
  <li>Delete a specific existing transfer - /transfers/{transferId} (DELETE)</li>
  <li>Get transfer by id - /transfers/{transferId} (GET)</li>
  <li>Update a specific existing transfer - /transfers/{transferId} (PUT)</li>
</ul>

<p>And withdrawals:</p>

<ul>
  <li>Get all withdrawals - /accounts/{id}/withdrawals (GET)</li>
  <li>Create a withdrawal - /accounts/{id}/withdrawals (POST)</li>
  <li>Delete a specific existing withdrawal - /withdrawals/{id} (DELETE)</li>
  <li>Get withdrawal by id - /withdrawals/{id} (GET)</li>
  <li>Update a specific existing withdrawal - /withdrawals/{id} (PUT)</li>
</ul>

<p>Details about banking customers:</p>

<ul>
  <li>Get all customers - /customers (GET)</li>
  <li>Create a customer - /customers (POST)</li>
  <li>Get customer by id - /customers/{id} (GET)</li>
  <li>Update a specific existing customer - /customers/{id} (PUT)</li>
</ul>

<p>As well as available merchants:</p>

<ul>
  <li>Get all merchants - /merchants (GET)</li>
  <li>Create a merchant - /merchants (POST)</li>
  <li>Get merchant by id - /merchants/{id} (GET)</li>
  <li>Update a specific existing merchant - /merchants/{id} (PUT)</li>
</ul>

<p>Available ATMS:</p>

<ul>
  <li>Get all ATMs - /atms (GET)</li>
  <li>Get ATM by id - /atms/{id} (GET)</li>
</ul>

<p>And Bank Branches:</p>

<ul>
  <li>Get all branches - /branches (GET)</li>
  <li>Get branch by id - /branches/{id} (GET)</li>
</ul>

<p>I am not sure what Capital One’s intentions are with introducing these APIs to the main stack of public APIs, but they are still available within a separate Hackathon sandbox environment:</p>

<ul>
  <li><a href="http://api.reimaginebanking.com/">Hackathon API</a></li>
</ul>

<p>Beyond the sandbox, hackathon, and production APIs available, Capital One provides all the expected support and communication channels that you expect out of an active API program:</p>

<ul>
  <li><a href="https://developer.capitalone.com/support/">Support</a></li>
  <li><a href="https://developer-support.capitalone.com/apex/DXSearchArticles">Knowledgebase</a></li>
  <li><a href="https://developer.capitalone.com/blogs/">Blog</a></li>
  <li><a href="https://twitter.com/CapitalOneDevEx">Twitter</a></li>
</ul>

<p>As well as not forgetting the required elements present from the legal department, helping set the tone for API engagements:</p>

<ul>
  <li><a href="https://www.capitalone.com/identity-protection/privacy/statement">Privacy policy</a></li>
  <li><a href="https://developer.capitalone.com/single/terms-and-conditions/">Terms of service</a></li>
</ul>

<p><a href="https://developer.capitalone.com/open-source/"><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/capital-one-open-source.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>Where the Capital One API program really begins to impress is with their usage of Github, and providing of open source solutions:</p>

<ul>
  <li><a href="https://github.com/capitalone">Capital One Github</a></li>
  <li><a href="https://github.com/CapitalOne-DevExchange">Capital One Devexchange Github</a></li>
  <li><a href="https://developer.capitalone.com/open-source/">Open Source</a></li>
</ul>

<p>It isn’t easy doing Open Source in a highly regulated industry, and Capital One not only is doing it, <a href="https://developer.capitalone.com/blog-post/open-source-in-a-regulated-environment-lessons-learned-on-our-open-source-journey-at-capital-one/">they actually share some of the story behind their struggle</a>. The open source projects they have been releasing, and the stories they’ve told about them on the blog tell just as important of a story as each of their APIs do. Further externalizing how the bank delivers technology at the bank, as well as with partners and 3rd party developers.</p>

<p>The Capital One DevExchange is what ALL US banks should be emulating. This isn’t just about publicly available APIs. This is about being able to deliver APIs consistently, in ways that are in sync with your lines of business. The more you can deliver this publicly, tell the story of it, and release open source code that demonstrates the value it brings, the more you will be able to do this internally, across groups, and amongst your partners. The biggest mistake any bank can be making in 2018 is thinking doing APIs is purely about retail banking. It is really about how agile and effective your bank will be leverage web technology internally, and externally within your company.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/06/capital-one-devexchange-provides-an-important-banking-api-blueprint/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and-constraints/">Serverless, Like Microservices Is About Understanding Our Dependencies And Constraints</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/death-valley-national-park_dali_three_just_road.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am not buying into all the hype around the recent serverless evolution in compute. However, like most trends I study, I am seeing some interesting aspects of how people are doing it, and some positive outcomes for teams who are doing it sensibly. I am not going all in on serverless when it comes to deploying or integrating with APIs, but I am using it for some projects, when it makes sense. I find AWS Lambda to be a great way to get in between the AWS API Gateway and backend resources, in order to conduct a little transformation. Keeping serverless as just one of many tools in my API deployment and integration toolbox, yet never going overboard with any single solution.</p>

<p>I put serverless into the same section of my toolbox as I do microservices. Serverless is all about decoupling how I deploy my APIs, helping me keep things doing small, meaningful things behind each of my API paths. Serverless forces me to think through how I am decoupling my backend, and pushes me to identify dependencies, acknowledge constraints, and get more creative in how I write code in this environment. To do one thing, and do it well, I need an intimate understanding of where my data and other backend resources are, and how I can distill a unit of compute down to the smallest unit as I possibly can. Identifying, reducing, and being honest about what my dependencies are is essential to serverless working or not working for me.</p>

<p>The challenge I’m having with serverless at the moment is that it can be easy to ignore many of the benefits I get from dependencies, while just assuming all dependencies are bad. Specifically around the frameworks I deploy as part of my API solutions. When coupled with AWS API Gateway this isn’t much of a problem, but all by itself, AWS Lambda doesn’t have all fo the other HTTP, transformation, header, request, and response goodness I enjoy from the API frameworks I’ve historically depend on. Showing me that understanding our dependencies is important, both good and bad. Not all dependencies are bad, as long as I am aware of them, have a knowledge of what they bring to the table, and it is a stable, healthy, and regularly evaluated relationships. This is a challenge I also come across regularly in microservices as well as serverless journeys, that we just throw things out for the sake of size, without thinking deeply about why we are doing something, and whether or not it is worth it.</p>

<p>I can see plenty of scenarios where I will be thinking that a serverless approach is good, but after careful evaluation of dependencies and constraints, I abandon this path as an option. I’m guessing we’ll see a goldilocks style of serverless emerge that helps us find this sweet spot. Similar to what we are finding with the microservices evolution, in that micro isn’t always about small–it is often just about finding the sweet spot between big and small, hot and cold, dependent and independent. Being sensible about how we use the tools in our API toolbox, and realizing most of these approaches are more about the journey, than they are about the actual tool.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/serverless-like-microservices-is-about-understanding-our-dependencies-and-constraints/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/an-openapi-rules-engine/">An OpenAPI-Driven, API Governance Rules Engine</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/rules/9968073905_95ce575233_z.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Phil Sturgeon (<a href="https://twitter.com/philsturgeon">@philsturgeon</a>) alerted me to a pretty cool project he is cooking up, called <a href="https://github.com/wework/speccy">Speccy</a>. Which provides a rules engine for validating your OpenAPI definitions. “Taking off from where <a href="https://twitter.com/PermittedSoc/">Mike Ralphson</a> started with linting in <a href="https://github.com/Mermade/swagger2openapi/">swagger2openapi</a>, Speccy aims to become the <a href="https://github.com/bbatsov/rubocop">rubocop</a> or <a href="https://eslint.org/">eslint</a> of OpenAPI”, and to “sniff your files for potentially bad things. “Bad” is objective, but you’ll see validation errors, along with special rules for making your APIs better.” Helping make sure your API definitions are as consistent as they possibly can be, and deliver on your API governance strategy (you have one right?)</p>

<p>With Speccy, there are a default set of rules, things like ensuring you have a summary or a description for each API path:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "operation-summary-or-description",
	"object": "operation",
	"enabled": true,
	"description": "operation should have summary or description",
	"or": ["summary", "description"]
}
</code></pre></div></div>

<p>Or making sure you add descriptions to your parameters:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "parameter-description",
	"object": "parameter",
	"enabled": true,
	"description": "parameter objects should have a description",
	"truthy": "description"
}
</code></pre></div></div>

<p>Or making sure you include tags for each aPI path:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "operation-tags",
	"object": "operation",
	"enabled": true,
	"description": "operation should have non-empty tags array",
	"truthy": "tags",
	"skip": "isCallback"
}
</code></pre></div></div>

<p>Then you can get more strict by requiring contact information:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "contact-properties",
	"object": "contact",
	"enabled": true,
	"description": "contact object should have name, url and email",
	"truthy": [ "name", "url", "email" ]&lt;br /&gt;
}
</code></pre></div></div>

<p>And make sure youi have a license applied to your API:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
	"name": "license-url",
	"object": "license",
	"enabled": true,
	"description": "license object should include url",
	"truthy": "url"
}
</code></pre></div></div>

<p>Speccy is available <a href="https://www.npmjs.com/package/speccy">as a Node package</a>, which you can easily run at the command line. Speccy is definitely what is needed out there right now, helping us validate the growing number of OpenAPI definitions in our life. As many companies are thinking about how they can apply API governance across their operations, they should be looking at contributing to <a href="https://github.com/wework/speccy">Speccy</a>. It is something I’ve been talking with API service providers about for some time, but haven’t seen an open source answer emerge, that can help us develop rules for what we expect of our OpenAPI definitions.</p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/openapi-logo.png" align="right" width="25%" style="padding: 15px;" /></p>
<p>My only feedback right now, is that we need lots of people using it, and helping contribute rules. Oh, and wrap it in an API, and make it available as an easy to use, and deploy containerized microservice. Then lets get to work on the Github Gist-driven marketplace of rules, where I can publish the rules I develop across the projects I’m working on, and of the clients I consult with. Let’s get to work making sure there are a wealth of rules, broken down into different categories for API providers to choose from. Then let’s get API tooling and service providers to begin baking a Speccy rules engine into their solutions, and allow for the import and management of open source rules.</p>

<p>Speccy only works with OpenAPI 3.0, which makes sense if we are going to be moving forward with this conversation. Spreccy is how we will validate that banking APIs are PSD2 compliant. It is how we will ensure healthcare APIs support the FHIR specification. I have other suggestions for the CLI and API usage of Speccy, but I’d rather see investment in the available rules, before I make too many functional suggestions. I think the rules are where we will begin to define what we are looking for in an OpenAPI rules engine, and that should drive the Speccy features which end up on the road map.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/an-openapi-rules-engine/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks/">People Seem to Want Lego Kits and Not a Bucket of Lego Blocks When It Comes To Doing APIs</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/legos/lego-millenium-falcon-instructions.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>While I wish everyone saw the modular potential of APIs and microservices like I do, I’ve come to the realization that most people are just interested in ready to go kits that walk them through every detail of doing APIs, rather than actually playing, learning, evolving, and learning to be productive with a big bucket of APIs. I’m not just focusing on business users here, I am talking about a significant portion of the developers I come across, who really don’t seem that interested in learning to apply API concepts, and understanding when and where to use them, they just want a set of instructions that walk them through each step of deploying an API.</p>

<p>I actually am a proponent of there being more boxed, lego style API kits that teach you how to build the product API, task API, press release API, and other common implementations. Robust, detailed, ready-to-go API implementations that would walk people through each and every step of defining, designing, deploying, managing, monitoring, testing, and documenting their API. I feel like this would significantly help folks think through what are healthy API practices, and be introduced to different ways of thinking around APIs. However, I do not want people to become reliant on only being able to operate within this paradigm, and not actually be able to fix, evolve, and deliver their own custom API solutions, using the healthy practices they are being introduced to.</p>

<p>People seem to just want shortcuts, and things done for them. People want the solutions packaged and delivered to their doorstep. They seem unable to be able to find the solutions on their own, or even be able to absorb a lesson delivered via a packaged solution. I’m not sure what the cure for this condition is. It is hard to tell whether it is vendor induced, or (would be) API provider induced. Have people been conditioned by vendors to just be spoon fed solutions? Or are people just lazy, and not interested in truly learning, truly tackling their technical debt, and finding a new path forward? IDK. I’m feeling like it is probably a little of both, feeding off of each other.</p>

<p>I get why the pre-built, ready-to-go Lego API kit is appealing. Everyone wants to have a full-blown millennium falcon when they are done working on a project, and not some blocky mcsquare flying machine. However, it takes time before you are able to deliver at that scope. You need to practice building smaller implementations, and yes playing with other pre-built kits, doing some reverse engineering, in addition to some forward engineering. I’m guessing this all comes down to if you truly want to know and understand, or if you are just looking for solutions. If you are just looking for solutions, I’m guessing in 5 years you’ll be eagerly buying the next solution for the API mess you are putting into place currently. It is the natural evolution of how technology gets bought and sold.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/people-seem-to-want-lego-kits-and-not-a-bucket-of-lego-blocks/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud-providers/">The Need for Standardized API Plans and Pricing to Compete with Cloud Providers</a></h3>
        <span class="post-date">05 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/docks_copper_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://cloudplatform.googleblog.com/2018/02/introducing-Cloud-Billing-Catalog-API-GCP-pricing-in-real-time.html">Google launched their Cloud Billing Catalog API, providing access to thee pricing for their cloud API catalog</a> the other day. <a href="https://docs.microsoft.com/en-us/azure/billing/">Azure has their billing API</a>, and <a href="https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/Welcome.html">AWS has their cost explorer API service</a>, showing that programmatic access to what API resources cost, as well as management of usage, billing, invoicing, and other aspects of doing business with APIs is becoming the normal mode of operating an API platform.</p>

<p>I’ve long used AWS, Google, and increasingly Azure as a blueprint for what us smaller API providers should be doing. They are full of positive and negative lessons for any API provider. However, I’m starting to see what they are doing as not just a blueprint, but potentially something that will force many of us API providers out of businesses if we cannot emulate what they are doing at scale. The tractor beam that is the cloud providers is strong. They bring a lot of benefits to the table. So much so, it is getting harder and harder for independent API providers to compete. Offering benefits to consumers that will become deal breakers with using other 3rd party API providers services, pushing API consumers to stay within their chosen cloud platform walled garden.</p>

<p>As a developer, if I can programmatically manage the plans, pricing, and billing for ALL the APIs I use via AWS, Google, and Azure, but I have to manually manage this across many different 3rd party providers, I am going to be hesitant when it comes to adopting any new services that aren’t within my cloud domain. As I depend on more APIs, the benefits of being able to programmatically manage the business of my API consumption is becoming increasingly critical. If the individual 3rd party API providers I use don’t begin to offer APIs for managing the business of my API consumption, and adopting a standardized interface across all the APIs I depend on, I’m going to favor my cloud native API solutions over the 3rd party, and custom ones–giving cloud providers a significant advantage.</p>

<p>It is something that smaller API providers are going to have to start thinking about, and stop being special little snowflakes, and consider how they can start standardizing and being interoperable. Otherwise the cloud API providers are going to continue to gain marketshare, and everyone will just use the APIs available to use in the cloud. Reducing the competition, diversity, and utility that the API sector has become known for. We will just use Amazon, Google, and Azure APIs, and if they don’t have it, it probably won’t be done. Innovation will only occur within the cloud marketplaces, and be way less vibrant than the API sector we’ve enjoyed over the last decade.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/05/the-need-for-standardized-api-plans-and-pricing-to-compete-with-cloud-providers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/">Thoughts On The Schema.Org WebAPI Type Extension</a></h3>
        <span class="post-date">02 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/schema-org/schema-org.png" width="30%" align="right" style="padding: 15px;" /></p>
<p>I’m putting some thought into <a href="https://webapi-discovery.github.io/rfcs/rfc0001.html#content-types">the Schema.Org WebAPI Type Extension proposal</a> by Mike Ralphson (Mermade Software) and Ivan Goncharov (APIs.guru), to “facilitate better automatic discovery of WebAPIs and associated machine and human-readable documentation”. It’s an interesting evolution in how we define APIs, in terms of API discovery, but I would also add potentially at “execute time”.</p>

<p>Here is what a base WebAPI type schema could look like:</p>

<p>```{</p>

<p>“@context”: “http://schema.org/”,</p>

<p>“@type”: “WebAPI”,</p>

<p>“name”: “Google Knowledge Graph Search API”,</p>

<p>“description”: “The Knowledge Graph Search API lets you find entities in the Google Knowledge Graph. The API uses standard schema.org types and is compliant with the JSON-LD specification.”,</p>

<p>“documentation”: “https://developers.google.com/knowledge-graph/”,</p>

<p>“termsOfService”: “https://developers.google.com/knowledge-graph/terms”,</p>

<p>“provider”: {</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"@type": "Organization",

"name": "Google Inc."   } }```
</code></pre></div></div>

<p>Then the proposed extensions could include the following:</p>

<ul>
  <li><strong>versions</strong> (OPTIONAL array of <a href="https://schema.org/Thing">thing</a> -&gt; <a href="http://meta.schema.org/Property">Property</a> -&gt; <a href="http://schema.org/softwareVersion">softwareVersion</a>). It is RECOMMENDED that APIs be versioned using [semver]</li>
  <li><strong>entryPoints</strong> (OPTIONAL array of <a href="https://schema.org/Thing">Thing</a> -&gt; <a href="https://schema.org/Intangible">Intangible</a> -&gt; <a href="https://schema.org/EntryPoint">EntryPoint</a>)</li>
  <li><strong>license</strong> (OPTIONAL, <a href="http://schema.org/CreativeWork">CreativeWork</a> or <a href="http://schema.org/URL">URL</a>) - the license for the design/signature of the API</li>
  <li><strong>transport</strong> (enumerated <a href="http://schema.org/Text">Text</a>: HTTP, HTTPS, SMTP,  MQTT, WS, WSS etc)&lt;/p&gt;</li>
  <li><strong>apiProtocol</strong> (OPTIONAL, enumerated <a href="http://schema.org/Text">Text</a>:  SOAP, GraphQL, gRPC, Hydra, JSON API, XML-RPC, JSON-RPC etc)</li>
  <li><strong>webApiDefinitions</strong> (OPTIONAL array of <a href="http://schema.org/EntryPoint">EntryPoints</a>) containing links to <a data-link-type="dfn" href="#machine-readable-api-definition" id="ref-for-machine-readable-api-definition-1">machine-readable API definition</a>s</li>
  <li><strong>webApiActions</strong> (OPTIONAL array of potential <a href="http://schema.org/Action">Actions</a>)</li>
</ul>

<p>The webApiDefinitions (EntryPoint) contentType property contains a reference to one of the following conten types:</p>

<ul>
  <li><strong>OpenAPI / Swagger in JSON</strong> - application/openapi+json or application/x-openapi+json</li>
  <li><strong>OpenAPI / Swagger in YAML</strong> - application/openapi</li>
  <li><strong>RAML</strong> - application/raml+yaml</li>
  <li><strong>API Blueprint in markdown</strong> - text/vnd.apiblueprint</li>
  <li><strong>API Blueprint parsed in JSON</strong> - application/vnd.refract.parse-result+json</li>
  <li><strong>API Blueprint parsed in YAML</strong> - application/vnd.refract.parse-result+yaml</li>
</ul>

<p>Then the webApiActions property brings a handful of actions to the table, with the following being suggested:</p>

<ul>
  <li><strong>apiAuthentication</strong> - Links to a resource detailing authentication requirements. Note this is a human-readable resource, not an authentication endpoint</li>
  <li><strong>apiClientRegistration</strong> - Links to a resource where a client may register to use the API</li>
  <li><strong>apiConsole</strong> - Links to an interactive console where API calls may be tested</li>
  <li><strong>apiPayment</strong> - Links to a resource detailing pricing details of the API</li>
</ul>

<p>I fully support extending the Schema.org WebAPI vocabulary in this way. It adds all the bindings needed to make the WebAPI type executable at runtime, as well as it states at discovery time. I like the transport and protocol additions, helping ensure the WebAPI vocabulary is as robust as it possibly can. webApiDefinitions provides all the technical details regarding the surface area of the API we need to actually engage with it at runtime, and webApiActions begins to get at some of the business of APIs friction that exists at runtime. Making for an interesting vocabulary that can be used to describe web APIs, which also becomes more actionable by providing everything you need to get up and running.</p>

<p>The suggestions are well thought out and complete. If I was to add any elements, I’d say it also needs a support link. There will be contact information embedded within the API definitions, but having a direct link along with registration, documentation, terms of service, authentication, and payment would help out significantly. I would say that the content type to transport and protocol coverage is deficient a bit. Meaning you have SOAP, but not referencing WSDL. I know that there isn’t a direct definition covering every transport and protocol, but eventually it should be as comprehensive as it can. (ie. adding AsyncAPI, etc. in the future)</p>

<p>The WebAPI type extensions reflect what we have been trying to push forward with our <a href="http://apisjson.org">APIs.json</a> work, but comes at it from a different direction. I feel there are significant benefits to having all these details as part of the Schema.org vocabulary, expanding on what you can describe in a common way. Which can then also be used as part of each APIs requests, responses, and messages. I don’t see APIs.json as part of a formal vocabulary like this–I see it more as the agile format for indexing APIs that exist, and building versatile collections of APIs which could also contain a WebAPI reference.</p>

<p>I wish I had more constructive criticism or feedback, but I think it is a great first draft of suggestions for evolving the WebAPI type. There are other webApiActions properties I’d like to see based upon my APIs.json work, but I think this represents an excellent first step. There will be some fuzziness between documentation and apiConsole, as well as gaps in actionability between apiAuthentication, and apiClientRegistration–thinks like application creation (to get keys), and opportunities to have Github, Twitter, and other OpenID/OAuth authentication, but these things can be worked out down the road. Sadly there isn’t much standardization at this layer currently, and I see this extension as a first start towards making this happen. As I said, this is a good start, and we have lots of work ahead as we see more adoption.</p>

<p>Nice work Mike and Ivan! Let me know how I can continue to amplify and get the word out. We need to help make sure folks are describing their APIs using Schema.org. I’d love to be able to automate the discovery of APIs, using existing search engines and tooling–I know that you two would like to see this as well. API discovery is a huge problem, which there hasn’t been much movement on in the last decade, and having a common vocabulary that API providers can use to describe their APIs, which search engines can tune into would help move us further down the road when it comes to having more robust API discovery.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/02/thoughts-on-the-schema-org-webapi-type-extension/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/">An Observable Industry Level Directory Of API Providers And Consumers</a></h3>
        <span class="post-date">01 Mar 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/vancouver_diego_rivera1.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I’ve been breaking down the work on banking APIs coming out of <a href="http://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/">Open Banking in the UK</a> lately. I recently took all their <a href="http://open.banking.blueprint.apievangelist.com/">OpenAPI definitions and published as a demo API developer portal</a>. Bringing the definitions out of the shadows a little bit, and showing was is possible with the specification. Pushing the project forward some more today I published <a href="http://open.banking.blueprint.apievangelist.com/#Open Banking - Directory APIs">the Open Banking API Directory specification to the project</a>, showing the surface area of the very interesting, and important component of open banking APIs in the UK.</p>

<p>The Open Banking Directory provides a pretty complete, albeit rough and technical approach to delivering observability for <a href="http://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/">the UK banking industry API ecosystem actor layer</a>. Everyone involved in the banking API ecosystem in UK has to be registered in the directory. It provides profiles of the banks, as well as any third party players. It really provides an unprecedented, industry level look at how you can make API ecosystems more transparent and observable. This thing doesn’t exist at the startup level because nobody wants to be open with the number of developers, or much else regarding the operation of their APIs. Making any single, or industry level API ecosystem, operate as black boxes–even if they claim to be an “open API”.</p>

<p>Could you imagine if API providers didn’t handle their own API management layer, and an industry level organization would handle the registration, certification, directory, and dispute resolution between API providers and API consumers? Could you imagine if we could see the entire directory of Facebook and Twitter developers, understand what businesses and individuals were behind the bots and other applications? Imagine if API providers couldn’t lie about the number of active developers, and we knew how many different APIs each application developers used? And it was all public data? An entirely different API landscape would exist, with entirely different incentive models around providing and consuming gAPIs.</p>

<p>The Open Banking Directory is an interesting precedent. It’s not just an observable API authentication and management layer. It also is an API. Making the whole thing something that can be baked into the industry level, as well as each individual application. I’m going to have to simmer on this concept some more. I’ve thought a lot about collective API developer and client solutions, but never anything like this. I’m curious to see how this plays out in a heavily regulated country and industry, but also eager to think about how something like this might work (or not) in government API circles, or even in the private sector, within smaller, less regulated industries.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/28/an-open-banking-blueprint-api-portal/">An Open Banking API Portal Blueprint</a></h3>
        <span class="post-date">28 Feb 2018</span>
        <p>I have been learning all about <a href="https://www.openbanking.org.uk/">the banking API efforts out of Open Banking in the UK lately</a>. They are evolving a set of read / write account and transaction API, as well as public data APIs for some of the common information 3rd party developers are looking to get their hands on. I'm intrigued with the traction the organization has gotten, and I want to be able to fully understand what they are developing, as well help contribute where I can.</p>
<p>To help me understand the API specification, as well as hopefully contribute to the conversation, I am publishing <a href="http://open.banking.blueprint.apievangelist.com/">an blueprint API portal for the API</a>. It is a demo portal, running on Github, which uses the API Evangelist graphical look, but I am also publishing documentation for v1.1.1 of the account and payments API, as well as v2.1 of the public data APIs. I'm looking to publish the API specification like any bank would, but it won't actually be a live API--yet. I'd like to turn it into a mock API, with some virtualized data to demonstrate what is possible.</p>
<p align="center"><a href="http://open.banking.blueprint.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/open-banking/open-banking-accounts-documentation-screenshot.png" width="90%" align="center" style="padding: 15px;" /></a></p>
<p>I've only had time to publish the overview of the project, and the documentation for each current version. I have a todo list of things I would like to invest in when I have more time. Eventually, I want it to be a complete, forkable Open Banking API portal that any bank in the UK could publish. Then I'm looking to create country specific versions to help push French, German, and other banks to push a portal. It doesn't have to be my solution that the banks use, but hopefully they'll at least use what I have provided as a blueprint. The goal isn't just to get them to use the portal, it is to get them implementing their bank's API developer portal in a standardized way--similar to the API specification from Open Banking, but this is the portal specification.</p>
<p>I will spend time on the portal over the next couple of weeks. If there is something you'd like to see accomplished, or something I'm missing entirely, feel free to <a href="https://github.com/european-banking-apis/open-banking-blueprint/issues">submit a Github issue for the project</a>. The project repository is a little messy right now as it is in full development, so if you fork, be careful--you might want to wait.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/28/an-open-banking-blueprint-api-portal/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/">What We Need To Be Machine Readable At API Run Time</a></h3>
        <span class="post-date">28 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/4882162452_fa3126b38d_b_spagetti_accident.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I had breakfast with Mike Amundsen (<a href="https://twitter.com/mamund">@mamund</a>) and Matt McLarty (<a href="https://twitter.com/MattMcLartyBC">@MattMcLartyBC</a>) of the CA API Academy team this morning in midtown this morning. As we were sharing stories of what each other was working on, the topic of what is needed to execute an API call came up. Not the time consuming find an API, sign up for an account, figure out the terms of service and pricing version, but all of this condensed into something that can happen in a split second within applications and systems.</p>

<p>How do we distill down the essential ingredients of API consumption into a single, machine readable unit that can be automated into what Mike Amundsen calls, “find and bind”. This is something I’ve been thinking a lot about lately as I work on my API discovery research, and there are a handful of elements that need to be present:</p>

<ul>
  <li><strong>Authentication</strong> - Having keys to be able to authentication.</li>
  <li><strong>Surface Area</strong> - What is the host, base url, path, headers, and parameters for a request.</li>
  <li><strong>Terms of Service</strong> - What are the legal terms of service for consumption.</li>
  <li><strong>Pricing</strong> - How much does each API request cost me?</li>
</ul>

<p>We need these elements to be machine readable and easily accessible at discover and runtime. Currently the surface area of the API can be described using OpenAPI, that isn’t a problem. The authentication details can be included in this, but it means you already have to have an application setup, with keys. It doesn’t include new users into the equation, meaning, discovering, registering, and obtaining keys. I have a draft specification I call “API plans” for the pricing portion of it, but it is something that still needs a lot of work. So, in short, we are nowhere near having this layer ready for automation–which we will need to scale all of this API stuff.</p>

<p>This is all stuff I’ve been beating a drum about for years, and I anticipate it is a drum I’ll be beating for a number of more years before we see come into focus. I’m eager to see Mike’s prototype on “find and bind”, because it is the only automated, runtime, discovery, registration, and execute research I’ve come across that isn’t some proprietary magic. I’m going to be investing more cycles into my API plans research, as well as the terms of service stuff I started way back when alongside my API Commons project. Hopefully, moving all this forward another inch or two, and flesh out more of the machine readable components we’ll need at this layer.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/28/what-we-need-to-be-machine-readable-at-api-run-time/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/28/the-business-of-running-government-as-a-microservices-platform/">The Business of Running Government As A Microservices Platform</a></h3>
        <span class="post-date">28 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/capital-battle_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" /></p>
<p>I recently <a href="http://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">wrote a response to a recent Department of Veterans Affairs RFI which contained a section about the business of operating government as a microservices platform</a>.I know that many folks wouldn’t make it that far in the 10K word response, so I wanted to break it out into its own post. I feel pretty strongly about the potential of decoupling how we deliver technology across government, but for this to be successful we are also going to have to decouple the business and politics of it all as well. This post reflects <a href="http://public.data.api.management.apievangelist.com/">my current research and thinking about the business of APIs in government, and is part of some ongoing work I am doing around API management, public data, and how we begin to think differently about how government engages with the public in a digital age</a>.</p>

<p>There are many interpretations of what is a microservice, but for the purposes of this post, it is a simple set of APIs that meet one precise set of government services. The API definition, database, back-end code, management layer, documentation, support and all other essential elements are self-contained, and usually stored in a single Github, or Bitbucket repository, when delivering microservices. Each microservice possesses its own technical, business, and political contract, outline how the service will be delivered, managed, supported, communicated, and versioned. These contracts can be realized individually, or grouped together as a larger, aggregate contract that can be submitted, while still allowing each individual service within that contract to operate independently.</p>

<p><strong>Decoupling The Business Of Delivering Government Digital Services</strong>
The microservices approach isn’t just about the technical components. It is about making the business of delivering vital government services more modular, portable, and scalable. Something that will also decouple and shift the politics of delivering critical services to veterans. Breaking things down into much more manageable chunks that can move forward independently at the contract level. Helping both simplify, and streamline the deliver of services for both the provider, vendor, as well as any other stakeholders involved in the software lifecycle. Microservices isn’t just about decoupling the technology, it is about decoupling the business of delivering digital services:</p>

<ul>
  <li><strong>Micro Procurement</strong> - One of the benefits of breaking down services into small chunks, is that the money needed to deliver the service can become much smaller, potentially allowing for a much smaller, more liquid and flowing procurement cycle. Each service has a micro definition of the monetization involved with the service, which can be aggregated by groups of services and projects.</li>
  <li><strong>Micro Payments</strong> - Payments for service deliver can be baked into the operations and life cycle of the service. API management excels at measuring how much a service is accessed, and testing, monitoring, logging, security, and other stops along the API life cycle can all be measured, and payments can be delivered depend on quality of service, as well as volume of service.</li>
</ul>

<p>Amazon Web Services already has the model for defining, measuring, and billing for API consumption in this way. This is the bread and butter of the Amazon Web Services platform, and the cornerstone of what we know as the cloud. This approach to delivering, scaling, and ultimately billing or payment for the operation and consumption of resources, just needs to be realized within each agency, and the rest of the federal government. We have seen a shift in how government views the delivery and operation of technical resources using the cloud over the last five years, we just need to see the same shift for the business of APIs over the next five years.</p>

<p><strong>Changing The Way Government Does Business</strong>
API management is where you begin changing the way government does business. API management has been used for a decade to measure, limit, and quantify the value being exchanged at the API level. Now that API management has been baked into the cloud, we are starting to see the approach being scaled to deliver at a marketplace level. With over ten years of experience with delivering, quantifying, metering and billing at the API level, Amazon is the best example of this monetization approach in action, with two distinct ways of quantifying the business of APIs.</p>

<ul>
  <li><strong>AWS Marketplace Metering Service</strong> - SaaS style billing model which provides a consumption monetization model in which customers are charged only for the number of resources they use–the best known cloud model.</li>
  <li><strong>AWS Contract Service</strong> - Billing customers in advance for the use of software, providing an entitlement monetization model in which customers pay in advance for a certain amount of usage, which could be used to deliver certain amount of storage per month for a year, or a certain amount of end-user licenses for some amount of time.</li>
</ul>

<p>This provides a framework for thinking about how the business of microservices can be delivered. Within these buckets, AWS provides a handful of common dimensions for thinking through the nuts and bolts of these approaches, quantifying how APIs can be monetized, in nine distinct areas:</p>

<ul>
  <li><strong>Users</strong> – One AWS customer can represent an organization with many internal users. Your SaaS application can meter for the number of users signed in or provisioned at a given hour. This category is appropriate for software in which a customer’s users connect to the software directly (for example, with customer-relationship management or business intelligence reporting).</li>
  <li><strong>Hosts</strong> – Any server, node, instance, endpoint, or other part of a computing system. This category is appropriate for software that monitors or scans many customer-owned instances (for example, with performance or security monitoring). Your application can meter for the number of hosts scanned or provisioned in a given hour.</li>
  <li><strong>Data</strong> – Storage or information, measured in MB, GB, or TB. This category is appropriate for software that manages stored data or processes data in batches. Your application can meter for the amount of data processed in a given hour or how much data is stored in a given hour.</li>
  <li><strong>Bandwidth</strong> – Your application can bill customers for an allocation of bandwidth that your application provides, measured in Mbps or Gbps. This category is appropriate for content distribution or network interfaces. Your application can meter for the amount of bandwidth provisioned for a given hour or the highest amount of bandwidth consumed in a given hour.</li>
  <li><strong>Request</strong> – Your application can bill customers for the number of requests they make. This category is appropriate for query-based or API-based solutions. Your application can meter for the number of requests made in a given hour.</li>
  <li><strong>Tiers</strong> – Your application can bill customers for a bundle of features or for providing a suite of dimensions below a certain threshold. This is sometimes referred to as a feature pack. For example, you can bundle multiple features into a single tier of service, such as up to 30 days of data retention, 100 GB of storage, and 50 users. Any usage below this threshold is assigned a lower price as the standard tier. Any usage above this threshold is charged a higher price as the professional tier. Tier is always represented as an amount of time within the tier. This category is appropriate for products with multiple dimensions or support components. Your application should meter for the current quantity of usage in the given tier. This could be a single metering record (1) for the currently selected tier or feature pack.</li>
  <li><strong>Units</strong> – Whereas each of the above is designed to be specific, the dimension of Unit is intended to be generic to permit greater flexibility in how you price your software. For example, an IoT product which integrates with device sensors can interpret dimension “Units” as “sensors”. Your application can also use units to make multiple dimensions available in a single product. For example, you could price by data and by hosts using Units as your dimension. With dimensions, any software product priced through the use of the Metering Service must specify either a single dimension or define up to eight dimensions, each with their own price.</li>
</ul>

<p>These dimensions reflect the majority of API services being sold out there today, we don’t find ourselves in a rut with measuring value, like just paying per API call. Allowing government API plans to possess one or more dimensions, beyond any single use case.</p>

<ul>
  <li><strong>Single Dimension</strong> - This is the simplest pricing option. Customers pay a single price per resource unit per hour, regardless of size or volume (for example, $0.014 per user per hour, or $0.070 per host per hour).</li>
  <li><strong>Multiple Dimensions</strong> – Use this pricing option for resources that vary by size or capacity. For example, for host monitoring, a different price could be set depending on the size of the host. Or, for user-based pricing, a different price could be set based on the type of user (admin, power user, and read-only user). Your service can be priced on up to eight dimensions. If you are using tier-based pricing, you should use one dimension for each tier.</li>
</ul>

<p>This provides a business framework that government can provide for vendors and 3rd party developers, allowing them to operate their services within a variety of business models. Derived from many of the hard costs they face, and providing additional volume based revenue, based upon how may API calls of any particular service receives.</p>

<p>Beyond this basic monetization framework, I’d also recommend adding in an incentive framework that would dovetail with the business models proposed, but then provide different pricing levels depending on how well the services perform, and deliver on the agreed upon API contract. There are a handful of bullets I’d consider here.</p>

<ul>
  <li><strong>Design</strong> - How well does a service meet API design guidelines set forth in governance guidance.</li>
  <li><strong>Monitoring</strong> - Has a service consistently met its monitoring goals, delivering against an agreed upon service level agreement (SLA).</li>
  <li><strong>Testing</strong> - Beyond monitoring, are APIs meeting granular interface testing, along a regular testing &amp; monitoring schedule.</li>
  <li><strong>Communication</strong> - Are service owners meeting expectations around communication around a service operations.</li>
  <li><strong>Support</strong> - Does a service meet required support metrics, making sure it is responsive and helpful.</li>
  <li><strong>Ratings</strong> - Provide a basic set of metrics, with accompanying ratings for each service.</li>
  <li><strong>Certification</strong> - Allowing service providers to get certified, receiving better access, revenue, and priority.</li>
</ul>

<p>All of the incentive framework is defined and enforced via the API governance strategy for the platform. Making sure all microservices, and their owners meet a base set of expectations. When you take the results and apply weekly, monthly, and quarterly against the business framework, you can quickly begin to see some different pricing levels, and revenue opportunities around all microservices emerge. You deliver consistent, reliable, highly ranked microservices, you get paid higher percentages, enjoy greater access to resources, and prioritization in different ways via the platform–if you don’t, you get paid less, and operate fewer services.</p>

<p>This model is already visible on the AWS platform. All the pieces are there to make it happen for any platform, operating on top of the AWS platform. The marketplace, billing, and AWS API Gateway connection to API plans exists. When you combine the authentication and service composition available at the AWS API Gateway layer, with the IAM policy solutions available via AWS, an enterprise grade solution for delivering this model securely at scale, comes into focus.</p>

<p><strong>Incentivizing Government API Vendors and Contractors</strong>
Keep everything small, and well defined. Measure, reported upon, and priced using the cloud model, connecting to a clear set of API governance guidance and expectations. The following areas can support paying and incentivizing contractors based upon not just usage, but also meeting the API contract.</p>

<ul>
  <li><strong>Management</strong> - API management puts all microservices into plans, then log, meter, and track on value exchanged at this level.</li>
  <li><strong>Marketplace</strong> - Turning the platform into a marketplace that can be occupied by a variety of internal, pattern, vendor, 3rd party, and public actors.</li>
  <li><strong>Monetization</strong> - Granular understanding of all the resources it takes to deliver each individual service, and understand the costs associated with operating at scale.</li>
  <li><strong>Plans</strong> - A wealth of API plans in place at the API gateway level, something that is tied to IAM policies, and in alignment with API governance expectations.</li>
  <li><strong>Governance</strong> - Providing a map, and supporting guidance around government platform API governance. Understanding, measuring, and enforcing consistency across the API lifecycle–platform wide.</li>
  <li><strong>Value Exchange</strong> - Using the cloud model, which is essentially the original API management, marketplace, and economy model. Not just measuring consumption, but used to maximize and generate revenue from the value exchanged across the platform.</li>
</ul>

<p>When you operate APIs on AWS and Azure, the platform as a service layer can utilize and benefit from the underlying infrastructure as a service monetization framework. Meaning, you can use AWS’s business model for managing the measuring, paying, and incentivizing of microservice operators. All the gears are there, they just need to be set in motion to support the management of a government API marketplace platform.</p>

<p>I have been studying Amazon full time for almost eight years. I’ve been watching Azure play catch up for the last three years. I run my infrastructure, and a handful of clients on AWS. I understand the API landscape of both providers, and how they can be woven into this vision for the business of government APIs. I see the AWS API stack, and the Azure API stack, as a starter set of services that can be built upon to deliver a government microservices platform. All the components are there. It just need the first set of services to be defined, delivering the essential building blocks any platform needs–things like compute, storage, dns, messaging, etc. The progress to other more outward, application, and system integration services.</p>

<p>My objective with this approach is to enable government services to be delivered as individual, self-contained units, that can be used as part of a larger orchestration of government services. Open up government and letting some sunlight in. Think about what Amazon has been able to achieve by delivering its own internal operations as services, and remaking not just retail, but also touching almost every other industry with Amazon Web Services. The Amazon Web Services myth story provides a powerful and compelling narrative for any company, organizations, institution, or government agency to emulate.</p>

<p>My proposal is not meant to be a utopian vision for how government works. However it is meant to shine a light on existing ways of delivering services via the cloud, with APIs at the center. Helping guide each service in its own individual journey, while also serving the overall mission of the platform–to help the veteran be successful in their own personal journey.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/28/the-business-of-running-government-as-a-microservices-platform/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/27/what-it-the-streamdata-io-api-gallery/">What Is The Streamdata.io API Gallery?</a></h3>
        <span class="post-date">27 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation.</p>

<p><strong>What is the Streamdata.io API Gallery?</strong><br />
It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas.</p>

<p><strong>What are we looking to accomplish with the Streamdata.io API Gallery?</strong><br />
Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible.</p>

<p><strong>What is the opportunity around the Streamdata.io API Gallery?</strong><br />
Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow.</p>

<p><strong>Why is this relevant to Streamdata.io, and their road map?</strong><br />
It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which potential customers might possibly find interesting, bringing them new API consumers, while also opening their eyes up to the event-driven opportunity that exists out there.</p>

<p><strong>Distilling Things Down A Bit More</strong><br />
Ok, that answers the general questions about what the Streamdata.io API Gallery is, and why we are building it. Now I want to distill down a little bit more to help me articulate the gallery as part of a series of presentations, existing as just a handful of bullet points. Helping get the point across in hopefully 60 seconds or less.</p>

<ul>
  <li>What is the Streamdata.io API Gallery?
    <ul>
      <li>API directory, for finding individual units of compute within specific topics.</li>
      <li>OpenAPI (fka Swagger) driven, making each unit of value usable at run-time.</li>
      <li>APIs.json indexed, making the collections of resources easy to search and use.</li>
      <li>Github hosted, making it forkable and continuously deployable and integrate(able).</li>
    </ul>
  </li>
  <li>Why is the Streamdata.io Gallery relevant?
    <ul>
      <li>It maps out the API universe with an emphasis on the value each individual API path possesses.</li>
      <li>Categories, tags, and indexes APIs into collections which are published to Github.</li>
      <li>Provides a human and machine friendly view of the existing publish and subscribe landscape.</li>
      <li>Begins to organize the API universe in context of a real time event-driven messaging world.</li>
    </ul>
  </li>
  <li>What is the opportunity around the Streamdata.io API Gallery?
    <ul>
      <li>Redefining the API landscape from an event-driven perspective.</li>
      <li>Quantify, qualify, and rank APIs to understand what is the most interesting and highest quality.</li>
      <li>Help API providers realize events occurring via their existing platforms.</li>
      <li>Begin moving beyond a request and response model to an event-driven reality.</li>
    </ul>
  </li>
</ul>

<p>There is definitely a lot more going on within the Streamdata.io API Gallery, but I think this captures the essence of what we are trying to achieve. A lot of what we’ve done is building upon my existing API Stack work, where I have worked to profile and index public APIs using OpenAPI and APIs.json, but this round of work is taking things to a new level. With API Stack I ended up with lists of companies and organizations, each possessing a list of APIs. The Streamdata.io API Gallery is a list of API resources, broken down by the unit of value they bring to the table, which is further defined by whether it is a GET, POST, or PUT–essentially a publish or subscribe opportunity.</p>

<p>Additionally, I am finally finding traction with the API rating system(s) I have been developing for the last five years. Profiling and measuring the companies behind the APIs I’m profiling, and making this knowledge available not just at discover time, but potentially at event and run time. Basically being able to understand the value of an event when it happens in real time, and be able to make programmatic decisions regarding whether we care about the particular event or not. Eventually, allowing us to subscribe only to the events that truly matter to us, and are of the highest value–then tuning out the rest. Delivering API ratings in an increasingly crowded and noisy event-driven API landscape.</p>

<p>We have the prototype for the Streamdata.io API Gallery ready to go. We are still adding APIs, and refining how they are tagged and organized. The rating system is very basic right now, but we will be lighting up different dimensions of the rating(s) algorithm, and hopefully delivering on different angles of how we quantify the value of events that occurring. I’m guessing we will be doing a soft launch in the next couple of weeks to little fanfare, and it will be something that builds, and evolves over time as the API index gets refined and used more heavily.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/27/what-it-the-streamdata-io-api-gallery/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting-session/">Three Areas I Would Like To Cover When We Sit Down For An API Consulting Session</a></h3>
        <span class="post-date">27 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_146_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m putting together some presentations for a handful of upcoming engagements, where I’m wanting to help my audience understand what an initial engagement will look like. While I am looking to have just a handful of bullets that can live on a single, or handful of slides, I also want a richer narrative to go along with it. To achieve this I rely on my blog, which helps me work my way through the details of what I do, and distill things down into something that I can deliver on the ground within the companies, organizations, institutions, and government agencies I am conducting business with.</p>

<p>When I am sitting down with a new audience, and working to help them understand how I can help them begin, jumpstart, revive, and move forward with their API journey, I’m usually breaking things into three main areas:</p>

<ul>
  <li><strong>Landscape Mapping</strong> - Establish a map of what currently is within an organization.
    <ul>
      <li><strong>Internal Resources</strong> - What existing web services, APIs, teams, and resources exist?</li>
      <li><strong>External Objectives</strong> - What are the external objectives of doing APIs?</li>
    </ul>
  </li>
  <li><strong>Strategy Development</strong> - Craft a coherent strategy for moving forward with APIs.
    <ul>
      <li><strong>API Lifecycle</strong> - Lay out a step by step list of stops along a modern API life cycle.</li>
      <li><strong>API Support</strong> - Identify how the strategy and operations will be supported within an organization.</li>
      <li><strong>API Evangelism</strong> - Consider how the message around API operations will spread internally, and externally.</li>
    </ul>
  </li>
  <li><strong>Execution</strong> - Identify a clear set of next steps regarding how APIs will evolve.
    <ul>
      <li><strong>Infrastructure</strong> - What services, tooling, and other API infrastructure is needed?</li>
      <li><strong>Resources</strong> - What resources have been identified for moving the API conversation forward?</li>
      <li><strong>Governance</strong> - What is the governance strategy for measuring, reporting upon, and enforcing the deliver of APIs across the API lifecycle presented.</li>
    </ul>
  </li>
</ul>

<p>When I present to a new group of people within an organization, this is the outline I am looking to flesh out. I have to understand what is already occurring (or not) on the ground, which is why I need the landscape map. Then, borrowing from my existing API research I can help develop a a detailed strategy, which includes the critical elements of how we will be supporting and evangelizing the effort–which without, API efforts will always struggle. After that, I want to quickly get to work on how we will be executing on this vision, even if it just involves more investment in the landscape map, and overall strategy.</p>

<p>I am working on more detailed materials to hand out prior to, and at the time I sit down with new clients, but I wanted to articulate in a single page, and using a simple set of bullets what I am looking to accomplish with any new consulting relationship. With a map in hand, and an strategy in mind, I’m confident that I can help folks I talk with move forward with their API journey in a more meaningful way. Something not everyone I talk with is confident in doing on their own, but with a little assistance, I’m pretty sure they will be able to get to work defining what the API journey will look like for their organization.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/27/three-areas-i-would-like-to-cover-when-we-sit-down-for-an-api-consulting-session/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/27/mapping-out-the-api-landscape/">Mapping Out The API Landscape</a></h3>
        <span class="post-date">27 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_174_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>As I prepare to launch the Streamdata.io API Gallery, I am doing a handful of presentations to partners. As part of this process I am looking to distill down the objectives behind the gallery, and the opportunity it delivers to just a handful of talking points I can include in a single slide deck. Of course, as the API Evangelist, the way I do this is by crafting a story here on the blog. To help me frame the conversation, and get to the core of what I needed to present, I wanted to just ask a couple questions, so that I can answer them in my presentation.</p>

<p><strong>What is the Streamdata.io API Gallery?</strong>
It is a machine readable, continuously deployed collection of OpenAPI definitions, indexed used APIs.json, with a user friendly user interface which allows for the browsing, searching, and filtering of individual APIs that deliver value within specific industries and topical areas.</p>

<p><strong>What are we looking to accomplish with the Streamdata.io API Gallery?</strong>
Discover and map out interesting and valuable API resources, then quantify what value they bring to the table while also ranking, categorizing, and making them available in a search engine friendly way that allows potential Streamdata.io customers to discover and understand what is possible.</p>

<p><strong>What is the opportunity around the Streamdata.io API Gallery?</strong>
Identify the best of breed APIs out there, and break down the topics that they deliver within, while also quantifying the publish and subscribe opportunities available–mapping out the event-driven opportunity that has already begun to emerge, while demonstrating Streamdata.io’s role in helping get existing API providers from where they are today, to where they need to be tomorrow.</p>

<p><strong>Why is this relevant to Streamdata.io, and their road map?</strong>
It provides a wealth of research that Streamdata.io can use to understand the API landscape, and feed it’s own sales and marketing strategy, but doing it in a way that generates valuable search engine and social media exhaust which potential customers might possibly find interesting, bringing them new API consumers, while also opening their eyes up to the event-driven opportunity that exists out there.</p>

<p><strong>Distilling Things Down A Bit More</strong>
Ok, that answers the general questions about what the Streamdata.io API Gallery is, and why we are building it. Now I want to distill down a little bit more to help me articulate the gallery as part of a series of presentations, existing as just a handful of bullet points. Helping get the point across in hopefully 60 seconds or less.</p>

<ul>
  <li>What is the Streamdata.io API Gallery?
    <ul>
      <li>API directory, for finding individual units of compute within specific topics.</li>
      <li>OpenAPI (fka Swagger) driven, making each unit of value usable at run-time.</li>
      <li>APIs.json indexed, making the collections of resources easy to search and use.</li>
      <li>Github hosted, making it forkable and continuously deployable and integrate(able).</li>
    </ul>
  </li>
  <li>Why is the Streamdata.io Gallery relevant?
    <ul>
      <li>It maps out the API universe with an emphasis on the value each individual API path possesses.</li>
      <li>Categories, tags, and indexes APIs into collections which are published to Github.</li>
      <li>Provides a human and machine friendly view of the existing publish and subscribe landscape.</li>
      <li>Begins to organize the API universe in context of a real time event-driven messaging world.</li>
    </ul>
  </li>
  <li>What is the opportunity around the Streamdata.io API Gallery?
    <ul>
      <li>Redefining the API landscape from an event-driven perspective.</li>
      <li>Quantify, qualify, and rank APIs to understand what is the most interesting and highest quality.</li>
      <li>Help API providers realize events occurring via their existing platforms.</li>
      <li>Begin moving beyond a request and response model to an event-driven reality.</li>
    </ul>
  </li>
</ul>

<p>There is definitely a lot more going on within the Streamdata.io API Gallery, but I think this captures the essence of what we are trying to achieve. A lot of what we’ve done is building upon my existing API Stack work, where I have worked to profile and index public APIs using OpenAPI and APIs.json, but this round of work is taking things to a new level. With API Stack I ended up with lists of companies and organizations, each possessing a list of APIs. The Streamdata.io API Gallery is a list of API resources, broken down by the unit of value they bring to the table, which is further defined by whether it is a GET, POST, or PUT–essentially a publish or subscribe opportunity.</p>

<p>Additionally, I am finally finding traction with the API rating system(s) I have been developing for the last five years. Profiling and measuring the companies behind the APIs I’m profiling, and making this knowledge available not just at discover time, but potentially at event and run time. Basically being able to understand the value of an event when it happens in real time, and be able to make programmatic decisions regarding whether we care about the particular event or not. Eventually, allowing us to subscribe only to the events that truly matter to us, and are of the highest value–then tuning out the rest. Delivering API ratings in an increasingly crowded and noisy event-driven API landscape.</p>

<p>We have the prototype for the Streamdata.io API Gallery ready to go. We are still adding APIs, and refining how they are tagged and organized. The rating system is very basic right now, but we will be lighting up different dimensions of the rating(s) algorithm, and hopefully delivering on different angles of how we quantify the value of events that occurring. I’m guessing we will be doing a soft launch in the next couple of weeks to little fanfare, and it will be something that builds, and evolves over time as the API index gets refined and used more heavily.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/27/mapping-out-the-api-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/">One Of The Best API Getting Started I Have Come Across</a></h3>
        <span class="post-date">26 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/starling/starling-home-page.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m working my way through banking and Fintech companies in the UK, and I stumbled across the Starling banking API. I began doing my usual clicking around as I do with any API, looking at the documentation, the getting started, and other primary links. After landing on <a href="https://developer.starlingbank.com/get-started">the Starling getting started page</a>, I have to say that it is the single best example of a getting started page I have ever come across in my time as API Evangelist. It is robust, informative, well laid out, and has everything you need to well, get started.</p>

<p>The Starling getting started page is broken up into six separate sections:</p>

<p>1) Register Your Application<br />
2) Setup Starter Kit<br />
3) Play in the Sandbox<br />
4) Personal Access<br />
5) Going Live<br />
6) Contact Us<br /></p>

<p>Each getting started section has a simple, concise description with relevant visuals and code samples, as well as possession simple action buttons, like sign, login, register application, and the other meaningful things you need to get started. The Starling getting started is going to become my go to example of how to create an API getting started page. You can really tell whoever put it together spent a lot of time refining it, and walking through it until it was 100% complete.</p>

<p>Starling even has a sandbox, marketplace, and a join Slack button. I can’t rave about their approach enough. I’m going to turn it into a case study regarding how to create a getting started page, and showcase on the home page of the site. I wish every API put as much energy into their getting started page as Starling has. It would take the friction out of on-boarding APis, and make it a much more pleasant experience.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/26/one-of-the-best-api-getting-started-i-have-come-across/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/">The Banking API Actors In The UK</a></h3>
        <span class="post-date">26 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/shakespeare/shakespeare.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been profiling <a href="https://www.openbanking.org.uk/">the work of the Open Banking Implementation Entity when nit comes to banking API standards in the UK</a>. As part of my getting up to speed on the banking ecosystem in the UK, and Europe, I’ve been posting a series of small blog posts, outlining different aspects of how things work, and who the players are. While going through the Open Banking documentation, I came across a great list of the “actors” int he Open Banking API ecosystem, which taught me a lot about who is involved, and was worth reposting here as a list.</p>

<p>The Open Banking eco-system consists of a number of actors, which may be a natural person or an entity:</p>

<ul>
  <li><strong>Payment Service User (PSU)</strong> - Person - Payment Services User is a natural or legal person making use of a payment service as a payee, payer or both</li>
  <li><strong>Payment Service Provider (PSP)</strong> - Legal Entity - A legal entity (and some natural persons) that provide payment services as defined by PSD2 Article 4(11)</li>
  <li><strong>Account Servicing Payment Service Provider (ASPSP)</strong> - Legal Entity - Account Servicing Payment Service Providers provide and maintain a payment account for a payer as defined by the PSRs and, in the context of the Open Banking Ecosystem are entities that publish Read/Write APIs to permit, with customer consent, payments initiated by third party providers and/or make their customers’ account transaction data available to third party providers via their API end points.</li>
  <li><strong>Third Party Providers / Trusted Third Parties (TPP)</strong> - Legal Entity - Third Party Providers are organisations or natural persons that use APIs developed to Standards to access customer’s accounts, in order to provide account information services and/or to initiate payments. Third Party Providers are either/both Payment Initiation Service Providers (PISPs) and/or Account Information Service Providers (AISPs).</li>
  <li><strong>Payment Initiation Service Provider (PISP)</strong> - Legal Entity - A Payment Initiation Services Provider provides an online service to initiate a payment order at the request of the payment service user with respect to a payment account held at another payment service provider.</li>
  <li><strong>Account Information Service Provider (AISP)</strong> - Legal Entity - An Account Information Service provides account information services as an online service to provide consolidated information on one or more payment accounts held by a payment service user with one or more payment service provider(s).</li>
  <li><strong>TPP Primary Technical Contact (TPP-PTC)</strong> - Person - A Primary Technical Contact is an individual nominated by a TPP to have access to the Directory and will be able to nominate other Directory technical users. This should be a main point of contact on technical configuration and a senior member of staff with responsibility for the management of the Open Banking digital identity.</li>
  <li><strong>TPP Secondory Technical Contact (TPP-STC)</strong> - Person - A person that carries out technical operations on behalf of a TPP. A TPP-STC has the same permissions as a TPP-PTC except for the ability to nominate other Directory technical users.</li>
  <li><strong>ASPSP Primary Technical Contact (ASPSP-PTC)</strong> - Person - A Primary Technical Contact is an individual nominated by the ASPSP to have access to the Directory and will be able to nominate other Directory technical users. This should be a main point of contact on technical configuration and a senior member of staff with responsibility for the management of the Open Banking digital identity.</li>
  <li><strong>ASPSP Secondory Technical Contact (ASPSP-STC)</strong> - Person - A person that carries out technical operations on behalf of an ASPSP. An ASPSP-STC has the same permissions as a ASPSP-PTC except for the ability to nominate other Directory technical users.</li>
  <li><strong>Regulatory Bodies</strong> - Legal Entity - Government or industry bodies that have a regulatory role in the payments industry. This includes, but is not limited to, the UK Competition &amp; Markets Authority (CMA), HM Treasury (HMT), EBA, etc.	No</li>
  <li><strong>Member State Competent Authorities (MSCA)</strong> - Legal Entity - Regulatory Body	The regulatory body (or bodies) in each of the EU member states that is responsible for maintaining a register of payment institutions.</li>
  <li><strong>Financial Conduct Authority (FCA)</strong> - Legal Entity - The Financial Conduct Authority is the competent authority for the UK	No</li>
  <li><strong>Open Banking Limited (OB)</strong> - Legal Entity - The Open Banking Implementation Entity is the delivery organization working with the CMA9 and other stakeholders to define and develop the required APIs, security and messaging standards that underpin Open Banking.</li>
  <li><strong>OB Directory Administrator</strong> - Person - A person working for OB that is responsible for executing various technical processes related to the Directory on behalf of OB.</li>
</ul>

<p>I wish I could find breakdowns of the actors within every industry I work in like this. I’ve been studying up on how the banking industry works, but my knowledge moved forward quite a bit after studying this. It is the best explanation of the acronyms I’ve been coming across lately, and is the best breakdown of everyone involved in an API ecosystem I have ever come across in my eight years of studying the space.</p>

<p>Open Banking is doing some really interesting work on the banking API front. This is just one of many artifacts I’m coming across that demonstrate they really have their act together when it comes to developing banking API specifications in the UK.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">Round Two Of The Department of Veterans Affairs Lighthouse Platform RFI</a></h3>
        <span class="post-date">24 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/23_113_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m spending some more time thinking about APIs at the Department of Veterans Affairs (VA), in response to round two of their request for information (RFI). <a href="http://apievangelist.com/2017/10/26/my-response-on-the-department-of-veterans-affairs-rfi-for-the-lighthouse-api-management-platform/">A couple months back I had responded to an earlier RFI</a>, providing as much information as I could think of, for consideration as part of their API journey. As a former VA employee, and son of two Vietnam Vets (yes two), you can say I’m always willing to invest some in APIs over at the VA.</p>

<p>To provide a response, I have taken the main questions they asked, broken them out here, and provided answers to the best of my ability. In my style, the answers are just free form rants, based upon my knowledge of the VA, and the wider API space. It is up to the VA, to decide what is relevant to them, and should be included in their agency API strategy.</p>

<p><strong><em>2. Current Scope<br />
While the acquisition strategy for Lighthouse has not yet been formalized, VA envisions that the program will consist of multiple contracts.  For example, a contract for recommending policy and standards to form governance would likely be separate from an API build team.  The key high level activities below are anticipated to be included within these contracts, and VA is requesting feedback from industry on how these activities should be aligned between multiple contracts.  The list below is not inclusive of all tasks required to support this program.  Additionally, VA intends to provide the IAM solution and the provisioning of necessary cloud resources to host the proposed technology stack.  VA’s current enterprise cloud providers are Microsoft Azure and Amazon Web Services.</em></strong></p>

<p><strong>Microservice Focused Operational &amp; Implementation</strong><br />
Lighthouse should embrace a microservices way of doing things, so that the platform can avoid legacy trappings when it comes to delivering software at the VA, which have resulted in large, monolithic systems, possessing enormous budgets, and entrenched teams, that are able to develop a resistance to change and evolution. This microservices way of doing things should be adopted internally, as well as externally, then applied to the technology, business, and politics of delivering ALL Lighthouse infrastructure.</p>

<p>All contracts should be defined and executed in a modular way, with the only distinction between  projects being operational, or for specific project implementations. Everything should be delivered as microservices, no matter whether it is in support of operating the Lighthouse platform, or delivering services to Lighthouse-driven applications. The technology and business of each service should be self-contained, modular, and focusing on doing one thing, and doing it well. Ensuring all services executed as part of Lighthouse operations are decoupled, working independently, allowing for easily defining, delivering, managing, evolving, and deprecating of every operational and implementational service that makes Lighthouse work.</p>

<p>Operational services will be the first projects delivered via the platform, and will be used to establish and mature the Lighthouse project deliver workflow, but then going forward, every additional operational, as well as specific implementation focused service will utilize the same workflow and life cycle.</p>

<ul>
  <li><strong>Definitions</strong> - Everything begins as a set of definitions. Leveraging OpenAPI, JSON Schema, Dockerfiles, and other common definitions to provide a human, and machine readable definition of every project, which is ultimately delivered as a microservice.</li>
  <li><strong>Github</strong> - Each microservice begins as either a public or private Github repository, with a README index of the definition of what a service will deliver. Providing a self-contained, continuously deployed and integration blueprint of what a service does.</li>
  <li><strong>Architecture</strong> - Always providing a comprehensive outline all backend architecture used to support a specific service, including the technical, as well as the business, and security policy elements of what it takes to deliver the required service.</li>
  <li><strong>Tooling</strong> - Always providing a comprehensive outline of any tools used as part of delivering a service, to provide what is needed from a front-end delivery and execution vantage point.</li>
  <li><strong>Lifecycle</strong> - Establish a lifecycle, that each service will need to pass through, ensuring consistent delivery, and management of services that adhere to governance standards.
    <ul>
      <li><strong>define</strong> - What definitions are required for services?</li>
      <li><strong>design</strong> - What is the API design guidance involved?</li>
      <li><strong>mock</strong> - How are APIs and data virtualized as part of development?</li>
      <li><strong>portal</strong> - Which portals are service published to, or will possess?</li>
      <li><strong>document</strong> - What documentation is required and delivered?</li>
      <li><strong>test</strong> - Where are the code, as well as interface level tests?</li>
      <li><strong>clients</strong> - What client environment are in use for design, development, and testing?</li>
      <li><strong>**</strong>* - Pause there, and repeat until the desired service is realized…</li>
      <li><strong>deploy</strong> - How are services delivered as part of a containerized, continuous deployment pipeline?</li>
      <li><strong>dns</strong> - What DNS is needed to address and route traffic to services?</li>
      <li><strong>manage</strong> - What API management level services are in place to secure, log, limit, and report of API and service consumption?</li>
      <li><strong>logging</strong> - What is the logging stack, how is it shipped, analyzed, and reported upon?</li>
      <li><strong>monitor</strong> - What monitors are required and in place for each service?</li>
      <li><strong>performance</strong> - How is performance measured and reported upon?</li>
      <li><strong>sdk</strong> - What client libraries, SDKs, and samples in place for service integration?</li>
      <li><strong>depenencies</strong> - What internal service, and external API dependencies are in play?</li>
      <li><strong>licensing</strong> - What is the data, code, interface, and other licensing that apply?</li>
      <li><strong>privacy</strong> - Are privacy policies in place, and considered for the platform, partners, developers, and end-users.</li>
      <li><strong>terms</strong> - Are terms of service in place, and independently considered for each service?</li>
      <li><strong>monetization</strong> - What are the operating costs, and other monetization considerations?</li>
      <li><strong>plans</strong> - What API consumption plans, rate limits, and policies in place to govern service usage?</li>
      <li><strong>support</strong> - What support mechanisms are in place, with relevant point of contacts?</li>
      <li><strong>communication</strong> - What communication channels are in place, such as blogs, social, and messaging channels?</li>
      <li><strong>observability</strong> - What is the observability of each service, from open source to monitoring, and CI/CD workflows, ensuring it can be audited?</li>
      <li><strong>discovery</strong> - What is required to register, route, and discover an API as part of overall operations?</li>
      <li><strong>evangelism</strong> - What is the plan for making sure a service is known, used, and evangelized amongst target audience?</li>
    </ul>
  </li>
  <li><strong>Governance</strong> - How is each step along the life cycle measured, reported upon, and audited as part of governance, to understand how a service is meeting platform requirements, and evolving along a maturity path–allowing for innovation to occur, and newer ideas to flourish, but also allow more hardened, secure, and mature services to rise to the top.</li>
</ul>

<p>The OpenAPI, JSON Schema, and other definitions for each microservice will ultimately be the contract for each project. Of course, to deliver the first set of operational platform services (compute, storage, DNS, pipeline, logging, etc.) these independent contracts might need to be grouped into a single, initial contract. Something that will also occur around different groups of services being delivered at any point in the future, but each individual service should be self-contained, with its own contract definition existing in it’s Github repository core.</p>

<p><em><strong>Question: API Roadmap Development (Backlog, Future)</strong></em><br />
Each service being delivered via Lighthouse will possess its own self-contained road map as part of its definition. Providing a standardized, yet scalable way to address what is being planned, is being delivered, operated, and when anything will ultimately be deprecated.</p>

<ul>
  <li><strong>Github Issues</strong> - Each Github repository has it’s own issues for managing all conversations around the service road map. Tags and milestones can be used to designate the past, present, future, and other relevant segmentation of the road map.</li>
  <li><strong>Micro / Macro</strong> - Each services posses micro level detail about the road map, which is available via Github APIs, in a machine readable way for inclusion at the macro level, serving governance, reporting, and higher level road map considerations.</li>
  <li><strong>Communication</strong> - Each service owner is responsible for road map related communication, support, and management providing their piece of the overall road map puzzle.</li>
</ul>

<p>The Lighthouse platform road map should work like an orchestra, with each participant bringing their contribution, but platform operators and conductors defining the overall direction the platforms is headed. At scale, Lighthouse will be thousands of smaller units, organized by hundreds of service owners and stewards, serving millions of end-users, with feedback loops in service through the stack.</p>

<p><em><strong>Question: Outreach (Internal &amp; External Parties)</strong></em><br />
Outreach is essential to the viability of any platform, and represents the business and political challenges that lie ahead for the VA, or any government agency looking to work seamlessly with public and private sector partners, as well as the public at large. There will be many services involved with Lighthouse operations that will need to be private, but the default should always be public, allowing for as much transparency and observability as possible, which will feed platform outreach in a positive way.</p>

<ul>
  <li><strong>Github Project Pages</strong> - Each Github repository can have a public facing Github Pages static site portal and landing page. Allowing for individual service, or group portals to exist, providing a destination for all stakeholders to get involved.</li>
  <li><strong>Github Social Framework</strong> - Github provides a wealth of outreach and communication solutions from organization and repository search, to issues and wikis, and tagging services with individual topics. All of which can be used as part of outreach and engagement in a private or public setting.</li>
  <li><strong>Twitter</strong> - Microblogging provides a great way to publish regular updates, and provide communication around platform operations.</li>
  <li><strong>Linkedin</strong> - Enterprise development groups, especially those in service of the government tend to use Github for establishing their profile, and maintaining their presence, which can be incorporated into all outreach efforts.</li>
  <li><strong>Blogs</strong> - The platform should possess its own public and / or private blogs, as well as potentially more topically, service, or project based blogs that expand outreach to the long tail of platform operations.</li>
</ul>

<p>This type of outreach around platform operations is something that scares the hell out of government folks, and the majority of government APIs operation are critically deficient in the area of outreach. This has to change. If there is no feedback loop in place, and outreach doesn’t occur regularly and consistently, the platform will not succeed. This is how the API world operates.</p>

<p><em><strong>Question: Management of API Request Process (Internal (VA)/External (Non-VA))</strong></em><br />
New services will always be needed. Operational and implementation related requests should all be treated the same. Obviously there will be different prioritization mechanisms in place, but API requests should just be the birth of any new service, allowing it to begin its journey, and transit through the API lifecycle described above. Not all requests will reach deployment, and not all deployments will reach maturity, but all API requests should be treated equally.</p>

<ul>
  <li><strong>Definitions</strong> - Each API request begins with a definition. A simple description of what a service will do.</li>
  <li><strong>Github</strong> - Each API request begins its journey as a Github repository, with a README containing its basic definition, and conversation around its viability within Github issues.</li>
  <li><strong>JSON Schema</strong> - As part of each request, all data that will be leverage as part of service operations should be defined as JSON Schema, and included in the Github repository.</li>
  <li><strong>OpenAPI</strong> - Additionally, the access to the service, and its underlying data and resource should be defined using a machine readable OpenaPI definition, outlining the contract of the service.</li>
  <li><strong>Certification</strong> - Some stakeholders will have submitted API requests before, and better understand the process, and be certified owners of existing services, working as part of trusted organizations, expediting and prioritizing the request process.</li>
  <li><strong>Template(s)</strong> - The most common service patterns to emerge should be defined as template, providing seeds and starter projects to help expedite and streamline the API request process, ensuring all the moving parts are there to make a decision, in a forkable, replicable package.</li>
</ul>

<p>New API requests should be encourage. Anyone should be able to submit a new service, replicate, or augment an existing service, or respond to a platform API RFP. The life cycle described above should be open to everyone looking to submit an API request. Allowing them to define, design, mock, and iterate their submission. Even providing a nearly usable representation of a service, even before the idea or service is accepted. Forcing everyone to flesh out their service, deliver a viable proof of concept, that will streamline the API acceptance process.</p>

<p><em><strong>Question: Propose, Implement and Manage the PaaS (technology stack)</strong></em><br />
As mentioned before, this aspect of Lighthouse should be delivered as microservices, alongside every other service being delivered via the platform. It just so happens that this portion of the stack will be the first to be delivered, and be iterated upon, evolved, and deprecated just like any other service. To put this in perspective, I will outline the AWS, and Azure infrastructure need to support management of the platform later on in this post, while considering the fact that AWS and Azure have been on the same journey that the VA is on with Lighthouse, something that has been playing out for the last decade.</p>

<p>The VA wants to be the Amazon of serving veterans. They want internal groups, vendors, contractors, veteran health and service organizations, and independent developers to come build their solutions on the Lighthouse platform. The VA should uses its own services for internal service delivery, as well as supporting external projects. The operational side of Lighthouse platform should be all microservice projects, with the underlying infrastructure being Azure or AWS solutions, providing a common platform as a service stack that can be leveraged, no matter where the actual service is deployed.</p>

<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/68_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>

<p><em><strong>Question: DevOps Continuous Integration and Continuous Delivery (CI/CD) of APIs</strong></em><br />
Every service in support of operations or implementations via the Lighthouse platform will exist as a self-contained Github repository, with all the artifacts needed to be included in any application pipeline. The basic DNA blueprint for each service should be crafted to support any single CI/CD service, or ideally even multiple types of CI/CD and orchestration solutions like AWS and Azure both support.</p>

<ul>
  <li><strong>Microservices</strong> - Lighthouse CI/CD will be all about microservice orchestration, and using a variety of pipelines to deliver and evolve initially hundreds, and eventually thousands of services in concert.</li>
  <li><strong>Github</strong> - Github will the cellular component driving the Lighthouse CI/CD workflow, providing individual service “legos” that can be composed, assembled, disassembled, and delivered in any way.</li>
  <li><strong>Definitions</strong> - Each microservice will contain all the artifacts needed for supporting the entire life cycle listed above, driven by a variety of CI/CD pipelines. Leveraging dockerfiles, build packages, OpenAPI definitions, schema, and other definitions to continuously deliver and integrate across platform operations.</li>
</ul>

<p>Both AWS and Azure provide CI/CD workflows, which can be used to satisfy the portion of the RFI. I will list out all the AWS and Azure services I think should be considered below. Additionally, Jenkins, CircleCI, or other 3rd party CI/CD could easily be brought in to deliver on this aspect of platform delivery. The microservices core can be used as part of any pipeline delivery model.</p>

<p><em><strong>Question: Environment Operations &amp; Maintenance (O&amp;M)</strong></em><br />
Again, everything operates as microservices, and gets delivered independently as services that can be configured and maintained as part of overall platform operations and maintenance, or in service of individual services, and groups of services supporting specific implementations.</p>

<ul>
  <li><strong>Microservices</strong> - Everything is available as microservices, allowing the underlying environment operations and maintainenace to be orchestration, and optimized in real time.</li>
</ul>

<p>Each of the AWS and Azure services listed below are APIs. They allow for the configuration and management of each service via API or CLI, allowing the architecture to be seamlessly managed as part of the overall API stack, as well as the CI/CD pipeline. Making environment operations and maintenance, just part of the continuous delivery cycle(s).</p>

<p><em><strong>Question: Release Management</strong></em><br />
Release occurs at the granular service level. With Github and CI/CD as the vehicle for moving release forward daily, versioning, defining, and communicating all the way. With the proper code and API level testing in place, release management can happen safely at scale.</p>

<ul>
  <li><strong>Github</strong> - Github version control, branches, and release management should  be used as part of the overall release management strategy.</li>
  <li><strong>Versioning</strong> - Establishment of a service versioning strategy for minor and major code, and interface releases, allowing independent release management that can occur at the higher orchestration level</li>
  <li><strong>CI/CD Pipelines</strong> - Everything should be a pipeline, broken down by logical operational, organization, and project boundaries, operating on a continuous release cycle.</li>
  <li><strong>Microservices</strong> - Everything is operated independently, and released independently via containers, with approach dependency management as part of each release.</li>
  <li><strong>Definitions</strong> - OpenAPI and JSON Schema are versioned and use to act as the contract for each release.</li>
  <li><strong>Communications</strong> - Along with each release, comes a standard approach to notification, communication, and support.</li>
</ul>

<p>Release management will horizontally take a significant amount of time to wrap your head around. Moving forward hundreds, and thousands of services in concert won’t be easy. However it will be more resilient, and forgiving than moving forward a single monolith.</p>

<p><em><strong>Question: API Analytics</strong></em><br />
Awareness should be baked in by default to the Lighthouse platform, measuring everything, and reporting on it consistently, providing observability across all aspects of operations in alignment with security policies. Analysis should be its own set of operational services, that span the entire length of the Lighthouse platform.</p>

<ul>
  <li><strong>Log Shipping</strong> - The database, container, web server, management, and DNS logs for ALL services should be shipped, and centralized, for complete access and analysis.</li>
  <li><strong>APIs</strong> - Centralized logs should be its own service, with programmatic access to logs for all platform services.</li>
  <li><strong>Modular</strong> - Analytics should be modular, bit-size API-driven elements that can be mixed, composed, published, and visualized in reusable ways.</li>
  <li><strong>Embeddables</strong> - Modular, embeddable UI elements should be developed as applications on top of platform analytics APIs, allowing for portable dashboard that can be remixed, reused, and evolved.</li>
  <li><strong>Search</strong> - The logging and reporting layer of the platform should have a core search element, allowing all logs to searched, as well as the logs for how API consumers are analyzing logs (mind blown).</li>
  <li><strong>Continouous</strong> - As with all other services, analytics, reporting, and visualizations should be continuous, and ever evolving and deployed on a day to day, week to week basis.</li>
</ul>

<p>A standard logging strategy across all services is how we achieve a higher level of API analytics, going beyond just database or web server statics, and even API management analytics, providing end to end, comprehensive platform service measurement, analysis, reporting, and visualization. Allowing platform operators, consumers, and auditors to access and understand how all service are being used, or not being used.</p>

<p><em><strong>Question: Approval to Operate (ATO) Support for Environments</strong></em><br />
Every service introduced as part of the Lighthouse platform should have all the information required to support ATO, with it baked into the governance and maturity life cycle for any service. It actually lends itself well to the maturity elements of the lifecycle above, ensuring there is ATO before anything is deployed.</p>

<ul>
  <li><strong>Definitions</strong> - All definitions are present for satisfying ATO.</li>
  <li><strong>Github</strong> - Everything is self-contained within a single place for submission.</li>
  <li><strong>Governance</strong> - ATO is part of the governance process, while still allowing for innovation.</li>
  <li><strong>Micro / Macro</strong> - ATO for each individual service can be considered, as well as at the project, group levels, understanding where services fit in at macro level.</li>
</ul>

<p>ATO can be built into the templated API request and submission process discussed earlier, allowing for already approved architecture, tooling, and patterns to be used over and over, streamlining the ATO cycle. Helping service developers enjoy more certainty around the ATO process, while still allowing for innovation to occur, pushing the ATO definition and process when it makes sense.</p>

<p><em><strong>Question: Build APIs including system level APIs that connect into backend VA systems</strong></em><br />
Everything is a microservice, and there are plenty of approaches to ensure that legacy backend systems can enjoy continued use and evolution through evolved APIs. The API life cycle allows for the evolution of existing backend systems that operate in the cloud and on-premise in small, bit-size service implementations.</p>

<ul>
  <li><strong>Gateway</strong> - AWS API Gateway and Azure API management makes it easy to publish newer APIs on top of legacy backend systems.</li>
  <li><strong>Facades</strong> - Establishing facade patterns for modernizing, and evolving legacy systems, allowing them to take on a new interface, while still maintaining existing system.</li>
  <li><strong>OpenAPI</strong> - Map out newer APIs using OpenAPI, then importing into gateways and wiring up with backend systems.</li>
  <li><strong>Schema</strong>- Mapping out the schema transformations from backend systems to front-end API requests and responses using JSON Path, and JSON Schema.</li>
  <li><strong>Microservices</strong> - Delivering newer APIs on top of legacy systems in smaller, more evolvable services.</li>
</ul>

<p>From the frontend, you shouldn’t be able to tell whether a legacy VA system is in use, or newer cloud infrastructure. All applications should be using APIs, and all APIs should be delivered as individual or groups of microservices, that do one thing and does it well. As APIs evolve, the backend systems should be decoupled and evolved as well, but until that becomes possible, all consumption of data, content, and other resources will be routed through the Lighthouse API stack.</p>

<p><em><strong>Question: API key management or managing third party access (authorization, throttling, etc.)</strong></em><br />
Both theAWS API Gateway, and Azure API Management allow for the delivery of modern API management infrastructure that can be used to govern internal, partner, and 3rd party access to resources. All applications should be using APIs, and ALL APIs should be using a standardized API management approach, no matter whether the consumption is internal or external. Ensuring consistent authorization, throttling, logging, and other aspects of doing business with APIs.</p>

<ul>
  <li><strong>IAM</strong> - Leverage API keys, JWT, and OAuth in conjunction with IAM policies governing which backend resources are available to API consumers.</li>
  <li><strong>Gateway</strong> - All API traffic is routed through the AWS API Gateway and Azure API management layers, allowing for consistent and comprehensive management across all API consumption.</li>
  <li><strong>Management</strong> - Apply consistent logging, rate limiting, transformations, error and security at the API management level, ensuring all services behave in the same way.</li>
  <li><strong>Plans</strong> - Establishing of a variety of API plans that dictate API levels of access, which services are accessible to different API key levels, that are in sync with backend IAM policies.</li>
  <li><strong>Logging</strong> - Every API call is logged, and contains user and application keys, allow ALL API consumption to be audited and reported upon, and responded to in real time.</li>
  <li><strong>Security</strong> - Providing a single point of entry, and the ability to shut down access, striking the balance between access and security which is the hallmark of doing APIs.</li>
</ul>

<p>API management is baked into the cloud. It is a discipline that has been evolving for over a decade, and is available on both the AWS and Azure platforms. The tools are there, Lighthouse just needs to establish a coherent strategy for authentication, service composition, logging, reporting, and responding to API consumption at scale in real time. Staying out of the way of consumers, while also ensuring that they only have access to the data, content, and other resources they are allowed to, in alignment with overall governance.</p>

<p><em><strong>Management of API lifecycle in cloud, hybrid, and/or on premise environments</strong></em><br />
All operational aspects of the Lighthouse platform should be developed as independent microservices, with a common API–no matter what the underlying architecture is. The DNS service API should be the same, regardless of whether it is managing AWS or Azure DNS, or possibly any other on-premise or 3rd party service–allowing for platform orchestration using a common API stack.</p>

<ul>
  <li><strong>Microservices</strong> - Each operational service is a microservice, with possibly multiple versions, depending on the backend architecture in use.</li>
  <li><strong>Containers</strong> - Every operational service is operated as a container, allowing it to run in any cloud environment.</li>
  <li><strong>Github</strong> - All services live as a Github repository, allowing it to be checked out and forked via any cloud platform.</li>
</ul>

<p>The modular, containerized, microservice approach to delivering the Lighthouse platform will allow for the deployment, scaling, and redundant implementation of services in any cloud environment, as well as on-premise, or hybrid scenarios. All services operate using the same microservice footprint, using containers, and a consistent API surface area, allowing for the entire platform stack to be orchestrated against no matter where the actual service resides.</p>

<p>_**Question: 3. Use Case<br />
To better provide insight into aligning activities to contracts, VA has provided the use case below. Please walk through this use case discussing each activity and the contract it would be executed under.</p>

<p><strong>Veteran Verification Sample Use Case: VA has a need for a Veteran Verification API to verify a Veteran status from a number of VA backend systems to be shared internally and externally as an authoritative data source.  These backend systems potentially have conflicting data, various system owners, and varying degrees of system uptime._</strong></p>

<p>This is a common problem within large organizations, institutions, and government agencies. This is why we work to decouple, modularize, and scale not just the technology of building applications on backend systems, but also the business, and politics of it all. Introducing a competitive element when it comes to data management access, and building in redundancy, resilience, and a healthier incentive model into how we provide access to critical data, content, and other resources.</p>

<p>I have personal experience with this particular use case. One of the things I did while working at the VA, was conduct public data inventory, and move forward the conversation around a set of veteran benefit web services, which included asking the question–who had the authoritative record for a veteran? Many groups felt they were the authority, but in my experience, nobody actually did entirely. The incentives in this environment weren’t about actually delivering a meaningful record on a veteran, it was all about getting a significant portion of the budget. I recommend decoupling the technology, business, and politics of providing access to veterans data using a microservices approach.</p>

<ul>
  <li><strong>Microservices</strong> - Break the veterans record into separate, meaningful services.</li>
  <li><strong>Definitions</strong> - Ensure the definitions for the schema and API are open and accessible.</li>
  <li><strong>Discovery</strong> - Make sure that the Veteran Verification API is full discoverable.</li>
  <li><strong>Testing</strong> - Make sure the Verification API is fully tested on a regular basis.</li>
  <li><strong>Monitoring</strong> - Ensure that there are regular monitors for the Verification API.</li>
  <li><strong>Redundancy</strong> - Encourage multiple implementations of the same API to be delivered and owned by separate groups in separate regions, with circuit breaker behavior in APIs and applications.</li>
  <li><strong>Balancing</strong> - Load balance between services and regions, allowing for auto-scaled APIs.</li>
  <li><strong>Aggregation</strong> - Encourage the development of aggregate APIs that bridge multiple source, providing aggregate versions of the veteran’s record, encouraging new service owners to improve on existing services.</li>
  <li><strong>Reliability</strong> - Incentivize and reward reliability with Verification API owners, through revenue and priority access.</li>
</ul>

<p>There should be no single owner of any critical VA service. Each service should have redundant versions of the service, available in different regions, and managed by separate owners. Competition should be encouraged, with facade and aggregate introduced, putting pressure on core service providers to deliver quality, or their service(s) will be de-prioritized, and newer services will be given traffic and revenue priority. The same backend database can be exposed via many different APIs, with a variety of owners and incentives in place to encourage the quality of service.</p>

<p>APIs, coupled with the proper terms of service in place can eliminate an environment where defensive data positions are established. If other API owners can get access to the same data, and offer a better quality API, then evangelize and gain traction with application owners, entrenched API providers will no longer flourish. Aggregate and facade APIs allow for the evolution of existing APIs, even if the API owners are unwilling to move and evolve. Shifting the definition of what is authoritative, making it much more liquid, allowing it to shift and evolve, rather than just be diluted and meaningless, as it is often seen in the current environment.</p>

<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/69_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>

<p>_<strong>Question: 4. Response<br />
In addition to providing the requested content above, VA asks for vendors to respond to the following questions:</strong></p>

<p><strong>Describe how you would align the aforementioned activities between contracts, and the recommended price structure for contracts?_</strong></p>

<p>Each microservice would have its own technical, business, and political contract, outline how the service will be delivered, managed, supported, communicated, and versioned. These contracts can be realized individually, or grouped together as a larger, aggregate contract that can be submitted, while still allowing each individual service within that contract to operate independently.</p>

<p>As mentioned before, the microservices approach isn’t just about the technical components. It is about making the business of delivering vital VA services more modular, portable, and scalable. Something that will also decouple and shift the politics of delivering critical services to veterans. Breaking things down into much more manageable chunks that can move forward independently at the contract level.</p>

<ul>
  <li><strong>Micro Procurement</strong> - One of the benefits of breaking down services into small chunks, is that the money needed to deliver the service can become much smaller, potentially allowing for a much smaller, more liquid and flowing procurement cycle. Each service has a micro definition of the monetization involved with the service, which can be aggregated by groups of services and projects.</li>
  <li><strong>Micro Payments</strong> - Payments for service deliver can be baked into the operations and life cycle of the service. API management excels at measuring how much a service is accessed, and testing, monitoring, logging, security, and other stops along the API life cycle can all be measured, and payments can be delivered depend on quality of service, as well as volume of service.</li>
</ul>

<p>Amazon Web Services already has the model for defining, measuring, and billing for API consumption in this way. This is the bread and butter of the Amazon Web Services platform, and the cornerstone of what we know as the cloud. This approach to delivering, scaling, and ultimately billing or payment for the operation and consumption of resources, just needs to be realized by the VA, and the rest of the federal government. We have seen a shift in how government views the delivery and operation of technical resources using the cloud over the last five years, we just need to see the same shift for the business of APIs over the next five years.</p>

<p>_<strong>Question: The Government envisions a managed service (ie: vendor responsible for all aspects including licenses, scaling, provisioning users, etc.) model for the entire technology stack.  How could this be priced to allow for scaling as more APIs are used?  For example, would it be priced by users, API calls, etc.?_</strong></p>

<p>API management is where you start this conversation. It has been used for a decade to measure, limit, and quantify the value being exchanged at the API level. Now that API management has been baked into the cloud, we are starting to see the approach being scaled to deliver at a marketplace level. With over ten years of experience with delivering, quantifying, metering and billing at the API level, Amazon is the best example of this monetization approach in action, with two distinct ways of quantifying the business of APIs.</p>

<ul>
  <li><strong>AWS Marketplace Metering Service</strong> - SaaS style billing model which provides a consumption monetization model in which customers are charged only for the number of resources they use–the best known cloud model.</li>
  <li><strong>AWS Contract Service</strong> - Billing customers in advance for the use of software, providing an entitlement monetization model in which customers pay in advance for a certain amount of usage, which could be used to deliver certain amount of storage per month for a year, or a certain amount of end-user licenses for some amount of time.</li>
</ul>

<p>This provides a framework for thinking about how the business of microservices can be delivered. Within these buckets, AWS provides a handful of common dimensions for thinking through the nuts and bolts of these approaches, quantifying how APIs can be monetized, in nine distinct areas:</p>

<ul>
  <li><strong>Users</strong> – One AWS customer can represent an organization with many internal users. Your SaaS application can meter for the number of users signed in or provisioned at a given hour. This category is appropriate for software in which a customer’s users connect to the software directly (for example, with customer-relationship management or business intelligence reporting).</li>
  <li><strong>Hosts</strong> – Any server, node, instance, endpoint, or other part of a computing system. This category is appropriate for software that monitors or scans many customer-owned instances (for example, with performance or security monitoring). Your application can meter for the number of hosts scanned or provisioned in a given hour.</li>
  <li><strong>Data</strong> – Storage or information, measured in MB, GB, or TB. This category is appropriate for software that manages stored data or processes data in batches. Your application can meter for the amount of data processed in a given hour or how much data is stored in a given hour.</li>
  <li><strong>Bandwidth</strong> – Your application can bill customers for an allocation of bandwidth that your application provides, measured in Mbps or Gbps. This category is appropriate for content distribution or network interfaces. Your application can meter for the amount of bandwidth provisioned for a given hour or the highest amount of bandwidth consumed in a given hour.</li>
  <li><strong>Request</strong> – Your application can bill customers for the number of requests they make. This category is appropriate for query-based or API-based solutions. Your application can meter for the number of requests made in a given hour.</li>
  <li><strong>Tiers</strong> – Your application can bill customers for a bundle of features or for providing a suite of dimensions below a certain threshold. This is sometimes referred to as a feature pack. For example, you can bundle multiple features into a single tier of service, such as up to 30 days of data retention, 100 GB of storage, and 50 users. Any usage below this threshold is assigned a lower price as the standard tier. Any usage above this threshold is charged a higher price as the professional tier. Tier is always represented as an amount of time within the tier. This category is appropriate for products with multiple dimensions or support components. Your application should meter for the current quantity of usage in the given tier. This could be a single metering record (1) for the currently selected tier or feature pack.</li>
  <li><strong>Units</strong> – Whereas each of the above is designed to be specific, the dimension of Unit is intended to be generic to permit greater flexibility in how you price your software. For example, an IoT product which integrates with device sensors can interpret dimension “Units” as “sensors”. Your application can also use units to make multiple dimensions available in a single product. For example, you could price by data and by hosts using Units as your dimension. With dimensions, any software product priced through the use of the Metering Service must specify either a single dimension or define up to eight dimensions, each with their own price.</li>
</ul>

<p>These dimensions reflect the majority of API services being sold out there today, we don’t find ourselves in a rut with measuring value, like just paying per API call. Allowing Lighthouse API plans to possess one or more dimensions, beyond any single use case.</p>

<ul>
  <li><strong>Single Dimension</strong> - This is the simplest pricing option. Customers pay a single price per resource unit per hour, regardless of size or volume (for example, $0.014 per user per hour, or $0.070 per host per hour).</li>
  <li><strong>Multiple Dimensions</strong> – Use this pricing option for resources that vary by size or capacity. For example, for host monitoring, a different price could be set depending on the size of the host. Or, for user-based pricing, a different price could be set based on the type of user (admin, power user, and read-only user). Your service can be priced on up to eight dimensions. If you are using tier-based pricing, you should use one dimension for each tier.</li>
</ul>

<p>This provides a framework that Lighthouse can provide to 3rd party developers, allowing them to operate their services within a variety of business models. Derived from many of the hard costs they face, and providing additional volume based revenue, based upon how may API calls of any particular service receives.</p>

<p>Beyond this basic monetization framework, I’d add in an incentive framework that would dovetail with the business models proposed, but then provide different pricing levels depending on how well the services perform, and deliver on the agreed upon API contract. There are a handful of bullets I’d consider here.</p>

<ul>
  <li><strong>Design</strong> - How well does a service meet API design guidelines set forth in governance guidance.</li>
  <li><strong>Monitoring</strong> - Has a service consistently met its monitoring goals, delivering against an agreed upon service level agreement (SLA).</li>
  <li><strong>Testing</strong> - Beyond monitoring, are APIs meeting granular interface testing, along a regular testing &amp; monitoring schedule.</li>
  <li><strong>Communication</strong> - Are service owners meeting expectations around communication around a service operations.</li>
  <li><strong>Support</strong> - Does a service meet required support metrics, making sure it is responsive and helpful.</li>
  <li><strong>Ratings</strong> - Provide a basic set of metrics, with accompanying ratings for each service.</li>
  <li><strong>Certification</strong> - Allowing service providers to get certified, receiving better access, revenue, and priority.</li>
</ul>

<p>All of the incentive framework is defined and enforced via the API governance strategy for the platform. Making sure all microservices, and their owners meet a base set of expectations. When you take the results and apply weekly, monthly, and quarterly against the business framework, you can quickly begin to see some different pricing levels, and revenue opportunities around all microservices emerge. You deliver consistent, reliable, highly ranked microservices, you get paid higher percentages, enjoy greater access to resources, and prioritization in different ways via the platform–if you don’t, you get paid less, and operate fewer services.</p>

<p>This model is already visible on the AWS platform. All the pieces are there to make it happen for any platform, operating on top of the AWS platform. The marketplace, billing, and AWS API Gateway connection to API plans exists. When you combine the authentication and service composition available at the AWS API Gateway layer, with the IAM policy solutions available via AWS, an enterprise grade solution for delivering this model securely at scale, comes into focus.</p>

<p><em><strong>Question: Is there a method of paying or incentivizing the contractor based on API usage?</strong></em><br />
I think I hit on this with the above answer(s). Keep payments small, and well defined. Measured, reported upon, and priced using the cloud model, connecting to a clear set of API governance guidance and expectations. The following areas can support paying and incentivizing contractors based upon not just usage, but also meeting the API contract.</p>

<ul>
  <li><strong>Management</strong> - API management puts all microservices into plans, then log, meter, and track on value exchanged at this level.</li>
  <li><strong>Marketplace</strong> - Turning the platform into a marketplace that can be occupied by a variety of internal, pattern, vendor, 3rd party, and public actors.</li>
  <li><strong>Monetization</strong> - Granular understanding of all the resources it takes to deliver each individual service, and understand the costs associated with operating at scale.</li>
  <li><strong>Plans</strong> - A wealth of API plans in place at the API gateway level, something that is tied to IAM policies, and in alignment with API governance expectations.</li>
  <li><strong>Governance</strong> - Providing a map, and supporting guidance around the Lighthouse platform API governance. Understanding, measuring, and enforcing consistency across the API lifecycle–platform wide.</li>
  <li><strong>Value Exchange</strong> - Using the cloud model, which is essentially the original API management, marketplace, and economy model. Not just measuring consumption, but used to maximize and generate revenue from the value exchanged across the platform.</li>
</ul>

<p>When you operate APIs on AWS and Azure, the platform as a service layer can utilize and benefit from the underlying infrastructure as a service monetization framework. Meaning, you can use AWS’s business model for managing the measuring, paying, and incentivizing of microservice owners. All the gears are there, they just need to be set in motion to support the management of a government API marketplace platform.</p>

<p><em><strong>Based on the information provided, please discuss your possible technology stack and detail your experience supporting these technologies.</strong></em><br />
Both Amazon Web Services and Azure provide the building blocks of what you need to execute the above. Each cloud platform has its own approach to delivering infrastructure at scale. Providing an interesting mix of API driven resources you can jumpstart any project.</p>
<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/27_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><strong>AWS</strong>
First, let’s take a look at what is relevant to this vision from the Amazon Web Services side of things. These are all the core AWS solutions on the table, with dashboard, API, and command line access to get the job done.</p>

<p><strong>Compute</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/ec2/?hp=tile&amp;so-exp=below">Amazon EC2</a> - Virtual Servers in the Cloud</li>
  <li><a href="https://aws.amazon.com/ec2/autoscaling/?hp=tile&amp;so-exp=below">Amazon EC2 Auto Scaling</a> - Scale Compute Capacity to Meet Demand</li>
  <li><a href="https://aws.amazon.com/ecs/?hp=tile&amp;so-exp=below">Amazon Elastic Container Service</a> - Run and Manage Docker Containers</li>
  <li><a href="https://aws.amazon.com/eks/?hp=tile&amp;so-exp=below">Amazon Elastic Container Service for Kubernetes</a> - Run Managed Kubernetes on AWS</li>
  <li><a href="https://aws.amazon.com/ecr/?hp=tile&amp;so-exp=below">Amazon Elastic Container Registry</a> - Store and Retrieve Docker Images</li>
  <li><a href="https://aws.amazon.com/lightsail/?hp=tile&amp;so-exp=below">Amazon Lightsail</a> - Launch and Manage Virtual Private Servers</li>
  <li><a href="https://aws.amazon.com/fargate/?hp=tile&amp;so-exp=below">AWS Fargate</a> - Run Containers without Managing Servers or Clusters</li>
  <li><a href="https://aws.amazon.com/batch/?hp=tile&amp;so-exp=below">AWS Batch</a> - Run Batch Jobs at Any Scale</li>
  <li><a href="https://aws.amazon.com/lambda/?hp=tile&amp;so-exp=below">AWS Lambda</a> - Run your Code in Response to Events</li>
  <li><a href="https://aws.amazon.com/serverlessrepo/?hp=tile&amp;so-exp=below">AWS Serverless Application Repository</a> - Discover, Deploy, and Publish Serverless Applications Auto Scaling</li>
</ul>

<p><strong>Storage</strong>
<a href="https://aws.amazon.com/s3/?hp=tile&amp;so-exp=below">Amazon S3</a> - Scalable Storage in the Cloud
<a href="https://aws.amazon.com/ebs/?hp=tile&amp;so-exp=below">Amazon EBS</a> - Block Storage for EC2
<a href="https://aws.amazon.com/efs/?hp=tile&amp;so-exp=below">Amazon Elastic File System</a> - Managed File Storage for EC2
<a href="https://aws.amazon.com/glacier1/?hp=tile&amp;so-exp=below">Amazon Glacier</a> - Low-cost Archive Storage in the Cloud
<a href="https://aws.amazon.com/storagegateway/?hp=tile&amp;so-exp=below">AWS Storage Gateway</a> - Hybrid Storage Integration</p>

<p><strong>Database</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/rds/aurora/?hp=tile&amp;so-exp=below">Amazon Aurora</a> - High Performance Managed Relational Database</li>
  <li><a href="https://aws.amazon.com/rds/?hp=tile&amp;so-exp=below">Amazon RDS</a> - Managed Relational Database Service for MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB</li>
  <li><a href="https://aws.amazon.com/dynamodb/?hp=tile&amp;so-exp=below">Amazon DynamoDB</a> - Managed NoSQL Database</li>
  <li><a href="https://aws.amazon.com/elasticache/?hp=tile&amp;so-exp=below">Amazon ElastiCache</a> - In-memory Caching System</li>
  <li><a href="https://aws.amazon.com/redshift/?hp=tile&amp;so-exp=below">Amazon Redshift</a> - Fast, Simple, Cost-effective Data Warehousing</li>
</ul>

<p><strong>Authentication</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/iam/?hp=tile&amp;so-exp=below">AWS Identity &amp; Access Management</a> - Manage User Access and Encryption Keys</li>
  <li><a href="https://aws.amazon.com/cognito/?hp=tile&amp;so-exp=below">Amazon Cognito</a> - Identity Management for your Apps</li>
  <li><a href="https://aws.amazon.com/single-sign-on/?hp=tile&amp;so-exp=below">AWS Single Sign-On</a> - Cloud Single Sign-On (SSO) Service</li>
  <li>a href=”https://aws.amazon.com/cloudhsm/?hp=tile&amp;so-exp=below”&gt;AWS CloudHSM&lt;/a&gt; - Hardware-based Key Storage for Regulatory Compliance</li>
</ul>

<p><strong>Management</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/api-gateway/?hp=tile&amp;so-exp=below">Amazon API Gateway</a> - Build, Deploy, and Manage APIs</li>
  <li><a href="https://aws.amazon.com/autoscaling/?hp=tile&amp;so-exp=below">AWS Auto Scaling</a> - Scale Multiple Resources to Meet Demand</li>
  <li><a href="https://aws.amazon.com/cloudformation/?hp=tile&amp;so-exp=below">AWS CloudFormation</a> - Create and Manage Resources with Templates</li>
</ul>

<p><strong>Logging</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/cloudwatch/?hp=tile&amp;so-exp=below">Amazon CloudWatch</a> - Monitor Resources and Applications</li>
  <li><a href="https://aws.amazon.com/cloudtrail/?hp=tile&amp;so-exp=below">AWS CloudTrail</a> - Track User Activity and API Usage</li>
</ul>

<p><strong>Network</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/vpc/?hp=tile&amp;so-exp=below">Amazon VPC</a> - Isolated Cloud Resources</li>
  <li><a href="https://aws.amazon.com/cloudfront/?hp=tile&amp;so-exp=below">Amazon CloudFront</a> - Global Content Delivery Network</li>
  <li><a href="https://aws.amazon.com/route53/?hp=tile&amp;so-exp=below">Amazon Route 53</a> - Scalable Domain Name System</li>
</ul>

<p><strong>Discovery</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/application-discovery/?hp=tile&amp;so-exp=below">AWS Application Discovery Service</a> - Discover On-Premises Applications to Streamline Migration</li>
  <li><a href="https://aws.amazon.com/servicecatalog/?hp=tile&amp;so-exp=below">AWS Service Catalog</a> - Create and Use Standardized Products</li>
</ul>

<p>Migration</p>
<ul>
  <li><a href="https://aws.amazon.com/dms/?hp=tile&amp;so-exp=below">AWS Database Migration Service</a> - Migrate Databases with Minimal Downtime</li>
  <li><a href="https://aws.amazon.com/server-migration-service/?hp=tile&amp;so-exp=below">AWS Server Migration Service</a> - Migrate On-Premises Servers to AWS</li>
</ul>

<p><strong>Orchestration</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/codedeploy/?hp=tile&amp;so-exp=below">AWS CodeDeploy</a> - Automate Code Deployment</li>
  <li><a href="https://aws.amazon.com/codepipeline/?hp=tile&amp;so-exp=below">AWS CodePipeline</a> - Release Software using Continuous Delivery</li>
  <li><a href="https://aws.amazon.com/config/?hp=tile&amp;so-exp=below">AWS Config</a> - Track Resource Inventory and Changes</li>
  <li><a href="https://aws.amazon.com/systems-manager/?hp=tile&amp;so-exp=below">AWS Systems Manager</a> - Gain Operational Insights and Take Action</li>
</ul>

<p><strong>Monitoring</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/trustedadvisor/?hp=tile&amp;so-exp=below">AWS Trusted Advisor</a> - Optimize Performance and Security</li>
  <li><a href="https://aws.amazon.com/premiumsupport/phd/?hp=tile&amp;so-exp=below">AWS Personal Health Dashboard</a> - Personalized View of AWS Service Health</li>
</ul>

<p><strong>Security</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/guardduty/?hp=tile&amp;so-exp=below">Amazon GuardDuty</a> - Managed Threat Detection Service</li>
  <li><a href="https://aws.amazon.com/certificate-manager/?hp=tile&amp;so-exp=below">AWS Certificate Manager</a> - Provision, Manage, and Deploy SSL/TLS Certificates</li>
  <li><a href="https://aws.amazon.com/shield/?hp=tile&amp;so-exp=below">AWS Shield</a> - DDoS Protection</li>
  <li><a href="https://aws.amazon.com/waf/?hp=tile&amp;so-exp=below">AWS WAF</a> - Filter Malicious Web Traffic</li>
</ul>

<p><strong>Analytics</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/athena/?hp=tile&amp;so-exp=below">Amazon Athena</a> - Query Data in S3 using SQL</li>
  <li><a href="https://aws.amazon.com/cloudsearch/?hp=tile&amp;so-exp=below">Amazon CloudSearch</a> - Managed Search Service</li>
  <li><a href="https://aws.amazon.com/redshift/?hp=tile&amp;so-exp=below">Amazon Redshift</a> - Fast, Simple, Cost-effective Data Warehousing</li>
</ul>

<p><strong>Integration</strong></p>
<ul>
  <li><a href="https://aws.amazon.com/step-functions/?hp=tile&amp;so-exp=below">AWS Step Functions</a> - Coordinate Distributed Applications</li>
  <li><a href="https://aws.amazon.com/sqs/?hp=tile&amp;so-exp=below">Amazon Simple Queue Service (SQS)</a> - Managed Message Queues</li>
  <li><a href="https://aws.amazon.com/sns/?hp=tile&amp;so-exp=below">Amazon Simple Notification Service (SNS)</a> - Pub/Sub, Mobile Push and SMS</li>
  <li><a href="https://aws.amazon.com/amazon-mq/?hp=tile&amp;so-exp=below">Amazon MQ</a> - Managed Message Broker for ActiveMQ</li>
</ul>

<p>I’m a big fan of the AWS approach. Their marketplace, and AWS API gateway provide unprecedented access to backend cloud, and on-premise resources, which can be secured using AWS IAM. Amazon Web Services products a robust infrastructure as a services, adequate enough to deliver any platform as a services solutions.</p>

<p><strong>Azure</strong></p>

<p>Next, let’s look at the Azure stack to see what they bring to the table. There is definitely some overlap with the AWS list of resources, but Microsoft has a different view of the landscape than Amazon does. However, similar to Amazon, most of the building blocks are here to deliver on the proposal above.</p>

<p><strong>Compute</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/virtual-machines/">Virtual Machines</a> - Provision Windows and Linux virtual machines in seconds</li>
  <li><a href="https://azure.microsoft.com/en-us/services/app-service/">App Service</a> - Quickly create powerful cloud apps for web and mobile</li>
  <li><a href="https://azure.microsoft.com/en-us/services/functions/">Functions</a> - Process events with serverless code</li>
  <li><a href="https://azure.microsoft.com/en-us/services/batch/">Batch</a> - Cloud-scale job scheduling and compute management</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-instances/">Container Instances</a> - Easily run containers with a single command</li>
  <li><a href="https://azure.microsoft.com/en-us/services/service-fabric/">Service Fabric</a> - Develop microservices and orchestrate containers on Windows or Linux</li>
  <li>a href=”https://azure.microsoft.com/en-us/services/virtual-machine-scale-sets/”&gt;Virtual Machine Scale Sets&lt;/a&gt; - Manage and scale up to thousands of Linux and Windows virtual machines</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-service/">Azure Container Service (AKS)</a> - Simplify the deployment, management, and operations of Kubernetes</li>
  <li><a href="https://azure.microsoft.com/en-us/services/cloud-services/">Cloud Services</a> - Create highly-available, infinitely-scalable cloud applications and APIs</li>
  <li><a href="https://azure.microsoft.com/en-us/services/virtual-machines/linux-and-open/">Linux Virtual Machines</a> - Provision virtual machines for Ubuntu, Red Hat, and more</li>
  <li><a href="https://azure.microsoft.com/en-us/services/">Windows Virtual Machines</a> - Provision virtual machines for SQL Server, SharePoint, and more</li>
</ul>

<p><strong>Storage</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/">Storage</a> - Durable, highly available, and massively scalable cloud storage</li>
  <li><a href="https://azure.microsoft.com/en-us/services/backup/">Backup</a> - Simple and reliable server backup to the cloud</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storsimple/">StorSimple</a> - Lower costs with an enterprise hybrid cloud storage solution</li>
  <li><a href="https://azure.microsoft.com/en-us/services/site-recovery/">Site Recovery</a> - Orchestrate protection and recovery of private clouds</li>
  <li><a href="https://azure.microsoft.com/en-us/services/data-lake-store/">Data Lake Store</a> - Hyperscale repository for big data analytics workloads</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/blobs/">Blob Storage</a> - REST-based object storage for unstructured data</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/unmanaged-disks/">Disk Storage</a> - Persistent, secured disk options supporting virtual machines</li>
  <li><a href="https://azure.microsoft.com/en-us/services/managed-disks/">Managed Disks</a> - Persistent, secured disk storage for Azure virtual machines</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/queues/">Queue Storage</a> - Effectively scale apps according to traffic</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/files/">File Storage</a> - File shares that use the standard SMB 3.0 protocol</li>
</ul>
<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/27_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><strong>Deployment</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/app-service/api/">API Apps</a> - Easily build and consume Cloud APIs</li>
</ul>

<p><strong>Containers</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/app-service/">App Service</a> - Quickly create powerful cloud apps for web and mobile</li>
  <li><a href="https://azure.microsoft.com/en-us/services/batch/">Batch</a> - Cloud-scale job scheduling and compute management</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-registry/">Container Registry</a> - Store and manage container images across all types of Azure deployments</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-instances/">Container Instances</a> - Easily run containers with a single command</li>
  <li><a href="https://azure.microsoft.com/en-us/services/service-fabric/">Service Fabric</a> - Develop microservices and orchestrate containers on Windows or Linux</li>
  <li><a href="https://azure.microsoft.com/en-us/services/container-service/">Azure Container Service (AKS)</a> - Simplify the deployment, management, and operations of Kubernetes</li>
</ul>

<p><strong>Databases</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-database/">SQL Database</a> - Managed relational SQL Database as a service</li>
  <li><a href="https://azure.microsoft.com/en-us/services/cosmos-db/">Azure Cosmos DB</a> - Globally distributed, multi-model database for any scale</li>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-data-warehouse/">SQL Data Warehouse</a> - Elastic data warehouse as a service with enterprise-class features</li>
  <li><a href="https://azure.microsoft.com/en-us/services/cache/">Redis Cache</a> - Power applications with high-throughput, low-latency data access</li>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-server-stretch-database/">SQL Server Stretch Database</a> - Dynamically stretch on-premises SQL Server databases to Azure</li>
  <li><a href="https://azure.microsoft.com/en-us/services/storage/tables/">Table Storage</a> - NoSQL key-value store using semi-structured datasets</li>
  <li><a href="https://azure.microsoft.com/en-us/services/postgresql/">Azure Database for PostgreSQL</a> - Managed PostgreSQL database service for app developers</li>
  <li><a href="https://azure.microsoft.com/en-us/services/mysql/">Azure Database for MySQL</a> - Managed MySQL database service for app developers</li>
  <li><a href="https://azure.microsoft.com/en-us/services/database-migration/">Azure Database Migration Service</a> - Simplify on-premises database migration to the cloud</li>
</ul>

<p><strong>Authentication</strong>
<a href="https://azure.microsoft.com/en-us/services/active-directory/">Azure Active Directory</a> - Synchronize on-premises directories and enable single sign-on
<a href="https://azure.microsoft.com/en-us/services/multi-factor-authentication/">Multi-Factor Authentication</a> - Add security for your data and apps without adding hassles for users
<a href="https://azure.microsoft.com/en-us/services/key-vault/">Key Vault</a> - Safeguard and maintain control of keys and other secrets
<a href="https://azure.microsoft.com/en-us/services/active-directory-b2c/">Azure Active Directory B2C</a> - Consumer identity and access management in the cloud</p>

<p><strong>Management</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/api-management/">API Management</a> - Publish APIs to developers, partners, and employees securely and at scale</li>
</ul>

<p><strong>Logging</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/log-analytics/">Log Analytics</a> - Collect, search, and visualize machine data from on-premises and cloud</li>
  <li><a href="https://azure.microsoft.com/en-us/services/traffic-manager/">Traffic Manager</a> - Route incoming traffic for high performance and availability</li>
</ul>

<p><strong>Monitoring</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/monitor/">Azure Monitor</a> - Highly granular and real-time monitoring data for any Azure resource</li>
  <li><a href="https://azure.microsoft.com/en-us/features/azure-portal/">Microsoft Azure portal</a> - Build, manage, and monitor all Azure products in a single, unified console</li>
</ul>

<p><strong>Analytics</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/hdinsight/">HDInsight</a> - Provision cloud Hadoop, Spark, R Server, HBase, and Storm clusters</li>
  <li><a href="https://azure.microsoft.com/en-us/services/hdinsight/apache-spark/">Apache Spark for Azure HDInsight</a> - Apache Spark in the cloud for mission critical deployments</li>
  <li><a href="https://azure.microsoft.com/en-us/services/hdinsight/apache-storm/">Apache Storm for HDInsight</a> - Real-time stream processing made easy for big data</li>
  <li><a href="https://azure.microsoft.com/en-us/services/sql-data-warehouse/">SQL Data Warehouse</a> - Elastic data warehouse as a service with enterprise-class features</li>
  <li><a href="https://azure.microsoft.com/en-us/services/log-analytics/">Log Analytics</a> - Collect, search, and visualize machine data from on-premises and cloud</li>
  <li><a href="https://azure.microsoft.com/en-us/services/data-lake-store/">Data Lake Store</a> - Hyperscale repository for big data analytics workloads</li>
  <li><a href="https://azure.microsoft.com/en-us/services/data-lake-analytics/">Data Lake Analytics</a> - Distributed analytics service that makes big data easy</li>
  <li><a href="https://azure.microsoft.com/en-us/services/analysis-services/">Azure Analysis Services</a> - Enterprise grade analytics engine as a service</li>
  <li><a href="https://azure.microsoft.com/en-us/services/databricks/">Azure Databricks</a> - Fast, easy, and collaborative Apache Spark-based analytics platform</li>
</ul>

<p><strong>Network</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/cdn/">Content Delivery Network</a> - Ensure secure, reliable content delivery with broad global reach</li>
  <li><a href="https://azure.microsoft.com/en-us/services/dns/">Azure DNS</a> - Host your DNS domain in Azure</li>
  <li><a href="https://azure.microsoft.com/en-us/services/virtual-network/">Virtual Network</a> - Provision private networks, optionally connect to on-premises datacenters</li>
  <li><a href="https://azure.microsoft.com/en-us/services/traffic-manager/">Traffic Manager</a> - Route incoming traffic for high performance and availability</li>
  <li><a href="https://azure.microsoft.com/en-us/services/load-balancer/">Load Balancer</a> - Deliver high availability and network performance to your applications</li>
  <li><a href="https://azure.microsoft.com/en-us/services/network-watcher/">Network Watcher</a> - Network performance monitoring and diagnostics solution</li>
</ul>

<p><strong>Orchestration</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/scheduler/">Scheduler</a> - Run your jobs on simple or complex recurring schedules</li>
  <li><a href="https://azure.microsoft.com/en-us/services/automation/">Automation</a> - Simplify cloud management with process automation</li>
  <li><a href="https://azure.microsoft.com/en-us/services/automation-control/">Automation &amp; Control</a> - Centrally manage all automation and configuration assets</li>
</ul>

<p><strong>Integration</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/data-factory/">Data Factory</a> - Orchestrate and manage data transformation and movement</li>
  <li><a href="https://azure.microsoft.com/en-us/services/logic-apps/">Logic Apps</a> - Automate the access and use of data across clouds without writing code</li>
  <li><a href="https://azure.microsoft.com/en-us/services/event-grid/">Event Grid</a> - Get reliable event delivery at massive scale</li>
</ul>

<p><strong>Search</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/search/">Azure Search</a> - Fully-managed search-as-a-service</li>
</ul>

<p><strong>Discovery</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/active-directory-ds/">Azure Active Directory Domain Services</a> - Join Azure virtual machines to a domain without domain controllers</li>
</ul>

<p><strong>Security</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/security-center/">Security Center</a> - Unify security management and enable advanced threat protection across hybrid cloud workloads</li>
  <li><a href="https://azure.microsoft.com/en-us/services/security-compliance/">Security &amp; Compliance</a> - Enable threat detection and prevention through advanced cloud security</li>
  <li><a href="https://azure.microsoft.com/en-us/services/ddos-protection/">Azure DDoS Protection</a> - Protect your applications from Distributed Denial of Service (DDoS) attacks</li>
</ul>

<p><strong>Governance</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/azure-policy/">Azure Policy</a> - Implement corporate governance and standards at scale for Azure resources</li>
</ul>

<p><strong>Monetization</strong></p>
<ul>
  <li><a href="https://azure.microsoft.com/en-us/services/cost-management/">Cost Management</a> - Optimize what you spend on the cloud, while maximizing cloud potential</li>
</ul>

<p><strong>Experience</strong><br />
I have been studying Amazon full time for almost eight years. I’ve been watching Azure play catch up for the last three years. I run my infrastructure, and a handful of clients on AWS. I understand the API landscape of both providers, and how they can be woven into vision proposed so far.</p>

<p>I see the AWS API stack, and the Azure API stack, as a starter set of services that can be built upon to deliver the base Lighthouse implementation. All the components are there. It just need the first set of Lighthouse services to be defined, delivering the essential building blocks  any platform needs, things like compute, storage, dns, messaging, etc. I recommend that the VA Lighthouse team take the AWS API stack, and mold it into v1 of the Lighthouse API stack. Take the momentum from AWS’s own API journey, build upon it, and set into motion the VA Lighthouse API journey.</p>

<p>Enable VA services to be delivered as individual, self-contained units, that can be used as part of a larger VA orchestration of veteran services. Open up the VA and let some sunlight in. Think about what Amazon has been able to achieve by delivering its own internal operations as services, and remaking not just retail, but also touching almost every other industry with Amazon Web Services. <a href="https://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/">The Amazon Web Services myth story</a> provides a powerful and compelling narrative for any company, organizations, institution, or government agency like the VA to emulate.</p>

<p>This proposal is not meant to be a utopian vision for the VA. However it is meant to, as the name of the project reflects, shine a light on existing ways of delivering services via the cloud. Helping guide each service in its own individual journey, while also serving the overall mission of the platform–to help the veteran be successful in their own personal journey.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/pano-lighthouse_copper_circuit.jpg" width="100%" align="center" /></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/22/aws-iam-like-policies-for-aws-api-gateway-and-marketplace-billing/">AWS IAM-Like Policies For AWS API Gateway And Marketplace Billing</a></h3>
        <span class="post-date">22 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/server-cloud1_internet_numbers.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>The primary reason I’ve been adopting more AWS solutions as part of my API stack, and using tools I have historically felt lock me into the AWS ecosystem, is the available of AWS identity and access management (IAM). I just cannot deliver secure at this level as a small business owner, and their robust solution lets me dial in exactly what I need when it comes to defining who has access to what across my API infrastructure. I can define different policies, and apply them at the API management layer using both AWS Lambda and AWS API Gateway. Keeping everything separated, yet with a single API stack as the point of entry, for all consumers and applications.</p>

<p>I want all of this security goodness, but for the business of my APIs. Similar to the engine that drives the relationship between me as an AWS Marketplace user and AWS, I want a framework for applying business policies at the plan level within AWS API Gateway. I want to determine who has access to which resources, as well as what they can use, but I want to be able to meter this usage, and charge different rates. Compute, storage, and bandwidth for my partners is different than for retail API consumers, with a mix of resource and API call based metrics.</p>

<p><a href="https://plans.apievangelist.com/2017/10/23/api-monetization-framework-introduced-by-aws-marketplace/">The AWS monetization policies would reflect the AWS Marketplace framework</a>, giving me a mix of metering and contract based billing, reflecting single or multi-dimensional usage across the eight areas of consumption they support currently. I want to be able to establish common monetization policies across all my microservices, and allow product managers to implement them consistently at scale using AWS API Gateway. Like security, these API product managers shouldn’t be experts in the economics of the services being offered, they should just be able to apply from a common pool of business policies, and provide feedback on how to evolve, when appropriate.</p>

<p>This concept is very much in the realm of traditional API management service composition, but would possess a machine readable policy format just like IAM policies. API monetization policies could be reported upon, providing breakdown of consumption of resources at the backend system, or front-end API path level, helping translate the monetization side of our API strategy, into actual API plans that can be executed at run-time. Providing a standardized, scalable, quantifiable way to measure the value exchange that occurs at the API level. Done in a way that could be applied internally, or external with partners, and 3rd party developers. Making the business of my APIs more consistent, modular, and reusable–just like the rest of my API infrastructure.</p>

<p>I think AWS has a significant advantage in this area. They have the advanced resource management infrastructure, as well as the business side of all of this from managing their own APIs, but also from slowly rolling it out as part of the AWS Marketplace. AWS API Gateway has the plan, and marketplace key, providing the beginning of the implementation. All we need is the standardized policies based upon their existing pricing framework, and the ability to measure and report upon at the AWS API Gateway plan level. The working parts are there, it just needs to be brought together. It might also be something someone could piece together from logging, and other existing outputs on the AWS platform, and create an external reporting and billing solution. IDK. Just brainstorming, what I’d like to see, and getting it here on the blog before the thought passes.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/22/aws-iam-like-policies-for-aws-api-gateway-and-marketplace-billing/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/">Provisioning A Default App And Keys For Your API Consumers On Signup</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/sabre/sabre-travel-signup-keys.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I sign up for a lot of APIs. I love anything that reduces friction when on-boarding, and allows me to begin making an API call in 1-3 clicks. I’m a big fan of API providers that allow me to signup using my Github OAuth, preventing me from having to sign up for yet another account. I’m also a big fan of providers who automatically provision an application for me as part of the signup, and have my API keys waiting for me as soon as I’ve registered.</p>

<p><a href="https://developer.sabre.com/member/register">While signing up for the Sabre travel API I saw that they provisioned my application as part of the API sign up process in a way that was worth showcasing</a>. Saving me the time and hassle of having to add a new application after I’ve signed up. Stuff like this might seem like a pretty small detail when developing an API on-boarding process, but when you are signing up for many different APIs, and trying to manage your time–these little details add up to be a significant time saver.</p>

<p>Ideally, API providers would auto-provision a default application along with the signup, but I like the idea of also giving me the option to name my application while registering. When crafting your API registration flow, make sure you spend time signing up multiple times, and try to put yourself in your API consumers shoes. I even recommend signing up for an account each week, repeatedly experiencing what your consumers will be exposed to. I also recommend spending time signing up for other APIs on a regular basis, to experience what they offer–you will always surprised by what I find.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/provisioning-a-default-app-and-keys-for-your-api-consumers-on-signup/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/open-banking-in-the-uk-openapi-template/">An Open Banking in the UK OpenAPI Template</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p>After learning more about <a href="https://www.openbanking.org.uk/">what Open Banking is doing for APIs in the UK</a>, I realized that I needed an OpenAPI template for the industry specification. There are six distinct schema available as part of the project, and I wanted a complete OpenAPI to describe which paths were available, as well as the underlying response schema. I got work crafting one from the responses that were available within <a href="https://www.openbanking.org.uk/open-data-apis/">the Open Banking documentation</a>.</p>

<p>Open Banking had schema available for their API definitions, but OpenAPI is the leading API and data specification out there today, so it makes sense that there should be an OpenAPI available, helping all participating banking API providers take advantage of all the tooling available within the OpenAPI community. To help support, I have published my Open Banking OpenAPI definition as a Github Gist:</p>

<script src="https://gist.github.com/kinlane/57c720c18e4d0ad370ad92c0ab9613f7.js"></script>

<p>I’ve applied this OpenAPI definition to the 17 banks they have listed, and will be including them in the next publishing of <a href="http://theapistack.com">my API Stack project</a>. Open Banking provides a common definition that can be used across many banks, and an OpenAPI template allows me to quickly apply the common template to each individual bank. Generating bank specific documentation, SDK and code samples, monitoring, tests, and other client tooling. Helping me put the valuable data being made available via each API to work.</p>

<p>I’d like to see more organizations like Open Banking emerge. I’d also like to help ensure they all make OpenAPI templates available for any API and schema specifications they establish. The API lifecycle is increasingly OpenAPI defined, and when you make your guidance available in the OpenAPI format, you are enabling actors within any industry to quickly get up and running with designing, deploying, managing, testing, monitoring, and almost every other stop along a modern API lifecycle. Increasing the chances of adoption for any API standards you are putting out there.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/open-banking-in-the-uk-openapi-template/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/">An Opportunity Around Providing A Common OpenAPI Enum Catalog</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/enums/bitcoin-pools.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m down in the details of the OpenAPI specification lately, working my way through hundreds of OpenAPI definitions, trying to once again make sense of the API landscape at scale. I’m working to prepare as many API path definitions as I possibly can to be runnable within one or two clicks. OpenAPI definitions, and Postman Collections are essential to making this happen, both of which require complete details on the request surface area for an API. I need to know everything about the path, as well as any headers, path, or query parameters that need to included. A significant aspect of this definition being complete includes default, and enum values being present.</p>

<p>If I can’t quickly choose from a list of values, or run with a default value, when executing an API, the time to seeing a live response grows significantly. If I have to travel back to the HTML documentation, or worse, do some Googling before I can make an API call, I just went from seconds to potentially minutes or hours before I can see a real world API response. Additionally, if there are many potential values available for each API parameter, enums become critical building blocks to helping me understand all the dimensions of an API’s surface area. Something that should have been considered as part of the API’s design, but often just gets left as part of API documentation.</p>

<p>When playing with a Bitcoin API with the following path /blocks/{pool_name}, I need to the list of pools I can choose from. When looking to get a stock market quote from an API with the following path, /stock/{symbol}/quote, I need a list of all the ticker symbols. Having, or not having these enum values at documentation, and execute time, are essential. Many of these lists of values are so common, developers take them for granted. Assuming that API consumers just have them laying around, and really aren’t worth including in documentation. You’d think we all have lists of states, countries, stock tickers, Bitcoin pools, and other data just laying around, but even as the API Evangelist, I often find myself coming up short.</p>

<p>All of this demonstrates a pretty significant opportunity for someone to create a Github hosted, searchable, forkable list of common OpenAPI enum lists. Providing an easy place for API providers, and API consumers to discover simple, or complex lists of values that should be present in API documentation, and included as part of all OpenAPIs. I recommend just publishing each enum JSON or YAML list as a Github Gist, and then publishing as a catalog via a simple Github Pages website. If I don’t see something pop up in the next couple of months, I’ll probably begin publishing something myself. However, I need another API related project like I need a hole in the head, so I’m holding off in hopes another hero or champion steps up and owns the enum portion of the growing OpenAPI conversation.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/an-opportunity-around-providing-a-common-openapi-enum-catalog/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/">What Is Open Banking In The UK?</a></h3>
        <span class="post-date">21 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/open-banking/open-banking-screenshot.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am profiling banks in the UK as part of an effort move forward my <a href="http://theapistack.com">API Stack</a> work, and populate the <a href="https://streamdata.io/developers/api-gallery/">Streamdata.io API Gallery</a>. One significant advantage that banks in the UK have over other countries in the EU, and even in the US, is the help of <a href="https://www.openbanking.org.uk">Open Banking</a>. To help profile the organization, I’ll just borrow from their website to define who they are and what they do.</p>

<p><em>The Open Banking Implementation Entity was created by the UK’s Competition and Markets Authority to create software standards and industry guidelines that drive competition and innovation in UK retail banking.</em></p>

<p>In 2016, The Competition and Markets Authority (CMA) published a report on the UK’s retail banking market which stated that older, larger banks do not have to compete hard enough for customers’ business, and smaller and newer banks were finding it difficult to grow and access the UK banking market. To solve this problem, they proposed a number of remedies including Open Banking, which defines API standards that are intended to help level that playing field.</p>

<p>The role of Open Banking is to:</p>

<ul>
  <li>Design the specifications for the Application Programming Interfaces (APIs) that banks and building societies use to securely provide Open Banking</li>
  <li>Support regulated third party providers and banks and building societies to use the Open Banking standards</li>
  <li>Create security and messaging standards</li>
  <li>Manage the Open Banking Directory which allows regulated participants like banks, building societies and third party providers to enroll in Open Banking</li>
  <li>Produce guidelines for participants in the Open Banking ecosystem</li>
  <li>Set out the process for managing disputes and complaints</li>
</ul>

<p>This approach to standardizing API definitions is the type of leadership that is needed to move API conversation forward in ALL industries. I know in the US, many enjoy viewing regulations as always bad, but this type of organizational designation can go a long way towards moving an industry forward in a concerted fashion. Doing the hard work to establish a common API definition, and play a central role in helping ensure each actor within an industry is implementing the definition as expected.</p>

<p>I’d like to see more organizations emerge that reflect Open Banking’s mission, in a variety of industries. Many companies do not have the time, expertise, or desire to do the homework and understand what needs to occur on the API front. Speaking from experience, there is’t a lot of vendor-free funding to do this kind of work, and it is something that will require public sector investment. In my opinion, this doesn’t always have to be government led, but there should be industry neutral funding available to move forward the conversation in a way that benefits everyone involved, without a focus on any single product or service.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/20/relationship-between-openapi-path-summary-tags-and-asyncapi-topics/">Relationship Between OpenAPI Path, Summary, Tags and AysncAPI Topics</a></h3>
        <span class="post-date">20 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/23_160_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m working my way through several hundred OpenAPI definitions that I have forked from <a href="https://apis.guru/">APIs.guru</a>, <a href="https://any-api.com/">Any API</a>, and have automagically generated from API documentation scrape scripts I have developed over time. Anytime I evolve a new OpenAPI definition, I first make sure the summary, description, and tags are as meaningful as they possibly can. Sadly this work is also constrained by how much time I have to spend with each API, as well as how well designed their API is in the first place. I have a number of APIs that help me enrich this automatically, by mining the API path, applying regular expressions, but often times it takes a manual review to add tags, polish summaries, and make the OpenAPI details as meaningful as I possibly can, in regards to what an API does.</p>

<p>As I’m taking a break from this work, I’m studying up on <a href="https://www.asyncapi.com/">AsyncAPI</a>, trying to get my head around how I can be crafting API definitions for the message-based, event-driven, streaming APIs I’m profiling alongside my regular API research. One of the areas the AsyncAPI team is pushing forward is around the concept of a topic–_“to create a definition that suites most use cases and establish the foundation for community tooling and better interoperability between products when using AsyncAPI.”_ or to elaborate further, <em>“a topic is a string representing where an AsyncAPI can publish or subscribe. For the sake of comparison they are like URLs in a REST API.”</em> Now I’m thinking about the relationships between the API design elements I’m wrestling with in my API definitions, and how the path, summary, and tags reflect what Async is trying to articulate with their topics discussion.</p>

<p><strong>{organization}.{group}.{version}.{type}.{resources}.{event}</strong></p>

<ul>
  <li><strong>organization</strong> - the name of the organization or company.</li>
  <li><strong>group</strong> - the service, team or department in charge of managing the message..</li>
  <li><strong>version</strong> - the version of the message for the given service. This version number should remain the same unless changes in the messages are NOT backward compatible.</li>
  <li><strong>type</strong> - the type of the message, e.g., is it a command or an event?. This value should always be event unless you’re trying to explicitly execute a command in another service, i.e., when using RPC.</li>
  <li><strong>resources</strong> - resources and sub-resources, in a word (or words) describing the resource the message refers to. For instance, if you’re sending a message to notify a user has just signed up, the resource should be user. But, if you want to send a message to notify a user has just changed her full name, you could name it as user.full_name.</li>
  <li><strong>event</strong> - an event or command name, in case message type is event, this should be a verb in past tense describing what happened to the resource, and in case message type is command, this should be a verb in infinitive form describing what operation you want to perform.</li>
</ul>

<p><strong>Example(s):</strong></p>

<ul>
  <li>hitch.accounts.1.event.user.signedup</li>
  <li>hitch.email.1.command.user.welcome.send</li>
</ul>

<p>As I’m crafting OpenAPI definitions, and publishing them to Github, I’m using Jekyll to give me access to the large numbers of OpenAPI definitions I’ve published, and indexed using APIs.json, as Liquid objects. For each site. I can references APIs path using a dotted notation, such as site.twilio.send-sms-get. I haven’t polished my naming conventions, and simply taking the path, stripping out everything but the alpha, numeric characters for the file names, but it got me thinking about how I might want to get more structured in how I name the individual units of compute I’m publishing using OpenAPI, and often times as Postman Collections.</p>

<p>As I publish these API definitions to Github, as part of my API profiling for inclusion in the Streamdata.io API, I’m looking to establish a map of the surface area, that I can potentially turn into webhooks, streamings, and other approaches to real time message delivery. This is why I’m looking to understand AsyncAPI, to help quantify the result of this work. After I map out the surface area of the APIs, and quantify the topics at play, and obtain an API key, I need a way to then map out the real time streams of messages that will get passed around. To do this, I will need a way to turn each potential API response and its resulting request into a topic definition into a well defined, measurable input or output–AsyncAPI is going to help me do this.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/20/relationship-between-openapi-path-summary-tags-and-asyncapi-topics/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/">People Who Provide Enum For Their OpenAPI Definitions Are Good People</a></h3>
        <span class="post-date">20 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/64_116_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m processing a significant amount of OpenAPI definitions currently, as well as crafting a number of them from scraped API documentation. After you work with a lot of OpenAPI definitions, aiming to achieve a specific objective, you really get to know which aspects of the OpenAPI are the most meaningful, and helpful when they are complete. <a href="http://apievangelist.com/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi-definition/">I talked about the importance of summary, description, and tags last week</a>, and this week I’d like to highlight how helpful it is when the stewards of OpenAPI definitions include enum values for their parameters, and I think they are just good people. ;-)</p>

<p>Enums are simply just a list of potential values for each of the parameters you outline as part of your API definition. So if you have state as a parameter for use in the request of your API, you have a list of the 50 US states as the enum. If you the parameter is color, you have just the color black, because we all know it is the only color(all the colors). ;-) If you provide a parameter that will accept a standard set of inputs, you should consider providing an enum list to help your consumers understand the potential for that parameter. Outlining the dimensions of the parameter in a simple JSON or YAML array of every single possible value.</p>

<p>I can’t articulate how many times I have to go looking for a list of values. Sometimes it is present within the description for the OpenAPI, but often times I have to go back to the portal for the API, and follow a link to a page that lists out the values. That is, if an API provider decides to provide this information at all. The thoughtful ones do, the even more thoughtful ones put it in their OpenAPI definitions as enum values. Anytime I come across a list of enums that I can quickly build an array, select, and other common aspects of doing business with APIs, I’m a happy camper.</p>

<p>Which is why you find me writing up enums. Boring. Boring. Boring. However, it is something that makes me happy, potentially multiple times in a single day, and imagine that multiplied by the number of developers you have, or maybe “had”, depending on how frustrating it is to find the values that can be used in your API’s parameters. In my opinion, enums add rich dimensions to what your API does, and can be as important as the overall design of your API. Depending on how you’ve designed your API, you may have invested heavily in design, or may be leaning on your API parameters to do the heavy lifting of helping you–making them even more important when it comes to documenting them as part of your API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/20/people-who-provide-enum-for-their-openapi-definitions-are-good-people/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using-apis/">Insecurity Around Providing Algorithmic Transparency And Observability Using APIs</a></h3>
        <span class="post-date">16 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/68_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://streamdata.io/blog/benchmark-quantifying-api-performance/">I’m working on a ranking API for my partner Streamdata.io to help quantify the efficiencies they bring to the table when you proxy an existing JSON web API using their service</a>. I’m evolving an algorithm they have been using for a while, wrapping it in a new API, and applying it across the APIs I’m profiling as part of my <a href="http://theapistack.com">API Stack</a>, and the <a href="https://streamdata.io/developers/api-gallery/">Streamdata.io API Gallery</a> work. I can pass the ranking API any OpenAPI definition, and it will poll and stream the API for 24 hours, and return a set of scores regarding how real time the API is, and what the efficiency gains are when you use Streamdata.io as a proxy for the API.</p>

<p>As I do this work, I find myself thinking more deeply about the role that APIs can play in helping make algorithms more transparent, observable, and accountable. My API ranking algorithm is pretty crude, but honestly it isn’t much different than many other algorithms I’ve seen companies defend as intellectual property and their secret sauce. Streamdata.io is invested in the ranking algorithm and API being as transparent as possible, so that isn’t a problem here, but each step of the process allows me to think through how I can continue to evangelize other algorithm owners to use APIs, to make their algorithms more observable and accountable.</p>

<p>In my experience, most of the concerns around keeping algorithms secret stem from individual insecurities, and nothing actually technical, mathematical, or proprietary. The reasons for the insecurities are usually that the algorithm isn’t that mathematically sophisticated (I know mine isn’t), or maybe it is pretty flawed (I know mine is currently), and people just aren’t equipped to admit this (I know I am). I’ve worked for companies who venomously defend their algorithms and refuse to open them up, because in the end they know they aren’t defensible on many levels. The only value the algorithm possesses in these scenarios is secrecy, and the perception that there is magic going on behind the scenes. When in reality, it is a flawed, crude, simple algorithm that could actually be improved upon if it was opened up.</p>

<p>I’m not insecure about my lack of mathematical skills, or the limitations of my algorithm. I want people to point out its flaws, and improve upon my math. I want the limitations of the algorithm to be point out. I want API providers and consumers to use the algorithm via the API (when I publish) to validate, or challenge the algorithmic assumptions being put forth. I’m not in the business of selling smoke and mirrors, or voodoo algorithmics. I’m in the business of helping people understand how inefficient their API responses are, and how they can possibly improve upon them. I’m looking to develop my own understanding of how can make APIs more event-driven, real time, and responsive. I’m not insecure about providing transparency and observability around the algorithms I develop, using APIs–all algorithm developers should be as open and confident in their own work.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/16/insecurity-around-providing-algorithmic-transparency-and-observability-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/16/using-jekyll-and-openapi-to-evolve-api-documentation-and-storytelling/">Using Jekyll And OpenAPI To Evolve My API Documentation And Storytelling</a></h3>
        <span class="post-date">16 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/68_158_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m reworking my API Stack work as independent sets of <a href="https://jekyllrb.com/docs/collections/">Jekyll collections</a>. Historically I just dumped all <a href="http://apisjson.org/">APIs.json</a>, and OpenAPIs into the central data folder, and grouped them into folders by company name. Now I am breaking them out into tag based collections, using a similar structure. Further evolving how I document and tell stories using each API. I have been published a single OpenAPI for each platform, but now I’m publishing a separate OpenAPI for each API path–we will see where this goes, it might ultimately end up biting me in the ass. I’m doing this because I want to be able to talk about a single API path, and provide a definition that can be viewed, interpreted, and executed against, independent of the other paths–Jekyll+OpenAPI is helping me accomplish this.</p>

<p>With each API provider possessing its own APIs.json index, and each API path having its own OpenAPI definition, I’m able to mix up how I document and tell stories around these APIs. I can list them by API provider, or by individual API path. I can filter based upon tags, and provide execute-time links that reference each individual unit of API. I have separate JavaScript functions that can be referenced if the API path is GET, POST, or PUT. I can even inherit other relevant links like API sign up or terms of service as part of its documentation. I can reference all of this as part of larger documentation, or within blog posts, and other pages throughout the website–which will be refreshed whenever I update the OpenAPI definition.</p>

<p>If you aren’t familiar with how Jekyll works. It is a static content solution, that allows you do develop collections. You can put CSV, JSON, or YAML into these collections (folders), and they become objects you can reference using Liquid syntax. So if I put Twitter’s APIs.json, and OpenAPI into a folder within my social collection, I can reference as site.social.twitter which is the APIs.json for Twitter’s entire API operations, and I can reference individual APIs as site.social.twitter.search for the individual OpenAPI defining the Twitter search API path. This decouples API documentation for me, and allows me to not just document APIs, but tell stories with  API definitions, making my API portals much more interactive, and hopefully engaging.</p>

<p>I just got my API stack approach refreshed using this new format. Now I just need to go through all my APIs and rebuild the underlying Github repository. I have thousands of APIs that I track on, and I’m curious how this approach holds up at scale. While <a href="http://theapistack.com">API Stack</a> is a single repository, I can essentially publish any collection of APIs I desire to any of the hundreds of repositories that make up the API Evangelist network. Allowing me to seamless tell stories using the technical details of API operations, and the individual API resources they serve up. Further evolving how I tell stories around the APIs I’m tracking on. While my API documentation has always been interactive, I think this newer, more modular approach, reflects the value each unit of value an API brings to the table, rather than just looking to document all the APIs a provider possesses.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/16/using-jekyll-and-openapi-to-evolve-api-documentation-and-storytelling/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi-definition/">The Importance of the API Path Summary, Description, and Tags in an OpenAPI Definition</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/15_190_800_500_0_max_0_1_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am creating a lot of OpenAPI definitions right now. <a href="http://apis.how/streamdata.io">Streamdata.io</a> is investing in me pushing forward my <a href="http://theapistack.com">API Stack</a> work, where I profile API using OpenAPI, and index their operations using APIs.json. From the resulting indexes, we are building out the <a href="https://streamdata.io/developers/api-gallery/">Streamdata.io API Gallery</a>, which shows the possibilities of providing streaming APIs on top of existing web APIs available across the landscape. The OpenAPI definitions I’m creating aren’t 100% complete, but they are “good enough” for what we are needing to do with them, and are allowing me to catalog a variety of interesting APIs, and automate the proxying of them using Streamdata.io.</p>

<p>I’m finding the most important part of doing this work is making sure there is a rich summary, description, and set of tags for each API. While the actual path, parameters, and security definitions are crucial to programmatically executing the API, the summary, description, and tags are essential so that I can understand what the API does, and make it discoverable. As I list out different areas of my API Stack research, like <a href="http://market.data.apievangelist.com/">the financial market data APIs</a>, it is critical that I have a title, and description for each provider, but the summary, description, and tags are what provides the heart of the index for what is possible with each API.</p>

<p>When designing an API, as a developer, I tend to just fly through writing summary, descriptions, and tags for my APIs. I’m focused on the technical details, not this “fluff”. However, this represents one of the biggest disconnects in the API lifecycle, where the developer is so absorbed with the technical details, we forget, neglect, or just don’t are to articulate what we are doing to other humans. The summary, description, and tags are the outlines in the API contract we are providing. These details are much more than just the fluff for the API documentation. They actually describe the value being delivered, and allow this value to be communicated, and discovered throughout the life of an API–they are extremely important.</p>

<p>As I’m doing this work, I realize just how important these descriptions and tags are to the future of these APIs. Whenever it makes sense I’m translating these APIs into streaming APIs, and I’m taking the tags I’ve created and using them to define the events, topics, and messages that are being transacted via the API I’m profiling. I’m quantifying how real time these APIs are, and mapping out the meaningful events that are occurring. This represents the event-driven shift we are seeing emerge across the API landscape in 2018. However, I’m doing this on top of API providers who may not be aware of this shift in how the business of APIs is getting done, and are just working hard on their current request / response API strategy. These summaries, descriptions, and tags, represent how we are going to begin mapping out the future that is happening around them, and begin to craft a road map that they can use to understand how they can keep evolving, and remain competitive.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/the-importance-of-the-api-path-summary-description-and-tags-in-an-openapi-definition/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/">A Really Nice API Application Showcase Over At The Intrinio Market Data API</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p>I am profiling financial market data APIs currently, and as I’m doing my work profiling APIs, I’m always on the hunt for interesting elements of their API operations that I can showcase for my readers. While looking at the financial market data API from Intrinio, I found that <a href="https://intrinio.com/marketplace/apps">I really, really like their application showcase</a>, which providers a pretty attractive blueprint for how we can showcase what is being develop on top of our APIs.</p>

<p>The Intrinio application showcase is just clean looking, and has the bells and whistles you’d expect like categories, search, detail or list view, and detail pages providing you all the information you need about the application, and where you can find tutorials, code, and other relevant resources.</p>

<p align="center"><a href="https://intrinio.com/marketplace/apps"><img src="https://s3.amazonaws.com/kinlane-productions/intrinio/intrinio-app-showcase.png" /></a></p>

<p>Another thing I really like is it isn’t just about web and mobile applications. They have spreadsheet integrations, and help walk you through how to “apply” each type of integration. This is what the application in API means to me. It isn’t always just about finished web, mobile, and device applications. It is about applying the resources available via the programmatic interfaces to some problem you have in your world.</p>

<p>Anyways, the Intrinio application showcase is totally worth profiling as part of my research. It is a great blueprint for other API providers to follow when crafting their own application showcases. This post give me a single URL that I can share with folks, and reference throughout my stories, white papers, guides, and talks. I’d love to see this become the standard for how API providers showcase their applications, keeping things simple, clean, and bringing value to their consumers.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/a-really-nice-api-application-showcase-over-at-the-intrinio-market-data-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/how-big-or-small-is-an-api/">How Big Or Small Is An API?</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/31_156_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working to build out <a href="https://streamdata.io/developers/api-gallery/">the API Gallery for Streamdata.io</a>, profiling a wide variety of APIs for inclusion in the directory, adding to the wealth of APIs that could be streamed using the service. As I work to build the index, I’m faced with the timeless question regarding, what is an API? Not technically what an API does, but what is an API in the context of helping people discover the API they are looking for. Is Twitter an API, or is the Twitter search/tweets path an API? My answer to this question always distills down to a specific API path, or as some call it an API endpoint. Targeting a specific implementation, use case, or value generated by a single API provider.</p>

<p>Like most things in the API sector, words are used interchangeably, and depending on how much experience you have in the business, you will have much finer grained definitions about what something is, or isn’t. When I’m talking to the average business user, the Twitter API is the largest possible scope–the entire thing. In the context of API discovery, and helping someone find an API to stream or to solve a specific problem in their world, I’m going to resort to a very precise definition–in this case, it is the specific Twitter API path that will be needed. Depending on my audience, I will zoom out, or zoom in on what constitutes a unit of API. The only consistency I’m looking to deliver is regarding helping people understand, and find what they looking for–I’m not worried about always using the same scope in my definition of what an API is.</p>

<p><a href="https://streamdata.io/blog/robust-market-data-apis-alphavantage/">You can see an example of this in action with the Alpha Vantage market data API I’m currently profiling</a>, and adding to the gallery. Is Alpha Vantage is a single API, or 24 separate APIs? In the context of the Streamdata.io API Gallery, it will be 24 separate APIs. In the context of telling the story on the blog, there is a single Alpha Vantage API, with many paths available. I don’t want someone searching specifically for a currency API to have to wade through all 24 Alpha Vantage paths, I want them to find specifically the path for their currency API. When it comes to API storytelling, I am fine with widening the scope of my definition, but when it comes to API discovery I prefer to narrow the scope down to a more granular unit of value.</p>

<p>For me, it all comes down the definition of what an API is. It is all about applying a programmatic interface. If I’m applying in a story that targets a business user, I can speak in general terms. If I’m applying to solve a specific business problem, I’m going to need to get more precise. This precision can spin out of control if you are dealing with developers who tend to get dogmatic about programming languages, frameworks, platforms, and the other things that make their worlds go round. I’m not in the business of being “right”. I’m in the business of helping people understand, and solve the problems they have. Which gives me a wider license when it comes to defining how big or small an API can be. It is a good place to be.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/how-big-or-small-is-an-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/15/some-common-features-of-an-api-application-review-process/">Some Common Features Of An API Application Review Process</a></h3>
        <span class="post-date">15 Feb 2018</span>
        <p><a href="https://twitter.com/ktinboulder/status/961601920887607296"><img src="https://s3.amazonaws.com/kinlane-productions/kelly-taylor-app-approval-tweet.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="https://twitter.com/ktinboulder/status/961601920887607296">I received a tweet from my friend Kelly Taylor with USDS</a>, asking for any information regarding establishing an “approve access to production data” for developers. <a href="https://bluebutton.cms.gov/developers/">He is working on an OAuth + FHIR implementation for the Centers for Medicare and Medicaid Services (CMS) Blue Button API</a>. Establishing a standard approach for on-boarding developers into a production environment always makes sense, as you don’t want to give access to sensitive information without making sure the company, developer, and application has been thoroughly vetted.</p>

<p>As I do with my work, I wanted to think through some of the approaches I’ve come across in my research, and share some tips and best practices. <a href="https://bluebutton.cms.gov/developers/#production-api-access">The Blue Button API team has a section published regarding how to get your application approved</a>, but I wanted to see if I can expand on, while also helping share this information with other readers. This is a relevant use case that I see come up regularly in healthcare, financial, education, and other mainstream industries.</p>

<p><strong>Virtualization &amp; Sandbox</strong><br />
The application approval conversation usually begins with ALL new developers being required to work with a sandboxed set of APIs, only providing production API access to approved developers. This requires having a complete set of virtualized APIs, mimicking exactly what would be used in production, but in a much safer, protected environment. One of the most important aspects of this virtualized environment is that there also needs to be robust sets of virtualized data, providing as much parity regarding what developers will experience when they enter the production environment. The sandbox environment needs to be as robust and reliable as the production, which is a mistake I see made over and over from providers, where the sandbox isn’t reliable, or as functional, and developers never are able to reach production status in a consistent and reliable way.</p>

<p><strong>Doing a Background Check</strong><br />
Next, as reflected in the Blue Button teams approach, you should be profiling the company and organization, as well as the individual behind each application. <a href="http://apievangelist.com/2016/03/30/best-buy-will-not-issue-api-keys-to-free-email-accounts-and-wants-to-get-to-know-your-company/">You see company’s like Best Buy refusing any API signup that doesn’t have an official company domain that can be verified</a>. In addition to requiring developers provide a thorough amount of information about who they are, and who they work for, many API providers are using background and profiling services like <a href="https://clearbit.com/">Clearbit</a> to obtain more details about a user based upon their email, IP address, and company domain. Enabling different types of access to API resources depending on the level of scrutiny a developer is put under. I’ve seen this level of scrutiny go all the way up to requiring the scanning of drivers license, and providing corporate documents before production access is approved.</p>

<p><strong>Purpose of Application</strong><br />
One of the most common filtering approaches I’ve seen centers around asking developer about the purpose of their application. The more detail the better. As we’ve seen from companies like Twitter, the API provider holds a lot of power when it comes to deciding what types of applications will get built, and it is up to the developer to pitch the platform, and convince them that their application will serve the mission of the organization, as well as any stakeholders, and end-users who will be leveraging the application. This process can really be a great filter for making sure developers think through what they are building, requiring them to put forth a coherent proposal, otherwise they will not be able to get full access to resources. This part of the process should be conducted early on in the application submission process, reducing frustrations for developers if their application is denied.</p>

<p><strong>Syncing The Legal Department</strong><br />
Also reflected in the Blue Button team’s approach is the syncing of the legal aspects of operating an API platform, and it’s applications. Making sure the application’s terms of service, privacy, security, cookie, branding, and other policies are in alignment with the platform. One good way of doing this is offering a white label edition of the platforms legal documents for use by the each application. Doing the heavy legal work for the application developers, while also making sure they are in sync when it comes to the legal details. Providing legal develop kits (LDK) will grow in prominence in the future, just like providing software development kits (SDK), helping streamline the legalities of operating a safe and secure API platform, with a wealth of applications in its service.</p>

<p><strong>Live or Virtual Presentation</strong><br />
Beyond the initial pitch selling an API provider on the concept of an application, I’ve seen many providers require an in-person, or virtual demo of the working application before it can be added to a production environment, and included in the application gallery. It can be tough for platform providers to test drive each application, so making the application owners do the hard work of demonstrating what an application does, and walking through all of its features is pretty common. I’ve participated on several judging panels that operate quarterly application reviews, as well as part of specific events, hackathons, and application challenges. Making demos a regular part of the application lifecycle is easier to do when you have dedicated resources in place, with a process to govern how it will all work in recurring batches, or on a set schedule.</p>

<p><strong>Getting Into The Code</strong><br />
As part of the application review process many API providers require that you actually submit your code for review via Github. Providing details on ALL dependencies, and performing code, dependency, and security scans before an application can be approved. I’ve also see this go as far as requiring the use of specific SDKs, frameworks, or include proxies within the client layer, and requiring all HTTP calls be logged as part of production applications. This process can be extended to include all cloud and SaaS solutions involved, limiting where compute, storage, and other resources can be operated. Requiring all 3rd party APIs in use be approved, or already on a white list of API providers before they can be put to use. This is obviously the most costly part of the application review process, but depending on how high the bar is being set, it is one that many providers will decide to invest in, ensuring the quality of all applications that run in a production environment.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/apple-app-review.png" align="right" width="40%" style="padding: 15px;" /></p>

<p><strong>Regular Review &amp; Reporting</strong><br />
One important thing about the application review process is that it isn’t a one time process. Even once an application is accepted an added into the production environment, this process will need to be repeated for each version release of the application, along with the changes to the API. Of course the renewal process might be shorter than the initial approval workflow, but auditing and regular check-in should be common, and not forgotten. This touches on the client level SDK, and API management logging needs of the platform, and that regular reporting upon application usage and activity should be available in real time, as well as part of each application renewal. API operations is always about taking advantage the real time awareness introduced at the API consumption layer, and staying in tune with the healthy, and not so healthy patterns that emerge from logging everything an application is doing.</p>

<p><strong>Business Model</strong><br />
It is common to ask application developers about their business model. The absence of a business model almost always reflects the underlying exploitation and sale of data being access or generated as part of application’s operation. Asking developers how they will make money and sustain their operations, along with regular checkins to make sure it is truly in effect, is an easy to ensure that applications are protecting the interests of the platform, its partners, and the applications end-users.</p>

<p>There are many other approaches I’ve seen API providers require before accepting an application into production. However, I think we should also be working hard to keep the process simple, and meaningful. Of course, we want a high bar for quality, but as with everything in the API world, there will always be compromises in how we deliver on the ground. Depending on the industry you are operating the bar will be made higher, or possibly lowered a little to allow for more innovation. I’ve included a list of some of the application review process I found across my research–showing a wide range of approaches across API providers we are all familiar with. Hopefully that helps you think through the application review process a little more. It is something I’ll write about again in the future as I push forward my research, and distill down more of the common building blocks I’m seeing across the API landscape.</p>

<p><strong>Some Leading Application Review Processes</strong><br /></p>

<ul>
  <li><a href="https://www.instagram.com/developer/review/">Instagram</a></li>
  <li><a href="https://developer.concur.com/manage-apps/app-certification.html">SAP Concur</a></li>
  <li><a href="https://developer.paypal.com/docs/classic/lifecycle/goingLive/">Paypal</a></li>
  <li><a href="https://developers.google.com/adsense/host/review_main">Adsense</a></li>
  <li><a href="https://help.shopify.com/api/listing-in-the-app-store/app-requirements-and-success-criteria/app-review-checklist">Shopify</a></li>
  <li><a href="https://developer.riotgames.com/application-process.html">Riot Games</a></li>
  <li><a href="https://api.slack.com/security-review">Slack</a></li>
  <li><a href="https://www.docusign.com/blog/dsdev-docusign-go-live-process-now-automated/">Docusign</a></li>
  <li><a href="http://dev.splunk.com/view/app-cert/SP-CAAAE8P">Splunk</a></li>
  <li><a href="https://go.developer.ebay.com/compatible-application-check-and-checklist-going-live">Ebay</a></li>
  <li><a href="https://developer.apple.com/app-store/review/">Apple</a></li>
</ul>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/15/some-common-features-of-an-api-application-review-process/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/code-generating-openapi-still-prevailing-approach/">Code Generation Of OpenAPI (fka Swagger) Still The Prevailing Approach</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/gears-numbers-blue.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>Over 50% of the projects I consult on still generate OpenAPI (fka Swagger) from code, rather then the other way around. When I first begin working with any API development group as an advisor, strategist, or governance architect I always ask, “are you using OpenAPI?” Luckily the answer is almost always yes. The challenge is that most of the time they don’t understand the full scope of how to use OpenAPI, and are still opting for the more costly approach–writing code, then generating OpenAPI from annotations. It has been over five years since Jakub Nesetril(@jakubnesetril) of Apiary first decoupled this way of doing API design first, but clearly we still have a significant amount of work when it comes to API definition and design literacy amongst development groups.</p>

<p>When you study where API services and tooling are headed it is clear that API deployment, and the actual writing of code is getting pushed further down in the life cycle. Services like Stoplight.io, and Postman are focusing on enabling a design, mock, document, test, and iterate approach, with API definitions (OpenAPI, Postman, etc) at the core. The actual deployment of API, either using open source frameworks, API gateways, or other method, is coming into the picture more downstream. Progressive API teams are hammering out exactly the API they need without ever writing any code, making sure the API design is dialed in before the more expensive, and often permanent code gets written and sent to production.</p>

<p>You will see me hammering on this line of API design first messaging on API Evangelist over the next year. Many developers still see OpenAPI (fka Swagger) about generating API documentation, not as the central contract that is used across every stop along the API lifecycle. Most do not understand that you can mock instead of deploying, and even provide mock data, errors, and other scenarios, allowing you to prototype applications on top of API designs. It will take a lot of education, and awareness building to get API developers up to speed that this is all possible, and begin the long process of changing behavior on the ground. Teams just are used to this way of thinking, but once they understand what is possible, they’ll realize what they have been missing.</p>

<p>I need to come up with some good analogies for generating API definitions from code. It really is an inefficient, and a very costly way to get the job done. Another problem is that this approach tends to be programming language focused, which always leaves its mark on the API design. I’m going to be working with both Stoplight.io and Postman to help amplify this aspect of delivering APIs, and how their services and tooling helps streamline how we develop our APIs. I’m going to be working with banks, insurance, health care, and other companies to improve how they deliver APIs, shifting things towards a design-first way of doing business. You’ll hear the continued drumbeat around all of this on API Evangelist in coming months, as I try to get the attention of folks down in the trenches, and slowly shift the behavior towards a better way of getting things done.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/code-generating-openapi-still-prevailing-approach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/the-growing-importance-of-github-topics-for-your-api-seo/">The Growing Importance of Github Topics For Your API SEO</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-topics-screenshot.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>When you are operating an API, you are always looking for new ways to be discovered. I study this aspect of operating APIs from the flip-side–how do I find new APIs, and stay in tune with what APIs are to? Historically we find APIs using ProgrammableWeb, Google, and Twitter, but increasingly Github is where I find the newest, coolest APIs. I do a lot of searching via Github for API related topics, but increasingly Github topics themselves are becoming more valuable within search engine indexes, making them an easy way to uncover interesting APIs.</p>

<p><a href="https://streamdata.io/blog/robust-market-data-apis-alphavantage/">I was profiling the market data API Alpha Vantage today</a>, and one of the things I always do when I am profiling an API, is I conduct a Google, and then secondarily, a Github search for the APIs name. Interestingly, <a href="https://github.com/topics/alpha-vantage">I found a list of Github Topics while Googling for Alpha Vantage API</a>, uncovering some interesting SDKs, CLI, and other open source solutions that have been built on top of the financial data API. Showing the importance of operating your API on Github, but also working to define a set of standard Github Topic tags across all your projects, and helping encourage your API community to use the same set of tags, so that their projects will surface as well.</p>

<p>I consider Github to be the most important tool in an API providers toolbox these days. I know as an API analyst, it is where I learn the most about what is really going on. It is where I find the most meaningful signals that allow me to cut through the noise that exists on Google, Twitter, and other channels. Github isn’t just for code. As I mention regularly, 100% of my work as API Evangelist lives within hundreds of separate Github repositories. Sadly, I don’t spend as much time as I should tagging, and organizing projects into meaningful topic areas, but it is something I’m going to be investing in more. Conveniently, I’m doing a lot of profiling of APIs for my partner Streamdata.io, which involves establishing meaningful tags for use in defining real time data stream topics that consumers can subscribe to–making me think a little more about the role Github topics can play.</p>

<p>One of these days I will do a fresh roundup of the many ways in which Github can be used as part of API operations. I’m trying to curate and write stories about everything I come across while doing my work. The problem is there isn’t a single place I can send my readers to when it comes to applying this wealth of knowledge to their operations. The first step is probably to publish Github as its own research area on Github (mind blown), as I do with my other projects. It has definitely risen up in importance, and can stand on its own feet alongside the other areas of my work. Github plays a central role in almost every stop along the API life cycle, and deserves its own landing page when it comes to my API research, and priority when it comes to helping API providers understanding what they should be doing on the platform to help make their API operations more successful.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/the-growing-importance-of-github-topics-for-your-api-seo/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/summary-of-aws-api-gateway-as-an-api-deployment-and-management-solution/">A Summary Of AWS API Gateway As An API Deployment and Management Solution</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/aws/aws-api-gateway-icon.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was providing an overview of Kong, AWS API Gateway, and other solutions for a team I’m advising a couple weeks back. I was just looking to distill down some of the key features, and provide an overview to a large, distributed team. This work lends itself well to publishing here on the blog, so <a href="http://apievangelist.com/2018/02/13/summary-of-kong-as-an-api-management-solution/">I published an overview of Kong yesterday</a>, and today I wanted to publish the summary of the AWS API Gateway. The API gateway solution from AWS has some overlap with what Kong delivers, but I consider it to be more of an API deployment, as well as an API management gateway.</p>

<p>The <a href="https://aws.amazon.com/api-gateway/">AWS API Gateway brings API</a> deployment front and center, allowing you to define and deploy APIs that are wired up to your backend (AWS) infrastructure:</p>

<ul>
  <li><strong>API Endpoint</strong> - a host name of the API. the API endpoint can be edge-optimized or regional, depending on where the majority of your API traffic originates from. You choose a specific endpoint type when creating an API.</li>
  <li><strong>Backend Endpoint</strong> - A backend endpoint is also referred to as an integration endpoint and can be a Lambda function, an HTTP webpage, or an AWS service action, or a mock interface.</li>
  <li><strong>Swagger / OpenAPI</strong> - Using Swagger to import and export API configuration and definitions.</li>
</ul>

<p>Then the gateway brings a wealth of API management features, providing a look at how it has been baked into the cloud now:</p>

<ul>
  <li><strong>Accounts</strong> - Creation and management of Accounts.</li>
  <li><strong>Keys</strong> - Creation and management of API Keys</li>
  <li><strong>Certificates</strong> - Adding and management of certifications</li>
  <li><strong>Documentation</strong> - Publishing of ApI documentation</li>
  <li><strong>Domains</strong> - Mapping of domains</li>
  <li><strong>Response</strong> - Custom Gateway responses.</li>
  <li><strong>Models</strong> - Management of schema models.</li>
  <li><strong>Validation</strong> - Validation of API requests</li>
  <li><strong>SDK Generation</strong> - Generating of client SDKs</li>
  <li><strong>Staging</strong> - Establishing of stages</li>
  <li><strong>Tags</strong> - Tagging of resources</li>
  <li><strong>Templates</strong> - Mapping template used to transform a payload.</li>
  <li><strong>Plans</strong> - Establishing of different plans for API usage.</li>
  <li><strong>VPC</strong> - Usage of VP under the caller’s account in a region.</li>
  <li><strong>Regions</strong> - Deployment of gateways in different AWS regions.</li>
  <li><strong>Serverless</strong> - Usage of Lambda for serves integration.</li>
  <li><strong>Logging</strong> - Logging using Cloudwatch.</li>
  <li><strong>IAM</strong> - You can use AWS administration and security tools, such as AWS Identity and Access Management (IAM) and Amazon Cognito, to authorize access to your APIs.</li>
</ul>

<p>Then of course, everything with AWS has two separate programmatic interfaces for you to work with everything:</p>

<ul>
  <li><strong>API</strong> - Programmatic access through hypermedia API.</li>
  <li><strong>Command Line</strong> - The AWS CLI is an open source tool built on top of the AWS SDK for Python that provides commands for interacting with AWS services.</li>
</ul>

<p>AWS API Gateway doesn’t have some of the bells and whistles associated with other leading API management solutions, however it makes up for this with its API deployment capabilities–answering the age old question, which of the API management solutions will help me deploy my APIs. If you are operating your infrastructure within AWS, then AWS API Gateway makes a lot of sense. The connectivity it brings to the table is hard to ignore. What really sold it for me, is the IAM part of the equation. Before using AWS, I never had fine grained policies for what backend systems my APIs can or cannot access.</p>

<p>I avoided AWS API Gateway for a while. I was waiting for it to mature, and looking for enough benefit to get me beyond my vendor lock-in fears with API infrastructure. The IAM and Serverless aspects of delivering APIs are the features that pushed me to the point where I’m now using it for about 50% of my API infrastructure. It isn’t as portable, and versatile as solutions like Kong or Tyk are, but it does provide a solid set of API deployment and management features for me to put to work on projects that are already running in the AWS cloud.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/summary-of-aws-api-gateway-as-an-api-deployment-and-management-solution/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/14/your-microservices-effort-will-fail-because-you-will-never-decouple-your-business/">Your Microservices Effort Will Fail Because You Will Never Decouple Your Business</a></h3>
        <span class="post-date">14 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/containership_deep_connections.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m regularly surprised by companies who are doing microservices which are failing to see the need the change organizational culture, and that microservices will be some magic voodoo to fix all their legacy technical debt. That simply decoupling and breaking down the technology, without any re-evaluation of the business and politics behind, will fix everything, and set the company, organizations, institution, or government agency on a more positive trajectory. In coming years, we will continue to hear stories about why microservices do not work, from endless waves or groups who were unable to do the hard work to decouple, and reorganize the operations behind the services they provide.</p>

<p>The monolith legacy systems I’m seeing targeted are widely seen as purely technology, which is why it is often labeled as technical debt. What is missing from this targeting and labeling is any acknowledgement of the people and decisions behind the monolith. The years of business, political, and cultural investment into the monolith. How will we every unwind, or properly address the monolith, if we do not see the organizational, human, and business aspects of why it exists in the first place? Are we talking about the business decisions that went into creating and perpetuating the monolith? It is highly likely we will be making some of the same decisions with microservices, which could end up being worse than when we made them with a single system. Distributed mess, is often more painful than consolidated mess.</p>

<p>I’m seeing endless waves of large organizations mandating that their teams invest in microservices, with no mandating for microteams, microbudgets, microdecisionmaking, or any of the other decoupling needed to make microservices truly work independently. I attach micro as a joke. I really don’t feel micro is the constant that needs applying when it comes to services, or the business and organizational mechanism behind them. However, it is the word du jour, and one that gets at some of the illnesses our organizations are facing in 2018. In reality, it is more about decoupling and decomposing the technology, business, and politics of our operations, into meaningful units that can be deployed, operate, and deprecated independent of each other.</p>

<p>My point is that your microservices effort will fail if you aren’t addressing the business side of the equation. If your microservices team(s) still exist within your legacy organizational structure, you really haven’t decoupled or decomposed anything. The old way of making decisions, dealing with budget impacts, will still reflect what happened with the previous monolith. Your technology will be independently operating, but still beholden to the same ways of deciding and funding what actually happens on the ground. The result will resemble having a entirely new motor, where you are running without lubricant, or possibly old, thick, expired lubricant that prevents your new motor from ever delivering at full capacity, and eventually breaking down in ways you have never imagined while operating your existing monolith.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/14/your-microservices-effort-will-fail-because-you-will-never-decouple-your-business/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/13/summary-of-kong-as-an-api-management-solution/">A Summary Of Kong As An API Management Solution</a></h3>
        <span class="post-date">13 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/kong/get-kong-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was breaking down what the API management solution Kong delivers for a customer of mine, and I figured I’d take what I shared via the team portal, and publish here on the blog. It is an easy way for me to create content, and make my consulting work more transparent here on the blog. I am using Kong as part of several healthcare and financial projects currently, and I am actively employing it to ensure customers are properly managing their APIs. I wasn’t the decision maker on any of these projects when it came to choosing the API management layer, I am just the person who is helping standardize how they are using API services and tooling across the API life cycle for these projects.</p>

<p>First, <a href="https://konghq.com">Kong is an open source API management solution</a> with an easy to install <a href="https://konghq.com/install/">community edition</a>, and <a href="https://konghq.com/kong-enterprise-edition/">enterprise level support when needed</a>. They provide an <a href="https://konghq.com/api-admin-gui/">admin interface</a>, and <a href="https://konghq.com/api-dev-portal/">developer portal</a> for the API management proxy, but there is also a growing number of community editions like <a href="https://ajaysreedhar.github.io/kongdash/">KongDash</a>, and <a href="ttps://pantsel.github.io/konga/">Konga</a> emerging to make it a much more richer ecosystem. And of course, <a href="https://getkong.org/docs/0.12.x/admin-api/">Kong has an API for managing the API management layer</a>, as every API service and tooling provider should have.</p>

<p>Now, let’s talk about what Kong does for helping in the deploying of your APIs:</p>

<ul>
  <li><strong>API Routing</strong> - The API object describes an API that’s being exposed by Kong. Kong needs to know how to retrieve the API when a consumer is calling it from the Proxy port. Each API object must specify some combination of hosts, uris, and methods</li>
  <li><strong>Consumers</strong> - The Consumer object represents a consumer - or a user - of an API. You can either rely on Kong as the primary datastore, or you can map the consumer list with your database to keep consistency between Kong and your existing primary datastore.</li>
  <li><strong>Certificates</strong> - A certificate object represents a public certificate/private key pair for an SSL certificate.</li>
  <li><strong>Server Name Indication (SNI)</strong> - An SNI object represents a many-to-one mapping of hostnames to a certificate.</li>
</ul>

<p>Then it focuses on the core aspects of what is needed to help manage your APIs:</p>

<ul>
  <li><strong>Authentication</strong> - Protect your services with an authentication layer.</li>
  <li><strong>Traffic Control</strong> - Manage, throttle, and restrict inbound and outbound API traffic.</li>
  <li><strong>Analytics</strong> - Visualize, inspect, and monitor APIs and microservice traffic.</li>
  <li><strong>Transformations</strong> - Transform requests and responses on the fly.</li>
  <li><strong>Logging</strong> - Stream request and response data to logging solutions.</li>
</ul>

<p>After that, it has a bunch of added features to help make it a scalable, evolvable solution:</p>

<ul>
  <li><strong>DNS-based loadbalancing</strong> - When using DNS based load balancing the registration of the backend services is done outside of Kong, and Kong only receives updates from the DNS server.</li>
  <li><strong>Ring-balancer</strong> - When using the ring-balancer, the adding and removing of backend services will be handled by Kong, and no DNS updates will be necessary.</li>
  <li><strong>Clustering</strong> - A Kong cluster allows you to scale the system horizontally by adding more machines to handle more incoming requests. They will all share the same configuration since they point to the same database. Kong nodes pointing to the same datastore will be part of the same Kong cluster.</li>
  <li><strong>Plugins</strong> - lua-nginx-module enables Lua scripting capabilities in Nginx. Instead of compiling Nginx with this module, Kong is distributed along with OpenResty, which already includes lua-nginx-module. OpenResty is not a fork of Nginx, but a bundle of modules extending its capabilities.</li>
  <li><strong>API</strong> - Administrative API access for programmatic control.</li>
  <li><strong>CLI Reference</strong> - The provided CLI (Command Line Interface) allows you to start, stop, and manage your Kong instances. The CLI manages your local node (as in, on the current machine).</li>
  <li><strong>Serverless</strong> - Invoke serverless functions via APIs.</li>
</ul>

<p>There are a number of API management solutions available out there today. I will profile each one  I am actively using as part of my work on the ground. I’m agnostic towards which provider my clients should use, but I like having the details about what features they bring to the table readily available via a single URL, so that I can share when these conversations come up. I have many <a href="http://management.apievangelist.com">API management solutions profiled as part of my API management research</a>, but in 2018 there are just a handful of clear leaders in the game. I’ll be focusing on the ones who are still actively investing in the API community, and the ones I have an existing relationship with in a partnership capacity. Streamdata.io is a reseller of Kong in France, making it something I’m actively working with in the financial space, and also something I’m using within the federal government, also bringing it front and center for me in the United States.</p>

<p>If you have more questions about Kong, or any other API management solution, feel free to reach out, and I’ll do my best to answer any questions. We are also working to provide more API life cycle, strategy, and governance services along with <a href="http://skylight.digital">my government API partners at Skylight</a>, and through <a href="http://apis.how/streamdata">my mainstream API partners at Streamdata.io</a>. If you need help understanding the landscape and where API management solutions like Kong fits in, me and my partners are happy to help out–just let us know.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/13/summary-of-kong-as-an-api-management-solution/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/13/aggregating-multiple-github-account-rss-feeds-into-single-json-api-feed/">Aggregating Multiple Github Account RSS Feeds Into Single JSON API Feed</a></h3>
        <span class="post-date">13 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/octocat-aggregate.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>Github is the number one signal in my API world. The activity that occurs via Github is more important than anything I find across Twitter, Facebook, LinkedIn, and other social channels. Commits to repositories and the other social activity that occurs around coding projects is infinitely more valuable, and telling regarding what a company is up to, than the deliberate social media signals blasted out via other channels is. I’m always working to dial in my monitoring of Github using the Github API, but also via the RSS feeds that are present on the public side of the platform.</p>

<p>I feel RSS is often overlooked as an API data source, but I find that RSS is not only alive and well in 2018, it is something that is actively used on many platforms. The problem with RSS for me, is the XML isn’t always conducive to working with in many of my JavaScript enabled applications, and I also tend to want to aggregate, filter, and translate RSS feeds into more meaningful JSON. To help me accomplish this for Github, I crafted a simple PHP RSS aggregator and converter script which I can run in a variety of situations. I published the basic script to Github as a Gist, for easy reference.</p>

<script src="https://gist.github.com/kinlane/30461b54300f29da462db4f63fccd6f5.js"></script>

<p>The simple PHP script just takes an array of Github users, loops through them, pulls their RSS feeds, and then aggregates them into a single array, sorts by date, and then outputs as JSON. It is a pretty crude JSON API, but it provides me with what I need to be able to use these RSS feeds in a variety of other applications. I’m going to be mining the feeds for a variety of signals, including repo and user information, which I can then use within other applications. The best part is this type of data mining doesn’t require a Github API key, and is publicly available, allowing me to scale up much further than I could with the Github API alone.</p>

<p>Next, I have a couple of implementations in mind. I’m going to be creating a Github user leaderboard, where I stream the updates using Streamdata.io to a dashboard. Before I do that, I will have to aggregate users and repos, incrementing each commit made, and publishing as a separate JSON feed. I want to be able to see the raw updates, but also just the most changed repositories, and most active users across different segments of the API space. <a href="http://apis.how/streamdata">Streamdata.io allows me to take these JSON feeds and stream them to the dashboard using Server-Sent Events(SSE)</a>, and then applying each update using JSON Patch. Making for a pretty efficient way to put Github to work as part of my monitoring of activity across the API space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/13/aggregating-multiple-github-account-rss-feeds-into-single-json-api-feed/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/13/having-a-developer-yourdomain-is-clear-differentiator-in-the-api-game/">Having A Developer.[YourDomain] Is Clear Differentiator In The API Game</a></h3>
        <span class="post-date">13 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/developer-you-com.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am profiling US, UK, French, and German banks as part of some research I am doing for <a href="http://apis.how/streamdata">Streamdata.io</a>. I am profiling how far along in the API journey these banks are, and one clear differentiator for me is whether a bank has a developer.[bankdomain] subdomain setup for their APIs or not. The banks that have a dedicated subdomain for their API operations have a clear lead over those who do not. The domain doesn’t do much all by itself, but it is clear that when a bank can get this decision made, many of the other decisions that need to be made are also happening in tandem.</p>

<p>This isn’t unique just to banking. This is something I’ve written about several times over the years, and remains constant after looking at thousands of APIs over the last eight years. When a company’s API presence exists within the help section of their website, the API is almost always secondary to the core business. When a company relies on a 3rd party service for their API and developer presence, it almost always goes dormant after a couple months, showing that APIs are just not a priority within the company. Having a dedicated subdomain, landing page, and set of resources dedicated to doing APIs goes a long way towards ensuring an API program gains the momentum it needs to be successful within an organization, and industry.</p>

<p>I know that having a dedicated subdomain for API operations seems like a small thing to many folks. However, it is one of the top symptoms of a successful API in my experience. Making data, content, and algorithms available in a machine readable way for use in other applications by 3rd party via the web is something every company, organization, institution, and government agency should be doing in 2018. It is the next iteration of the web, and is not something that should be a side project. Having a dedicated subdomain demonstrates that you understand this, and an API won’t just be the latest trend at your organization. Even if your APIs are entirely private in the beginning, having a public portal for your employees, partners, and other stakeholders will go along way towards helping you get the traction you are looking for in the API game.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/13/having-a-developer-yourdomain-is-clear-differentiator-in-the-api-game/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/13/streaming-and-event-driven-architecture-represents-maturity-in-your-api-journey/">Streaming And Event-Driven Architecture Represents Maturity In The API Journey</a></h3>
        <span class="post-date">13 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/downtheline_dark_dali.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Working with Streamdata.io has forced a shift in how I see the API landscape. When I started working with their proxy I simply saw it about doing API in real time. I was hesitant because not every API had real time needs, so I viewed what they do as just a single tool in my API toolbox. While Server-Sent Events, and proxying JSON APIs is just one tool in my toolbox, like the rest of the tools in my toolbox it forces me to think through what an API does, and understand where it exists in the landscape, and where the API provider exists in their API journey. Something I’m hoping the API providers are also doing, but I enjoy doing from the outside-in as well.</p>

<p>Taking any data, content, media, or algorithm and exposing as an API, is a journey. It is about understanding what that resource is, what it does, and what it means to the provider and the consumer. What this looks like day one, will be different from what it looks like day 365 (hopefully). If done right, you are engaging with consumers, and evolving your definition of the resource, and what is possible when you apply it programmatically through the interfaces you provide. API providers who do this right, are leveraging feedback loops in place with consumers, iterating on their APIs, as well as the resources they provide access to, and improving upon them.</p>

<p>Just doing simple web APIs puts you on this journey. As you evolve along this road you will begin to also apply other tools. You might have the need for webhooks to start responding to meaningful events that are beginning to emerge across the API landscape, and start doing the work of defining your event-driven architecture, developing lists of most meaningful topics, and events that are occurring across your evolving API platform. Webhooks provide direct value by pushing data and content to your API consumers, but they have indirect value in helping you define the event structure across your very request and response driven resource landscape. Look at <a href="https://developer.github.com/v3/activity/events/types/">Github webhook events</a>, or <a href="https://api.slack.com/events/api">Slack webhook events</a> to understand what I mean.</p>

<p>API platforms that have had webhooks in operation for some time have matured significantly towards and event-driven architecture. Streaming APIs isn’t simply a boolean thing. That you have data that needs to be streamed, or you don’t. That is the easy, lazy way of thinking about things. Server-Sent Events (SSE) isn’t just something you need, or you don’t. It is something that you are ready for, or you aren’t. Like webhooks, I’m seeing Server-Sent Events (SSE) as having the direct benefits of delivering data and content as it is updated, to the browser or for other server uses. However, I’m beginning to see the other indirect benefits of SSE, and how it helps define the real time nature of a platform–what is real time? It also helps you think through the size, scope, and efficiency surrounding the use of APIs for making data, content, and algorithms available via the web. Helping us think through how and when we are delivering the bits and bytes we need to get business done.</p>

<p>I’m learning a lot by applying <a href="http://apis.how/streamdata">Streamdata.io</a> to simple JSON APIs. It is adding another dimension to the API design, deployment, and management process for me. There has always been an evolutionary aspect of doing APIs for me. This is why you hear me call it the API journey on a regular basis. However, now that I’m studying event-driven architecture, and thinking about how tools like webhooks and SSE assist us in this journey, I’m seeing an entirely new maturity layer for this API journey emerge. It goes beyond just getting to know our resources as part of the API design, and deployment process. It builds upon API management and monitoring and helps us think through how our APIs are being consumed, and what the most meaningful and valuable events are. Helping us think through how we deliver data and content over the web in a more precise manner. It is something that not every API provider will understand right away, and only those a little further along in their journey will be able to take advantage of. The question is, how do we help others see the benefits, and want to do the hard work to get further along in their own API journey.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/13/streaming-and-event-driven-architecture-represents-maturity-in-your-api-journey/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/12/more-outputs-are-better-when-it-comes-to-establishing-an-api-observability-ranking/">More Outputs Are Better When It Comes To Establishing An API Observability Ranking</a></h3>
        <span class="post-date">12 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/ellisisland_blue_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been evolving an observability ranking for the APIs I track on for a couple years now. I’ve bene using the phrase to describe my API profiling and measurement approach <a href="http://apievangelist.com/2016/10/25/thinking-about-an-api-observability-stack/">since I first learned about the concept from Stripe</a>. There are many perspectives floating around the space about what observability means in the context of technology, however mine is focused completely on APIs, and is more about communicating with external stakeholders, more than it is just about monitoring of systems. To recap, <a href="https://en.wikipedia.org/wiki/Observability">the Wikipedia definition for observability is</a>:</p>

<blockquote>
  <p><em>Formally, a system is said to be observable if, for any possible sequence of state and control vectors, the current state can be determined in finite time using only the outputs (this definition is slanted towards the state space representation). Less formally, this means that from the system’s outputs it is possible to determine the behavior of the entire system. If a system is not observable, this means the current values of some of its states cannot be determined through output sensors.</em></p>
</blockquote>

<p>Most of the conversations occurring in the tech sector are focused on monitoring operations, and while this is a component of my definition, I lean more heavily on the observing occurring beyond just internal groups, and observability being about helping keep partners, consumers, regulators, and other stakeholders be more aware regarding how complex systems work, or do not work. I feel that observability is critical to the future of algorithms, and making sense of how technology is impacting our world, and APIs will play a critical role in ensuring that the platforms have the external outputs required for delivering meaningful observability.</p>

<p>When it comes to quantifying the observability of platforms and algorithms, the more outputs available the better. Everything should have APIs for determining the inputs and outputs of any algorithm, or other system, but there should also be operational level APIs that give insight into the underlying compute, storage, logging, DNS, and other layers of delivering technological solutions. There should also be higher level business layer APIs surrounding communication via blog RSS, Twitter feeds, and support channels like email, ticketing, and other systems. The more outputs around platform operations there are, the more we can measure, quantify, and determine how observable a platform is using the outputs that already exist for ALL the systems in use across operations.</p>

<p>To truly measure the observability of a platform I need to be able to measure the technology, business, and politics surrounding its operation. If communication and support exist in the shadows, a platform is not observable, even if there are direct platform APIs. If you can’t get at the operational layer like logging, or possibly Github repositories used as part of continuous integration or deployment pipelines, observability is diminished. Of course, not all of these outputs should all be publicly available by default, but in many cases, there really is no reason they can’t. At a minimum there should be API access, with some sort of API management layer in place, allowing for 3rd party auditors, and analysts like me to get at some, or all of the existing outputs, allowing us to weigh in on an overall platform observability workflow.</p>

<p>As I continue to develop my API observability ranking algorithm, the first set of values I calculate are the number of existing outputs an API has. Taking into consideration the scope of core and operational APIs, but also whether I can get at operations via Twitter, Github, LinkedIn, and other 3rd party APIs. I find many of these channels more valuable for understanding platform operations, than the direct APIs themselves. Chatter by employees, and commits via Github can provide more telling signals about what is happening, than the intentional signals emitted directly by the platform itself. Overall, the more outputs available the better. API observability is all about leveraging existing outputs, and when companies, organizations, institutions, and government agencies are using solutions that have existing APIs, they are more observable by default, which can have a pretty significant impact in helping us understand the impact a technological solution is having on everyone involved.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/12/more-outputs-are-better-when-it-comes-to-establishing-an-api-observability-ranking/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/12/labeling-your-high-usage-apis-and-externalizing-api-metrics-within-your-api-documentation/">Labeling Your High Usage APIs and Externalizing API Metrics Within Your API Documentation</a></h3>
        <span class="post-date">12 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/alpha-advantage/alpha-vantage-high-usage-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am profiling a number of market data APIs as part of my research with <a href="http://apis.how/streamdata">Streamdata.io</a>. As I work my way through the process of profiling APIs I am always looking for other interesting ideas for stories on API Evangelist. <a href="https://www.alphavantage.co/documentation/">One of the things I noticed while profiling Alpha Vantage</a>, was that they highlighted their high usage APIs with prominent, very colorful labels. One of the things I’m working to determine in this round of profiling is how “real time” APIs are, or aren’t, and the high usage label adds another interesting dimension to this work.</p>

<p>While reviewing API documentation it is nice to have labels that distinguish APIs from each other. Alpha Vantage has a fairly large number of APIs so it is nice to be able to focus on the ones that are used the most, and are more popular. For example, as part of my profiling I focused on the high usage technical indicator APIs, rather than profiling all of them. I need to be able to prioritize my work, and these labels helped me do that. Providing one example of the benefit that these types of labels can bring to the table. I’m guessing that there are many other time saving aspects of labeling popular APIs, beyond just saving me time.</p>

<p>This type of labeling is an interesting way of externalizing API analytics in my opinion. Which is another interesting concept to think about across API operations. How can you take the most meaningful data points across your API management processes, and distill them down, externalize and share them so that your API consumers can benefit from valuable API metrics? In this context, I could see a whole range of labels that could be established, applied to interactive documentation using OpenAPI tags, and made available across API documentation, helping make APIs even more dynamic, and in sync with how they are actually being used, measured, and making an impact on operations.</p>

<p>I’m a big fan of making API documentation even more interactive, alive, and meaningful to API consumers. I’m thinking that tagging and labeling is how we are going to do this in the future. Generating a very visual, but also semantic layer of meaning that we can overlay in our API documentation, making them even more accessible by API consumers. I know that Alpha Advantages’s high usage labels have saved me significant amounts work, and I’m sure there are other approaches that could continue delivering in this way. It is something I’m keeping a close eye in this increasingly event-driven, API landscape, where API integration is becoming more dynamic and real time.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/12/labeling-your-high-usage-apis-and-externalizing-api-metrics-within-your-api-documentation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/12/be-clear-about-your-api-pricing/">Be Clear About Your API Pricing</a></h3>
        <span class="post-date">12 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/green-gears-matrix.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m profiling a large number of APIs right now, and I am ranking APIs based upon how easy or difficult they are to access. Whether or not an API provide has a business model is part of the ranking, and how clearly articulated the access and pricing is around that model is a critical part of my profiling algorithm. The APIs that end up included in the API gallery I’m developing for <a href="http://apis.how/streamdata">Streamdata.io</a>, and available as part of my wider <a href="http://theapistack.com">API Stack research</a> will all have to possess easy to articulate access levels. Not all of them will be free, but the ones that cost money will have straightforward pricing that can be articulate in a single sentence–something that seems to be elusive with many of the API providers I am profiling.</p>

<p>I am regularly confused regarding the myriad of ways in which API providers obfuscate the pricing for their APIs. I’ve long been weary of API providers who don’t have a clear business model, but when they have a pricing page, but bu do not consistently apply it to APIs, I’m just left confounded. I can’t always tell if it is done maliciously, or they just haven’t approached their API through an external lens. If I find a pricing page, and the plans seem reasonable, and I’ve plugged my credit card in, but then I still don’t have access to some APIs, and there is no clear labeling of which APIs I have access to as part of my plan, I just can’t spend the afternoon testing and seeing which APIs return a 403 to understand the landscape. The API service composition, and pricing tiers needs to be coherent and front and center, otherwise I just have to move on. If I can’t communicate what is going on to others, it won’t be included in my work.</p>

<p>I do not have a problem with different tiers of access, as long as they are communicated, and information about them is accessible. I won’t complain when some APIs are out of reach to me, and placed in premium tiers–I’ll just pass that information on to my readers. However, if I have to do some sort of secret handshake, or call some special sales hotline to understand what is going on, in my experience there are usually other illnesses occurring behind the scene, and I’m pretty well conditioned to just move on. If you have a publicly available API, be clear about your pricing. Even if I need approval for higher levels of usage, or it costs me to gain access to high level tiers. Don’t play games, there are too many APIs out there to mess around with hidden API pricing plans, unless of course you really aren’t interested in folks covering your API, and putting them to use, which I feel like some of these companies are actually hoping occurs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/12/be-clear-about-your-api-pricing/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/12/a-dedicated-guest-blogger-program-for-your-api/">A Dedicated Guest Blogger Program For Your API</a></h3>
        <span class="post-date">12 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/intrinio/intrinio-guest-blogger-program.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I get endless waves of people wanting to “guest post” on API Evangelist. It isn’t something I’m interested in because of the nature of API Evangelist, and that it really is just my own stream of consciousness, and not about selling any particular product or service. However, if you are an API provider, looking for quality content for your blog, having a formal approach to managing guest bloggers might make sense. Sure, you don’t want to accept all the spammy requests that you will get, but with the right process, you could increase the drumbeat around your API, and build relationships with your partners and API consumers.</p>

<p>There is an example of this in action at the financial data marketplace Intrinio, with <a href="https://intrinio.com/bloggers">their official blogger program</a>. The blogging program for the platform has a set of established benchmarks defined by the Intrinio team, to establish quality for any post that is accepted as part of the program. What I find really interesting, is that they also offer three months of free access to data feeds for API consumers who publish a post via the platform. “Exceptional” participants in the program may have their free access extended, and ALL participants will receive discounts on paid data access subscriptions via the platforms APIs.</p>

<p>This is the type of value exchange I like to see via API platforms. Too many APIs are simple one way streets, paying for GET access to data, content, media, and algorithms. API management shouldn’t be just about about metering one way access and charging for it. Sensible API management should measure value exchange around ALL platform resources, including blog and forum posts, and other activities API providers should be incentivizing via their platforms. This is one of the negative side effects of REST I feel–too much focus on resources, and not about the events that occur around these resources. Something we are beginning to move beyond in an event-driven API landscape.</p>

<p>Next, I will be profiling the concept of having dedicated data partner programs for your API platform. Showcasing how your API consumers can submit their own data APIs for resell alongside your own resources. In my opinion, every API platform should be opening up every resource for GET, POST, PUT, and DELETE, as well as allow for the augmenting, aggregation, enrichment, and introduction of other data, content, media, and algorithms, to add more value to what is already going on. Opening up a dedicated guest blogger program modeled after Intrinio’s is a good place to start. Learning about how to set up guidelines and benchmarks for submission, and evolving your API management to allow for incentivizing of participation. Once you get your feet wet with the blog, you may want to expand to other resources available via the platform, making your API operations a much more community thing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/12/a-dedicated-guest-blogger-program-for-your-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/you-have-to-know-where-all-your-apis-are-before-you-can-deliver-on-api-governance/">You Have to Know Where All Your APIs Are Before You Can Deliver On API Governance</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/64_185_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I wrote an earlier article that <a href="http://apievangelist.com/2017/12/20/basic-api-design-guidelines-are-you-first-step-towards-api-governance/">basic API design guidelines are your first step towards API governance</a>, but I wanted to introduce another first step you should be taking even before basic API design guides–cataloging all of your APIs. I’m regularly surprised by the number of companies I’m talking with who don’t even know where all of their APIs are. Sometimes, but not always, there is some sort of API directory or catalog in place, but often times it is out of date, and people just aren’t registering their APIs, or following any common approach to delivering APIs within an organization–hence the need for API governance.</p>

<p>My recommendation is that even before you start thinking about what your governance will look like, or even mention the word to anyone, you take inventory of what is already happening. Develop an org chart, and begin having conversations. Identify EVERYONE who is developing APIs, and start tracking on how they are doing what they do. Sure, you want to get an inventory of all the APIs each individual or team is developing or operating, but you should also be documenting all the tooling, services, and processes they employ as part of their workflow. Ideally, there is some sort of continuous deployment workflow in place, but this isn’t a reality in many of the organization I work with, so mapping out how things get done is often the first order of business.</p>

<p>One of the biggest failures of API governance I see is that the strategy has no plan for how we get from where we are to where we ant to be, it simply focuses on where we want to be. This type of approach contributes significantly to pissing people off right out of the gate, making API governance a lot more difficult. Stop focusing on where you want to be for a moment, and focus on where you are. Build a map of where people are, tools, services, skills, best and worst practices. Develop a comprehensive map of where organization is today, and then sit down with all stakeholders to evaluate what can be improved upon, and streamlined. Beginning the hard work of building a bridge between your existing teams and what might end up being a future API governance strategy.</p>

<p>API design is definitely the first logical step of your API governance strategy, standardizing how you design your APIs, but this shouldn’t be developed from the outside-in. It should be developed from what already exists within your organization, and then begin mapping to healthy API design practices from across the industry. Make sure you are involving everyone you’ve reached out to as part of inventory of APIs, tools, services, and people. Make sure they have a voice in crafting that first draft of API design guidelines you bring to the table. Without buy-in from everyone involved, you are going to have a much harder time ever reaching the point where you can call what you are doing governance, let alone seeing the results you desire across your API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/you-have-to-know-where-all-your-apis-are-before-you-can-deliver-on-api-governance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/riot-games-regional-endpoints/">Riot Games Regional API Endpoints</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/riot-games/riot-games-developer-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m slowly categorizing all the APIs I find who are offering up some sort regional availability as part of their operations. With the easy of deployment using leading cloud services, it is something I am beginning to see more frequently. However, there is still a wide variety of reasons why an API provider will invest in this aspect of their operations, and I’m looking to understand more about what these motivations are. Sometimes it is because they are serving a global audience, and latency kills the experience, but other times I’m seeing it is more about the maturity of the API provider, and they’ve have such a large user base that they are getting more requests to deliver resources closer to home.</p>

<p><a href="https://developer.riotgames.com/regional-endpoints.html">The most recent API provider I have come across who is offering regional API endpoints is from Riot Games</a>, the makers of League of Legends, who offers <a href="https://developer.riotgames.com/regional-endpoints.html">twelve separate regions for you to chose from</a>, broken down using a variety of regional subdomains. The Riot Games API provides a wealth of meta data around their games, and while they don’t state their reasons for providing regional APIs, I’m guessing it is to make sure the meta data is localized to whichever country their customers are playing in. Reducing an latency across networks, making the overall gaming and supporting application experience as smooth and seamless as possible. Pretty standard reasons for doing regional APIs, and providing a simple example of how you do this at the DNS level.</p>

<p><a href="https://developer.riotgames.com/api-status/">RIot Games also provides a regional breakdown of the availability of their regional endpoints on their API status page</a>, adding another dimension to the regional API delivery conversation. If you are providing regional APIs, you should be monitoring them, and communicating this to your consumers. This is all pretty standard stuff, but I’m working to document every example of regional APIs I come across as part of my research. I’m considering adding a separate research area to track on the different approaches so I can publish a guide, and supporting white papers when I have enough information organized. All part of my work to understand how the API business operates, and is expanding. Showcasing how the leaders are delivering resources via APIs in a scalable way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/riot-games-regional-endpoints/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/consistentency-and-branding-across-api-portals/">Consistency in Branding Across API Portals</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/subway/london-underground.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://www.bbc.co.uk/programmes/b0903ppd">I recently watched a BBC documentary about the history of the branding used as part of the London Underground</a>. I’m pretty absorbed lately with using public transit as an analogy for complex API implementations, and moving beyond just using subway maps, I thought the branding strategy for the London Underground provided other important lessons for API providers. The BBC documentary went into great detail regarding how much work was put into standardizing the font, branding, and presentation of information for each London Underground, to help reduce confusion, and help riders get where they needed, and making the city operate more efficiently.</p>

<p>As I continue to study the world of <a href="http://documentation.apievangelist.com">API documentation</a>, I think we have so much work ahead of us when it comes to standardizing how we present our API portals. Right now every API portal is different, even often times with multiple portals from the same company–see Amazon Web Services for example. I think we underestimate the damage this has to the overall API experience for consumers, and why we see API documentation like Swagger UI, Slate, and Read the Docs have such an impact. However this is just documentation, and we need this to occur as part of the wider API portal user experience. I’ve seen some standardized open source API portal solutions, and there are a handful of API portal services out there, but there really is no standard for how we deliver, brand, and operate the wider API experience.</p>

<p>I have <a href="https://apievangelist.com/2015/04/10/my-minimum-viable-api-footprint-definition/">my minimum viable API portal definition</a>, and have been tracking on the common building blocks of API operations for eight years now, but there are no “plug and play” solutions that users can implement, following any single approach. I have the data, and <a href="http://portal.minimum.apievangelist.com/">I even have a simple Twitter Bootstrap version of my definition (something I’m upgrading ASAP)</a>, but in my experience people get very, very, very hung up on the visual aspects of this conversation, want different visual elements, and quickly get lost on the functional details. I’m working with my partners APIMATIC to help standardize their portal offering, but honestly it is something that needs to be wider than just me, and any single provider. It is something that needs to emerge as a common API portal standard. If we can bring this into focus, I think we will see API adoption significantly increase, reducing much of the confusion we all face getting up and running with any new API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/consistentency-and-branding-across-api-portals/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/keeping-api-schema-simple-for-wider-adoption/">Keeping API Schema Simple For Wider Adoption</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-csv.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>One aspect of <a href="http://apievangelist.com/2018/02/03/api-is-not-just-rest/">my talk at APIDays Paris this last week</a>, included a slide about considering to allow API consumers to negotiate CSV responses from our API. Something that would probably NEVER occur to most API providers, and probably would make many even laugh at me. I’m used to it, and don’t care. While not something that every API provider should be considering, depending on the data you are serving up, and who your target API consumer ares, it is something that might make sense. Allowing for the negotiation of CSV responses represents lowering the bar for API consumption, and widening the audience who can put our APIs to work.</p>

<p>I was doing more work around public data recently, and was introduced to an interesting look at some lessons from developing open data standards. I’m doing a deep dive into municipal data lately as part of my partnership with <a href="http://apis.how/streamdata">Streamdata.io</a>, and I found <a href="http://www.opennorth.ca/2017/12/21/from-development-to-adoption-lessons-from-three-open-standards.html">the lessons they published interesting</a>, and something that reflects my stance on API content negotiation.</p>

<blockquote>
  <p><em>From the development and maintenance of the API, it quickly became clear that adjusting scripts after every election (and by-election) and website modification, was quickly becoming unsustainable. To address this issue, a simple CSV schema was developed to encourage standardisation of this data from the outset. The schema was designed to be as simple and easy to understand and implement as possible. Comprised of just 21 fields, 7 of which are recommended fields, the schema does not have hierarchical relationships between terms and can be implemented in a single CSV file. By making the standard this simple, we were able to get a number of adopters onboard and outputting their lists of elected representatives on their own open data portals.</em></p>
</blockquote>

<p>When it comes to APIs, simplicity rules. The simpler you can make your API, the more impact you will make. Allowing for the negotiation of CSV responses from your API when possible allows API consumers to go from API to a spreadsheet in just one or two clicks. This is huge when it comes to on boarding business users with the concepts of APIs, and what they do, and allows them to easily put valuable data resources to work in their native environment–the spreadsheet. This is something many API consumers won’t understand, but when it comes to seeking meaningful API adoption, it is something that expand the reach of any API beyond the developer class, putting it within reach of business users.</p>

<p>I am a big fan of pushing our APIs to allow for the negotiation of CSV. XML, and JSON by default, whenever possible. I’m also a fan of delivering richer experiences by allowing for the negotiation of hypermedia media types. While delivering hypermedia takes a significant amount of thought and investment, allowing for the negotiation of CSV, XML, and JSON doesn’t take a lot of work. When delivering your APIs, I highly recommend thinking about who your API consumers are, and whether offering CSV responses might shift the landscape even a little bit, making your valuable data resources a little more usable by business users who won’t necessarily be delivering web or mobile applications.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/keeping-api-schema-simple-for-wider-adoption/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/api-quota-api-webhooks-and-server-sent-events-sse/">API Quota API, Webhooks, and Server-Sent Events (SSE)</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/goldsilverfalls/creativity/file-00_00_40_84.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am profiling market data APIs as part of my partnership with <a href="http://apis.how/streamdata">Streamdata.io</a>. It is a process I enjoy, because it provides me with a number of interesting stories I can tell here on API Evangelist. Many of the APIs I profile just frustrate me, but there are always the gems who are doing interesting things with their APIs, and understand providing APIs, as well as consuming APIs. One API that I’ve been profiling, and I am able to put to use in my work to build a gallery of real time data APIs, was <a href="https://1forge.com">1Forge</a>.</p>

<p><a href="https://1forge.com/forex-data-api/api-documentation">1Forge provides dead simple APIs for accessing market data</a>, and surprise!! – you can sign up for a key, and begin making API calls within minutes. It might not sound like that big of a deal, but after going through 25+ APIs, I only have about 5 API keys. I’m working on an OpenAPI definition for 1Forge, so I can begin to poll, and stream the data they make available, including it in the Streamdata.io API gallery I’m building. However, as I was getting up and running with the API, I noticed <a href="https://1forge.com/forex-data-api/api-documentation">their quota endpoint</a>, which allows me to check my usage quote with the 1Forge API–something that I thought was story worthy.</p>

<p>The idea of an endpoint to check my applications usage quota for an API seems like a pretty fundamental concept, but sadly it is something I do not see very often. It is something that should be default for ALL APIs, but additionally I’d like to see a webhook for, letting me know when my API consumption reaches different levels. Since I’m talking about Streamdata.io, it would also make sense to offer a Server-Sent Event (SSE) for the API quote endpoint, allowing me to bake the usage quota for all the APIs I depend on into a single API dashboard–streaming real time usage information across the APIs depend on, and maybe displaying things in RED when I reach certain levels.</p>

<p>An API quota API is useful for when you depend on a single API, but is something that becomes almost critical when it comes to depending on many APIs. These are one of those APIs that API providers are going to need to realize has to be present by default for their API platforms. It is something that can keep us humans in tune with our consumption, but more importantly can help us programmatically manage our API consumption, and adjust our polling frequency automatically as reach the limits of our API access tier, or even upgrade as we realize our rate limit constraints are too tight for a specific application. I’m going to add an API quota API to my list of default administrative APIs that API providers should be offering. Updating the default set of resources we should have available for ALL APIs we are operating.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/api-quota-api-webhooks-and-server-sent-events-sse/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/the-more-we-know-about-you-the-more-api-acces-you-get/">The More We Know About You The More API Access You Get</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/65_144_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2018/01/24/where-am-i-in-the-sales-funnel-for-your-api/">I’ve been trash talking APIs that identify me as part of some sort of sales funnel</a>, and automate the decision around whether or not I get access to their API. My beef isn’t with API providers profiling me and making decisions about how much access I get, it is about them limiting profiles making it so I do not get access to their APIs at all. Their narrow definitions of the type of API consumers they are seeking does not include me, even though I have thousands of regular readers of my blog who do fit their profile. In the end, it is their loss, not mine, that they do not let me in, but the topic is still something I feel should be discussed out in the open, hopefully expanding the profile definitions for some API providers who may not have considered the bigger picture.</p>

<p>I’ve highlighted the limiting profiling of API consumers that prevent access to APIs, but now I want to talk about how profiling can be sensibly used to limit access to API resources. Healthy API management always has an entry level tier, but what tiers are available after that often depend on a variety of other data points. One thing I see API providers regularly doing is requiring API consumers to provide more detail about who they are and what they are doing with an API. I don’t have any problem with API providers doing this, making educated and informed decisions regarding who an API consumer is or isn’t. As the API Evangelist I am happy to share more data points about me to get more access. I don’t necessarily want to do this to sign up for your entry level access tier, just so I can kick the tires, but if I’m needing deeper access, I am happy to fill our a fuller profile of myself, and what I am working on.</p>

<p>Stay out of my way when it comes to getting started and test driving your APIs. However, it is perfectly acceptable to require me to disclose more information, require me to reach out an connect with your team, and other things that you feel are necessary before giving me wider access to your APIs, and provide me with looser rate limits. I encourage API providers to push on API consumers before you give away the keys to the farm. Developing tiered levels of access is how you do this. Make me round off the CRM entry for my personal profile, as well as my company. Push me to validate who I am, and that my intentions are truly honest. I encourage you to reach out to each one of your API consumers with an honest “hello” email after I sign up. Don’t require me to jump on the phone, or get pushy with sales. However, making sure I provide you with more information about myself, my project and company in exchange for higher levels of API access is a perfectly acceptable way of doing business with APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/the-more-we-know-about-you-the-more-api-acces-you-get/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/headers-used-for-grpc-over-http2/">Learning About The Headers Used for gRPC over HTTP/2</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-grpc.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am learning more about gRPC and HTTP/2, as part of the recent expansion of my API toolbox. I’m not a huge fan of Protocol Buffers, however I do get the performance gain they introduce, but I am very interested in learning more about how HTTP/2 is being used as a transport. While I’ve been studying how websockets, Kafka, MQTT, and other protocols have left the boundaries of HTTP and are embracing the performance gains available in the pure TCP realm, I’m more intrigued by the next generation of HTTP as a transport.</p>

<p>Part of my learning process is all about understanding the headers available to us in the HTTP/2 realm. I’ve been learning more about the next generation HTTP headers from the <a href="https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md">gRPC Github repository</a> which provides details on the request and response headers in play.</p>

<p><strong>HTTP/2 API Request Headers</strong></p>

<ul>
  <li>Request-Headers → Call-Definition *Custom-Metadata</li>
  <li>Call-Definition → Method Scheme Path TE [Authority] [Timeout] Content-Type [Message-Type] [Message-Encoding] [Message-Accept-Encoding] [User-Agent]</li>
  <li>Method → “:method POST”</li>
  <li>Scheme → “:scheme “ (“http” / “https”)</li>
  <li>Path → “:path” “/” Service-Name “/” {method name} # But see note below.</li>
  <li>Service-Name → {IDL-specific service name}</li>
  <li>Authority → “:authority” {virtual host name of authority}</li>
  <li>TE → “te” “trailers” # Used to detect incompatible proxies</li>
  <li>Timeout → “grpc-timeout” TimeoutValue TimeoutUnit</li>
  <li>TimeoutValue → {positive integer as ASCII string of at most 8 digits}</li>
  <li>TimeoutUnit → Hour / Minute / Second / Millisecond / Microsecond / Nanosecond</li>
  <li>Hour → “H”</li>
  <li>Minute → “M”</li>
  <li>Second → “S”</li>
  <li>Millisecond → “m”</li>
  <li>Microsecond → “u”</li>
  <li>Nanosecond → “n”</li>
  <li>Content-Type → “content-type” “application/grpc” [(“+proto” / “+json” / {custom})]</li>
  <li>Content-Coding → “identity” / “gzip” / “deflate” / “snappy” / {custom}</li>
  <li>Message-Encoding → “grpc-encoding” Content-Coding</li>
  <li>Message-Accept-Encoding → “grpc-accept-encoding” Content-Coding *(“,” Content-Coding)</li>
  <li>User-Agent → “user-agent” {structured user-agent string}</li>
  <li>Message-Type → “grpc-message-type” {type name for message schema}</li>
  <li>Custom-Metadata → Binary-Header / ASCII-Header</li>
  <li>Binary-Header → {Header-Name “-bin” } {base64 encoded value}</li>
  <li>ASCII-Header → Header-Name ASCII-Value</li>
  <li>Header-Name → 1*( %x30-39 / %x61-7A / “_” / “-“ / “.”) ; 0-9 a-z _ - .</li>
  <li>ASCII-Value → 1*( %x20-%x7E ) ; space and printable ASCII</li>
</ul>

<p><strong>HTTP/2 API Response Headers</strong></p>

<ul>
  <li>Response → (Response-Headers *Length-Prefixed-Message Trailers) / Trailers-Only
Response-Headers → HTTP-Status [Message-Encoding] [Message-Accept-Encoding] Content-Type *Custom-Metadata</li>
  <li>Trailers-Only → HTTP-Status Content-Type Trailers</li>
  <li>Trailers → Status [Status-Message] *Custom-Metadata</li>
  <li>HTTP-Status → “:status 200”</li>
  <li>Status → “grpc-status” 1*DIGIT ; 0-9</li>
  <li>Status-Message → “grpc-message” Percent-Encoded</li>
  <li>Percent-Encoded → 1*(Percent-Byte-Unencoded / Percent-Byte-Encoded)</li>
  <li>Percent-Byte-Unencoded → 1*( %x20-%x24 / %x26-%x7E ) ; space and VCHAR, except %</li>
  <li>Percent-Byte-Encoded → “%” 2HEXDIGIT ; 0-9 A-F</li>
</ul>

<p>I’m enjoying getting down to the nitty gritty details of how HTTP/2 works. I’m intrigued by the multi-directionality of it. Being able to use just like HTTP/1.1 with simple requests and responses, but also being able to introduce bi-directional API calls, where you can make many different API calls as you want. I don’t think I will get any time to play with in the near future. I have way too much work. However, I do like learning about how it is being used, and I think Google is the most forward thinking when it comes to HTTP/2 adoption in the API sector–providing multi-speed APIs in JSON using HTTP/1.1, or Protocol Buffers using HTTP/2.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/headers-used-for-grpc-over-http2/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/i-appreciate-you-wanting-to-jump-on-the-phone-but-i-have-other-apis-to-test-drive/">I Appreciate The Request To Jump On Phone But I Have Other APIs To Test Drive</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/desertroad/clean_view/file-00_00_00_00.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://apis.how/streamdata">Streamdata.io</a> is investing in my <a href="http://theapistack.com">API Stack</a> work as we build out their API Gallery of valuable data streaming APIs. I’m powering through hundreds of APIs and using my approach to profiling APIs that I have been developing over the last eight years of operating API Evangelist. I have a large number of APIs to get through, so I don’t have a lot of time to spend on each API. I am quickly profiling and ranking them to quickly identify which one’s are worth my time. While there are many elements that get in the way of me actually being able to obtain an API key and begin using an API, one of the more frustrating elements when API providers require me to jump on the phone with them before I can test drive any APIs.</p>

<p>I’ve encountered numerous APIs that require me talk to a sales person before I can do anything. I know that y’all think this is savvy. This is how business is done these days, but it just isn’t the way you start conversations with API consumers. Sure, there should be support channels available when I need them, but it SHOULD NOT be the way you begin a conversation with us API consumers. I’ve heard all the reasons possible for why companies feel like they need to do this, and I guarantee that all of them are based upon out of date perspectives around what APIs are all about. Often times they are a bi-product of not having a modern API management solution in place, and a team that lacks a wider awareness of the API sector and how API operations works.</p>

<p>In 2018, I shouldn’t have to talk to you on the phone to understand what your API does, and how it fits into what I’m working on. Most of the time I do not even know what I’m working on. I’m just kicking the tires, seeing what is possible, and considering how it fits into my bigger picture. What good does it do for me to jump on the phone if I don’t even know what I’m working on? I can’t tell you much. You can’t share API responses with me. You will able to do less than if you just give me access to APIs, and allow me to make API calls. You don’t have to allow me to make too many calls, just a handful to get going. You don’t even have to give me access to ALL the APIs, just enough of them to wet my appetite and help me understand what it is that you do. This is done using modern API management solutions, and service composition. Giving you the control over exactly how mcuh of your resources I will have access to, until I prove myself worthy.</p>

<p>The APIs I come across that require me to jump on sales call will have to wait until later. I just won’t have the time to evaluate their value, and understand where they fit into my work. Which means they probably won’t ever make it into my project, or any of my storytelling around the work. Which means many of these APIs will not get the free exposure to my readers, helping them understand what is possible. It is just one of many self-inflicted wounds API providers make along the way when they leave their enterprise blinders on, and are too restrictive around their API resources. Sales still has a place in the API game, but the overall API strategy has significantly evolved in the last five years, and is something that is pretty easy to see if you spend time playing with other leading APIs on the market. Demonstrating that these providers probably haven’t done much due diligence about what is out there, which often is just yet another symptom of a poorly run API program, making passing on it probably a good idea.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/i-appreciate-you-wanting-to-jump-on-the-phone-but-i-have-other-apis-to-test-drive/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/03/api-is-not-just-rest/">API Is Not Just REST</a></h3>
        <span class="post-date">03 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-days-paris-2018-api-is-not-just-rest.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>This is one of my talks from <a href="http://www.apidays.io/events/paris-2017">APIDays Paris 2018</a>. Here is the abstract: The modern API toolbox includes a variety of standards and methodologies, which centers around REST,  but also includes Hypermedia, GraphQL, real time streaming, event-driven architecture , and gRPC. API design has pushed beyond just basic access to data, and also can be about querying complex data structures, providing experience rich APIs, real-time data streams with Kafka and other standards, as well as also leveraging the latest algorithms and providing access to machine learning models. The biggest mistake any company, organization, or government agency can do is limit their API toolbox to be just about REST. Learn about a robust and modern API toolbox from the API Evangelist, Kin Lane.</em></p>

<p><strong>Diverse Toolbox</strong><br />
After eight years of evangelizing APIs, when I participate in many API conversations, some people still assume I’m exclusively talking about REST as the API Evangelist–when in reality I am simply talking about APIs that leverage the web. Sure, REST is a dominant design pattern I shine a light on, and has enjoyed a significant amount of the spotlight over the last decade, but in reality on the ground at companies, organizations, institutions, and government agencies of all shapes and sizes, I find a much more robust API toolbox is required to get the job done. REST is just one tool in my robust and diverse toolbox, and I wanted to share with you what I am using in 2018.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-1.png" /></p>

<p>The toolbox I’m referring tool isn’t just about what is needed to equip an API architect to build out the perfect vision of the future. This is a toolbox that is equipped to get us from present day into the future, acknowledging all of the technical debt that exists within most organizations which many are looking to evolve as part of their larger digital transformation. My toolbox is increasingly pushing the boundaries of what I’ve historically defined as an API, and I’m hoping that my experiences will also push the boundaries of what you define as an API, making you ready for what you will encounter on the ground within organizations you are delivering APis within.</p>

<p><strong>Application Programming Interface</strong><br />
API is an acronym standing for application programming interface. I do not limit the scope of application in the context to just be about web or mobile application. I don’t even limit it to the growing number of device-based applications I’m seeing emerge. For me, application is about applying the digital resources made available via an programmatic interface. I’m looking to take the data, content, media, and algorithms being made available via APIs and apply them anywhere they are needed on the web, within mobile and device applications, or on the desktop, via spreadsheets, digital signage, or anywhere else that is relevant, and sensible in 2018.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-2.png" /></p>

<p>API does not mean REST. I’m really unsure how it got this dogmatic association, nor do I care. It is an unproductive legacy of the API sector, and one I’m looking to move beyond. Application programming interfaces aren’t the solution to every digital problem we face. They are about understanding a variety of protocols, messaging formats, and trying to understand the best path forward depending on your application of the digital resources you are making accessible. My API toolbox reflects this view of the API landscape, and is something that has significantly evolved over the last decade of my career, and is something that will continue to evolve, and be defined by what I am seeing on the ground within the companies, organizations, institutions, and government agencies I am working with.</p>

<p><strong>SOAP</strong><br />
I have been working with databases since 1987, so I fully experienced the web services evolution of our industry. During the early years of the web, there was a significant amount of investment  into thinking about how we exchanged data across many industries, as well as within individual companies when it came to building out the infrastructure to deliver upon this vision. The web was new, but we did the hard work to understand how we could make data interoperability in a machine readable way, with an emphasis on the messages we were exchanging. Looking back I wish we had spent more time thinking about how we were using the web as a transport, as well as the influence of industry and investment interests, but maybe it wasn’t possible as the web was still so new.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-3.png" /></p>

<p>While web services provided a good foundation for delivering application programming interfaces, it may have underinvested in its usage of the web as a transport, and became a victim of the commercial success of the web. The need to deliver web applications more efficiently, and a desire to hastily use the low cost web as a transport quickly bastardized and cannibalized web services, into a variety of experiments and approaches that would get the job done with a lot less overhead and friction. Introducing efficiencies along the way, but also fragmenting our enterprise toolbox in a way which we are still putting back together.</p>

<p><strong>XML &amp; JSON RPC</strong><br />
One of the more fractious aspects of the web API evolution has been the pushback when API providers call their XML or JSON remote procedure call (RPC) APIs, RESTful, RESTish, or other mixing of philosophy and ideology, which has proven to be a dogma stimulating event. RESTafarians prefer that API providers properly define their approach, while many RPC providers could care less about labels, and are looking to just get the job done. Making XML and JSON RPC a very viable approach to doing APIs, something that still persists almost 20 years later.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-4.png" /></p>

<p>Amazon Web Services, Flickr, Slack, and other RPC APIs are doing just fine when it comes to getting the job done, despite the frustration, ranting, and shaming by the RESTafarians. It isn’t an ideal approach to delivering programmatic interfaces using the web, but it reflects its web roots, and gets the job done with low cost web infrastructure. RPC leaves a lot of room for improvement, but is a tool that has to remain in the toolbox. Not because I am designing new RPC APIs, but there is no doubt that at some point I will have to be integrating with an RPC API to do what you need to get done in my regular work.</p>

<p><strong>REST at Center</strong><br />
Roy Fielding’s dissertation on representational state transfer, often referred to as simple REST, is an amazing piece of work. It makes a lot of sense, and I feel is one of the most thorough looks at how to use the web for making data, content, media, and algorithms accessible in a machine readable way. I get why so many folks feel it is the RIGHT WAY to do things, and one of the reasons it is the default approach for many API designers and architects–myself included. However, REST is a philosophy, and much like microservices, provides us with a framework to think about how we put our API toolbox to work, but isn’t something that should blind us from the other tools we have within our reach.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-5.png" /></p>

<p>REST is where I begin most conversations about APIs, but it doesn’t entirely encompass what I mean when every time I use the phrase API. I feel REST has given me an excellent base for thinking about how I deliver APIs, but will slow my effectiveness if I leave my REST blinders on, and let dogma control the scope of my toolbox. REST has shown me the importance of the web when talking about APIs, and will continue to drive how I deliver APIs for many years. It has shown me how to structure, standardize, and simplify how I do APIs, and help my applications reach as wide as possible audience, using commonly understood infrastructure.</p>

<p><strong>Negotiating CSV</strong><br />
As the API Evangelist, I work with a lot of government, and business users. One thing I’ve learned working with this group is the power of using comma separated values (CSV) as a media type. I know that us developers and database folks enjoy a lot more structure in our lives, but I have found that allowing for the negotiation of CSV responses from APIs, can move mountains when it comes to helping onboard business users, and decision makers to the potential of APIs–even if the data format doesn’t represent the full potential of an API. CSV responses is the low bar I set for my APIs, making them accessible to a very wide business audience.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-6.png" /></p>

<p>CSV as a data format represents an anchor for the lowest common denominator for API access. As a developer, it won’t be the data format I personally will negotiate, but as a business user, it very well could mean the difference between using an API or not. Allowing me to take API responses and work with them in my native environment, the Excel spreadsheet, or Google Sheets environment. As I am designing my APIs, I’m always thinking about how I can make my resources available to the masses, and enabling the negotiation of CSV responses whenever possible, helps me achieve my wier objectives.</p>

<p><strong>Negotiating XML</strong><br />
I remember making the transition from XML to JSON in 2009. At first I was uncomfortable with the data format, and resisted using it over my more proven XML. However, I quickly saw the potential for the scrappy format while developing JavaScript applications, and when developing mobile applications. While JSON is my preferred, and default format for API design, I am still using XML on a regular basis while working with legacy APIs, as well as allowing for XML to be negotiated by the APIs I’m developing for wider consumption beyond the startup community. Some developers are just more comfortable using XML over JSON, and who knows, maybe by extending an XML olive branch, I might help developers begin to evolve in how they consume APIs.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-7.png" /></p>

<p>Similar to CSV, XML represents support for a wider audience. JSON has definitely shifted the landscape, but there are still many developers out there who haven’t made the shift. Whether we are consumers of their APIs, or providing APIs that target these developers, XML needs to be on the radar. Our toolbox needs to still allow for us to provide, consume, validate, and transform XML. If you aren’t working with XML at all in your job, consider yourself privileged, but also know that you exist within a siloed world of development, and you don’t receive much exposure to many systems that are the backbone of government and business.</p>

<p><strong>Negotiating JSON</strong><br />
I think about my career evolution, and the different data formats I’ve used in 30 years. It helps me see JSON as the default reality, not the default solution. It is what is working now, and reflects not just the technology, but also the business and politics of doing APIs in a mobile era, where JavaScript is widely used for delivering responsive solutions via multiple digital channels. JSON speaks to a wide number of developers, but we can’t forget that it is mostly comprised of developers who have entered the sector in the last decade.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-8.png" /></p>

<p>JSON is the default media type I use for any API I’m developing today. No matter what my backend data source is. However, it is just one of several data formats I will potentially open up for negotiation. I feel like plain JSON is lazy, and whenever possible I should be thinking about a wider audience by providing CSV and XML representations, but I should also be getting more structured and standardized in how I handle the requests and responses for my API. While I want my APIs to reach as wide as possible audience, I also want them to deliver rich results that best represents the data, content, media, algorithms, and other digital resources I’m serving up.</p>

<p><strong>Hypermedia Media Types</strong><br />
Taking the affordances present when humans engage with the web via browsers for granted is one of the most common mistakes I make as an API design, developer, and architect. This is a shortcoming I am regularly trying to make up for by getting more sophisticated in my usage of existing media types, and allowing for consumers to negotiate exactly the content they are looking for, and achieve a heightened experience consuming any API that I deliver. Hypermedia media types provide a wealth of ways to deliver consistent experiences, that help be deliver many of the affordances we expect as we make use of data, content, media, and algorithms via the web.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-9.png" /></p>

<p>Using media types like Hal, Siren, JSON API, Collection+JSON, and JSON-LD are allowing me to deliver a much more robust API experience, to a variety of API clients. Hypermedia reflects where I want to be when it comes to API design and architecture that leverages the web, but it is a reflection I have to often think deeply about as I still work to reach out to a wide audience, forcing me to make it one of several types of experience my consumers can negotiate. While I wish everyone saw the benefits, sometimes I need to make sure CSV, XML, and simpler JSON are also on the menu, ensuring I don’t leave anyone behind as I work to bridge where we are with where I’d like to go.</p>

<p><strong>API Query Layers</strong><br />
Knowing my API consumers is an important aspect of how I use my API toolbox. Depending on who I’m targeting with my APIs, I will make different decisions regarding the design pattern(s) I put to work. While I prefer investing resources into the design of my APIs, and crafting the URLs, requests, and responses my consumers will receive, in some situations my consumers might also need much more control over crafting the responses they are getting back. This is when I look to existing API query languages like Falcor or GraphQL to give my API consumers more of a voice in what their API responses will look like.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-10.png" /></p>

<p>API query layers are never a replacement for a more RESTful, or hypermedia approaches to delivering web APIs, but they can provide a very robust way to hand over control to consumers. API design is important for providers to understand, and define the resources they are making available, but a query language can be very powerful when it comes to making very complex data and content resources available via a single API URL. Of course, as with each tool present in this API toolbox, there are trade offs with deciding to use an API query language, but in some situations it can make the development of clients much more efficient and agile, depending on who your audience is, and the resources you are looking to make available.</p>

<p><strong>Webhooks</strong><br />
In my world APIs are rarely a one way street. My APIs don’t just allow API consumers to poll for data, content, and updates. I’m looking to define and respond to events, allowing data, and content to be pushed to consumers. I’m increasingly using Webhooks as a way to help my clients make their APIs a two-way street, and limit the amount of resources it takes to make digital assets available via APIs. Working with them to define the meaningful events that occur across the platform, and allow API consumers to subscribe to these events via Webhooks. Opening the door for API providers to deliver a more event-driven approach to doing APIs.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-11.png" /></p>

<p>Webhooks are the 101 level of event-driven API architecture for API providers. It is where you get started trying to understand the meaningful events that are occurring via any platform. Webhooks are how I am helping API providers understand what is possible, but also how I’m training API consumers in a variety of API communities about how they can deliver better experiences with their applications. I see webhooks alongside API design and management, as a way to help API providers and consumers better understand how API resources are being used, developing a wider awareness around which resources actually matter, and which ones do not.</p>

<p><strong>Websub</strong><br />
In 2018, I am investing more time in putting Websub, formerly known as the word which none of us could actually pronounce, PubSubHubbub. This approach to making content available by subscription as things change has finally matured into a standard, and reflects the evolution of how we deliver APIs in my opinion. I am using Websub to help me understand not just the event-driven nature of the APIs I’m delivering, but also that intersection of how we make API infrastructure more efficient and precise in doing what it does. Helping us develop meaningful subscriptions to data and content, that adds another dimension to the API design and even query conversation.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-12.png" /></p>

<p>Websub represents the many ways we can orchestrate our API implementations using a variety of content types, push and pull mechanisms, all leveraging web as the transport. I’m intrigued by the distributed aspect of API implementations using Websub, and the discovery that is built into the approach. The remaining pieces are pretty standard API stuff using GETs, POSTs, and content negotiation to get the job done. While not an approach I will be using by default, for specific use cases, delivering data and content to known consumers, I am beginning to put Websub to work alongside API query languages, and other event-driven architectural approaches. Now that Websub has matured as a standard, I’m even more interested in leveraging it as part of my diverse API toolbox.th</p>

<p><strong>Server Sent Events (SSE)</strong><br />
I consider webhooks to be the gateway drug for API event-driven architecture. Making API integrations a two street, while also making them more efficient, and potentially real time. After webhooks, the next tool in my toolbox for making API consumption more efficient and real time are server-sent events (SSE). Server-sent events (SSE) is a technology where a browser receives automatic updates from a server via a sustained HTTP connection, which has been standardized as part of HTML5 by the W3C. The approach is primarily used to established a sustained connection between a server, and the browser, but can just as easily be used server to server.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-13.png" /></p>

<p>Server-sent events (SSE) delivers one-way streaming APIs which can be used to send regular, and sustained updates, which can be more efficient than regular polling of an API. SSE is an efficient way to begin going beyond the basics of client-server request and response model and pushing the boundaries of what APIs can do. I am using SSE to make APIs much more real time, while also getting more precise with the delivery of data and content, leverage other standards like JSON Patch to only provide what has changed, rather than sending the same data out over the pipes again, making API communication much more efficient.</p>

<p><strong>Websockets</strong><br />
Shifting things further into real time, websockets is what I’m using to deliver two-way API streams that require data be both sent and received, providing full-duplex communication channels over a single TCP connection. WebSocket is a different TCP protocol from HTTP, but is designed to work over HTTP ports 80 and 443 as well as to support HTTP proxies and intermediaries, making it compatible with the HTTP protocol. To further achieve compatibility, the WebSocket handshake uses the HTTP Upgrade header to change from the HTTP protocol to the WebSocket protocol, pushing the boundaries of APIs beyond HTTP in a very seamless way.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-14.png" /></p>

<p>SSE is all about the one-way efficiency, and websockets is about two-way efficiency. I prefer keeping things within the realm of HTTP with SSE, unless I absolutely need the two-way, full-duplex communication channel. As you’ll see, I’m fine with pushing the definition of API out of the HTTP realm, but I’d prefer to keep things within bounds, as I feel it is best to embrace HTTP when doing business on the web. I can accomplish a number of objectives for data, content, media, and algorithmic access using the HTTP tools in my toolbox, leaving me to be pretty selective when I push things out of this context.</p>

<p><strong>gRPC Using HTTP/2</strong><br />
While I am forced to use Websockets for some existing integrations such as with Twitter, and other legacy implementations, it isn’t my choice for next generation projects. I’m opting to keep things within the HTTP realm, and embracing the next evolution of the protocol, and follow Google’s lead with gRPC. As with other RPC approaches, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types. gRPC embraces HTTP/2 as its next generation transport protocol, and while also employing Protocol Buffers, Google’s open source mechanism for the serialization of structured data.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-15.png" /></p>

<p>At Google, I am seeing Protocol Buffers used in parallel with OpenAPI for defining JSON APIs, providing two speed APIs using HTTP/1.1 and HTTP/2. I am also seeing Protocol Buffers used with HTTP/1.1 as a transport, making it something I have had to integrated with alongside SOAP, and other web APIs. While I am integrating with APIs that use Protocol Buffers, I am most interested in the usage of HTTP/2 as a transport for APIs, and I am investing more time learning about the next generation headers in use, and the variety of approaches in which HTTP/2 is used as a transport for traditional APIs, as well as multi-directional, streaming APIs.</p>

<p><strong>Apache Kafka</strong><br />
Another shift I could not ignore across the API landscape in 2017 was the growth in adoption of Kafka as a distributed streaming API platform. Kafka focuses on enabling providers to read and write streams of data like a messaging system, and develop applications that react to events in real-time, and store data safely in a distributed, replicated, fault-tolerant cluster. Kafka was originally developed at LinkedIn, but is now an Apache open source product that is in use across a number of very interesting companies, many of which have been sharing their stories of how efficient it is for developing internal data pipelines. I’ve been studying Kafka throughout 2017, and I have added it to my toolbox, despite it pushing the boundaries of my definition of what is an API beyond the HTTP realm.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-16.png" /></p>

<p>Kafka has moved out of the realm of HTTP, using a binary protocol over TCP, defining all APIs as request response message pairs, using its own messaging format. Each client initiates a socket connection and then writes a sequence of request messages and reads back the corresponding response message–no handshake is required on connection or disconnection. TCP is much more efficient over HTTP because it allows you to maintain persistent connections used for many requests. Taking streaming APIs to new levels, providing a super fast set of open source tools you can use internally to deliver the big data pipeline you need to get the job done. My mission is to understand how these pipelines are changing the landscape and which tools in my toolbox can help augment Kafka and deliver the last mile of connectivity to partners, and public applications.</p>

<p><strong>Message Queuing Telemetry Transport (MQTT)</strong><br />
Continuing to round off my API toolbox in a way that pushes the definition of APIs beyond HTTP, and helping me understand how APIs are being used to drive Internet-connected devices, I’ve added Message Queuing Telemetry Transport (MQTT), an ISO standard for implementing publish-subscribe-based messaging protocol to my toolbox. The protocol works on top of the TCP/IP protocol, and is designed for connections with remote locations where a light footprint” is required because compute, storage, or network capacity is limited. Making MQTT optimal for considering when you are connecting devices to the Internet, and unsure of the reliability of your connection.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-17.png" /></p>

<p>Both Kafka, and MQTT have shown me in the last couple of years, the limitations of HTTP when it comes to the high and low volume aspects of moving data around using networks. I don’t see this as a threat to APIs that leverage HTTP as a transport, I just see them as additional tools in my toolbox, for projects that meet these requirements. This isn’t a failure of HTTP, this is simply a limitation, and when I’m working on API projects involving internet connected devices I’m going to weight the pros and cons of using simple HTTP APIs, alongside using MQTT, and being a little more considerate about the messages I’m sending back and forth between devices and the cloud over the network I have in place. MQTT reflects my robust and diverse API toolbox, as one that gives me a wide variety of tools I’m familiar with and can use in different environments.</p>

<p><strong>Mastering My Usage Of Headers</strong><br />
One thing I’ve learned over the years while building my API toolbox is the importance of headers, and they are something that have regularly been not just about HTTP headers, but the more general usage of network networks. I have to admit that I understood the role of headers in the API conversation, but had not fully understood the scope of their importance when it comes to taking control over how your APIs operate within a distributed environment. Knowing which headers are required to consume APIs is essential to delivering stable integrations, and providing clear guidance on headers from a provider standpoint is essential to APIs operating as expected on the open web.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-18.png" /></p>

<p>Content negotiation was the header doorway I walked through that demonstrated the importance of HTTP headers when it comes to deliver the meaningful API experiences. Being able to negotiate CSV, XML, and JSON message formats, as well as being able to engage with my digital resources in a deeper way using hypermedia media types. My headers mastery is allowing me to better orchestrate an event-driven experience via webhooks, and long running HTTP connections via Server-Sent Events. They are also taking me into the next generation of connectivity using HTTP/2, making them a critical aspect of my API toolbox. Historically, headers have often been hidden in the background of my API work, but increasingly they are front and center, and essential to me getting the results I’m looking for.</p>

<p><strong>Standardizing My Messaging</strong><br />
I have to admit I had taken the strength of message formats present in my web service days for granted. While I still think they are bloated and too complex, I feel like we threw out a lot of benefits when we made the switch to more RESTful APIs. Overall I think the benefits of the evolution were positive, and media types provide us with some strong ways to standardize the messages we pass back and forth. I’m fine operating in a chaotic world of message formats and schema that are developed in the moment, but I’m a big fan of all roads leading to standardization and reuse of meaningful formats, so that we can try to speak with each other via APIs in more common formats.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-19.png" /></p>

<p>I do not feel that there is one message format to rule them all, or that even one for each industry. I think innovation at the message layer is important, but I also feel like we should be leveraging JSON Schema to help tame things whenever possible, and standardize as media types. Whenever possible, reuse existing standards from day one is preferred, but I get that this isn’t always the reality, and in many cases we are handed the equivalent of a filing cabinet filled with handwritten notes. In my world, there will always be a mixed of known and unknown message formats, something that I will always work to tame, as well as be increasingly apply machine learning models to help me identify, evolve, and make sense of–standardizing things in any way I possibly can.</p>

<p><strong>Knowing (Potential) Clients</strong><br />
I am developing APIs for a wide variety of clients. Some are designed for web applications, others are mobile applications, and some are devices. They could be spreadsheets, widgets, documents, and machine learning models. The tables could be flipped, and the APIs exist on device, and the cloud becomes the client. Sometimes the clients are known, other times they are unknown, and I am looking to attract new types of clients I never envisioned. I am always working to understand what types of clients I am looking to serve with my APIs, but the most important aspect of this process is understanding when there will be unknown clients.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-20.png" /></p>

<p>When I have a tightly controlled group of target clients, my world is much easier. When I do not know who will be developing against an API, and I am looking to encourage wider participation, this is when my toolbox comes into action. This is when I keep the bar as low as possible regarding the design of my APIs, the protocols I use, and the types of data formats and messages I use. When I do not know my API consumers and the clients they will developing, I invest more in API design, and keep my default requests and responses as simple as possible. Then I also allow for the negotiation of more complex, higher speed, more control aspects of my APIs by consumers who are in the know, targeting more specific client scenarios.</p>

<p><strong>Using The Right Tools For The Job</strong><br />
API is not REST. It is one tool in my toolbox. API deployment and integration is about having the right tool for the job. It is a waste of my time to demand that everyone understand one way of doing APIs, or my way of doing APIs. Sure, I wish people would study and learn about common API patterns, but in reality, on the ground in companies, organizations, institutions, and government agencies, this is not the state of things. Of course, I’ll spend time educating and training folks wherever I can, but my role is always more about delivering APIs, and integrating with existing APIs, and my API toolbox reflects this reality. I do not shame API providers for their lack of knowledge and available resources, I roll up my sleeves, put my API toolbox on the table, and get to work improving any situation that I can.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-21.png" /></p>

<p>My API toolbox is crafted for the world we have, as well as the world I’d like to see. I rarely get what I want on the ground deploying and integrating with APIs. I don’t let this stop me. I just keep refining my awareness and knowledge by watching, studying, and learning from what others are doing. I often find that when someone is in the business of shutting down a particular approach, or being dogmatic about a single approach, it is usually because they aren’t on the ground working with average businesses, organizations, and government agencies–they enjoy a pretty isolated, privileged existence. My toolbox is almost always open, constantly evolving, and perpetually being refined based upon the reality I experience on the ground, learning from people doing the hard work to keep critical services up and running, not simply dreaming about what should be.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/03/api-is-not-just-rest/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

	<table width="100%" border="1" style="background-color:#FFF; border: 0px #FFF;">
		<tr style="background-color:#FFF; border: 0px #FFF;">
			<td align="left">
				<a href="/blog/page5" class="button"><< Prev</a></li>
			</td>
			<td></td>
			<td align="right">
				<a href="/blog/page7" class="button">Next >></a>
			</td>
		</tr>
	</table>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/" target="_blank"><img src="https://apievangelist.com/images/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/" class="image"><img src="https://apievangelist.com/images/postman-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
