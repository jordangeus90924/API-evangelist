<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/29/extract-as-much-value-as-you-can-from-your-api-community-and-give-nothing-back/">Extract As Much Value As You Can From Your API Community And Give Nothing Back</a></h3>
        <span class="post-date">29 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algorotoscope/stories/36745180055_45289923eb_z.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>You are in a sweet spot. You got a fat six figure job in the coolest department of your company, building out your API platform. You have a decent budget (never as much as you want) to throw hackathons, run Google and Twitter ads, and you can buy schwag to give away at your events. Sure there is a lot of pressure to deliver, but you are doing pretty well. All you gotta do is convince 3rd party developers to do thing with your companies APIs, develop web, mobile, voice, and other applications that generate buzz and deliver the return on investment your bosses are looking for.</p>

<p>It is all about you and your team. Let’s get to work growth hacking! Attract as may new users as we can, and convince them to build as much as we possibly can. Let’s get them to develop SDKs, write articles for us on their blog, speak at our events, favorite things on hacker news, and whatever activities that we can. Your objective is to extract as much value from your API operations as you possibly can, and give nothing back. Expect developers to work for free. Expect your hackathons attendees to come up with the next great idea, build it, and hand it over to you for very little in return. This isn’t a partnership, this is an API ecosystem, and your team is determined to win at all costs.</p>

<p>Your API isn’t a two-way street. All roads lead to your success, and your bosses getting what they want. You don’t care that 3rd party developers should be compensated, or that they have any rights to their intellectual property. The 5% of them that successfully build applications, we will offer them a job in exchange for it, or we’ll just replicate it internally, decrease their rate limits, and increase their error rates so that they can’t compete. Sure you want people to still feel inspired, but not enough so that they’ll ever be able to sustain their applications–the only sustainable application around here will be owned by the platform. After all, this is all just business–it is nothing personal.</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/29/extract-as-much-value-as-you-can-from-your-api-community-and-give-nothing-back/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/29/the-reason-your-api-sucks-is-ther-are-no-women-and-people-of-color-on-your-team/">The Reason Your API Sucks Is There Are No Women And People Of Color On Your Team</a></h3>
        <span class="post-date">29 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/kin-lane/kin-lane.jpg" align="right" width="35%" style="padding: 15px;" /></p>
<p>I know that many of you are insecure about your APIs. You aren’t transparent with your numbers, and many aspects of your API operations. You are stressed out because you built it, and nobody came. You were able to artificially inflate your new user numbers, and API calls through paid campaigns, and bot activity, but nobody is using it, and you just can’t figure out why. You are asking yourself why don’t anyone see the value your API brings to the table? Why aren’t you getting the traction you thought you would get when you first came up with the idea?</p>

<p>You aren’t getting any traction with your API because it sucks. It was a bad idea. Nobody wants it. It sucks because it doesn’t provide any value in a highly competitive space, and you naively thought that if you built it everyone would come. You probably have a number of people around you telling you that your idea is great, and the API will be a hit. You’ve probably had this most of your life, and are used to people telling you that your ideas are great. It is why you feel so uncomfortable around anyone that is critical, because you just aren’t used to being told you that your ideas are dumb. It hurts your feelings.</p>

<p>This is why you surround yourself with people who look, act and think like you do. It is why you don’t think women and people of color have the skills needed to work on your dumb, useless ideas. You don’t have the balls to surround yourself with anyone who doesn’t think like you. If you did, you might have been told early on that your idea wasn’t worthwhile, or you might have gotten additional feedback or criticism that would have helped shape it into something useful. I know this is hard for you to hear, and you think you are really smart, and you probably read one of my cheerleader blog posts about how great APIs are, but in reality, if it doesn’t have any use in the real world nobody will care.</p>

<p>Don’t make this mistake with your API. Make sure your API team is as diverse as possible, and then work hard to make sure your community also becomes as inclusive you can. It is a lot more work to do thing this way, but it will pay off, and it will save you from potentially launching an API that completely sucks like this again, and doesn’t have any reach beyond just the echo chamber you exist in.</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/29/the-reason-your-api-sucks-is-ther-are-no-women-and-people-of-color-on-your-team/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/28/your-internal-dysfunction-is-not-my-api-problem/">Your Internal Dysfunction Is Not My API Problem</a></h3>
        <span class="post-date">28 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/35910978054_906047b6cb_z.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>You hear a lot of discussion regarding public API vs private API. From my vantage point there is only web APIs that use public DNS, but I find that folks hung up on the separation usually have many other hangups about things they like to keep behind the firewall, and under the umbrella of private. <a href="http://apievangelist.com/2017/08/10/my-focus-on-public-apis-applies-internally-as-well/">These are usually the same folks who like to tell me that my public API stories don’t apply to them</a>, and when you engage these folks in any ongoing fashion you tend to find that they are looking to keep a whole lot of dysfunction out of view from the public, and all the talk really has very little to do with APIs.</p>

<p>I spend my days studying the best practices across the leading API providers, and understanding what is working and what is not working when it comes to operating APIs. I have seven years of research I’m happy to share with folks, and I entertain a number requests to jump on calls, participate in webinars, do hangouts, and go onsite to do workshops and talks. I’m usually happy to do these things, and when it is a government agency, non-profit organization, and sometimes higher educational institutions, I am happy to these things at no charge. I like sharing what I know, and letting folks decide what they can use from the knowledge I’ve aggregated.</p>

<p>When I engage with folks I expect folks to not always be trusted–they don’t know me. However, I’m always surprised when folks think I have an agenda, looking to change them too fast, that I’m trying to shove something down their throat, and disrupt their position. First, I am always being invited in. I’m not a sales guy. I do not have anything to sell you except for my knowledge (which revenue goes to just goes back into doing what I do). There is regularly the quiet IT person who has carefully defended their position in the company and what they do not know for years, telling me this is all horse shit, and who do I think I am. Or the administrator who is always running around with their head cut-off, and feels the need to tell me that I do not understand them, and there is no way that any knowledge that I have is at all applicable to them–what the hell am I even doing here?</p>

<p>Hey, I’m just looking to share. If you don’t want it, I’m happy to tell me stories elsewhere, you don’t have to jump my shit. I’m genuinely trying to help, and share stories about what is working in other situations. I’m not looking to take your job, or make you do things you don’t have time or capacity for. Your dysfunction is not my API problem. I’m guessing this dysfunction is probably why you don’t have any sort of public API presence, and why your existing internal and partner APIs are not everything you’d like them to be. I am just going to quietly pick up my things and go back to what I was doing, I’m sorry that I’ve disrupted your chaotic world, and took up your time. I most likely won’t be back, but if you ever need to learn anything about APIs, and need a break, you know where to find me.</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/28/your-internal-dysfunction-is-not-my-api-problem/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/28/api-rant-vs-api-research/">API Rants vs. API Research</a></h3>
        <span class="post-date">28 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/36349140070_d5ec39cb34_z.jpg" align="right" width="40%" style="padding: 15px" /></p>
<p>I know many of you read my blog for the valuable nuggets of information extracted from my regular research into the world of APIs. I spend a great deal of time sifting through very boring, mundane, and sometimes valuable API related goings on. I have managed to muster the energy each week for the last seven years to sift through thousands of feeds, Tweets, and Github repositories looking for nuggets of API wisdom, best practices, and sometimes bad practices, to share here on the blog. Some weeks I find this an easy task, something I really enjoy the process, but most weeks it is a chore–some weeks I don’t give a shit at all. This is one of those weeks. Well, last week was too, but instead of NO blog posts, this week I’m going to shift things up so that I can get on track.</p>

<p>I have had series of folks piss in my Cheerios lately, regarding the free and unpaid work that I do, and as a result, I find myself without any writing mojo for the second week in a row, and not caring about sifting through all your API startup blah blah blah. 1/3 is about being rude and bro assholes, 1/3 of it is that APIs are boring and y’all have no imagination, and 1/3 of it is I’m a mentally ill asshole. The result of all of this is that you get a week full of API rants, instead of API research. Sooooooo, if this side of my personality turns you off, I recommend you tuning out API Evangelist for at least a week, until I feel better, and find the energy to do what it is that I do. There will still be plenty of substance in my posts, and things will still be VERY API related, and something that you can apply in your regular work, I will just be taking off all filters, and it will be written directly from my brain instead of my API research.</p>

<p>I work really hard doing what I do. I work seven days a week, with no regular paycheck or institutional support. I do make a decent living from partners who step up to support me (and pay their bills), but it is a hustle to keep doing what I do. It always grinds on me when folks with good jobs attack me like I have some sort of agenda in their game, or the no name bros come out of the wood work to question my credibility when they know nothing about me. I spend a lot of time filtering out what I’m really thinking, sticking to the facts, helping make my nuggets of API research wisdom more digestible, but this week ain’t one of them, so hold on. Once I feel better, I will go back to normal, and make sure I point out the good that is going on in the space, but in the current mode y’all ain’t worth it, and I’m going to unload and rant for the next 30 or 40 posts. Normally I’d keep this kind of stuff just on kinlane.com, but since it will all be API focused, I’m going to give you the uncensored version here on apievangelist.com. Enjoy!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/28/api-rant-vs-api-research/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/28/disaster-api-rate-limits/">Disaster API Rate Limit Considerations</a></h3>
        <span class="post-date">28 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/google-maps/google-maps-hurricane-harvey.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>This API operations consideration won’t apply to every API, but for APIs that provide essential resources in a time of need, I wanted to highlight an API rate limit cry for help that came across my desk this weekend. <a href="https://twitter.com/Pinboard/status/901658084002713600">Our friend over at Pinboard alerted me</a> to someone in Texas asking for some help in getting Google to increase the Google Maps API rate limits for an app they were depending on as Hurricane Harvey:</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Hey <a href="https://twitter.com/Google">@google</a> <a href="https://twitter.com/googlemaps">@googlemaps</a> <a href="https://twitter.com/GoogleMapsAPI">@googlemapsapi</a> can you please remove the limit on api access for <a href="https://twitter.com/ATXfloods">@atxfloods</a>? This is an emergency and we rely on it.</p>&mdash; Jen Savage (@savagejen) <a href="https://twitter.com/savagejen/status/901656342213033984">August 27, 2017</a></blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>The app they depended on had ceased working and was showing a Google Maps API rate limit error, and they were trying to get the attention of Google to help increase usage limits. As Pinboard points out, it would be nice if Google had more direct support channels to make requests like this, but it would be also great if API providers were monitoring API usage, aware of applications serving geographic locations being impacted, and would relax API rate limiting on their own. There are many reasons API providers leverage their API management infrastructure to make rate limit exceptions and natural disasters seems like it should be top of the list.</p>

<p>I don’t think API providers are being malicious with rate limits in this area. I just think it is yet another area where technologists are blind to the way technology is making an impact (positive or negative) on the world around us. Staying in tune to the needs of applications that help people in their time of need seems like it will have to components, 1) knowing your applications (you should be doing this anyways) and identifying the ones that have a public service, and 2) staying in tune with natural and other disasters that are happening around the world. We see larger platforms like Facebook and Twitter rolling out permanent solutions to assist communities in their times of needs, and it seems like something that other smaller platforms should be tuning into as well.</p>

<p>Disaster support and considerations will be an area of API operations I’m going to consider adding into my research, and spending more time to identify best practices, and what platforms are doing to better serve communities in a time of need using APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/28/disaster-api-rate-limits/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/28/this-weeks-troubling-api-patent/">This Weeks Troubling API Patent</a></h3>
        <span class="post-date">28 Aug 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/blog/Daimler_Reitwagen_color_drawing_1885%20%20DE%20patent%2036423%20-%20Basic%20original%20patent%20first%20motorcycle%20in%20the%20world.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I found myself looped into another API patent situation. I’m going to write this up as I would any other patent story, then I will go deeper because of my deeper personal connection to this one, but I wanted to make sure I called this patent what it is, and what ALL API patents are–a bad idea. Today’s patent is for <a href="http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=/netahtml/PTO/srchnum.html&amp;r=1&amp;f=G&amp;l=50&amp;s1=20170102925.PGNR.">an automatch process and system for software development kit for application programming interface</a>:</p>

<blockquote>
  <p>Title: Automatch process and system for software development kit for application programming interface<br /><br />
Patent# : US 20170102925 A1<br /><br />
Abstract: A computer system and process is provided to generate computer programming code, such as in a Software Development Kit (SDK). The SDK generated allows an application to use a given API. An API description interface of the system is operable to receive API-description code describing one or more endpoints of the API. A template interface is operable to receive one or more templates of code defining classes and/or functions in a programming language which can be selected by the selection of a set of templates. A data store is operable to use a defined data structure to store records of API description code to provide a structured stored description of the API. A code generation module is operable to combine records of API with templates of code which are arranged in sets by the language of the code they contain. The combining of records and code from templates may use pointers to a data structure which is common to corresponding templates in different sets to allow templates of selected languages to be combined with any API description stored.<br /><br />
Original Assignee: Syed Adeel Ali, Zeeshan Bhatti, Parthasarathi Roop, APIMatic Limited</p>
</blockquote>

<p>If you have been in the API space for as long as I have you know that the generation of API SDKs using an API definition is not original or new, it is something that has been going on for quite some time, with many open source solutions available on the landscape. It is something everyone does, and there are many services and tooling available out there to deliver SDKs in a variety of languages for your API. It is something that just isn’t patent worthy (if there was such a thing anymore). It shouldn’t exist, and the authors should know better, and the US Patent Office should know better, but in the current digital environment we exist in there isn’t a lot of sensibility and logic when it comes to business in general, let alone intellectual property.</p>

<p>I haven’t pulled any patents as part of my <a href="http://patents.apievangelist.com/">API patent research</a> in some time, so this particular patent hasn’t come across my radar. I was alerted to the patent via a Tweetstorm, shared with me in Slack by Adeel (one of the authors of the patent):</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">3 years after <a href="https://twitter.com/SwaggerApi">@SwaggerApi</a> codegen goes OSS, <a href="https://twitter.com/APIMatic">@APIMatic</a> patents… swagger-codegen. Douchbag <a href="https://twitter.com/hashtag/patent?src=hash">#patent</a> bs! <a href="https://twitter.com/OpenApiSpec">@OpenApiSpec</a> <a href="https://t.co/zLV8eNsHcQ">https://t.co/zLV8eNsHcQ</a></p>&mdash; Tony Tam (@fehguy) <a href="https://twitter.com/fehguy/status/900736518884425728">August 24, 2017</a></blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>In which my response was, “oh. ouch. well, gonna have to think on this one sir. I’ll let you know my response, but I’m going to guess you might not like how I feel either. I’m not pro-patent. let me simmer on it for a couple days and I’ll share my thoughts.”  As I was simmering I was pulled into the conversation again by Tony Tam:</p>

<blockquote class="twitter-tweet" data-conversation="none" data-cards="hidden" data-partner="tweetdeck"><p lang="en" dir="ltr">Avoiding the point. You patented something in OSS for years--ironically even used by the <a href="https://twitter.com/uspto">@uspto</a>.  I&#39;m sure <a href="https://twitter.com/kinlane">@kinlane</a> has thoughts on this</p>&mdash; Tony Tam (@fehguy) <a href="https://twitter.com/fehguy/status/901230276742848512">August 25, 2017</a></blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>In which my response was:</p>

<blockquote class="twitter-tweet" data-conversation="none" data-cards="hidden" data-partner="tweetdeck"><p lang="en" dir="ltr">I do! But too many for the Twitterz!</p>&mdash; Kin Lane (@kinlane) <a href="https://twitter.com/kinlane/status/901230878613012480">August 25, 2017</a></blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>I was taking my usual time, gathering my thoughts in my notebook. Taking walks. Simmering. Then I wake up Saturday morning to this Tweet:</p>

<blockquote class="twitter-tweet" data-conversation="none" data-cards="hidden" data-partner="tweetdeck"><p lang="en" dir="ltr">That looks like a sponsored story to me (given the issues with apimatic generated code). Is that the case? CC <a href="https://twitter.com/apievangelist">@apievangelist</a> <a href="https://twitter.com/kinlane">@kinlane</a></p>&mdash; H Zena (@hip_zena) <a href="https://twitter.com/hip_zena/status/901409442058588160">August 26, 2017</a></blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>The comment did get me to Tweet a couple times, but ultimately I’m not going to engage or deviate from my initial response to gather my thoughts and write a more thoughtful post. The machine wants me to respond emotionally, fractionally, and in ways that can be taken out of context. This is how the technology space works, and keeps you serving it’s overlords–the money folks behind.</p>

<p>I do have to admit, when I first responded I was going to take a much harder line against APIMATIC, but after seeing the personal attacks on Adeel, his attempts to defend himself, then this no presence Twitter account coming at me personally, bringing into question that maybe I was being paid to write articles I changed the tone of this story significantly. The conversation around this patents shows what is wrong with the business and politics of APIs, more than anything API patents have done to the community to date.</p>

<p>First, Adeel doesn’t understand the entire concept of what Tony meant when he said patent, any more than he grasps what Tony meant by douchbag. This isn’t a jab at Adeel. It’s truth. Adeel is a Pakistani, living in New Zealand. I’ve spent the last couple of years engaging with him in person, and virtually via hangouts, Slack, and email. I’m regularly having to explain some very western concepts to him, and often find myself think deeply about what I’m saying to make sure I am bridging things properly. Not because Adeel isn’t smart (he’s exceptionally smart), it is just because of the cultural divide that exists between him and I.</p>

<p>Adeel didn’t file their patent out of some competitive ambition, or stealing of open source ideas as referenced in the Tweetstorm. He did it because it is what the academic environment where APIMATIC was born encouraged it, and it is something that is associated with smart ideas by the institution. This concept isn’t unique to New Zealand, it is something that still flourishes in the United States. Where the cultural bridge was necessary, is when it comes to why patents are bad. In these academic environments, you have a good idea, you patent it. It is something that historically acceptable, and encouraged. <a href="http://apievangelist.com/2017/06/08/patents-as-a-measure-of-individual-success/">You see this across institutions and within enterprises around the globe, with patents as a measure of individual success</a>. Adeel and his team had a good idea, so they patented it, he didn’t think he was doing anything wrong.</p>

<p>Addeel isn’t being any more aggressive or vindictive <a href="http://apievangelist.com/2016/01/12/i-just-cannot-get-behind-api-patents-especially-when-they-apply-to-http-and-hypermedia/">than Sal or Matt were with their hypermedia patents</a>, or <a href="http://apievangelist.com/2016/03/25/the-unintended-consequences-of-api-patents/">Jon Moore</a> were with their patents. It is what their VCs, companies, and parent institutions encourage them to do with their good ideas (go after them and make your mark on the world). I fault all of these individuals just as much as I fault Adeel, or even Tony for that matter, when it comes to the name of an open source product suddenly becoming a trademarked product. Ironically this conversation is going up, <a href="http://apievangelist.com/2017/08/22/thank-you-tony/">right after a post regarding how much I respect Tony for his work</a>, despite me be VERY upset about Swagger not remaining attached to an open source product we all had contributed so much to, and helped spread the word about. If we are worried about the sanctity of open source, we should be defending all dimensions of intellectual property. I know, not a perfect comparison, but I imagine Tony feels similar to what I did when this happened. Which I let him know via email, but have never gone after anyone individually, or personally, sicking my dogs on him, although I’m sure it might have felt that way.</p>

<p>All of this makes me think deeper about the relationship between open source and APIs. <a href="http://apievangelist.com/2015/12/02/what-licensing-should-i-be-considering-when-i-take-open-source-software-and-offer-up-as-an-api/">What are the responsibilities of companies who wrap open source technology with an API and offer a commercial service?</a> How does licensing, trademarks, patents, and other intellectual property concerns need to be respected? I’m guessing I could go through many of the API patents in my research and find thatmost filings that did not consider or respect open source offerings that came before them. People had a good idea. They were in an environment that encourages patents, and they filed for a patent to show their idea was worthwhile–that USPTO stamp of approval is widely recognized as a way to acknowledge your idea is worthy.</p>

<p>I want to make clear. I am not defending patents. I am defending Adeel. I am doing so strongly, because of the no name person who decided to chime in and question the credibility of my API storytelling. Otherwise I would have laid things out, and told Adeel he needed to learn the lesson and move on. I just saw everyone piling on Adeel like he was a bad person, and some tech bro just patenting things to get the upper hand. I have known Adeel for years, and know that is not the situation. Once I started getting piled on as well, then it switched things into personal mode for me, and now I feel the need to defend my friend, but not his actions. The tech community loves to pile on, and I deal with wave after wave of tech bros who don’t read my work and accuse me of things that people who DO read my work would NEVER acuse me of. Adeel is my friend. Adeel is an extremely intelligent, honorable, kind-hearted person, and it pissed me off when people started piling on him.</p>

<p>Now that you understand my personal relationship, let’s address my business relationship. I am an advisor to APIMATIC. Not because I’m paid, or there are stock options, it is because he is my friend. In August (couple of weeks ago), Adeel and his investors had offered me a 0.25% share capital in the company but there has been no paperwork, nothing signed, and no deals made as of today. Honestly, after the $318 check I got for my latest advisory role, which I spent about 60 hours of work on, and travel costs out of my own pocket, I have little interest in pushing the conversations forward to the point where things ever get formalized. It just isn’t a priority for me, and damn sure has never influenced my writing about APIMATIC, let alone a story from June of 2015, like the Tweetstorm participant accused me of, without doing any due diligence on who I am. You are always welcome to ask ANY of my partners if I take money to write positive things about their products, or guest posts, you’ll get the same answer from all of them–it is something I get asked regularly, and just don’t give a shit about doing.</p>

<p>API patents are a bad idea. Patent #US 20170102925 A1, for an automatch process and system for software development kit for application programming interface is a bad idea. API patents are a bad idea because people think they are a sign of being smart when they are not. API patents are a bad idea because corporations, institutions, and organizations keep telling people it is a sign of being smart, and people believe it. API patents are a bad idea because the entire US Patent system is broken, because it is an intellectual property relic of a physical age, being leveraged in a digital realm where things are meant to be interoperable and reusable. API patents are bad idea because y’all keep doing them thinking they are a good idea, when they just open you up to being a tool that allows corporations to lock up ideas, and provide a vehicle for others to use them as leverage in a court of law, and behind closed doors negotiations. <a href="http://apievangelist.com/2016/09/02/after-looking-through-23414-api-patents-i-think-it-will-just-come-down-to-who-litigates/">Ultimately whether or not patents are bad will come down to who litigates with their patent portfolios, and the patents they acquire</a>. The scariest part is most of this leveraging and strong-arming won’t always happen in a public court, it will happen behind closed doors in arbitration, and with venture capital negotiations.</p>

<p>Which really brings me to the absurdity of this latest patent Tweetstorm. I’m all for showcasing that patents are a bad idea, and even shaming companies and individuals for doing them. However, I saw this one get personal a couple of times, and even took a jab at me. Y’all really shouldn’t do this shit in glass houses, because patents are just part of your intellectual property problems. The NDAs y’all are signing are a bad idea. Those deals you are making with VCs are mostly bad ideas. Y’all are just as ignorant, or maybe in denial about the deals you are making as Adeel is about the negative impacts of the US patent system. Most of these deals you make will result in your startup being wielded in more damaging ways than Adeel’s patent ever will. Yet you keep doing startups, signing deals, and attacking your competition, and people like me just investing in the community. Adeel is currently reading about patents, which I regret to say won’t provide him with the answers he is looking for. There are few books on the subject. <a href="https://www.amazon.com/s/ref=nb_sb_noss_1?url=search-alias%3Daps&amp;field-keywords=lawrence+lessig">Maybe reading something from Lawrence Lessig might get you part of the way there</a>. The real answers you are looking for come from experience, and that is what you just received. This was an easy lesson, just wait until APIMATIC acquired by a bigco, and see your patents wielded in ways you never imagined–those will be some harder lessons.</p>

<p>I don’t have a problem with calling out Adeel for APIMATIC’s patent–it is what should happen. I have a problem with him being called douchbag, and the other personal attacks, and assumptions on his (and my) character that occurred within the Tweetstorm. By all means, call him out on Twitter. By all means, educate him around the damage to OSS and API community by doing patents. Don’t make these things personal, assume malicious intent, and recklessly begin questioning the character of everyone involved. Also, if you are just spending your time calling out you competitors for this behavior, because it bothers you, and you don’t call out your partners, and the companies you work for, and the services you use for their abusive use of OSS and damaging intellectual property claims, you have significantly weakened your argument, and won’t always be able to count on me to jump into the discussion and have your back. I do this shit full time, with no financial backing, corporate or institutional cover. I’m out here full time, not hiding behind a Twitter handle, holding folks accountable whenever and wherever I can. If you are doing a startup, learn from this story. Educate your institutions, companies, and investors about how API patents are damaging everything we are doing, and never will actually demonstrate you have a novel idea, it is just locking up other ideas.</p>

<p>In the end, everything we know as API is already patented. <a href="http://patents.apievangelist.com/">Look through the thousands of API patents in my research</a>. Hell, <a href="https://apievangelist.com/2017/06/06/patent-api-descriptions/">Microsoft already has the patent on API definitions</a>, so what does it matter if there is a patent on generating SDKs from API definitions? Also, <a href="http://apievangelist.com/2015/01/25/doing-the-research-in-preparation-for-my-patent-on-a-patent-api/">my patent on API patents is pending</a>, so it will all be game over then. Mwahahaha!! ;-) As Tony said, the patent system is broken. Let’s keep letting our companies, organizations, institutions, and most importantly the USPTO know that it is broken. Let’s keep calling out folks who still think API patents are a good idea (sorry Adeel), and schooling them on the why, but let’s not be dicks about it. Let’s not assume people have bad intentions. Let’s understand the history of patents and that many people are still taught that they are a sign of having good ideas and being what you do as part of regular business operations. Let’s just make sure we also lead by building a better API service or tooling, as Adeel brought up in his defense. Also, when he said this, he wasn’t making this an APIMATIC is better than Swagger Codegen (or others), he is just focused on making a better product in general. I’m sorry but he has. Y’all can focus on the merits of Swagger Codegen vs. APIMATIC, but what he’s done with the SDK docs, portal, and continuous integration, ARE great improvements. Much like the commercial service Swagger has become (ya know, the Swagger in Swagger Codegen) and evolved on top of the open source API specification and tooling Tony set into motion for ALL of us to build upon–thanks again Tony.</p>

<p>P.S. I wouldn’t have been so hard on Tony if I hadn’t been looped in to defend intellectual property and OSS like this.<br />
P.S.S. I wouldn’t have been so easy on Adeel, if no name McTwitter account had accused me of selling out when I don’t do that.<br />
P.S.S.S. Adeel, patents are bad because people use them in bad ways, and the US Patent Office is underfunded, understaffed, and can’t tell what is good or bad patents–this is the way bigcos want them.<br />
P.S.S.S.S. Sorry I’m such an asshole everyone, but I hope y’all are getting somewhat used to it after seven years.<br /></p>

<p>**Disclosure: **I am an advisor to APIMATIC (very proud of it)!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/28/this-weeks-troubling-api-patent/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/22/county-level-marijuana-regulation-in-california-using-apis/">County Level Marijuana Regulation In California Using APIs</a></h3>
        <span class="post-date">22 Aug 2017</span>
        <p><a href="http://www.ukiahdailyjournal.com/article/NP/20170810/NEWS/170819962"><img src="https://s3.amazonaws.com/kinlane-productions/weed/mendocino-county-split-from-state-on-cannabis+track-and-trace.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>Counties across the State of California are scrambling to get everything in order now that marijuana is legal, and <a href="http://www.ukiahdailyjournal.com/article/NP/20170810/NEWS/170819962">the 3rd party vendors working with the state are using an API to try and bridge the regulatory needs of each county,</a> as they look to regulate the brand new industry. It sounds like the marijuana regulatory API isn’t 100% ready for prime time, but it is interesting to hear that state is looking to “mitigate the burden of counties” when it comes to production of marijuana using APIs.</p>

<p>I have been curating news about APIs in use across the growing marijuana industry, but this is the fist story I’ve written on the subject. Now that I’m seeing APIs use as part of the regulatory engine for the industry, things are getting a little more real, and not just be about finding seeds, stores, and other industry data. I’ll keep scratching around to see what I can find out about the software vendors mentioned in the article, and see if I can get my hands on any documentation, or a link to any active portal. I’m curious to see where this marijuana regulatory API train is headed.</p>

<p>Since the marijuana industry is a completely new one for cities, counties, and states to manage, there is an opportunity to leverage new technology like APIs as part of the interactions between government entities, with the help of 3rd party providers. <a href="https://apievangelist.com/2017/05/05/taxation-on-public-data-via-the-api-management-layer/">Maybe there is even some opportunity for revenue generation on top of these APIs, allowing for government to fund the software development in a way that it could also be used across other government systems</a>. Push things forward with the rollout and expansion of the marijuana industry, and use that to fund other government systems that lack the funds, and are often years behind. The problems states face in working with counties doesn’t stop with this new marijuana industry, and there are so many other aspects of government operations that could benefit from APIs helping “mitigate the burden of counties” trying to comply with state laws and regulations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/22/county-level-marijuana-regulation-in-california-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/22/looking-at-facebook-blueprint-as-i-study-api-training-programs/">Looking At Facebook Blueprint As I Study API Training Programs</a></h3>
        <span class="post-date">22 Aug 2017</span>
        <p><a href="https://www.facebook.com/blueprint"><img src="https://s3.amazonaws.com/kinlane-productions/facebook/facebook-blueprint-screenshot.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I am preparing a training section of my API Evangelist research, and part of the process involves learning about what other API providers and API service providers are up to in this area. On my list to look through is <a href="https://www.facebook.com/blueprint">Facebook Blueprint</a>, their training area for the platform. The courses present there aren’t specifically for the Facebook API, and is targeting primarily business uses, but the approach translates to API focused training materials, and showcases what is a priority for Facebook when it comes to educating their platform consumers.</p>

<p>As part of my API training research I want to understand the building blocks employed by Facebook so that I can apply as part of my API Evangelist training efforts, and help other API providers and service providers apply as part of their operations as well. Here are a few of the API training building blocks I found present:</p>

<ul>
  <li><strong>Courses</strong> - A variety of online courses that teach you about everything Facebook.</li>
  <li><strong>Webinars</strong> - The webinars they provide around the content they are publishing.</li>
  <li><strong>Live</strong> - The live, in person workshops and courses they provide around the world.</li>
  <li><strong>Case Studies</strong> - Case studies of companies who have used Facebook courses.</li>
  <li><strong>Press</strong> - Press about the Facebook Blueprint, and how they are spreading the word.</li>
  <li><strong>Certifications</strong> - Facebook specific certifications that you can archive.</li>
  <li><strong>Exams</strong> - The tests that are available around the facebook courses.</li>
  <li><strong>How it Works</strong> - Some details about how Facebook training works.</li>
  <li><strong>Policies</strong> - The legal side of things, covering all of our bases.</li>
  <li><strong>FAQ</strong> - Some of the frequently asked questions around the training platform.</li>
  <li><strong>Support &amp; Help</strong> - Where you can get support and more help when it comes to training.</li>
</ul>

<p>Facebook breaks down their training into course categories and learning paths, providing two main ways for potential students to find what they are looking for. Facebook Blueprint provides…well, a blueprint that other API providers and service providers can consider when pulling together your trainings for your API training. Providing online, and offline courses is just the start, and as Facebook shows, there are some additional building blocks that need to be present for it to work.</p>

<p>I have some other API platform training areas to go through before I settle in on a strategy for my own API Evangelist training efforts. Ultimately I want my approach to become a blueprint that my customers can put to work. I’m even looking to turn my API training blueprint into a course by itself, so that I can help some of my customers with their training efforts, and even train the trainers in some situations, providing them with course content, and the awareness of the API space they need to be successful delivering courses within their company, organizations, institution, and government agency. In 2018, I’m investing in my storytelling on API Evangelist, as well as industry guides, white papers, and training material for my readers, and privately wherever it is needed.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/22/looking-at-facebook-blueprint-as-i-study-api-training-programs/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/22/considering-how-machine-learning-apis-might-violate-privacy-and-security/">Considering How Machine Learning APIs Might Violate Privacy and Security</a></h3>
        <span class="post-date">22 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/290x195cloudsecurity2014.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://gearsofbiz.com/carbon-black-edr-service-exposing-customer-data-through-cloud-scanning-update/7096">I was reading about how Carbon Black, an endpoint detection and response (EDR) service, was exposing customer data via a 3r party API service they were using</a>. The endpoint detection and response provider allows customers to optionally scan system and program files using the VirusTotal service. Carbon Black did not realize that premium subscribers of the VirusTotal service get access to the submitted files, allowing an company or government agency with premium access to VirusTotal’s application programming interface (API) can mine those files for sensitive data.</p>

<p>It provides a pretty scary glimpse at the future of privacy and security in a world of 3rd party APIs if we don’t think deeply about the solutions we bake into our applications and services. Each API we bake into our applications should always be scrutinized for privacy and security concerns, making sure end-users aren’t being subjected to unnecessary situations. This situation sounds like it was both API provider and consumer contributing to the privacy violation, and adjusting platform access levels, and communicating with API consumers would be the best path forward.</p>

<p>Beyond just this situation, I wanted to write about this topic as a cautionary tale for the unfolding machine learning API landscape. Make sure we are thinking deeply about what data and content we are making available to platforms via artificial intelligence and machine learning APIs. Make sure we are asking the hard questions about the security and privacy of data and content we are running through machine learning APIs. Make sure we are thinking deeply about what data and content sets we are running through the machine learning APIs, and reducing any unnecessary exposure of personal data, content, and media.</p>

<p>It is easy to be captivated by the magic of artificial intelligence and machine learning APIs. It is easy to view APIs as something external, and not much of a privacy or security threat. However, with each API call we are inviting a 3rd party API into our databases, files, and other private systems. Let’s make sure we have an honest conversation with our API providers about how data and content is accessed, stored, cached, and used as part of any AI or ML process. Let’s make sure we get clarification on which partners, or other 3rd party providers are getting access to data and content that is indexed and executed as part of AI and ML API requests and responses. How long are videos or images stored? How long is data stored?</p>

<p>I’m seeing more discussion around dependencies going on in the API space. Which software libraries, and APIs are we depending on for our applications and services. I’m feeling like this conversation is going to continue expanding and security, privacy, and observability is going to become a more significant part of these dependency discussions. It will be a conversation that continues to push API deployment on-premise, and on-premise, being observable about how ML and AI API operations are being logged, stored, and track on. I’m going to keep watching how APIs are intentionally or unintentionally violating security and privacy like this, and keep an eye on the API dependency conversation to see how it evolves as part of this security and privacy discussion.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/22/considering-how-machine-learning-apis-might-violate-privacy-and-security/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/22/thank-you-tony/">Thank You Tony</a></h3>
        <span class="post-date">22 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/tony-tam.jpg" align="right" width="25%" style="padding: 15px;" /></p>
<p>Tony Tam, the creator of the OpenAPI specification, formerly known as Swagger, <a href="https://www.linkedin.com/pulse/i-say-goodbye-swagger-smartbear-openapi-whats-next-tony-tam">has announced he will be exiting his role at OAI and SmartBear</a>. Tony says the specification is in good hands with Ron Ratovsky (<a href="https://twitter.com/webron">@webron</a>), Darrel Miller (<a href="https://twitter.com/darrel_miller">@darrel_miller</a>), and others in the OAI. Tony doesn’t give any hints about what he’ll be up to, but will be walking away from his baby entirely.</p>

<p>I have given Tony a hard time during the transition from Wordnik to SmartBear, and the creation of the OpenAPI, but I am a huge fan of what he has done, and super bummed to see him go–hoping he won’t leave the API community completely. There are many building blocks that go into doing APIs and OpenAPI, or Swagger, is the most significant single building block that has emerged in the seven years I’ve been doing API Evangelist. Swagger has had a profound impact on the world of APIs, and OpenAPI will continue doing this in the future, if the right conditions are still present across the API landscape.</p>

<p>Swagger has helped us talk about our APIs. Swagger has helped us collaborate around our APIs. Swagger has opened up a whole lifecycle of API tooling to help us along our journey. I always felt like Swagger reflected Tony’s personality, and with it’s evolution to OpenAPI, and the OpenAPI Initiative means it’s grown beyond it’s creator. OpenAPI is in good hands. I think it is a good time for Tony to step away, and feel like his baby has begun to grow up, becoming much bigger than what he can do on his own (even with Ron’s amazing help).</p>

<p>Thank you for all your work Tony. You made your mark on the API space. You managed to develop something that was useful for API documentation and code generation, but quickly became about design, testing, monitoring, and every other stops along the API lifecycle. I am stoked to have had the chance to work with you, and spend time telling stories about your important work. I hope you find some time to read some good books, and take time for yourself, and hopefully you don’t go to far from the API space, or at least come back and visit from time to time.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/22/thank-you-tony/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/21/the-first-question-when-starting-an-api-is-always-should-we-be-doing-this/">The First Question When Starting An API Is Always: Should We Be Doing This?</a></h3>
        <span class="post-date">21 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/supreme-court-statues.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was doing some more work on my list of potential female speakers from the API space. I have some slots to fill for @APIStrat, and I saw another API event was looking for suggestions when it came to speakers. A perfect time to invest some more cycles into finding female API talent. Twitter and Github is always where I go for discovery. I picked up where I left off working on this last time, turned on my search tools that use the Twitter and Github API, and got to work enriching the algorithm that drives my API talent search.</p>

<p>Next up on my task list was to <a href="https://api-evangelist-data.github.io/names/">deploy a name microservice,</a> that would help me filter Twitter and Github users by gender. I’m interested in API folks of all type, but for this round I need to be able to weight by female. <a href="http://names.mongabay.com/">I found a list of the top names from the United States which had them broken down by gender</a>. I copied and pasted into a Google Sheet, fired up a Github repository, and published the spreadsheet of data to Github as YAML–giving me a <a href="https://github.com/api-evangelist-data/names/blob/master/_data/male.yaml">male.yaml</a>, and <a href="https://github.com/api-evangelist-data/names/blob/master/_data/female.yaml">female.yaml</a> listing of names. I will be be use these names in a variety of web and API applications, but I wanted to be able to help filter any search results by a female name for this project. I understand the limitations of this approach, but it is good enough for what I am looking to accomplish today.</p>

<p>Next, I use <a href="https://github.com/api-evangelist-data/names/blob/master/_data/female.yaml">my new name microservice</a> as a filter for any Twitter or Github account I’m paying attention to as part of my API monitoring. Quickly giving me a list of accounts to look through as I am developing my list of women doing interesting things with APIs. Once I’m done I have a list of Twitter accounts, and Github accounts, I prepare them as a Google Sheet, then get ready to publish the YAML within a Github repository as my API women microservice. Then I pause. Should I be doing this? In this current online environment do I want to be building out lists of women, without their consent? Sure their Twitter and Github accounts are public, but should I be singling them out in an easy to access list? IDK.</p>

<p>I am doing this work for my @APIStrat conference, and I want to share the information easily with other conference folks. I also want to showcase women doing interesting things in the API space. However, I don’t want to help automate the targeting and harassment of these women. By publishing a machine readable list of their Twitter and Github accounts I’m doing the heavy lifting for any potential online troll. I’m going to simmer on this one for a week. Luckily I can easily publish the list as a private repository, and share with anyone who asks using Github, by just adding them as collaborator on the repository. This situation has reminded me that with each microservice that I publish I should be pausing and always asking myself if I should be doing this in the first place. What are the possibilities for abuse? Am I potentially making someone unsafe with the service I publishing?</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/21/the-first-question-when-starting-an-api-is-always-should-we-be-doing-this/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/21/making-sense-of-api-activity-with-webhook-events/">Making Sense Of API Activity With Webhook Events</a></h3>
        <span class="post-date">21 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/border-traffic.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2017/08/15/where-to-begin-with-webhooks-for-the-human-services-data-api/">I was doing some webhooks research as part of my human services work</a> and I found myself studying the types of events used as part of webhook orchestration for <a href="https://developer.github.com/webhooks/">Github</a>, <a href="https://developer.box.com/reference#event-triggers">Box</a>, <a href="https://stripe.com/docs/api#event_types">Stripe</a>, and <a href="https://api.slack.com/events/api">Slack</a>. Each of the event type lists for each of these platforms tell a lot about what is possible with each API, and the webhooks that get triggered as part of these events show what is important to developers who are integrating with each of these APIs. These event type lists really help make sense of the API activity for each of these APIs, providing a nice list to follow when developing your integration strategy.</p>

<p>What I really like as I look through each of these webhook event lists is that they are usually in pretty plain language, describing events that matter, not just row updates with a timestamp. These events can be very broad, triggering a webhook when anything happens to a resource, or it could be granular and be all about a specific type of change, or anything in between. Each event type represents something API consumers want from an API, and would be normally polling the API for, but since there are webhook events, developers can get a push of data or a ping whenever an event occurs.</p>

<p>Another thing that the presence of webhooks, and a robust list of events represent for me is the maturity of a platform. Github, Box, Stripe, and Slack are all very mature and robust platforms. There are meaningful events defined, and the platform behaves as a two way street, accepting requests, but also making requests whenever a meaningful event occurs. I’m getting to a place where I feel like basic webhook infrastructure should be default for all API providers. The problem is I don’t think there are well enough defined models for API providers to follow when they are planning this part of their API operations. Something we will need to tackle as an industry pretty soon, helping make sure webhooks behave in consistent ways, and have a standard API interface for orchestrating with.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/21/making-sense-of-api-activity-with-webhook-events/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/21/api-foraging-and-wildcraft/">API Foraging And Wildcraft</a></h3>
        <span class="post-date">21 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/drone-recovery/diamond04.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was in Colorado this last week at a CA internal gathering listening to my friend Erik Wilde talking about APIs. One concept he touched on was what he called API gardening, where different types of API providers approached the planting, cultivating, and maintenance of their gardens in a variety of different ways. I really like this concept, and will be working my way through the slide deck from his talk, and see what else I can learn from his work.</p>

<p>As he was talking I was thinking about a project I had just published to Github, which leverages the Google Sheets API, and the Github API to publish data as YAML, so I could publish as a simple set of static APIs. I’d consider this approach to be more about foraging and wildcrafting, then it is about tending to my API garden. My API garden just happens spread across a variety of services, often free and public space, in many different regions. I will pay for a service when it is needed, but I’d rather tend smaller, wilder, APIs, that grow in existing spaces–in the cracks.</p>

<p>I use free services, and build upon the APIs for the services I am already using. I publish calendars to Google Calendar, then pull data and public APIs using the Google APIs. I use the Flickr and Instagram APIs for storing, publishing, sharing, and integrating with my applications using their APIs. I pull data from the Twitter API, Reddit API, and store in Google Sheets. All of these calendar, image, messaging, and link data will ultimately get published to Github as YAML, which then is shared as XML, JSON, Atom, CSV via static APIs. I am always foraging for data using public services, then planting the seeds, and wildcrafting other APIs–in hopes something will eventually grow.</p>

<p>Right now Github is my primary jam, but since I’m just publishing static JSON, XML, YAML, and other media types, it can be hosted anywhere. Since it’s Github it can be integrated anywhere, directly via the static API paths I generate, or by actually cloning the underlying Github repository. I have a Jekyll instance that I can run on AWS EC2 instance or Docker container, and will be experimenting more with Dropbox, Google Drive, and other free, or low cost storage solutions that have a public component. I’m looking to have a whole toolbox of deployment options when it comes to publishing my data and content APIs across the landscape. Most of these are low use, long tail usage data and content I’m using across my storytelling, so I’m not worried about things being heavily manicured, just providing fruit in a sustained way for as low of a price as I possibly can.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/21/api-foraging-and-wildcraft/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/21/api-deployment-comes-in-many-shapes-and-sizes/">API Deployment Comes In Many Shapes And Sizes</a></h3>
        <span class="post-date">21 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-deploy.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>Deploying an API is an interesting concept that I’ve noticed folks struggle with a little bit when I bring it up. My research into API deployment was born back in 2011 and 2012 when my readers would ask me  which API management provider would help them deploy an API. How you actually deploy an API varies pretty widely from company to company. Some rely on gateways to deploy and API from an existing backend system. Some hand-craft their own API using open source API frameworks like <a href="https://s3.amazonaws.com/kinlane-productions/partners/tyk-logo.png">Tyk</a> and deploy alongside your existing web real estate. Others rely on software as a services solutions like <a href="http://apis.how/5ytnitnakm">Restlet</a> and <a href="http://apis.how/bgdteovduo">Dreamfactory</a> to connect to a data or content source and deploy an API in the clouds.</p>

<p>Many folks I talk with simply see this as developing their APIs. I prefer to break out development into define, design, deploy, and then ultimately manage. In my experience, a properly defined and designed API can be deployed into a variety of environments. The resulting OpenAPI or other definition can be used to generate server side code necessary to deploy an API, or maybe used in a gateway solution like AWS API Gateway. For me, API deployment isn’t just about the deployment of code behind a single API, it includes the question about where we are deploying the code, acknowledging that there are many places available when it comes to deploying our API resources.</p>

<p>API deployment can be permanent, ephemeral, or maybe just a sandbox. API deployment can be in a Docker container, which by default deploys APIs for controlling the underlying compute for the API you deploying. Most importantly, API deployment should be a planned, well-honed event. APIs should be able to be redeployed, load-balanced, and taken down without friction. It can be easy to find one way of deploying APIs, maybe using similar practices surrounding web deployments, or dependent on one gateway or cloud service. Ideally, API deployment comes in many shapes and sizes, and is something that can occur anywhere in the clouds, on-premise, or on-device. Pushing our boundaries when it comes to which platforms we use, which programming languages we speak, and what API deployment means.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/21/api-deployment-comes-in-many-shapes-and-sizes/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/18/http-status-codes-and-the-politics-of-apis/">HTTP Status Codes And The Politics Of APIs</a></h3>
        <span class="post-date">18 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/runscope/runscope-200-ok.jpeg" align="right" width="40%" style="padding: 15px;" /></p>
<p>The more I learn about the world of APIs, the more I understand how technology, business, and politics are all woven together into one often immovable monolith. Many things in the world of APIs seem purely like a technical thing, but in reality they are wrapped in, and wielded intentionally and unintentionally as part of larger business, and sometimes a personal agenda. An example of this can be found with the presence, or lack of presence with HTTP status codes, which the default status is usually 200 OK, 404 not found, or 500 internal error.</p>

<p>While these seem like very granular technical details of whether or not an HTML, XML, CSV, or JSON document is returned or not as part of a single web request, there usage often dictates what is happening behind the firewall, and often times more importantly, what is not happening. I find people’s awareness that HTTP status codes exist (or not) a significant sign of their view of the wider web world. If they are aware they exist they most likely have some experience engaging with other experienced partners using the web. If they don’t, they most likely live a pretty isolated existence–even if they do have a web presence.</p>

<p>Beyond just knowing that HTTP status codes exist, understanding the importance of, and the nuance surrounding each individual one demonstrates you are used to engaging with external actors at scale, leveraging web technology. I have to put out there that I DO NOT have an intimate knowledge of all HTTP status codes, because I have not exercised them as part of large scale projects, but it is something I do grasp the scope and importance of from the projects I have worked on. This is not a boolean thing, you knowing HTTP status codes or not. This is the result of many journeys, with many partners, across many different types of digital resources. You can tell how many journeys someone has been on, based on how they view, and wield HTTP status codes.</p>

<p>I encounter folks in my journeys who are dismissive of my focus on HTTP status codes, but I find most of these folks to be purposely vague in the signals they regularly send, and are used to keeping things complex, inconsistent, or just never very well defined so that they can keep things suiting their changing desire and needs. This is something that works well internally, and within a small group of trusted partners, but it rarely is conducive to doing business at scale on the web. Some companies thrive in chaos, because they have the upper hand. They have defined the chaos, and benefit from being in the know of how the chaos works, and invest heavily in keeping a gap between themselves, and those who are not in the know. Forcing external entities to always be on their toes when it comes to understanding what is going on, unable to truly ever know if things are indeed 200 OK, or most likely just a 500 internal server error.</p>

<p>In this situation I always think of my friend Darrel Miller (<a href="https://twitter.com/darrel_miller">@darrel_miller</a>) who as an API architect can recite every single HTTP status code, and a usage scenario because of his vast knowledge of APIs and web standards. However, a side effect of this reality is that if you ask Darrel almost any question, you will get a real honest, direct, and usually very precise answer. Some folks might look at this as Darrel not having filters, or often possessing too many opinions, but I’m a big fan of his approach. With Darrel, you rarely ever unclear about his motivations, feelings, and experience with any topic. I find this world much easier to navigate than the vague, unclear, often purposely obfuscated responses I get from some companies, organizations, institutions, and government agencies I work with. When doing business at scale on the web it always helps to provide clear signals that have a shared meaning, and being precise and upfront with what is going on behind the scenes, not hiding behind the black, white, and grey nature of just 200, 404, and 500.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/18/http-status-codes-and-the-politics-of-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/17/the-patent-application-information-retrieval-bulk-data-api/">The Patent Application Information Retrieval Bulk Data API</a></h3>
        <span class="post-date">17 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/uspto/uspto-pair-bulk-data-api.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I stumbled across <a href="https://pairbulkdata.uspto.gov/">the Patent Application Information Retrieval Bulk Data API</a> from the US Patent Office the other day. It provides a much more usable approach to getting at patent information than what I am using at the moment. Right now I am downloading XML files and searching for the occurrence of a handful of keywords. If I want to make a change I have to fire up a new AWS instance, change the code, and reprocess the downloaded files. The Patent Application Information Retrieval Bulk Data API gives me a much more efficient interface to work with.</p>

<p>The Patent Application Information Retrieval Bulk Data API contains the bibliographic, published document and patent term extension data tabs in Public PAIR from 1981 to present, with some additional data dating back to 1931. It has leveraged COTS semantics, maintains an open architecture, and the query syntax follows the standard Apache Solr search syntax, with API responses following the Solr formats. Providing for a much more powerful interface for querying patent data, which goes back further back in time then what I’ve been doing currently. I’m really interested in doing an API patent search for the 1990s, or maybe even earlier.</p>

<p>The Patent Application Information Retrieval Bulk Data API is a well designed API, <a href="https://pairbulkdata.uspto.gov/swagger/index.html">with an attractive API portal and documentation</a>, driven with an OpenAPI. The USPTO provide access to the patent data in the way that I think all government agencies should be doing it. You can use the API, or get at a JSON or XML download of the data. The API is “part of the US Patent and Trademark Office’s (USPTO) commitment to fostering a culture of open government as described by the 2013 Executive Order to make open and machine-readable data the new default for government information.” Which is pretty cool in my book, something we desperately are needing to become a reality across all federal agencies in 2017.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/17/the-patent-application-information-retrieval-bulk-data-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/17/testing-out-the-concept-of-api-transit-instead-of-api-lifecycle/">Testing Out The Concept Of API Transit Instead Of API Lifecycle</a></h3>
        <span class="post-date">17 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/talks/november-2015/subway-map-15.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>It isn’t often that I make up acronyms, terms, or phrases. <a href="https://apievangelist.com/2013/02/27/merging-api-automation-and-interoperability-into-api-reciprocity/">For a number of years I pushed forward the concept of API reciprocity</a>, but eventually conceded to the notion of integration platform as a service (iPaaS). Even with this failure I’m playing around with the evolution a new concept around how we map out our entire API operations, an area we commonly call the API lifecycle, but I’m exploring with calling it API transit.</p>

<p>When I think about the API lifecycle I am regularly reminded that it is something that is rarely a linear thing from define to deprecation, or even something that goes round and round in a circular format as the name lifecycle implies. This always brings me back to <a href="http://apievangelist.com/2014/12/01/my-turkey-holiday-project-a-subway-map-api/">my API subway map work</a>, with the development of <a href="http://subway.map.apievangelist.com/">my subway map API</a>, and <a href="https://apievangelist.com/2015/11/29/the-api-lifecycle-my-talk-from-defrag-and-apistrat/">the keynote I gave on the subject at APIStrat in Austin</a>. The subway map concept provides a pretty comprehensive way to map out the unlimited directions in which an API operations might take, and I’m looking to see if it can handle both the provider side needs, as well as that of the API consumer.</p>

<p>While I like the subway map analogy, I really like the more general concept of API transit. The concept of an APi subway map seems one dimensional, where API transit seems like it could handle multiple dimensions. I’m not sure there is a fit here, but I wanted to explore the definition of transit, as well as use the phrase in some API storytelling to test out a little bit, and see if it even is coherent. Ok, so what do my friends over at <a href="https://developer.oxforddictionaries.com/documentation">the Oxford Dictionaries API</a> have to say on the subject?</p>

<p>etymologies:</p>

<ul>
  <li>late Middle English (denoting passage from one place to another): from Latin transitus, from transire ‘go across’</li>
</ul>

<p>definitions:</p>

<ul>
  <li>the carrying of people or things from one place to another</li>
  <li>the conveyance of passengers on public transport.</li>
  <li>the action of passing through or across a place</li>
</ul>

<p>I like the idea that API transit could be about the process of carrying people (users data) or things (anything digital) from one place to another – with an API transmit map helping visualize this. I like that the map can be used by API providers as a guide for all the stops along the API lifecycle, but instead of being linear or circular, it can be all the above. Each API transit line could visualize an API lifecycle, and help API providers deliver consistently along each layer of API operations. The same maps, and lines can also be used to help API consumers navigate all API resources available via any API transit district.</p>

<p>“Denoting passage from one place to another” could be applied to training API providers about what is now called the API lifecycle, but could also be a series of journeys along pre-specified API transit lines, teaching them about concepts around API design, deployment, management, testing, and other aspects of API operations. These same lines can be used to guide each stop along the API lifecycle, helping act as an assembly line for delivering, maintaining, and even deprecating APIs. Once APIs make it to production, the same API transit map can be used to help engage with API consumers, helping move the people and their things from one place to another.</p>

<p>I’m not 100% sold on the concept, and it is something I’m not sure I will keep using. I do want to invest more time into my API subway map, which I’m going to rebrand as API transit map for the next wave of development. I have the API working, I just need to get the routing to work properly, so that it will lay down each of my API transit lines in a coherent way, allowing them to overlap, and work with and around each other. I have a model in my head for how I can use to help teach folks about key concepts of API operations from definition to deprecation. I also have a model for how it can guide the delivery and management of APIs, acting as a map and framework for managing API operations. I just don’t have the API consumer portion of the equation. I’m guessing it will take me another couple years before this comes into focus for me, but I still enjoy working on the concept, and pushing it forward a little bit each year.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/17/testing-out-the-concept-of-api-transit-instead-of-api-lifecycle/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/17/why-arent-there-more-api-focus-agencies/">Where Are All The API Focused Agencies?</a></h3>
        <span class="post-date">17 Aug 2017</span>
        <p><a href="https://goodapi.co/"><img src="https://s3.amazonaws.com/kinlane-productions/good-api/good-api-agency.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="http://apievangelist.com/2017/08/16/the-importance-of-api-stories/">Earlier this week at the CA API Academy virtual gathering I spoke at in Boulder CO</a>, the question around why there aren’t more API focused agencies came up. We were talking about the need for consulting services around common areas of API operations like design, deployment, management, testing, as well as training around API lifecycle related topics. We are seeing some movement in the area of API focused agencies, but not enough to cover the current demand.</p>

<p>We are seeing full service shops like <a href="http://apivista.com/about/">APIvista</a>, and <a href="https://goodapi.co/">Good API</a> emerge. There is also movement on the agency level when it comes to integration platform as a service (iPaaS), over at <a href="https://lefthookdigital.com/">Left Hook Digital</a>, helping companies leverage Zapier, and integrate with API platforms. There is definitely significant movement in the number of API focused agencies, but we are going to need more to meet the demand for API design, deployment, management, testing, and other stops along the API lifecycle.</p>

<p>I’m guessing it will take a couple years for this side of the API business to mature. Then I’m figuring we will begin see to see more specialized API agencies emerge, helping with evangelism, support, API design, and maybe even industry focused iPaaS, similar to what we are seeing with LeftHook. As mainstream companies are waking up to the potential of APIs, they are going to need a lot of professional help to ensure they are successful in their API journey. We are going to need a number of general service, specialized, regional, and industry specific API agencies to help get us through the next couple of years.</p>

<p>I am focusing on trying to scale the API training portion of this need. I am ramping up development of API training courses that span my API lifecycle and API stack research. Helping API providers and consumers navigate the world of APIs. I’m talking with partners about working together to develop and distribute API training, and will be working to train some trainers at SMB, SME, and the enterprise. Eventually I’m guessing we will also be needing some API focused training agencies to help carry the load in being the last mile of delivery the API curriculum we will be developing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/17/why-arent-there-more-api-focus-agencies/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/16/the-85-stops-along-the-api-lifecycle-i-track-on/">The 85 Stops Along The API Lifecycle That I Track On</a></h3>
        <span class="post-date">16 Aug 2017</span>
        <p>I am preparing a talk for tomorrow, and I needed a new list of each stop along the API lifecycle, and since each of my project exist as Github repositories, and are defined as a YAML and JSON data store, I can simply define a new liquid template for generating a new HTML listing of all the stops along the API lifecycle–after generating this list I figured I’d share here as a story.</p>

<p>Here are the 85 stops along the API lifecycle landscape from my vantage point as the API Evangelist:</p>

<table>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://definitions.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-api-definitions.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://definitions.apievangelist.com" target="_blank">Definitions</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://design.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-api-design-fiction.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://design.apievangelist.com" target="_blank">Design</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://versioning.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-version.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://versioning.apievangelist.com" target="_blank">Versioning</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://hypermedia.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-hypermedia.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://hypermedia.apievangelist.com" target="_blank">Hypermedia</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://dns.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-globe.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://dns.apievangelist.com" target="_blank">DNS</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://low.hanging.fruit.apievangelist.com/" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-grapes.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://low.hanging.fruit.apievangelist.com/" target="_blank">Low Hanging Fruit</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://scraping.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-scraping.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://scraping.apievangelist.com" target="_blank">Scraping</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://database.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-database-new.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://database.apievangelist.com" target="_blank">Database</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://deployment.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-api-deployment.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://deployment.apievangelist.com" target="_blank">Deployment</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://rogue.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-pirate-flag.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://rogue.apievangelist.com" target="_blank">Rogue</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://microservices.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-microservice.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://microservices.apievangelist.com" target="_blank">Microservices</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://algorithms.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-algorithms.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://algorithms.apievangelist.com" target="_blank">Algorithms</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://search.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-search.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://search.apievangelist.com" target="_blank">Search</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://machine-learning.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-machine-learning.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://machine-learning.apievangelist.com" target="_blank">Machine Learning</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://proxy.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-proxy.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://proxy.apievangelist.com" target="_blank">Proxy</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://virtualization.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-virtulization.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://virtualization.apievangelist.com" target="_blank">Virtualization</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://containers.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-shipping-container.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://containers.apievangelist.com" target="_blank">Containers</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://management.apievangelist.com/" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/services/api-management.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://management.apievangelist.com/" target="_blank">Management</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://serverless.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-serverless.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://serverless.apievangelist.com" target="_blank">Serverless</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://portal.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-portal.jpg" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://portal.apievangelist.com" target="_blank">Portal</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://getting-started.apievangelist.com" target="_blank"><img src="http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/building-blocks/bw-running.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://getting-started.apievangelist.com" target="_blank">Getting Started</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://documentation.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-documentation.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://documentation.apievangelist.com" target="_blank">Documentation</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://frequently-asked-questions.apievangelist.com" target="_blank"><img src="http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/building-blocks/bw-question-mark.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://frequently-asked-questions.apievangelist.com" target="_blank">Frequently Asked Questions</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://support.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-support.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://support.apievangelist.com" target="_blank">Support</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://communications.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-chat.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://communications.apievangelist.com" target="_blank">Communications</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://road-map.apievangelist.com/" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-roadmap.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://road-map.apievangelist.com/" target="_blank">Road Map</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://issues.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-bug.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://issues.apievangelist.com" target="_blank">Issues</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://change-log.apievangelist.com" target="_blank"><img src="http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/building-blocks/bw-recycling.jpg" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://change-log.apievangelist.com" target="_blank">Change Log</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://monitoring.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-heart-monitor.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://monitoring.apievangelist.com" target="_blank">Monitoring</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://testing.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-testing.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://testing.apievangelist.com" target="_blank">Testing</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://performance.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-performance.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://performance.apievangelist.com" target="_blank">Performance</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://caching.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-cache.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://caching.apievangelist.com" target="_blank">Caching</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://reliability.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-reliability.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://reliability.apievangelist.com" target="_blank">Reliability</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://authentication.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-authentication-2.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://authentication.apievangelist.com" target="_blank">Authentication</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://encryption.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-encryption.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://encryption.apievangelist.com" target="_blank">Encryption</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://vulnerabilities.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-vulnerabilities.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://vulnerabilities.apievangelist.com" target="_blank">Vulnerabilities</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://breaches.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-breach.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://breaches.apievangelist.com" target="_blank">Breaches</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://security.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-padlock.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://security.apievangelist.com" target="_blank">Security</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://terms-of-service.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-terms-of-use.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://terms-of-service.apievangelist.com" target="_blank">Terms of Service (TOS)</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://surveillance.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-surveillance-camera.jpg" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://surveillance.apievangelist.com" target="_blank">Surveillance</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://privacy.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-privacy.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://privacy.apievangelist.com" target="_blank">Privacy</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://cybersecurity.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-cybersecurity.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://cybersecurity.apievangelist.com" target="_blank">Cybersecurity</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://reclaim.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-fist.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://reclaim.apievangelist.com" target="_blank">Reclaim</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://transparency.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-transparency.jpg" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://transparency.apievangelist.com" target="_blank">Transparency</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://observability.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-observable.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://observability.apievangelist.com" target="_blank">Observability</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://licensing.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-creative-commons.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://licensing.apievangelist.com" target="_blank">Licensing</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://copyright.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-copyright.gif" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://copyright.apievangelist.com" target="_blank">Copyright</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://accessibility.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-accessbility.jpg" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://accessibility.apievangelist.com" target="_blank">Accessibility</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://branding.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-brand.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://branding.apievangelist.com" target="_blank">Branding</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://regulation.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-government-regulation.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://regulation.apievangelist.com" target="_blank">Regulation</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://patents.apievangelist.com/" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-certificate.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://patents.apievangelist.com/" target="_blank">Patents</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://discovery.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/services/api-discovery.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://discovery.apievangelist.com" target="_blank">Discovery</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://client.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-client.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://client.apievangelist.com" target="_blank">Client</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://command-line-interface.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-command-line.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://command-line-interface.apievangelist.com" target="_blank">Command Line Interface</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://bots.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-robots.jpeg" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://bots.apievangelist.com" target="_blank">Bots</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://internet-of-things.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-internet-of-things.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://internet-of-things.apievangelist.com" target="_blank">Internet of Things</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://industrial.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-industrial.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://industrial.apievangelist.com" target="_blank">Industrial</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://network.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-network.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://network.apievangelist.com" target="_blank">Network</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://ide.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-ide.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://ide.apievangelist.com" target="_blank">IDE</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://sdk.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-sdk.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://sdk.apievangelist.com" target="_blank">SDK</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://plugin.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-plugin.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://plugin.apievangelist.com" target="_blank">Plugin</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://browsers.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-browser.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://browsers.apievangelist.com" target="_blank">Browsers</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://embeddable.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-embeddable.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://embeddable.apievangelist.com" target="_blank">Embeddable</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://visualization.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-visualization.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://visualization.apievangelist.com" target="_blank">Visualization</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://analysis.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-analysis.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://analysis.apievangelist.com" target="_blank">Analysis</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://logging.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-logging.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://logging.apievangelist.com" target="_blank">Logging</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://aggregation.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/trends/aggregation-trend.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://aggregation.apievangelist.com" target="_blank">Aggregation</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://ipaas.apievangelist.com/" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/trends/reciprocity-trends.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://ipaas.apievangelist.com/" target="_blank">iPaaS</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://webhooks.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/webhooks.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://webhooks.apievangelist.com" target="_blank">Webhooks</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://integrations.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-integration-automation.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://integrations.apievangelist.com" target="_blank">Integrations</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://migration.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-migration.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://migration.apievangelist.com" target="_blank">Migration</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://backups.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-backup.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://backups.apievangelist.com" target="_blank">Backups</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://realtime.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/trends/real-time-2.jpg" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://realtime.apievangelist.com" target="_blank">Real Time</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://orchestration.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-orchestration.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://orchestration.apievangelist.com" target="_blank">Orchestration</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://voice.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-microphone.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://voice.apievangelist.com" target="_blank">Voice</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://spreadsheets.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-excel-icon.jpg" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://spreadsheets.apievangelist.com" target="_blank">Spreadsheets</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://investment.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-investment.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://investment.apievangelist.com" target="_blank">Investment</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://monetization.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-dollar-sign.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://monetization.apievangelist.com" target="_blank">Monetization</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://plans.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-plan.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://plans.apievangelist.com" target="_blank">Plans</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://partners.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-partner.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://partners.apievangelist.com" target="_blank">Partners</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://certification.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-certification.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://certification.apievangelist.com" target="_blank">Certification</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://acquisitions.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-acquisitions.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://acquisitions.apievangelist.com" target="_blank">Acquisitions</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://evangelism.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://evangelism.apievangelist.com" target="_blank">Evangelism</a></td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://showcase.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-spotlights.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://showcase.apievangelist.com" target="_blank">Showcase</a>  </td>
   </tr>
   <tr>
      <td valign="middle" align="right" width="10%"><a href="http://deprecation.apievangelist.com" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-deprecation-2.png" width="50" style="padding: 5px;" align="center" /></a></td>
      <td valign="middle" align="left"><a href="http://deprecation.apievangelist.com" target="_blank">Deprecation</a></td>
   </tr>
</table>

<p>I’m always presenting <a href="http://apievangelist.com/api-lifecycle/">my API lifecycle research</a> as a listing, or in a linear fashion. I always feel like I should be creating an actual lifecycle visualization, but then I always end up feeling like I should just invest in my subway API map work, and create more robust way to represent how the API lifecycle truly looks.</p>

<p>Anyways, these 85 areas represent the scope of <a href="http://apievangelist.com/api-lifecycle/">my API industry research</a>, and provide a framework for thinking about not just the individual API lifecycle, but also the bigger picture of our API operations and partnerships. Not all of these areas apply to every API provider, but they do provide one perspective of the API landscape that all API providers can learn from. If there are any other stops along the lifecycle you think should be represented, I’d love to hear your thoughts. For example, I’m looking at adding two new areas: 1) training, and 2) fake. Helping me track on how API providers are training internally, with partners, and 3rd party developers, as well as think about the world of API driven fake news, products, advertising, users, and other illnesses emerging across the landscape.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/16/the-85-stops-along-the-api-lifecycle-i-track-on/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/16/wildcard-webhook-events/">Wildcard Webhook Events</a></h3>
        <span class="post-date">16 Aug 2017</span>
        <p><a href="http://webhooks.apievangelist.com"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-asterisk.png" align="right" width="30%" style="padding: 15px;" /></a></p>
<p>I have been studying the approach of a variety of webhook implementations in preparation for an API consulting project I’m working on. Even though I’m very familiar with how webhooks works, and confident in my ability to design and develop a solution, I’m ALWAYS looking to understand what leading API providers are up to, and how I can improve my knowledge and awareness.</p>

<p>With his round of research, Github has provided me with several webhook nuggets for my API storytelling notebook. One of their web features I though was the notion of a wildcard webhook event:</p>

<blockquote>
  <p><a href="https://developer.github.com/webhooks/">Wildcard Event</a> - We also support a wildcard (*) that will match all supported events. When you add the wildcard event, we’ll replace any existing events you have configured with the wildcard event and send you payloads for all supported events. You’ll also automatically get any new events we might add in the future.</p>
</blockquote>

<p>I have been working to identify a set of objects and associated webhook events, and the notion of a wildcard event is interesting. It seems like you could apply this globally, or to specific objects / resources, allowing you to get pushes for any events that occur. I’m not sure I’ll have time to apply this feature in my current project, but it is worth adding to my webhook toolbox for future projects.</p>

<p>There are three other features I’ve extracted from Github’s approach to webhooks that I’ve added to my API storytelling notebook, to hopefully craft into future blog posts. I’ll be adding these all as potential webhook building blocks that API providers can consider. I’m hoping to find some more time and money to invest into <a href="http://webhooks.apievangelist.com">my webhook research</a> this fall, and be able to publish a formal guide for the world of webhooks. I’m always surprised by the lack of formal guidance when it comes to webhooks, and is something I’d like to see change in the near future.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/16/wildcard-webhook-events/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/16/the-importance-of-api-stories/">The Importance Of API Stories</a></h3>
        <span class="post-date">16 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/photos/ca-panel.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am an API storyteller before am an API architect, designer, or evangelist. My number one job is to tell stories about the API space. I make sure there is always (almost) 3-5 stories a day published to API Evangelist about what I’m seeing as I conduct my research on the sector, and thoughts I’m having while consulting and working on API projects. I’ve been telling stories like this for seven years, which has proven to me how much stories matter in the world of technology, and the worlds that it is impacting–which is pretty much everything right now.</p>

<p>Occasionally I get folks who like to criticize what I do, making sure I know that stories don’t matter. That nobody in the enterprise or startups care about stories. Results are what matter. Ohhhhh reeeaaaly. ;-) I hate to tell you, it is all stories. VC investment in startups is all about the story. The markets all operate on stories. Twitter. Facebook. LinkedIn. Medium. TechCrunch. It is all stories. The stories we tell ourselves. The stories we tell each other. The stories we believe. The stories we refuse to believe. It is all stories. Stories are important to everything.</p>

<p><a href="https://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/">The mythical story about Jeff Bezos’s mandate that all employees needed to use APIs</a> internally is
still 2-3% of my monthly traffic, down from 5-8% for the last couple of years, and it was written in 2012 (five years ago). I’ve seen this story on the home page of the GSA internal portal, and framed hanging on the wall in a bank in Amsterdam. Stories are important. Stories are still important when they aren’t true, or partially true, like the Amazon mythical tale is(n’t). Stories are how we make sense of all this abstract stuff, and turn it into relatable concepts that we can use within the constructs of our own worlds. Stories are how the cloud became a thing. Stories are why microservices and Devops is becoming a thing. Stories are how GraphQL wants to be a thing.</p>

<p>For me, most importantly, telling stories is how I make sense of the world. If I can’t communicate something to you here on API Evangelist, it isn’t filed away in my mental filing cabinet. Telling stories is how I have made sense of the API space. If I can’t articulate a coherent story around API related technology, and it just doesn’t make sense to me, it probably won’t stick around in my storytelling, research, and consulting strategy. Stories are everything to me. If they aren’t to you, it’s probably because you are more on the receiving end of stories, and not really influencing those around you in your community, and workplace. Stories are important. Whether you want to admit it or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/16/the-importance-of-api-stories/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/16/api-kindergarten/">API Kindergarten For Business And IT Leaders</a></h3>
        <span class="post-date">16 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-alphabet-apple.png" align="right" width="25%" style="padding: 15px" /></p>
<p><a href="http://apievangelist.com/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/">I’m working on a number of API courses and lessons lately</a>. Some of these are API 101 courses, while others are more advanced courses for the seasoned API provider, and consumer. As I think about what is needed when it comes to classes and workshops across the API sector, I’m considering doing an API Kindergarten series, where business and IT leaders can learn the basics of doing business with APIs.</p>

<p>The curriculum for the API kindergarten program include hands on lessons on how to play nicely, get along with others, the importance of sharing, and helping them learn the important soft skills like not shitting your pants. I’m always surprised at the lack of basic skills by company, organizational, institutional, and government leadership when it comes to the essentials of why APIs work, and think a little primer on things might help some realize they shouldn’t be doing APIs in the first place, or maybe prevent some major crisis down the road.</p>

<p>The number of folks who tell me directly that they are all in on this API thing, yet when it comes to practice clearly are not has grown significantly in 2017. I feel like we need a whole series or curriculum to help make sure business and IT leadership is up for the challenge is desperately needed. Just wait, until I begin working on my sex edit course for the middle schoolers, where we teach them about safely integrating, and what is appropriate API behavior, and what is not. Honestly, I could spend days finding equivalences between the real world and doing APIs (not real world) and creating classes around them–fun stuff!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/16/api-kindergarten/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/15/which-platforms-have-control-over-the-conversations-around-their-bots/">Which Platforms Have Control Over The Conversation Around Their Bots</a></h3>
        <span class="post-date">15 Aug 2017</span>
        <p>I spend a lot of time monitoring API platforms, thinking about different ways of identifying which ones are taking control of the conversation around how their platforms operate. One example of this out in the wild can be found when it comes to bots, by doing a quick look at which of the major bot platforms own the conversation around this automation going on via their platforms.</p>

<p>First you take a look at Twitter, by doing a quick Google search for Twitter Bots:</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/bots/bots-twitter-search.png" align="center" width="65%" /></p>

<p>Then you take a look at Facebook, by doing a quick Google search for Facebook Bots:</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/bots/bots-facebook-search.png" align="center" width="65%" /></p>

<p>Finally take a look at Slack, by doing a quick Google search for Slack Bots:</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/bots/bots-slack-search.png" align="center" width="65%" /></p>

<p>It is pretty clear who owns the conversation when it comes to bots on their platform. While Twitter and Facebook both have information and guidance about doing bots they do not own the conversation like Slack does. Something that is reflected in the search engine placement. It is also something that sets the tone of the conversation that is going on within the community, and defines the types of bots that will emerge on the platform.</p>

<p>As I’ve said before, if you have a web or mobile property online today, <a href="https://apievangelist.com/2017/07/11/either-you-own-the-conversation-around-your-apis-or-someone-else-will/">you need to be owning the conversation around your API or someone eventually will own it for you</a>. The same comes to automation around your platform, and the introduction of bots, and automated users, traffic, advertising, and other aspects of doing business online today. Honestly, I wouldn’t want to be in the business of running a platform these days. It is why I work so hard to dominate and own my own presence, just so that I can beat back what is said about me, and own the conversation on at least Google, Twitter, LinkedIn, Facebook, and Github.</p>

<p>Seems like to me, if you are going to enable automation on your platform via APIs, it should be something that you own completely, and make sure you provide some pretty strong guidance and direction.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/15/which-platforms-have-control-over-the-conversations-around-their-bots/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/15/elastic-search-security-apis/">The ElasticSearch Security APIs</a></h3>
        <span class="post-date">15 Aug 2017</span>
        <p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api.html"><img src="https://s3.amazonaws.com/kinlane-productions/elastic-search/elasticsearch-security-apis.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I was looking <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api.html">at the set of security APIs over at Elasticsearch</a> as I was diving into my API security research recently. I thought the areas they provide security APIs for the search platform was worth noting and including in not just my API security research, but also <a href="http://search.apievangelist.com">search</a>, deployment, and probably overlap with <a href="http://authentication.apievangelist.com">my authentication research</a>.</p>

<ul>
  <li><strong>Authenticate API</strong> - The Authenticate API enables you to submit a request with a basic auth header to authenticate a user and retrieve information about the authenticated user.</li>
  <li><strong>Clear Cache API</strong> - The Clear Cache API evicts users from the user cache. You can completely clear the cache or evict specific users.</li>
  <li><strong>User Management APIs</strong> - The user API enables you to create, read, update, and delete users from the native realm. These users are commonly referred to as native users.</li>
  <li><strong>Role Management APIs</strong> - The Roles API enables you to add, remove, and retrieve roles in the native realm. To use this API, you must have at least the manage_security cluster privilege.</li>
  <li><strong>Role Mapping APIs</strong> - The Role Mapping API enables you to add, remove, and retrieve role-mappings. To use this API, you must have at least the manage_security cluster privilege.</li>
  <li><strong>Privilege APIs</strong> - The has_privileges API allows you to determine whether the logged in user has a specified list of privileges.</li>
  <li><strong>Token Management APIs</strong> - The token API enables you to create and invalidate bearer tokens for access without requiring basic authentication. The get token API takes the same parameters as a typical OAuth 2.0 token API except for the use of a JSON request body.</li>
</ul>

<p>Come to think of it, I’ll add this to my <a href="http://management.apievangelist.com">API management research</a> as well. <a href="http://apievangelist.com/2015/03/04/adding-four-new-building-building-blocks-providing-an-api-management-api-blueprint/">Much of this overlaps with what should be a common set of API management services as well</a>. Like much of my research, there are many different dimensions to my API security research. I’m looking to see how API providers are securing their APIs, as well as how service providers are selling security services to APIs providers. I’m also keen on aggregating common API design patterns for security APIs, and quantity how they overlap with other stops along the API lifecycle.</p>

<p>While the cache API is pretty closely aligned with delivering a search API, I think all of these APIs provide a potential building block to think about when you are deploying any API, and represents the Venn diagram that is API authentication, management, and security. I’m going through the rest of the <a href="https://www.elastic.co/">Elasticsearch</a> platform looking for interesting approaches to ensuring their search solutions are secure. I don’t feel like there are any search specific characteristics of API security that I will need to include in my final API security industry guide, but Elasticsearch’s approach has re-enforced some of the existing security building blocks I already had on my list.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/15/elastic-search-security-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/15/where-to-begin-with-webhooks-for-the-human-services-data-api/">Where To Begin With Webhooks For The Human Services Data API</a></h3>
        <span class="post-date">15 Aug 2017</span>
        <p><a href="https://www.slideshare.net/progrium/web-hooks-and-the-programmable-world-of-tomorrow-presentation/21-REST_Hooksrest_and_web_hooks"><img src="https://s3.amazonaws.com/kinlane-productions/webhooks/RESTHooks.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I am getting to work on a base set of webhook specification for <a href="http://org.open.referral.adopta.agency/">my human services data API work</a>, and I wanted to take a fresh drive through a handful of the leading APIs I’m tracking on. I’m needing to make some recommendations regarding how human services data APIs should be pushing information via APIs, as we as providing APIs. Webhooks are fascinating to me because they really are just APIs in reverse. Webhooks are just an API request, where the target URL is a variable, allowing an API call to be made from a platform, to any target URL, on an triggering events, or on a schedule as a job.</p>

<p>Here are six of the API providers I took a look at while doing this webhook research:</p>

<ul>
  <li><a href="https://developer.box.com/v2.0/docs/getting-started-with-webhooks-v2"><strong>Box</strong></a></li>
  <li><a href="[https://gumroad.com/webhooks"><strong>Gumroad</strong></a></li>
  <li><a href="https://developer.venmo.com/docs/webhooks"><strong>Venmo</strong></a></li>
  <li><a href="https://developer.github.com/webhooks/"><strong>Github</strong></a></li>
  <li><a href="https://stripe.com/docs/webhooks"><strong>Stripe</strong></a></li>
  <li><a href="https://api.slack.com/incoming-webhooks"><strong>Slack</strong></a></li>
</ul>

<p>All of these API providers offer webhooks, allowing developers to create an API call that will be fired off when a specific event occurs. These events are usually tied to a specific object. Box is documents. Github is a repository. Stripe is a payment. With human services it will be an organization, location, or service. There are a handful of key concepts at play when it comes to webhooks, making them an important part of the equation:</p>

<ul>
  <li><strong>Object</strong> - The object in which an event is occurring. For this project it is organizations, locations, services, contacts, and potentially other elements of API operations.</li>
  <li><strong>Events</strong>  - This is a list of events that can occur against all the objects that will trigger the execution of a webhook.</li>
  <li><strong>Target</strong> - The URL of the webhook. This is the variable of the outgoing API call, allowing them to be defined by API consumers, nd executed by the API provider.</li>
  <li><strong>Fat</strong> - The webhook will carry a payload, submitting a predefined schema, usually of the associated object to the target.</li>
  <li><strong>Ping</strong> - The webhook does not carry a payload, simply pinging the target of a webhook, letting API consumers know an even has occurred.</li>
  <li><strong>Status</strong> - The ability to identify the status of a webhook.</li>
  <li><strong>Errors</strong> - The errors that should be returned as part of webhook execution, and shared as it’s status.</li>
  <li><strong>Retries</strong> - Allowing for webhook execution to be replayed, executing a specific event that occurred in the past.</li>
  <li><strong>Signatures</strong> - Allowing for the signature of each webhook request to be verified for security and integrity purposes.</li>
  <li><strong>Test</strong> - Enable API cnosumers to test a webhook and see if it will work, sending over real or sample data.</li>
  <li><strong>History</strong> - Providing a complete history of webhook execution for API consumers to search, browse, and review.</li>
</ul>

<p>The external focus, and their event based nature are the most notable characteristics of a webhook, but it is the transactional nature of their supporting systems that seem to make them such a utility that can help alleviate the load on APIs. There are a number of other characteristics of webhooks, but this gets at the core of what they do, and provide me with what I need to move my human services API conversations forward. I’m looking to have a handful of examples of webhook implementations for well-known API platforms to share with folks, and begin setting the stage for an initial API design and definition for a human services webhook implementation, based upon common practices already in use.</p>

<p>I find webhooks interesting because are not a standard, and there really aren’t much in the way of best practices. Just some common examples of how they are used by existing API providers. Webhooks are just APIs. It is just the target URL that is variable, and the objects and events that can be unique to each API platform. In human services implementations I don’t just see webhooks as being about making API calls to other applications. I see webhooks as something that can satisfy pushing of data between many different API implementations, beginning to set the stage for interoperability between hundreds or even thousands of human service providers, pushing and pulling data as needed, and prescribed by each API provider. Allowing human service providers to speak a common language and seamlessly share information across a variety of partnerships. Getting closer to Greg Bloom’s vision, the creator of <a href="http://openreferral.org">Open Referral</a>, and the person behind the Human Services Data Specification (HSDS), for how all of this should be working.</p>

<p>**Photo Credit: **<a href="https://www.slideshare.net/progrium/web-hooks-and-the-programmable-world-of-tomorrow-presentation/21-REST_Hooksrest_and_web_hooks">REST and webhooks are two sides of the same coin. In “Web Hooks and the Programmable World of Tomorrow“, Jeff Lindsay, October 2008</a></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/15/where-to-begin-with-webhooks-for-the-human-services-data-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/15/addressing-bulk-api-operations-as-separate-set-of-services/">Addressing Bulk API Operations As Separate Set Of Services</a></h3>
        <span class="post-date">15 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/cargo-ship-zoomed-in-on-sea_light_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>Part of the feedback I’ve received from <a href="https://openreferral.github.io/api-specification/definition/">the Human Services Data API (HSDA) evolution from v1.0 to v1.1</a> was that the API didn’t allow for volume or bulk GET, POST, PUT, or DELETE. This was intentionally in the incremental release which focused on just making sure the API reflected 100% of the surface are for the Human Services Data Specification (HSDS). I wanted to separate out the needs of bulk API consumers, so that I could think about it separately from the more simple, micro-use integrations the default Human Services Data API would accommodate. I don’t want the industrial grade needs of database and system administrators overriding the simple access needs of other individual API consumers.</p>

<p>To kick off my human services bulk API definition I wanted to spend some time looking at other bulk implementations from a handful of leading providers:</p>

<ul>
  <li><a href="https://developers.intercom.com/v2.0/page/bulk-api-overview">Intercom</a></li>
  <li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html">ElasticSearch</a></li>
  <li><a href="https://developer.salesforce.com/page/File:Bulk_overview.png">SalesForce</a></li>
  <li><a href="https://www.diffbot.com/dev/docs/bulk/">Diffbot</a></li>
</ul>

<p>As I went through these implementations, and searched through Stack Overflow about bulk HTTP POSTs, I really didn’t see much difference on the technical front. Bulk APIs are primarily about acknowledging heavy duty data consumption, and primarily used HTTP POST for allowing the submission of either a) large individual POST, or b) large number of POST. Technically there isn’t much to them, where you start finding the nuance of bulk APIs over regular APIs is in the process surrounding the API implementation, things like having tasks, jobs, history, notifications, webhooks, email, and logging. Meaning there is just a lot more process and expectation around these APIs, which also most likely translates into more robust background architecture, and rules regarding the process involved with access.</p>

<p>Bulk APIs seem more about a separation of concerns. Bulk GET and POSTs require more infrastructure, and to do it properly you need process, and checks and balances to make sure a bulk operation is successfully executed, and there is sufficient history, notifications, and other events around bulk transactions. I’d say that bulk API operations should always be approached as a separate concern from the more mainstream web, mobile, single page application, embeddable, and spreadsheet applications. Providing a separate API subdomain, or paths allowing for the separation of infrastructure concerns, to make sure the API meets the availability concerns of both API provider and consumer. Next, I’ll spend some more time mapping out the task or job oriented structure of bulk APIs, but also for other API operations, to help make sure I do not reinvent the wheel when it comes to the design of my bulk API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/15/addressing-bulk-api-operations-as-separate-set-of-services/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/14/some-microservice-thoughts-around-my-human-services-api-work/">Some Microservice Thoughts Around My Human Services API Work</a></h3>
        <span class="post-date">14 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/algo-microservices.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://org.open.referral.adopta.agency/">The Human Services Data API I have been working on is about defining a set of API paths for working with organizations, locations, and services that are delivering human services in cities around the world</a>. As I’m working to evolve the OpenAPI for the Human Services Data API (HSDA), I’m constantly mindful of bloat, unnecessary expansion of features, and always working to separate things by concern. My thoughts have evolved this due to <a href="http://apievangelist.com/2017/08/11/a-hack-day-event-to-help-the-link-sf-app-speak-human-services-data-specification/">a hackathon I attended this week in San Francisco where a team at Optmizely worked to decouple an existing human services application from its backend and help teach it to speak Human Services Data Specification (HSDS)</a>–allowing it to speak a common language around the services that us humans depend on daily.</p>

<p>As the hackathon team was decoupling the single page application (React) from the API backend (Firebase) I took the API calls behind and published to Github as two JSON files. <a href="https://raw.githubusercontent.com/optimizely/linksf/master/core/sf_location.json">One of the files was locations, which contained 217 human service locations in San Francisco</a>, and metadata, which contained a handful of categories being used to organize and display the locations. In this situation, there is no notion of an organization, just 217 locations, offering human services across five categories. This legacy application, and forward engineering hackathon project was quickly becoming a microservices project, ironically it is a microservice project that was about delivering human services. ;-)</p>

<p>Looking at this unfolding project through a microservices lens, I needed to provide a single service. In the context <a href="https://link-sf.com/">of Link-SF</a>, the original project, I needed to offer a service that would deliver 217 locations where people can find human services in the areas of food, housing, hygiene, medical, and technology via an web, or mobile application. To help me achieve my goals I began to step through each of the steps of the lifecycle of any self-contained microservices:</p>

<ul>
  <li>Github - Each of my services begins with a Github repository, <a href="https://github.com/adopta-agency/human-services-link/">so I created a new repo</a>.</li>
  <li>Definitions - I defined the service I wanted as <a href="https://github.com/adopta-agency/human-services-link/blob/master/_data/api-commons/openapi.yaml">an OpenAPI</a>.</li>
  <li>Design - I worked to keep the design of it as simple as I possibly can.</li>
  <li>DNS - I relied on Github’s DNS for this service, but may setup my own subdomain.</li>
  <li>Database - I published a YAML data file into the data folder for the Github repository, acting as the database for my service, leverage Github and Jekyll for helping me broker database connectivity.</li>
  <li>Deployment - I deployed <a href="https://adopta-agency.github.io/human-services-link/apis/locations.json">a simple static JSON API</a> driven from the locations YAML store.</li>
  <li>Management - I am using Github as the management platform for my API, helping me deploy and manage consumption of my service. I’m using the Github API as the programmatic layer for managing my service operations to help me continuously deploy and integrate with this service.</li>
  <li>Portal - I am using Github pages as the portal for my service, providing both human and programmatic access to my service, making human service locations more available.</li>
  <li>Documentation - I published static HTML, and Swagger UI documentation for the service.</li>
  <li>Support - I have published a support page, and will be providing support via Github issues, Twitter, and email.</li>
  <li>Communications - I have a blog published, and will be publishing the story of the service using the Jekyll CMS blogging solution.</li>
  <li>Caching - The API, portal, and all aspects of the service I’ve launched is being cached by Github, riding on the backs of their Content Delivery Network (CDN).</li>
  <li>Reliability - While Github has been known to have stability issues, it is still some of the most reliable way to host data and API driven services.</li>
  <li>Encryption - I’m leveraging the Github DNS, and using their encryption in transit by default. I will be using my own subdomain and encryption in the future.</li>
  <li>Security - I’m offloading platform security to Github. They have more resources than I do. I’m also looking to keep things more secure be keeping my services as static as possible.</li>
  <li>Testing - I am setting up a series of monitors to ensure the service stays available, and delivering expected data and promised schema. I will be publishing a status update and history page.</li>
  <li>Transparency - Everything involved with my service runs on Github, either as a public or private repository, with the entire lifecycle transparent publicly, or privately to whoever is given access to the repository using Github.</li>
  <li>Observability - The entire service runs on Github. The entire lifecycle is observable in the Github history, and leverage their existing infrastructure for identity and access management (IAM), and continuous integration and deployment.</li>
  <li>Discovery - There is an APIs.json available in the root of the project, providing a machine readable index of the schema, data, API, and other resources available via this service.</li>
  <li>Client - I’ve published an HTML / JavaScript client on the home page of the project, with an editor for managing the data, which leverages the Github API for reading and writing data to the services repository.</li>
</ul>

<p>I’m calling my new service, born out of the Link-SF legacy application <a href="https://adopta-agency.github.io/human-services-link/">Human Services Link</a>. However, the only thing remaining of the Link-SF application is the data. I’ve forward engineered the schema, API, and web interface to all run on Github as a single, yet decoupled service. It is a reduction of the overall Human Services Data Specification (HSDS) schema, focusing in on just providing services at a handful of locations, reducing complexity and scope, to deliver a specific service–do one thing and do it well. I will be launching an Amazon EC2, and containerized version of this human services microservice, but I wanted to look at this one incarnation of it though the microservices lens. Sure, its not your classic microservices definition, but I think it holds up to a microservices test, it just makes some different architectural choices than the rest of the community might have made.</p>

<p>My human services microservice definition solves a single problem, for a specific type of end-user. It was originally geographically limited to San Francisco, but with with this evolution I’ve made sure there is a state/province field, as well as country, so that solution can be deployed for any city. I want the compute, database, DNS, API, portal, docs, and UI elements and admin tools to all be self contained for the delivery of a single microservice, but I wanted it all forkable, so you could launch this service over and over, in any city around the globe. There is still a lot of work to be done on this project before it is ready for prime time, but I’ve planted the seeds when it comes to delivering microservices in my world this way. I’m hoping it will be something I can’t ignore, and will keep pushing forward as part of my overall vision of how we can deliver sustainable API driven services.</p>

<p>I really like the static nature of this approach. I really like the forkability of this approach, and how you can use Github to separate out concerns by organization. I really like how I can offload much of the operation of my backend to Github, including security, CDN, and scalability / reliability. It is definitely not a pattern everyone should be following when doing microservices, but it does provide a static forkable pattern that can really help keep services very focused and self contained, but available to work in service of a bigger architectural picture. I’ll keep playing around with this approach to delivering human service microservices, and see what I can push forward and make stick. I’m hoping it can providing a low coast way for cash strapped municipalities to better deliver up to date information about human services to their entire population.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/14/some-microservice-thoughts-around-my-human-services-api-work/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/14/decoupling-the-business-of-my-human-services-microservice/">Decoupling The Business Of My Human Services Microservice</a></h3>
        <span class="post-date">14 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/containership_dali_three.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="https://apievangelist.com/2017/08/14/some-microservice-thoughts-around-my-human-services-api-work/">I’ve been looking at my human services API work through a microservices lens, triggered by the deployment of a reduced functionality version of the human services implementation I was working on this week</a>. I’m thinking a lot about the technical side of decoupling services using APIs, but I want to also take a moment and think about the business side of decomposing services, while also making sure they are deployed in a way that meets both the provider and consumer side of the business equation. My human services microservice implementation is in the public service, which is a space where the business conversation often seems to disappear behind closed doors, but in reality needs to be front and center with each investment (commit) made into any service.</p>

<p>Let’s take a moment to look at the monetization strategy and operational plan for my human service microservice. Yes, a public data microservice should have a monetization strategy and plan for operating and remaining sustainable. The goals for this type of microservice will be radically different than it would be for a commercial microservice, but it should have one all the same.</p>

<ul>
  <li>Monetization - How am I evaluating the investment into this project alongside any value that is generated, which I can potentially capture or exchange some value for some money to keep going.
    <ul>
      <li>Acquisition - What did it take for me to acquire the data and skills necessary to make the deployment possible.</li>
      <li>Development - What time was invested in setting up the platform, developing the schema, data, definitions, code, and visual elements.</li>
      <li>Operations - What does it take to operate the service? Maintain it, answer questions, provide support, and other realities of providing an online service today.</li>
      <li>Direct Value - What are the direct benefits of having this service available to people looking for human services, or organizations looking to provide human services.</li>
      <li>Indirect Value - What are the indirect benefits of having this service available, like increased conversation around human services, or maybe traffic and awareness of the Open Referral organization.</li>
      <li>Partners - What partnership opportunities are actively being sought out, or will be opened up by having this service available.</li>
      <li>Reporting - What type of reporting is necessary to operate and monetize this service, from tracking page views to understanding who is integrated with the data, and consuming data via APIs, or possibly through continuous integration of the Github repository.</li>
    </ul>
  </li>
  <li>Plan - What is my plan for making this service available to the public, partners, or maybe internally use across my own projects.
    <ul>
      <li>Elements - My human services location API is designed to be publicly available, forkable, and reusable by anyone.</li>
      <li>Limits - There are no limits on how each human services microservice can be used, or forked and reused. Ideally, any implementation provides attribution, and acknowledges the source of framework or data, but there really are no rules.</li>
      <li>Metrics - I am measuring unique page views on each page, including of the machine readable YAML, JSON, and other formats. I’m also tracking on stars, forks, and commits for each service.</li>
      <li>Commercial - Providing a clear track for commercial vendors to understand that the project needs their support, and can be improved upon, and evolved through their underwriting and support.</li>
    </ul>
  </li>
</ul>

<p>I need to have a coherent snapshot of what I’ve invested into each of my service. I need to have a base plan for how I will be executing the business side of this service–even if it just making something available for free. There are two dimensions to this conversation: 1) My view as the creator of this service 2) The view of folks who fork and implement as a service. Both dimensions should have a monetization snapshot, and a plan for executing within this business snapshot. I need all of this decoupled from any other service I am offering, but ideally they all use a common set of reusable patterns, just like the technical aspects of my microservices effort.</p>

<p>Just like needing the compute, database, DNS, and other technical layers to be stable and scalable across my microservices, I need the costs associated with these elements predictable and affordable. I need to know what it costs to define, design, deploy, manage, and deprecate my services. I need to know the best path forward for making them public, keeping them private, and being honest with commercial partners about the value that is being generated, both directly and indirectly. I need a way to report my costs, and revenue across hundreds or thousands of these services. I need to be able to scale this both horizontally across many services, but also vertically for single services which get deployed over and over, reused and continuously integrate wherever they are needed using Github.</p>

<p>I’ll keep applying this model across my human services project. I’m thinking that I’m going to be developing a whole buffet of human service microservices that run a 100% on Github like this, but I am also playing around with other varietals that run on other popular cloud platforms as well. I’m not one to subscribe to any particular API dogma, I’m just looking to explore what is possible, and do all of it as cheaply as I possibly scan, growing the number of projects I’m able to tackle. Of course, none of this would be possible without my partners funding my work, and helping me connect the technical and business aspects of doing API Evangelist.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/14/decoupling-the-business-of-my-human-services-microservice/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/14/platform-qa-responsibility/">API Platform FAQ And QA Responsibility</a></h3>
        <span class="post-date">14 Aug 2017</span>
        <p><a href="https://aws.amazon.com/answers/"><img src="https://s3.amazonaws.com/kinlane-productions/amazon/aws-answers-icons.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>The discussion around whether or not you should be hosting your own questions and answers (QA) and frequently asked questions (FAQ) for your API has continued, with many of the leading API pioneers asserting responsibility over the operations of these important API resources. Amazon noticed that answers about their platform on Quora and Stack Exchange were usually out of date and often just plain wrong, <a href="https://aws.amazon.com/answers/">prompting them to launch their own QA solution</a>.</p>

<p>I have written about using API providers using Stack Overflow for may years now. It the last few years I’ve had my readers push back on this for a variety of reasons, from the Stack Overflow community being primarily a white male bro-fest, to finding things being unreliable, out of date, and often a pretty hostile and unfriendly place for people to try and learn about APIs. I’d say that I still use Stack Overflow for about 40% of my querying of API and programming related subjects, but since I’m a white male who has been doing software for 30 years, I’m a little more resistant to the bro-fest. But, I get it, and hear what  folks are saying, and get it is not always a suitable environment.</p>

<p>Going back and forth on this subject, I’m back in the camp where API providers should be investing in operating their own QA, FAQ, and support forums. It’s definitely requires a significant amount of investment, policing, and sometimes taking stances that are unpopular, but if you are in this for the long game, it will be worth it. After watching AWS for a decade, you can see how incorrect information about your API operations can really begin to become a liability, and you might want to keep a tighter grip on where your API consumers go look for their answers. An added bonus is that you also get to set the tone for the types of questions that get answered, and the inclusiveness that will exist across your FAQ, QA, and Support.</p>

<p>I really need to get my core API design guides like <a href="http://definitions.apievangelist.com/">definitions</a>, <a href="http://design.apievangelist.com/">design</a>, <a href="http://deployment.apievangelist.com/">deployment</a>, and <a href="http://management.apievangelist.com/">management</a> out the door, because I need to diving into areas like support, and gather all my thoughts regarding how API providers should be approaching this critical layer of operations. I feel like support is one of the most defining characteristics of sustainable API providers, right up there with communications I’d say. I don’t care how good your API is technically, if you don’t have a solid approach to supporting and communicating with your API community, I’m guessing you won’t be around very long, or if you do survive, your platform will be something savvy API consumers steer clear of.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/14/platform-qa-responsibility/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/14/investing-time-to-learn-api-best-practices-so-you-do-not-reinvent-the-wheel/">Investing The Time To Learn API Best Practices So You Do Not Reinvent The Wheel</a></h3>
        <span class="post-date">14 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/photos/light-wheel.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was on a call the other day with a group of people who are in the trenches of organizations and companies working hard to deliver human services in cities around the country. We were meeting to kick of the design phase of a new type of API, and after they shared all their thoughts via project documentation, they were asking me to help identify examples of best practices from the space. The group felt they didn’t have the time, or the awareness of what is going on to be able to identify the best practices that already exist across the space.</p>

<p>This is one of the reasons I stay out of the weeds of individual projects. I may help define, design, and even shadow the deployment and management, but I work hard to avoid the tractor beam of ongoing projects so that I can pay attention to the bigger picture and help share stories about what I’m seeing. I feel like there should be people like me in each industry helping shine a light on, and aggregating of best practices when it comes to the API life cycle. There is just too much work to be done, and it helps to have folks who have domain expertise, not just lightly understanding what is going on across many sectors like I do.</p>

<p>I think it is fine for the sector to depend on API analysts like me, but I think that groups should also be investing in the time to pick up their heads up and pay attention to what else is going on when it comes to APIs in their industry. I understand that many groups are busy keeping systems operational, and dealing with real world problems, but dedicated reading of blogs, white papers, and tuning into social channels for other API providers is important as well. Each decision made on API design, deployment, and management should be established from time spent reading, and learning about other existing approaches–minimizing the amount of wheel reinvention that occurs.</p>

<p>The amount of time you invest in learning about other successful API providers will pay off in the amount of time it saves you when defining, designing, deploying, and managing an API. Sadly, it is one of those areas that some companies aren’t always equipped to measure the return on investment from something like this, resulting in many companies limiting the amount of time API developers spend reading blogs, and actively researching existing implementations, leading design patterns, and healthy API practices. If your boss is ever giving you grief about the amount of time you are spending on learning about other APIs, make sure and point them in my direction. I’m happy to help them understand why you don’t want to be reinventing the wheel when it comes to web APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/14/investing-time-to-learn-api-best-practices-so-you-do-not-reinvent-the-wheel/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/11/link-relation-types-for-apis/">Link Relation Types for APIs</a></h3>
        <span class="post-date">11 Aug 2017</span>
        <p><a href="https://tools.ietf.org/html/draft-wilde-service-link-rel-03"><img src="https://s3.amazonaws.com/kinlane-productions/erik-wilde/link-relation-types-for-web-services.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I have been reading through a number of specifications lately, trying to get more up to speed on what standards are available for me to choose from when designing APIs. Next up on my list is <a href="https://tools.ietf.org/html/draft-wilde-service-link-rel-03">Link Relation Types for Web Services</a>, by Erik Wilde. I wanted to take this informational specification and repost here on my site, partially because I find it easier to read, and the process of breaking things down and publishing as a posts helps me digest the specification and absorb more of what it contains.</p>

<p>I’m particularly interested in this one, because Erik captures what I’ve had in my head for APIs.json property types, but haven’t been able to always articulate as well as Erik does, let alone published as an official specification. I think his argument captures the challenge we face with mapping out the structure we have, and how we can balance the web with the API, making sure as much of it becomes machine readable as possible. I’ve grabbed the meat of Link Relation Types for Web Services and pasted here, so I can break down, and reference across my storytelling.</p>

<hr />

<ol>
  <li>Introduction<br />
One of the defining aspects of the Web is that it is possible to interact with Web resources without any prior knowledge of the specifics of the resource.  Following Web Architecture by using URIs, HTTP, and media types, the Web’s uniform interface allows interactions with resources without the more complex binding procedures of other approaches.</li>
</ol>

<p>Many resources on the Web are provided as part of a set of resources that are referred to as a “Web Service” or a “Web API”.  In many cases, these services or APIs are defined and managed as a whole, and it may be desirable for clients to be able to discover this service information.</p>

<p>Service information can be broadly separated into two categories: One category is primarily targeted for human users and often uses generic representations for human readable documents, such as HTML or PDF. The other category is structured information that follows some more formalized description model, and is primarily intended for consumption by machines, for example for tools and code libraries.</p>

<p>In the context of this memo, the human-oriented variant is referred to as “documentation”, and the machine-oriented variant is referred to as “description”.</p>

<p>These two categories are not necessarily mutually exclusive, as there are representations that have been proposed that are intended for both human consumption, and for interpretation by machine clients. In addition, a typical pattern for service documentation/description is that there is human-oriented high-level documentation that is intended to put a service in context and explain the general model, which is complemented by a machine-level description that is intended as a detailed technical description of the service.  These two resources could be interlinked, but since they are intended for different audiences, it can make sense to provide entry points for both of them.</p>

<p>This memo places no constraints on the specific representations used for either of those two categories.  It simply allows providers of aWeb service to make the documentation and/or the description of their services discoverable, and defines two link relations that serve that purpose.</p>

<p>In addition, this memo defines a link relation that allows providers of a Web service to link to a resource that represents status information about the service.  This information often represents operational information that allows service consumers to retrieve information about “service health” and related issues.</p>

<ol>
  <li>
    <p>Terminology<br />
The key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”,”SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be interpreted as described in RFC 2119 [RFC2119].</p>
  </li>
  <li>
    <p>Web Services<br />
“Web Services” or “Web APIs” (sometimes also referred to as “HTTP API” or “REST API”) are a way to expose information and services on the Web. Following the principles of Web architecture[ they expose URI-identified resources, which are then accessed and transferred using a specific representation.  Many services use representations that contain links, and often these links are typed links.</p>
  </li>
</ol>

<p>Using typed links, resources can identify relationship types to other resources.  RFC 5988 [RFC5988] establishes a framework of registered link relation types, which are identified by simple strings and registered in an IANA registry.  Any resource that supports typed links according to RFC 5988 can then use these identifiers to represent resource relationships on the Web without having to re-invent registered relation types.</p>

<p>In recent years, Web services as well as their documentation and description languages have gained popularity, due to the general popularity of the Web as a platform for providing information and services.  However, the design of documentation and description languages varies with a number of factors, such as the general application domain, the preferred application data model, and the preferred approach for exposing services.</p>

<p>This specification allows service providers to use a unified way to link to service documentation and/or description.  This link should not make any assumptions about the provided type of documentation and/or description, so that service providers can choose the ones that best fit their services and needs.</p>

<p>3.1.  Documenting Web Services<br />
In the context of this specification, “documentation” refers to information that is primarily intended for human consumption.Typical representations for this kind of documentation are HTML andPDF.</p>

<p>Documentation is often structured, but the exact kind of structure depends on the structure of the service that is documented, as well as on the specific way in which the documentation authors choose to document it.</p>

<p>3.2.  Describing Web Services<br />
In the context of this specification, “description” refers to information that is primarily intended for machine consumption.Typical representations for this are dictated by the technology underlying the service itself, which means that in today’s technology landscape, description formats exist that are based on XML, JSON, RDF, and a variety of other structured data models.  Also, in each of those technologies, there may be a variety of languages that a redefined to achieve the same general purpose of describing a Web service.</p>

<p>Descriptions are always structured, but the structuring principles depend on the nature of the described service.  For example, one of the earlier service description approaches, the Web ServicesDescription Language (WSDL), uses “operations” as its core concept, which are essentially identical to function calls, because the underlying model is based on that of the Remote Procedure Call (RPC) model.  Other description languages for non-RPC approaches to services will use different structuring approaches.</p>

<p>3.3.  Unified Documentation/Description<br />
If service providers use an approach where there is no distinction of service documentation Section 3.1 and service descriptionSection 3.2, then they may not feel the need to use two separate links.  In such a case, an alternative approach is to use the”service” link relation type, which has no indication of whether it links to documentation or description, and thus may be better fit if no such differentiation is required.</p>

<ol>
  <li>Link Relations for Web Services<br />
In order to allow Web services to represent the relation of individual resources to service documentation or description, this specification introduces and registers two new link relation types.</li>
</ol>

<p>4.1.  The service-doc Link Relation Type<br />
The “service-doc” link relation type is used to represent the fact that a resource is part of a bigger set of resources that are documented at a specific URI.  The target resource is expected to provide documentation that is primarily intended for human consumption.</p>

<p>4.2.  The service-desc Link Relation Type<br />
The “service-desc” link relation type is used to represent the fact that a resource is part of a bigger set of resources that are described at a specific URI.  The target resource is expected to provide a service description that is primarily intended for machine consumption.  In many cases, it is provided in a representation that is consumed by tools, code libraries, or similar components.</p>

<ol>
  <li>Web Service Status Resources<br />
Web services providing access to a set of resources often are hosted and operated in an environment for which status information may be available.  This information may be as simple as confirming that a service is operational, or may provide additional information about different aspects of a service, and/or a history of status information, possibly listing incidents and their resolution.</li>
</ol>

<p>The “status” link relation type can be used to link to such a status resource, allowing service consumers to retrieve status information about a Web service’s status.  Such a link may not be available from all resources provided by a Web service, but from key resources such as a Web service’s home resource.</p>

<p>This memo does not restrict the representation of a status resource in any way.  It may be primarily focused on human or machine consumption, or a combination of both.  It may be a simple “traffic light” indicator for service health, or a more sophisticated representation conveying more detailed information such as service subsystems and/or a status history.</p>

<ol>
  <li>IANA Considerations<br />
The link relation types below have been registered by IANA perSection 6.2.1 of RFC 5988 [RFC5988]:</li>
</ol>

<p>6.1.  Link Relation Type: service-doc<br /></p>

<p>Relation Name: service-doc<br />
   Description: Linking to service documentation that is primarily   intended for human<br /> consumption.<br />
   Reference: [[ This document ]]<br /></p>

<p>6.2.  Link Relation Type: service-desc<br /></p>

<p>Relation Name: service-desc<br />
   Description: Linking to service description that is primarily   intended for consumption by machines.<br />
   Reference: [[ This document ]]<br /></p>

<p>6.3.  Link Relation Type: status<br /></p>

<p>Relation Name: status<br />
   Description: Linking to a resource that represents the status of a   Web service or API.<br />
   Reference: [[ This document ]]<br /></p>

<hr />

<p><strong>Adding Some Of My Own Thoughts Beyond The Specification</strong>
This specification provides a more coherent service-doc, and service-desc that I think we did with humanURL, and support for multiple API definition formats (swagger, api blueprint, raml) as properties for any API. This specification provides a clear solution for human consumption, as well as one intended for consumption by machines. Another interesting link relation it provides is status, helping articulate the current state of an API.</p>

<p>It makes me happy to see this specification pushing forward and formalizing the conversation. I see the evolution of link relations for APIs as an important part of the API discovery and definition conversations in coming years. Processing this specification has <a href="https://github.com/apis-json/api-json/issues">helped jumpstart some conversation around APIs.json</a>, as well as other specifications like <a href="http://apievangelist.com/2017/08/03/api-discovery-using-json-home/">JSON Home</a> and <a href="http://apievangelist.com/2017/08/03/microservice-discovery-using-pivio/">Pivio</a>.</p>

<p><a href="https://tools.ietf.org/html/draft-wilde-service-link-rel-03">Thanks for letting me build on your work Erik!</a> - I am looking forward to contributing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/11/link-relation-types-for-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/11/about-api-data-gov/">About api.data.gov</a></h3>
        <span class="post-date">11 Aug 2017</span>
        <p><a href="https://api.data.gov/about/"><img src="https://s3.amazonaws.com/kinlane-productions/api-data-gov/9299911959_bdc195fb56_o.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m going to borrow, modify, and improve on the content from <a href="https://api.data.gov">api.data.gov</a>, because it is an important effort I want my readers to be aware of, because I want more of them to help apply educate other federal agencies regarding why it is a good idea to bake api.data.gov into their API operations, and help apply pressure until EVERY federal agency is up and running using a common API management layer.</p>

<p>Ok, so what is <a href="https://api.data.gov">api.data.gov</a>? api.data.gov is a free API management service for federal agencies. Our aim is to make it easier for you to release and manage your APIs. api.data.gov acts as a layer above your existing APIs. It transparently adds extra functionality to your APIs and helps deal with some of the repetitive parts of managing APIs.</p>

<p>Here are the features of api.data.gov:</p>

<ul>
  <li>You’re in control: You still have complete control of building and hosting your APIs however you like.</li>
  <li>No changes required: No changes are required to your API, but when it’s accessed through api.data.gov, we’ll transparently add features and handle the boring stuff.</li>
  <li>Focus on the APIs: You’re freed from worrying about things like API keys, rate limiting, and gathering usage stats, so you can focus on building the next great API.</li>
  <li>Make it easy for your users: By providing a standard entry point to participating APIs, it’s easier for developers to explore and use APIs across the federal government.</li>
</ul>

<p><a href="https://api.data.gov">api.data.gov</a> handles the API keys for you:</p>

<ul>
  <li>API key signup: It’s quick and easy for users to signup for an API key and start using it immediately.</li>
  <li>Shared across services: Users can reuse their API key across all participating api.data.gov APIs.</li>
  <li>No coding required: No code changes are required to your API. If your API is being hit through api.data.gov, you can simply assume it’s from a valid user.</li>
</ul>

<p><a href="https://api.data.gov">api.data.gov</a> tracks all the traffic to your API and give you tools to easily analyze it:</p>

<ul>
  <li>Demonstrate value: Understand how your API is being used so you can gauge the value and success of your APIs.</li>
  <li>Visualize usage and trends: View graphs of the overall usage trends for your APIs.</li>
  <li>Flexible querying: Drill down into the stats based on any criteria. Find out how much traffic individual users are generating, or answer more complex questions about aggregate usage.</li>
  <li>Monitor API performance: We gather metrics on the speed of your API, so you can keep an eye on how your API is performing.</li>
  <li>No coding required: No code changes are required to your API. If your API is being hit through api.data.gov, we can take care of logging the necessary details.
Documentation</li>
</ul>

<p><a href="https://api.data.gov">api.data.gov</a> helps with publishing documentation for your API:</p>

<ul>
  <li>Hosted or linked: We can host the documentation of your API, or, if you already have your own developer portal, we can simply link to it.</li>
  <li>One stop shop: As more agencies add APIs to api.data.gov, users will be able to discover and explore more government APIs all at one destination.</li>
</ul>

<p><a href="https://api.data.gov">api.data.gov</a> helps you rate limit because you might not want to allow all users to have uncontrolled access to your APIs:</p>

<ul>
  <li>Prevent abuse: Your API servers won’t see traffic from users exceeding their limits, preventing additional load on your servers.</li>
  <li>Per user limits: Individual users can be given higher or lower rate limits.</li>
  <li>No coding required: No code changes are required to your API. If your API is being hit, you can simply assume it’s from a user that hasn’t exceeded their rate limits.</li>
</ul>

<p><a href="https://api.data.gov">api.data.gov</a> is powered by the open source project API Umbrella. You can contribute to the development of this platform, or setup your own instance and run the entire stack yourself. If you’re interested in exploring any of this for your APIs, <a href="https://api.data.gov/contact/#apidatagov-service-contact">please contact us</a>. In general, it’s easy to take any existing API your agency has (or is in the process of building) and put api.data.gov in front of it. This can be an easy way to get started and see what type of functionality api.data.gov might provide for your API.</p>

<p><a href="https://api.data.gov">api.data.gov</a> is all about consistent API management across federal government, which means developers will be able to get at government data, content, and algorithms more efficiently, and integrate them into web, mobile, and other applications. We need more government agencies to be doing this, taking advantage of api.data.gov and get to work developing an awareness around who is consuming their API resources. Eventually API management will be how government agencies will be generating revenue on top  of valuable API resources, charging commercial users enough so that each agency can cover the costs of operations, and hopefully make more of an investment in the resources they are making available.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/11/about-api-data-gov/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/11/embeddable-api-tooling-discovery-with-json-home/">Embeddable API Tooling Discovery With JSON Home</a></h3>
        <span class="post-date">11 Aug 2017</span>
        <p><a href="http://apievangelist.com/2017/08/03/api-discovery-using-json-home/">I have been studying JSON Home</a>, trying to understand how it sizes up to <a href="http://apisjson.org">APIs.json</a>, and <a href="http://apievangelist.com/2017/08/03/microservice-discovery-using-pivio/">other formats I’m tracking on like Pivio</a>. JSON Home has a number of interesting features, and I thought one of their examples was also interesting, and was relevant to <a href="http://embeddable.apievangelist.com/">my API embeddable research</a>. In this example, JSON Home was describing a widget that was putting an API to use as part of its operation.</p>

<p>Here is the snippet from the JSON Home example, providing all details of how it works:</p>

<script src="https://gist.github.com/kinlane/40fabbb3133ddd1c65249dfdc87999cd.js"></script>

<p>JSON Home seems very action oriented. Everything about the format leads you towards taking some sort of API driven action, something that makes a lot of sense when it comes to widgets and other embeddables. I could see JSON Home being used as some sort of definition for button or widget generation and building tooling, providing a machine readable definition for the embeddable tool, and what is possible with the API(s) behind.</p>

<p>I’ve been working towards embeddable directories and API stacks using APIs.json, providing distributed and embeddable tooling that API providers and consumers can publish anywhere. I will be spending more time thinking about how this world of API discovery can overlap with the world of API embeddables, providing not just a directory of buttons, badges, and widgets, but one that describes what is possible when you engage with any embeddable tool. I’m beginning to see JSON Home similar to how I see Postman Collections, something that is closer to runtime, or at least deploy time. Where APIs.json is much more about indexing, search, and discovery–maybe some detail about where the widgets are, or maybe more detail about what embeddable resources are available.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/11/embeddable-api-tooling-discovery-with-json-home/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/11/a-hack-day-event-to-help-the-link-sf-app-speak-human-services-data-specification/">A Hack Day Event To Help The Link-SF App Speak Human Services Data Specification (HSDS)</a></h3>
        <span class="post-date">11 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/optimizely/optimizely-hackathon.JPG" align="right" width="40%" style="padding: 15px;" /></p>
<p>I went up to San Francisco on Wednesday to participate in a social good hack day at <a href="https://www.optimizely.com/">Optimizely</a>. They held their event at their downtown offices, where 20+ employees showed up to hack on some social good projects. Open Referral and our partner <a href="https://www.benetech.org/">Benetech</a> had suggested Human Services Data Specification (HSDS) as a possible project, which resulted in us being one of the hack projects for the event.</p>

<p>The Open Referral Human Services Data Specification (HSDS) team consisted of five Optimizely developers.</p>

<ul>
  <li>Derek Hammond - Software Engineer</li>
  <li>Michael Fields - Software Engineer</li>
  <li>Zachary Power - Software Engineering Intern</li>
  <li>Quinton Dang - Software Engineer</li>
  <li>Asa Schachar - Engineering Manager</li>
</ul>

<p>The overall strength of the team leaned toward being front-end web and mobile developers, so we decided <a href="https://github.com/zendesk/linksf">to “forward engineer” the Link-SF application</a>, which provides a simple web or mobile application to help folks find a variety of human services in a handful of categories like food, housing, hygiene, medical, and technology. Link-SF is an ongoing collaboration between the <a href="http://www.tenderlointechnologylab.org/">Tenderloin Technology Lab</a> and <a href="http://www.zendesk.com/">Zendesk, Inc.</a>, and we wanted to help contribute to their work, while also making the application potentially deployable by other cities and regions.</p>

<p>Once the team got to work on the project they identified that we could get at the data behind the SF application. The team decide they would forward engineer the dataset, the API, and the UI for the web and mobile application, making it all speak <a href="https://github.com/openreferral/specification">Human Services Data Specification (HSDS)</a>–here is what they did:</p>

<ul>
  <li>Took the Link-SF datasets and saved as a single JSON file.</li>
  <li>Converted the JSON schema to use HSDS – the changes weren’t significant.</li>
  <li>Made it so that the app reads <a href="https://raw.githubusercontent.com/optimizely/linksf/master/core/sf_location.json">location data from a Github repository</a> (you can change this to your url)</li>
  <li>Updated the taxonomy fetch to use the new data</li>
  <li>Ensured the UI worked with the new data</li>
</ul>

<p><a href="https://github.com/optimizely/linksf">You can find the project in an Optimizely Github repository</a>, which they are going to invest some more time into cleaning up this week–so don’t judge! ;-) If you want to run our app locally, <a href="https://github.com/optimizely/linksf/blob/master/docs/SETUP.md">you can save a file as <code class="highlighter-rouge">config.js</code>and follow the instructions on the setup page</a>. I am going to play with the application some more, and wait for them to clean up the page before we add the project to our official open source Human Services Data Specification (HSDS) solutions. Adding another HSDS compliant tool that any city, county, or other organization could deploy for their own purposes.</p>

<p>We didn’t have much time for the hackathon. It went from 2:30 PM to 8:30 PM, and I was impressed what the team was able to get done. I like their hack which used Github as a backend, and the speed at which they were able to work together to fork the application and make working using HSDS. It provides an interesting open source blueprint that other cities could also fork, and implement with their own localized datasets. Their work shows what is possible when you decouple the backend API (or JSON file) from the front-end application, and utilize a common schema and API definition. It transforms the application from a single use application into a multi-use application that can be used over and over again.</p>

<p>Thank you to <a href="https://www.optimizely.com/">Optimizely</a> for putting on the hackathon. Thank you to <a href="https://www.benetech.org/">Benetech</a> for making the event happen. Thank you to the five developers who help moved the HSDS conversation forward. Once the Github repository gets cleaned up, I will spend some time playing with the application, and publish a follow-up story. I’d like to make the project push button deployable, so that organizations could launch a new instance of Link-[Any City] for low or no cost. We have another hackathon coming up in September on the east coast, and I learned a lot at this one about what I should have ready when it comes to help hackers be successful in the short time we have. It has been a while since I’ve attended a hackathon, and forgot how they can be a good vehicle for moving projects forward–at least the social good type of event. ;-)</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/11/a-hack-day-event-to-help-the-link-sf-app-speak-human-services-data-specification/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/10/image-logging-with-amazon-s3-api/">Image Logging With Amazon S3 API</a></h3>
        <span class="post-date">10 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algorotoscope/stories/freeway_atari_missle.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I have been slowly evolving my network of websites in 2017, overhauling the look of them, as well as how they function. I am investing cycles into pushing as much of my infrastructure towards being as static as possible, minimizing my usage of JavaScript wherever I can. I am still using a significant amount of JavaScript libraries across my sites for a variety of use cases, but whenever I can, I am looking to kill my JavaScript or backend dependencies, and reduce the opportunity for any tracking and surveillance.</p>

<p>While I still keep Google Analytics on my primary API Evangelist sites, as my revenue depends on it, whenever possible I keep personal projects without any JavaScript tracking mechanisms. Instead of JavaScript I am defaulting to image logging using Amazon S3. Most of my sites tend to have some sort of header image, which I store in a common public bucket on Amazon S3, all I have to do is turn on logging, and then get at logging details via the Amazon S3 API. Of course, images get cached within a user’s browser, but the GET for my images still gives me a pretty good set of numbers to work with. I’m not concerned with too much detail, I just generally want to understand the scope of traffic a project is getting, and whether it is 5, 50, 500, 5,000, or 50,000 visitors.</p>

<p>My two primary CDNs are Amazon S3 and Github. I’m trying to pull together a base strategy for monitoring activity across my digital footprint. My business presence is very different than my personal presence, but with some of my personal writing, photography, and other creations I still like to keep a finger on the pulse of what is happening. I am just looking to minimize the data gathering and surveillance I am participating in these days. Keeping my personal and business websites static, and with a minimum footprint is increasingly important to me. I find that a minimum viable static digital footprint protects my interests, maximize my control over my work, and minimizes the impact to my readers and customers.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/10/image-logging-with-amazon-s3-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/10/patent-number-9325732-computer-security-threat-sharing/">Patent Number 9325732: Computer Security Threat Sharing</a></h3>
        <span class="post-date">10 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/soldier_computer_dark_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>The main reason that I tend to rail against API specific patents is that much of what I see being locks up reflects the parts and pieces that are making the web work. <a href="http://apievangelist.com/2016/01/12/i-just-cannot-get-behind-api-patents-especially-when-they-apply-to-http-and-hypermedia/">I see things like hypermedia, and other concepts that are inherently about sharing, collaboration, and reuse</a>–something that should never be patented. This concept applies to other patents I’m seeing, but rather than being about the web, it is about trust, and sharing of information. Things that shouldn’t be locked up, and exist within realms where the concept of patents actually hurt the web and APIs.</p>

<p>Today’s patent is out of Amazon, who are prolific patenters of web and API concepts. This one though is about <a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=/netahtml/PTO/search-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN/9325732">the sharing of security threat sharing</a>. Outlining something that should be commonplace on the web.</p>

<p>Title -  <a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=/netahtml/PTO/search-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN/9325732">Computer security threat sharing</a><br />
Number - 09325732<br />
Owner - Amazon Technologies, Inc.<br />
Abstract -  A computer security threat sharing technology is described. A computer security threat is recognized at an organization. A partner network graph is queried for security nodes connected to a first security node representing the organization. The first security node is connected to at least a second security node representing a trusted security partner of the organization. The second security node is associated with identification information. The computer security threat recognized by the organization is communicated to the trusted security partner using the identification information associated with the second security node.<br /></p>

<p>I’m sorry. I just do not see this as unique, original, or remotely a concept that should be patentable. <a href="http://apievangelist.com/2017/08/08/patent-web-of-trust-management-in-a-distributed-system/">Similar to a previous patent I wrote about on trust</a>, I just don’t think that sharing of security information needs to be locked up. The USPTO should recognize this. I feel like this type of patent shows how broken the patent process is, and how distorted company’s views on what is a patentable idea. Honestly, these types of patents feel lazy to me, and lack any creativity, skills, or sensible view of how the web works.</p>

<p>I feel like I should start rating these patents with some sort of Rotten Tomato score, and start giving companies some sort of patent ranking for their portfolio. Something that encompasses the scope, lack of creativity, originality, and damaging effects of the patent. This reminds me that I need to finish my work pulling court cases <a href="https://www.courtlistener.com/api/rest-info/">from the Court Listener API</a>, and index them for any companies, and their patent portfolios. Ultimately this is where the real damage to APIs and the web will play out, similar to <a href="https://apievangelist.com/2014/05/10/where-will-your-api-stand-in-the-oracle-v-google-api-copyright-debate/">the Oracle vs. Google API copyright affair</a>, but I will keep sharing stories of these ridiculous patents, and maybe even start ranking them all by how much they stink.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/10/patent-number-9325732-computer-security-threat-sharing/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/10/my-focus-on-public-apis-applies-internally-as-well/">My Focus On Public APIs Also Applies Internally</a></h3>
        <span class="post-date">10 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/photos/public-market.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>A regular thing I hear from folks when we are having conversations about the API lifecycle, is that I focus on public APIs, and they are more interested in private APIs. Each time I hear this I try to take time and assess which parts of my public API research wouldn’t apply to internal APIs. You wouldn’t publish your APIs to pubic API search engines like APIs.io or ProgrammableWeb, and maybe not evangelizing your APIs at hackathons, but I’d say 90% of what I study is applicable to internal APIs, as well as publicly available APIs.</p>

<p>With internal APIs, or private network partner APIs you still need a portal, documentation, SDKs, support mechanisms, and communication and feedback loops. Sure, how you use the common building blocks of API operations that I track on will vary between private and public APIs, but this shifts from industry to industry, and API to API as well–it isn’t just a public vs. private thing. I would say that 75% of my API industry research is derived from public API operations–it is just easier to access, and honestly more interesting to cover than private ones. The other 25% of internal API conversation I’m having, always benefit from thinking through the common API building blocks of public APIs, looking for ways they can be more successful with internal and partner APIs.</p>

<p>I’d say that a significant part of the mindshare for the microservices philosophy is internally focused. I think this is something that will come back to hurt some implementations, cutting out many of the mechanisms and common elements required in a successful API formula. Things like portals, documentations, SDKs, testing, monitoring, discovery, support, communications all contribute to an API working, or not working. I’ve said it before, and I’ll say it again. I’m not convinced that there is any true separation in public vs private APIs, and there remains to be a great deal we can learn from public API providers, and put to work across internal operations, and with our most trusted partners.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/10/my-focus-on-public-apis-applies-internally-as-well/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/10/observability-in-botnet-takedown-by-government-on-private-infrastructure/">Observability In Botnet Takedown By Government On Private Infrastructure</a></h3>
        <span class="post-date">10 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-bot-api.png" align="right" width="25%" style="padding: 15px;" /></p>

<p>I’m looking into how to make <a href="http://security.apievangelist.com">API security</a> more transparent and observable lately, and looking for examples of companies, institutions, organizations, politicians, and the government are calling for observability into wherever APIs are impacting our world. Today’s example comes out of <a href="http://www.politico.com/tipsheets/morning-cybersecurity">POLITICO’s Morning Cybersecurity email newsletter</a>, which has become an amazing source of daily information for me, regarding transparency around the take down of bot networks.</p>

<p><em>“If private companies cooperate with government agencies - for example, in the takedown of botnets using the companies’ infrastructure - they should do so as publicly as possible, <a href="https://www.ntia.doc.gov/files/ntia/publications/cdt-ntia-nistcommentsbotnetsfinal.pdf">argued the Center for Democracy &amp; Technology</a> . “One upside to compulsory powers is that they presumptively become public eventually, and are usually overseen by judges or the legislative branch,” CDT argued in its filing. “Voluntary efforts run the risk of operating in the dark and obscuring a level of coordination that would be offensive to the general public. It is imperative that private actors do not evolve into state actors without all the attendant oversight and accountability that comes with the latter.”</em></p>

<p>I’ve been tracking on the transparency statements and initiatives of all the API platforms. At some point I’m going to assemble the common building blocks of what is needed for executing platform transparency, and I will be including these asks of the federal government. As the Center for Democracy &amp; Technology states this relationship between the public and private sector when it comes to platform surveillance needs to be more transparent and observable in all forms. Bots, IoT, and the negative impacts of API automation needs to be included in the transparency disclosure stack. If the government is working with platform to discover, surveil, or shutdown bot networks there should be some point in which operations should be shared, including the details of what was done.</p>

<p>We need platform <a href="http://transparency.apievangelist.com">transparency</a> and <a href="http://observability.apievangelist.com">observability</a> at the public and private sector layer of engagement. Sure, this sharing of information would be time sensitive, respecting any investigations and laws, but if private sector infrastructure is being used to surveil and shut down U.S. citizens there should be an accessible, audit-able log for this activity. Of course it should also have an API allowing auditors and researchers to get all relevant information. Bots are just one layer of the API security research I’m doing, and the overlap in the bot world when it comes to API transparency, observability, and security is an increasingly significant vector when it comes to policing, <a href="http://surveillance.apievangelist.com">surveillance</a>, but also when it comes to protecting the privacy and safety of platform people (citizens).</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/10/observability-in-botnet-takedown-by-government-on-private-infrastructure/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/09/continous-integration-and-deployment-for-government-procurement/">Continuous Integration And Deployment For Government Procurement</a></h3>
        <span class="post-date">09 Aug 2017</span>
        <p><a href="https://buyandsell.gc.ca/procurement-data/tender-notice/PW-17-00786791"><img src="https://s3.amazonaws.com/kinlane-productions/canada/open-by-default-portal-procurement.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I was reading <a href="https://buyandsell.gc.ca/procurement-data/tender-notice/PW-17-00786791">the Open by Default Portal Procurement Pilot for the Treasury Board of Canada</a>, where section 6, Licensing states: “To support the objectives of the open government initiative, the Solution must be open source and licensed in accordance with the Massachusetts Institute of Technology License (“MIT License”). Under the resulting contract, the Contractor will be required to deposit the Solution’s source code on the GitHub platform (https://github.com) – under the MIT License.”</p>

<p>This just seems like the way it should be for all government technology solutions. I’ve heard the naysayers in federal government say that proprietary software is the best route, but if it drives public infrastructure, in my opinion the code should be publicly available in this way. Honestly, code should be deployed at regular intervals throughout the development and deployment process, opening up the code to QA and security audits by the public, and 3rd parties. I hope this approach evolves into more of a continuous deployment and integration workflow when it comes to delivering software in government, where vendors have to plugin, open up their delivery cycles to more scrutiny, and leverage Github as the center of each procurement step from start to finish. Heck, let’s connect payments to each stop along the way.</p>

<p>I’m a proponent of this not just to make the delivery of government software more observable and accountable. I want this process out in the open to help other agencies learn from the journey. Tune into the process, and maybe reuse, build upon, and evolve existing solutions as part of their operations. I will keep an eye on what is going on up in Canada, when it comes to requiring vendors publish code to Github. I also know there are similar efforts in the U.S. and other countries which I’ll also start scratching at and learning more about. It is definitely a healthy pattern I’d like to see more of, and I will continue to invest time shining a light on any government taking a lead like Canada is.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/09/continous-integration-and-deployment-for-government-procurement/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/09/an-open-source-api-security-intelligence-gathering-processing-and-distribution-framework/">An Open Source API Security Intelligence Gathering, Processing, And Distribution Framework</a></h3>
        <span class="post-date">09 Aug 2017</span>
        <p><a href="https://blogs.cisco.com/security/open-source-threat-intel-gosint"><img src="https://s3.amazonaws.com/kinlane-productions/GOSINT/gosint.gif" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I was reading about GOSINT, <a href="https://blogs.cisco.com/security/open-source-threat-intel-gosint">the open source intelligence gathering and processing framework over at Cisco</a>. “GOSINT allows a security analyst to collect and standardize structured and unstructured threat intelligence. Applying threat intelligence to security operations enriches alert data with additional confidence, context, and co-occurrence. This means that you are applying research from third parties to your event data to identify similar, or identical, indicators of malicious behavior.” The framework is written in Go, with a front-end in JavaScript frontend, and usage of APIs as threat intelligence sources.</p>

<p><a href="https://github.com/ciscocsirt/gosint">When you look at configuration section on the README for GOSINT</a>, you’ll see information for setting up threat intelligence feeds, including Twitter API, <a href="https://otx.alienvault.com/api/">Alien Vault the Open Threat Community API</a>, <a href="https://www.virustotal.com/">VirusTotal API</a>, and <a href="https://crits.github.io/">the Collaborative Research Into Threats (CRITS)</a>.  GOSINT acts as an API aggregator for a variety of threat information, which then allows you to scour the information for threat indicators, which you can evolve over time, providing a pretty interesting model for not just threat information sharing, but also API driven aggregation, curation and sharing.</p>

<p>GOSINT also has the notion of behaving as a “transfer station”, where you can export refined data as CSV or CRITS format. Right here seems like an opportunity for some Github integration, adding continuous integration and deployment to open source intelligence and processing workflows. Making sure refined, relevant threat information is available where it is needed, via existing API deployment and integration workflows. Wouldn’t take much to publish CSV, YAML, and JSON files to Github which can then be used to drive distributed dashboards, visualizations, and other awareness building tools. Plus, the refined threat information is now published as CSV/JSON/YAML on Github where it can be ingested by any system of application with access to the Github repository.</p>

<p>GOSINT is just one of the interesting tooling I’m coming across as I turn up the volume on <a href="http://security.apievangelist.com">my API security research</a>, thanks to <a href="https://www.elasticbeam.com/">the investment of ElasticBeam my API security partner</a>. They’ve invested in an API security guide, as well as white paper, which is something that will generate a wealth of stories like this along the way, as I find interesting API security artifacts. I’m looking to map out the API security landscape, but I’m also interested in understanding open source API aggregation, analysis, and syndication platforms that integrate with existing CI/CD workflows, to help feed <a href="http://org.open.referral.adopta.agency/">my existing human services API work</a>, and other city, state, and federal government API projects I’m working on.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/09/an-open-source-api-security-intelligence-gathering-processing-and-distribution-framework/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/09/open-sourcing-your-api-like-version-eye/">Open Sourcing Your API Like VersionEye</a></h3>
        <span class="post-date">09 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/versioneye/version-eye-containers.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m always on the hunt for healthy patterns that I would like to see API providers, and API service providers consider when crafting their own strategies. It’s what I do as the API Evangelist. Find common patterns. Understand the good ones, and the bad ones. Tell stories about both, helping folks understand the possibilities, and what they should be thinking about as they plan their operations.</p>

<p>One very useful API that notifies you about security vulnerabilities, license violations and out-dated dependencies in your Git repositories, <a href="https://www.versioneye.com/api/">has a nice approach to delivering their API</a>, as well as <a href="https://github.com/versioneye">the other components of their stack</a>. You can either use VersionEye in the cloud, or you can deploy on-premise:</p>

<ul>
  <li><a href="https://github.com/versioneye/versioneye-core">versioneye-core</a> - Models, Services &amp; Mails for VersionEye</li>
  <li><a href="https://github.com/versioneye/crawl_r">crawl_r</a> - VersionEye crawlers implemented in Ruby.</li>
  <li><a href="https://github.com/versioneye/versioneye-security">versioneye-security</a> - Security Crawler for VersionEye</li>
  <li><a href="https://github.com/versioneye/versioneye-api">versioneye-api</a> - JSON REST API for VersionEye</li>
  <li><a href="https://github.com/versioneye/versioneye-tasks">versioneye-tasks</a> - Thin wrapper around the versioneye-core.</li>
  <li><a href="https://github.com/versioneye/versioneye">versioneye</a> - VersionEye.com</li>
</ul>

<p><a href="https://hub.docker.com/u/versioneye/">VersionEye also has their entire stack available as Docker images</a>, ready for deployment anywhere you need them. I wanted have a single post that I can reference when talking about possible open source, on-premise, continuous integration approaches to delivering API solutions, that actually have a sensible business model. VersionEye spans the areas that I think API providers should consider investing in, delivering SaaS or on-premise, while also delivering open source solutions, and generating sensible amounts of revenue.</p>

<p>Many APIs I come across do not have an open source version of their API. They may have open source SDKs, and other tooling on Github, but rarely does an API provider offer up an open source copy of their API, as well as Docker images. VersionEye’s approach to operating in the cloud, and on-premise, while leveraging open source and APIs, as well as dovetailing with existing continuous integration flows is worth bookmarking. I am feeling like this is the future of API deployment and consumption, but don’t get nervous, there is still plenty of money to be be made via the cloud services.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/09/open-sourcing-your-api-like-version-eye/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/09/a-fresh-look-at-the-embeddable-tools-built-on-the-twitter-api/">A Fresh Look At The Embeddable Tools Built On The Twitter API</a></h3>
        <span class="post-date">09 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/twitter/twitter-websites-embeddable.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>Over the years I have regularly showcased Twitter as an example API driven embeddable tools like buttons, badges, and widgets. In 2017, after spending some time in the Twitter developer portal, it is good to see <a href="https://dev.twitter.com/products/web">Twitter still investing in their embeddable tools</a>. The landing page for the Twitter embeddables still provides the best example out there of the value of using APIs to drive data and content across a large number of remote web sites.</p>

<p>Twitter has distinct elements of their web embeddables:</p>

<ul>
  <li>Tweet Button - That classic tweet button, allowing users to quickly Tweet from any website.</li>
  <li>Embedded Tweets - Taking any Tweet and embedding on a web page showing its full content.</li>
  <li>Embedded Timeline - Showing curated timelines on any website using a Twitter embeddable widget.</li>
  <li>Follow Button - Helping users quickly follow your Twitter account, or your companies Twitter account.</li>
  <li>Twitter Cards - Present link summaries, engaging images, product information, or inline video as embeddable cards in timeline.</li>
</ul>

<p>Account interactions, messaging, posting, and other API enabled function made portable using JavaScript allowing it to be embedded and executed on any website. JavaScript widgets, buttons, and other embeddables are still a very tangible, useful example of APIs in action. Something I can talk about to anyone about, helping them understand why you might want to do APIs, or at least know about APIs.</p>

<p>We bash on Twitter a lot in the API community. However, after a decade of operation, you have to give it to them. They are still doing it. They are still keeping it simple with embeddable tools like this. I can confidently say that APIs are automating some serious illness on the Twitter API platform at the moment, and there are many things I’d like to be different with the Twitter API, but I am still pleased that I can keep finding examples from the Twitter platform to showcase on API Evangelist seven years of writing about them.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/09/a-fresh-look-at-the-embeddable-tools-built-on-the-twitter-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/08/patent-web-of-trust-management-in-a-distributed-system/">Patent #9397835, Web of trust management in a distributed system</a></h3>
        <span class="post-date">08 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/gypsy-eyes_blue_circuit.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I found a couple more API patents in my notebook that I wanted to get published. I try to take time regularly to publish the strangest API related patents I can find. Today’s patent is out of Amazon, which I find to be a fascinating outlet for patent storytelling. It isn’t squarely in the realm of APIs like some of my others, but I think tells a fascinating story by itself, showing how the web and the concept of a patent are colliding.</p>

<p>Title - <a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=/netahtml/PTO/search-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN/9397835">Web of trust management in a distributed system</a><br />
Number - 9397835<br />
Owner - Amazon Technologies, Inc.<br />
Publication Date - 2016-07-19<br />
Application Date - 2014-05-21<br /></p>

<p>Abstract - A web of trust is used to validate states of a distributed system. The distributed system operates based at least in part on a domain trust. A root of trust issues the domain trust issues a domain trust. Domain trusts are updatable in accordance with rules of previous domain trusts so that a version of a domain trust is verifiable by verifying a chain of previous domain trust versions._</p>

<p>I like that trust is being patented. Digital trust as a patentable concept that Amazon can now delegate if they choose. I’m just fascinated by what concepts are now fair game for patenting, as they enter into the digital realm. Now I’m curious how many physical trust patents might exist. Is the management of trust patented in the physical world in any way? I guess I could see some of the components for determining trust could be patented, but I find the fact that trust, or more specifically trust management is patentable, as a troubling thing.</p>

<p>It’s no secret that I’m anti API patents. I’m rarely convinced of the uniqueness of anything digital, warranting the issuing of a patent by the USPTO. I have to say that in a world where trust is patentable, the environment for suspect behavior will flourish. Pretty much what we are seeing play out on the web on a daily basis. I’m adding this patent to my <a href="http://authentication.apievangelist.com/">API authentication</a> and <a href="http://security.apievangelist.com/">API security</a> research, because it is a component that exists already in these areas, and should be readily available for any provider to execute as they see fit.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/08/patent-web-of-trust-management-in-a-distributed-system/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/08/http-as-substrate/">HTTP as a Substrate</a></h3>
        <span class="post-date">08 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/drone-recovery/babyfoot04.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am spending a significant amount of time reading RFCs lately. I find the documents to be very cumbersome to read, but the more you read, the more tolerant you become. When I browse through RFCs I’m always reminded of how little I actually know about the web. In an effort to push forward my education, and maybe yours along the way, I’m going to be cherry picking specific sections of the interesting RFCs I’m digesting here on the blog. <a href="https://tools.ietf.org/html/rfc3205">Today’s RFC is 3205, filed under Best Current Practice”, and is on the use of HTTP as a Substrate</a>.</p>

<p>_Recently there has been widespread interest in using Hypertext Transfer Protocol (HTTP) [1] as a substrate for other applications- level protocols. Various reasons cited for this interest have included:</p>

<ul>
  <li>familiarity and mindshare,</li>
  <li>compatibility with widely deployed browsers,</li>
  <li>ability to reuse existing servers and client libraries,</li>
  <li>ease of prototyping servers using CGI scripts and similar extension mechanisms, authentication  and SSL or TLS,</li>
  <li>the ability of HTTP to traverse firewalls, and</li>
  <li>cases where a server often needs to support HTTP anyway.</li>
</ul>

<p>The Internet community has a long tradition of protocol reuse, dating back to the use of Telnet as a substrate for FTP and SMTP.  However, the recent interest in layering new protocols over HTTP has raised a number of questions when such use is appropriate, and the proper way to use HTTP in contexts where it is appropriate. Specifically, for a given application that is layered on top of HTTP:</p>

<ul>
  <li>Should the application use a different port than the HTTP default  of 80?</li>
  <li>Should the application use traditional HTTP methods (GET, POST,  etc.) or should it define new methods?</li>
  <li>Should the application use http: URLs or define its own prefix?</li>
  <li>Should the application define its own MIME-types, or use something  that already exists (like registering a new type of MIME-directory structure)?</li>
</ul>

<p>This memo recommends certain design decisions in answer to these  questions.</p>

<p>This memo is intended as advice and recommendation for protocol designers, working groups, implementors, and IESG, rather than as a strict set of rules which must be adhered to in all cases. Accordingly, the capitalized key words defined in RFC 2119, which are intended to indicate conformance to a specification, are not used in this memo._</p>

<p>I love the notion of <a href="https://tools.ietf.org/html/rfc3205">HTTP as a substrate</a>. The definition of substrate is “a substance or layer that underlies something, or on which some process occurs, in particular” or also, “the surface or material on or from which an organism lives, grows, or obtains its nourishment”. I also love the notion of providing guidance for this line of thought. There are many things contained in this document I have learned from my time in the space, and included in my storytelling, without much thought regarding where it originated, or how accurate it was. I particularly like the notion of HTTP as a material in which an organism lives, but maybe more of a digital organism, or a bot. A reminder that everything that may take seed, flourish and grow in this environment, might not always be a good thing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/08/http-as-substrate/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/08/api-message-integry-with-json-web-token-jwt/">API Message Integrity with JSON Web Token (JWT)</a></h3>
        <span class="post-date">08 Aug 2017</span>
        <p><a href="https://jwt.io"><img src="https://s3.amazonaws.com/kinlane-productions/json-web-token/json-web-token.png" align="right" width="30%" style="padding: 15px;" /></a></p>
<p>I don’t have any production experience deploying <a href="https://jwt.io/introduction/">JSON Web Tokens (JWT)</a>, but it has been something I’ve been reading up on, and staying in tune with for some time. I often reference JWT as the leading edge for API authentication, but there is one aspect of JWT I think is worth me referencing more often–message integrity. JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.</p>

<p>JWT can not only be used for authentication of both message sender/receiver, it can ensure the message integrity as well, leveraging a digital signature hash value of the message body to ensure the message integrity during transmission. It adds another interesting dimension to the <a href="http://security.apievangelist.com">API security</a> conversation, and while not be applicable in all APIs, I know many that it would make a lot of sense. Many of the networks we use today and applications we depend on today are proxied, creating an environment where message integrity should always come into question, and JWT gives us another tool in our toolbox to help us keep things secure.</p>

<p>I’m working my way through each layer of API operations, looking for aspects of API security that are often obscured, hidden, or just not discussed as they should be. I feel like JWT is definitely one track of API security that has evolved the conversation significantly over the last couple years, and is something that can make a significant impact on the space with just a little more storytelling and education. I’m going to make sure API request and response message integrity is a regular part of my API security storytelling, curriculum, and live talks that I develop.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/08/api-message-integry-with-json-web-token-jwt/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/08/reducing-developers-to-a-transaction-with-apis-microservices-serverless-devops-and-the-blockchain/">Reducing Developers To A Transaction With APIs, Microservices, Serverless, Devops, and the Blockchain</a></h3>
        <span class="post-date">08 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/containership_dark_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>A topic that keeps coming up in discussions with my partner in crime Audrey Watters (@audreywatters) about our podcast is around the future of labor in an API world. I have not written anything about this, which means I’m still in early stages of any research into this area, but it has come up in conversation, and reflected regularly in my monitoring of the API space, I need to begin working through my ideas in this area. A process that helps me better see what is coming down the API pipes, and fill the gaps in what I do not know.</p>

<p>Audrey has long joked about my API world using a simple phrase: “reducing everything to a transaction”. She says it mostly in jest, but other times I feel like she wields it as the Cassandra she channels. I actually bring up the phrase more than she does, because it is something I regularly find myself working in the service of as the API Evangelist. By taking a pro API stance I am actively working to reduce legacy business, institutional, and government processes down and breaking them down into a variety of individual tasks, or if you see things through a commercial lens, transactions.</p>

<p><strong>Microservices</strong><br />
A microservices philosophy is all about breaking down monoliths into small bite size chunks, so they can be transacted independently, scaled, evolved, and deprecated in isolation. Microservices should do one thing, and do it well (no backtalk). Microservices should do what it does as efficiently as possible, with as few dependencies as possible. Microservices are self-contained, self-sufficient, and have everything they need to get the job done under a single definition of a service (a real John Wayne of compute). And of course, everything has an API. Microservices aren’t just about decoupling the technology, they are are about decoupling the business, and the politics of doing business within SMB, SME, enterprises, institutions, and government agencies–the philosophy for reducing everything to a transaction.</p>

<p><strong>Containers</strong><br />
A microservice way of thinking about software that is born in the clouds, a bi-product of virtualization and API-ization of IT resources like storage and compute. In the last decade, as IT services moved from the basement of companies into the cloud, a new approach to delivering the compute, storage, and scalability needed to drive this new microservices way of doing business emerged that was called containers. In 2017 businesses are being containerized. The enterprise monolith is being reduced down to small transactions, putting the technology, business, and politics of each business transaction into a single container, for more efficient development, deployment, scaling, and management. Containers are the vehicle moving the microservices philosophy forward–the virtualized embodiment of reducing everything to a transaction.</p>

<p><strong>Serverless</strong><br />
Alongside a microservice way of life, driven by containerization, is another technological trend (undertow) called serverless. With the entire IT backend being virtualized in the cloud, the notion of the server is disappearing, lightening the load for developers in their quest for containerizing everything, turning the business landscape into microservices, than can be distilled down to a single, simple, executable, scalable function. Serverless is the codified conveyor belt of transactions rolling by each worker on the factory floor. Each slot on a containerized, serverless, microservices factory floor possessing a single script or function, allowing each transaction to be executed,  and replicated allowing it to be applied over and over, scaled, and fixed as needed. Serverless is the big metal stamping station along a multidimensional digital factory assembly line.</p>

<p><strong>DevOps</strong><br />
Living in microservices land, with everything neatly in containers, being assembled, developed, and wrenched on by developers, you are increasingly given more (or less) control over the conveyor belt that rolls by you on the factory floor. As a transaction developer you are given the ability to change direction of your conveyor belt, speed things up, apply one or many metal stamp templates, and orchestrate as much, or as little of the transaction supply chain as you can keep up with (meritocracy 5.3.4). Some transaction developers will be closer to the title of architect, understanding larger portions of the transaction supply chain, while most will be specialized, applying one or a handful of transaction templates, with no training or awareness of the bigger picture, simply pulling the Devops knobs and levers within their reach.</p>

<p><strong>Blockchain</strong><br />
Another trend (undertow) that has been building for sometime, that I have managed to ignore as much as I can (until recently) is the blockchain. Blockchain and the emergence of API driven smart contracts has brought the technology front and center for me, making it something i can ignore, as I see signs that each API transaction will soon be put in the blockchain. The blockchain appears to becoming the decentralized (ha!) and encrypted manifestation of what many of us has been calling the API contract for years. <a href="https://azure.microsoft.com/en-us/blog/introducing-enterprise-smart-contracts/">I am seeing movements from all the major cloud providers</a>, and lesser known API providers to <a href="https://medium.com/@kevinsimper/serverless-is-the-jump-before-ethereum-smart-contracts-121d92e67426">ensure that all transactions are put into the blockchain</a>, providing a record of everything that flows through API pipes, and has been decoupled, containerized, rendered as serverless, and available for devops orchestration.</p>

<p><strong>Ignorance of Labor</strong><br />
I am not an expert in labor, unions, and markets. Hell, I still haven’t even finished my Marx and Engels Reader. But, I know enough to be able to see that us developers are fucking ourselves right now. Our quest to reduce everything to a transaction, decouple all the things, and containerize and render them serverless makes us the perfect tool(s) for some pretty dark working conditions. Sure, some of us will have the bigger picture, and make a decent living being architects. The rest of us will become digital assembly line workers, stamping, maintaining a handful of services that do one thing and do it well. We will be completely unaware of dependencies, or how things are orchestrated, barely able to stay afloat, pay the bills, leaving us thankful for any transactions sent our way.</p>

<p>Think of this frontline in terms of Amazon Mechanical Turk + API + Microservices + Containers + Serverless + Blockhain. There is a reason young developers make for good soldiers on this front line. Lack of awareness of history. Lack of awareness of labor. Makes great digital factory floor workers, stamping transactions for reuse elsewhere in the digital assembly line process. This model will fit well with current Silicon Valley culture. There will still be enough opportunity in this environment for architects and cybersecurity theater conductors to make money, exploit, and generate wealth. Without the defense of unions, government or institutions, us developers will find ourselves reduced to transactions, stamping out other transactions on the digital assembly line floor.</p>

<p>I know you think your savvy. I used to think this too. Then after having the rug pulled out from under me, and the game changed around me by business partners, investors, and other actors who were playing a game I’m not familiar with, I have become more critical. You can look around the landscape right now and see numerous ways in which power has set its sights on the web, and completely distorting any notion of the web being democratic, open, inclusive, or safe environment. Why do us developers think it will be any different wit us? Oh yeah, privilege.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/08/reducing-developers-to-a-transaction-with-apis-microservices-serverless-devops-and-the-blockchain/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/08/my-url-shortner-is-just-an-api-with-postman-as-my-client/">My URL Shortener Is Just An API With Postman As My Client</a></h3>
        <span class="post-date">08 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-url-square.png" align="right" width="25%" style="padding: 15px;" /></p>
<p>I have my own URL shortener for API Evangelist called apis.how. I use it to track the click through rates for some of my research projects, and partner sponsorships. I’ve had the URL shortener in operation for about two years now, and I still do not have any type of UI for it, relying 100% on Postman for adding, searching, and managing the URLs I am shortening, and tracking on.</p>

<p>My URL shortener just hasn’t raised to a level of priority where I’ll invest any time into an administrative interface, or dashboard for my URL shortener. I used Bitly and Google for a while, but I really just needed a simple shortening with basic counts, nothing more. When I bought the domain I launched a handful of API endpoints to support, allowing me to add, update, search, and remove URLs, as well as track the click throughs, and query how many clicks a link received for each mont. I can easily accomplish all of this through the Postman interface, making basic calls to my simple API–no over-engineering necessary.</p>

<p>I have been considering running a daily job that pulls view counts for URLs and publishing to Github as YAML, where I can drive a simple visualization, but honestly I’m not that numbers oriented. I like what API clients like <a href="https://www.getpostman.com/">Postman</a> and <a href="https://restlet.com/">Restlet</a> provide. Even though I’m equipped to make calls using JavaScript, PHP, or CURL, I prefer just accessing via my web client–no coding necessary. I wouldn’t manage all my systems in this way, but for really basic ones like my URL shortener, I’m not sure I will ever actually evolve it beyond just being a simple API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/08/my-url-shortner-is-just-an-api-with-postman-as-my-client/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/07/the-subtle-ways-in-which-power-asserts-itself-in-face-of-api-engagements/">The Subtle Ways In Which Power Asserts Itself In Face Of API Engagements</a></h3>
        <span class="post-date">07 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/power-lines-empty-space_copper_circuit.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m rarely surprised by, but still often caught off guard by the subtle ways in which power asserts itself when faced with change the introduced by API projects. In my 30 years as a database professional I’ve seen numerous overt, covert, and subversive ways in which existing holders of power (data), but I often still get blindsided by the creative, and subtle ways in which folks defend what they already have, and work to keep things from changing.</p>

<p>While doing unfunded work to define industry level API specifications, and help move forward the API conversation in multiple industries, I’ve been encountering two pockets of friction I want to understand better, so I can develop some sort of grease, that might make things smoother. There are two main pockets of practitioners in this setting, implementors (those you publish an API), and vendors (those who currently sell solutions to implementors). My role in any industry as the API Evangelist is to help ultimately define and showcase healthy, common API definitions, that can be reused across the API lifecycle–from design to deprecation.</p>

<p><strong>The Vendor Question</strong>
Trying to understand the culture, and motivations of any SMB, SME, or enterprise is often a full time job I do not have the time or resources for. I work pretty hard to understand any of the key players in any industry being touched by APIs, but will still have much I need to learn. One thing I do know, is that I should never take a company for face value, because behind the mask there is always much more going on. Honestly, APIs are actually a pretty good barometer of what is going on behind the scenes, something I don’t think some API providers fully grasp before they decide to jump into the API game.</p>

<p>The most obvious thing a vendor in an industry will do is ignore you. When approached about participating in discussions around a common API definition, they’ll just not return your emails, tweets, and other front door solicitations–this is fine, many are busy, or just very confident in their place within an industry. Next, you’ll get the folks who will join you at the table in some capacity, or other. Most are busy, and will have limited time–that is understood. What I am trying to understand is their behavior once they are at the table, because I feel that many of them aren’t actually at the table to participate. They are at the table to be present, influence the discussion, which often times might mean keeping the discussion from moving forward at all.</p>

<p>As someone who does not have a job, and regular paycheck, I’m fascinated by the amount of time that can be wasted by SMB, SME, and enterprise folks when you are trying to talk about a common API definition. I don’t care if you have a machine readable way to define the conversation using OpenAPI for the API, and JSON schema for the schema, folks will find a way to dance around any clear process and framework. The first hurdle is always learning to speak OpenAPI and JSON schema, and helping vendors become fluent in OpenAPI, allowing conversations be anchored to specific APIs, paths, headers, parameters, responses, and schema. This is no guarantee of forward motion, but it does help keep things focused, and potentially productive around establishment of a common API contract.</p>

<p>With vendors, all roads lead to their products, and their solutions. The question is always how we can best get them to invest in shared public API infrastructure, over routing all conversations to their platform, on their roads. Money and investment is always the quickest way I see vendors take control of conversations, and route things in their direction. Let’s work on this project, or this infrastructure involving a specific implementation of our choosing, and we’ll move forward the API standards conversation in those terms, and within the context of our choosing and funding. Sure, some environments might actually be conducive to moving things forward, but I think we need to think deeply about how we decouple the technology, business, and politics of all vendor project engagements.</p>

<p>All vendors are suspect. Even the nice ones. Honestly, those are the ones that worry me the most. I’ve had way more time of mine wasted by nice vendors, talking about participating, putting meetings in the calendar, and collaborating on press releases that never see the light of day, to know better. Honestly, the opinionated, straight forward vendors are the ones I worry about least–you know where they stand. From my view all vendor projects need to have a defined (de)coupling from specific vendor interests and the standards interest. Keeping this API is the best way do this, as the central API specification will have an OpenAPI and JSON schema, as will the individual project. Using OpenAPI as a contract for vendor engagement provides a nice scaffolding for this (de)coupling, keeping any billable hours spent measured by commits to the OpenAPI, JSON Schema, or actually now that I’m thinking about it an APIs.json or other discovery document.</p>

<p>Now I am getting to the goal of writing this post. Each vendor related project will begin and end as a Github repository with an OpenAPI, JSON Schema, and APIs.json as the central definition, and contract. This will be the central (de)coupling business contract, even if the vendor, and other parties are not fluent in OpenAPI, JSON Schema, or APIs.json. I will maintain the contract, and communicate it out in all proposals, meetings, and project documentation, encouraging awareness and literacy around all three specifications when possible. With API definitions published to Github as central contract, it regularly allows me to assess progress made, and measure the effectiveness of individual projects, making decision on how to fund and move forward, as well as cut them loose when they are no longer making sense.</p>

<p><strong>The Implementors Dilemma</strong>
Individual API implementations are an entirely different beast from vendors, and while they may live within the tractor beam of one or many vendors, they often have their own silos, resource deficient, and organizational isolation that drives it’s quest to retain power. In each of these industries I am approaching things from a position of harmonization, standardization, interoperability, and reuse, which is often out of view of individual implementors. Individual industry level API implementations want to be successful in their context, and often want the benefits of harmonization, standardization, interoperability, and reuse that are being suggested, but rarely want to, or have the time to do the had work needed to make necessary changes. This will manifest itself in many different ways when you are trying to talk about API standards, something that is often in response to, or driven by the power motivations of vendors who have their ear.</p>

<p>Literacy and awareness are the biggest challenges with individual implementors. Whether federal government, city government, higher educational institutions, or small regional nonprofit organizations, almost every group I come across is underfunded, and under-resourced when it comes to their IT needs, and the delivery of data and content systems. Many folks I meet are well meaning, but lack the skills and training necessary to deliver scalable IT systems, having often landed in their jobs by accident, and being in the right place at the right time. Some are aware of these deficiencies, but lack time and resources they need to get the training they need, while others are completely unaware, and in denial, often taking a very hostile and combative approach when it comes to new ideas, even if they may seem like common practice in the wider API sector.</p>

<p>The majority of organizations I come across are not ready for API. They are not confident in what they know, and what they do not know. They want to share data and information, but often aren’t setup technically or culturally to share data and information. Implementors want to be able to leverage data from other implementations, and reuse open source code, services, and tooling, but will rarely want to do the hard work required to reciprocate in these environments. While well meaning, many are so far behind because of many decisions they’ve made along the way to stay behind, and will require a significant amount of training and investment to bring into alignment with industry-wide standards. Some will never be brought into alignment, and should be replaced–whether they are well meaning or not.</p>

<p>Similar to all vendors, each individual API implementation should have it’s own contract and repository. If an OpenAPI cannot be mocked for an implementation, bringing into focus existing schema, and access requirements for an individual API implementation, it is unlikely that an implementation will ever be moved forward. OpenAPI, JSON Schema, and APIs.json help quantify:</p>

<ul>
  <li>API Operations - What APIs are there, as well as all supporting resources like documentation, registration, terms of service, and other things that govern doing business online today.</li>
  <li>API Access - What are the technical details for each API path, parameter, and schema, providing a detailed list of what each access request and response involves.</li>
  <li>Data &amp; Content - What data and content is being made available, often reflecting backend databases and other systems, but ideally reflects common schema that can be reused in other systems.</li>
</ul>

<p>If an organization can not come together have a constructive conversation around access to data, content as part of a new approach to working with external groups via an API operations, it is unlikely and individual implementation will ever be viable. OpenAPI, JSON Schema, and APIs.json can all exist as YAML documents, which is (should be) plain language, no code, brackets or otherwise, allowing business and technical groups to both participate. Similar to vendor implementations, I will be providing oversight of each Github repository, and central project OpenAPI, JSON Schema, and APIs.json index, but ideally each individual implementation becomes the keeper of the contract for each of their projects, requiring only minimal engagement from other implementations, vendors, or at the standards level of the discussion.</p>

<p><strong>Bringing This Home For Next Round Of Conversations</strong>
Ok, I actually have to invest in the next rounds of these industry level API discussions. I think I’ve settled in on my Github repo managed, OpenAPI, and APIs.json governance of these large number of relationships I need to engage in, manage, and measure. I’ve been blindsided by several conversations at the implementation, vendor, and industry levels, across city and federal government levels, and small, medium, and larger corporations lately. My goal in this post is to help me mitigate damage in future conversations, and keep myself moving along at a faster pace, with least amount of friction on my end as I can. When dealing with so many personalities, as I am across a handful of industries, it is something can be very exhausting if I do not find a way to insulate myself.</p>

<p>Each vendor and implementation involved with the defining of an industry API standard with live as a Github repository. Being able to sign up for a Github account, and participate in issues is a minimum entry fee. Not that they’ll be excluded. They’ll just be identified as making a choice to stay behind. The presence and robustness of an APIs.json will be the next layer of definition, providing essential links to API operations for either the vendor or the implementation. In my opinion, this is always a great way to size up a company, organization, institutions, and government agency. Similar to how you can size up an organization by their web, and social media presence, if they don’t have an existing API or open data effort already, there are other deficiencies right around the corner, or behind the firewall.</p>

<p>Next, the presence and evolution of a coherent OpenAPI and JSON Schema will bring into focus the viability of a vendor partnership, and individual implementation viability. If an OpenAPI can be established and moved forward, there is potential. If there hasn’t been commits in months, and difference between commits productive, with a healthy amount of issue activity, I’m guessing a vendor relationship, or individual API implementation needs further assessment regarding its viability, and sustainability. Of course, any final decisions are made by a human (me), as everything I’m discussing here operates under the API Evangelist, and Adopta.Agency network of API research projects.</p>

<p>The majority of my time as API Evangelist is spent researching the technology, business, and politics of API operations across ALL industries. I’ve created my Adopta.Agency domain to help me manage my investment in common API definitions, schema, and actual implementations of public open data and API projects. I’m trying to establish an approach to help define common OpenAPI, JSON Schema, and APIs.json for a number of industries, that allows me to move forward a number of conversations, with the least amount of friction, and impact on me. I don’t enjoy getting sucked into politics, and prefer staying at the highest level possible when it comes to helping influence what is API today–something that is impacting a significant portion of our world, not just the tech sector.</p>

<p>I’ve been tracking on many different API industries using small data driven Github repositories for three years now. <a href="http://theapistack.com">I track on over 1500 APIs using my API Stack research</a>, which has morphed into what I call the Stack.Network, where I profile specific industries or sectors of how we do business online today–companies like <a href="http://amazon.web.services.stack.network/">Amazon</a>, <a href="http://microsoft.stack.network/">Microsoft</a>, and <a href="http://google.stack.network/">Google</a>. My API Stack research is all about identifying common patterns across individual API implementations, aggregating them into over 1500 individual building blocks that contribute to my definition of <a href="http://apievangelist.com/api-lifecycle/">the almost 100 stops along a modern API lifecycle</a>. Each of my stacks tends to represent an industry like SMS, or news, but I also have individual company stacks like <a href="http://facebook.stack.network/">Facebook</a>, or possibly even <a href="http://gsa.index.apievangelist.com/">the federal government like with the General Services Administration (GSA)</a>. Each of these projects have an OpenAPI, JSON Schema, and APIs.json core, which gives me a distilled down definition of each stack for inclusion across my research.</p>

<p>I’m looking to define I want to expand my existing approach to quantifying the world of APIs to include specific APIs, schema, implementations, and collections, and open it up for collaboration and discussion with a variety of vendors who may serve multiple industries, as well as specific implementations within an industry. Honestly, nothing really changes for me in all of this, except that I will be encouraging vendors and implementations to at least participate, and ideally move forward their own OpenAPI, JSON Schema, and <a href="http://apisjson.org">APIs.json</a>. I’m also looking to encourage them to be a little more Github fluent when it comes to defining a common industry API definition and schema. I see Github as just another social network like Facebook, Twitter, Instagram, and Pinterest, except the actions taken often revolve around code, and machine readable templates, instead of messages, images, video, and other common social building blocks.</p>

<p>Ok, phew. If you are still reading this, you should probably develop a hobby. I’m writing this to help me get my ducks in order. If it makes sense to you, and helps you in your API journey–awesome! I feel I have to regularly write about the feels I get in my working as the API Evangelist. It is about me talking through what I’m seeing, and finding a way forward. The scope at which some of these API conversations are occurring at can be dizzying, and if I don’t step back from time to time, and just work through my feels, they tend to pile up and I stumble more. This post help me come around full circle to realize some of the new projects I’ve been taking on are actually just an extension of what I’ve already been doing as I map the world of APIs, I’m just turning up the intimacy level on some of the vendor and implementation level relationships. However I’ve done a pretty good job of building an insulator between me an the space, I just need to keep doing what I’ve been doing for the last seven years, and confidently push some more of these common API definitions, schema, and collections out the door.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/07/the-subtle-ways-in-which-power-asserts-itself-in-face-of-api-engagements/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/07/slow-moving-ransomware-is-the-new-business-model/">Slow Moving Ransomware As The New Business Model</a></h3>
        <span class="post-date">07 Aug 2017</span>
        <p><a href="https://thebioscope.net/2010/11/23/tied-to-the-tracks/"><img src="https://bioscopic.files.wordpress.com/2010/11/barneyoldfield.jpg" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="https://gcn.com/Articles/2017/07/06/NYPD-Palantir-crime-analysis.aspx">I was reading about the difficulties the City of New York was having when it comes to migrating off of the Palantir platform</a>, while also reading about the latest cybersecurity drama involving ransomware. I’m spending a lot of time studying cybersecurity lately, partly because they involve APIs, but mostly because it is something that is impacting every aspect of our lives, including our democracy, education, and healthcare. One thing I notice on the cybersecurity stage, is that everything is a much more extreme, intense, representation of what is going on in the mainstream tech industry.</p>

<p>Ransomware is software that gets installed on your desktop or servers and locks up all your data until you pay the software developer (implementor) a ransom. Ransomware is just a much faster moving version of what many of us in the software industry call vendor lock-in. This is what you are seeing with Palantir, and the City of New York. What tech companies do is get you to install their software on your desktop or servers, or convince you to upload all your data into the cloud, and use their software. This is business 101 in the tech industry. You either develop cloud-based software, something that runs on-premise, or you are a mix of both. Ideally, your customers become dependent on you, and they keep paying your monthly, quarterly, or annual subscriptions (cough cough ransom).</p>

<p>Here is where the crafty API part of the scheme comes in. Software providers can also make APIs that allow your desktop and server to integrate with their cloud solutions, allowing for much deeper integration of data, content, and algorithms. The presence of APIs SHOULD also mean that you can more easily get your data, content, and algorithms back, or have kept in sync the whole time, so that when you are ready to move on, you don’t have a problem getting your data and content back. The problem is, that APIs “CAN” enable this, but in many situations providers do not actually give you complete access to your data, content, or algorithms via API, and enable the true data portability and sync features you need to continue doing business without them.</p>

<p>This is vendor lock-in. It is a much friendlier, slower moving version of ransomware. As a software vendor you want your software to be baked in to a customers operations so they are dependent on you. How aggressively you pursue this, and how much you limit data portability, and interoperability, dictates whether you are just doing business as usual, or engaging in vendor lock-in. One thing I’m hopeful for in all of this, are the vendors who see transparency, observability, interoperability, and portability of not just the technical, but also the business and politics of delivering technology as a legitimate competitive advantage. This means that they will always be able to out maneuver, and stay ahead of software vendors who practice vendor lock-in and ransomware, whether of the slow or fast moving variety.</p>

<p>Photo Credit: <a href="https://thebioscope.net/2010/11/23/tied-to-the-tracks/">Ford Sterling with the sledgehammer and Mabel Normand tied in the rails in Barney Oldfield’s Race for a Life (1913), from moma.org</a>.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/07/slow-moving-ransomware-is-the-new-business-model/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/07/api-industry-standards-negotiation-by-media-type/">API Industry Standards Negotiation By Media Type</a></h3>
        <span class="post-date">07 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-services.png" align="right" width="30%" style="padding: 15px;" /></p>
<p>I am trying to help push forward the conversation around <a href="https://openreferral.github.io/api-specification/definition/">the API definition for the Human Services Data Specification (HSDS)</a> in a constructive way amidst a number of competing interests. I was handed a schema for sharing data about about organizations, locations, and services in a CSV format. I took this schema and exposed it with a set of API paths, keeping the flat file structure in tact, making no assumptions around how someone would need to access the data. I simply added the ability to get HSDS over the web as JSON–I would like to extend to be HTML, CSV, JSON, and XML, reaching as wide as possible audience with the basic implementation.</p>

<p>As we move forward discussions around HSDS and HSDA I’m looking to use media types to help separate the different types of access people are looking for using media types. I don’t want to leave folks who only have basic CSV export or import capabilities behind, but still wanted to provide guidance for exchanging HSDA over the web. To help organize higher levels of demand on the HSDS schema I’m going to break out into some specialized media types as well as the default set:</p>

<ul>
  <li><strong>Human Services Data Specification (HSDS)</strong> - text/csv - Keeping data package basic, spreadsheet friendly, yet portable and exchangeable.</li>
  <li><strong>Human Services Data API (HSDA)</strong> - application/json and text/xml, text/csv, and text/html - Governing access at the most basic level, keeping true to schema, but allowing for content negotiation over the web.</li>
  <li><strong>Human Services Data API (HSDA) Hypermedia</strong> - (application/hal+json and application/hal+xml)  - Allowing for more comprehensive responses to HSDA requests, addressing searching, filtering, pagination, and relationship linking between any HSDS returend.</li>
  <li><strong>Human Services Data API (HSDA) Bulk</strong> - (application/vnd.hsda.bulk) - Focusing on heavy system to system bulk transfers, and eventually syncing, backups, archives, and migrations. Dealing with the industrial levels of HSDA operations.</li>
  <li><strong>Human Services Data API (HSDA) Federated</strong> - (application/vnd.hsda.federated) - Allowing for a federated HSDA implementation that allows for the moderation of all POST, PUT, and DELETE by a distributed set of partners. Might also accompany the bulk system where partners can enable sync or bulk extraction for use in their own implementations.</li>
</ul>

<p>I am working to define an industry level API standard. I am not operating an individual API implementation (well I do have several demos), so media types allows me to enable each vendor, or implementation to negotiate the type of content they desire. If they are interested developing single page applications or conversational interfaces they can offer up the hypermedia implementation. If they are system administrators and looking to load up large datasets, or extract large datasets, they can work within the HSDA Bulk realm. In the end I can see any one of these APIs being deployed in isolation, as well as all four of them living side by side, driving a single HSDS/A compliant platform.</p>

<p>This is all preliminary thought. All I have currently is HSDS, and HSDA returning JSON. I’m just brainstorming about what possible paths there are forward, and what I think the solution involves content negotiation, at the vendor, implementation, and consumption levels. Content type negotiation seems to provide a great way to break up and separate concerns, keeping simple things simple, and some of the more robust integrations segregated, while still working in concert. I’m always impressed by the meaningful impact something like content type has had on the web API design conversation, and always surprised when I learn new approaches to leveraging the content type header as part of API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/07/api-industry-standards-negotiation-by-media-type/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/07/providing-code-citations-in-machine-learning-apis/">Providing Code Citations In Machine Learning APIs</a></h3>
        <span class="post-date">07 Aug 2017</span>
        <p><a href="https://algorithmia.com/algorithms/bkyan/StyleThief"><img src="https://s3.amazonaws.com/kinlane-productions/algorithmia/machine-learning-citation.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="https://algorithmia.com/algorithms/bkyan/StyleThief">I was playing around with the Style Thief, an image transfer API from Algorithmia</a>, and I noticed the citation for the algorithm behind. The API is an adaptation of <a href="https://github.com/anishathalye/neural-style/">Anish Athalye’s Neural Style Transfer</a>, and I thought the algorithmic citation of where the work was derived from was an interesting thing to take note of for my machine learning API research.</p>

<p>I noticed on Algorithmia’s page there was <a href="http://www.bibtex.org/">a Bibtex citation</a>, which referenced the author, and project Github repository:</p>

<p>@misc{athalye2015neuralstyle,<br />
   author = {Anish Athalye},<br />
   title = {Neural Style},<br />
   year = {2015},<br />
   howpublished = {\url{https://github.com/anishathalye/neural-style}},<br />
   note = {commit xxxxxxx}<br />
}<br /></p>

<p>This provides an interesting way to address citation in not just machine learning, but with open source driving algorithmic APIs in general. It gives me food for thought when it comes to <a href="http://apievangelist.com/2015/12/02/what-licensing-should-i-be-considering-when-i-take-open-source-software-and-offer-up-as-an-api/">what licensing I should be considering when wrapping open source software with an API</a>. I’ve been <a href="http://apievangelist.com/2017/08/04/including-api-dependencies-within-api-definition/">thinking about dependencies a lot lately when it comes to APIs and their definitions</a>, and I’d consider citation or attribution to be in a similar category. I guess rather then technical dependency, it is more in the business and legal dependency category.</p>

<p>Similar to how Pivio allows you to reference dependencies for your microservices, I’m thinking that <a href="http://apicommons.org">API Commons</a>, or some other format like Bibtext could provide a machine readable citation, that could be indexed as part of an <a href="http://apisjson.org">APIs.json index</a>. Allowing us API designers, architect, and providers to provide proper citation for where our work is derived. These aren’t just technical dependencies, but also business and political dependencies, that we should ensuring are present with each API we deploy, providing an observable provenance of where ideas come from, and a honest look at how we build on the work of each other.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/07/providing-code-citations-in-machine-learning-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/04/when-you-see-api-rate-limiting-as-security/">When You See API Rate Limiting As Security</a></h3>
        <span class="post-date">04 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/art-museum/art-museum_copper_circuit_2.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m neck deep into my assessment of the world of API security this week, a process which always yields plenty of random thoughts, which end up becoming stories here on the blog. One aspect of API security I keep coming across in this research is the concept of API rate limiting as being security. This is something I’ve long attributed with API management service providers making their mark on the API landscape, but as I dig deeper I think there is more to this notion of what API security is (or isn’t). I think it has more to do with API providers, than companies selling their warez to these API providers.</p>

<p>The API management service providers have definitely set the tone for API security conversation(good), by standing up a gateway, and providing tools for limiting what access is available–I think many data, content, and algorithmic stewards are very narrowly focus on security being ONLY about limiting access to their valuable resources. Many folks I come across see their resources as valuable, when they begin doing APIs they have a significant amount of concern around putting their resources on the Internet, and once you secure and begin rate limiting things, all security concerns appear to have been dealt with. Competitors, and others just can’t get at your valuable resources, they have to come through the gate–API security done.</p>

<p>Many API providers I encounter have unrealistic views of the value of their data, content, and algorithms, and when you match this with their unrealistic views about how much others want access to this valuable content you end up with a vacuum which allows for some very narrow views of what API security is. To help support this type of thinking, I feel like the awareness generated from API management is often focused on generating revenue, and not always about understanding API abuse, and is also something can create blindspots when it comes to database, server, and DNS level logging and layers where security threats emerge. I’m assuming folks often feel comfortable that the API management layer is sufficiently securing things by rate limiting, and we can see all traffic through the analytics dashboard. I’m feeling that this one of the reasons folks aren’t looking up at the bigger API security picture.</p>

<p>From what I’m seeing, assumptions that the API management layer is securing things can leave blind spots in other areas like DNS, threat information gathering, aggregation, collaboration, and sharing. I’ve come across API providers who are focused in on API management, but don’t have visibility at the database, server, container, and web server logging levels, and are only paying attention to what their API management dashboard provides access to. I feel like API management opened up a new found awareness for API provides, something that has evolved and spread to <a href="http://monitoring.apievangelist.com/">API monitoring</a>, <a href="http://testing.apievangelist.com/">API testing</a>, and <a href="http://performance.apievangelist.com/">API performance</a>. I feel like the next wave of awareness will be in the area of API security. I’m just trying to explore ways that I can help my readers and clients better understand how to expand their vision of API security beyond their current field of vision.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/04/when-you-see-api-rate-limiting-as-security/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/04/including-api-dependencies-within-api-definition/">Including API Dependencies Within Your API Definition</a></h3>
        <span class="post-date">04 Aug 2017</span>
        <p><a href="http://apievangelist.com/2017/08/03/microservice-discovery-using-pivio/">I was learning about Pivio, a discovery specification for microservices the other day, and found their focus on microservice dependency to be pretty interesting</a>. API dependencies has been an area I have found myself increasingly thinking about, as well as tracking on in my API research. I’m pretty close to publishing a project dedicated to understanding API, and microservices dependencies, which would overlap with containers, serverless, and other aspects of the API lifecycle that are all about breaking down the monolith.</p>

<p>Each service definition using Pivio has a depends_on object, which allows for defining both internal and external service dependencies. Here is a snippet from a sample Pivio document to help articulate this interesting feature:</p>

<script src="https://gist.github.com/kinlane/728a92155bc507a526182b3b767a73d1.js"></script>

<p>This is where you can start connecting the cords between all of your services, something that is applicable to any APIs, whether you’ve drank the microservices kool-aid or not. I find it interesting that Pivio has internal, and external. I’d love to see an OpenAPI linked off each of the services it depends on. I also am fascinated with the first question for external, why? What a great first question for any dependency–it should also be available for internal services as well. Every API provider should be inquiring why a dependency exist whenever possible, and having it quantified in this way just opens up more chances for this question to get asked.</p>

<p>Seeing dependencies quantified in Pivio makes me happy. It has been something I’ve wanted to reflect in <a href="http://apisjson.org">APIs.json</a> for some time now. Currently, I do not know of any way to quantify the relationship between APIs, and Pivio provides a glimpse at one possible way we might be able to map this world out. <a href="http://www.cytoscape.org/">I have been learning more about Cytoscape, a network data integration, analysis, and visualization framework</a>. Having a machine readable API dependencies definition would allow me to create a network visualization of any API or microservices discovery catalog. It wouldn’t take much work at all to map out API and microservice dependencies at the internal and external levels, further helping to quantify the platforms we operate.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/04/including-api-dependencies-within-api-definition/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/04/the-words-we-use-to-describe-our-api-technology/">Understanding The Words We Use To Describe Machine Learning APIs</a></h3>
        <span class="post-date">04 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/mining-machine-learning.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I spend a lot of time trying out new APIs, working to understand what it is they do, or do not do. I have a pretty robust way of looking at APIs, profiling the company, and the APIs they offer, but when I’m wading through the marketing content, API documentation, and other resources, I am regularly stumped by the language that is used to describe what an API does. Honestly, this problem isn’t exclusive to machine learning APIs, but with the recent explosion in artificial intelligence, machine learning, deep learning, cognitive and other types of algorithmic voodoo, the words being used seem to have gone to entirely new levels.</p>

<p>I am interested in understanding what it is an API does. I want to go from zero to understanding in 2.34 seconds. I don’t want to wade through marketing, and documentation to understand what an API does. I want to find simple, concise language that properly describes an API. In the world of artificial intelligence, this can be difficult to do, and is something that varies from provider to provider. Some machine learning API providers are better at describing what they do, while others seem to prefer hype, and fluff when it comes to explaining what is actually possible.</p>

<p>As I continue my work profiling <a href="http://amazon.web.services.stack.network/">Amazon</a>, <a href="http://microsoft.stack.network/">Microsoft</a>, and <a href="http://google.stack.network/">Google APIs</a> I want to develop an approach to helping me separate what an API does, and what a provider says it does. I am going to continue profiling each API <a href="https://www.openapis.org/">using OpenAPI</a>, and labeling them with a common set of tags I’m using to quantify what machine learning APIs actually do. As I’m doing this I also decided to add an extra tag field called x-hype-tags, which gives me a way to track each of the additional words I found in the marketing and documentation, that I may not be actually using to describe what the API does–maintaining much of the API providers intent.</p>

<p>One thing that contributes to the confusion of what is machine learning is that, very similar to APIs, it can be almost anything. While packaged in many different ways, most of the major machine learning providers have a machine learning engine for training and executing models, as well as some ready to go models that accomplish some common ML tasks for developers. Here are the primary machine learning engines I’ve been profiling out of the major cloud providers:</p>

<ul>
  <li><a href="https://aws.amazon.com/machine-learning/"><strong>Amazon Machine Learning</strong></a> - Amazon Machine Learning is a service that makes it easy for developers of all skill levels to use machine learning technology.</li>
  <li><a href="https://cloud.google.com/ml/"><strong>Google Machine Learning</strong></a> - Google Cloud Machine Learning Engine is a managed service that enables you to easily build machine learning models that work on any type of data, of any size</li>
  <li><a href="https://azure.microsoft.com/en-us/services/machine-learning/"><strong>Azure Machine Learning</strong></a> - A fully-managed cloud service that enables you to easily build, deploy, and share predictive analytics solutions.</li>
</ul>

<p>I am going to be profiling these engines in their own way, but before I understand the infinite possibilities, I want to better understand the finite possibilities with each of these machine learning platforms. To help me develop a way of tagging, and quantifying what APIs do, and the hype around them I took a single type of machine learning API from each of these providers, and augmented their OpenAPI with more descriptive tagging.</p>

<p>To begin, I started with <a href="https://aws.amazon.com/rekognition/?hp=tile&amp;so-exp=below">Amazon’s Rekognition API</a>, crafting a basic OpenAPI that outlines the technical details of each possible API request and response, with the added tagging describing what the API does:</p>

<script src="https://gist.github.com/kinlane/a5b721d555e92aa81cf386a6f85c8e09.js"></script>

<p>Then I went through <a href="https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/">Azure’s Computer Vision</a>, crafting a basic OpenAPI that outlines the technical details of each possible API request and response, with the added tagging describing what the API does:</p>

<script src="https://gist.github.com/kinlane/cc69c6f6a25cbe6ae021941e45fb2f31.js"></script>

<p>Then last I look at <a href="https://cloud.google.com/vision/">Google’s Cloud Vision API</a>, crafting a basic OpenAPI that outlines the technical details of each possible API request and response, with the added tagging describing what the API does:</p>

<script src="https://gist.github.com/kinlane/a559ae524f321167d708e229c9792e23.js"></script>

<p>These OpenAPIs aren’t 100% complete. I’m more concerned with developing the tagging schema for use across many APIs, than I am with the perfect OpenAPI. I’ll publish them to Github and continue to evolve, polish, and work to make each OpenAPI as complete as possible. I’m looking to settle in on a schema, before I got to work adding the extra x-hype-tags to additional OpenAPIs, pushing beyond what OpenAPI 2.0 dictates when it comes to API tagging. I’ll use the primary tags for each OpenAPI as the distilled down categorization of what each ML API does, while the x-hype-tags will give me a summary of what is used in the marketing copy, and the API documentation for each ML API provider.</p>

<p>It is important to me that I can quickly pull up APIs that accomplish specific business tasks–this requires each API to be tagged well. I also want to be able to apply some sort of hype score to each API, which reflects the number of adjectives and buzzwords in use in marketing and documentation. I feel these x-hype-tags reflect the hype in ML API space, and the tags reflect the realities of what ML APIs are actually capable of doing. When I profile new ML APIs I will be adding any word that catches my eye to the x-hype-tags, but I will be thinking about more deeply, and more carefully evolving the core set of more meaningful APIs I am using to categorize the ML APIs I track on.</p>

<p>Once I profile Amazon, Microsoft’s, and Google’s ML APIs in this way I am going to go through <a href="http://algorithmia.com">Algorithmia</a>, and other leading ML API providers and profiling their APIs, adding to both my core machine learning tagging vocabulary, as well as my machine learning hype tagging dictionary. Then I’ll step back a little and think about the bigger picture of the words we are choosing to describe our ML APIs. I am looking to develop a better way to be cataloging the types of ML APIs out there, and understand the language API providers are choosing to use when they describe what it is that they deliver. We need to be able to make sense of the ML APIs we already have, as well as each of the upcoming waves of AI and ML voodoo that is being developed right now, and due to be unleashed on the world in coming years.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/04/the-words-we-use-to-describe-our-api-technology/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/03/plugin-infrastructure-for-every-stop-along-the-api-lifecycle/">Plugin Infrastructure For Every Stop Along The API Lifecycle</a></h3>
        <span class="post-date">03 Aug 2017</span>
        <p><a href="https://tyk.io/features/extend-tyk/"><img src="https://s3.amazonaws.com/kinlane-productions/tyk/tyk-plugins.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m continuing <a href="http://ipaas.apievangelist.com">my integration platform as a service (iPaaS) research</a>, understanding <a href="http://apievangelist.com/2017/08/02/making-sure-your-api-service-connects-to-other-stops-along-lifecycle/">how API providers are quickly integration with other platform</a>, I am also looking into how API service providers are opening up their services to the entire API lifecycle. I’m seeing API service provides offer up a default set of integrations with other platforms, ad in some cases using Zapier by default–opening up 750+ other API driven platforms pretty quickly. Another dimension of this that I’m tracking on is when API service providers offer up plugin infrastructure, allowing other platforms to develop plug and play integrations that any platform user can take advantage of.</p>

<p>You can see this in action over at my partner Tyk, who has <a href="https://tyk.io/features/extend-tyk/">a nice set of plugins for their API management solution</a>. They start with three language focused middleware plugins allowing you to write scripts in Java, Lua, and JavaScript. Then they offer two gRPC plugins, which might be another post all by itself. While very similar to the iPaaS, or custom integration solution I’ve seen from other API providers, I’m going to be categorizing plugin approaches to integration like this separately, because it invites developers to come develop integrations as plugin–something that is very API in my book.</p>

<p><a href="http://plugin.apievangelist.com/">I’ve added a separate research area to tune into what types of integrations platforms are introducing via plugin infrastructure</a>. I’m trying to understand how plugins are evolving from being more about platform, browser, and other common forms and becoming more API lifecycle middleware (for lack of better term), like Tyk. I want to be aware of each of their approaches, and how different stops along the API lifecycle are embedding scripting engines, injecting integrated features into operations, and making it part of any continuous integration and deployment workflow(s). Like the other areas of my API lifecycle research, I will stop in from time to time, and understand if plugin API infrastructure is evolving and becoming more of a thing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/03/plugin-infrastructure-for-every-stop-along-the-api-lifecycle/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/03/when-describing-your-machine-learning-apis-work-extra-hard-keep-things-simple/">When Describing Your Machine Learning APIs Work Extra Hard To Keep Things Simple</a></h3>
        <span class="post-date">03 Aug 2017</span>
        <p><a href="http://bryanmmathers.com/"><img src="https://s3.amazonaws.com/kinlane-productions/contrafabulists/machine+learning.jpg" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m spending a significant amount of time learning about machine learning APIs lately. Some of what I’m reading is easy to follow, while most of it is not. A good deal of what I’m reading is technically complex, and more on the documentation side of the conversation. Other stuff I come across is difficult to read, not because it is technical, but because it is more algorithmic marketing magic, and doesn’t really get at what is really going on (or not) under the hood.</p>

<p>If you are in the business of writing marketing copy, documentation, or even the API design itself, please work extra hard to keep things simple and in plain language. I read so much hype, jargon, fluff, and meaningless content about artificial intelligence and machine learning each day, I take pleasure anytime I find simple, concise, and information descriptions of what ML APIs do. In an exploding world of machine learning hype your products will stand out if they are straight up, and avoid the BS, which will pretty quickly turn off the savvy folks to whatever you are peddling.</p>

<p>Really, this advice applies to any API, not just machine learning. It’s just the quantity of hype we are seeing around AI and ML in 2017 is reaching some pretty extreme levels. Following the hype is easy. Writing fluffy content doesn’t take any skills. Writing simple, concise, plain language names, descriptions, and other meta data for artificial intelligence and machine learning APIs takes time, and a significant amount of contemplation regarding the message you want to be sending. The ML APIs I come across that get right to the point, are always the ones that stick around in my mind, and find a place within my research and storytelling.</p>

<p>We are going to continue to see an explosion in the number of algorithmic APIs, delivering across the artificial intelligence, machine learning, deep learning, cognitive, and other magical realms. The APIs that deliver real business value will survive. The ones that have simple intuitive titles, and concise yet informative description that avoid hype and buzz will be the ones that get shared, reused, and ultimately float to the top of the pile and sticking around. I’m spending upwards of 5-10 hours a week looking through AI and ML API descriptions, and when I come across something that is clearly bullshit I don’t hesitate to flag, and push it back to the back warehouses of my research, keeping my time focused on the APIs which I can easily articulate what they do, and will also make sense to my readers.</p>

<p><strong>Photo Credit:</strong> <a href="http://bryanmmathers.com/">Bryan Mathers</a> (Machine Learning)</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/03/when-describing-your-machine-learning-apis-work-extra-hard-keep-things-simple/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/03/api-discovery-using-json-home/">API Discovery Using JSON Home</a></h3>
        <span class="post-date">03 Aug 2017</span>
        <p><a href="https://tools.ietf.org/html/draft-nottingham-json-home-06"><img src="https://s3.amazonaws.com/kinlane-productions/json-home/json-home-widget-example.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m have finally dedicated some time to learning more about <a href="https://tools.ietf.org/html/draft-nottingham-json-home-06">Home Documents for HTTP APIs, or simply JSON Home</a>. I see JSON Home as a nice way to bring together the technical components for an API, very similar to what I’ve been trying to accomplish with APIs.json. One of the biggest differences I see is that I’d say APIs.json was born out of the world of open data and APIs, where JSON Home is born of the web (which actually makes better sense).</p>

<p>I think the JSON Home description captures the specifications origins very well:</p>

<blockquote>
  <p>The Web itself offers one way to address these issues, using links [RFC3986] to navigate between states.  A link-driven application discovers relevant resources at run time, using a shared vocabulary of link relations [RFC5988] and internet media types [RFC6838] to support a “follow your nose” style of interaction - just as a Web browser does to navigate the Web.</p>
</blockquote>

<p>JSON Home provides any potential client with a machine readable set of instructions it can follow, involving one, or many APIs–providing a starting page for APIs which also enables:</p>

<ul>
  <li>Extensibility - Because new server capabilities can be expressed as link relations, new features can be layered in without introducing a new API version; clients will discover them in the home document.</li>
  <li>Evolvability - Likewise, interfaces can change gradually by introducing a new link relation and/or format while still supporting the old ones.</li>
  <li>Customisation - Home documents can be tailored for the client, allowing different classes of service or different client permissions to be exposed naturally.</li>
  <li>Flexible deployment - Since URLs aren’t baked into documentation, the server can choose what URLs to use for a given service.</li>
</ul>

<p>JSON Home, is a home page specification which uses JSON to provide APIs with a a launching point for the interactions they offer, by providing a coherent set links, all wrapped in a single machine readable index. Each JSON begins with a handful of values:</p>

<ul>
  <li>title - a string value indicating the name of the API</li>
  <li>links - an object value, whose member names are link relation types [RFC5988], and values are URLs [RFC3986].</li>
  <li>author - a suitable URL (e.g., mailto: or https:) for the author(s) of the API</li>
  <li>describedBy - a link to documentation for the API</li>
  <li>license - a link to the legal terms for using the API</li>
</ul>

<p>Once you have the general details about the JSON Home API index, you can provide a collection of resource objects possessing links that can be indicated using an href property with a URI value, or template links which uses a URI template. Just like a list of links on a home page, but instead of a browser, it can be used in any client, for a variety of different purposes.</p>

<p>Each of the resources allow for resource hints, which allow clients to obtain relevant information about interacting with a resource beforehand, as a means of optimizing communications, as well as sharing which behaviors will be available for an API. Here are the default hints available for JSON Home:</p>

<ul>
  <li>allow - Hints the HTTP methods that the current client will be able to use to interact with the resource; equivalent to the Allow HTTP response header.</li>
  <li>formats - Hints the representation types that the resource makes available, using the GET method.</li>
  <li>accept-Patch - Hints the PATCH [RFC5789] request formats accepted by the resource for this client; equivalent to the Accept-Patch HTTP response header.</li>
  <li>acceptPost  - Hints the POST request formats accepted by the resource for this client.</li>
  <li>acceptPut - Hints the PUT request formats accepted by the resource for this client.</li>
  <li>acceptRanges - Hints the range-specifiers available to the client  for this resource; equivalent to the Accept-Ranges HTTP response header [RFC7233].</li>
  <li>acceptPrefer - Hints the preferences [RFC7240] supported by the resource.  Note that, as per that specifications, a preference can be ignored by the server.</li>
  <li>docs - Hints the location for human-readable documentation for the relation type of the resource.</li>
  <li>preconditionRequired - Hints that the resource requires state-changing requests (e.g., PUT, PATCH) to include a precondition, as per [RFC7232], to avoid conflicts due to concurrent updates.</li>
  <li>authSchemes- Hints that the resource requires authentication using the HTTP Authentication Framework [RFC7235].</li>
  <li>status -  Hints the status of the resource.</li>
</ul>

<p>These hints provide you with a base set of the most commonly used sets of information, but then there is also a HTTP resource hint registration where all hints are registered. Hints can be added, allowing for the addition of custom defined hints, providing additional information beforehand about what can be expected from a resource link included as part of a JSON Home index. It is a much more sophisticated approach describing the behaviors of links than we included in APIs.json, with the formal hint registry being very useful and well-defined.</p>

<p>I”d say that JSON Home has all the features for defining a single, or collections of APIs, but really reflects its roots in the web, and possesses a heavy focus on enabling action with each link. While this is part of the linking structure of APIs.json, I feel like the detail and the mandate for action around each resource in a JSON Home index is much stronger. I feel like JSON Home is in the same realm as Postman Collections, but when it comes to API discovery. I always feel like a Postman Collection is more transactional than OpenAPI is by default. There is definitely overlap, but Postman Collections always feels one or two step closer to some action being taken than OpenAPI does–I am guessing it is because of it’s client roots, similar to the web roots of JSON Home, and also OpenAPIs roots in documentation.</p>

<p>Ok. Yay! I have Pivio, and now JSON Home both loaded in my brain. I have a feel for what they are trying to accomplish, and have found some interesting layers I hadn’t considered while doing my APIs.json centered API discovery work. Now I can step back, and consider the features of all three of these API discovery formats, establish a rough Venn diagram of their features, and consider how they overlap, and compliment each other. I feel like we are moving towards an important time for API discovery, and with the growing number of APIs available we will see more investment in API discovery specifications, as well as services and tooling that help us with API discovery. I’ll keep working to understand what is going on, establish at least a general understanding of each API discovery specifications, and report back here about what is happening when I can.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/03/api-discovery-using-json-home/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/03/different-search-engines-for-api-discovery/">Different Search Engines For API Discovery</a></h3>
        <span class="post-date">03 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/services/api-discovery.png" align="right" width="30%" style="padding: 15px" /></p>
<p>I was learning about <a href="http://pivio.io/">the microservices discovery specification Pivio</a>, which is a schema for framing the conversation, but also an uploader, search, and web interface for managing a collection of microservices. I found their use of ElasticSearch as the search engine for their tooling worth thinking about more. When we first launched <a href="http://apisjson.org">APIs.json</a>, we created <a href="http://apis.io">APIs.io</a> as the search engine–providing a custom developed public API search engine. I hadn’t thought of using ElasticSearch as an engine for searching APIs.json treated as a JSON document.</p>

<p>Honestly, I have been relying on the Github API as the search engine for my API discovery. Using it to uncover not just APIs.json, but OpenAPI, API Blueprint, and other API specification formats. This works well for public discovery, but I could see ElasticSearch being a quick and dirty way to launch a private or public engine for an API discovery, catalog, directory, or type of collection. I will add ElasticSearch, and other platforms I track on as part of my <a href="http://deployment.apievangelist.com">API deployment research</a> as a <a href="http://discovery.apievangelist.com">API discovery building block</a>, evolving the approaches I’m tracking on.</p>

<p>It is easy to think of API discovery as directories like ProgrammableWeb, or marketplaces like Mashape, and public API search engines like APIs.io–someone else’s discovery vehicle, which you are allowed to drive when you need. However, when you begin to consider other types of API discovery search engines, you realize that a collection of API discovery documents like JSON Home, Pivio, and APIs.json can quickly become your own personal API discovery vehicle. I’m going to write a separate piece on how I use Github as my API discovery engine, then I think I’ll step back and look at other approaches to searching JSON or YAML documents to see if I can find any search engines that might be able to be fine tuned specifically for API discovery.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/03/different-search-engines-for-api-discovery/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/03/microservice-discovery-using-pivio/">Microservice Discovery Using Pivio</a></h3>
        <span class="post-date">03 Aug 2017</span>
        <p><a href="http://pivio.io/"><img src="https://s3.amazonaws.com/kinlane-productions/pivio/pivio-microservice-documentation-for-your-platform.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m finally feeling the ground move in the area of <a href="http://discovery.apievangelist.com/">API discovery</a> a little bit. It is an area I’m perpetually trying to create my own seismic activity with <a href="http://apisjson.org">APIs.json</a>, but it always makes me happy to see other folks contributing to the conversation in a significant way. My friend Erik Wilde (<a href="https://twitter.com/dret">@dret</a>) turned me onto <a href="http://pivio.io/">Pivio for microservice discovery</a>, providing a machine readable approach to API discovery that resembles what I’ve been trying to do with APIs.json, but has several very interesting service-centric characteristics.</p>

<p>According to the project website Pivio is a “description in your source code with all the metadata of your service describing itself. This can be as simple as the name, owner and VCS information as well as useful information about runtime environment, used licenses and service dependencies. Pivio is a service registry for humans.” Pivio is a microservice discovery schema, and a web platform for uploading, searching, and viewing your catalog of microservices. It is in alpha version, but already provides a very interesting take on discovery from the microservices perspective.</p>

<p>Pivio begins with a common set of mandatory fields:</p>

<ul>
  <li>id - Unique id in pivio. You can ask the pivio service for a unique id.</li>
  <li>name - The name of the artifact. This is intended for humans.</li>
  <li>short_name - A very brief name for the service.</li>
  <li>type - The type of this artifact. Values could be service, library or mobile_app.</li>
  <li>owner - Which team is responsible for this artifact.</li>
  <li>description - What does this service do?</li>
</ul>

<p>Next Pivio begins to break out some interesting collections:</p>

<ul>
  <li>contact - Who should be contacted if one has a question.</li>
  <li>vcs - Where is the location of source code that runs this service.</li>
  <li>tags - Add tags for organizing each service as part of larger collection.</li>
  <li>lifecycle - In which lifecycle is this component? Only in development, in production or out of service.</li>
  <li>links - All sort of links which might be interesting. Candidates are homepage, buildchain, api docs</li>
</ul>

<p>Then Pivio breaks out some very useful service-focused collections:</p>

<ul>
  <li>Provides - What and where does this artifact provides services? description Should be a human readable description. service_name is the unique identification of the particular interface. port, protocol and transport_protocol are self describing.</li>
  <li>Dependences
    <ul>
      <li>internal - To which other service_name (from provides) services does this service talk?</li>
      <li>external - To which external target needs this artifact to talk to? This is meant to show if 		this service talks to another one outside of your system, like a public API of another service 		provider ‘in the cloud’.</li>
    </ul>
  </li>
  <li>Service Context
    <ul>
      <li>belongs_to_bounded_context - General rule is that every service belongs to a bounded context.</li>
      <li>visibility
  	- private: intended usage is only by the owner
        <ul>
          <li>public: exposes an api for other owners.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Runtime
    <ul>
      <li>cpu - the size of the processor
        <ul>
          <li>ram - how much memory it has</li>
        </ul>
      </li>
      <li>disk - what is the size of disk</li>
      <li>host_type - metal, vm, docker</li>
      <li>network_zone - dmz, backend, core, database</li>
      <li>environment - development, test, production</li>
    </ul>
  </li>
  <li>Software Dependencies - Name, versions, licensing of the software behind each of the services.</li>
  <li>Custom extensions - Have your own keys in this configuration you can simply add your own key word/yaml file.</li>
</ul>

<p>There are overlapping elements of Pivio and API.json, and you can accomplish man similar things with both API discovery formats, but Pivio brings to the table a serious technical view of APIs through a microservice lens. I am really interested in the context, dependencies, and like the runtime, and extension part as well. I think the lifecycle, VCS, and CI/CD aspects of the Pivio schema points to a more meaningful discovery approach in coming years.</p>

<p>Pivio has several supporting open source tools built on the microservices discovery schema which are available from the Github repository at: https://github.com/pivio/:</p>

<ul>
  <li>Client: https://github.com/pivio/pivio-client</li>
  <li>Server: https://github.com/pivio/pivio-server</li>
  <li>Web: https://github.com/pivio/pivio-web</li>
</ul>

<p>I’m not that interested in the client, server, and web applications just yet. I’d like to think more about the context, dependencies, lifecycle, VCS, CI/CD, and the search mechanism they are employing. I’m also going to spend some cycles this week getting up to speed on <a href="https://tools.ietf.org/html/draft-nottingham-json-home-03">JSON Home</a>, as well as Pivio. I’ve been trying to make some time to learn more about JSON Home lately, but just haven’t had the time. Now that I have Pivo on my desk, I’m kind of forced to make sure I can speak intelligenctly to both of these API discovery specification formats.</p>

<p>It is pretty easy to reference a pivio.yaml discovery document from within an APIs.json, and vice versa. They both allow for extensibility using links. After thinking about JSON Home a little bit, I want to try and establish some sort of Venn diagram of JSON Home, Pivio, and APIs.json to better understand where and when you’d want to use each format, and how they can overlap and work together. I feel like the world of APIs is finally maturing to a point where discovery is a growing pain point, so we are going to see more investment in a variety of approaches to helping make it easier to discover, and maintain rich catalogs of microservices and APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/03/microservice-discovery-using-pivio/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/02/understanding-global-api-performance-at-the-multi-cloud-level/">Understanding Global API Performance At The Multi-Cloud Level</a></h3>
        <span class="post-date">02 Aug 2017</span>
        <p><a href="http://apimetrics.io/google-location-map/">APIMetrics has a pretty addictive map showing the performance of API calls between multiple cloud providers, spanning many global regions</a>. The cloud location latency map “shows relative performance of a standard, reference GET request made to servers running on all the Google locations and via the Google global load balancer. Calls are made from AWS, Azure, IBM and Google clouds and data is stored for all steps of the API call process and the key percentiles under consideration.”</p>

<p align="center"><a href="http://apimetrics.io/google-location-map/"><img src="https://s3.amazonaws.com/kinlane-productions/api-metrics/apimetrics-cloud-location-performance-map.png" align="center" width="80%" style="padding: 15px;" /></a></p>

<p>It is interesting to play with the destination of the API calls, changing the region, and visualizing how API calls begin to degrade to different regions. It really sets the stage for how we should start thinking about the deployment, monitoring, and testing of our APIs. Region, by region, getting to know where our consumers are, and making sure APIs are deployed within the cloud infrastructure that delivers the best possible performance. It’s not just testing your APIs in a single location from many locations, it is also rethinking where your APIs are deployed, leveraging a multi-cloud reality and using all the top cloud provider, while also making API deployment by region a priority.</p>

<p>I’m a big fan of what APIMetrics is doing with the API performance visualizations and mapping. However, I think their approach to using HTTPbin is a significant part of this approach to monitoring and visualizing API performance at the multi-cloud level, while also making much of the process and data behind it all public. I want to put some more thought into how they are using HTTPbin behind this approach to multi-cloud API performance monitoring. I feel like there is potential her for applying this beyond just API performance, and think about other testing, security, and critical aspects of reliability and doing business online with APIs today.</p>

<p>After thinking where else this HTTPbin approach to data gathering could be applied, I want to think more about how the data behind APIMetrics cloud location latency map can be injected into other conversations, when it comes where we are deploying APIs, and running our API tests. Eventually I would like to see this type of multi-cloud API performance data alongside data for security and privacy compliance data, and even the regulations of each country as they apply to specific industries. Think about a time when we can deploy our APIs exactly where we want them based upon performance, privacy, security, regulations, and other critical aspects of doing business in the Internet age.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/02/understanding-global-api-performance-at-the-multi-cloud-level/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/02/learning-about-real-time-advertising-bidding-transparency-using-ads-txt/">Learning About Real-Time Advertising Bidding Transparency Using Ads.txt</a></h3>
        <span class="post-date">02 Aug 2017</span>
        <p><a href="https://iabtechlab.com/ads-txt/"><img src="https://s3.amazonaws.com/kinlane-productions/ads-txt/ads.txt-about.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="https://blog.lukaszolejnik.com/real-time-bidding-transparency-via-ads-txt/">I was learning about real-time bidding transparency using Ads.txt from Lukasz Olejnik</a>. The mission of the ads.txt project is to “<a href="https://iabtechlab.com/ads-txt/">increase transparency in the programmatic advertising ecosystem. Ads.txt stands for Authorized Digital Sellers and is a simple, flexible and secure method that publishers and distributors can use to publicly declare the companies they authorize to sell their digital inventory.</a>” While Ads.txt isn’t an API, it is an open, machine readable definition that is working to make advertising more transparent and observable to everyone, not just people in the ad-tech space.</p>

<p>Ads.txt works similar to robots.txt, and is a simple text file that lives in the root of a domain, listing the companies that have permission to sell advertising. The format is new, so there isn’t a lot of adoption yet, but <a href="https://www.washingtonpost.com/ads.txt">you can see one in action over at the Washington Post</a>. Helping make platforms observable is why I perform as the API Evangelist. I see them as one of the important tools we have for making systems, algorithms, and platforms more observable, and less of a black box. I see ads.txt having similar potential for the world of advertising, and something that eventually could have an API, for helping make sense of the very distributed, brokered, and often dark world of online advertising.</p>

<p>Honestly, I know almost nothing about online advertising. I have a basic level of understanding of Google Analytics, Adwords, and Adsense, as well as reading the blogs, and documentation for many advertising APIs I come across in my regular monitoring of the API space. I am just interested in ads.txt as an agent of observability, and pulling back the current on who is buying and selling our digital bits online. I am adding ads.txt to <a href="http://definitions.apievangelist.com/">my API definitions research</a>. This will allow me to keep an eye on the project, see where it goes, and think about the API level for aggregation of the ad network data, on maybe Github or something–caching ads.txt on a daily basis. Who knows where it will go. I’ll just keep learning, and playing around until it comes into a little more focus.</p>

<p>I’m guessing that not all companies will want to be transparent like this. Folks who have the upper hand, or are just doing shady things, rarely like to have the curtain pulled back. I’ll keep an eye ads.txt adoption, and also spend some time thinking thinking about transparency and observability at the API monetization and plans level, going beyond just advertising, which is the staple of the web, and think about how we can keep things open when it comes to API business models.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/02/learning-about-real-time-advertising-bidding-transparency-using-ads-txt/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/02/when-cities-use-a-common-api-definition-to-report-non-emergency-issues/">When Cities Use A Common API Definition To Report Non-Emergency Issues</a></h3>
        <span class="post-date">02 Aug 2017</span>
        <p>I am taking a deeper look at Open311, as part of some wider municipal level API research and development I am doing. I am going to be helping evolve an OpenAPI for the project, as well as JSON schema for the API and underlying data model. As I’m working my way through the Open311 portal reacquainting myself with the open format for reporting of non-emergency issues within cities, I came across the list of cities who have implemented Open311, and get a glimpse at what the future of APIs at the city level can be.</p>

<p><a href="http://wiki.open311.org/GeoReport_v2/Servers/">When you land on the Open311 GeoReport v2 Servers listing page you get a table of the twenty-one cities who have published an Open311 API</a>, with the name, country, API discovery document, API key request location, documentation, production and sandbox environment URLs. Twenty-one separate cities, twenty-one separate APIs for reporting non-emergency issues, all using the same API definition. This is the way that all APIs at the city, county, state, and federal levels should work. They should leverage common schema and API definition, providing a federated list of resources that can be integrated into any application.</p>

<p align="center"><a href="http://wiki.open311.org/GeoReport_v2/Servers/"><img src="https://s3.amazonaws.com/kinlane-productions/open311/open311-api-list.png" width="80%" style="padding: 15px;" align="center" /></a></p>

<p>Imagine when all cities have a common 311 API for reporting of non-emergency issues, as well as a 211 API for finding human services within a city. Imagine when the entire stack of city services all use common API definition(s), and schema across all API requests and responses. In this environment any city will be able take existing SDKs, or open source application, tools, or plugin, and put to work for a new city, with little or no changes. This is the way city, county, state, and federal agency software and application development should work. There should be a buffet of open source solutions for each layer of government operations.</p>

<p>When it comes to this level of API operations within cities, Open311 is the furthest along down the road in the journey. This is why I want to help them further develop the OpenAPI, and supporting JSON schemas, take a look at their overall portal, and operations, to see where else I can lend a hand, and contribute to their momentum. I am looking to bring Open311 alignment with the work I am doing on the Open Referral Human Services Data API, which is essentially Open211. It’s gonna take me a few weeks to get up to speed on everything going on with Open311, and find the bandwidth to work on the OpenAPI, and JSON schema, alongside <a href="http://org.open.referral.adopta.agency/">my Human Services API work</a>, but I am invested in seeing both these open government API standards move forward, and enjoy wider adoption by cities around the world, so I will find the time somewhere.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/02/when-cities-use-a-common-api-definition-to-report-non-emergency-issues/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/02/making-sure-your-api-service-connects-to-other-stops-along-lifecycle/">Making Sure Your API Service Connects To Other Stops Along The API Lifecycle</a></h3>
        <span class="post-date">02 Aug 2017</span>
        <p><a href="https://www.runscope.com/docs/api-testing/integrations"><img src="https://s3.amazonaws.com/kinlane-productions/runscope/runscope-connected-services.png" align="right" width="40%" style="padding: 15px; " /></a></p>
<p>I am continuing <a href="http://ipaas.apievangelist.com/">my integration platform as a service research</a>, and spending a little bit of time trying to understand how API providers are offering up integrations with other APIs. Along the way, I also wanted to look at how API service providers are doing it as well, opening themselves up to other stops along n API lifecycle. To understand how API service providers are allowing their users to easily connect to other services I’m taking a look at how my partners are handling this, starting with <a href="https://www.runscope.com/docs/api-testing/integrations">connected services at Runscope</a>.</p>

<p>Runscope provides ready to go integration of their API monitoring and testing services with twenty other platforms, delivering a pretty interesting Venn diagram of services along the API lifecycle:</p>

<ul>
  <li>Slack - Slack to receive notifications from Runscope API test results and Traffic Alerts.</li>
  <li>Datadog  - Datadog to create events and metrics from Runscope API test results.</li>
  <li>Splunk Cloud  - Splunk Cloud to create events for API test results.</li>
  <li>PagerDuty  - A PagerDuty service to trigger and resolve incidents based on Runscope API test results or Traffic Alerts.</li>
  <li>Amazon Web Services   - Amazon Web Services to import tests from API Gateway definitions.</li>
  <li>Ghost Inspector  - Ghost Inspector to run UI tests from within your Runscope API tests.</li>
  <li>New Relic Insights - New Relic Insights to create events from Runscope API test results.</li>
  <li>Microsoft Teams  - Microsoft Teams to receive notifications from Runscope API test results.</li>
  <li>HipChat  - HipChat to receive notifications from Runscope API test results and Traffic Alerts.</li>
  <li>StatusPage.io - StatusPage.io to create metrics from Runscope API test results.</li>
  <li>Big Panda - Big Panda to create alerts from Runscope API test results.</li>
  <li>Keen IO - Keen IO to create events from Runscope API test results.</li>
  <li>VictorOps - A VictorOps service to trigger and resolve incidents based on Runscope API test results or Traffic Alerts.</li>
  <li>Flowdock - Flowdock to receive notifications from Runscope API test results and Traffic Alerts.</li>
  <li>AWS CodePipeline - Integrate your Runscope API tests into AWS CodePipeline.</li>
  <li>Jenkins  - Trigger a test run on every build with the Jenkins Runscope plugin.</li>
  <li>Zapier - integrate with 250+ services like HipChat, Asana, BitBucket, Jira, Trello and more.</li>
  <li>OpsGenie - OpsGenie to send alerts from Runscope API test results.</li>
  <li>Grove  - Grove to send messages to your IRC channels from Runscope API test results and Traffic Alerts.</li>
  <li>CircleCI  - Run your API tests after a completed CircleCI build.</li>
</ul>

<p>Anyone can integrate <a href="https://www.runscope.com/docs/api">API monitoring and testing into operation using the Runscope API</a>, but these twenty services are available by default to any user, immediately opening up several important layers of our API operations. Immediately you see the messaging, notifications, chat, and other support layers. Then you see the continuous integration / deployment, code, and SDK layers. Then you come across Zapier, which opens up a whole other world of endless integration possibilities. I see <a href="https://www.runscope.com">Runscope</a> owning the monitoring, testing, and performance stops along the API lifecycle, but their connected services puts other stops like deployment, management, logging, analysis, and many others also within reach.</p>

<p>I am working on a way to track the integrations between API providers, and API service providers. I’d like to be able to visualize the relationships between providers, helping me see the integrations that are most important to different groups of end users. I’m a big advocate for API providers to put iPaaS services like <a href="https://zapier.com/app/explore">Zapier</a> and <a href="https://github.com/DataFire">DataFire</a> to work, opening up a whole world of integrations to their developers and end users. I also encourage API service providers to work to understand how Zapier can open up other stops along the API lifecycle. Next, everyone should be thinking about deeper integrations like Runscope is doing with their connected services, and make sure you always publish a public page showcasing integrations, making it part of documentation, SDKs, and other aspects of your API service platform.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/02/making-sure-your-api-service-connects-to-other-stops-along-lifecycle/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/01/craft-an-openapi-for-an-existing-threat-intelligence-sharing-api-specification/">Craft An OpenAPI For An Existing Threat Intelligence Sharing API Specification</a></h3>
        <span class="post-date">01 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-threat-info-sharing-api.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2017/07/10/opportunity-to-develop-a-threat-intelligence-apis-json/">I wrote about the opportunity around developing an aggregate threat information API</a>, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of the <a href="https://oasis-open.github.io/cti-documentation/">Structured Threat Information Expression (STIX)</a>, a structured language for cyber threat intelligence, and <a href="https://oasis-open.github.io/cti-documentation/">Trusted Automated Exchange of Intelligence Information (TAXII)</a>, and transport mechanism for sharing cyber threat intelligence.</p>

<p>This is why I write about my projects openly like this, so that my readers can help me identify existing approaches for tackling whatever I am focusing on. I prefer to never reinvent the wheel, and build on top of any existing work that is already available. I’m thinking the next step is to craft an OpenAPI fo TAXII, and STIX. Creating a machine readable blueprint for deploying, managing, and documenting a threat intelligence API. I couldn’t find any existing work on an OpenAPI definition, so this seems like a logical place to begin working to build on, and augment the work of <a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=cti">the Cyber Threat Intelligence Technical Committee</a>. Clearly, the working group has created a robust set of specifications, but I’d like to help move it closer to implementation with an OpenAPI.</p>

<p><a href="https://github.com/threat-intelligence">I have created a Github organization to help organize any work on this project</a>. I have forked the project for STIX and TAXII there, as well as started <a href="https://github.com/threat-intelligence/planning">a planning repository</a> to coordinate any work I’m contributing to the conversation. I have also created a repository for working on and publishing the OpenAPI that will define the project. Once we have this, I’d like to start thinking about the development of a handful of server side implementations in maybe Node.js, Python, PHP, or other common programming language. Here are the next steps I’d like to see occur around this project:</p>

<ul>
  <li><strong>OpenAPI</strong> - Create an OpenAPI for STIX and TAXII to provide a single representation of a threat intelligence sharing API.</li>
  <li><strong>Threat List</strong> - Take the <a href="http://apievangelist.com/2017/07/10/opportunity-to-develop-a-threat-intelligence-apis-json/">threat intelligence list I originally published</a>, and identify how any of the sources would map to OpenAPI.</li>
  <li><strong>Storytelling</strong> - Tell stories throughout the process to attract the attention of other players, contributors, and investors, so that this project can live on.</li>
</ul>

<p>I’m not looking to own this project 100%. I just don’t have the time and resources. However I do want to see an OpenAPI move forward, as well as a wealth of open source resources for deploying, integrating, and aggregating around threat intelligence sharing. This work is bigger than any single player, and is something that needs to be open, spanning thousands of providers, not controlled by a handful of gatekeepers. Players in the threat intelligence sharing game need to be able to decide who they consume and share threat intelligence with, something that will require a federated world of APIs that all speak in a common language. <a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=cti">The Cyber Threat Intelligence Technical Committee</a> is off to a great start. I just want to contribute with some cycles to help bring their work in alignment with what is going on in the mainstream world of APIs, while also beating a drum so that I can bring more attention to any work going on in this important area. Our world is going to need significant investment in the area of threat intelligence sharing if we are going to be successful online in coming years.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/01/craft-an-openapi-for-an-existing-threat-intelligence-sharing-api-specification/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/01/us-energy-information-administration-excel-add-in-and-google-add-on/">U.S. Energy Information Administration Excel Add-In and Google Add-On</a></h3>
        <span class="post-date">01 Aug 2017</span>
        <p><a href="https://www.eia.gov/opendata/excel/"><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/eia/eia-spreadsheet-add-ons.png" align="right" width="35%" style="padding: 15px;" /></a></p>
<p>I was looking through a number of federal government API implementations last week in preparation of a talk I did in Washington DC. The result of research like this is always a notebook full of interesting stories to tell about what federal agencies are up to with APIs. Today’s story is out of the U.S. Energy Information Administration (EIA), with <a href="https://www.eia.gov/opendata/excel/">their Excel Data Add-In and Google Add-On tooling</a> which allows you to download energy data from <a href="https://www.eia.gov/opendata/">EIA’s data API</a> and economic data from <a href="http://research.stlouisfed.org/fred2/">the St. Louis Federal Reserve’s Economic Data (FRED) API</a> directly into your spreadsheet(s).</p>

<p>I’m regularly looking out for innovative uses of spreadsheets when it comes to deploying, as well as consuming APIs, because I believe it is the best way we have to turn average business users into API consumers, by piping in data into the environment they are already using each day. EIA’s data API contains 1.6 million energy series, and the St. Louis Federal Reserve’s API contains 240,000 economic series. Making valuable federal agency maintained data available within spreadsheets like this using APIs is something ALL other agencies should be emulating. First, agencies need to be doing public APIs, then they need to make sure they are also investing in spreadsheet tooling like the EIA is.</p>

<p>I’m adding this example of using Microsoft Excel and Google Sheets as an API client for not just federal government, but also for such valuable commerce and energy data, to <a href="http://spreadsheets.apievangelist.com/">my APIs and spreadsheets research</a>. I’m also going to be on the hunt for open source solutions for delivering spreadsheet API connectivity like this. There should be a wealth of open source tooling that federal agencies can put to work when it comes to delivering data to spreadsheets, both internally, and externally with private sector partners. In a time where it is easy to get pretty depressed on a daily basis about APIs in the federal government, it makes me happy to find shining examples of APIs being put to work in such meaningful, and useful ways.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/01/us-energy-information-administration-excel-add-in-and-google-add-on/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/01/api-sdk-licensing-notifications-using-versioneye/">API SDK Licensing Notifications Using VersionEye</a></h3>
        <span class="post-date">01 Aug 2017</span>
        <p><a href="https://www.versioneye.com/"><img src="https://s3.amazonaws.com/kinlane-productions/versioneye/01-veye-licenses-7679418f2968513011985476db94b59b0ab65abd5c030a6a92189fc0e1170722.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I have been watching <a href="https://www.versioneye.com/">VersionEye</a> for a while now. If you aren’t familiar, they provide a service that will notify you of security vulnerabilities, license violations and out-dated dependencies in your Git repositories. I wanted to craft a story specifically about their licensing notification services, which can check all your open source dependencies against a license white list, then notify you of violations, and changes at the SDK licensing level.</p>

<p>The first thing I like here, is the notion of an API SDK licensing whitelist. The idea that there is a service that could potentially let you know which API providers have SDKs that are licensed in a way that meets your integration requirements. I think it helps developers who are building applications on top of APIs understand which APIs they should or shouldn’t be using based upon SDK licensing, while also providing an incentive for API providers to get their SDKs organized, including the licensing–you’d be surprised at how many API providers do not have their SDK house in order.</p>

<p>VersionEye also provides CI/CD integration, so that you can stop a build based on a licensing violation. Injecting the politics of API operations, from an API consumers perspective, into the application lifecycle. I’m interested in VersionEye’s CI/CD, as well as security vulnerabilities, but I wanted to make sure this approach to keeping an eye on SDK licensing was included in my <a href="http://sdk.apievangelist.com">SDK</a>, <a href="http://monitoring.apievangelist.com">monitoring</a>, and <a href="http://licensing.apievangelist.com">licensing</a> research, influencing my storytelling across these areas. Some day all API providers will have a wide variety of SDKs available, each complete with clear licensing, published on Github, and indexed as part of an <a href="http://apisjson.org">APIs.json</a>. We just aren’t quite there yet, and we need more services like VersionEye to help build awareness at the API SDK licensing level to get us closer to this reality.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/01/api-sdk-licensing-notifications-using-versioneye/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/01/the-trusted-automated-exchange-of-intelligence-information/">The Trusted Automated Exchange of Intelligence Information (TAXII)</a></h3>
        <span class="post-date">01 Aug 2017</span>
        <p><a href="https://oasis-open.github.io/cti-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions/taxii/taxii-logo.png" align="right" width="30%" style="padding: 15px;" /></a></p>
<p>I recently wrote about <a href="http://apievangelist.com/2017/07/10/opportunity-to-develop-a-threat-intelligence-apis-json/">the opportunity around developing an aggregate threat information API</a>, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of <a href="https://oasis-open.github.io/cti-documentation/">the Trusted Automated Exchange of Intelligence Information (TAXII)</a>, as one possible approach to defining a common set of API definitions and tooling for the exchange of threat intelligence.</p>

<p>The description of TAXII from the project website describes it well:</p>

<blockquote>
  <p>Trusted Automated Exchange of Intelligence Information (TAXII) is an application layer protocol for the communication of cyber threat information in a simple and scalable manner. TAXII is a protocol used to exchange cyber threat intelligence (CTI) over HTTPS. TAXII enables organizations to share CTI by defining an API that aligns with common sharing models. TAXII is specifically designed to support the exchange of CTI represented in STIX.</p>
</blockquote>

<p>I breezed through <a href="https://docs.google.com/document/d/1eyhS3-fOlRkDB6N39Md6KZbvbCe3CjQlampiZPg-5u4/edit#heading=h.4do73o99e2l7">the documentation for TAXII version 2.0</a>, and it looks pretty robust, and a project that has made some significant inroads towards accomplishing what I’d like to see out there for sharing threat intelligence. I’m still understanding the overlap of TAXII, the transport mechanism for sharing cyber threat intelligence, and STIX, the structured language for cyber threat intelligence, but it looks like a robust, existing approach defining the schema and an API for sharing threat intelligence.</p>

<p>Next, I am going to gather my thoughts around both of these existing definitions, and look at establishing an OpenAPI that represents STIX and TAXII, providing a machine readable definition for sharing threat intelligence. I think having an OpenAPI will provide a blueprint that can be used to define a handful of server side implementations in a variety of programming languages. I was happy to be directed to this existing work, saving me significant time and energy when it comes to this conversation. Now I don’t have to jumpstart it, I just have to contribute to, and augment the work that is already going on.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/01/the-trusted-automated-exchange-of-intelligence-information/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/31/professional-api-deployment-templates/">Professional API Deployment Templates</a></h3>
        <span class="post-date">31 Jul 2017</span>
        <p><a href="http://apievangelist.com/2017/06/14/gsa-api-standards-with-working-prototype-api-and-portal/"><img src="https://s3.amazonaws.com/kinlane-productions/gsa/gsa-prototype-api-portal.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I wrote about <a href="http://apievangelist.com/2017/06/14/gsa-api-standards-with-working-prototype-api-and-portal/">the GSA API prototype the other day</a>. It is an API prototype developed by the GSA, providing an API that is designed in alignment with <a href="https://tech.gsa.gov/guides/API_standards/">GSA API design guidelines</a>, complete with an API portal for delivering documentation, and other essential resources any API deployment will need. The GSA provides us with an important approach to delivering open source blueprints that other federal agencies can fork, reverse engineer and deploy as their own custom API implementation.</p>

<p>We need more of this in the private sector. We need a whole buffet of APIs that do a variety of different things, in every language, and platform or stack that we can imagine. Need a contact directory API, or maybe a document storage API, URL shortener API–here is a forkable, downloadable, open source solution you can put to work immediately. We need the WordPress for APIs. Not the CMS interface WordPress is known for, just a simple API that is open source, and can be easily deployed by anyone in any common hosting, and serverless environments. Making the deployment of common API patterns a no-brainer, and something anyone can do, anywhere in the cloud, on-premise, or on-device.</p>

<p>Even though these little bundles of API deployment joy would be open source, there would be a significant amount of opportunity routing folks to other add-on, and premium services on top of any open source API deployment, as well as providing cloud deployment opportunities for developers–similar to the separation between WordPress.com and WordPress.org. I could see a variety of service providers emerge that could cater to the API deployment needs of various industries. Some could focus on more data specific solutions, while others could focus on content, or even more algorithmic, machine learning-focused API deployment solutions.</p>

<p>If linked data, hypermedia, gRPC, and GraphQL practitioners want to see more adoption in their chosen protocol, they should be publishing, evolving, and maintaining robust, reverse engineer-able, forkable, open source examples of the APIs they think companies, organizations, institutions, and government agencies should be deploying. People emulate what they know, and see out there. From what I can tell, many API developers are pretty lazy, and aren’t always looking to craft the perfect API, but if they had a working example, tailored for exactly their use case (or close enough), I bet they would put it to work. I track on a lot of open source API frameworks, I am going to see if I can find more examples like the GSA prototype out there, where providers aren’t just delivering an API, they are delivering a complete package including API design, deployment, management, and a functional portal to act as the front door for API operations.</p>

<p>Delivering professional API deployment templates could prove to be a pretty interesting way to develop new customers who would also be looking for other API services that target other stops along the API lifecycle like API monitoring, testing, and continuous integration and deployment. I’ll keep my eye out for open source API deployment templates that fit the professional definition I’m talking about. I will also give a little nudge to some API service providers, consulting groups, and agencies I know who are targeting the API space, and see if I can’t stimulate the creation of some professional API deployment templates for some interesting use cases.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/31/professional-api-deployment-templates/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/31/making-the-business-of-apis-more-modular-before-you-do-the-tech/">Making The Business Of APIs More Modular Before You Do The Tech</a></h3>
        <span class="post-date">31 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/containership_dark_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I have been immersed in how APIs are being done in the federal government for the last week or so, looking for positive API behavior I can showcase and focus on in my storytelling. I was walking through each step of my API lifecycle, sizing up the federal government for each area I track the private sector on when it comes to APIs. I was taking a look at the areas of microservices, containerization, and serverless. You know the modularization of IT infrastructure in government? I couldn’t find much rubber meeting the road when it comes to microservices or containerization in my research, but I did see hints of modularizing the business aspects of doing APIs in the federal government.</p>

<p><a href="https://modularcontracting.18f.gov/modular-procurement/">Over at 18F you can find some interesting discussion around micro-procurement</a>, “a procurement model that breaks what would traditionally be a large, monolithic contract into several shorter-term, lower dollar amount contracts.” I feel that breaking down the business of defining, designing, deploying, managing, and even testing your APIs into small projects is an important first step for many companies, organizations, institutions, and agencies. While not all organizations will be the same, many will need to break down the business of procuring API design, deployment, and management services, before they can even getting to work breaking down the technical components of what is needing to be delivered.</p>

<p>I have been working with <a href="https://skylight.digital/">my partners at Skylight</a> on our approach to breaking down and delivering API projects, discussing the technical, business, and political aspects of doing things in as modular, bite-size chunks as we possibly can. This involves exploring the other side of the micro-procurement coin, with <a href="https://fcw.com/blogs/lectern/2017/07/kelman-microconsulting.aspx">micro-consulting</a>, providing API related services in small, modular projects. I’m exploring the delivery of <a href="http://apievangelist.com/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/">API training and curriculum</a>, as well as white paper, guide, and other content-centric services using a micro-consulting approach–keeping engagements small, focused, and delivering additional services that support one or many individual API implementations.</p>

<p>Micro procurement and consulting isn’t a fit for every scenario, but I can see it working well when it comes to helping companies, organizations, institutions, and government agencies thinking about how they can break projects down into smaller chunks, which will often involve delivering services across numerous stops along the API life cycle, from design to deprecation. Designing and deploying an API under a micro project approach limits the scope of what I’m able to deliver, easily keeping each API doing one thing, and (hopefully) doing it well. Making it easier for me to deliver microservices, complete with definitions, design guides, container, deployment, management layer, documentation, testing, and other essential elements for delivering APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/31/making-the-business-of-apis-more-modular-before-you-do-the-tech/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/">You See Duplicate Work While I See Common Patterns</a></h3>
        <span class="post-date">31 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/carryload_diego_rivera1.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>Someone asked me on Twitter recently how I deal the duplicate work required to manage a large volume of OpenAPIs. All the same things you have to do when crafting the headers, parameters, responses, and schema across every OpenAPI you are crafting. My response was that I don’t see these things as repetitive or duplicate work, I see these things as common patterns across the resources I am making available. They main reason I think they seem repetitive is the tooling we are currently using needs to play catch up, and help us better apply common patterns across all our APIs–dealing with the duplicate, repetitive work for us.</p>

<p>I’m confident that open source API design tooling like <a href="http://www.apicur.io/">Apicurio</a> are going to help us better manage the common patterns we should be applying across our OpenAPIs. I’m also hopeful that <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md">OpenAPI 3.0</a> contributes to reuse of parameters, schema, errors, and other common building blocks across the request and response surface of our API. I’m counting on OpenAPI + Apicurio as well as other API definition and design tooling to step up and do the hard work wen it comes to helping us manage the common patterns across our APIs, and make the reuse of common patterns across our APIs a good thing, and never a burden.</p>

<p>This reuse shouldn’t just being within any company, and we should be reusing and sharing patterns from across the space, including common web concepts and standards. The fact that you use ISO 8601 for all your dates, while employing a handful of date field names over and over across your systems isn’t repetition or duplicate work, that is consistency, and sensible reuse of common API patterns. It is the job of API design service and tooling providers to help us get over this hump, and craft notebooks, catalogs, collections, dictionaries, and stores of the common patterns we will need to be applying (over and over again) across our API definitions–OpenAPI.</p>

<p>I enjoy these regular reminders regarding how differently folks see the API space. I also enjoy the focus on the role that API specification formats and tooling will play in helping carry the load enough so folks don’t see detailed API definitions as a burden. I regularly come across OpenAPIs that you can tell are incomplete because someone was just doing the minimum amount of work they needed to deliver documentation for an API. It should be a no-brainer for API designers and developers to make sure there is always a robust, complete, and detailed OpenAPI for any existing API–while ensuring common patterns are applied at every turn.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/31/balancing-domain-expertise-with-the-disruptive-power-of-upstarts-doing-apis/">Balancing Domain Expertise With The Disruptive Power Of Upstarts Who Do APIs</a></h3>
        <span class="post-date">31 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/shipping-energy-trucking.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>APIs aren’t good, or bad, nor are they neutral. APIs do the bidding of their providers, and sometimes their consumers. In my experience APIs are more often used for bad than they are ever used for good, something I try to be as vocal as I can about, while working hard to shine a light on the good that is possible. After many years of trying to help folks understand APIs, one of the biggest challenges I face involves the unrealistic rhetoric of startups. The overoptimistic vision and promises of what APIs will do, coupled with an an often limiting awareness of the challenges and complexity of industries where APIs are targeting, making for a pretty toxic, non-cooperative environment for actually getting anything done.</p>

<p>I work hard to keep APIs alive in a variety of industries that have seen multiple waves of startups trumpeting their disruption and change horns, while also often belittling and underestimating the people within the industry. <a href="http://www.dcvelocity.com/articles/20170725-capital-amnesia/">I recently came across a post recently that captures the challenge we all face when we are looking to make change within established, and often entrenched industries using APIs</a>. I feel this paragraph captures it well:</p>

<blockquote>
  <p>The new players, and the venture capital/private equity money backing them, think they are entering a world full of Luddites. Yet the brokers we’ve talked to—and we know it’s not everybody—are quite IT-oriented. In a world where visibility is paramount, they are keenly aware of technology’s role in keeping them competitive. They are investing in IT and will continue to do so as prices drop. Meanwhile, many bring vast experience in mastering the physical part of the solution that the startups can’t touch.</p>
</blockquote>

<p>It is interesting to come across this friction in the freight brokerage industry. It is something I’ve seen in industry after industry, and with each wave of startups doing APIs. In some spaces startups will find success, but in others they will find themselves stopped cold by the entrenched positions some technical groups possess. I have experience developing trucking and shipping web applications, and predict there will be endless waves of automation that impact the freight brokerage space in coming years. However, I think startups will do much better if they focus on partnering with folks already in the space, investing in the existing expertise, rather than just going with the classic disruption rhetoric we’ve seen.</p>

<p>I am fascinated by how APIs can lay the groundwork for collaboration and better access to resources, or they can also lead to the introduction of significant friction and disruption within a sector. I’m just glad there is discussion about the role funding plays when it comes to each players motivation within a particular industry, acknowledging the legacy debt owned by the entire sector, as well as the disruptive nature of upstarts when they don’t partner with, or leverage the existing domain expertise within a space. I’ll keep an eye on what is going on in the freight brokerage sector, and see if I can better understand how APIs are becoming the solution, or contributing to friction going down the road.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/31/balancing-domain-expertise-with-the-disruptive-power-of-upstarts-doing-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/27/state-of-apis-in-the-federal-government/">State of APIs In The Federal Government</a></h3>
        <span class="post-date">27 Jul 2017</span>
        <p><em>This is my talk from Washington DC with Steve Willmott of <a href="https://www.3scale.net/">3Scale</a> by <a href="https://www.redhat.com">Red Hat</a> about <a href="http://redhatapievents.com/dc">transforming enterprise IT with containers, APIs, and integration</a>, where I assess the current state of APIs in the federal government, and the opportunity in the private sector when it comes to working with government data.</em></p>

<p><strong>API Evangelist</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/kin-lane-api-evangelist-cartoon.png" width="12%" style="padding: 15px;" align="right" /></p>
<p>My name is Kin Lane. I am the API Evangelist. I have studied the technology, business, and politics of Application Programming Interfaces, or more commonly known as APIs, full time since 2010. I spend time looking through the growing number of APIs available today, as well as the evolving group of service providers selling their solutions to API providers. I take what I learn across the space and publish as over 80 independent research projects that I run on Github, covering a growing number of stops along the API lifecycle.</p>

<p>In 2011, I began studying and writing about federal government APIs. I have long had an interest in politics, and how our government works (or doesn’t work), which was in alignment with thinking about how I could take what I’ve learned about APIs and apply to the federal government. By 2013, my research and storytelling about APIs attracted the attention of folks in government, which led to an invitation to come work on open data and API projects at multiple agencies. This move took my work to new levels, opening up some interesting doors that have opened my eyes to the scope of APIs within federal government.</p>

<p><strong>Presidential Fellow</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/kin-lane-presidential-innovation-fellow.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>In the summer of 2013 I was invited to be part of the round two Presidential Innovation Fellowship, and work at the Department of Veterans affairs doing web service and API inventory, as well as assist with the wider open data efforts of the Obama administration. I worked in DC until the government shutdown in October, when I decided to leave my position so that I could continue doing my work around veterans data, and continue meeting my obligations as the API Evangelist.</p>

<p>My time as a Presidential Innovation Fellow (PIF) gave me access to a number of interesting folks, doing interesting things at a variety of federal agencies, including now 18F and USDS. During my work as the API Evangelist I have spent a lot of time expanding on these relationships by helping craft API strategies, telling stories publicly on my blog, and speaking on conference calls and on-site when it makes sense. While the PIF program wasn’t a good fit for my career, the opportunity forever changed how I see government, and how I see APIs and open data making an impact at this scale.</p>

<p><strong>Adopta.Agency</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/adopta-agency-primary.png" width="15%" style="padding: 15px;" align="right" /></p>
<p>After I left DC, I began working on a project <a href="http://kinlane.com/2014/01/18/adopt-a-federal-government-dataset/">I called Adopt a Federal Agency Dataset</a>, where anyone could choose a single dataset published by a federal agency, fork and “adopt” the dataset using Github. The goal is to get civic minded folks spending time improving the quality of data, converting to another format, or even publishing a simple API using the open data. I got to work building a prototype, and talking with a variety folks about the projects viability.</p>

<p>Eventually my work paid off, and I attracted the attention of the Knight Foundation, and <a href="https://www.knightfoundation.org/grants/201551217/">received $35K USD to develop a prototype template for the project</a>. Over the course of the next year I developed a <a href="http://adopta.agency/">website for the project</a>, a <a href="http://adopta-agency.github.io/adopta-blueprint/">forkable blueprint for adopting datasets that runs 100% on Github</a>, and applied the template to a handful of open data projects including the White House budget, and Department of Education data. I have continued to support and evolve the work, regularly adopting federal agency data sets, and working to develop common schema and API definitions for use across government of all shapes and sizes.</p>

<p><strong>APIs In The Federal Government</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/fed-agencies-logos-032515.jpg" width="30%" style="padding: 15px;" align="right" /></p>
<p>Over four years of work, including my fellowship in Washington DC, I have definitely fired up my passion for understanding how government works (or doesn’t). I’ve continued to keep an eye on open data and API efforts across federal agencies, and regularly meeting and talking with folks who are doing things with APIs in government. I spent at least 2-3 hours a week, and sometimes more on playing with government APIs, and studying their approach to understand where I can help move the conversation forward.</p>

<p>I feel like I could just dedicated my career as the API Evangelist to government APIs, but I find what I learn in the private sector is extremely useful in the public sector, and vice versa. There is a symbiotic relationship between the viability of APIs in the federal government, and the viability of APIs at higher educational institutions, the enterprise, small businesses and agencies, all the way to upstarts in Silicon Valley. This relationship has kept me studying the intersection of public and private sector API operations, and tracking on, and contributing where I can to the API momentum in DC.</p>

<p><strong>96 /Developer Portals</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/gsa-prototype-city-pairs-api.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>As of this week the <a href="https://raw.githubusercontent.com/18F/API-All-the-X/gh-pages/_data/developer_hubs.yml">GSA has 96 developer portals for federal agencies</a> present. A portion of these developer portals are simply data portals, a result of open data efforts in the last administration. There is another portion of them that you can tell do not have a complete strategy, and are more of a landing page for a couple of APIs, and some open data. However, there are some API developer portals present that have some high value datasets and APIs, with robust documentation, code samples, and all the other bells and whistles you’d expect of an API in the private sector.</p>

<p>Even with the inconsistencies in federal agency developer portals, all the top agencies now have an API portal presence, with a growing number of lesser agencies following their lead. This is significant. This goes beyond just these federal agencies being more transparent and machine readable in how they operate. Many of the resources being made available also have the potential to impact markets, and are powering businesses of all shapes and sizes. This momentum is setting the stage for a future where ALL federal agencies have and open data and API portal that businesses, researchers, and developers know they can find the latest data, content, and algorithms across federal agencies.</p>

<p><strong>400+ APIs</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/open-fec-portal.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>As of this week the <a href="https://raw.githubusercontent.com/18F/API-All-the-X/gh-pages/_data/individual_apis.yml">GSA has over 400 APIs listed</a> across the 96 developer programs found across federal agencies. There are numerous consistency and design issues across these APIs, and many of them are simply downloads, which may have some form filter as a gatekeeper. While we don’t have any numbers to support what kind of growth we’ve seen in federal government APIs over the last five years, off the top of my head I’d say we’ve seen up to 50% growth each year in the number of APIs published by agencies.</p>

<p>In my experience APIs breed more APIs. When one group sees another group doing APIs, they often emulate what they see. Folks in the private sector have been emulating the API moves of pioneers like SalesForce, Amazon, Twitter, and Twilio for years, we are seeing this play out at the federal government level. Some agencies are emulating what they see in the private sector, but I’d wager a significant number of agencies will more likely emulate what they see out of other agencies. This is how we are going to take all of this to the next level, building on the hard work of USDS, 18F, and other individual agencies like Census, NASA, and others who are truly pushing the needle forward.</p>

<p><strong>133 Github Orgs</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/68747470733a2f2f662e636c6f75642e6769746875622e636f6d2f6173736574732f3238323735392f313333353931312f32386233656336362d333563302d313165332d386565362d3636323732623966343138362e706e67.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>The almost hundred developer portals, and over 400 APIs gives me hope about APIs in the federal government, but one area that leaves me the most optimistic is the number of agencies who are operating on Github. Doing my time at the Department of Veterans of Affairs (VA), I was the one who setup the VA Github organization, which became a lifeline for every one of the API and open data projects I was working on. Github usage across these agencies goes beyond just managing code, and agencies are using it to define policy, provide documentation, solicit feedback, and much more.</p>

<p>This usage of Github sets the stage for the next wave of growth in API operations within federal governments where Github is a central actor at every stage of the API life cycle. Github adoption is the most important signal I use to understand the viability of any API effort in the private sector. The companies who are using Github effectively almost always also possess a strong position in the space, directly influenced by the way they use Github to manage code, schema, definitions, and other machine readable, reusable components of API operations. I’m predicting that the federal agencies using Github possess a similar advantage if the federal API ecosystem, in coming years.</p>

<p><strong>18F and USDS</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/18f-usds.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>A significant amount of the API leadership I’m seeing across federal agencies has been stimulated by tech focused groups at the White House and GSA. <a href="https://www.usds.gov/">U.S. Digital Service (USDS)</a> and 18F are working with multiple agencies to deliver API driven solutions, and software development practices. These groups have played a critical role in bringing outside tech practices into the federal government and doing the hard work to begin applying, evolving, and hardening these practices across agencies.</p>

<p>USDS and 18F are doing great work, and while I’m not 100% up to speed on their road map, the last couple of conversations I’ve had, they are growing and expanding–looking to keep taking on new projects. I’d like to see similar models develop, but from the outside-in, beginning to shift the landscape when we think about government consulting and vendors. 18F has done an amazing job at not just rethinking the technology part of the equation, but also thinking about the business and politics of it all, something that I think should be replicated externally, creating a new realm of API opportunity outside the federal government firewall.</p>

<p><strong>Transparency In Federal Government</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-government.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Open data and APIs provide transparency into how government works (or doesn’t work). It gives us a look into the budgets of federal agencies, and better understand what government does, manages, and the impact it has on society. I fully support the Obama’s administration mandate requiring ALL federal agencies publish their data inventory as machine readable files, as well as APIs when possible. This is how government should operate by default, ensuring that agencies are more accountable to the public on a real-time basis.</p>

<p>I’m looking to continue the trend of transparency with data and APIs across federal government–doing more of what we’ve been doing for the last 5 years. However, in coming years I’d like to elevate this to become more about observability, where things are transparent, but there is also monitoring, testing, communication, service level agreements, and other terms of service that make government more observable by the public, as well as government auditors. This should be the default state of technology in government, where each agency is able to invest in the Venn diagram of operations that can become API observability.</p>

<p><strong>Resources That Help The Private Sector</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-business-of-apis.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Publishing data and APIs isn’t all about making the federal government more transparent. Many of the API and data resources I come across have significant commercial value as well. The healthcare industry benefits from pharmaceutical data being open as part of OpenFDA are significant. Every citizen can benefit from the web, mobile, and automobile applications developed on top of the <a href="https://ridb.recreation.gov/">recreational information database (RIDB)</a> which powers <a href="https://www.recreation.gov/">Recreation.gov</a>, and the growing ecosystem on top of the RIDB API. I can keep going, talking about APIs at Commerce, NOAA, and Census–these APIs all have significant impact on the way business works.</p>

<p>There are many ways in which the federal government has been stepping up to provide valuable data for use by companies of all shapes and sizes. Census data is baked into many systems. Labor data is getting baked into the business models of startups. Regulations.gov API allow the rules of the road to get exposed for business consideration. This open data and API stuff at the federal government level is getting pretty important when it comes to how our economy and society works. While I do have some serious concern about the current administrations leadership when it comes to APIs I feel the current momentum around opening up data and APIs can be sustained, if approached in the right away.</p>

<p><strong>Considering The API Lifecycle</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-lifecycle.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Alright, that describes the momentum we have with APIs in the federal government. I want to step back and size up this momentum, and take a look at some of the deficiencies I’m seeing right now in federal government developer portal and API implementations. In my regular work as the API Evangelist I look at the entire API space through the lens of almost 85 stops along an API lifecycle, from definition to deprecation. I’m going to take a handful of the core stops and use them as a lens to look at the state of federal government APIs, helping me frame a discussion about how we might move this conversation forward.</p>

<p>I’m looking to see the federal government API space alongside how I look at every other API in the private sector. I’m not saying that everything that gets done by commercial API providers should be emulated by the federal government, I’m just looking to make sure both sides are learning from each other, and somewhat in sync. I’m looking to try and figure out how we can keep the momentum going that I referenced, but also make sure federal agencies are not falling behind in all the key areas commercial API providers are using to dial in their API operations.</p>

<p><strong>Definitions</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-api-definitions.png" width="14%" style="padding: 15px;" align="right" /></p>
<p>Starting with the most important stop along the API lifecycle, I’m beginning to see more common web and API definitions being applied as part of federal government API operations. I’m always on the hunt for agencies using common web standards and specifications as part of their designing, using common definitions for dates, currency, and other key elements. I’m also finding more APIs defined using OpenAPI, API Blueprint, and other commonly used API definition specifications, providing a machine readable definition of the surface area of any government API.</p>

<p>Even with the definitions in play currently, it is still a small portion of the 400+ APIs I’m seeing. Most APIs are custom definitions, without much investment in recycling and reuse. The federal government needs a heavy dose of evangelism around common definitions, and the development of templates, and schema that agencies can leverage in their projects. This is one significant area I see an opportunity for the private sector to step up, and help bring robust toolboxes full of open definitions for APIs, and underlying schema applying the leading web concepts and standards.</p>

<p><strong>Design</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-api-design-fiction.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Along with the increased presence of common approaches to defining APIs and schema, there is an emergence of more of a design-centric approach to delivering APIs. Sadly, this is only present in a handful of leading federal government APIs. Most of them are still pretty classic IT and developer focused implementations, often reflecting the backend system where they were generated. The current API design practices reflect that of the commercial sector around 2012, where you developed then documented an API, without much consideration for design beyond just RESTful principles.</p>

<p>The last administration worked hard to put forth API design guidelines, something that has been continued by USDS and 18F, but this needs to continue in practice. <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">You can see an example of this in action over at the GSA with their prototype City Pairs API</a>–which is a working example of a well designed API, portal, and documentation that any agency can fork, and reverse engineer their own design compliant API. A significant area of movement in the private sector that will contribute to the evolution of API design at the federal level involves the availability of <a href="http://www.apicur.io/">open source API design editors like Apicurio</a>, which is a sign of the maturing private sector API design space, which the federal government should be able to leverage across agencies.</p>

<p><strong>Deployment</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-api-deployment.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>18F gives us a regular look into how they deploy the APIs behind their projects, going into detail on the technology stack employed, and other detail about how it was done. Sadly, this isn’t a common practice across federal agencies, leaving API deployment something that happens in the dark. This practice prevents agencies from learning through each agencies deployment, keeping every API deployment in a silo. When it makes sense from a security perspective, all server side APIs should be open sourced, and available on an agencies Github organization–making all API deployments available for reverse engineering, and deployment by other agencies.</p>

<p>I have to point out again <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">the City Pairs API prototype from the GSA</a>, which open sources the API on Github, and provides step by step instructions on how to set it up, including the entire stack of technology used to operate it. This is how all government APIs should be, and opens up a significant opportunity for the private sector to step up and provide open source APIs that federal agencies can deploy as part of their stack, opening the door to other service and consulting opportunities. The federal government will benefit from the standardization introduced by the GSA, and will continue to grow with the type of API storytelling out of the 18F, eventually bringing API deployment out of the shadows.</p>

<p><strong>Modularization</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-microservice.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Before I begin the shift from the more technical stops along the life cycle and move into the business, politics, and more operational side I wanted to talk about modularization of APIs in the federal government. I’m talking about how APIs are designed, and deployed, considering a microservices way of doing things. I’m also talking about containerization, and ensuring the deployment of APIs are modular, scalable, and well defined. Something that makes them more deployable in any infrastructure on-premise or in the cloud.</p>

<p>Another area of modularization to consider in my evaluation of federal government APIs can be found over at 18F with their leadership in <a href="https://18f.gsa.gov/2016/11/15/modular-procurement-state-local-government/">modular procurement</a>. Identifying that the underlying technology isn’t the only thing we we need to be decoupling, and that the business and politics surrounding each API needs to be modular as well. I am seeing some talk of microservices, containerization, and serverless, mostly out of the GSA, but it will be another area we need to see growth in when it comes to API operations in the federal government. Acknowledgment that we need to be decoupling the technology, business, and politics of government resources.</p>

<p><strong>Management</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/api-management.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>API management is another area that I’ve seen some significant growth and maturity in federal agency API operations. Not only did we see one of the first open source API management solutions Umbrella emerge out of a federal agency, we have seen over 10 federal agencies adopt this as part of their API operations. We need more of this. API Management should be baked into all federal government APIs, standardizing how APIs are secured, rate limited, logged, and measured. Standardizing how all digital government resources are accessed and metered–establishing a common awareness across all federal agency APIs.</p>

<p>This layer of API operations is what is going to set the stage for the next wave of growth in not just government APIs, but also government revenue. Commercial API providers have been leverage service composition at this layer to meter and charge for access to some resources, charging different rates to the public, partners, and even amongst internal groups within a company. This approach to managing digital government resources will emerge as the standard for generating revenue from public resources similar to how timber leases, mining claims, and other physical resources are managed–except this is all being done on public servers, not public lands.</p>

<p><strong>Portal</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-portal.jpg" width="18%" style="padding: 15px;" align="right" /></p>
<p>Wrapping around any APIs you deploy, and providing a gateway to solid API management practices always begins with a proper developers portal. The almost 100 developer portals for federal agencies that now exist are due to the hard work of the GSA, direction of OMB, under the guidance of the Obama administration. A central portal is key to application developers, system integrators, and even other agencies knowing where to find data and API resources. Most of the existing API developer portals leave much to be desired, but when you look at the efforts out of CFPB, 18F, and the GSA, you get a glimpse of what is possible in government.</p>

<p>I have to point out <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">the City Pairs API prototype from the GSA</a> yet again. This approach to providing API design, deployment, management, and portal guidance is how consistent developer portal operations will spread across all federal agencies. Each agency should be focusing on what they do best, and not worrying about having to reinvent the wheel each time they deploy a new agency portal, project, or other implementation. There should be a wealth of open source API portals available on Github for agencies to fork and employ immediately to support any API implementation.</p>

<p><strong>Documentation</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-documentation.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>I am seeing Swagger UI, Slate, and a handful of other interesting API documentation solutions as I browse through the 400+ federal government APIs. This evolution in open source documentation is partly responsible to the API definition portion of the conversation, as OpenAPI is the engine of Swagger UI, and other API documentation solutions. This approach to delivering API documentation across government APIs, and available as part of any portal template has encouraged significant reuse, and consistency  across government API implementations–something that should spread to every API implementation, at every federal agency.</p>

<p>I want to point out the OpenFDA has moved the needle when it comes to API documentation by providing more interactive documentation, including visualizations as part of their API implementations. We need to make API definition driven documentation the default practice across federal government APIs, but it is also significant that we are seeing the API documentation be evolved as part of the valuable information government agencies are making available. This is a good sign that there are healthy API documentation efforts emerging, as this type of innovation is always a by-product of healthy API ecosystems.</p>

<p><strong>Communications</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-chat.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>I’m seeing healthy communication around data, API, and developer portals from the <a href="https://usedgov.github.io/news/">Department of Education</a>, <a href="https://www.census.gov/data/developers/updates.html">Census</a>, <a href="https://18f.gsa.gov/blog/">18F</a>, <a href="https://www.usds.gov/blog">USDS</a>, and a handful of others, but for the most part communication around API operations is non-existent at the federal level. This is a problem. This makes many APIs look dead or lifeless, with very little information about what is happening beyond just the documentation. Communication with integrators and developers is an essential part of API operations, and a big reason growth in API efforts at the federal level are slower than anticipated.</p>

<p>I understand that federal agencies are constrained by rules regarding how they can communicate with the public, and operating a blog isn’t as straightforward as in the private sector. However, it is clear that it can be done. Plenty of agencies have blogs, and active social media accounts, this practice just has to be applied across API operations. I’m guessing once we see blogs, Twitter and Github accounts default across federal government agency API operations, we’ll see a significant uptick in the number of integrations and applications that are putting government resources to use in their operations.</p>

<p><strong>Road Map</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-roadmap.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>While <a href="https://tech.gsa.gov/roadmap/">you can find road maps for agencies like the one over at the GSA that include APIs</a>, communicating the road map for a federal agency API really isn’t a thing. Honestly, I don’t see much evidence of communicating what is coming down the road with federal agency API operations, what is happening currently, or what has happened in the past by providing a change log. <a href="https://developer.usajobs.gov/Guides/Change-Log">The USA Jobs API has a change log</a>, and some API related code have change logs as part of their API operations, but communicating around change in government API platforms isn’t really a common concept.</p>

<p>The absence of road maps, issues, and change logs for API operations at the federal level is a problem. I’m hopeful that the usage of Github by federal agencies might shift this reality of API operations currently, but I’m also hopeful that this could be maybe be automated for agencies as part of their API deployment, management, and portal solutions. Taking another concern off the table for API providers, while still ensuring what API consumers and integrators will need to stay in tune with federal government resources being exposed via APIs.</p>

<p><strong>Monitoring</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-heart-monitor.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Moving on to the monitoring, testing, and other awarness aspects of API operations. Sadly, there is no evidence of API monitoring and testing at the federal government levels. I searched my news archives, and looked for evidence across the APIs I’ve been reviewing as part of this research. I am not seeing any organized approaches to monitoring API endpoints, sharing the results with the developer community, or the mention of any common API testing tooling in the space.</p>

<p>This is another critical issue regarding the state of APIs in the federal government. As I was browsing around looking for APIs I came across several portals and APIs that had gone dark, demonstrating what a problem it is that nothing is being monitored even for availability. Ideally federal agencies are monitoring the uptime and availability of APIs already, and should be moving on to more detailed, meaningful testing. The only good news on this front I found, <a href="https://www.opm.gov/developer/documentation/current-status-api/">was a single API for monitoring whether or not the government in Washington DC is operating or not</a>–at least we have that, right?</p>

<p><strong>Security</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-padlock.png" width="15%" style="padding: 15px;" align="right" /></p>
<p>Continuing down into the realm of bad news, let’s talk about what I’m seeing when it comes to API security. I am seeing a small bright spot, if we are talking about acknowledging the need for encryption in transit to be default for APIs, but API security seems to stop there. Sure we are seeing some APIs adopting API umbrella to key up APIs, but this is more API management then it is API security. All leaving me concerned about what is going on behind the scenes, as well as thinking security concerns and lack of healthy practices are probably a significant area of friction for new APIs to get out the door in the first place.</p>

<p>Security is the number one concern of agencies when you begin to talk with them about APIs. It needs to be front and center in all API conversations. I’m talking about formal strategies regarding how to security APIs, with official guidance from API leadership, and published pages sharing security practices with the community. API security begins with healthy API management practices, something I’m seeing across federal government implementations, but it is something that evolves with sensible API monitoring, and testing practices, which I’m not seeing. The absence of this awareness is a problem, and needs to become baked into all API efforts at the federal level as soon as possible.</p>

<p><strong>Reliability</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-reliability.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>My personal confidence in the reliability of government APIs is pretty low. The availability of APIs I was depending on for my work during my Presidential Innovation Fellowship during the fall shutdown in 2013 was the beginning of my concern, but the stability, longevity, support, communication, and lack of road map or change logs for APIs all have contributed to my concerns over the years. The transition from the previous administration to the current one has moved up my levels of concern significantly. I’m just not convinced some agencies will always be good stewards of the APIs they are making available.</p>

<p>API reliability is just as much a business and political challenge as it is a technical one. Staffing and budget issues will most likely contribute more to API reliability than actual server or network problems. I know that if I was building any business on top of any data or API from a federal agency I would be building in multiple layers of redundancy wherever possible, and work to always have a plan B in place in case an API goes away completely. I feel that this is an area where the private sector can step up, and not just provide vendor solutions that deliver reliable API solutions, but also help from the outside-in, providing hosting, caching, virtualization, and other approaches to ensuring there is redundancy, and always access to vital resources being made available via federal government APIs.</p>

<p><strong>Discovery</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-api-discovery.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>The GSA, and 18F has done an amazing job aggregating API resources for use by all federal agencies. This includes providing a listing of federal government <a href="https://api-all-the-x.18f.gov/pages/developer_hubs/">developer portals</a>, as well as <a href="https://api-all-the-x.18f.gov/pages/individual_apis/">all the known agency APIs</a>, in CSV, XML, JSON, and YAML formats.  I use these lists to help maintain <a href="http://federal.government.stack.network/agencies/">my own federal agency API directory</a>, and keep in tune with what is going on.</p>

<p>Another significant area of opportunity for investment in federal government APIs is a discovery engine. <a href="http://gsa.index.apievangelist.com/">I’ve worked to create APIs.json indexes for some federal government APIs including the GSA</a>, but this work is time intensive, and ideally is something that should be done by each API provider, within a government agency. You can see this in action over at Trade.gov, <a href="http://developer.trade.gov/apis.json">who has published an APIs.json for their APIs</a>, which also includes links to documentation, OpenAPI (swagger) definitions, and terms of service. Ideally, all federal agencies would have an APIs.json index in the root of their website, similar to the data.json file that already exists for federal government data inventory. It will take time, but eventually federal agencies will see the benefits in making <a href="http://apisjson.org/">their APIs discoverable using APIs.jso</a>n, making it available for inclusion in API search engines like <a href="http://apis.io/">APis.io</a>.</p>

<p><strong>Clients</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-client.png" width="20%" style="padding: 15px;" align="right" /></p>
<p>There are a wealth of SDKs, code libraries, and frameworks emerging across the Github accounts of federal agencies. <a href="https://government.github.com/">You can find a list of these government Github accounts on the government section of Github</a>. Some agencies do well at showcasing code as part of their API operations, while others you have to hunt a bit, and many who don’t offer any code at all as part of their API offerings. I’d say that client tooling and code across federal government APIs is a mess right now. There really isn’t a single discovery mechanism beyond Github, or any standard for developing and presentation across agencies, or even just across many APIs in a single federal agency.</p>

<p>The generation, development, maintenance, and integration of code for use as part of federal API integration is a pretty significant opportunity for the private sector. The APIs are publicly available, many with existing API definitions, it wouldn’t take much to develop and maintain OpenAPIs for all federal agency APIs, and regularly update SDKs, code samples, and other client tooling that put agency APIs to use. Client tooling should be another thing agency API providers shouldn’t have to worry about. They should be just focusing on maintaining an OpenAPI, and then leverage private sector tooling and services to generate client code, and put common client solutions like Postman and Restlet to use when working with federal government APIs.</p>

<p><strong>Integration</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-integration-automation.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>After client and SDK concerns I like to expand and consider wider integration solutions, and particularly the integration platform as a service (iPaaS) question. In 2017, there are open source, and software as a service solutions for integrating with APIs, and providing client tools that anyone, even non-developers can put to use. Service like <a href="https://zapier.com/">Zapier</a>, and <a href="https://github.com/DataFire">open source DataFire</a> allow for integration with existing 3rd party APIs, and custom API integration, which can be used by business users, as well as developer and IT groups.</p>

<p>You can find If This Then That (IFTTT) recipes <a href="https://ifttt.com/usagov">for working with USA.gov</a>, and <a href="https://zapier.com/blog/federal-government-terms-of-service-amendment/">guidance from Zapier for API providers to make their API terms of service government friendly</a>, but there is not a lot of other movement when it comes to the federal government making use if iPaaS solutions. While not as far along as we should be, I’d say the stage is set for federal agencies to begin thinking through their integration strategies, leveraging solutions like Zapier, because you not only get an integration solution, you get access to the experience that comes with integrating with over 750+ APIs.</p>

<p><strong>Spreadsheets</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-excel-icon.jpg" width="20%" style="padding: 15px;" align="right" /></p>
<p>Another significant area for API client integration as well as API deployment is the spreadsheet. Both Microsoft and Google Spreadsheets allow for consumption of many data, content, or algorithmic APIs right in the spreadsheet. When you consider spreadsheets across federal agencies, you see how this could significantly change how federal government APIs are put to use internally at the agencies where they are deployed, inter-agency, or even by external 3rd party researchers, partners, and integrators.</p>

<p>You can find quite a few <a href="https://catalog.data.gov/dataset?res_format=Excel">data downloads in spreadsheet format at data.gov</a>. Use the interesting <a href="https://www.eia.gov/opendata/excel/">Excel data add-in and Google Sheets add-on over at the U.S. Energy Information Administration (EIA)</a>, and see <a href="https://18f.gsa.gov/2016/05/02/from-spreadsheet-to-api-to-app-a-better-contract-forecast-tool/">the cool spreadsheet to API work over at 18F</a> to consider a handful of examples of the role spreadsheets play in the federal API game. There needs to be a significant amount of investment in spreadsheet to API and API to spreadsheet integration across the federal government in coming years, to reach beyond the technical community audience.</p>

<p><strong>Plans</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-plan.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Now that we are seeing API management practices being adopted by federal agencies, I’d like to plant the seeds for some of the next steps for agencies when it comes to making public resources available for commercial use. API Umbrella, and <a href="https://api.data.gov/">api.data.gov</a> help manage API key usage for developers and the daily and hourly rate limits on accessing api.data.gov APIs, ensure API resources stay available, and aren’t overused by any single consumer.</p>

<p>Commercial API providers like Amazon, Google, and others often charge different rates for different API resources, while also providing a free level of access for the public, or possibly researchers. There are also many API service providers who provide advanced API management solutions for billing against API usage, charging for high levels of access, or maybe establish unlimited usage levels for a select handful of trusted consumers. It is common to have multiple plans for API access, and charging for API consumption in some cases, helping pay for infrastructure and other hard costs.</p>

<p>I get that this is a controversial topic in my circles–paying for accessing public data? In short, data is valuable, and making sure it is available, and usable creates value. It costs money to make data available as APIs is a reliable and sustainable way. It makes sense to charge for high volume of commercial access to APIs, similar to how government charges commercial operators to access resources on public lands. It may seem like a crazy notion now, but in the future, this is how government agencies will generate much needed revenue from public resources.</p>

<p><strong>Partners</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-partner.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Continuing the API management and plans conversation, I’d like to highlight what I’d consider to be an under-realized, and under-utilized partner network of federal government API consumers. You can see what I am talking about over at <a href="http://www.opendata500.com/us/">the Open Data 500 project</a>, which is “the first comprehensive study of U.S. companies that use open government data to generate new business and develop new products and services.” This approach to quantifying the companies using federal government open data, needs to be applied to APIs, establishing a feedback loop with any company who integrates with a federal agency APIs.</p>

<p>The federal government has many partners, but is rarely ever showcasing or leveraging these partners in any meaningful way when it comes to API integration, or application development. The friction of existing laws regarding how government can work with companies causes some of this, however much of this is due to not understanding how 3rd party development works within API ecosystems at the federal level. Some efforts like the Open Data 500 have been established to help quantify how open data is being used across industries, but in my experience much more work is needed to help understand who is using federal government resources, and who might be a fit for heightened levels of engagement to strengthen partner relationships between the public and private sector.</p>

<p><strong>Legal Department</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-terms-of-use.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>The federal government has endless number of laws on the book governing how it operates, with a significant portion of these laws introduced to guide the government when it comes to using the Internet, purchasing and developing software in an Internet age. Along with the technical, and business aspects of doing APIs, federal agencies are in need of beginning the process of publishing and negotiating terms of service, privacy policy, branding guidelines, service level agreement, breach, vulnerability, and other critical guidelines out in the open, with all stakeholders (public included) present.</p>

<p>You see the beginning of this in action over at 18F in the GSA where they are drafting, sharing, and managing key aspects of developing software and managing system integrations with APIs out in the open, making the public a first class citizen in these discussions. The legal department for API operations can be conducted via Github, including partners, developers, and the public in the process. There is a precedent, and the benefits for not just making government transparency, but making technology better serve citizens exists. Agencies will learn API design, deployment, and security, privacy, and terms of service by emulating what they see–if there are no examples out there for them to follow, these legal aspects of operating government will remain behind closed doors.</p>

<p><strong>Evangelism</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/KL_InApiWeTrust-1000.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Building on my earlier comments on the lack of communication out of existing federal agencies when it comes to data and API efforts, the advocacy and evangelism of APIs out of federal agencies is near non-existent. You see efforts from a handful of agencies trying to do more outreach and evangelism around their data and API projects, often led by 18F and USDS, but evangelism is something that just isn’t in the DNA of federal government as it currently exists.</p>

<p>Getting the word out about federal APIs is essential to establishing a vital feedback loop that government agencies will need to improve upon systems, data collection practices, and generally serving the public. Without outreach few will know these API resources exists. Without outreach agencies will never know who is using, or who might be looking to use government resources. Evangelism isn’t just about hackathons and conferences, it is also about communications, support, and other essential aspects of developing software and integrating with systems. With no outreach, all of this occurs in a silo, and behind closed doors, limiting what is possible.</p>

<p><strong>Some Momentum</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-momentum.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>There is a significant amount of momentum when it comes to data and APIs at the federal government level coming out of the last administration. With the leadership from the Obama administration we see agencies stepping up to publish their public data inventory, and increasing the number of developer portals and APIs that are available. This momentum augments existing IT Momentum to grow and expand government IT infrastructure, with API investments reflecting expansion in government doing business securely on the open web, leverage the low-cost, scalable benefits of building using existing web technology.</p>

<p>While this momentum is real, it is seriously in danger of slowing, and even reversing course in the current administration. All the leadership available regarding API implementation has gone away, leaving agencies alone in their API strategy development. Without leadership many of these API efforts will wither and die as they are. With a vacuum in leadership there is a significant opportunity for vendors, and outside leadership to step in, take ownership of open source projects, standards, and guidance, and play an important role in keeping projects active, alive, and even growing.</p>

<p><strong>Lots of Work</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-work.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>The most common advice I give agencies regarding what they can be doing on the API front is doing more of the same. Keep opening up data. Keep publishing APIs. Keep supporting and expanding your developer portals. We have lots of redundant work ahead of us to make sure all federal IT systems can securely communicate over the web to serve all constituents, and support all government workers. There are a number of areas like monitoring, testing, security, communication, and evangelism that should be invested in, but we also just need to do the hard work keeping what is already on the table active.</p>

<p>It is unrealistic to expect the federal government to do all this work alone. Which is why ALL federal agencies should be doing APIs, to open up data and internal systems to other federal agencies, vendors, partners, and the general public to help carry the load. It is perfectly acceptable for work around public data and other resources to happen out in the open via API integrations, Github, and other platforms and services already in operation. This is why APIs can’t be shuttered or allowed to go dormant. Any government project data project in operation should be externalized, API-ized, and opened up for a collaborative approach to running government.</p>

<p><strong>Private Sector Opportunity</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-opportunity.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>The most important part of this state of APIs in federal government story is about what the private sector can step up and do. I mentioned several opportunities for companies, organizations, and institutions to step up and help be stewards of federal government data, APIs, and open source code. This means doing business through existing federal government procurement channels, but it also means stepping up and investing in existing federal agency open data and API efforts, engaging with agencies via Github repositories, forums, and social media.</p>

<p>You see companies like Amazon, Microsoft, Zapier and Github stepping up with government focused section of their platform, specialized products and services, and terms of service tailored for government adoption. We need more of this. We need more startups to step up, incentivize, and lead when it comes to APIs in government. Federal government APIs aren’t just about what government is doing with APIs, it is also about what the private sector does to use these APIs, and how they engage with government agencies whenever possible to hep improve the quality of APIs, and software coming out of federal agencies.</p>

<p>This reality is even more dire in a Trump administration. In the current administration any API leadership has been halted, and the two groups (USDS &amp; 18F who are leading the charge are facing friction, and seeing top talent exit back to the private sector, unable to make the impact they had envisioned when they first signed up for government service. The API hard work is continuing across the federal government, within each agency, but in coming years we need the bulk of work to be assumed by the privacy sector, letting dedicated government employees know that we have their back, are here to support, integrate, and build on their hard work, while providing valuable and constructive feedback wherever we can.</p>

<p><em>Thank you to Red Hat for bringing me out, and gathering folks at Tyson’s Corner for the interesting discussion about APIs–some interesting people showed up including government, vendors, and consulting groups.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/27/state-of-apis-in-the-federal-government/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/26/we-have-a-hostile-ceo-which-requires-a-shift-in-our-api-strategy/">We Have A Hostile CEO Which Requires A Shift In Our API Strategy</a></h3>
        <span class="post-date">26 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/raven-fence.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>As I work my way through almost one hundred federal government API developer portals, almost 500 APIs, and 133 Github accounts for federal agencies the chilling effect of the change of leadership in this country becomes clear. You can tell the momentum across hundreds of federal agency built up over the last five years is still moving, but the silence across blogs, Twitter accounts, change logs, and Github repos shows that the pace of acceleration is in jeopardy.</p>

<p>When you are browsing agency developer portals you come across phrases like this, “As part of the <a href="https://www.whitehouse.gov/sites/default/files/omb/egov/digital-government/digital-government.html">Open Government Initiative</a>, the BusinessUSA codebase is available on the BusinessUSA GitHub Open Source Repository.” With the link to the Open Government Initiative leading to a a page on the White House website that has been removed–<a href="https://obamawhitehouse.archives.gov/open/documents/open-government-directive">something you can easily find on the Obama archives</a>. I am coming across numerous examples like this of how the change in leadership has created a vacuum when it comes to API and open data leadership, at a time when we should be doubling down on sharing of data, content, and putting algorithms to work across the federal government.</p>

<p>After several days immersed in federal government developer areas it is clear we have a hostile CEO that will require us to shift in our API strategy. After six months it is clear that the current leadership has no interest transparency, observability, or even the efficiency in government that is achieved from focusing opening up data via public, but secure APIs. This doesn’t mean the end of our open data and API efforts, it just means we lose the top down leadership we’ve enjoyed for the last eight years when it came to technology in government, and efforts will have to shift to a more bottom up approach, with agencies and departments often setting their own agenda.</p>

<p>This is nothing new, and it won’t be the last time we face this working with APIs across the federal government. Even during times where we have full support of leaders we should always be on the look out for threats, either technical, business, or political. Across once active API efforts I’m regularly finding broken links to previous leadership documents and resources at the executive level. We need to make sure that we shift these resources to a more federated approach in the future, where we reference central resources, but keep a cached copy locally to allow for any future loss of leadership. This is one reason we should be emphasizing the usage of Github across agencies, which offloads the storage and maintenance of materials to each individual agency, group, or even at the project level.</p>

<p>It is easy to find yourself frustrated in the current environment being cultivated by the leadership at the moment. However, with the right planning and communication we should be able to work around, and develop API implementations that are resilient to change, whether they are technical, budgetary, or on the leadership front as we are dealing with now. Don’t give up hope. If you need someone to talk with about your project please feel free to reach out publicly or privately. There are many folks still working hard on APIs inside and outside the federal government firewall, and they need our help. If you find yourself abandoning a project, please try to make sure as much of the work is available on your agencies Github repository, including code, definitions, and any documentation. This is the best way to ensure your work will continue to live on. Thank you for your service.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/26/we-have-a-hostile-ceo-which-requires-a-shift-in-our-api-strategy/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/25/a-lack-of-communication-around-federal-government-apis/">A Lack Of Communication Around Federal Government APIs</a></h3>
        <span class="post-date">25 Jul 2017</span>
        <p><a href="https://www.census.gov/data/developers/updates.html"><img src="https://s3.amazonaws.com/kinlane-productions/census/census-api-updates.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I personally understand the challenges with communicating publicly when you work for the federal government. It is one of the top reasons I do not work in federal government anymore. It would kill me if I couldn’t blog each day without friction–it is how I create and ideate. Even with this understanding I find myself regularly frustrated with the lack of communication by owners of APIs across federal government agencies. There are numerous agencies who do successfully communicate around their APIs and open data projects, but the majority of APIs I come across have little, or no communication around their API operations.</p>

<p>Have a blog, Twitter, or Github account might seem like a nice to have, but in reality they are often the only sign that anyone is home, and an API is reliable, and make the the difference between choosing to integrate with an API, or not. A blog or Twitter account, and whats new feature box on the home page of an API developer portal can send the winning (or losing) signal that an API is actually active and alive. Developers come across a lot of APIs that are dormant or abandoned, and the presence of common communication channels (blog, Twitter, Facebook, LinkedIn, Github) are the signal we often need before we are willing to invest the time into learning another new API, or signing up for yet another developer account.</p>

<p>I know that it is possible to handle API communications in a healthy way at government agencies–<a href="https://18f.gsa.gov/blog/">18F</a>, <a href="https://www.census.gov/data/developers/updates.html">Census</a>, and others are doing it right. There is some serious storytelling friction occurring in government. I see the same illness in corporate and other institutional API platforms–geeks and IT folks aren’t always the best at getting the word out about what they are doing. However, I think there is additional friction at the government level. We’ve seen a significant increase in blogging, and social network usage usage across government agencies, we need to investigate how we can incentivize federal government API operators to get a little more chatty with their work.</p>

<p>Communication around API operations is an essential building block. Federal government isn’t immune to this. If you aren’t telling the story about why developers should be using it, and actively communicating with your integrators, you will never find the success you seek when doing APIs at your agency. You don’t have to launch a wildly active and popular blog or social media presence. You just need something. A simple blog with RSS, hosted on your Github account. A Twitter account. Something. Please. Anything. Thank you!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/25/a-lack-of-communication-around-federal-government-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/25/api-management-across-all-government-agencies/">API Management Across All Government Agencies</a></h3>
        <span class="post-date">25 Jul 2017</span>
        <p><a href="https://api.data.gov/about/"><img src="https://s3.amazonaws.com/kinlane-productions/18f/9302707420_dbc7c2c437_o.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>This isn’t a new drum beat for me, but is one I wanted to pick it up again as part of the federal government research and speaking I’m doing this month. It is regarding the management of APIs across federal government. In short, helping agencies successfully secure, meter, analyze, and develop awareness of who is using government API resources. API management is a commodity in the private technology sector, and is something that has been gaining momentum in government circles, but we have a lot more work ahead to get things where we need them.</p>

<p>The folks over at 18F have done a great job of helping bake API management into government APIs using <a href="https://apiumbrella.io/">API Umbrella</a>, resulting in these twelve federal agencies:</p>
<ul>
    <li><a href="https://api.data.gov/docs/business-usa/">BusinessUSA.gov API</a></li>
    <li><a href="https://api.data.gov/docs/usda/">Department of Agriculture</a></li>
    <li><a href="https://api.data.gov/docs/commerce/">Department of Commerce</a></li>
    <li><a href="https://api.data.gov/docs/ed/">Department of Education</a></li>
    <li><a href="https://api.data.gov/docs/fcc/">Federal Communications Commission</a></li>
    <li><a href="https://api.data.gov/docs/fec/">Federal Election Commission</a></li>
    <li><a href="https://api.data.gov/docs/fda/">Food and Drug Administration</a></li>
    <li><a href="https://api.data.gov/docs/gsa/">General Services Administration</a></li>
    <li><a href="https://api.data.gov/docs/nasa/">National Aeronautics and Space Administration</a></li>
    <li><a href="https://api.data.gov/docs/nih/">National Institutes of Health</a></li>
    <li><a href="https://api.data.gov/docs/nrel/">National Renewable Energy Laboratory</a></li>
    <li><a href="https://api.data.gov/docs/regulations/">Regulations.gov API</a></li>
</ul>
<p>This doesn’t just mean that each of these agencies are managing their APIs. It also means that all of these agencies are managing their APIs in a consistent way, using a consistent tool. Something that is allowing these agencies to effectively manage:</p>
<ul>
  <li><a href="https://api.data.gov/docs/api-key">API Key Usage</a> - How to use your API key after signing up.</li>
  <li><a href="https://api.data.gov/docs/rate-limits">Web Service Rate Limits</a> - Daily and hourly rate limits on accessing api.data.gov APIs.</li>
  <li><a href="https://api.data.gov/docs/errors">General Web Service Errors</a> - General error codes that can be returned by any api.data.gov API.</li>
  <li><a href="https://api.data.gov/docs/https">HTTPS Usage</a> - Information about HTTPS usage on api.data.gov.</li>
</ul>

<p>I know that both 18F and USDS are working are hard on this, but this is an area we need agencies to step up in, as well as the private sector. We need any vendor doing API deployment projects for any agency to work together to make sure their agency is using a standardized approach. This means that vendors should make the investment when it comes to reaching out to the GSA, and 18F to <a href="https://api.data.gov/about/">make sure you are up to speed on what is needed to leverage the work already in motion at api.data.gov</a>.</p>

<p>Doing API management in a consistent way across ALL federal government APIs is super critical to all of this scaling as we all envision. The federal government possess a wealth of valuable data and content that can benefit the private sector. This isn’t just about making the federal government more transparent and observable, this is also about making these valuable resources available in a usable, sustainable way to the private sector–industries will be better off for it. I’m happy to see the progress these twelve agencies have made when it comes to API management, but we need to get to work helping every other agency play catch up, making it something that is baked into ALL API deployment projects by default.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/25/api-management-across-all-government-agencies/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/25/adding-vulnerability-disclosure-to-my-api-building-block-recommendations/">Adding Vulnerability Disclosure To My API Building Block Recommendations</a></h3>
        <span class="post-date">25 Jul 2017</span>
        <p><a href="https://18f.gsa.gov/vulnerability-disclosure-policy/"><img src="https://s3.amazonaws.com/kinlane-productions/18f/vulnerabilities-disclosure-policy.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I am working through the almost 100 federal government agency developer portals and the almost 500 APIs that exist across these agencies, looking for the good and bad of APIs in government at this level. One of interesting building blocks I’ve stumbled across, that I would like to shine a light on for other public and private sector API providers to consider in their own operations is <a href="https://18f.gsa.gov/vulnerability-disclosure-policy/">a vulnerability disclosure</a>.</p>

<p>I feel that 18F description of their vulnerability disclosure says it best:</p>

<blockquote>
  <p>As part of a U.S. government agency, the General Services Administration (GSA)’s Technology Transformation Service (TTS) takes seriously our responsibility to protect the public’s information, including financial and personal information, from unwarranted disclosure.</p>
</blockquote>

<blockquote>
  <p>We want security researchers to feel comfortable reporting vulnerabilities they’ve discovered, as set out in this policy, so that we can fix them and keep our information safe.</p>
</blockquote>

<blockquote>
  <p>This policy describes what systems and types of research are covered under this policy, how to send us vulnerability reports, and how long we ask security researchers to wait before publicly disclosing vulnerabilities.</p>
</blockquote>

<p>This should be default across all federal, state, county, and municipal government agencies. Hell, it should be default across all companies, organizations, and institutions. One of the reasons we have so much dysfunction in the security realm that elevates the discussion to theatrical levels with cybersecurity is that we aren’t having honest conversations about the vulnerabilities that exist. Few platforms want these conversations to occur, let alone set the tone of the conversation in such an open way. Without any guidance, and fear of retaliation, developers and analysts who find vulnerabilities will continue to hold back on what they find.</p>

<p>Vulnerability disclosure seems like something that ALL API provides should possess. There is no reason you can’t fork <a href="https://github.com/18F/vulnerability-disclosure-policy/blob/master/vulnerability-disclosure-policy.md#vulnerability-disclosure-policy">the GSA vulnerability policy</a> and share it as the official tone of the vulnerability disclosure conversation on your platform. Encouraging all API developers to understand what the tone of the conversation will look like when they stumble across a vulnerability while integrating with your API. I’m adding the concept of having a vulnerability disclosure to <a href="http://vulnerabilities.apievangelist.com/">my API vulnerability research</a>, and I am going to add GSA’s version as a tool in the API vulnerability toolbox, providing a template that other providers can put to work.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/25/adding-vulnerability-disclosure-to-my-api-building-block-recommendations/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/25/i-am-speaking-on-state-of-apis-in-federal-government-thursday-in-dc/">I Am Speaking On State Of APIs In Federal Government Thursday In DC</a></h3>
        <span class="post-date">25 Jul 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/steve_and_i_apistrat_2016.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://redhatapievents.com/dc">I am joining my friend Steve Willmott in DC this week to talk about federal government APIs</a>. We will  be gathering at Tysons’ Biergarten between 1:30 and 5:00 PM this Thursday to talk APIs. Both Steve and I will be speaking individually, with some QA, and a happy hour afterwards as an opportunity for more discussion.</p>

<p>I am looking forward for the opportunity to hanging with my friend Steve, as the last time we’ve hung out and spoke together was APIStrat in Boston, but at APIStat we are always running a conference, and not actually focused on our views of the APIs space. So, I am eager to learn more detail about what 3Scale is up to as part of the Red Hat machine, and specifically some of the containerization, microservices, and virtualization discussions they are leading lately.</p>

<p>Anyways, I will be in DC all day Thursday. Come join the conversation. I won’t have much time in DC, so the gathering will be the best opportunity to grab a moment of my time. I’ll be talking about the state of APIs in federal government, something I’m reminded during my research and preparation for my talk is probably the most important discussion we should be having in the API space right now.</p>

<p>Looking forward to seeing you all in DC. Thanks to <a href="redhat.com">Red Hat</a> for bringing me out to DC, and making this conversation possible. I’ll see you Thursday.</p>

<p><strong>Event Details:</strong>
<strong>Date:</strong> Thursday, July 27, 2017</p>

<p><strong>Time:</strong> 1:30 p.m. – 5:00 p.m.
<strong>Registration:</strong> 1:30 – 2:00 p.m.
<strong>Presentations:</strong> 2:00 – 3:30 p.m.
<strong>Happy Hour:</strong> 3:30 – 5:00 p.m.</p>

<p><strong>Location:</strong>
Tysons’ Biergarten
8346 Leesburg Pike
Tysons, VA 22182</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/25/i-am-speaking-on-state-of-apis-in-federal-government-thursday-in-dc/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/24/federal-government-apis-in-a-trump-administration/">Federal Government APIs In A Trump Administration</a></h3>
        <span class="post-date">24 Jul 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I haven’t written much about APIs in the federal government since the election. I’m still having conversations, and investing time into monitoring what is going on in the federal government, but honestly in the name of self-care I have to turn my head from what is going on with the current administration. It’s no secret that I’m not a Trump supporter, and honestly I have trouble not getting angry with Trump supporters when it comes to making the federal government more transparent and observable with data and APIs. The current tone the administration is taking when it comes to transparency, observability, and accountability will take us decades to recover from, making conversations about federal government APIs very difficult to have in many scenarios.</p>

<p>Luckily, I’m regularly reminded that there are MANY good people at government agencies who are doing amazing things, allowing me find more energy for thinking about APIs in federal government. I’ve been preparing for <a href="http://redhatapievents.com/dc">a talk I’m doing in DC this week with 3Scale by Red Hat</a>, which is priming the pump for a presentation I’m doing for the General Services Administration later in August. Both of these talks give me the chance to think about federal government, and invest some energy into finding the good that is going on in the federal government when it comes to APIs. It will also give me some time to take a look at what challenges exist when doing APIs at the federal level of government, with some acknowledgement of the current leadership in the White House.</p>

<p>First, I’m going to go agency by agency, taking a fresh look at anything API going on at the top level agencies, with a quick secondary look at some of the lesser known agencies. After this, I want to take a look at who is behind any API project that I’m coming across–understanding what I can about the internal groups doing APIs, any inter-agency efforts, including efforts out of <a href="https://18f.gsa.gov/">18F</a> and <a href="https://www.usds.gov/">USDS</a>. I’m looking to get a pulse for the API appetite that still exists at each agency, but also refresh the good work coming out of the forward leaning tech groups at GSA, and at the White House. From personal conversations I am having, as well as my regular monitoring of the space, I know there are still many good things still going on.</p>

<p>Second, I’m going to take another look at external forces when it comes to APIs in the federal government. I’m talking about API consumers, and companies or organizations who are building things with open data and APIs out of federal agencies. I’m also looking to better understand the vendor landscape when it comes to delivering API related projects. One of the biggest reasons APIs isn’t often seen as living up to it’s potential is because we aren’t very good at telling the stories about how the private sector is using public sector APIs, and there isn’t enough invested by federal agencies when it comes to getting the word out about their valuable resources. Many legacy rules and regulations about how the private sector and public sector can or cannot work together tends to make people in government nervous about being too vocal about this stuff–something that needs to change.</p>

<p>Third, I am sizing up the federal government in the context of <a href="http://apievangelist.com/api-lifecycle/">my API lifecycle research</a>. I am using this to drive <a href="http://redhatapievents.com/dc">my talk this Thursday in Washington DC</a>, and the one I’m doing in August with the GSA. I’m looking to start with <a href="http://definitions.apievangelist.com/">API definitions</a> and try to quantify what I’m seeing at the federal government level, then work through each stop along the API lifecycle until I get to <a href="http://deprecation.apievangelist.com/">deprecation</a>. I’m going to use this research to help quantify the “state of the union” when it comes to APIs in the federal government. I want to better understand how APIs are being designed, deployed, managed, testing, monitored, and the other critical aspects of API operations I’m tracking on in the wider API industry. As I am doing this work I will be sizing up how well the federal government is doing when it comes to each area, but also identify where they can improve, and evangelists like me might be able to reach out and help each agency.</p>

<p>Look out for more federal government API stories as I’m working through my research from the last week in this area. I need to pull together a “state of the union” presentation for Thursday, but I’m looking to refresh my advice for federal agencies regarding where they should be investing more resources, or maybe where the private sector can step in and help carry the load. <a href="http://apievangelist.com/2013/10/17/shutdown-of-government-open-data-and-apis-is-not-government-services-business-as-usual/">I’m feeling like many of my older thoughts about changing government from the outside-in are extremely relevant during a Trump administration</a>. I want to focus on the good work that is continuing across federal agencies, but I want to renew my thoughts on what the private sector should be doing as well. I feel pretty strongly that the load around operating critical federal government APIs should be shared between the public and private sector, with the percentage of the load teetering back and forth depending on the type of administration we have. We should acknowledge that some times the private sector should carry larger portion of the load, and other times the federal government should carry a larger portion of the load–with less friction as things teeter back and forth.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/24/federal-government-apis-in-a-trump-administration/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/24/the-hack-education-gates-foundation-grant-data-has-an-api/">The Hack Education Gates Foundation Grant Data Has An API</a></h3>
        <span class="post-date">24 Jul 2017</span>
        <p><a href="http://hackeducation.com/2017/07/18/personalization"><img src="https://s3.amazonaws.com/kinlane-productions/hack-education/hack-education-personalize-learning-and-the-power-of-the-gates-foundation-to-shape-education-policy.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I have been helping my partner in crime Audrey Watters (<a href="https://twitter.com/audreywatters">@audreywatters</a>) adopt my approach to managing data project(s) using Google Sheets and Github, as part of her work on ed-tech funding. She is going through many of the leading companies, and foundations behind the funding of technology used across the education sector, and doing the hard work of connecting the dots behind how technology gets funded in this critical layer of our society.</p>

<p>I want Audrey (and others), to be self-sufficient when it comes to managing their data projects, which is why I’ve engineered it to use common services (Google Sheets, Github), with any code and supporting elements as self-contained as possible–something Github excels at when it comes to managing data, content, and code. <a href="https://github.com/Hack-Education-Data">While Audrey is going to town creating spreadsheets and repos</a>, I wanted to highlight a single area of her research into the grants that the Gates Foundation are handing out. She has worked hard to normalize data across many years (1998-2017) of PDF and HTML data into a single Google Sheet, <a href="https://github.com/Hack-Education-Data/gates-foundation/tree/master/_data">then she has published as individual YAML files which live on Github</a>–making her work forkable and reusable by anyone.</p>

<p>Once published, Audrey is the first person to fork the YAML, and <a href="http://hackeducation.com/2017/07/18/personalization">put to work in her storytelling around ed-tech funding</a>, but each of her project repos also come with <a href="http://funding.hackeducation.com/gates-foundation.html">an API for her research by default</a>. Well, ok, it isn’t a full-blown searchable API, but in addition to being able to get data in YAML format, she has a JSON API for each year of the Gates Foundation grants (ie. <a href="https://hack-education-data.github.io/gates-foundation/apis/2016/">2016</a>). Increasing the surface area when it comes to collaborating and building on top of her work, which can be forked using Github, or accessed via the machine readable YAML and JSON files.</p>

<p>While she is busy creating new Google Sheets and repos for other companies, I wanted to add one more tool to her toolbox, an APIs.json index for her project APIs. <a href="https://github.com/Hack-Education-Data/gates-foundation/blob/master/_data/apis.yaml">I added an apis.yaml index of all her APIs</a>, which I also published to <a href="https://hack-education-data.github.io/gates-foundation/apis.json">the root of her project as an apis.json version</a>. Now, in addition to publishing YAML files for all the data driving her research, and enabling it all to have a JSON API, there is a single index available to quickly browse an index of machine readable feeds for all her ed-tech funding research. Did I mention, all of this on Google Sheet and Github, which both are free to use, if you leverage Github as a publicly available data project? Making it a pretty dead simple way for ANYONE to manage open data projects, and tell data-driven stories on a budget.</p>

<p>If you want to see the scope of what she is up to, head over to her <a href="https://github.com/Hack-Education-Data">Hack Education Data Github organization</a>. You can follow the storytelling side of all of this on her work at <a href="http://hackeducation.com">Hack Education</a>. What is scary about all of this, is that she is only getting started in this work. In August, we are moving to New York City where she is beginning <a href="http://spencerfellows.org/">her Spencer Fellowship in Education Reporting at Columbia</a>, where she will be focused 100% on this research. I’m looking forward to seeing what she does with <a href="https://contrafabulists-lessons.github.io/google-sheet-to-github-website/">this type of data management using Google Sheets and Github</a>, and working to support here where I can, but more importantly learning from how she takes the tools I’ve given her and evolve them to support her unique brand of data-driven storytelling in the education space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/24/the-hack-education-gates-foundation-grant-data-has-an-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/24/finding-things-i-want-to-write-about-when-apis-are-dumb/">Finding Things I Want To Write About When APIs Are Dumb</a></h3>
        <span class="post-date">24 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/training/gargoyle_light_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>You ever wake up some days, and find yourself not caring about APIs, or much else in the realm of technology? No? Well, I do. Regularly. I find myself in this headspace on this fine Monday morning, and without a weeks worth of stories scheduled, it is a very bad place to be as the API Evangelist. Part of this problem is me–I am a pain in my ass. However, a another portion of it is just about staying motivated, engaged, and producing compelling (ha) content on a regular basis for the blog, and other projects I’m working on.</p>

<p>There are almost a hundred stories in my notebook and all of them seem really, really dumb to me this morning. I can’t seem to muster up the energy to take any of them and turn into even a three paragraph API blah blah blah story. It’s just words right? I should be able to do it. I churn out meaningless API words all the time, non-stop for the last seven years! I should be able to do it today. What is wrong with you man? C’mon, you should be able to just turn it on, and the words will flow. Not today. Like many days before I am going to need to trick myself into turning on the faucet.</p>

<p>The best place to start (for me) when I have lost my writing mojo, is to find a project I truly care about 100%. This is why I work on <a href="http://org.open.referral.adopta.agency/">the human services API project</a>, and look for ways that I can help my partner in crime Audrey Watters (@audreywatters) with <a href="http://hackeducation.com">her Hack Education work</a>, as she is always focused on the most critical area we face when it comes to our use of technology–education. Understanding how technology is helping, or hurting us when it comes to educating every human on earth is serious business, and something that might just help pull me from my writing funk. Let’s give it a shot.</p>

<p><a href="http://hackeducation.com/2017/07/18/personalization">Audrey is working on some pretty interesting ed-tech funding research</a> which <a href="http://funding.hackeducation.com/gates-foundation.html">uses my Google Sheet to Github approach to publishing data</a>–this is way more interesting than the other commercial API blah blah blah in my notebook. I will write about that. I’m already feeling like I’m on my way towards finding some writing mojo with this warm up post. Crafting a whiney warmup fluff piece like this helps me identify the things I actually care about, and maybe after writing about an important topic, demonstrating the good that I see in APIs will help me churn out the rest of my work this week, and keep the API Evangelist gears a turning–I have too my writing ahead to not have any mojo. If you have ever wondered how it is I’m able to churn out so much content, hopefully this post provides a little insight into how the sausage is made.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/24/finding-things-i-want-to-write-about-when-apis-are-dumb/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/">First Handful Of Lessons Using My Google Sheet Github Approach</a></h3>
        <span class="post-date">24 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/lessons/google-sheet-to-github.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>With my recent shift to using Google Sheets as my data backend for my research, and my continued usage of Github as my data project publishing platform, I started pushing out some new API related lessons. I wanted to begin formalizing my schema and process for this new approach to delivering lessons with some simple topics, so I got to work taking my 101, and history of APIs work, and converting them into a multi-step lesson.</p>

<p>Some of my initial 101 API lessons are:</p>

<ul>
  <li><strong>API 101</strong> (<a href="http://101.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist/101">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRZ5VwkOard0nnwu8N_C-XMjAgOAElAMBrs7HHMKPtoGApmrau9yHPxVcNdiFfzLX6y7gKrPn12j6pr/pubhtml">Google Sheet</a>) - Just a general overview of what is API, targeting average user.</li>
  <li><strong>API Provider 101</strong> (<a href="http://101.consumer.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist/101-provider">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRZ5VwkOard0nnwu8N_C-XMjAgOAElAMBrs7HHMKPtoGApmrau9yHPxVcNdiFfzLX6y7gKrPn12j6pr/pubhtml">Google Sheet</a>) - Working to evolve an opening pitch to would be API providers.</li>
  <li><strong>API Consumer 101</strong> (<a href="http://101.provider.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist/101-consumer">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRZ5VwkOard0nnwu8N_C-XMjAgOAElAMBrs7HHMKPtoGApmrau9yHPxVcNdiFfzLX6y7gKrPn12j6pr/pubhtml">Google Sheet</a>) - Working to get better at providing information for API consumers.</li>
  <li><strong>The History of APIs</strong> (<a href="http://history.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist/history/tree/gh-pages">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRZ5VwkOard0nnwu8N_C-XMjAgOAElAMBrs7HHMKPtoGApmrau9yHPxVcNdiFfzLX6y7gKrPn12j6pr/pubhtml">Google Sheet</a>) - Continuing to expand on my history of APIs story.</li>
</ul>

<p>I will keep working those 101 lessons. Editing, polishing, expanding, and as I found out with this revision–removing some elements of APIs that are fading away. While my 101 stories are always working to reach as wide as possible, my wider research is always based in two sides of the API coin, with information about providing APis, while also keep my API consumer hat on, and thinking about the needs of developers and integrators.</p>

<p>Now that I have the 101 lessons under way I wanted to focus on my API life cycle research, and work on creating a set of high level lessons for each of <a href="http://apievangelist.com/api-lifecycle/">the 80+ stops I track on along a modern API life cycle</a>. So I got to work on the lesson for API definitions, which I think is the most important stop along any API life cycle–one that actually crosses with every other line.</p>

<ul>
  <li><strong>Definitions</strong> (<a href="http://definitions.lesson.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist-api-provider-lessons/definitions">Github Repo</a>) (https://docs.google.com/spreadsheets/d/13WXRAA30QMzKXRu-dH8gr-UrAQlLLDAD9pBAmwUPIS4/edit#gid=0)</li>
</ul>

<p>After kicking off a lesson for my API life cycle that speaks to API providers, I wanted to shift gears at look at things from the API consumer side of things, and kick off a lesson for what I consider to be one of the more important APIs today–Twitter.</p>

<ul>
  <li><strong>Twitter API</strong> (<a href="http://twitter.lesson.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist-api-consumer-lessons/twitter">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vTgmzyXYB3CdPdvSL8xy9Eyg7lJ-Z0zCjuyktpwHo4Pdj1x_Rod_sxCl2WCQ27aw5WrgcAk-T28hzXE/pubhtml">Google Sheet</a>)</li>
</ul>

<p>Like my life cycle research I will continue creating lessons for each area of my API Stack research, where I am studying the approaches of specific API platforms, and the industries they are serving. Next I will be doing Facebook, Instagram, Reddit, and other APIs that are having a significant impact on our world. I’m looking to create lessons for all the top APIs that have a big brand recognition, and leverage them to help onboard a new wave of API curious folks.</p>

<p>My API industry research all lives as separate data driven Github repositories, using Google Sheets as the central data store. I edit all the stories published across these sites using Prose.io, but the data behind all my research live in a series of spreadsheets. This model has been extended to my API lessons, and I’ll be shifting my storytelling to leverage more of a structured approach in the future.  To help onboard folks with the concept I’ve also created a lesson, about how you create data-driven projects like this:</p>

<ul>
  <li><strong>Google Sheets To Github Website</strong> (<a href="https://contrafabulists-lessons.github.io/google-sheet-to-github-website/">Website</a>) (<a href="https://github.com/contrafabulists-lessons/google-sheet-to-github-website/">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vSJoniCTFaaQuB7vB6mVkq6PMzQpQqxNomkKWcCpnZOsOwszGTbaiiLUP06wjsqDcSIueQgKsoVsyzT/pubhtml">Google Sheet</a>) - Walking through how you can use Google Sheets, and a Github Pages site to manage data driven websites.</li>
</ul>

<p>All of these lessons are works in progress. It is why they run on Github, so that I can incrementally evolve them. An essential part of this is getting feedback from folks on what they’d like to learn. I’m happy to open up and collaborate around any of these lessons using Google Sheets or Github–you just let me know which one is more your jam. I am collaborating with my partner in crime Audrey Watters (@audreywatters) using this format, and I’m finding it to be a great way to not just manage my world, but also create and manage new worlds with other people.</p>

<p>While each of the lessons use the same schema, structure, and process, I’m reserving the right to publish the lessons in different ways, experimenting with different variations in the layout. You’ll notice the Twitter and Google Sheets to Github Website lessons have a Github issues associated with each step, as I’m looking to stimulate conversations about what makes good (or bad) curriculum when it comes to learning about APIs and the platforms I’m building on. When it comes to my API lifecycle and stack work I am a little more opinionated and not looking for as much feedback at such a granular level, but because each lesson does living on Github, folks are still welcome to edit, and share their thoughts.</p>

<p>I have hundreds of lessons that I want to develop. The backlog is overwhelming. Now that I have the schema, base process, and first few stories published, I can just add to my daily workload and publish new stories, and evolve existing ones as I have time. If there are any lessons you’d like to see, either at the 101, provider, or consumer level let me know–feel free to hit me up through any channel. I’m going to be doing these lessons for my clients, either publishing them privately or publicly to Github repositories, and developing API life cycle curriculum in this way. I am also going to develop a paid version of the lesson, which will perform alongside my API industry guides, as simple, yet rich walk throughs of specific API industry concepts–for a small fee, to support what I do. Ok, lots of work ahead, but I’m super stoked to have these first few lessons out the door, even if there is a lot of polishing still to be done.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/21/structured-threat-information-expression-language/">Structured Threat Information Expression (STIX)</a></h3>
        <span class="post-date">21 Jul 2017</span>
        <p><a href="https://oasis-open.github.io/cti-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions/stix/stix-logo.png" align="right" width="35%" style="padding: 15px;" /></a></p>
<p><a href="http://apievangelist.com/2017/07/10/opportunity-to-develop-a-threat-intelligence-apis-json/">I wrote about the opportunity around developing an aggregate threat information API</a>, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of the <a href="https://oasis-open.github.io/cti-documentation/">Structured Threat Information Expression (STIX)</a>, as one possible schema for definining and sharing the information I’m talking about. Here is a quick summary of STIX is from the website:</p>

<blockquote>
  <p>Structured Threat Information Expression (STIX™) is a language for describing cyber threat information in a standardized and structured manner to enable the exchange of cyber threat intelligence (CTI). STIX characterizes an extensive set of CTI to include indicators of adversary activity, as well as contextual information characterizing cyber adversary motivations, capabilities, and activities and best courses of action for defense and mitigation.</p>
</blockquote>

<p>I haven’t dug into STIX too much, so I’m not making recomendations on the value it brings to the table yet, but I want to make sure we take a look at any existing work that was already on the, and make sure we aren’t reinventing the wheel with any part of an aggregated threat information API. At first glance STIX looks like a pretty damn good start for a potential API schema, that speaks a common language, and is seeing adoption with other existing threat information storage and sharing providers.</p>

<p>I am adding STIX to my research into threat information sharing, and wider <a href="http://security.apievangelist.com/">API security research</a>. <a href="https://apievangelist.com/2017/06/21/i-am-working-with-elastic-beam-to-help-define-api-security/">I am currently diving deeper into API security thanks to investment from Elastic Beam, and I will be publishing a guide, as well as an API security white paper</a> as part of the work. I’m going to try and provide some intelligence to a group of folks who expressed interest in developing an aggregate threat information sharing API. I’m hoping to better flesh out my thoughts on how API security and threat information sharing might feed into an overall API industry ranking scoring system that would help us understand which APIs are secure and observable enough to warrant usage, or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/21/structured-threat-information-expression-language/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/21/requiring-all-platform-partners-use-the-API-so-there-is-an-application-defined/">Requiring ALL Platform Partners Use The API So There Is A Registered Application</a></h3>
        <span class="post-date">21 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/google/google-apps-connected.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://kinlane.com/2017/07/16/opting-in-out-to-sharing-our-data-through-partnerships/">I wrote a story about Twitter allowing users to check or uncheck a box regarding sharing data with select Twitter partners</a>. While I am happy to see this move from Twitter, I feel the concept of information sharing being simply being a checkbox is unacceptable. I wanted to make sure I praised Twitter in my last post, but I’d like to expand upon what I’d like to see from Twitter, as well as ALL other platforms that I depend on in my personal and professional life.</p>

<p>There is no reason that EVERY platform we depend on couldn’t require ALL partners to use their API, resulting in every single application of our data be registered as an official OAuth application. The technology is out there, and there is no reason it can’t be the default mode for operations. There just hasn’t been the need amongst platform providers, as as no significant demand from platform users. Even if you don’t get full access to delete and adjust the details of the integration and partnership, I’d still like to see companies, share as many details as they possibly can regarding any partner sharing relationships that involve my data.</p>

<p>OAuth is not the answer to all of the problems on this front, but it is the best solution we have right now, and we need to have more talk about how we can make it is more intuitive, informative, and usable by the average end-users, as well as 3rd party developers, and platform operators. API plus OAuth is the lowest cost, widely adopted, standards based approach to establishing a pipeline for ALL data, content, and algorithms operate within that gives a platform the access and control they desire, while opening up access to 3rd party integrators and application developers, and most importantly, it gives a voice to end-users–we just need to continue discussing how we can keep amplifying this voice.</p>

<p>To the folks who will DM, email, and Tweet at me after this story. I know it’s unrealistic and the platforms will never do business like this, but it is a future we could work towards. I want EVERY online service that I depend on to have an API. I want all of them to provide OAuth infrastructure to govern identify and access management for personally identifiable information. I want ALL platform partners to be required to use a platforms API, and register an application for any user who they are accessing data on behalf. I want all internal platform projects to also be registered as an application in my OAuth management area. Crazy talk? Well, Google does it for (most of) their internal applications, why can’t others? Platform apps, partner apps, and 3rd party apps all side by side.</p>

<p>The fact that this post will be viewed as crazy talk by most who work in the technology space demonstrates the imbalance that exists. The technology exists for doing this. Doing this would improve privacy and security. The only reason we do not do it is because the platforms, their partners and ivnestors are too worried about being this observable across operations. There is no reason why APIs plus OAuth application can’t be universal across ALL platforms online, with ALL partners being required to access personally identifiable information through an API, with end-uses at least involved in the conversaiton, if not given full control over whether or not personally identifiable information is shared, or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/21/requiring-all-platform-partners-use-the-API-so-there-is-an-application-defined/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/21/misconceptions-about-what-openapi-is-still-slowing-conversations/">Misconceptions About What OpenAPI Is(nt) Still Slowing Conversations</a></h3>
        <span class="post-date">21 Jul 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/desert_dragon_light_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve been pushing forward conversations around <a href="http://org.open.referral.adopta.agency/">my Human Services Data API (HSDA) work</a> lately, and hitting some friction with folks around the finer technical details of the API. I feel the friction around these API conversations could be streamlined with OpenAPI, but with most folks completely unaware of what OpenAPI is and does, there is friction. Then for the handful of folks who do know what OpenAPI is and does, I’m seeing the common misconceptions about what they think it is slowing the conversation.</p>

<p>Let’s start with the folks who are unaware of what OpenAPI is. I am seeing two main ways that human services data vendors and implementations have conversations about what they need: 1) documentation, and 2) code. The last wave of HSDA feedback was very much about receiving a PDF or Word documentation about what is expected of an application and an API behind it. The next wave of conversations I’m having are very much here are some code implementations to demonstrate what someone is looking to articulate. Both very expensive waves of articulating and sharing what is needed for the future, or to develop a shared understanding. My goal throughout these conversations is to help folks understand that there are other more efficient, low costs ways to articulate and share what is needed–OpenAPI.</p>

<p>Beyond the folks who are not OpenAPI aware, the second group of folks who see OpenAPI as a documentation tool, or code generation tool. Interestingly enough a vantage point that is not very far evolved beyond the first group. Once you know what you have, you document it using OpenAPI, or you generate some code samples from it. Relinquishing OpenAPI to a very downstream tool, something you bring in after all the decisions are made. I had someone say to me, that OpenAPI is great, but we need a way to be having a conversation about each specific API request, the details of the that request, with a tangible response to that request–which I responded, “that is OpenAPI”. Further showing that I have a significant amount of OpenAPI education ahead of me, before we can efficiently use it within these conversations about moving the industry specification forward. ;-(</p>

<p>The reasons OpenAPI (fka Swagger) began as documentation, then code generation, then exploded as a mocking, and API design solution was the natural progression of things. I feel like this progression reflects how people are also learning about the API design life cycle, and in turn the OpenAPI specification itself. This is why the name change from Swagger to OpenAPI was so damaging in my opinion, as it is further confusing, and setting back these conversation for many folks. No use living in the past though! I am just going to continue doing the hard work of helping folks understand what OpenAPI is, and how it can help facilitate conversations about what an API is, what an API should do, and how it can be delivering value for humans–before any code is actually written, helping make sure everyone is on the same page before moving to far down the road.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/21/misconceptions-about-what-openapi-is-still-slowing-conversations/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/20/charles-proxy-generated-har-to-openapi-using-api-transformer/">Charles Proxy Generated HAR To OpenAPI Using API Transformer</a></h3>
        <span class="post-date">20 Jul 2017</span>
        <p><a href="https://twitter.com/jpmonette/status/885545611906428928"><img src="https://s3.amazonaws.com/kinlane-productions/charles-to-openapi/har-conversion.png" align="right" width="25%" style="padding: 15px;" /></a></p>
<p>I was responding to Jean-Philippe M. (@jpmonette) tweet regarding <a href="https://apievangelist.com/2015/06/21/parsing-charles-proxy-exports-to-generate-swagger-definitions-while-also-linking-them-to-each-path/">whether or not I had moved forward my auto generation of OpenAPIs from traffic captured by Charles Proxy</a>. It is one of many features of my internal systems I have not gotten around to finishing, but thankfully he actually answered his own question, and found a better solution than even I had–using my friends over at API Transformer.</p>

<p>I had been exploring ways for speeding up the process of generating OpenAPI specs for the APIs that I’m reviewing, something that becomes very tedious when working with large APIs, as well as just profiling the sheer number of APIs I am looking profile as part of my work. I haven’t been profiling many APIs lately, but the approach Jean-Philippe M. came up is petty damn easy, leaving me feeling pretty silly that I hadn’t connected the dots myself.</p>

<p>Here is what you do. Fire up Charles Proxy:</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/charles-to-openapi/charles-session.png" align="center" width="80%" style="padding: 15px;" /></p>

<p>Then open up Postman, and make any API calls. Of course you could also proxy mobile application or website API calls through your Charles Proxy, but Postman is a great way to for a majority of the APIs I depend on.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/charles-to-openapi/postman-apis-how.png" align="center" width="80%" style="padding: 15px;" /></p>

<p>After you’ve made the calls to all the APIs you are looking to generate an OpenAPI for, save your Charles Proxy session as a .har file, which is the last option on the dropdown menu available while saving. Then you head over <a href="https://apimatic.io/transformer">to API Transformer</a> and upload your .har file, and select OpenAPI (Swagger) 2.0 as the output–push convert.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/charles-to-openapi/api-transformer-convert.png" align="center" width="80%" style="padding: 15px;" /></p>

<p><a href="https://apimatic.io/transformer">API Transformer will then push a fresh OpenAPI to your desktop</a>, or allow you to publish via a portal, and generate an SDK using <a href="https://apimatic.io">APIMATIC</a>. Automated (mostly) generation of OpenAPI definitions from API traffic you generate through your browser, Postman, Restlet Client, mobile application, or other tooling.</p>

<p>I have abandoned my internal systems, except for my stack of APIs, and depending mostly on 3rd party services like Charles Proxy, Postman, and API Transformer. So I won’t be moving forward the custom solution I had developed. However, there still might be benefit of automatically saving .har files to my Dropbox sync folder, then using the Dropbox API, and API Transformer API to automate the conversation of .har files to OpenAPI, and write them back to the appropriate Dropbox folder.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/20/charles-proxy-generated-har-to-openapi-using-api-transformer/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/20/100k-view-of-bot-space-from-the-api-evangelist-perspective/">100K View Of Bot Space From The API Evangelist Perspective</a></h3>
        <span class="post-date">20 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bots-satellites.jpg" align="right" width="40%" style="padding: 15px" /></p>
<p>I had a friend ask me for my thoughts on bots. It is a space I tend to rant about frequently, but isn’t an area I’m moving forward <a href="http://bots.apievangelist.com/">any meaningful research</a> in, but it does seem to keep coming up and refuses to ever go way. I think bots are a great example of yet another thing that us technologists get all worked up about and think is the future, but in reality, while there will only be a handful of viable use cases, and bots will cause more harm, than they ever will do any good, or fully enjoy a satisfactory mainstream adoption.</p>

<p>First, bots aren’t new. Second, bots are just automation. Sure, there will be some useful automation implementations, but more often than not, bots will wreak havoc and cause unnecessary noise. Conveniently though, no matter what happens, there will be money to be made deploying and defending against each wave of bot investment. Making bots is pretty representative of how technology is approached in today’s online environment. Lot’s of tech. Lot’s of investment. Regular waves. Not a lot of good sense.</p>

<p><strong>Top Bot Platforms</strong><br />
Ok, where can you deploy and find bots today? These are the dominant platforms where I am seeing bots emerge:</p>

<ul>
  <li>Twitter - Building bots on the public social media platform using their API.</li>
  <li>Facebook - Building Facebook messenger bots to unleash on the Facebook Graph.</li>
  <li>Slack - Building more business and productivity focused bots on Slack.</li>
</ul>

<p>There are other platforms like Telegram, and <a href="http://bots.apievangelist.com/2017/07/05/a-bot-that-does-useful-things-for-me/">folks developing interesting Github bots</a>, but these three platforms dominate the conversation when it comes to bots in 2017. Each platform brings it’s own tone when it comes to what bots are capable of doing, and who is developing the bots. Another important thing to note across these platforms is that Slack is really the only one working to own the bot conversation on their platform, while on Facebook and Twitter allow the developer community to own the conversation about exactly what are bots.</p>

<p><strong>Conversational Interfaces</strong><br />
When it comes to bots, and automation, I’m always left thinking more broadly about other conversational interfaces and Siri, or more specifically Amazon Alexa. The Amazon Alexa platform operates on a similar level to Slack when it comes to providing developers with a framework, and tooling to define and deliver conversational interfaces. Voice just happens to be the interface for Amazon, where the chat and messaging window is the interface for Slack, as well as Twitter and Facebook. Alexa is a bot, consuming API resources alongside the other popular definitions of what is a bot on messaging and social channels–expanding the surface area for how bots are deployed and engaged with in 2017.</p>

<p><strong>Bots And APIs</strong><br />
To me, bots are just another client application for APIs. In early days APIs were about syndicating content on the web, then they were used to deliver resources to mobile applications, and now they are delivering content, data, and increasingly algorithms to devices, conversational interfaces, signage, automobiles, home appliances, and on and on. When any user asks a bot a question, the bot is the making one or many API calls to get the sports statistic, news and weather report, or maybe the purchase of a product. There will be many useful scenarios in which APIs will be able to deliver critical resources to conversational interfaces, but like many other client implementations, there will be many, many bad examples along the way.</p>

<p><strong>Algorithmic Shift</strong><br />
In 2017, the API space is shifting gears from primarily data and content based APIs, to a more algorithmic focus. Artificial intelligence, machine learning, deep learning, cognitive, and other algorithmically fueled interfaces are emerging, wrapped in APIs, intent on delivering “smart” resources to the web, mobile, and conversational interfaces. We will continue to see an overwhelming amount of discussion at the intersection of bots, API, and AI in coming years, with very little actual results delivered–regardless, there will be lots of money to be made by a few, along the way. Algorithms will play a central role in ensuring the “intelligence” behind bots stay a black box, and sufficiently pass as at least magic, if not entirely passed off as comparable to human intelligence.</p>

<p><strong>Where Will The Bot Money Be?</strong><br />
When it comes to making money with bots, there will only be a couple value creation centers. First, the platforms where bots operate will do well (most of them)–I am not sure they all will generate revenue directly from bots, but they will ensure bots are driving value that is in alignment platform revenue goals. Next, defensive bot solutions will generate sufficient amounts of revenue identifying and protecting businesses, institutions, and government agencies from the bot threat. Beyond that, venture capital folks will also do well investing in both the bot disruption, and bot defensive layers of the conversation–although VCs who aren’t directly involved with bot investment, will continue to be duped by fake users, customers, and other bot generated valuations. Leaving bot blemishes on their portfolios.</p>

<p><strong>Who Will Lose With Bots?</strong><br />
Ultimately it is the rest of us who will come out with on the losing side of these “conversations”. Our already very noisy worlds will get even noisier, with more bot chatter in the channels we currently depend on daily. The number of humans we engage with on a daily basis will decrease, and the number of frustrating “conversation” we find ourselves stuck in will increase. Everything fake will continue inflate, and find new ways to morph, duping many of us in new and exciting ways. Markets will be noisy, emotional, and always artificially inflated. Elections will continue be just an an outright bot assault on voters, leaving us exhausted, numb, and pretty moldable by those who have the biggest bot arsenals.</p>

<p><strong>Some Final Thoughts On Bots</strong><br />
I am continuing to see interesting bots emerge on Twitter, Facebook, Slack, and other channels I depend on like Github. I have no doubts that bots and conversational solutions will continue to grow, evolve, and result in a viable ecosystem of users, service providers, and investors. However, I predict it will be very difficult for bots to ever reach an acceptable mainstream status. As we’ve seen in every important conversation we are having online today, some of most badly behaved amongst us always seem to dominate any online conversation. Why is this? Bots. We will see this play out in almost every business sector.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/20/100k-view-of-bot-space-from-the-api-evangelist-perspective/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/20/managing-platform-terms-of-service-in-a-site-policy-repository/">Managing Platform Terms of Service In A Site Policy Repository</a></h3>
        <span class="post-date">20 Jul 2017</span>
        <p><a href="https://github.com/blog/2393-open-sourcing-our-site-policies-and-new-changes-to-our-terms-of-service"><img src="https://s3.amazonaws.com/kinlane-productions/github/github-site-policy.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="https://github.com/blog/2393-open-sourcing-our-site-policies-and-new-changes-to-our-terms-of-service">Github is releasing an update to their platform Terms of Service and Corporate Terms of Service</a>. Guess what platform their are using to manage the evolution, and release of their terms of service? Github of course! They are soliciting feedback, along with clarifications and improvements to their terms of service, with an emphasis on helping making things more readable! #nice</p>

<p>Github has provided a deadline for everyone to submit comments by the end of the month, then they’ll spend about a week going through the comments before making any changes. It provides a pretty useful way for any platform to manage their terms of service in a way that gives the community a voice, and provides some observability into the process for everyone else who might not feel confident enough to chime in on the process. This can go a long way towards building trust with the community, even if they don’t directly participate in the process.</p>

<p>Managing terms of service using Github makes sense for all providers, not just Github. It provides an open, transparent, and participatory way to move forward one of the most important documents that is governing API consumption. It is logical that the drafting, publishing, and evolution of platform terms be done out in the open, where the community can watch and participate. Pushing forward the design of the legal document in sync with the design, deployment, management, SDKs and other aspects of API operations. Bringing the legal side of things out of the shadows, and making it part of the conversation within the community.</p>

<p>Eventually, I’d like to see the terms of service, privacy policies, service level agreements, and other legal documents that govern API operations managed and available on Github like this. It gives the wider API community the chance to play a more significant role in hammering out the legal side of API operations, ensuring this are easier to follow and understand, and maybe even standardized across APIs. Who knows, maybe some day terms of service, privacy policies, and service level agreements will all be available in plain language, as well as machine readable YAML, shifting how the API contract will scale.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/20/managing-platform-terms-of-service-in-a-site-policy-repository/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/20/the-plivo-support-portal-knowledge-base/">The Plivo Support Portal And Knowledge Base</a></h3>
        <span class="post-date">20 Jul 2017</span>
        <p><a href="https://support.plivo.com"><img src="https://s3.amazonaws.com/kinlane-productions/plivo/plivo-support-knowledge-base.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m always watching out for how existing API providers are shifting up their support strategies in their communities as part of my work. This means staying into tune with their communications, which includes processing their email newsletters and developer updates. Staying aware of what is actually working, and what is not working, based upon active API service providers who are finding ways to make it all work.</p>

<p>Plivo opted out to phase out direct emails at the end of the month, and pushing developers to use <a href="https://support.plivo.com">the Plivo support portal</a>, and the ticketing system. The support portal provides a knowledge base which provides a base of self-service support before any developer actually uses the support ticketing system to:</p>

<ul>
  <li>Create, manage, respond to and check the status of your support ticket(s)</li>
  <li>Select improved ticket categories for more efficient ticket routing and faster resolution</li>
  <li>Receive resolution suggestions from our knowledge base before you submit a ticket to help decrease resolution time</li>
</ul>

<p>Email only support isn’t always the most optimal way of handling support, and using a ticketing system definitely provides a nice trail to follow for both sides of the conversations. The central ticketing system also provides a nice source of content to feed into the self-service support knowledge base, keeping self-service support in sync with direct support activity.</p>

<p>I’m going to continue to track on which API providers offer a ticketing solution, as well as a knowledge base. I’m feeling like these are what I’m going to recommend to new API providers as what I consider to be default support building blocks that EVERY API platform should be starting with, covering the self-service and direct support requirements of a platform. I’m going to start pushing 1-3 support solutions like ZenDesk, also giving API providers some options when it comes to quickly delivering adequate support for their platforms.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/20/the-plivo-support-portal-knowledge-base/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/19/more-investment-in-api-security/">More Investment In API Security</a></h3>
        <span class="post-date">19 Jul 2017</span>
        <p><a href="https://www.elasticbeam.com/"><img src="https://s3.amazonaws.com/kinlane-productions/elastic-beam/elasticbeam-security.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m getting some investment from <a href="https://www.elasticbeam.com/">ElasticBeam</a> to turn up the volume on my <a href="http://security.apievangelist.com/">API security research</a>, so I will be telling more stories on the subject, and publishing an industry guide, as well as a white paper in coming weeks. I want my API security to become a first class area of my API research, along side definitions, design, deployment, management, monitoring, testing, and performance.</p>

<p><a href="https://www.owasp.org/index.php/OWASP_API_Security_Project">Much of my API security research is built on top of OWASP’s hard work</a>, but honestly I haven’t gotten very far along in it. I’ve managed to curated a handful of companies who I’ve come across in my research, but haven’t had time to dive in deeper, or fully process all the news I’ve curated there. It takes time to stay in tune with what companies are up to, and I’m thankful for ElasticBeam’s investment to help me pay the bills while I’m heads down doing this work.</p>

<p>I am hoping that my API security research will also help encourage you to invest more into API security. As I do with my other partners, I will find ways of weaving ElasticBeam into the conversation, but my stories, guides, and white papers will be about the wider space–which Elastic Beam fits in. I’m hoping they’ll <a href="http://apis.how/8nlsropidv">compliment Runscope as my partner when it comes to monitoring, testing, and performance</a> (see how I did that, I worked Runscope in too), adding the security dimension to these critical layers of operating a reliable API.</p>

<p>One thing that attracted me to conversations with ElasticBeam was that they were developing a solution that could augment existing API management solutions like 3Scale and Amazon Web Services. I’ll have a talk with the team about integrating with <a href="http://apis.how/zflfesymzk">Tyk</a>, <a href="http://apis.how/bgdteovduo">DreamFactory</a>, and <a href="http://apis.how/5ytnitnakm">Restlet</a>–my other partners. Damn I’m good. I got them all in here! Seriously though, I’m thankful for these partners investing in what I do, and helping me tell more stories on the blog, and produce more guides and papers.</p>

<p>I feel like 3Scale has long represented what I’ve been doing over seven years–a focus on API management. Restlet, DreamFactory, and Tyk represent the maturing and evolution of this layer. While Runscope really reflects the awareness that has been generated at the API management layer, but evolving to serve not just API providers, but also API consumers. I feel like ElasticBeam reflects the next critical piece of the puzzle, moving the API security conversation beyond the authentication and rate limiting of API management, or limiting the known threats, and making it about identifying the unknown threats our API infrastructure faces today.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/19/more-investment-in-api-security/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/19/the-most-imortant-aspect-of-the-api-discussions-is-leaning-to-think-outside-our-boxes/">The Most Important Aspect Of The API Discussion Is Learning To Think Outside Our Boxes</a></h3>
        <span class="post-date">19 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-box.png" width="25%" align="right" style="padding: 10px;" /></p>
<p>There are many good things to come out of doing APIs properly. Unfortunately there are also many bad things that can come out of doing APIs badly, or with misaligned expectations. It is easy to focus on the direct benefits of doing APIs like making data resources available to partners, or maybe developing a mobile application. I prefer looking for the more indirect benefits, which are more human, more than they are ever technical.</p>

<p>As I work with different groups on a variety of API definitions and strategies, one very significant part of the process I see, is people being forced to think outside their box. APIs are all about engaging around data, content, and algorithms on the web, with 3rd parties that operate outside your box. You are forced to lookup, and outward a bit. Not everyone I engage with is fully equipped to do this, for a variety of reasons, but overall the API process does make folks just a little more critical than they do with even their websites.</p>

<p>The web has come with a number of affordances. Those same affordances aren’t always present in API discussions forcing folks to have more conversations around why we are doing APIs (an answer shouldn’t always be yes), and discussing the finer details not just storing your data, and managing your schema, but doing in a way that will play nicely with other external systems. You may be doing things one way internally, and it might even be working for you, but it is something that can only get better with each outside partner, or consumer you are exposed to along your journey. Even with all of the internal politics I encounter in my API conversations, the API process always leaves me enjoying almost any outcome.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/19/the-most-imortant-aspect-of-the-api-discussions-is-leaning-to-think-outside-our-boxes/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/19/does-your-platform-have-an-integrations-page/">Does Your Platform Have An Integrations Page?</a></h3>
        <span class="post-date">19 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/airtable/airtable-integrations-page.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m continuing to come across more dedicated integration pages for the API platforms I’m test driving, and keeping an eye on. <a href="https://airtable.com/integrations">This time it is out of spreadsheet and database hybrid AirTable, that allows you to easily deploy an API complete with a portal, with a pretty robust integrations page for their platform</a>. Airtable’s dedicated integrations page is made easier since they use Zapier, which helps them aggregate over 750+ APIs for possible integration.</p>

<p>Airtable is pretty slick all by itself, but once you start wiring it up to some of the other API driven platforms we depend on, <a href="https://apievangelist.com/2017/07/11/each-airtable-datastore-comes-with-complete-api-and-developer-portal/">it becomes a pretty powerful tool for data aggregation, and then publishing as an API</a>. I don’t understand why a Zapier-driven API integrations page isn’t default for every API platform out there. API consumption today isn’t just about deploying web or mobile applications, it is about moving data and content around the web–making sure it is where we need it, when we need it.</p>

<p>I’m playing with different variations of the API integrations page lately. <a href="https://apievangelist.com/2017/07/12/a-zapier-advocate-and-dedicated-api-resources-page-for-your-company/">I’m exploring the idea of how I can encourage some higher education folks I know, and government open data folks I know to be Zapier advocates within their organizations, and publish a static integrations page, showing the integrations solutions available around the platforms they depend on</a>. Dedicated integration pages help API developers understand the potential of any API, and they help non-developers also understand the potential, but in a way they can easily put into action to solve problems in their world. I’m going to keep beating the API integration page drum, and <a href="https://apievangelist.com/2017/04/26/zapier-was-pretty-savvy-in-their-approach-to-launching-their-partner-api/">now that Zapier has their partner API</a> you will also hear me talking about Zapier a lot more.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/19/does-your-platform-have-an-integrations-page/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/19/containerized-microservices-monitoring-driving-api-infrastructure-visualizations/">Containerized Microservices Monitoring Driving API Infrastructure Visualizations</a></h3>
        <span class="post-date">19 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/netsil/1-P8w_-2-oCz0QfV6OENawJQ.png" align="right" width="40%" style="padding: 15px" /></p>
<p>While I track on what is going on with visualizations generated from data, I haven’t seen much when it comes to API driven visualizations, or specifically visualization about API infrastructure, that is new and interesting. <a href="https://blog.netsil.com/kubernetes-monitoring-needs-maps-6ef673d840c7">This week I came across an interesting example in a post from Netsil about mapping microservices so that you can monitor them</a>. They are a pretty basic visualization of each database, API, and DNS element for your stack, but it does provide solid example of visualizing not just the deployment of database and API resources, but also DNS, and other protocols in your stack.</p>

<p>Netsil microservices visualization is focused on monitoring, but I can see this type of visualization also being applied to design, deployment, management, logging, testing, and any other stop along the API lifecycle. I can see API lifecycle visualization tooling like this becoming more common place, and play more of a role in making API infrastructure more observable. Visualizations are an important of the storytelling around API operations that moves things from just IT and dev team monitoring, making it more observable by all stakeholders.</p>

<p>I’m glad to see service providers moving the needle with helping visualize API infrastructure. I’d like to see more embeddable solutions deployed to Github emerge as part of API life cycle monitoring. I’d like to see what full life cycle solutions are possible when it comes to my partners like deployment visualizations from <a href="http://apis.how/zflfesymzk">Tyk</a> and <a href="http://apis.how/bgdteovduo">Dreamfactory APIs</a>, and <a href="https://s3.amazonaws.com/kinlane-productions/partners/3scale-red-hat-logo.png">management visualizations with 3Scale APIs</a>, and monitoring and testing visualizations using <a href="http://apis.how/8nlsropidv">Runscope</a>. I’ll play around with pulling data from these provides, and publishing to Github as YAML, which I can then easily make available as JSON or CSV for use in some basic visualizations.</p>

<p>If you think about it, thee really should be a wealth of open source dashboard visualizations that could be embedded on any public or private Github repository, for every API service provider out there. API providers should be able to easily map out their API infrastructure, using any of the API service providers they are using already using to operate their APIs. Think of some of the embeddable API status pages we see out there already, and what Netsil is offering for mapping out infrastructure, but something for ever stop along the API life cycle, helping deliver visualizations of API infrastructure no matter which stop you find yourself at.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/19/containerized-microservices-monitoring-driving-api-infrastructure-visualizations/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/18/one-api-development-partner-every-api-provider-should-have/">One API Development Partner Every API Provider Should Have</a></h3>
        <span class="post-date">18 Jul 2017</span>
        <p><a href="https://zapier.com/engineering/zapier-issues/"><img src="https://s3.amazonaws.com/kinlane-productions/zapier/4b21d50900beffcc0bcfa2c09bcc7bfe.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>Yet another reason to be making sure Zapier is part of your API operations–issue management. Zapier is now providing an important window into how people are integrating with your API(s)–now any public API connected to Zapier can see filtered, categorized feedback from their users with <a href="https://zapier.com/engineering/zapier-issues/">Zapier Issues</a>, and use that information to improve upon their APIs and integrations. This is the biggest movement I’ve seen in <a href="http://issues.apievangelist.com/">my API issues research</a> since I first started doing it on April of 2016.</p>

<p><a href="https://zapier.com/engineering/zapier-issues/">Zapier Issues</a> doesn’t just provide you with a look at the issues that arise within API integrations (the bad news), it also provides you with a feedback look where you can engage with Zapier users who have integrated with your API, and hear feature requests (the good news), and other road map influencing suggestions. Zapier sees, “thousands of app combinations and complex workflows from more than 1.5 million people—and we want to give you more insight into how your best customers use your app on Zapier.”</p>

<p>It is another pretty big reason that ALL API providers should be baking Zapier into their platforms. Not only will you be opening up API consumption to the average business user, you can now get feedback from them, and leverage the wisdom Zapier has acquired integrating with over 750 APIs. As an API provider you should be jumping at this opportunity to get this type of feedback on your API resources. Helping you make sure your APIs more usable, stable, reliable, and providing the solutions that actual business users are needing to solve the problems they encounter in their daily lives.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/18/one-api-development-partner-every-api-provider-should-have/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/18/specialized-collections-of-machine-learning-apis-could-be-interesting/">Specialized Collections Of Machine Learning APIs Could Be Interesting</a></h3>
        <span class="post-date">18 Jul 2017</span>
        <p><a href="https://algorithmia.com/enterprise"><img src="https://s3.amazonaws.com/kinlane-productions/algorithmia/algorithmia-enterprise.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I was learning more <a href="https://algorithmia.com/enterprise">about CODEX, from Algorithmia, their enterprise platform for deploying machine learning API collections on premise or in the cloud</a>. Algorithmia is taking the platform in which their algorithmic marketplace is deployed on and making it so you can deploy it anywhere. I feel like this is where the algorithmic-centered API deployment is heading, potentially creating some very interesting, and hopefully specialized collections of machine learning APIs.</p>

<p>I talked about how the economics of what Algorithmia is doing interests me. I see the potential when it comes to supporting machine learning APIs that service an image or video processing pipeline–something I’ve enjoyed thinking about with my drone prototype. Drone is just one example of how specialized collections of machine learning APIs could become pretty valuable when they are deployed exactly where they are needed, either on-premise or in any of the top cloud platforms.</p>

<p>Machine learning marketplaces operated by the cloud giants will ultimately do fine because of their scale, but I think where the best action will be at is delivering curated, specialized machine learning models, tailored to exactly what people need, right where they need them–no searching necessary. I think recent moves by Google to put TensorFlow on mobile phones, and Apple making similar moves show signs of a future where our machine learning APIs are portable, operating on-premise, on-device, and on-network.</p>

<p>I see Algorithmia having two significant advantages right now. 1) they can deploy their marketplace anywhere, and 2) they have the economics, as well as the scaling of it figured out. Allowing for specialized collections of machine learning APIs to have the metering, and revenue generation engines built into them. Imagine a future where you can deploy and machine learning and algorithmic API stack within any company or institution, or the factory floor in an industrial setting, and out in the field in an agricultural or mining situation–processing environmental data, images, or video.</p>

<p>Exploring the possibilities with real world use cases of machine learning is something I enjoy doing. I’m thinking I will expand on my drone prototype and brainstorm other interesting use cases beyond just my drone video. Thinking about how I can develop prototype machine learning API collections, that could be used for a variety my content, data, image, or video side-projects. I think when it comes to machine learning I’m more interested in specialty collections over the general machine learning hype I”m seeing peddled in the mainstream right now.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/18/specialized-collections-of-machine-learning-apis-could-be-interesting/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/18/diagramming-the-components-of-api-observability/">Diagramming The Components Of API Observability</a></h3>
        <span class="post-date">18 Jul 2017</span>
        <p><a href="https://apievangelist.com/2014/03/17/politics-of-apis/">I created a diagram of the politics of APIs sometime ago that has really held true for me</a>, and is something I’ve continue to reference as part of my storytelling. I wanted to do a similar thing to help me evolve my notion of <a href="https://apievangelist.com/2016/10/25/thinking-about-an-api-observability-stack/">API observability</a>. Like the politics of APIs, observability overlaps many areas of <a href="http://apievangelist.com/api-lifecycle/">my API life cycle research</a>. Also like the politics of APIs, observability involves many technical, business, and legal aspects of operating a platform online today.</p>

<p>Here is my first draft of a Venn diagram beginning to articulate what I see as the components of API observability:</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/observable/api-observability-venn.png" width="75%" style="padding: 15px;" align="center" /></p>

<p>The majority of the API observability conversation in the API space currently centers around logging, monitoring, and performance–driven by internal motivations, but done in a way that is very public. I’m looking to push forward the notion of API observability to transcend the technical, and address the other operational, industry, and even regulatory concerns that will help bring observability to everyone’s attention.</p>

<p>I do not think we should always be doing API, AI, ML and the other tech buzzwords out there if we do not have to–saying no to technology can be done. In the other cases where the answer is yes, we should be doing API, AI, and ML in an observable way. This is my core philosophy. The data, content, algorithms, and networks we are exposing using APIs, and using across web, mobile, device, and network applications should be observable by internal groups, as well as partners, and public stakeholders as it makes sense. There will be industry, community, and regulatory benefits for sectors that see observability as a positive thing, and go beyond just the technical side of observability, and work to be more observable in all the areas I’ve highlight above.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/18/diagramming-the-components-of-api-observability/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/18/http-status-codes-are-an-essential-part-of-api-design-and-deployment/">HTTP Status Codes Are An Essential Part Of API Design And Deployment</a></h3>
        <span class="post-date">18 Jul 2017</span>
        <p><a href="https://www.runscope.com/"><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/runscope/runscope-200-ok.jpeg" alt="" align="right" width="40%" /></a>
It takes a lot of work provide a reliable API that people can depend on. Something your consumers can trust, and will provide them with consistent, stable, meaningful, and expected behavior. There are a lot of affordances built into the web, allowing us humans to get around, and make sense of the ocean of information on the web today. These affordances aren’t always present with APIs, and we need to communicate with our consumers through the design of our API at every turn.</p>

<p>One area I see IT and developer groups often overlook when it comes to API design and deployment are <a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes">HTTP Status Codes</a>. That standardized list of meaningful responses that come back with every web and API request:</p>

<ul>
  <li><strong>1xx Informational</strong> - An informational response indicates that the request was received and understood. It is issued on a provisional basis while request processing continues.</li>
  <li><strong>2xx Success</strong> - This class of status codes indicates the action requested by the client was received, understood, accepted, and processed successfully.</li>
  <li><strong>3xx Redirection</strong> - This class of status code indicates the client must take additional action to complete the request. Many of these status codes are used in URL redirection.</li>
  <li><strong>4xx Client errors</strong> - This class of status code is intended for situations in which the client seems to have errored.</li>
  <li><strong>5xx Server error</strong> - The server failed to fulfill an apparently valid request.</li>
</ul>

<p>Without HTTP Status codes, application won’t every really know if their API request was successful or not, and even if an application can tell there was a failure, it will never understand why. HTTP Status Codes are fundamental to the web working with browsers, and apis working with applications. HTTP Status Codes should never be left on the API development workbench, and API providers should always go beyond just 200 and 500 for every API implementation. Without them, NO API platform will ever scale, and support any number of external integrations and applications.</p>

<p>The most important example I have of the importance of HTTP Status Codes I have in my API developers toolbox is when <a href="http://apievangelist.com/2012/06/02/tracking-federal-agencies-progress-on-api-deployment/">I was working to assist federal government agencies in becoming compliant with the White House’s order for all federal agencies to publish a machine readable index of their public data inventory of their agency website</a>. As agencies got to work publishing JSON and XML (an API) of their data inventory, I got to work building an application that would monitor their progress, indexing the available inventory, and providing a dashboard the the GSA and OMB could use to follow their progress (or lack of).</p>

<p>I would monitor the dashboard in real time, but weekly I would also go through many of the top level cabinet agencies, and some of the more prominent sub agencies, and see if there was a page available in my browser. There were numerous agencies who I found had published their machine readable public data inventory, but had returned a variety of HTTP status codes other than 200-resulting in my monitoring application to consider the agency not compliant. <a href="http://kinlane.com/2013/11/06/knowing-your-http-status-codes-in-federal-government/">I wrote several stories about HTTP Status Codes</a>, in which the GSA, and White House groups circulated with agencies, but ultimately I’d say this stumbling block was one of the main reasons that cause this federated public data API project to stumble early on, and never gain proper momentum–a HUGE loss to an open and more observable federal government. ;-(</p>

<p>HTTP Status Codes aren’t just a nice to have thing when it comes to APIs, they are essential. Without HTTP  Status Codes each application will deliver unreliable results, and aggregate or federated solutions that are looking to consume many APIs will become much more difficult and costly to develop. Make sure you prioritize HTTP Status Codes as part of your API design and deployment process. At the very least make sure all five layers of HTTP Status Codes are present in your release. You can always get more precise and meaningful with specific series HTTP status codes later on, but ALL APIs should be employing all five layers of HTTP Status Codes by default, to prevent friction and instability in every application that builds on top of your APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/18/http-status-codes-are-an-essential-part-of-api-design-and-deployment/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/17/writing-api-stories-that-speak-to-my-audience-but-also-influences-their-view-of-technology/">Writing API Stories That Speak To But Also Influences Their View Of Technology</a></h3>
        <span class="post-date">17 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-storytelling.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I know that some of my friends who follow API Evangelist shake their heads when I talk about API business models, partner programs, and many of the business sides of API operations. Much of my work will have an almost delusional attraction towards the concept of an API. Heavily doused in a belief in technology as a solution. This isn’t accidental. This is API Evangelist. A persona I have developed to help me make a living, and help influence where we go (or don’t go) with technology.</p>

<p>I am delusional enough to think I can influence change in how the world uses technology. I’m borderline megalomaniac, but there really is not sufficient ego to get me quite all the way there. While still very, very, very minor, I feel I have influenced where technology has flowed over my seven years as the API Evangelist. Even if it just slowing the speed (seconds) at which the machines turn on us, and kills us all. If nothing else, I know there are few folks out there who I have touched, and shaped how they see, use, and allow technology in their lives (cause they told me so).</p>

<p>Through my storytelling on API Evangelist, I am always looking for the next convert–even if it takes years and hundreds of stories. A significant portion of this outreach involves telling stories that reach my intended audience–usually startups, business, institutional, and government agency workers and influencers. To reach them I need to tell stories that speak to them, and feed their current goals around finding success in their startup, or their role within businesses, institutions, and government agencies. With this in mind, I am always trying to bend my stories in their direction, talking about topics that they’ll care about, and tune into.</p>

<p>Once I have their attention, I will work on them in other ways. I’ll help them think about their business model, but also help them understand transparency and communication when it comes to executing this model. I will help them understand the best practices for managing an API using open source solutions like Tyk or Dreamfactory, and the leading approaches to using Runscope for monitoring and testing, while also encouraging them to me more <a href="https://apievangelist.com/2016/10/25/thinking-about-an-api-observability-stack/">observable</a> with these practices. Making sure companies tell stories about what they are doing, and how they are doing it all–the good and bad.</p>

<p>I’m always working to build bridges to folks who might not see this whole API thing like I do. I’d  say that many of these bridges will never get fully walked across by my target audience, but when someone does, and my stories influence the way they see or use technology even a little bit–mission accomplished. I’m constantly testing new ways or reaching out, speaking in the language of my target audience (without selling out), using trendy terms like microservices, devops, and serverless, but this isn’t just about following the latest fad. It is meant to capture your attention, build some trust, and then when it matters I can share some information about what really matters in all of this–in hopes of influencing how you see technology, and how it can be used a little more sensibly, securely, or maybe not even at all. ;-)</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/17/writing-api-stories-that-speak-to-my-audience-but-also-influences-their-view-of-technology/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/17/bot-observability-for-every-platform/">Bot Observability For Every Platform</a></h3>
        <span class="post-date">17 Jul 2017</span>
        <p><a href="http://bots.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-bot-showcase.png" align="right" width="45%" style="padding: 15px" /></a></p>
<p><a href="http://bots.apievangelist.com/">I lightly keep an eye on the world of bots, as APIs are used to create them</a>. In my work I see a lot of noise about bots usually in two main camps: 1) pro-bot - bots are the future, and 2) anti-bot - they are one of the biggest threats we face on the web. This is a magical marketing creating formula, which allows you to sell products to both sides of the equation, making money off of bot creation, as well as bot identification and defense–it is beautiful (if you live by disruption).</p>

<p>From my vantage point, I’m wondering why platforms do not provide more bot observability as a part of platform operations. There shouldn’t be services that tell us which accounts are bots, the platform should tell us by default, which users are real and which are automated (you know you know). Platforms should embrace automation and providing services and tooling to assist in their operation, which includes actual definitions of what is acceptable, and what unacceptable bot behavior. Then actually policing this behavior, and being observable in your actions around bot management and enforcement.</p>

<p>It feels like this is just another layer of technology that is being bastardized by the money that flow around technology so easily. Investment in lots of silly useless bots. Investment in bot armies that inflate customer numbers, advertising, and other ways of generating attention (to get investment), and generate revenue. It feels like Slack is the only leading bot platform that has fully embraced the bot conversation. Facebook and Twitter lightly reference the possibilities, and have made slight motions when it comes to managing the realities of bots, but when you Google “Twitter Bots” or “Facebook Bots”, neither of them dominate the conversation around what is happening–which very telling around how they view the world of bots.</p>

<p>Slack has <a href="https://openreferral.slack.com/apps/category/At0MQP5BEF-bots">a formal bots directory</a>, and has <a href="https://api.slack.com/bot-users">defined the notion of a bot user</a>, separating them from users–setting an example for bot developers to disclose who is bot, and who is not. They talk about <a href="https://medium.com/slack-developer-blog/hard-questions-about-bot-ethics-4f80797e34f0">bot ethics</a>, and <a href="https://medium.com/slack-developer-blog/the-bot-rulebook-a442d9fb21cb">rules for building bots</a>, and <a href="https://slackhq.com/a-beginner-s-guide-to-your-first-bot-97e5b0b7843d">do a lot of storytelling about their vision for bots</a>. Providing a pretty strong start towards getting a handle on the explosion of bots on their platform–taking the bull by the horns, owning the conversation, and setting the tone.</p>

<p>I’d say that Slack has a clearer business model for bots–not that people are actually going to pay for your bot (they aren’t), but a model is present. You can some smell of revenue strategies on Facebook, but it just feels like all roads lead to Facebook, and advertising partners there. I’d say Twitter has no notion of a botlike business model for developers. This doesn’t mean that Facebook and Twitter bots don’t generate revenue for folks targeting Facebook and Twitter, or play a role in influencing how money flows when it comes to eyeballs and clicks. Indirectly, Twitter and Facebook bots are making folks lots of money, it is just that platforms have chosen not to observable when it comes their bot practices and ecosystems.</p>

<p>Platform observability makes sense for not just platform, and bot developers, as Slack demonstrates it makes sense for end-users. Incentivizing bots generating value, instead of mayhem. I’m guessing advertising-driven Facebook and Twitter have embraced the value of mayhem–with advertising being the framework for generating their revenue. Slack has more of a product, with customers they want to make happy. With Facebook and Twitter the end-users are the product, so the bot game plays to a different tune.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/17/bot-observability-for-every-platform/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/17/making-all-sub-resources-available-within-the-core-set-of-human-service-apis/">Making All Sub-Resources Available Within The Core Set Of Human Service APIs</a></h3>
        <span class="post-date">17 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/hsds-organization.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I had recently taken <a href="http://openreferral.readthedocs.io/en/latest/reference/">the Human Services Data Specification (HSDS)</a> and exposed it as a set of API paths that provide access to about 95% of the schema, which we are calling the <a href="https://openreferral.github.io/api-specification/definition/">Human Services Data API (HSDA)</a>. When you make a call to the /organizations/ path, you receive an array collection of organizations that each match the HSDA organization schema. The same applies when you make a call to the /locations, /contacts, and /services, opening up access to the entire schema–minus three objects I pushed off until future releases.</p>

<p>After the core set of API paths /organization, /service, /location, /contact, there are a set of sub-resources available across those as it makes sense–including /phone, /programs, /physical_address, /postal_address, /regular_schedule, /holiday_schedule, /funding, /eligibility, /service_area, /required_document, /payment_accepted, /language, /accessiblity_for_disabilities, and /service_at_location_id. I took the HSDA schema, and published API paths for each sub-resource so that it exactly returned, and accepted HSDA compliant schema–making all aspects of the schema accessible via an API, with POST and PUT requests accepting compliant schema, and GET returning compliant schema.</p>

<p>One of the “stoppers” we received from several folks in the HSDS community during the feedback cycle going from version 1.0 of the API to version 1.1, was that the design was overly complex, and that it would serve any of the human services use cases on the table currently, unless you could get at all the sub resources directly with each core API path, eliminating the need for making additional call(s) to each sub-resource. You could get at everything about an /organization, /service, /location, and /contact in a single API URL.</p>

<p>Currently the core four API paths accept and return the following schema:</p>

<p><strong>Organization</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string (uuid)</td>
<td>Each organization must have a unique identifier.</td>
<td>True</td>
<td>True</td>
</tr>
<tr class="row-odd"><td>name</td>
<td>string</td>
<td>The official or public name of the organization.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>alternate_name</td>
<td>string</td>
<td>Alternative or commonly used name for the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>description</td>
<td>string</td>
<td>A brief summary about the organization. It can contain markup such as HTML or Markdown.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>email</td>
<td>string (email)</td>
<td>The contact e-mail address for the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>url</td>
<td>string (url)</td>
<td>The URL (website address) of the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>tax_status</td>
<td>string</td>
<td>Government assigned tax designation for for tax-exempt organizations.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>tax_id</td>
<td>string</td>
<td>A government issued identifier used for the purpose of tax administration.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>year_incorporated</td>
<td>date (%Y)</td>
<td>The year in which the organization was legally formed.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>legal_status</td>
<td>string</td>
<td>The legal status defines the conditions that an organization is operating under; e.g. non-profit, private corporation or a government organization.</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p><strong>Service</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each service must have a unique identifier.</td>
<td>True</td>
<td>True</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>The identifier of the organization that provides this service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>program_id</td>
<td>string</td>
<td>The identifier of the program this service is delivered under.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>name</td>
<td>string</td>
<td>The official or public name of the service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>alternate_name</td>
<td>string</td>
<td>Alternative or commonly used name for a service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>description</td>
<td>string</td>
<td>A description of the service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>url</td>
<td>string (url)</td>
<td>URL of the service</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>email</td>
<td>string (email)</td>
<td>Email address for the service</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>status</td>
<td>string</td>
<td>The current status of the service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>interpretation_services</td>
<td>string</td>
<td>A description of any interpretation services available for accessing this service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>application_process</td>
<td>string</td>
<td>The steps needed to access the service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>wait_time</td>
<td>string</td>
<td>Time a client may expect to wait before receiving a service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>fees</td>
<td>string</td>
<td>Details of any charges for service users to access this service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>accreditations</td>
<td>string</td>
<td>Details of any accreditations. Accreditation is the formal evaluation of an organization or program against best practice standards set by an accrediting organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>licenses</td>
<td>string</td>
<td>An organization may have a license issued by a government entity to operate legally. A list of any such licenses can be provided here.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>taxonomy_ids</td>
<td>string</td>
<td>(Deprecated) A comma separated list of identifiers from the taxonomy table. This field is deprecated in favour of using the service_taxonomy table.</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p><strong>Location</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each location must have a unique identifier</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>Each location entry should be linked to a single organization. This is the organization that is responsible for maintaining information about this location. The identifier of the organization should be given here. Details of the services the organisation delivers at this location should be provided in the services_at_location table.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>name</td>
<td>string</td>
<td>The name of the location</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>alternate_name</td>
<td>string</td>
<td>An alternative name for the location</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>description</td>
<td>string</td>
<td>A description of this location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>transportation</td>
<td>string</td>
<td>A description of the access to public or private transportation to and from the location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>latitude</td>
<td>number</td>
<td>Y coordinate of location expressed in decimal degrees in WGS84 datum.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>longitude</td>
<td>number</td>
<td>X coordinate of location expressed in decimal degrees in WGS84 datum.</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p><strong>Contact</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each contact must have a unique identifier</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>The identifier of the organization for which this is a contact</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>service_id</td>
<td>string</td>
<td>The identifier of the service for which this is a contact</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>service_at_location_id</td>
<td>string</td>
<td>The identifier of the &#8216;service at location&#8217; table entry, when this contact is specific to a service in a particular location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>name</td>
<td>string</td>
<td>The name of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>title</td>
<td>string</td>
<td>The job title of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>department</td>
<td>string</td>
<td>The department that the person is part of</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>email</td>
<td>string (email)</td>
<td>The email address of the person</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p>To ensure that all sub-resource area available as part of each of the requests and responses for all core API paths, we are going to have to evolve the HSDS schema to be:</p>

<p><strong>Organization</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string (uuid)</td>
<td>Each organization must have a unique identifier.</td>
<td>True</td>
<td>True</td>
</tr>
<tr class="row-odd"><td>name</td>
<td>string</td>
<td>The official or public name of the organization.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>alternate_name</td>
<td>string</td>
<td>Alternative or commonly used name for the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>description</td>
<td>string</td>
<td>A brief summary about the organization. It can contain markup such as HTML or Markdown.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>email</td>
<td>string (email)</td>
<td>The contact e-mail address for the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>url</td>
<td>string (url)</td>
<td>The URL (website address) of the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>tax_status</td>
<td>string</td>
<td>Government assigned tax designation for for tax-exempt organizations.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>tax_id</td>
<td>string</td>
<td>A government issued identifier used for the purpose of tax administration.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>year_incorporated</td>
<td>date (%Y)</td>
<td>The year in which the organization was legally formed.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>legal_status</td>
<td>string</td>
<td>The legal status defines the conditions that an organization is operating under; e.g. non-profit, private corporation or a government organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>services</td>
<td>array</td>
<td>Returns a collection of services for each organization</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>locations</td>
<td>array</td>
<td>Returns a collection of locations for each organization</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>contacts</td>
<td>array</td>
<td>Returns a collection of contacts for each organization</td>
<td>False</td>
<td>False</td>
</tr>    
<tr class="row-odd"><td>phones</td>
<td>array</td>
<td>Returns a collection of phones for each organization</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>programs</td>
<td>array</td>
<td>Returns a collection of programs for each organization</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>fundings</td>
<td>array</td>
<td>Returns a collection of fundings for each organization</td>
<td>False</td>
<td>False</td>
</tr>   
</tbody>
</table>

<p><strong>Service</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each service must have a unique identifier.</td>
<td>True</td>
<td>True</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>The identifier of the organization that provides this service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>program_id</td>
<td>string</td>
<td>The identifier of the program this service is delivered under.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>name</td>
<td>string</td>
<td>The official or public name of the service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>alternate_name</td>
<td>string</td>
<td>Alternative or commonly used name for a service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>description</td>
<td>string</td>
<td>A description of the service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>url</td>
<td>string (url)</td>
<td>URL of the service</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>email</td>
<td>string (email)</td>
<td>Email address for the service</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>status</td>
<td>string</td>
<td>The current status of the service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>interpretation_services</td>
<td>string</td>
<td>A description of any interpretation services available for accessing this service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>application_process</td>
<td>string</td>
<td>The steps needed to access the service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>wait_time</td>
<td>string</td>
<td>Time a client may expect to wait before receiving a service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>fees</td>
<td>string</td>
<td>Details of any charges for service users to access this service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>accreditations</td>
<td>string</td>
<td>Details of any accreditations. Accreditation is the formal evaluation of an organization or program against best practice standards set by an accrediting organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>licenses</td>
<td>string</td>
<td>An organization may have a license issued by a government entity to operate legally. A list of any such licenses can be provided here.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>taxonomy_ids</td>
<td>string</td>
<td>(Deprecated) A comma separated list of identifiers from the taxonomy table. This field is deprecated in favour of using the service_taxonomy table.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>contacts</td>
<td>array</td>
<td>Returns a collection of contacts for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>phones</td>
<td>array</td>
<td>Returns a collection of phones for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>regular_schedules</td>
<td>array</td>
<td>Returns a collection of regular schedules for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>holiday_schedules</td>
<td>array</td>
<td>Returns a collection of holiday schedules for each service.</td>
<td>False</td>
<td>False</td>
</tr>    
<tr class="row-odd"><td>fundings</td>
<td>array</td>
<td>Returns a collection of fundings for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>eligibilities</td>
<td>array</td>
<td>Returns a collection of eligibilities for each service.</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>service_areas</td>
<td>array</td>
<td>Returns a collection of service areas for each service.</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>required_documents</td>
<td>array</td>
<td>Returns a collection of required documents for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>payments_accepted</td>
<td>array</td>
<td>Returns a collection of payments accepted for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>languages</td>
<td>array</td>
<td>Returns a collection of languages for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
</tbody>
</table>

<p><strong>Location</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each location must have a unique identifier</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>Each location entry should be linked to a single organization. This is the organization that is responsible for maintaining information about this location. The identifier of the organization should be given here. Details of the services the organisation delivers at this location should be provided in the services_at_location table.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>name</td>
<td>string</td>
<td>The name of the location</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>alternate_name</td>
<td>string</td>
<td>An alternative name for the location</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>description</td>
<td>string</td>
<td>A description of this location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>transportation</td>
<td>string</td>
<td>A description of the access to public or private transportation to and from the location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>latitude</td>
<td>number</td>
<td>Y coordinate of location expressed in decimal degrees in WGS84 datum.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>longitude</td>
<td>number</td>
<td>X coordinate of location expressed in decimal degrees in WGS84 datum.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>phones</td>
<td>array</td>
<td>Returns a collection of phones for each location.</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>physical_addresses</td>
<td>array</td>
<td>Returns a collection of physical addresses for each location.</td>
<td>False</td>
<td>False</td>
</tr>    
<tr class="row-odd"><td>postal_addresses</td>
<td>array</td>
<td>Returns a collection of postal addresses for each location.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>regular_schedules</td>
<td>array</td>
<td>Returns a collection of regular schedules for each location.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>holiday_schedules</td>
<td>array</td>
<td>Returns a collection of holiday schedules for each location.</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>languages</td>
<td>array</td>
<td>Returns a collection of languages for each location.</td>
<td>False</td>
<td>False</td>
</tr>    
<tr class="row-odd"><td>accessiblity_for_disabilities</td>
<td>array</td>
<td>Returns a collection of accessiblity_for_disabilities for each location.</td>
<td>False</td>
<td>False</td>
</tr>    
</tbody>
</table>

<p><strong>Contact</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each contact must have a unique identifier</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>The identifier of the organization for which this is a contact</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>service_id</td>
<td>string</td>
<td>The identifier of the service for which this is a contact</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>service_at_location_id</td>
<td>string</td>
<td>The identifier of the &#8216;service at location&#8217; table entry, when this contact is specific to a service in a particular location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>name</td>
<td>string</td>
<td>The name of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>title</td>
<td>string</td>
<td>The job title of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>department</td>
<td>string</td>
<td>The department that the person is part of</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>email</td>
<td>string (email)</td>
<td>The email address of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>phones</td>
<td>array</td>
<td>Returns a collection of phones for each contact</td>
<td>False</td>
<td>False</td>
</tr>  
</tbody>
</table>

<p>Once we add all relevant sub-resources added as arrays to the HSDS schema, we can allow API consumers to POST and PUT, or GET using as little, or as much of the schema using the path, header, or parameter. Allowing for reading and writing HSDS at the granular level, or everything at once using a single path.</p>

<p>Next we need to consider this updated schema as part of the 1.2 release of HSDS. If we update HSDA to allow for filtering the schema across /organization, /service, /location, and /contact, and return each sub-resource as part of the API request or response, it will be out of sync with HSDS. Ideally, both HSDS, and HSDA move forward in sync with each version. I’m curious why this expanded schema became such an issue once we got to the API phase–it seems like it should have been part of v1.1 of HSDS, making the schema drive the API instead of the other way around.</p>

<p>I’m guessing that these concerns about schema don’t come into focus until we start talking about access to the schema, and data. <a href="http://apievangelist.com/2017/07/13/quantifying-the-difference-between-human-services-data-specification-hsds-and-its-api/">Making the separation and relationships between HSDS and HSDA all the more important</a>, providing a framework to move the schema forward in a way that is rooted in how it will actually be accessed. Which is why we do APIs…</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/17/making-all-sub-resources-available-within-the-core-set-of-human-service-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/16/learning-more-about-amazon-alexas-approach-to-apis-and-skills-development/">Learning More About Amazon Alexas Approach to APIs And Skills Development</a></h3>
        <span class="post-date">16 Jul 2017</span>
        <p><a href="https://developer.amazon.com/alexa"><img src="https://s3.amazonaws.com/kinlane-productions/amazon/alexa/avs_getting_started_1.png" align="right" width="40%" style="padding 15px" /></a></p>
<p>I have had <a href="https://developer.amazon.com/alexa">Amazon Alexa</a> in my cross hairs for some time now. I regularly digest stories about what Amazon is up to with Alexa, but haven’t had the time to think deeply about voice enablement, and their approach to developing what they call “skills”. I’m not 100% convinced voice enablement is the future of human compute interfaces, but I do see the role they can play in some situations, for some people. Plus, all the actions involved with Alexa and it’s ecosystem are all driven using APIs, which will almost always make me perk up, and pay a little more attention–I have a serious problem.</p>

<p>The Amazon Alexa platform centers around two specific areas of development:</p>

<ul>
  <li><strong>Alexa Voice Service</strong> - The actual voice enablement, and baking Alex voice into applications, devices, your home, car, and other physical objects in our world.</li>
  <li><strong>Alexa Skills Kit</strong> - The things that you can say to your Alex that will tigger specific actions, which make calls to APIs, and return something useful (or not).</li>
</ul>

<p>Its all about baking Alex Voice Service baked into as many devices you possibly can, and develop the catalog of skills that the voice enabled application can put to use. Ok. Well, my next question(s) are 1) what is a skill, and 2) what can you actually do with skills? Amazon provides some resources to help with the basics:</p>

<ul>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-different-types-of-skills"><strong>Understanding the Different Types of Skills</strong></a></li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-how-users-interact-with-skills"><strong>Understanding How Users Interact with Skills</strong></a></li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/requirements-to-build-a-skill"><strong>Requirements to Build a Skill</strong></a></li>
</ul>

<p>Ok, helps me grasp what is their definition of a skill a little bit, and how it delivers their view of a voice enabled user interaction. Next, what can a skill really do? Or, what types of “skills” does Amazon want you to build? They start with the lofty perspective of “anything”, or “<a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/overviews/understanding-custom-skills">custom  skills</a>”, to hook us technologists who like to think about at this level and fill in the gaps with our magical technological skills–a fundamental building block of API culture.</p>

<ul>
  <li>Look up information from a web service</li>
  <li>Integrate with a web service to order something (order a car from Uber, order a pizza from Domino’s Pizza)</li>
  <li>Interactive games</li>
</ul>

<p>This is API Evangelism 101. You start with everything and anything is possible, and work your way down from there. After lighting the imagination with custom skills, the focus in on a couple of specific types of actions, serving very specific purposes:</p>

<ul>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/overviews/understanding-the-smart-home-skill-api">Smart Home Skill API</a>
    <ul>
      <li>Turn lights on and off</li>
      <li>Change the brightness of dimmable lights</li>
      <li>Change the color or color temperature of a tunable light</li>
      <li>Change the temperature on a thermostat</li>
      <li>Query a lock to see if it is currently locked</li>
      <li>Ask for a smart home camera feed</li>
    </ul>
  </li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/building-smart-home-skills-for-entertainment-devices">Entertainment Device Control in the Smart Home Skill API</a>
    <ul>
      <li>Change the volume</li>
      <li>Change the channel</li>
      <li>Pause, rewind or fast forward music or video content</li>
    </ul>
  </li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-video-skill-api">Video Skills API</a>
    <ul>
      <li>Play a movie</li>
      <li>Find a TV show</li>
      <li>Change a channel</li>
      <li>Pause, rewind, or fast forward video content</li>
    </ul>
  </li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-flash-briefing-skill-api">Flash Briefing Skill API</a></li>
</ul>

<p>Now the concept of a skill comes into focus a little more for me. Amazon really wants to encourage developers to develop features that deliver in a home environment, entertaining us. Alexa performance and entertainment skills. So far I have just pulled references from Amazon Alexa’s documentation, if you want to see what has been developed by the community, you can head over <a href="https://www.amazon.com/b?node=13727921011">to the Alex skills catalog</a>. I also recommend <a href="https://github.com/dale3h/alexa-skills-list">checking out a pretty robust 3rd party skills list</a>, which gives a view of skills from the outside-in.</p>

<p>Alexa Voice Service and Skills Kit are the two core services, but when you browse the documentation, you see there is a 3rd area given just as much prominence–the <a href="https://developer.amazon.com/alexa-fund">Alexa Fund</a>, which “provides up to $100 million in investments to fuel voice technology innovation”. This is an important aspect of the skills development conversation, an opportunity to get funding to support the creation of skills. While Slack doesn’t use the word skills, <a href="https://slack.com/developers/fund">they also have a fund for investing in conversational interfaces</a> (messaging, chat, and bot). One thing to note here, <a href="https://developer.amazon.com/public/solutions/alexa/rewards-for-skill-developers">Amazon has additional rewards program where developers can earn rewards when developing specially game skills for the Alexa platform</a>–providing another glimpse into their strategy.</p>

<p>I am writing this post to support our Contrafabulist podcast, but I’m also doing it to feed my wider voice research as the API Evangelist. So I have to highlight some of the common building blocks of the Alexa approach to API management, helping me better understand Amazon’s approach to this set of API resources.</p>

<ul>
  <li><strong>Glossary</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-glossary</li>
  <li><strong>Forum</strong> - http://forums.developer.amazon.com/forums/category.jspa?categoryID=60</li>
  <li><strong>FAQs</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/docs/alexa-voice-service-developer-preview</li>
  <li><strong>Blog</strong> - https://developer.amazon.com/blogs/alexa/tag/AVS</li>
  <li><strong>Terms of Service</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/support/terms-and-agreements</li>
</ul>

<p>Beyond the common building blocks for operating their developer portal, supporting their APIs, they have some interesting design elements available for developers. Helping direct Alex developers to develop skills and voice-enabled applications that fit in with their objectives:</p>

<ul>
  <li><strong>Designing for AVS</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/designing-for-the-alexa-voice-service</li>
  <li><strong>Functional Design Guide</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/alexa-voice-service-functional-design-guide</li>
  <li><strong>UX Design Guidelines</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/alexa-voice-service-ux-design-guidelines</li>
  <li><strong>Marketing Brand Guidelines</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/marketing-brand-guidelines</li>
</ul>

<p>These design elements tell an interesting story regarding how Amazon is aligning the concept of skills development with their voice enabled API strategy. It also provides an interesting approach to design guides that other API providers might want to consider. After the design guides, Amazon provides some interesting code and hardware to help developers, providing starter kits to get going developing skills as well as actual physical voice integration:</p>

<ul>
  <li><strong>Projects and Sample Code</strong> - https://github.com/alexa/alexa-avs-sample-app</li>
  <li><strong>API and Reference</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/avs-api-overview</li>
  <li><strong>Development Kits for AVS</strong> - https://developer.amazon.com/dev-kits</li>
  <li><strong>Development Kits (Hardware)</strong> - https://developer.amazon.com/alexa-voice-service/dev-kits</li>
</ul>

<p>There are three different stories going on here for me which I think are relevant to how we think about, and approach our usage of technology. I’m really interested in Amazon Alexa because of:</p>

<ul>
  <li><strong>Voice Enablement</strong> - How do you enable voice applications using APIs? What role will voice play in the wider conversational API landscape?</li>
  <li><strong>Skills Concept</strong> - I am fascinated by the concept of a skill and how it applies to not just voice enablement, and conversational interface, but also representing a unit of compute or a transaction that occurs in API land (serverless, microservices, containers, etc.)</li>
  <li><strong>API Management</strong> - How do you manage a set of APIs that serve conversational interfaces and voice-enablement? What is different than regular API management, and what can others learn from their approach?</li>
</ul>

<p>As I said in the opening, I’m not 100% convinced that voice interfaces will be the future for everyone. I depend on the intimacy that exists between my fingers and the keyboard to make the magic happen each day–which might be a little noisy, but it doesn’t involve me rambling on, talking to a device. Even though I’m not into voice interfaces, and much of a talker, I am interested in the motivations behind, and the approach that Amazon is taking with their conversational interfaces. It is something i’m comparing with my research into Twitter, Slack, as well as Facebook. These companies are investing a lot into their ecosystems–you can see the signs of it over at Amazon with <a href="https://www.amazon.jobs/en/teams/alexa-skills?base_query=&amp;loc_query=&amp;job_count=10&amp;result_limit=10&amp;sort=relevant&amp;team_category%5B%5D=alexa-skills&amp;cache">the 137 job openings for their Alex team</a>.</p>

<p>Audrey and I are going to talk about Amazon Alexa on our Contrafabulists podcast this week, so you can tune in to get more thoughts of mine regarding Alexa, and voice enablement APIs. I’ll probably continue the exploration of my thoughts about this approach to interfaces, and particularly the development of “skills”. Which I think has an interesting overlap with APIs, and the modularization we are seeing as a result of compute (ie. containers, microservices, devops), as well as the impact APIs are having on labor (ie. Uber, Mechanical Turk, Task Rabbit). I feel like there is a lot more going on here than just developing fun skills for having conversations with Alexa in your home.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/16/learning-more-about-amazon-alexas-approach-to-apis-and-skills-development/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

<p align="center"><a href="http://apievangelist.com/archive/"><strong>View Previous Posts Via Archives</strong></a></p>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/" target="_blank"><img src="https://apievangelist.com/images/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/" class="image"><img src="https://apievangelist.com/images/postman-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
