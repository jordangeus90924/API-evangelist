<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }
    
    .container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
    }
    .video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }    

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/28/will-the-experian-api-focus-on-the-people-being-ranked/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_02_25_at_6.02.50_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/28/will-the-experian-api-focus-on-the-people-being-ranked/">Will The Experian API Focus On The People Being Ranked?</a></h3>
			<p><em>28 Feb 2017</em></p>
			<p>I was reading about Experian the credit score company "ventures nimbly into the API economy" this week. I'm happy to see any company begin their API journey, especially companies whose important algorithms impact our lives in such a major way. APIs are critical when it comes to shining a light on how algorithms work and don't work. According to the Experian developer page, "the Experian Connect API provides easy access to embed credit functionality on your websites and mobile apps. Consumer-empowered sharing allows you to create products and services for previously unreachable markets". Sadly I can't see much about the API itself, as you have to fill out a form and request access to see documentation&nbsp;or anything beyond just a basic description. The Experian Connect API says you get access to credit scores and reports, but most of it sounds like your standard marketing speak. I find API documentation and actually playing with an API provide a much more honest take on what's going on. Their approach reflects what I've seen from other very secretive companies who are used to maintaining tight control over their algorithms, processes, and partnerships.&nbsp; Another aspect of the Experian Connect API I noticed as I read was it doesn't focus on solutions for the end-users who is being scored and reported on, it is designed for businesses looking to pull reports on people for a variety of purposes. When you read the story about the Experian API, and the description on the Experian developer page, it is all very business focused--centering on the opportunity for businesses, not the people it scores and ranks.&nbsp; Credit scores are one of the OG big data companies, long tracking information on people, and selling access to business and the government. While Experian's venture into the API economy is worth noting, I'm guessing their API journey won't be a very public one. It's just not in their DNA. I wish they would see the value...[<a href="/2017/02/28/will-the-experian-api-focus-on-the-people-being-ranked/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/28/the-aws-serverless-api-portal/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/aws_serverless_portal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/28/the-aws-serverless-api-portal/">The AWS Serverless API Portal</a></h3>
			<p><em>28 Feb 2017</em></p>
			<p>I was looking through the Github accounts for Amazon Web Services and came across their Serverless API Portal--a pretty functional example of a forkable developer portal for your API, running on a variety of AWS services. It's a pretty interesting implementation&nbsp;because in addition to the tech of your API management it also helps you with the business side of things.&nbsp; The AWS Serverless Developer Portal "is a reference implementation for a developer portal application that allows users to register, discover, and subscribe to your API Products (API Gateway Usage Plans), manage their API Keys, and view their usage metrics for your APIs..[]..it also&nbsp;supports subscription/unsubscription through a SaaS product offering through the AWS Marketplace."--providing a pretty compelling API portal solution running on AWS. There are a couple things I think are pretty noteworthy: Application Backend (/lambdas/backend) - The application backend is a Lambda function built on the aws-serverless-express library. The backend is responsible for login/registration, API subscription/unsubscription, usage metrics, and handling product subscription redirects from AWS Marketplace. Marketplace SaaS Setup Instructions - You can sell your SaaS product through AWS Marketplace and have the developer portal manage the subscription/unsubscription workflows. API Gateway will automatically provide authorization and metering for your product and subscribers will be automatically billed through AWS Marketplace AWS Marketplace SNS Listener Function (Optional) (/listener) - The listener Lambda function will be triggered when customers subscribe or unsubscribe to your product through the AWS Marketplace console. AWS Marketplace will generate a unique SNS Topic where events will be published for your product. This is the required infrastructure we'll need to get to what I've been talking about for some time with my wholesale API and virtual API stack stories. Amazon is providing you with the infrastructure you need to set up the storefront for your APIs, providing the management layer you will need, including monetization via their marketplace. This is a retail layer, but because your&nbsp;infrastructure is setup in this way, there is no...[<a href="/2017/02/28/the-aws-serverless-api-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/28/a-checklist-for-api-observability/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_observability_lighthouse.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/28/a-checklist-for-api-observability/">A Checklist For API Observability</a></h3>
			<p><em>28 Feb 2017</em></p>
			<p>I have had the Wikipedia page for Observability open in a browser tab for weeks now. I learned about the concept from Stripe a while back and is something that I am looking to help define APIs from an external vantage point. In this world of fake news and wild promises of artificial intelligence and machine learning, we need these black boxes to be as observable as they can--I am hoping that APIs can be one of the tools in this observability toolbox. Stripe is approaching this concept from the inside, with a focus on stability and reliability of their API operations. I am focusing on this concept from the outside, to "measure how well internal states of a system can be inferred by knowledge of its external outputs". More of a regulatory, auditing, and journalistic way of thinking, but in the API way of doing things. Some of this is about understanding, but some of it is also about holding providers accountable for what they are peddling. The other day I mapped out what API monitoring means to me, rebranding it as what I'd prefer to call API awareness. I went through the elements I use to understand what is going on with APIs across the sector, but this time I am thinking about them in terms of observability. Meaning, not what I'm doing to be aware of APIs, but what is expected from providers to meet (my) minimum viable definition of a platform being observable. Discovery - Do APIs exist? Are they easily discoverable? Are they public? Can anyone put them to use?&nbsp; Applications&nbsp;- Find new applications built on top of APIs. People&nbsp;- People who are doing interesting things with APIs. Organization&nbsp;- Any company, group, or organization working with APIs. Services&nbsp;- API-centric services that might help solve a problem. Tools&nbsp;- Open source tools that put APIs to work solving a problem. Versions -&nbsp;What versions are currently in use, what new versions are available,...[<a href="/2017/02/28/a-checklist-for-api-observability/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/27/the-api-definition-for-the-tyk-api-gateway/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_02_25_at_9.00.37_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/27/the-api-definition-for-the-tyk-api-gateway/">The API Definition For The Tyk API Gateway</a></h3>
			<p><em>27 Feb 2017</em></p>
			<p>If you are selling a service you should have an API. It is something you hear me talk about a lot here on the blog. I push on this subject because it is important, and there are numerous API service providers out there who do not have an API&nbsp;or choose to not make them available. In a DevOps,&nbsp;continuous integration world, we need the entire stack to have APIs--making our API platforms programmatic, just like the data, content, and algorithms we are making available via the APIs we are deploying. If you need an example of this in the wild, you don't have to look much further than my partner in crime Tyk, who have a simple API for their API gateway--no matter where you deploy the gateway, you can manage it using it's APIs. The&nbsp;Tyk&nbsp;API Gateway API provides you with a base set of paths for you to manage your gateway. An open source, lightweight, fast and scalable API Gateway. Set rate limiting, request throttling, and auto-renewing request quotas to manage how your users access your API. Tyk supports access tokens, HMAC request signing, basic authentication and OAuth 2.0 to integrate old and new services easily. Tyk can record and store detailed analytics which can be segmented by user, error, endpoint and client ID across multiple APIs and versions. Integrate your existing or new applications with Tyk using a simple REST API, Tyk even support hot-reloads so you can introduce new services without downtime. Tyk API Management Paths Available (OpenAPI Spec) /tyk/apis/ -- Get APIs [GET] - Gets a list of *API Definition* objects that are currently live on the gateway /tyk/apis/ -- Create API [POST] - Create an *API Definition* object /tyk/apis/{apiID} -- Delete API [DELETE] - Deletes an *API Definition* object, if it exists /tyk/apis/{apiID} -- Get API [GET] - Gets an *API Definition* object, if it exists /tyk/apis/{apiID} -- Update API [PUT] - Updates an *API Definition* object, if it exists /tyk/health/...[<a href="/2017/02/27/the-api-definition-for-the-tyk-api-gateway/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/27/i-need-your-help-with-my-api-definition-industry-guide/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_02_26_at_8.38.50_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/27/i-need-your-help-with-my-api-definition-industry-guide/">I Need Your Help With My API Definition Industry Guide</a></h3>
			<p><em>27 Feb 2017</em></p>
			<p>I am approaching seven years doing API Evangelist. I have over 70 areas of my core API lifecycle research available on the website&nbsp;and have four of those areas (definitions, design, deployment, &amp; management) that I've been publishing industry guides for the last couple of years. In 2017, I want to take those guides, and hopefully a handful of other research areas to the next level. My guides have always been about the&nbsp;quantity of information, over the quality of the final guide. I want to turn that on its head and focus on the&nbsp;quality of information and presentation over the quantity, publishing an executive summary of each of my API industry research areas. With my new guide, I am looking to add a touch of design, but I'm also looking to expand&nbsp;the exposure and storytelling opportunities for my partners in the space. Using Adobe In Design I have been able to handle the design enhancements, but I am in need of help making sure my industry guide are ready for consumption by a wider, and more mainstream audience--this is where you come in. I need your feedback. Seriously, I need you to help me with everything copy editing to being&nbsp;an overall critic--let me know what works and what doesn't--I'm looking to make this a community affair. My API Definition research operates as a Github repository, providing access to the data and content behind each area. I use the Github Issues for each of my API research like I would for any other project I'm managing using Github. If you have the time, I would be grateful if you would take look at my API definition guide&nbsp;and submit a single Github issue with your&nbsp;feedback,. I'm even willing to give some exposure in each edition, thanking the folks who helped out in the 'about page' of the guide. Also, if you help out enough I'd be willing to give one of the sponsor slots to your company, project,...[<a href="/2017/02/27/i-need-your-help-with-my-api-definition-industry-guide/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/27/an-example-of-an-api-service-provider-using-hypermedia/"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/aws-api-gateway-icon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/27/an-example-of-an-api-service-provider-using-hypermedia/">An Example Of An API Service Provider Using Hypermedia</a></h3>
			<p><em>27 Feb 2017</em></p>
			<p>There&nbsp;is a growing number of hypermedia APIs available in the wild these days. However there aren't a lot of examples of hypermedia API service providers making the API lifecycle&nbsp;more dynamic and living. When people ask me for examples of hypermedia APIs out there I like to have a handful of URLs I can share with them, providing a diverse set they can consider as part of their own operations. One really good example of an API service provider putting hypermedia to use is Amazon Web Services--specifically with the AWS API Gateway. &nbsp;AWS describes it best in the documentation&nbsp;for the gateway API: The Amazon API Gateway web service is a resource-based API that uses Hypertext Application Language (HAL). HAL provides a standard way for expressing the resources and relationships of an API as hyperlinks. Using HAL, you use HTTP methods (GET, PUT, POST, DELETE) to submit requests and receive information about the API in the response. Applications can use the information returned to explore the functionality of the API. If you have used other common AWS APIs like EC2 or S3, then you know that they aren't the best designed APIs out there. They provide a lot of functionality&nbsp;but leave a lot to be desired when it comes to the actual design. The AWS API Gateway API is a well designed, highly functional API for managing the operations of your API. With each API call, you get the desired response, along with a collection of links defining what else is possible. I wish that all tools in our API toolbox were designed like the AWS API Gateway is. In this single API call you can see how hypermedia contributes to the life cycle of any API being managed. You can manage access, and evolve into a staging or production environment, and the other possibilities available when each resource is put to work. Instead of having to go back to the API documentation to learn what options...[<a href="/2017/02/27/an-example-of-an-api-service-provider-using-hypermedia/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/20/a-well-thought-out-api-platform/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_platform_upside_down.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/20/a-well-thought-out-api-platform/">A Well Thought Out API Platform</a></h3>
			<p><em>20 Feb 2017</em></p>
			<p>
I was playing with one of the API deployment solutions that I track on, appropriately called API Platform. It is an open source PHP solution for defining, designing, and deploying your linked data APIs. I thought their list of features provided a pretty sophisticated look at what an API can be, and was something I wanted to share.

Create a&nbsp;CRUD&nbsp;API in minutes
JSON-LD,&nbsp;Hydra,&nbsp;HAL&nbsp;native support
Automatic&nbsp;Swagger&nbsp;documentation
Built with&nbsp;Symfony&nbsp;and&nbsp;Doctrine
Docker&nbsp;integration
Data validation and error management
Pagination, filtering and sorting
Generate the data model using&nbsp;Schema.org
FOSUser,JWT, CORS and&nbsp;OAuth&nbsp;support
Implements&nbsp;OWASP's recos
Modular
Designed for speed and caching
Behat,&nbsp;PHPUnit&nbsp;and&nbsp;Postman&nbsp;spec &amp; testing
100% open source (MIT)

There are a couple of key elements here. API definition-driven with JSON-LD, Hydra, HAL, and OpenAPI Spec out of box. Containerized. Schema.org FTW! JWT, and OAuth. OWASP's security checklist. Postman Ready! These features make for a pretty compelling approach to designing and deploying your APIs. While I see some of these features in other platforms, it is the first with an open source solution possessing such an impressive resume.&nbsp;
I'm going to take this list and add to my list of API design, and deployment building blocks in my research. These are features that other API deployment solutions should be considering as part of their offering. This approach to API deployment may not be the right answer for every type of API, but I know many data and content focused APIs thatwouldl benefit significantly from a deployment solution like API Platform.
[<a href="/2017/02/20/a-well-thought-out-api-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/17/api-definitions-influencing-api-design/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-two-arrows.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/17/api-definitions-influencing-api-design/">API Definitions Influencing API Design</a></h3>
			<p><em>17 Feb 2017</em></p>
			<p>I was having a conversation about whether I should be putting my API definition or my API design work first--which comes earlier in the lifecycle of an API? The conclusion was to put definition first because you need a common set of definitions to work with when designing your API(s). You need definitions like HTTP and HTTP/2. In, 2017 you should be employing definitions like OpenAPI Spec, and JSON Schema. These definitions help set the tone of your API design process. In my opinion, one of the biggest benefits of designing, developing, and operating APIs on the web has been forcing developers to pick up their heads and pay attention to what everybody else is doing and wanting. I suffer from this. Doing web APIs, providing my own, and consuming 3rd party APIs forces me to pay attention to providers and consumers outside my bubble--this is good. Common definitions help us elevate the design of our APIs by leveraging common concepts, standards, and schema. Every time you employ ISO 8601, you have to think about folks in another time zone. Every time you use ISO 4217, you have to think about people who buy and sell their products and services in a different currency than you. When you use Schema.org, your postal addresses considers the world beyond just US zip codes, and consider a world wid web of commerce. I am placing definitions before design in my API research listing. In reality, I think this is just a cycle. Common definitions feed my design process, and the more experienced I get with design, the more robust my toolbox of API definitions gets. Ultimately this depends on what I'm calling a definition, but for the sake of this story I am considering the building blocks of the web as the first line of definitions, then secondarily the definitions that are using OpenAPI Spec and JSON Schema as the next line of definitions. Definitions influence my design process, and the design...[<a href="/2017/02/17/api-definitions-influencing-api-design/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/16/trying-to-define-api-awareness/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_light_bulb_bright_api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/16/trying-to-define-api-awareness/">Trying To Define API Awareness</a></h3>
			<p><em>16 Feb 2017</em></p>
			<p>I have a regular call with a really smart API person who is trying to move forward a really cool project for the API space. It is some thought provoking voodoo and I need to be able to write about it--this is how I flush out my thoughts and move forward. He is not quite ready to talk about his project publicly, so I will just talk about and explore in terms of my API Evangelist research and how it applies to the area(s) of the API space he is looking to make an impact. This topic spans several areas of my API research, but if I had to give it a single label I would call it API awareness. When you hear me talk about my monitoring the API space, API awareness is the result. I wanted to try and communicate this from my vantage point but also share with other analysts, practitioners, and even the average individual online today. This is my attempt to distil my approach to monitoring the API space and establishing a sustained awareness of APIs at any level. Individual ("Normals")It may sound crazy to you, but everyone should be API aware. No, they should be paying attention to APIs like I do, or even at the level of the average individual working in the tech sector, but they should have a baseline awareness, and here is my attempt at quantifying that: APIs Exist - Everyone should be aware that APIs exist, and hopefully have one or two examples of what they can do in their business or personal lives -- even if it's' just pulling tweets, photos, or getting news updates via an RSS feed. API Integration - Everyone should be aware that they can move data and content between the online services they use and depend on. If you know APIs exist and are aware of services like Zapier or Datafire, you will be more successful in what...[<a href="/2017/02/16/trying-to-define-api-awareness/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/16/api-lifecycle-service-providers-instead-of-walled-gardens/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/walled_garden_blue_gray_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/16/api-lifecycle-service-providers-instead-of-walled-gardens/">API Lifecycle Service Providers Instead Of Walled Gardens</a></h3>
			<p><em>16 Feb 2017</em></p>
			<p>It is a common tactic of older software companies to offer open source, services, and tools in a way that all roads just lead into their walled garden. There are many ways to push vendor lock-in and the big software vendors from 2000 through 2010 have mastered how to route you back to their walled gardens and make sure you stay there. Web APIs have set into motion a shift in how we architect our web, mobile, and device applications, as well as providing services to the life cycle that are behind the operation of these web APIs. While this change has the potential to positive it often it can be very difficult to tell apart the newer breed of software companies from the legacy version, amidst all the hype around technology and startups. I've been having conversations recently which are pushing me to think more about middleware, or what I'd refer to as API life cycle tooling. In my opinion, these are companies who are selling services and tools to the API life cycle, which in turn is fueling our web, mobile, device, and other applications. In my opinion, as a server provider, you should be selling to a company and API provider's life cycle needs, not your walled garden needs. I understand that you want all of a companies business, and you want to lock them into your platform, but that was how we did business 10 years ago. The API service providers I'm shining a light on in 2017 are servicing one or many stops along the API life cycle, supporting API definitions, and providing value without getting in the way, or locking customers in. They do this on-premise, or in the cloud of your choice, and allow you to seamless overlap many different API service providers providing a variety of solutions across the API life cycle. You will notice this patterns in the companies I partner with like APIMATIC, Restlet, Tyk, and Dreamfactory. I find I have...[<a href="/2017/02/16/api-lifecycle-service-providers-instead-of-walled-gardens/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/14/box039s-seamless-approach-to-api-documentation/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_02_13_at_10.20.02_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/14/box039s-seamless-approach-to-api-documentation/">Box&#039;s Seamless Approach To API Documentation</a></h3>
			<p><em>14 Feb 2017</em></p>
			<p>
The document platform Box updated their developer efforts recently, helping push&nbsp;forward the definition of what API documentation can be. I've long been advocating moving APIs out from the shadow of the developer portal, and make it more seamless with any UI, kind of like CloudFlare does with their DNS dashboard. There is no reason the API should have to be&nbsp;hidden from users--it should be right behind the UI for everyone to discover.
Box does this. You can interact with files just like it is the regular interface. When push the get the folder items, upload file, or other option available to you in the documentation--you get example API request and response in the right-hand column. It is a blend of a regular UI, and some of the attractive and interface documentation we've seen emerge lately like ReDoc. Making it easy to see and understand what an API does, while speaking in the context of solving a relevant problem for a human.
API documentation doesn't have to be overly technical and boring. It can look like a regular user interface, and the API can be right behind the UI curtain, providing a snapshot of the requests and responses that are doing the heavy lifting behind. I'm finally seeing the movement I have wanted to see with API documentation in 2017. I'm feeling like this is going to be common theme with the world of APIs for all of us--we will never see things move as fast as we want, but eventually the world evolves, and we will see investment in the areas that make a difference on the ground at API operations, and for the consumers who are putting APIs to work in their regular world.
[<a href="/2017/02/14/box039s-seamless-approach-to-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/14/api-lifemiddlewarecycle-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_gear_life_cycle.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/14/api-lifemiddlewarecycle-api/">API Life(middleware)Cycle API</a></h3>
			<p><em>14 Feb 2017</em></p>
			<p>I have had a series of calls with an analyst group lately, discussing the overall API landscape in 2017. They have a lot of interesting questions about the space, and I enjoyed their level of curiosity and awareness around what is going on--it helps me think through this stuff, and (hopefully) better explain it to folks who aren't immersed in API like I am.  This particular group is coming at it from a middleware perspective and trying to understand what APIs have done to the middleware market, and what opportunities exist (if at all). This starting point for an API conversation got me thinking about the concept of middleware in contrast to, or in relationship to what I'm seeing emerge as the services and tooling for the API life cycle. Honestly, when I jumped on this call I Googled the term middleware to provide me with a fresh definition. Middleware: software that acts as a bridge between an operating system or database and applications, especially on a network. What does that mean in the age of API? Did API replace this? There is middleware for deploying APIs from backend systems. There is middleware for brokering, proxying and providing a gateway for APIs. Making middleware as a term pretty irrelevant. I think middle traditionally meant a bridge between backend and the frontend, where web APIs make things omnidirectional--in the middle of many different directions and outcomes. The answer to the question of what has API done to middleware is just "added dimensions to its surface area". Where is the opportunity? "All along the API lifecycle". Middleware (aka services & tooling) is popping up through the life cycle to help design, deploy, manage, test, monitor, integrate, and numerous other stops along the API life cycle. All the features of our grandfathers API gateway are now available as a microservices buffet, allowing us to weave middleware nuggets into any system to system integrations as well as other web, mobile, and...[<a href="/2017/02/14/api-lifemiddlewarecycle-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/14/a-ckan-openapi-spec/"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/company/logos/ckan-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/14/a-ckan-openapi-spec/">A CKAN OpenAPI Spec</a></h3>
			<p><em>14 Feb 2017</em></p>
			<p>



I was working on publishing an index of the General Service Administration (GSA) APIs I currently have in my API monitoring system, and I remembered that I updated my Data.gov work publishing a cache of the index on Github. Part of this work I had left a note for myself about finding / creating an OpenAPI Spec for the Data.gov API, which since it is a CKAN implementation should be pretty easy--I hoped.
After Googling for a bit I found one created by the French government open data portal&nbsp;-- thank you!!. It looks pretty complete with 102 paths, and 79 definitions, providing a pretty nice jumpstart for anyone looking to documentation their CKAN open data implementation.&nbsp;

This API definition can be used to generate API documentation using Swagger UI or ReDoc, as well as generate SDKs using APIMATIC, and monitoring using Runscope or API Science. If you come across any other API definitions for CKAN, or any interesting documentation and other tools--please let me know, I want to keep aggregating CKAN related solutions.
Open source tools that have APIs, and have open API definitions like this are the future. These are the tools that companies, institutions, organizations, and government agencies should be putting to work in their operations because&nbsp;it helps reduce costs, but also having an API that uses common API specifications means it will speak the same language as other important tools and services, increasing the size of the toolbox available for implementatioperations your API operatons.

[<a href="/2017/02/14/a-ckan-openapi-spec/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/13/using-github-as-an-api-index-and-data-store/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/jekyll_open_referral.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/13/using-github-as-an-api-index-and-data-store/">Using Github As An API Index And Data Store</a></h3>
			<p><em>13 Feb 2017</em></p>
			<p>I am spending a lot of time studying how companies are using Github as part of their software and API development life cycle, and how the social coding platform is used. More companies like Netflix are using as part of their continuous integration workflow, something&nbsp;that&nbsp;API service providers like APIMATIC are looking to take advantage of with a new wave of services and tooling. This usage of Github goes well beyond just managing code, and are making the platform more of an engine in any continuous&nbsp;integration and API life cycle workflow. I run all my API research project sites on Github. I do this because it is secure&nbsp;and static, as well as introduces a very potent way to not just manage a single website, but over 200 individual open data and API projects. Each one of my API research areas leverages a Github Jeykll core, providing a machine readable index of the companies, news, tools, and other building blocks I'm aggregating throughout my research. Recently, this approach has moved beyond the core areas of my API research&nbsp;and is something I'm applying to my API discovery work, profiling the resources available with popular API platforms like Amazon Web Services, and across my government work like with my GSA index. Each of these projects managed using Github, providing a machine readable index of the disparate APiI, in a single APIs.json index which includes OpenAPI Specs for each of the APIs included. When complete, these indexes can provide a runtime discovery engine of APIs&nbsp;used as part of integrations, providing an&nbsp;index of single APIs, as well as potentially across many distributed APiI brought together into a single meaningful collection. I've started pushing this approach even further with my Knight Foundation funded Adopta.Agency work, and making the Github repository not just a machine-readable index of many APIs, I'm also using the _data folder as a JSON or YAML data store, which can then also be indexed as part of the APIs.json&nbsp;and...[<a href="/2017/02/13/using-github-as-an-api-index-and-data-store/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/13/the-reasons-why-we-pull-back-the-curtain-on-technology/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/curtain.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/13/the-reasons-why-we-pull-back-the-curtain-on-technology/">The Reasons Why We Pull Back The Curtain On Technology</a></h3>
			<p><em>13 Feb 2017</em></p>
			<p>Photo by Shelah I was trying to explain to a business analyst this week the difference between SDK and API, which he said was often used interchangeably by people he worked with. In my opinion SDK and API can be the same thing, depending on how you see this layer of our web, mobile, and device connectivity. The Internet has been rapidly expanding this layer for some time now, and unless you are watching it really don't see any difference between API and SDK--it is just where the software connects everything. For me, an SDK is where the data, content&nbsp;and algorithmic production behind the curtain is packaged up for you -- giving you a pre-defined&nbsp;look at what is possible, prepared for you with a specific language or platform in mind. Most of the hard work of understanding what is going on has been translated and crafted, providing you with a set of instructions of what you can do with this resource in your application--your integration is pretty rigidly defined, not much experimentation or hacking encouraged. An API has many of the same characteristics as an SDK, but the curtain is pulled back on the production a little bit more. Not entirely, but you do get a little more of a look at how things work, what data and content are available, and algorithmic resources are accessible. You still get the view which a provider intends you to have, but there are&nbsp;fewer assumptions about what you'll do with the resources put on the interface, leaving you to do more of the heavy lifting with how these resources will get put to use. Most of the early motivations behind choosing an open approach to web APIs over more closed and as proprietary SDK, pulling back the curtain on how we develop software, weren't entirely intentional. Companies like Flickr and Twitter weren't trying to make their mark on the politics of how we integrate software, they were busy...[<a href="/2017/02/13/the-reasons-why-we-pull-back-the-curtain-on-technology/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/10/where-are-the-interesting-api-bookmarklet-examples/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/share_bookmarklet_flow.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/10/where-are-the-interesting-api-bookmarklet-examples/">Where Are The Interesting API Bookmarklet Examples?</a></h3>
			<p><em>10 Feb 2017</em></p>
			<p>
I have been kvetching about the quality of embeddable tooling out there, so I'm working on&nbsp;discovering anything interesting. I started with bookmarklets, which I think is one of the most underutilized, and simplest examples of working with APIs on the web. Here are a couple of interesting bookmarklets for APIs out there:

Twitter - Probably the most iconic API and bookmarklet out there -- share to Twitter.
Pinboard - An API-driven bookmarklet for saving bookmarks that I use every day.
Hypothesis - A whole suite of API-driven bookmarklets for annotating the web.
Socrata - A pretty cool bookmarklet for quickly viewing documentation on datasets.
Tin Can API - A bookmarklet for recording self-directed learning experiences.

When you search for API bookmarklets you don't get much. Nothing stands out as being innovative. I will keep looking when I have time, and I'll keep curating and understanding any new approaches, and examples, and tooling when possible.
Ultimately it just confounds me, because a simple JS bookmarklet triggering one or more API interactions is a no brainer. We have examples of this in action, making an impact on login, sharing, annotation, and more, so why don't we have more examples? IDK. It is something I'll explore as I push forward my embeddable API research.
Maybe I'm just missing something...
[<a href="/2017/02/10/where-are-the-interesting-api-bookmarklet-examples/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/10/what-do-you-get-when-you-search-for-the-schema-org-logo/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_02_10_at_11.56.17_am.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/10/what-do-you-get-when-you-search-for-the-schema-org-logo/">What Do You Get When You Search For The Schema.org Logo?</a></h3>
			<p><em>10 Feb 2017</em></p>
			<p>
I spend a lot of time looking for logos of the companies that I write about. A lack of consistency around how companies manage (or don't) their logos, and make them available (or don't) regularly frustrates the hell out of me. While doing my regular work I found myself Googling for the Schema.org logl -- what came up made me smile.
When you Google for Schema.org logo you don't get the logo for Schema.org, you get the schema for a logo, which is the image property of a thing&nbsp;and is used by brands, organizations, places, products, and services. I still had to actually do a separate search to find the Schema.org logo, but it did make me smile, and make me think even deeper about how we manage (or don't) our bits online.
Schema.org is so important. It keeps popping up on my radar, and I'm seeing more examples of it being used as part of JSON-LD API and web search implementations. As I work on my human services data specification&nbsp;(HSDS) project I'm going to carve off time to weave JSON-LD and Schema.org into my storytelling. I can't just show people Schema.org and expect them to understand the importance, I'm going to have to show them with meaningful examples of it working out in the wild.
[<a href="/2017/02/10/what-do-you-get-when-you-search-for-the-schema-org-logo/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/09/having-a-program-for-researchers-baked-into-your-api-operations/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_02_09_at_12.15.02_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/09/having-a-program-for-researchers-baked-into-your-api-operations/">Having A Program For Researchers Baked Into Your API Operations</a></h3>
			<p><em>09 Feb 2017</em></p>
			<p>I wrote about the need for service level agreements dedicated to researchers who are depending on APIs a couple weeks ago, and while I was doing my work profiling of AWS, I came across their approach to supporting research. Amazon has a dedicated program research and technical computing on AWS, where they: "helps researchers process complex workloads by providing the cost-effective, scalable and secure compute, storage and database capabilities needed to accelerate time-to-science. With AWS, scientists can quickly analyze massive data pipelines, store petabytes of data and share their results with collaborators around the world, focusing on science not servers." Amazon has three distinct ways in which they are helping researchers, as well as the industries and people they impact: AWS Research Cloud Program - The AWS Research Cloud Program helps you focus on science, not servers---all with minimal effort and confidence that your data and budget are safe in the AWS Cloud. Government and education-based researchers are eligible to receive program benefits. Apply to join the program, in order to access the AWS Research Cloud Handbook and other cloud resources built for researchers, by researchers. AWS Research Initiative - The AWS Research Initiative (ARI) brings Amazon Web Services (AWS) and the National Science Foundation (NSF) together, with AWS providing AWS Cloud Services through provision of AWS Promotional Credits, awarded to NSF grant applicants to leverage Critical Techniques, Technologies and Methodologies for Advancing Foundations and Applications of Big Data Sciences and Engineering (BIGDATA). Open Data - Organizations around the globe are increasingly making their data open and available for the public to discover, access, and use. This is fueling entrepreneurship, accelerating scientific discovery, and creating efficiencies across many industries. Amazon Web Services provides a comprehensive tool kit for sharing and analyzing data at any scale.&nbsp; Amazon helps researchers by providing them with cloud resources for doing their research, assisting them with the budget of it all, while also opening you up to other grant opportunities,...[<a href="/2017/02/09/having-a-program-for-researchers-baked-into-your-api-operations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/09/api-management-is-getting-more-modular-and-composable/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/tyk_variants_1200px.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/09/api-management-is-getting-more-modular-and-composable/">API Management Is Getting More Modular And Composable</a></h3>
			<p><em>09 Feb 2017</em></p>
			<p>I've been keeping an eye on the API management space for about seven years now, and I actually have to say, even with all the acquisitions, IPOs, commoditization, etc, I am actually pretty happy with where the sector has evolved. API management always resembled its older cousin the API gateway for me, so when companies like 3Scale started offering a freemium model, that I could deploy in the cloud&nbsp;with a couple lines of code---I jumped on the API management bandwagon. It was easy&nbsp;and gave you all the service composition, onboarding, analytics, and metering tools you needed out of the box. I have been pushing on providers to provide an open source API management solution for quite some time, and providers like WSO2 finally stepped up to bring an enterprise-grade solution to the table, then solutions like API Umbrella also emerged for the government. Now in 2017, we have several open source solutions available to us, which makes me happy, but what I really like is how modular, versatile, and API-driven they are. I'm spending time learning more about my partner's solutions, and today I'm working my way through what is possible with Tyk. Tyk is what API management should be. It has all the user and organization management and assists you with the onboarding, authentication, service composition, rate limiting, and analytics that are core to any API management solution. However, what is really powerful for me is that you can deploy Tyk anywhere in the cloud, on-premise, on-device, and all of its features are API-driven, and interconnected because of it's APIs, webhooks, and other orchestration features. APIs aren't just about deploying a web API on a server, and making it available through a single base URL--they are everywhere, and our management tools need to reflect this. We don't just have a single API stack anymore and we use a handful of 3rd party APIs. We are weaving together many different internal and partner stacks, as...[<a href="/2017/02/09/api-management-is-getting-more-modular-and-composable/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/09/api-evangelist-joins-the-open-api-initiative-oai/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/open_api_initiative_members.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/09/api-evangelist-joins-the-open-api-initiative-oai/">API Evangelist Joins The Open API Initiative (OAI)</a></h3>
			<p><em>09 Feb 2017</em></p>
			<p>It was an interesting journey getting the API specification formerly known as Swagger into the Linux foundation last year. After SmartBear donated the spec to the newly formed Open API Initiative, I was considering joining the governing body behind the spec, but with all I had going on last year, I didn't feel it was the right time. Participating in governance groups hasn't really ever been my thing, and there are a handful of large organizations involved, and who am I really? I'm just a single person ranting about APIs. However, in 2017 I am changing my tune and will be joining the Open API Initiative (OAI) It is an important time for API definitions, and there is a lot riding on the success of OpenAPI, as well as API definitions in general. I feel like we need as many voices at the table as we possibly can. We need government agencies, enterprise, small businesses, startups, and individual analysts like myself. I feel pretty strongly that API definitions should be openly licensed, accessible, and following the most commonly used patterns available to us across the API sector. We don't need to all be speaking a different language when it comes to deploying compute resources or working with images, and API definitions are how we get there. After a ramping up period, I will work with OAI on their marketing strategy, helping tell stories, and learn about interesting implementations and use cases when possible--dovetailing nicely with what I am already doing. I'll be tuned into the conversation about the spec, but will definitely be standing off to the side as an observer for quite some time. I'm pretty stoked with what they've done with version 3.0 of the spec, and at this point, I have a lot to learn before I open up my mouth and chime in on anything. I'm just enjoying watching the personalities play out in the Slack room--this stuff should be a...[<a href="/2017/02/09/api-evangelist-joins-the-open-api-initiative-oai/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/08/the-unlimited-possibilities-when-you-become-api-definition-fluent/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/apimatic_dx_kits.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/08/the-unlimited-possibilities-when-you-become-api-definition-fluent/">The Unlimited Possibilities When You Become API Definition Fluent</a></h3>
			<p><em>08 Feb 2017</em></p>
			<p>
I was a regular check-in&nbsp;with one of my favorite API service providers this week, talking about some of the new features they are rolling out in coming weeks, and they demonstrated for me why API definitions are so important in 2017. &nbsp;APIMATIC got their start deploying SDKs for your API, but have quickly moved into providing API documentation, testing, continuous integration, and some additional stops that they have planned for release in coming months.
As I was sharing how happy I was with their movement into new areas of the API life cycle, and praising their agility&nbsp;when it came to rolling out new features, they responded with:
"The credit goes to machine-readable descriptions. Once you have them then there are endless possibilities. Thanks for motivating us to support all the description formats in first place :-)"
This is why being fluent in API definitions is so important to operating your API, or in APIMATIC's case, operating as an API service provider. There are an increasing number of API providers who support the importing and exporting of API definitions, but nobody has gone full API definition as APIMATIC as. They didn't just center their SDK, documentation, testing and continuous integration solutions around API definitions, they exposed their API definition translation engine as a service called API Transformer, allowing anyone else to follow their lead.
When people first learn about API definitions, the 101 lessons usually center around API documentation, and maybe secondarily the generation of the server or client-side&nbsp;SDKs. However API definitions are rapidly being used for every other stop along the life cycle from design to deprecation, as well as the unknown future stops along the way that only&nbsp;become possible because all your API resources are well defined, and machine-readable. It is this agility and flexibility that I want to incentivize, helping companies be as effective as APIMATIC has been when it comes to delivering new API-driven solutions.

[<a href="/2017/02/08/the-unlimited-possibilities-when-you-become-api-definition-fluent/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/08/sharing-compute-costs-for-open-data-and-api-consumers-using-the-cloud/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_02_08_at_8.18.41_am.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/08/sharing-compute-costs-for-open-data-and-api-consumers-using-the-cloud/">Sharing Compute Costs For Open Data And API Consumers Using The Cloud</a></h3>
			<p><em>08 Feb 2017</em></p>
			<p>I recently wrote about how Algorithmia offloads the compute costs around machine learning using AWS, structuring their image style transfer modeling so that the consumer pays the cost for deploy an AWS GPU instance. It is an interesting way to shift the burden of paying for the hard costs around API operations.
Another interesting approach I extracted from a story I wrote yesterday is from Amazon Web Services (AWS) with their approach to open data. Amazon Public Datasets are available as Amazon Elastic Block Store (Amazon EBS) snapshots and/or Amazon Simple Storage Service (Amazon S3) buckets. AWS hosts the master copy of the dataset, and when you want to use, you fire it up in your AWS account, and get to work.
I find some of these approaches to managing, and offloading the heavy costs of working with algorithms, datasets, and other resources interesting. I especially find this approach interesting because it is in the service of public data, providing an option for how the private sector can help share the load in managing government, research, and other public data, and offloading the heavy costs to those who are going to put the data to use. 
I will figure out how to add this into my API monetization and plan research. Providing a list of these types of approaches to providing on-premise, wholesale, and containerized or virtualized approaches to making data, content, and algorithms available. I will definitely keep looking for unique approaches to API deployment like this--after a couple of these grabbing my attention, I'm feeling like I am seeing another shift in both the technology and business of how we deploy and consume APIs.
[<a href="/2017/02/08/sharing-compute-costs-for-open-data-and-api-consumers-using-the-cloud/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/08/api-embeddables-in-a-conversational-interface-world/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/push_by_zapier_icon_button.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/08/api-embeddables-in-a-conversational-interface-world/">API Embeddables In A Conversational Interface World</a></h3>
			<p><em>08 Feb 2017</em></p>
			<p>I would say that embeddable tooling is one of saddest areas of the API space for me in recent years. When it comes to buttons, badges, widgets, and other embeddable goodies that put APIs work, the innovation has been extremely underwhelming. Login, like, share, and a handful of other embeddable tooling have taken hold, but there really isn't any sort of sophisticated approach to putting APIs to work using web, mobile, browser embeddables.&nbsp; The only innovation I can think of recently is from Zapier with their Push by Zapier solution -- allowing you to orchestrate with the zaps you've&nbsp;creative, putting APIs to work using the variety of recipes they've cooked up. I'm thinking that I will have to step up my storytelling around what is possible with Push by Zapier, helping folks understand the possibilities. Push by Zapier is a Google Chrome extension, making it more browser than embeddable, but the approach transcends embeddable, browser, and even into the conversation (bot, voice, etc.) for me. It's all about getting users frictionless access to API driven actions. Whether you are building Zapier pushes, Alexa Skills, or a Slackbot, you need to trigger, and daisy chain together API driven requests and responses ending in the desired location. I'm just looking for dead simple ways of doing all of this&nbsp;in my browser, embedded on web pages, and anywhere I desire. I'm just looking for a way to embed, link, and display the doorway to meaningful actions that anyone can implement, from wherever they want--where the action takes place is up to the API provider. I want a vocabulary of simple and complex embeddables,&nbsp;either just HTML, some JS, and maybe a little CSS magic to round off. When I explain to someone in a Tweet or email explaining that they can publish a news article to a Google spreadsheet, I want my sentence to accomplish what I'm trying to articulate. I want to be able to speak in API--I...[<a href="/2017/02/08/api-embeddables-in-a-conversational-interface-world/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/07/maintaining-on-premise-capacity-as-well-as-cloud-expertise/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/beachclouds_clean_view.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/07/maintaining-on-premise-capacity-as-well-as-cloud-expertise/">Maintaining On Premise Capacity As Well As Cloud Expertise</a></h3>
			<p><em>07 Feb 2017</em></p>
			<p>The "cloud" has done some very interesting things for individuals, companies, organizations, institutions, and government&nbsp;agencies, and is something that shouldn't be ignored. However, I watch organizations of all shapes and sizes make a similar mistake when it comes to outsourcing too much of their operations to vendors, and cloud services. Each organization's needs will be different, but technology leaders should be mindful of how they invest in talent, alongside how much they invest in external services. I struggle with this in my own business on a daily basis, but I've also seen small businesses make the same mistake, as well as witnessed the damage of this all the way up the Department of Veterans Affairs (VA) in the federal government. The VA has outsourced it's technological soul to vendors, leaving the ability to make sound architectural choices to those who control the puppet strings outside of the agency. I'm seeing a lot of smaller organizations doing this same thing, but with cloud services (outsource 2.0), instead of the traditional software vendor (outsource 1.0)--outsourcing their technological soul to the cloud. Outsourcing capacity is something I struggle with constantly. There is only so much I can do as a one-person shop, but there is also a limit on what I can spend on services in the cloud, creating a naturally occurring balancing effect. In addition to this natural effect, I am also constantly evaluating what I should be learning myself, and investing in my own internal capacity, as opposed to outsourcing it. I have to be extra careful regarding who I depend on because if service changes, is shut down, or maybe prices me out of range -- it could be pretty damaging to my operations. I am hyper aware of this vicious&nbsp;cycle when it comes to my dependence on the cloud or any single service. I was just having a conversation around the hard decisions that a school district was having to make in this area....[<a href="/2017/02/07/maintaining-on-premise-capacity-as-well-as-cloud-expertise/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/07/helping-carry-the-load-when-it-comes-to-public-data-and-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/carryload_diego_rivera1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/07/helping-carry-the-load-when-it-comes-to-public-data-and-apis/">Helping Carry The Load When It Comes To Public Data And APIs</a></h3>
			<p><em>07 Feb 2017</em></p>
			<p>I am finally getting back to my Knight Foundation funded grant work on Adopta Agency, I'm investing some research cycles into finding some tools that civic, science, journalism and other public data activists can put to use in their critical work. We've seen folks rise to the occasion when it came to climate data, helping migrate vital resources from federal government servers, something I'd like to see happen across other business sectors, as well as continue as an ongoing thing throughout this administration, and beyond. I have long been a proponent of the private sector sharing the load when it comes to managing public data and APIs. After leaving DC during the 2013 federal government shutdown I began evangelizing the importance of individuals and companies stepping up to help with the heavy lifting of making sure public data is available when we need it most--resulting in my Adopta.Agency work.  I feel pretty strongly that the federal government has an important role to play in this conversation, but I also feel that the private sector needs to step up and help--additionally, I also feel that is important that individuals step up and be present in the discussion. Github plays a central role in my Adopta.Agency work. Any government data I turn into an API lives on Github as JSON, CSV, or other machine-readable format--taking advantage fo the Github platform for managing, as well as helping me eliminate or completely reduce the costs of managing public data. I spend money each month on hosting my public data work (it's my addiction), but I couldn't do it without a place to park it and offset storage and bandwidth costs. With this in mind, I'm spending time trying to find other services that public data and API folks can put to work for them, either eliminating or reducing the cost of managing open public data on the web. My Adopta.Agency toolkit starts with Github and extends to the open source blueprint...[<a href="/2017/02/07/helping-carry-the-load-when-it-comes-to-public-data-and-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/07/dedicated-space-for-telling-the-stories-of-your-api-consumers/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_02_07_at_10.13.17_am.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/07/dedicated-space-for-telling-the-stories-of-your-api-consumers/">Dedicated Space For Telling The Stories Of Your API Consumers</a></h3>
			<p><em>07 Feb 2017</em></p>
			<p>Telling the story of what your&nbsp; API accomplishes may seem like a pretty simple, straightforward thing, but you'd be surprised how many API providers DO NOT do this on a regular basis, or do not have dedicated stories, showcase, or similar section to their website. This is why I beat this drum on a regular basis -- if you do not tell the story of the cool things people are doing with your API or your API services, they will never know how your solution works, and will probably never think of your service again--even with they actually have that specific problem that you solve. To help demonstrate this in a very meta way, I am going to showcase how my clients, showcase their clients. Deep man. Next up is my partner DreamFactory, providing six very compelling stories about how their API deployment and management platform is being put to use: TECHeGO - DreamFactory is at the core of TECHeGO's powerful ERP platform&nbsp; Verizon Cloud - Discover how DreamFactory gave Verizon Cloud a turnkey developer portal. Intel - Learn how Intel uses DreamFactory to expose legacy SQL data as a powerful REST API for AngularJS. Maxwell Lucas - Read how Maxwell Lucas uses DreamFactory to deliver a world-class travel advisory application for their enterprise companies. Senske Services - Find out how Senske Services uses DreamFactory to REST-enable Microsoft SQL Server for a mobile ticketing application. The Binary Workshop - Discover why Binary Workshop uses DreamFactory as the REST API platform for their co-working SaaS applications. Shortrunposters.com - Find out how DreamFactory enabled communication between the shop floor, a backend MySQL database, and a proprietary e-commerce portal. Honestly, this type of blog posts makes for easy content for me, and obviously, it makes my partner DreamFactory happy, but more importantly, it is yet ANOTHER reminder for you to tell the story of the people who are using your APIs and services. Telling stories is essential to...[<a href="/2017/02/07/dedicated-space-for-telling-the-stories-of-your-api-consumers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/06/what-i-would-like-to-see-from-api-providers-when-it-comes-to-public-analytics/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_02_05_at_1.14.24_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/06/what-i-would-like-to-see-from-api-providers-when-it-comes-to-public-analytics/">What I Would Like To See From API Providers When It Comes To Public Analytics</a></h3>
			<p><em>06 Feb 2017</em></p>
			<p>I'm putting some thought into the what a public analytics layer might look like for federal, state, county, and city governments. Something that looks like analytics.usa.gov, but for APIs. This is one of the things I really like about government is that you get to push forward ideas that you just can't convince folks to do in the private sector. There is no way companies will share their web or API traffic numbers publicly because there are too much smoke and mirrors involved in the process--for some reasons folks like accountability in government, but not in private sector??? API analytics are a slightly different beast than web analytics, so I wanted to step back and think about what is important to me, an API consumer, or potential API consumer when I am looking at what API does, or a group of APIs actually&nbsp;do: APIs - Depending on how APIs are grouped, if there are many APIs across the different organization, groups, or event external agencies, help me understand which APIs are available, giving me a quick snapshot of which are most used, and how they compare against each other. Paths - Within each API, which individual API paths are the most used, showing me the most used aspects of any available API. Not always a sign of popularity, as the API design may be a factor here, but it will help me see how others are using. Applications - Which applications&nbsp;are making the most use of an API. This may be a number of calls or the number of APIs they are putting to use as well. Are these applications web, mobile, analysis, visualization, a system to system integration, or something else--details about what the app accomplishes and delivers helps a lot. Industries - Which industries are applications that put an API to work servicing? Would be helpful to understand which industries are finding API resources useful, and actually getting traction. Users - I am...[<a href="/2017/02/06/what-i-would-like-to-see-from-api-providers-when-it-comes-to-public-analytics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/06/application-the-action-of-putting-something-into-operation/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/06/application-the-action-of-putting-something-into-operation/">Application: The Action of Putting Something Into Operation</a></h3>
			<p><em>06 Feb 2017</em></p>
			<p>I hate how technology dehumanizes things and went you bundle that with the current model for how things get funded, it tends to do this at scale, and with troubling efficiency. I'm the API Evangelist. I am not selling APIs as a technology solution, I am fighting to keep this sliver of our increasingly technical worlds open, and serving humans--otherwise I feel there is no hope for any of this to work with any kind of equity and compassion for the people it should be serving.
One of the reasons I blog is to help me refine the stories I tell in the API space, both virtually and in person. In an effort to make the API acronym more accessible to the masses, and also a reminder to the technorati that all of this is about doing meaningful things for the tech sector, I'm continuing to push my definition of API on the world. So, what does API stand for? Application Programming Interface. Historically application meant the&nbsp;web, and then in 2010 that started shifting to be more&nbsp;about mobile, and now it's being dominated by discussion about connecting other devices to the web, aka Internet of Things.
When I hear the word&nbsp;application, I think of the more non-technical definition of it: the action of putting something in operation. While this definition isn't perfect, it is better than simply thinking about web, mobile, and devices. It moves us closer to actually doing something useful, meaningful, functional, and actually serving humans. Sure, that operation could be in the service of systems and mechanisms that exploit and hurt people, but I feel like it gets us closer to having a conversation about why something is in operation, and not just simply about tech for tech's sake.
[<a href="/2017/02/06/application-the-action-of-putting-something-into-operation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/06/api-definitions-documentation-and-hypermedia/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/swingingbridge_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/06/api-definitions-documentation-and-hypermedia/">API Definitions, Documentation And Hypermedia</a></h3>
			<p><em>06 Feb 2017</em></p>
			<p>I wrote about what is at stake with API definitions currently and someone made a thoughtful comment on the importance of continuing to discuss hypermedia amidst all of this--I agree. I've long been an advocate for OpenAPI Spec and API Bueprint as a bridge from where we are, to where we need to be, getting us closer to the world hypermedia folks think we should have. I'd love it if every API allowed for content negotiation using one of the major hypermedia formats like HAL, Collection+JSON, or JSON-LD, but unfortunately, we have a lot of education and training ahead of us before we'll get there. A combination of APIs.json for discovery, and OpenAPI Spec for defining the request and response structure of an API can seem clunky compared to the elegant (hopefully) design of a hypermedia API, but not every API architect has the know-how, or the time and resources to always do things properly. This doesn't mean that we shouldn't strive for a better future, but it just isn't always a reality on the ground, and as easy to achieve as many hypermedia believers might envision. As the momentum picks up with API specification formats like OpenAPI, as well as the services and tooling popping up in the ecosystem we must have to keep an eye out for opportunities to make sure the bridge continues being extended to accommodate linking, relationships, and the other benefits hypermedia design patterns bring to the table. OpenAPI Spec 3.0 gives a slight nod towards what I'm talking about with the ability to add some basic linking--it falls short of true hypermedia flows, but it is one more step on the bridge I'm talking about. Technology has this amazing effect on us, that we either get too caught up in the future and forget the past, or even ignore the present, but I feel like OpenAPI Spec and other definitions are having the opposite effect where we forget the future, as well...[<a href="/2017/02/06/api-definitions-documentation-and-hypermedia/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/02/the-amazon-console-came-after-the-api-and-cli/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/aws_console.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/02/the-amazon-console-came-after-the-api-and-cli/">The Amazon Console Came After The API And CLI</a></h3>
			<p><em>02 Feb 2017</em></p>
			<p>I've spent a lot of time thinking about the Amazon Web Services ecosystem lately. I've gone through and generated OpenAPI Specs for the majority of their APIs, as well as an APIs.json&nbsp;index for the collection of valuable services. I have also written about the relationship between the Amazon API and CLI, and while doing this research I had jotted&nbsp;down thoughts about their approach to the Amazon Console. For most API providers the API is a secondary thing, implemented after their website, applications, and even mobile applications in many situations. When AWS launched in 2006 they were only API and CLI, and after a couple of years, they got to work on providing their AWS Console, which plays a pretty significant role in working with the platform. The AWS console is pretty utilitarian, and lacking in a lot of UI / UX polish, but it plays an important role in working with the platform&nbsp;and is generally in sync with what features are available via the API and CLI. I don't think there are any major lessons here, I just think it is interesting what has unfolded at Amazon, compared to how many other companies invest so much in the SaaS, and user interface portions of their operations, where AWS has definitely focused more heavily on the CLI and API, with the console only recently coming into focus. I do not think there is any 100% right way to do this, but I think making sure there are parity and consistency between what is available via a UI, and the API and CLI are important. AWS doesn't always accomplish keeping all of this in sync, but you can see they work hard at it. Most times the CLI and API docs are side by side and even interlinked, and with some of the services, they provide you with a link in the developer area to the AWS Console portion for a specific service. I think whichever comes...[<a href="/2017/02/02/the-amazon-console-came-after-the-api-and-cli/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/02/preparing-for-conversations-about-schema-definitions-and-scopes/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-conversation-bubbles.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/02/preparing-for-conversations-about-schema-definitions-and-scopes/">Preparing For Conversations About Schema, Definitions And Scopes</a></h3>
			<p><em>02 Feb 2017</em></p>
			<p>I am focusing heavily on schema, definitions, and scopes in 2017, because it is the most important layer in the tech sector, the API space, and is something that touches almost every industry, while also reaching into our personal worlds. I'm working on refining my argument in 2017 that I'm not selling APIs as a solution all by themselves, I'm pushing APIs to help us tame this insane beast that we've let out of the closet, and will never be able to put back in. SchemaWhether it is JSON schema, MSON or Data Packages, current approaches to defining the data used as part of each API request or response are defining what has become to be known as the API economy (for good and bad). The schema describes the digital bits that are being created and moved around online today. They are the videos, images, blog posts, messages, and other digital elements that make up our online business and personal worlds.  Defining schema are critical to platforms working properly, and having them in a machine readable way allows for them to be shared between platforms, providers, and applications. Defining these bits allows us to have a conversation about these bits, which can also open up the conversation about whether or not we should be collecting, storing, and sharing these bits in the first place. I know many tech people don't want to have these conversations outside their inner circles, but we should--it is important for all of this to work for everyone. DefinitionsAPI specification formats like OpenAPI Spec and API Blueprint have emerged to give us a way to define the surface area of APIs that are used across web, mobile, and device applications. These specification formats are allowing us to define, describe, and have a conversation around the valuable data, content, and algorithmic resources being made available via APIs using the web. The resulting API definitions are currently being used by companies as part of every...[<a href="/2017/02/02/preparing-for-conversations-about-schema-definitions-and-scopes/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/01/including-end-users-in-the-conversation-about-their-bits-being-sold/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_31_at_2.29.16_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/01/including-end-users-in-the-conversation-about-their-bits-being-sold/">Including End Users In the Conversation About Their Bits Being Sold</a></h3>
			<p><em>01 Feb 2017</em></p>
			<p>Fitbit recenttly announced a program to pay their wearable users up to $1500 for&nbsp;integrating their Charge 2 into the&nbsp;UnitedHealthcare Motion&nbsp;program powered by Qualcomm Life&rsquo;s 2net Platform. The "UnitedHealthcare Motion is an employer-sponsored wearable device&nbsp;wellness program&nbsp;that offers financial incentives for enrollees who meet daily step goals". Pulling back the curtain just a little bit on the value of your Internet of Things data, and specifically the devices you strap to your body. I am not a fan of corporations strapping devices to their employees as part of these wellness programs (or for any reason), and using cash incentive to achieve the desired behavior. I feel this is a doorway to some pretty dark human resources strategies, but I do think these events pull back the curtains on what is going on, even just a little bit for users. I am sure it's not Fitbit's intention to include end-users in all of their monetization of their data, but I see this as an opportunity to educate end-users in these situations. Most Internet users are not aware of the amount of information being gathered, bought and sold when they use the Internet, and their mobile phones. This lack of awareness is translating pretty nicely to the world of connected devices, adding some valuable demographic dimensions, to an already valuable user profile. While insurance companies are interested in improving their margins with this data, they are also interested in the new revenue streams they can create by selling data to other brokers, hedge funds, and more.&nbsp; One of the only hopes I have in this area is that startups will continue to pull back the curtain on this behavior either intentionally or unintentionally with products, services, and programs like the wellness program that Fitbit is offering. Showing users that there is value in their data, and this can be a positive first step in educating them about what is happening in the tech world. Once they get a taste...[<a href="/2017/02/01/including-end-users-in-the-conversation-about-their-bits-being-sold/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/01/amazon-dash-ok-idea-dumb-implementations/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/1027205_health_and_personal_care_project_snake_always_accessible_v2_1920x1080.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/01/amazon-dash-ok-idea-dumb-implementations/">Amazon Dash: Ok Idea. Dumb Implementations</a></h3>
			<p><em>01 Feb 2017</em></p>
			<p>The AWS IoT Button, based on the Amazon Dash Button hardware, was kind of sorta an interesting model, allowing you to trigger virtual things with a physical click of a button, but now they've virtualized their approach, which I guess is a decent enough of an idea (not new), but their implementation is just not that smart. I think they just went from virtual to physical, and back again that they kind of got whiplash, and didn't really think it through before launching. I'm not big on bashing people's technology implementations, as I would rather focus on shining a light on what is progressive in the space, but the area of embeddable tooling built using APIs has suffered so much in the last couple years, I'm not keen on big providers further sucking the oxygen out of the room--AWS can do better. Having a big branded virtualized button with a&nbsp;click for more info on one side, and purchase if you click on the other side seems like something your brands suggested, and not your actual consumers or developers. I can see the physical AWS IoT button getting some traction, even though the majority of the implementations will be cheezy. Making virtualized buttons should behave like the physical thing seems kind of useless. It seems like AWS could have designed a suite of well-designed HTML + JS button and bookmarklets that engineers can make very useful on the web as well as in a mobile environment. The branding, analytics, and overall experience could make things much more interesting&nbsp;than what you have now, and brand partners could get to work wiring these experiences up to make some pretty valuable API calls, and even daisy chain together multiple API calls across providers. API powered embeddable buttons is one of the areas of the API space that has left me underwhelmed. After we saw significant movement from Twitter and Facebook with some of their embeddable social buttons, there wasn't...[<a href="/2017/02/01/amazon-dash-ok-idea-dumb-implementations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/02/01/a-limited-medium-api-means-i-do-not-always-curate-what-is-published-there/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/medium.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/02/01/a-limited-medium-api-means-i-do-not-always-curate-what-is-published-there/">A Limited Medium API Means I Do Not Always Curate What Is Published There</a></h3>
			<p><em>01 Feb 2017</em></p>
			<p>One of the deciding factors of whether or not I put a new online service to use in my business depends on whether or not they have an API. Sometimes I have no&nbsp;choice in the matter, but if I have any say, a service must allow me to move data in and out of their system programmatically, keeping in sync with my own systems, otherwise I will not adopt the service as part of my regular operations. One platform I have integrated into my operations is the blogging platform Medium. I handpick some of my content for publishing to Medium, pushing to my account from my own management system using their publishing API. This works well for me, as I require that all of my content, images, and videos are created and originate in my own systems, and then syndicate to other platforms secondarily via APIs--the only downside is that I can't actually pull data from Medium via their API into my management system. The lack of a read API for Medium content doesn't really hurt me, as I employ a POSSE philosophy, but it does hurt other publishers to the platform because I do not curate their content on Medium on a regular basis. I have to manually go to the Medium.com domain to read API related posts (unless it's a known blog RSS feed), which is something I do not always have the time for. I'm will always prioritize content from platforms that allow me to pull into my news curation systems via RSS, Atom, and APIs, and when I have time I will visit siloed platforms like Medium. I understand that Medium is concerned about control and monetization of their platform, but a more complete and thoughtful API strategy that involves read and write APIs, embeddable solutions, and considering other leading content API strategies would actually benefit them, as well as their publishers. Not doing this will actually hurt publishers like me....[<a href="/2017/02/01/a-limited-medium-api-means-i-do-not-always-curate-what-is-published-there/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/31/the-importance-of-apis-in-journalism-right-now/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-news-icon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/31/the-importance-of-apis-in-journalism-right-now/">The Importance Of APIs In Journalism Right Now</a></h3>
			<p><em>31 Jan 2017</em></p>
			<p>
APIs are playing an increasing role in all aspects of our public life. Our current president has set the precedent that he will be using Twitter as a primary communication channel, cutting off traditional media and other channels--amplifying the importance of the Twitter API when it comes to doing your job as a journalist.
Journalists don't just need to be plugged into to major platform channels like Twitter, Facebook, Instagram, and others, they also need to be able to conduct research using these platform APIs. Journalists should be fluent in synchronous, and asynchronous pulling of social media and other data via leading APIs. Whether it's pulled through custom programming or using existing tools and services, successful journalists will have a robust toolbox for meeting their needs in this area.
Inversely it will be increasingly important that API platforms consider journalist access to APIs, and consider crafting specific service composition, rate limits, and plans for this level of access. Platforms should also be investing in and incentivizing their 3rd party developers to help develop tooling that aid journalists in their work. Ideally, these solutions would be open source&nbsp;or at least think deeply about monetization strategies in the context of access in support of these activities.
The current political climate should remind us in the API sector that APIs are not just about developing the next generation of startup applications, and are also about enabling end-users, as well as power users like journalists. If you need help thinking through journalist access to a platform, feel free to reach out, and I'm happy to help you think through this level of access from onboarding, to rate limits and the design of your APIs. This stuff is important, and I'm happy to help.
[<a href="/2017/01/31/the-importance-of-apis-in-journalism-right-now/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/27/if-you-are-doing-interesting-things-with-apis-please-tell-the-story/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-amplify-audio.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/27/if-you-are-doing-interesting-things-with-apis-please-tell-the-story/">If You Are Doing Interesting Things With APIs Please Tell The Story</a></h3>
			<p><em>27 Jan 2017</em></p>
			<p>
I wish I could write about everything interesting that is going on in the API space, but as a one man show, I struggle to keep up with discovering, reading and understanding what is going on, let alone carving out the time to write something thoughtful about it. Many interesting things get added to my notebook, but I simply do not find the time to finish and publish a story--sadly.
I'm always appreciative of folks who email me ideas for stories based upon interesting things with APIs, but it still doesn't always mean I will find the bandwidth to craft a post. What I really enjoy is when folks ping me about something interesting, but they also write their own story on their own domain, and share via multiple channels like Twitter, LinkedIn, etc. When you do this, it allows me to more easily amplify what you are doing--the chances i will see it are greater.
Even when a company writes about something, I can still choose to augment and add to what has already been written about a subject, via my own blog. Then if I don't have time to craft something I can still amplify what has already been written by a company on Twitter, LinkedIn, and other channels I publish on regularly. I know many companies rely on PR channel to distribute their message, but I don't think this should be instead of publishing on their own blog. Telling stories on your companies blog, reaching out via PR channels, and relying on analysts like me to amplify what you publish makes for&nbsp;a more complete storytelling strategy for your API.
[<a href="/2017/01/27/if-you-are-doing-interesting-things-with-apis-please-tell-the-story/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/27/i-got-a-response-regarding-my-facebook-threat-api-access/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_26_at_10.26.12_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/27/i-got-a-response-regarding-my-facebook-threat-api-access/">I Got A Response Regarding My Facebook Threat API Access</a></h3>
			<p><em>27 Jan 2017</em></p>
			<p>I am pushing forward my security research, and profiling what threat information APIs and platforms are up to. I rarely ever dive into any API without actually signing up for an API, getting some keys, and actually get to work making API calls. I have come across a number of APIs that are just fronts&nbsp;so that they can get in ProgrammableWeb directory, or just issue a press release that they have an API, so I usually prefer to fire things up and validate an API does what is being advertised. There is no better way to truly get to know an API than to actually make API calls against it and get to work doing some integration. While profiling the Facebook ThreatExchange API I did what I normally do--requested some keys. The platform doesn't allow for self-service access, so I had to wait for a response, which came as an email a couple days later: Hello Kin, Thank you for applying to ThreatExchange! Currently, we are in beta and focused on solving the challenges of companies with dedicated abuse detection or incident response teams seeking to share threat intelligence. At this moment, we don't feel the product is ready for your use-case, but we hope it will be in the near future and will reach back out once it is. We appreciate your patience and understanding. Please reach out to us if you feel this message is in error or if you have ideas on how we can best support your interest(s). Best, Facebook ThreatExchange Team I do not fault companies for not giving away instant self-service access to their API resources. There are plenty of badly behaved 3rd party developers out there. I do encourage them to try and consider other uses cases beyond just their partner implementations, and 3rd party developer integrations. Journalists, researchers, regulators, and analysts&nbsp;like me all need access&nbsp;and can bring a variety of benefits to the platform beyond integrations. When...[<a href="/2017/01/27/i-got-a-response-regarding-my-facebook-threat-api-access/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/27/funding-the-development-of-an-api-ranking-solution/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/API-plans-API-pricing-API-rating.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/27/funding-the-development-of-an-api-ranking-solution/">Funding The Development Of An API Ranking Solution</a></h3>
			<p><em>27 Jan 2017</em></p>
			<p>I have written before about how we are going to create the Standard and Poors or Moodys for the API industry, and how an API ratings could be used as an economic engine. This is a topic I have folks reach out to me about regularly, wanting to create such a rating system, for a variety of business and political reasons. It is something I'd like to continue to get ahead of before someone who is&nbsp;eviler than I am (I am pretty evil), decides to set something in motion that doesn't include me (ego). I may sound elitist saying this (I am), but there are very few people who get the API landscape at this level, understand the scope of this challenge, and can have a productive conversation about how to do this. Within 60 seconds of each Skype (yeah Skype)&nbsp;call I have with individuals about an API rating system I can tell which industry they come from&nbsp;and their understanding of what actually affect the quality of APIs--across the technical, business, and politics layers of operations. You have to have been in the space a while, and seen the positive and negative outcomes of API operations to really get the nuance of how you rank and rate API operations in a meaningful way. Developing an API rating and ranking system will take a lot of work, the involvement of the right folks, money, and time. This is an area that is floating higher up on my priority list and will be actively working to solidify&nbsp;a vision, before some of the folks I have talked with, ever get their way. The only existing model for this out in the open currently is from my friends at the API Rating Agency. They are some fo the handful&nbsp;of folks who I actually trust to execute on a vision of this importance and scale, but alas they ran into some of the common challenges around the scope of making...[<a href="/2017/01/27/funding-the-development-of-an-api-ranking-solution/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/25/what-is-at-stake-with-api-definitions-at-the-moment/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-cards-playing.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/25/what-is-at-stake-with-api-definitions-at-the-moment/">What Is At Stake With API Definitions At The Moment</a></h3>
			<p><em>25 Jan 2017</em></p>
			<p>I wrote angrily about Oracle's acquisition of Apiary last week, and this week I find myself deeply considering the API definition landscape, so I wanted to take another look at this event from the 100K view. In 2017, API definitions are touching every aspect of the API lifecycle, from design to deprecation,&nbsp;and are becoming key to defining, automating, and evolving many different industries from cloud computing to human services.&nbsp; I define API definitions as the specifications, schema, and scopes that are being used to map out the world of APIs. Specifications for describing APIs are nothing new, and approaches to defining data schema are well established as well. However by 2012 things were changing and Swagger emerged as an important tool for describing APIs in JSON, then YAML, using JSON Schema to define the underlying data definition. Around the same time, Apiary got to work on their own API Blueprint for describing APIs, and MSON for helping define the data and the relationships in play. Then, not to be left behind, Mulesoft "jumped" into the game with RAML as well--there are others, but this represents the top three. In 2016 Swagger was put into the Linux&nbsp;Foundation, under the&nbsp;Open API Initiative (OAI) and was reborn as the OpenAPI Spec, something that in 2017 is beginning to bear fruit with version 3.0 of the specification. Adding another dimension to this shift over the last year, we just saw Oracle purchase Apiary, and since Apiary is currently in the OAI, this means that Oracle is now in the OAI. I'm sure it is just a matter of time before Mulesoft also makes the move to join the OAI, but I'm not sure what this would mean for RAML--or what it means for API Blueprint. I do know OpenAPI Spec is the dominate format right now, and OAI is the leading group. I want to pause for a moment and also point out that Oracle is the company that...[<a href="/2017/01/25/what-is-at-stake-with-api-definitions-at-the-moment/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/25/the-openapi-specification-version-3-0-highlights/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/openapi_spec_structural_improvements.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/25/the-openapi-specification-version-3-0-highlights/">The OpenAPI Specification Version 3.0 Highlights</a></h3>
			<p><em>25 Jan 2017</em></p>
			<p>I am impressed with the work that the Open API Initiative (OAI) working group has accomplished with the&nbsp;version 3.0 release of the OpenAPI Specification. I have had zero involvement in moving the specification forward (something I'm changing), and after coming back to the effort I am impressed with what they've prioritized, and accomplished for this release.&nbsp; The highlights in version 3.0 of the OpenAPI Spec for me are: Components - The new components architecture&nbsp;really reflects "APIs" in my opinion, making things modular and reusable. Body - Catching up when it comes to allowing the body to be defined separately from the headers&nbsp;and parameters. Content Negotiation - You can now define content objects to define the&nbsp;relationship between response objects, media types, and schema. Linking - It isn't hypermedia, but it is definitely a nod towards hypermedia, allowing the linking of objects. Webhooks - You can now define callbacks that can be attached to a subscription operation describing an outbound operation. Schema - Increased investment in JSON Schema, including support of `oneOf`, `anyOf` and `not` support, as well allowing for alternative&nbsp;schema now. Hosts - You can now have multiple hosts, allowing you to more narrowly define the host for each path. Examples - Allows you to better describe, and provide examples of APIs responses and requests. Version Identifier - Not a big one, but removing the swagger: "2.0" identifier--it will now just be&nbsp;openapi. Cookies - I'm not a big fan of this being introduced, but it makes sense, and I'm sure is usable for many API operators. The OAI blog provides a five-part series covering the version 3.0 release. These ten areas are the highlights for me. I think they nailed it as far as what was needed, while also pushing into areas like linking and webhooks that I hadn't anticipated. I am looking forward to playing with converting some of my 2.0 specs to be 3.0 compliant--once I am a little more intimate with it, I...[<a href="/2017/01/25/the-openapi-specification-version-3-0-highlights/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/25/pull-the-social-media-accounts-for-gov-using-the-us-digital-registry-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_24_at_11.36.10_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/25/pull-the-social-media-accounts-for-gov-using-the-us-digital-registry-api/">Pull The Social Media Accounts For Gov Using The US Digital Registry API</a></h3>
			<p><em>25 Jan 2017</em></p>
			<p>Over the holidays I pulled the data.gov index of federal government data, and the next item on my list was to cache the results of the&nbsp;US Digital Registry API&nbsp;, providing me with a list of agencies, and their social media accounts. I pulled the JSON from the API, and then published to the Github repository for this site, so that I could use for several different applications. Drive Listings Of Federal Agency Social MediaI wanted a quick way to get at the Twitter and Github accounts for the federal government, and have in a single location (Github). I've published YAML data to Github, and using Jekyll I've created listings for the Twitter, Github, Facebook, Pinterest, Instagram, and LinkedIn accounts, making them easier to browse when I need them. Accessible In Machine Readable Format For OthersI have several uses for the listing of government agencies and their social media accounts. I needed the data in a machine-readable format on Github so that I could pull remotely, and even check out the Github repository if I need to. This approach also makes the data available for anyone else to fork and put to use in their own way using Github. Profile The Agencies Using Each Of The Social APIs&nbsp;Now that I have the social media account for these federal gencies&nbsp;pulled using the US Digital Registry API, I'm going to take each of the accounts, and pull their profile details, and any other relevant information using the APIs for each of the social media services. APIs for the win--providing me with a way to quickly profile the US federal government at scale, and stay in tune either real-time or when I think a checkup is required. I'm not sure what I'll do next on this project. I'd like to take a look at how active each account is, and maybe check in each week to see if any accounts have gone dormant, picked up in activity, or possibly...[<a href="/2017/01/25/pull-the-social-media-accounts-for-gov-using-the-us-digital-registry-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/24/making-your-api-portal-speak-to-the-widest-possible-audience/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_23_at_11.55.57_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/24/making-your-api-portal-speak-to-the-widest-possible-audience/">Making Your API Portal Speak To The Widest Possible Audience</a></h3>
			<p><em>24 Jan 2017</em></p>
			<p>I have the first draft of a developer portal&nbsp;ready for an API project I am working on, and before I move forward polishing it too much I wanted to step back and think about the goals behind the launch of this API portal, and the intended audience I'm targeting with its operation. I do not want this landing page to just speak to developers, I would like it to speak to as wide of an audience as possible.&nbsp; This particular API portal is meant to support the development of human service websites, mobile, and other applications in the Miami area. I do not want to scare off the "normals" with the home page. I want to make sure the language I use speaks to API newbies, while also giving experienced developers what they need to get up and running&nbsp;and solve any problems. While not all APIs will be targeting this wide of an audience, but when it comes to human services at the municipal level, we need to be reaching as many people as we can. I am looking for the developer area to speak to journalists, policy makers, analysts, interns, and even just data curious citizen activists. I think I have all the essential building blocks of an API portal present, things like getting started, documentation, and support, now I just need to polish the titles, text, and other elements so that they are as plain English&nbsp;as they possibly can. As I do this I shouldn't be assuming that a user will ever be actually integrating with an API, they might just need to understand the concept, what is possible with the API, and then engage with someone else who will actually be doing the technical heavy lifting. I'm going to have to put down the technical work on this project for a day or so&nbsp;before I can write content that speaks to non-developers--I am just too close to the code right now. I...[<a href="/2017/01/24/making-your-api-portal-speak-to-the-widest-possible-audience/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/24/loss-of-primary-and-foreign-keys-translating-from-data-package-to-openapi-spec/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_database_relationships.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/24/loss-of-primary-and-foreign-keys-translating-from-data-package-to-openapi-spec/">Loss Of Primary and Foreign Keys Translating From Data Package to OpenAPI Spec</a></h3>
			<p><em>24 Jan 2017</em></p>
			<p>I am keeping a version of an OpenAPI Spec in sync with a Data Package. It's not a perfect sync because the Data Package doesn't describe the surface area of the API, just the underlying data schema used on the backend. During project discussions, one of the concerns that was brought up focused on the loss of primary and foreign key references. During our next discussion, I wanted to have a more coherent explanation of why this was ok, and this post will help me do that. The OpenAPI Spec I've created has each resource in the Data Package&nbsp;represented but leaves out the database relationships represented by those keys in the backend. The API defines the basic CRUD (Create, Read, Update and Delete) for each resource represented, but allows the relationships&nbsp;to be expressed using the URI. Each item in the Data Package has a corresponding path, and each relationship is available as its own path as well--in this case an example might be /locations, and /locations/services/. All the relationships are defined and enforced in the URI given for each API requests, and HTTP takes care of the indexing, performance, and other considerations using caching, and other basic building blocks of the web. My friend James Higginbotham (@launchany) compared this to the concept of views in database backend speak, and OpenAPI Spec describes the HTTP version of&nbsp;OCI (oracle), or TSQL (MS SQL)--depending on what you speak. As an old database guy I like that, "web views", but relying on the request and responses employed as part of the API design. My explanation isn't as coherent as I'd like, but this gives me a start. I'm trying to help database folks who are keepers of the backend, and the schema better understands that what I'm doing with OpenAPI Spec augments and evolves their work. I do not want them to think I am looking to replace or compete with what they are bringing to the table. I'll...[<a href="/2017/01/24/loss-of-primary-and-foreign-keys-translating-from-data-package-to-openapi-spec/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/24/amazon-alexa-uses-http2/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-http2.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/24/amazon-alexa-uses-http2/">Amazon Alexa Uses HTTP/2</a></h3>
			<p><em>24 Jan 2017</em></p>
			<p>I track on the different approaches used by API providers so that I know where to find examples of leading approaches to API design and deployment. Then I write about them so that I have something to reference across my research. I keep an eye out for API providers who employ hypermedia as part of their API design, as well as companies who are putting HTTP/2 to work as part of their design and deployment. The Amazon Alexa Voice Service API employs HTTP/2 as part of their voice-enablement platform. I'm still learning about HTTP/2 so I was pleased to see the amount of education they provide in their documentation, outlining some of the key terms and concepts at play; Frame:&nbsp;The basic protocol unit in HTTP/2; each frame serves a different purpose, for example, HEADERS and DATA frames form the basis of HTTP requests and responses. Stream:&nbsp;An independent, bi-directional sequence of frames exchanged between a client and server within an HTTP/2 connection. For detailed information, see&nbsp;Streams and Multiplexing in RFC 7540. Interfaces:&nbsp;AVS exposes interfaces (SpeechRecognizer, AudioPlayer, SynchronizeState, etc.) that provide your product access to Alexa&rsquo;s built-in skills. Downchannel:&nbsp;A stream you create in your HTTP/2 connection, which is used to deliver directives from the cloud to your client. The downchannel remains open in a half-closed state from the device and open from AVS for the life of the connection. The downchannel is primarily used to send cloud-initiated directives and audio attachments to your client Cloud-initiated Directives:&nbsp;Directives sent from the cloud to your client. For example, when a user adjusts device volume from the Amazon Alexa App, a directive is sent to your product without a corresponding voice request. They also provide details on crafting the&nbsp;HTTP/2 Message Headers, how to construct the&nbsp;HTTP/2 Multipart Messages, and what to expect with&nbsp;HTTP/2 Responses. They have language SDKs in C / C++(nghttp2), C / C++(curl and libcurl), Java (OkHttp), Java Netty), Java(Jetty). I am particularly interested in learning more about how the...[<a href="/2017/01/24/amazon-alexa-uses-http2/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/23/no-database-behind-an-api-and-just-using-files-stored-on-github/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-github-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/23/no-database-behind-an-api-and-just-using-files-stored-on-github/">No Database Behind An API and Just Using Files Stored on Github</a></h3>
			<p><em>23 Jan 2017</em></p>
			<p>It is common for an API to just be a facade for a database. Meaning the data, and content served up via the API is read from and written to a database backend. This is probably the most common way to deploy an API, but increasingly I'm working to eliminate the database behind, and storing the content or data being served up via Github repositories.&nbsp; I find it easier to store individual YAML, JSON, and other machine readable files on Github, and just check out the repository as part of each API deployment. Each API has a different refresh rate determining how often I commit or pull a fresh copy of the content or data, but the API does all of its work with a locally checked out copy of the repository. Eliminating the need for a database backend from the required components to make the API operate. Why am I doing this? It helps me solve the database challenges when it comes to deploying in containers, and other more modular approaches to deploying APIs as microservices. The API provides a (hopefully) well-designed&nbsp;facade for the data and content stories, and allows me to use my verbs when reading, writing, and managing resources behind. It also injects the benefits of version control, and user and organizational engagement that Github brings to the table. I'm also using it in on-demand approaches to working with data. I have&nbsp;a lot of government, and other open data stored in Github repositories (free if public), and when I want to work with it, I can spin up a new instance or container, which checks out the latest Github repository, and provides access for reading and writing using a Github OAuth token. When done, the API can be terminated, committing&nbsp;any changes back to the repository, reducing the need for dormant compute resources. This approach also centralizes the data publicly on Github, allowing anyone else to check out and integrate with the JSON...[<a href="/2017/01/23/no-database-behind-an-api-and-just-using-files-stored-on-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/23/considering-the-logging-and-observability-layer-for-amazon-alexa-enablement/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/alexa_history.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/23/considering-the-logging-and-observability-layer-for-amazon-alexa-enablement/">Considering the Logging and Observability Layer for Amazon Alexa Enablement</a></h3>
			<p><em>23 Jan 2017</em></p>
			<p>I am going through the Amazon Alexa platform, profiling it as part of my voice API research, and also thinking deeply about the impact voice-enablement, and conversational interfaces will or will not make in our lives. I've been pretty vocal that I am not too excited about voice-enablement in my own world&nbsp;but it is something&nbsp;I understand other folks are, and I'm also interested in Amazon's approach to operating their platform--making it something I'm digging deeper into.&nbsp; I do not own any of the voice enabled&nbsp;Amazon devices, but I am playing with their simulator Echosim.io. I'm not interested in building any skills&nbsp;or applications for Alexa, but I am curious to learn how the platform functions, so I will be developing prototypes&nbsp;so that I can understand things better. One of the things I'm looking to understand as I'm getting up to speed is how the logging for the platform works, so I can evaluate the observability of the voice platform&nbsp;from developers, as well as an end-user perspective. From a developer perspective, I see that you have access to device&nbsp;synchronize state, user inactivity, and exceptions encountered via the Alexa Voice Service System Interface, and from an end-user perspective, there is a history section under device settings. It is a decent start from a logging perspective, but I'm thinking that transparency and observability at this level are going to be important to establish trust between end-users and the platform. With a lot of the speculation around the ambient capabilities of these devices, I'm thinking a robust logging, reporting, and auditing layer to Alexa will be critical to making folks feel like it is something they want in their homes. I'm thinking through the logging layer of my drone platform usage, and what is available to me as end-user, and developer. Alexa provides me another dimension of this logging layer, but this time in the service of voice-enablement. Further rounding off my view of what should be available when it...[<a href="/2017/01/23/considering-the-logging-and-observability-layer-for-amazon-alexa-enablement/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/23/adding-the-webhoseio-search-api-to-stack-of-apis-i-depend-on/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_22_at_8.53.04_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/23/adding-the-webhoseio-search-api-to-stack-of-apis-i-depend-on/">Adding The Webhose,io Search API To Stack Of APIs I Depend On</a></h3>
			<p><em>23 Jan 2017</em></p>
			<p>I have been looking for a decent search engine API to help me uncover new sources of information across the API space. I've always been frustrated with the APIs in this category since all of the good Google search APIs went away. I need to search the web, and specifically for blog and news posts with API related insights. In an effort to find a suitable solution I recently came across and spent time digging into one called&nbsp;Webhose.io, primarily because they had an APIs.JSON file. Webhose.io integrates data from hundreds of&nbsp;thousands of global online sources in the following areas: Social Media Data&nbsp;- Structured data from top social media sites.&nbsp; Forum Monitoring&nbsp;- Post data from across many forums. Blog Search&nbsp;- Pulls the latest from blogs even if no RSS feed. News Search&nbsp;- Access to news from PR sites that don't usually have a feed. Web Search&nbsp;- Gives me access to the latest content from many domains on the&nbsp;web. I'm not a big fan of forum monitoring (too much noise), but access to social media, blog, news, and web&nbsp;data and content is extremely valuable to me. Webhose.io indexes quite a few domains, and I was able to find #mce_temp_url#apievangelist.com, and my girlfriends hackeducation.com in there. Webhose.io allows you to query across categories, using a keyword, and by domain, and they also allow you to set how many days back you'd like to search (up to 30 days)--making it great for staying in tune with the latest content.&nbsp; I like the API mostly because it gets straight to the point, allowing you to quickly sign up for an account, easily craft API calls, then provides valuable, relevant data and content, enriched with tags, sentiment, and other goodies. They also provide a straightforward business model, allowing you to get 1000 free API calls per month, and incremental&nbsp;monthly packages with different quantities from there. I will be wiring up the API to my monitoring system, adding Webhose.io to the stack...[<a href="/2017/01/23/adding-the-webhoseio-search-api-to-stack-of-apis-i-depend-on/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/20/reducing-our-hard-work-to-a-transaction-with-apis-and-serverless/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-fist.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/20/reducing-our-hard-work-to-a-transaction-with-apis-and-serverless/">Reducing Our Hard Work To A Transaction With APIs and Serverless</a></h3>
			<p><em>20 Jan 2017</em></p>
			<p>I'm thinking about cloud pricing after my profiling of over 60 of the AWS API resources, as I play with building tools on Algorithmia, and evaluate a variety of serverless options. As someone who is regular blindsided by the devious undercurrents of business, while I'm busy focusing on shiny technological objects, I can't help but feel like us developers are contributing to the commoditization of our (currently) valuable skillset when it comes to APIs, microservices, devops, and serverless. This isn't exclusive to these areas of technology, and I think it is something we've all set into motion with APIs and microservices over the last decade. We are decoupling some very complex, and often large codebases and dependencies that take a certain amount of experience and skills to tackle, and reducing down to individual reusable resources that are automatically scaled, and may not require as many advanced skills to daisy chain and connect together. I'm all for doing this in the spirit of enabling the average business person to accomplish what they need on a daily, but from my experience in the savviness, and deviance of business leaders, I can't help but feel that eventually developer's labor will be devalued in this environment.  If we succeed, all company resources are available via APIs and microservices and all events and actions are present as "serverless" technology, and each individual resource will have a price tag, opening everything up for commoditization and extreme competition--this is good right? There will still be room for a handful of high paid architects in this environment, but new pieces of code will be able to be developed in isolation, at a very reduced price, with a huge amount of competition driving what developers will be paid way down. We are ultimately working against ourselves in this environment, and our best interest by achieving the current vision of "scale".  I do not think that the technology being employed (API, microservices, devops, and serverless) will be entirely...[<a href="/2017/01/20/reducing-our-hard-work-to-a-transaction-with-apis-and-serverless/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/19/the-state-of-california-drinking-water-program-repository/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_19_at_11.20.03_am.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/19/the-state-of-california-drinking-water-program-repository/">The State of California Drinking Water Program Repository</a></h3>
			<p><em>19 Jan 2017</em></p>
			<p>
One of the side projects I work on regularly is focused on moving forward the conversation around water data. My next wave of work is targeting the State of California&nbsp;Drinking Water Program Repository, and help&nbsp;make some of the valuable spreadsheets and CSV files more usable. Here are the six datasets I'm targeting for processing in coming weeks:

Annual Report Form - PDF of input form used for the Annual Report.
Annual Reports - Excel book of Annual Reports of public water systems from 2011 through 2015.
SDWIS Public Water Systems - CSV of active public water systems including contact, location, water source, and type.
Small Water Supplier Conservation Reports - CSV of small water suppliers reporting conservation results from June through November 2015.
Urban Water Supplier Conservation Reports - CSV of urban water supplier drought monitoring reports from 7/14 to present.
Water Quality Master Results - Zipped CSV file containing 6 M water quality results from 2013 to present including facility name, MCL, DLR, sample date, finding, and water system.

When it comes to public data there are few datasets that are going to as valuable as water data in coming years. I'd put healthcare and education at the top, but water is only going to become increasingly important as the world continues to evolve. If you have an interest in water data let me know, as I could use some help processing these important data sets.
[<a href="/2017/01/19/the-state-of-california-drinking-water-program-repository/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/19/oracle-acquiring-apiary/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/gorilla_red_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/19/oracle-acquiring-apiary/">Oracle Acquiring Apiary</a></h3>
			<p><em>19 Jan 2017</em></p>
			<p>Oracle has purchased API design provider Apiary. I'm a big fan of what Apiary does, and what the team has accomplished. I don't trade in Silicon Valley currency, so I'm not going to congratulate the team on their exit. For me, it is just a reminder of how we can't have anything nice in the space. No matter how good your team is, or how good your product or services are, the thousand&nbsp;pound gorillas will always come in the room and fuck things up.&nbsp; I am bummed about the acquisition of Apiary because they are essential to my API design origin story. Jakub, Z, and the Apiary team made API definitions to be more about API design&nbsp;than just API documentation. Pushing the conversation earlier on in the API lifecycle, opening up the concept that API definitions could be used for not just documenting your APIs after they are live, and all about design early on in the process, something that opened up for use across every stop along the API life cycle. This resulted in API definitions being core to mocking, deployment, management, SDKs, testing, monitoring, and much, much more. #thankyou Apiary was something I was proud to support in the API sector--Oracle is not. The negative mark this company has left in the space far outweighs anything good they could possibly do through acquisitions or any new solutions. With this post, I will get the usual waves of people telling me this is "just business", which is code for how people fuck each other over under their capitalist religion. So, go ahead and tell me how I live in a fantasy world, when in reality it is you who live in a fantasy world where it isn't about producing a good product, or service, it is just about your love of money, quest to be rich, and your willingness&nbsp;to allow for good things (and people) to be screwed over along the way. Over the...[<a href="/2017/01/19/oracle-acquiring-apiary/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/19/focusing-on-common-api-definitions-schema-scopes-and-specifications/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-gears.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/19/focusing-on-common-api-definitions-schema-scopes-and-specifications/">Focusing On Common API Definitions, Schema, Scopes and Specifications</a></h3>
			<p><em>19 Jan 2017</em></p>
			<p>The API universe is rapidly expanding as more companies, organizations, institutions, and government agencies are sharing data, content, and algorithms using web APIs. It has expanded so much in the last year that I can't keep up with everything that is going on. I can't test new APIs, and the emerging services and tooling from providers who are targeting the space fast enough--that is ok, I'm not sweating it.&nbsp; However, I do have to prioritize and focus on the areas where I can make the biggest impact when it comes to my understanding, and when it comes to helping the API community. While I will still be maintaining a general awareness of all technologies in 2017 I'm going to be heavily focused on three areas: API Definition - The machine-readable definition of an API interface, security and data models. Data Schema - A JSON schema, MSON, or other machine-readable description of data schema. OAuth Scope - Shared definitions of OAuth scopes in play for an API and it's resources. Web Specifications - Web concepts and specifications that make the web go around. Code plays a significant role, and specific implementations are important, but for me, these four areas represent the core of the API space. Defining, employing, discussing, and sharing in these areas is how the API space will scale, and meet the needs of specific industries. Investing in these areas will make the API sector healthier, stronger, and make an impact when it comes to combatting much of the focus on investment, and proprietary of technology that works against things being truly interoperable reusable. In 2017 I'm focusing the majority of my work in these areas. These are not the areas where the money will be in the future, but it will be what enables all the areas that do make startups money and make a difference on the ground at small businesses and the enterprise. Investment in common definitions, schema, scopes, and specifications...[<a href="/2017/01/19/focusing-on-common-api-definitions-schema-scopes-and-specifications/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/17/when-we-lose-trust-in-the-reporting-numbers-our-providers-feed-us/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-trust.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/17/when-we-lose-trust-in-the-reporting-numbers-our-providers-feed-us/">When We Lose Trust In The Reporting Numbers Our Providers Feed Us</a></h3>
			<p><em>17 Jan 2017</em></p>
			<p>What happens when we can't trust the numbers our service providers report to us? I personally do not stress over my analytics and traffic, views, and other numbers we are engineering our worlds to run by, but when you are paying for a service--I definitely have an opinion. Facebook recently had a series of misreporting events around their metrics, leaving us questioning the numbers we are fed by our service providers on a regular basis.
There is no way that we can be 100% sure our service providers are telling us the truth--we have to trust them. However, there are ways that API providers can be more transparent when it comes to the data behind the numbers. It is easy enough to open up the log files, and other data that went into calculating the numbers when operating a platform. Many of the advertising and other service providers in questions have APIs to control the parts of the systems a platform desires you to perform--they just don't like pulling back the curtain, and showing what is truly going on.
If you do not fully trust your provider's numbers make sure and let them know publicly and privately that you would like access to the raw data behind. APIs can be designed to provide access to any data, as well as provide observability into the algorithms that driving any process. This is not a question of whether it's possible technically, it is a question of whether it is possible politically, and whether or not your service provider is willing to be transparent enough with you to develop the required trust needed for all of this to work properly.
[<a href="/2017/01/17/when-we-lose-trust-in-the-reporting-numbers-our-providers-feed-us/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/17/patent-20150363171-generating-virtualized-api-from-narrative-api-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-patent-algorithms.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/17/patent-20150363171-generating-virtualized-api-from-narrative-api-documentation/">Patent #20150363171: Generating Virtualized API From Narrative API Documentation</a></h3>
			<p><em>17 Jan 2017</em></p>
			<p>I like to pick worrisome patents from my API patent research and share them on my blog regularly. Last week I talk about&nbsp;Patent #US9300759 B1: API Calls With Dependencies and today I want to talk about patent #US09471283:&nbsp;Generating virtualized application programming interface (API) implementation from narrative API documentation, which according to its abstract is: A virtualized Application Program Interface (API) implementation is generated based upon narrative API documentation that includes sentences that describe the API, by generating programming statements for the virtualized API implementation based on parsing the narrative API documentation and generating the virtualized API implementation based on upon the programming statements for the virtualized API implementation. The parsing of the narrative documentation may use a natural language parser and a domain-specific ontology for the API that may be obtained or created for the API. The virtualized API implementation may be generated using an API virtualizer. I generally won't talk smack about folks filing patents, except I'm a pretty strong believer that the API interface and the supporting elements that make an API do the API thing should be left out. All the elements present in this patent like virtualization, documentation, and narrative need to be open--this is how things work. Just when we are all beginning to get good when it comes to crafting useful APIs, learning to speak in complete sentences with multiple APIs, we are going to patent the process of telling stories with APIs? Sorry, it just doesn't compute for me. I know this is what ya'll&nbsp;do at bigcos, but it's counterproductive. Instead of filing this patent I wish they would have taken their idea and launched as open source tool, then also launch an accompanying service running in the cloud, and get to work letting us crafting narratives around our APIs. As I've said before, these patents really mean nothing, and it will all come down to keeping an eye on the court filings using the CourtListener API&nbsp;for any...[<a href="/2017/01/17/patent-20150363171-generating-virtualized-api-from-narrative-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/16/what-are-the-goals-behind-launching-an-api-portal/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/portal_screenshot.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/16/what-are-the-goals-behind-launching-an-api-portal/">What Are The Goals Behind Launching An API Portal?</a></h3>
			<p><em>16 Jan 2017</em></p>
			<p>I was getting ready to do some work on a developer portal for a project I'm working on and I wanted to stop, step back and try to define what the goals in launching this portal are. As the technologist on this project, it is easy for me to impose my belief in why I am launching&nbsp;this portal (to publish documentation), but I feel it is important that we get the perspective of all stakeholders--so, I asked everyone involved what the goals were. In the short term, the goals are to engage these groups around the API resources: Engage Third-Parties - Bring in new, and stimulate existing usage of data made available via APIs. Internal Departments - Ensure the internal groups are also engaged, and part of the outreach. While incentivizing the following things between engaged stakeholders: Aware - Help make people aware of available resources, and what is being done with them. Conversations - Stimulate focused conversations&nbsp;among&nbsp;stakeholders around data and APIs. Issues - Focus on solving tangible issues that actually make an impact in the community. Long term the goals to stimulate projects that matter, and can possibly bring in sustainable relationships and revenue that will help ensure the platform is up and running, and serving the target audiences. It takes money to do APIs properly, keeping the servers running, code evolving, active outreach and support in the community, and a passionate, engaged community to develop valuable projects. I know all of this sounds overly simplified, but it is actually the thought process I'm applying to this project. I would say it is oversimplified to say that we are just launching a portal so that people will use our APIs. We need some basic goals about who we are reaching out to, and what we are trying to accomplish--we can evolve, and continue to define from there. When I lay out the outline for this project's portal I will make sure each element that...[<a href="/2017/01/16/what-are-the-goals-behind-launching-an-api-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/16/profiling-facebook-threatexchange-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_15_at_6.40.58_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/16/profiling-facebook-threatexchange-api/">Profiling Facebook ThreatExchange API</a></h3>
			<p><em>16 Jan 2017</em></p>
			<p>I'm spending some cycles on discovering what "cybersecurity" or "security" API solutions are out there, but specifically looking at threat information related to operating on the web. First up on the list is Facebook's ThreatExchange API, and I wanted to go through and break down what they offer via their API as I work to define an OpenAPI Spec, and their overall API operations as I populate&nbsp;an&nbsp;APIs.json&nbsp;file.This process helps me better understand what Facebook is offering in this area, as well as producing a machine readable definition that I can use across the rest of my research.&nbsp; So, what is Facebook ThreatExchange? Learn about threats. Share threat information back. Everyone becomes more secure. Most threat intelligence solutions suffer because the data is too hard to standardize and verify. Facebook created the ThreatExchange platform so that participating organizations can share threat data using a convenient, structured, and easy-to-use API that provides privacy controls to enable sharing with only desired groups. &nbsp; Scale your intelligence -&nbsp;Threats like malware and phishing will often attack many targets. Safeguard yourself using shared intelligence from other&nbsp;participants. A better way to share -&nbsp;Utilize a structured platform to send and receive information about threats. Join ThreatExchange -&nbsp;Learn about threats. Share threat information back. Everybody becomes more secure. *in beta I'm wanting to understand Facebook's motivations behind doing the ThreatExchange API, and better understand the technical, business, and political aspects of providing a platform like this. When profiling a platform I always start with the endpoints, as I feel like they give the most honest accounting of what is going on. /malware_analyses - Search for malware samples by hash and other metadata. /malware_families - Search for malware families by name and other metadata. /threat_descriptors - Enables searching for indicators of compromise descriptors. /threat_indicators - Enables searching for indicators of compromise. /threat_tags - Enables searching for threat tags. /threat_exchange_members - Returns a list of current members of the ThreatExchange. Next, I look at the underlying...[<a href="/2017/01/16/profiling-facebook-threatexchange-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/16/no-innovation-around-terms-of-service-reveals-true-motives/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-terms-of-use.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/16/no-innovation-around-terms-of-service-reveals-true-motives/">No Innovation Around Terms of Service Reveals True Motives</a></h3>
			<p><em>16 Jan 2017</em></p>
			<p>Silicon Valley startups and entrepreneurs love to point out that they are trying to make the world a better place. Over a 25+ year career, I have fallen for the belief that I was improving a situation through technology. Hell, I still do this regularly as the API Evangelist, stating that a certain approach to opening up access to data, content, and algorithms can make things better, when in numerous situations it will not. I walk a fine line with this and I hope that I'm a little more critical about where technology should be applied, and focus primarily on making existing technology more accessible using APIs--not starting new ones. When you are critical of technology in the current climate, there are plenty of folks you like to push back on you, leaning on the fact that they are trying to make the world a better place. Not sure where this line of bullshit first began but is something I should look into. I mean, when you hang out with enough entrepreneurs you really begin see through the bullshit and hype, and you know that this is all about them selling you something, and then selling you and your data as part of an exit via acquisition or going public. It rarely ever has anything to do with making the world a better place, what was promised as part of the marketing, and helping the average end-user. This is where my entrepreneur friends have stopped reading&nbsp;and lean on the fact that they actually believe they are trying to make the world a better place, and their firm belief that Kin is an annoying hippie. You know I love you. You are a&nbsp;special snowflake. But, you are a minority, I am talking about the other 95% of the system you choose to be willfully ignorant of. If you want some evidence of this in the wild, take a look at all the world saving, innovative, revolution...[<a href="/2017/01/16/no-innovation-around-terms-of-service-reveals-true-motives/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/13/requiring-ssl-for-api-all-calls/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_11_28_at_9.58.49_am.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/13/requiring-ssl-for-api-all-calls/">Requiring SSL For API All Calls</a></h3>
			<p><em>13 Jan 2017</em></p>
			<p>
This is one of those regular public service announcements that if at all possible, you should be requiring SSL for all your API calls. I recently got an email from the IBM Watson team telling me that they would be enforcing encryption on all calls to the Alchemy API in February.
SSL is something I've started enforcing on my own internal APIs. I do not have wide usage of my APIs by third-party providers, but I do have a variety of systems making calls to my APIs, transmitting some potentially sensitive&nbsp;information--luckily nothing too serious, as I'm just a simple API Evangelist.
Encryption is an area I research regularly, hoping to stay in tune (as much as I can) with where discussions are going when it comes to encryption and API operations. Much of it doesn't apply to the average API provider&nbsp;but requiring encryption for API calls, and emailing your developers when and if you do begin enforcing, is what I'd consider an essential building block for all API providers.
I'll keep crafting a unique blog post each time I receive on of these emails from the APIs I depend on. Hopefully some day soon, all APIs will be SSL by default.

[<a href="/2017/01/13/requiring-ssl-for-api-all-calls/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/13/ifttt-vs-zapier-vs-datafire/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/data_fire_bulb.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/13/ifttt-vs-zapier-vs-datafire/">IFTTT vs Zapier vs DataFire</a></h3>
			<p><em>13 Jan 2017</em></p>
			<p>Integration Platform as a Service (iPaaS) solutions are something I've been tracking on for a while, and an area I haven't seen too much innovation in, except by Zapier for most of that time. I'm a big fan of what IFTTT enables, but I'm not a big fan of companies who build services that depend on APIs&nbsp;but do not offer APIs in turn, so you don't find me highlighting them as an iPaaS solution. Instead, you'll find me cheering for Zapier, who has an API, and even though I wish they had more APIs, I am grateful they paying it forward a little bit. I wish we had better solutions, but the politics of API operations seems to slow the evolution of iPaaS, usually leaving me disappointed. That was until recently&nbsp;when some of my favorite&nbsp;API hackers released DataFire: DataFire is an open source integration framework - think Grunt for APIs, or Zapier for the command line. It is built on top of open standards such as RSS and Open API. Flows can be run locally, on AWS Lambda, Google Cloud, or Azure via the Serverless framework, or on DataFire.io. "DataFire natively supports over&nbsp;250 public APIs&nbsp;including: &bull; Slack &bull; GitHub &bull; Twilio &bull; Trello &bull; Spotify &bull; Instagram &bull; Gmail &bull; Google Analytics &bull; YouTube, as well as MongoDB, RSS feeds, and&nbsp;custom integrations." They have a sample&nbsp;flows available as an individual Github repositories.&nbsp;Integrations can be added by the URL of an Open API (Swagger) specification or an RSS feed, you can also specify --raml, --io_docs, --wadl, or --api_blueprint. DataFire is new, so it has a lot of maturing to do as an API framework, but it has EVERYTHING that iPaaS solutions should have at its core in my opinion. It's API definition-driven, its open source,&nbsp;and there is a cloud version that any non-developer user can put to use. DataFire is encouraging everyone to share each of the&nbsp;flows as machine readable templates, each as their own Github...[<a href="/2017/01/13/ifttt-vs-zapier-vs-datafire/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/13/a-missed-opportunity-with-the-medium-api/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/medium_storytelling_network2.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/13/a-missed-opportunity-with-the-medium-api/">A Missed Opportunity With The Medium API</a></h3>
			<p><em>13 Jan 2017</em></p>
			<p>In addition to using the news of Medium's downsizing as a moment to stop and think about who owns our bits, I wanted to point out what a missed opportunity the Medium API is. Having an API is no guarantee of success, and after&nbsp;$132M&nbsp;in&nbsp;3 Rounds&nbsp;from&nbsp;21 Investors, I'm not sure an API can even help out, but it is fun to speculate about what might be possible if Medium had robust API in operation. Medium has an API, but it is just a Github repository, with reference to a handful of paths allowing you to get details on yourself, the publications you are part of, and post entries to the site. There are no APIs for allowing me to get the posts of other users, or publications, let alone any of the analytics, or traffic for this. I'm guessing there is no API vision or champion at Medium, which results in the simple, restrictive API we see today. Many media and content companies see APIs as giving away all the value they are trying to monetize, and are unaware of the control that modern approaches to API management bring&nbsp;to the table. Many people see the pain that other API pioneers have suffered like Twitter, and want to avoid the same problems, completely unaware that many of Twitter's ecosystem problems were Twitter-induced&nbsp;and not inflicted by 3rd party developer. If Medium had a proper developer portal, complete set of API paths, proper OAuth controls, and other API management tooling, they could open up innovation in content delivery, publishing, analytics, visualizations, voice, bot, and the numerous of other areas where APIs are changing how we consume, create, and engage with information. I get that control over the user experience is key to the Medium model, but there are examples of how this can be done well, and still have an API. The best part is it only costs you the operation of the API operations. I do not think...[<a href="/2017/01/13/a-missed-opportunity-with-the-medium-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/12/why-i-still-believe-in-apisthe-2017-edition/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/kl_inapiwetrust_500_filtered.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/12/why-i-still-believe-in-apisthe-2017-edition/">Why I Still Believe In APIs--The 2017 Edition</a></h3>
			<p><em>12 Jan 2017</em></p>
			<p>As I approach my seventh year as the API Evangelist&nbsp;and find myself squarely in 2017, I wanted to take a moment to better understand, and articulate why I still believe in APIs. To be the API Evangelist I have to believe in this, or I just can't do it. It is how my personality works--if I am not interested, or believe in something, you will never find me doing it for a living, let alone as obsessively as I have delivered API Evangelist. First, What Does API Mean To Me?There are many, many interpretations, and incarnations of "API" out there. I have a pretty wide definition of what is API, one that spans the technical, business, and politics of APIs. API does not equal REST, although it does employ the same Internet technologies used to drive the web. API is not the latest vendor solution&nbsp;or even standard. The web delivers HTML for humans to consume in the browser, and web APIs deliver machine-readable media types (XML, JSON, Atom, CSV, etc.) for use in other applications. When I say applications, I do not just mean the web, mobile, and devices applications--I mean other applications, as in "the action of putting something into operation".&nbsp;An API has to find harmony between the technical, business, and political aspects of API operations and strike a balance between platform, 3rd party developer, and end-user needs--with every stakeholder being well informed along the way. I Still Believe Early My API VisionWhen I close my eyes I still believe in the API vision that captured my attention in 2005 using the Delicious API, again in 2006 with the Amazon S3 and EC2 APIs, and with the Twitter API in 2007. Although today, I better understand that a significant portion of this early vision was very naive, and too trusting in the fact that people (API providers and consumers) would do the right thing with APIs. APIs use Internet technology to make data, content,...[<a href="/2017/01/12/why-i-still-believe-in-apisthe-2017-edition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/12/using-an-openapi-spec-as-central-truth-in-stakeholder-discussions/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/openreferral_logo_green.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/12/using-an-openapi-spec-as-central-truth-in-stakeholder-discussions/">Using An OpenAPI Spec As Central Truth In Stakeholder Discussions</a></h3>
			<p><em>12 Jan 2017</em></p>
			<p>I am working with Open Referral to evolve the schema for the delivery of human services, as well as helping craft a first draft of the OpenAPI Spec for the API definition. The governing organization is looking to take this to the next level, but there are also a handful of the leading commercial providers at the table, as well other groups closer to the municipalities who are implementing and managing Open211 human service implementations. I was working with Open Referral on this before checking out this last summer, and would like to help steward the process, and definition(s) forward further in 2017. This means that we need&nbsp;to speak using a common language when hammering out this specification&nbsp;and be using a common platform where we can record changes, and produce a resulting living document. I will be borrowing from existing work I've done on API definitions, schema, and scope across the API space, and putting together a formal process design specifically for the very public process of defining, and delivering human services at the municipal level. I use OpenAPI Spec (openapis.org) as an open, machine readable format to drive this process. It is the leading standard for defining APIs in 2017, and now is officially part of the Linux Foundation. OpenAPI Spec provides all stakeholders in the process with a common language when describing the Open Referral in JSON Schema, as well as the surface area of the API that handles responses &amp; requests made of the underlying schema. I have an OpenAPI Spec from earlier work&nbsp;on this project, with the JSON version of the machine-readable definition, as well as a YAML edition--OpenAPI Spec allows for JSON or YAML editions, which helps the format speak to a wider, even less technical audience. These current definitions are not complete agreed upon definitions for the human services specification, and are just meant to jumpstart the conversation at this point. OpenAPI Spec provides us with a common language...[<a href="/2017/01/12/using-an-openapi-spec-as-central-truth-in-stakeholder-discussions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/12/the-google-baseline-for-a-user-account-area/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_11_at_8.59.54_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/12/the-google-baseline-for-a-user-account-area/">The Google Baseline For A User Account Area</a></h3>
			<p><em>12 Jan 2017</em></p>
			<p>I have a minimum definition for what I consider to be a good portal for an API, and was spending some time thinking about a baseline definition for the API developer account portion of that portal, as well as potentially any other authenticated, and validated platform user. I want a baseline user account definition that I could use as aa base, and the best one out there off the top of my head would be from Google. To support my work I went through my Google account page and outlined the basic building blocks of the Google account: Sign-in &amp; Security - Manage your account access and security settings Signing in to Google - Control your password and account access, along with backup options if you get locked out of your account. Password &amp; sign-in method - Your password protects your account. You can also add a second layer of protection with 2-Step Verification, which sends a single-use code to your phone for you to enter when you sign in.&nbsp; Password - Manage your password. 2-Step Verification - Manage 2-Step verification. App Passwords - Create and manage application passwords. Account recovery options - If you forget your password or cannot access your account, we will use this information to help you get back in. Account Recovery Email - The email to send recovery instructions. Account Recovery Phone - The email to send recovery instructions. Security Question - A secret question to use as verification during recovery. Device activity &amp; notifications - Review which devices have accessed your account, and control how you want to receive alerts if Google thinks something suspicious might be happening. Recent security events - Review security events from the past 28 days. Recently used devices - Check when and where specific devices have accessed your account. Security alerts settings - Decide how we should contact you to let you know of a change to your account&rsquo;s security settings or if we...[<a href="/2017/01/12/the-google-baseline-for-a-user-account-area/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/11/your-state-issued-id-is-required-to-signup-for-this-online-service/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/shutterstock_upload_license.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/11/your-state-issued-id-is-required-to-signup-for-this-online-service/">Your State Issued ID Is Required To Signup For This Online Service</a></h3>
			<p><em>11 Jan 2017</em></p>
			<p>I am evaluating Shutterstock as a new destination for some of my photos and videos. I've been a Shutterstock user for their stock images, but I'm just getting going being a publisher. I thought it was worth noting that as part of their sign up process they require me to upload a copy of my state issued identification&nbsp;before I can sell photos or images as a Shutterstock publisher. This is something I've encountered with other affiliate&nbsp;, partner, and verified solutions. I've also had domains expire, go into limbo, and I have to fax in or upload my identification. It isn't something I haven't seen with many API providers yet, but I'm guessing it will be something we'll see more of with API providers further locking down their valuable resources.&nbsp; I am not sure how I feel about it being a regular part of the partner and developer validation&nbsp;process--I will have to think about it more. I'm just adding to the list of items I consider as part of the API management process. It makes sense to certify and establish trust with developers, but I'm not 100% sure this is the way to do it in the digital age. IDK, I will consider more, and keep an eye out for other examples of this with other API providers. Shutterstock publisher isn't necessarily connected directly to the API, but once I'm approved I will be uploading, and managing my account via their API, so it is a developer validation process for me. The topic of developer validation and trust keeps coming up in other discussions for me, and with the increasing number of APIs we are all developing with, it seems like we are going to need a more streamlined, multi-platform, and an API-driven solution to tackle this.&nbsp; For me, it would be nice if this solution was associated with my Github account, which plays a central&nbsp;role in all of my integrations. When possible, I create my...[<a href="/2017/01/11/your-state-issued-id-is-required-to-signup-for-this-online-service/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/11/intercom-providing-docker-images-of-their-sdks/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_10_at_7.57.42_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/11/intercom-providing-docker-images-of-their-sdks/">Intercom Providing Docker Images Of Their SDKs</a></h3>
			<p><em>11 Jan 2017</em></p>
			<p>
I regularly talk about the evolving world of API SDKs, showcasing what API service providers like APIMATIC are up to when it comes to orchestration, integration, other dimensions of providing client code for API integrations. A new example of this that I have found in the wild is from communication and support API platform Intercom, with their publishing of Docker images of their API SDKs. This overlaps my SDK research with the influence that&nbsp;containerization is having on the the world of providing and integrating with APIs.
Intercom provides Docker images for their Ruby, Node, Go, and PHP API SDKs. It's a new approach to making API code available to API consumers that I haven't seen before, (potentially) making their integrations easier, and quicker. I like their approach to providing the containers and specifically the fact they are looking for feedback on whether or not having SDK Docker containers offer any benefit to developers. I'm guessing this won't benefit all API integrators, but for those who have successfully adopted a containerized way of life, it might streamline the process and overall time to integration.
I just wanted to have a reference on my blog to their approach. I'll keep an eye on their operations, and see if their SDK Docker images become something that gets some traction when it comes to SDK deliver. Since they are sharing the story on their blog, maybe they'll also provide us with an update in a couple months regarding whether developers found it useful or not. If nothing else, their story has reminded me to keep profiling Intercom, and other similar API providers, as part of my wider API communication and support research.

[<a href="/2017/01/11/intercom-providing-docker-images-of-their-sdks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/11/evernote-reaffirming-our-commitment-to-your-privacy/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_10_at_1.23.35_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/11/evernote-reaffirming-our-commitment-to-your-privacy/">Evernote: Reaffirming Our Commitment to Your Privacy</a></h3>
			<p><em>11 Jan 2017</em></p>
			<p>A couple of weeks back, the online note-taking platform Evernote made a significant&nbsp;blunder&nbsp;of&nbsp;releasing a privacy policy update that revealed they would be reading our notes to improve their machine learning algorithms. &nbsp;It is something they have since rolled back with the following statement "Reaffirming Our Commitment to Your Privacy": Evernote recently announced a change to its privacy policy and received a lot of customer feedback expressing concerns. We&rsquo;ve heard that feedback and we apologize for the poor communication.We have decided not to move forward with those changes that would have taken effect on January 23, 2017. Instead, in the coming months we will be revising our existing Privacy Policy. The main thing to know is this: your notes remain private, just as they&rsquo;ve always been.Evernote employees have not read, and do not read, your note content. We only access notes under strictly limited circumstances: where we have your permission, or to comply with our legal obligations.Your privacy, and your trust in Evernote are the most important things to us. They are at the heart of our company, and we will continue to focus on that now and in the future. While I am thankful for their change of heart, I wanted to take a moment to point out the wider online environment that incentivizes this type of behavior. This isn't a single situation with Evernote reading our notes, this is the standard mindset for startups operating online, and via our mobile devices in 2017. This is just one situation that was called out, resulting in a change of heart by the platform. Our digital bits are being harvested in the name of machine learning and artificial intelligence across the platforms we depend on daily for our business and personal lives.&nbsp; In these startup's quest for profit, and ultimately their grand exit, they are targeting our individual and business bits. Use this free web application. Use this free mobile application. Plug this device in at home...[<a href="/2017/01/11/evernote-reaffirming-our-commitment-to-your-privacy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/10/the-design-process-helping-me-think-through-my-data-and-content/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_09_at_9.03.01_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/10/the-design-process-helping-me-think-through-my-data-and-content/">The Design Process Helping Me Think Through My Data And Content</a></h3>
			<p><em>10 Jan 2017</em></p>
			<p>I'm working on the next evolution in my API research, and I'm investing more time and energy into the design of the guides I produce as a result of each area of my research. I've long produced a 20+ page PDF dumps of the leading areas of my research like API design, definitions, deployment, and management, but with the next wave of industry guides, I want to polish my approach a little more.  The biggest critique I get from folks about the API industry guides I produce is that they provide too much information, aren't always polished enough, and sometimes contain some obvious errors. I'm getting better at editing, but this only goes so far, and I'm bringing in a second pair of eyes to review things before they go out. Another thing I'm introducing into the process is the use am of professional design software (Adobe InDesign) rather than just relying on PDF's generated from my internal system with a little HTML spit shine. While it is taking me longer to dust off my Adobe skills than I anticipated, I am finding the design process to be extremely valuable. I've often dismissed the fact that my API research needed to look good, and that it is more important that it is just available publicly on my websites. This is fine and is something that will continue, but I'm finding a more formal design process is helping me think through all of the material, better understand what is valuable, what is important, and hopefully better tell a story about why it is relevant to the API space. It is helping me take my messy backend data and content, and present it in a more coherent and useful way. As I'm formalizing my approach using my API definition guide, I'm moving on to my API design guide, and I can't help but see the irony in learning the value of design while publishing a guide on API design, where I highlight...[<a href="/2017/01/10/the-design-process-helping-me-think-through-my-data-and-content/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/10/patent-us9300759-b1-api-calls-with-dependencies/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_09_at_11.11.55_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/10/patent-us9300759-b1-api-calls-with-dependencies/">Patent US9300759 B1: API Calls With Dependencies</a></h3>
			<p><em>10 Jan 2017</em></p>
			<p>
I understand that companies file for patents to build their portfolios, and assert their stance in their industry, and when necessary use patents as leverage in negotiations, and in a court of law. There are a number of things that I feel patents logically apply to, but I have trouble understanding why folks insist on patenting things that make the web work, and this whole API thing work.
One such filing is patent number&nbsp;US9300759 B1: API Calls With Dependencies, which is defined as:
Techniques are disclosed for a client-and-server architecture where the client makes asynchronous API calls to the client. Where the client makes multiple asynchronous API calls, and where these API calls have dependencies (i.e., a result of one call is used as a parameter in a second call), the client may send the server these multiple asynchronous API calls before execution of a call has completed. The server may then execute these multiple asynchronous API calls, using a result generated from one call as a parameter to another call.
Maybe I live in a different dimension than everyone else, but this doesn't sound unique, novel, and really just feels like folks&nbsp;are mapping out all the things that are working on the web and filing for patents. I found this patent while going through the almost 1300 API related patents in Amazon Web Services portfolio. Many of their patents make sense to me, but this one felt like it didn't belong.
When I read these patents I worry about the future of the web. Ultimately I can only monitor the courts for API related patent litigation, and keep an eye out for new cases, as this is where the whole API patent game is going to play out. I'll keep squawking every time I read a patent that doesn't just belong, and when I see any new&nbsp;legal cases I will help shine a light on what is going on.

[<a href="/2017/01/10/patent-us9300759-b1-api-calls-with-dependencies/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/10/hoping-schema-becomes-just-as-important-as-api-definitions-in-2017/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/schema_starburst.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/10/hoping-schema-becomes-just-as-important-as-api-definitions-in-2017/">Hoping Schema Becomes Just As Important As API Definitions in 2017</a></h3>
			<p><em>10 Jan 2017</em></p>
			<p>The importance of a machine readable API definition has grown significantly over the last couple of years, with a lot of attention being spent (rightfully so) on helping educate API providers of the value of having an OpenAPI Spec, API Blueprint, or another format. This is something I want to continue contributing to in 2017, but I also want to also shine a light on the importance of having your data schema well defined. When you look through the documentation of many API providers, some of them provide an example request which might give hints at the underlying data model, but you rarely ever see API providers openly share their schema in any usable format. You do come across some of a complete OpenAPI Spec or API Blueprints from time to time, but usually, when you find API definitions, the schema definition portion is incomplete.&nbsp; Not having your schema well defined, shareable, and machine-readable is one of the contributing factors to a lack of common, shared schema in the API space. We have healthy examples of this in action with Schema.org, but for some reason, many of us don't bring schema front and center in our API operations. We are accepting them as input for our API requests, and returning them with API responses, but don't always share examples of this in our documentation, complete sections in our API definitions, or share JSON Schema as part of our developer resources. All contributing to a lack of consistency within a single API operation, as well as the wider industry. I am going to spend more time in 2017 talking to people about the schema they use in their API operations and shining a light on existing schema that has been published by API providers. I will be also continuing to support important schema like Open Referral that helps streamline how our world works. It is no secret that when we speak using common schema things work...[<a href="/2017/01/10/hoping-schema-becomes-just-as-important-as-api-definitions-in-2017/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/09/the-api-driven-marketplace-that-is-my-digital-self/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/kinlane_physical_digital_self_publish.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/09/the-api-driven-marketplace-that-is-my-digital-self/">The API Driven Marketplace That Is My Digital Self</a></h3>
			<p><em>09 Jan 2017</em></p>
			<p>I spend a lot of time studying and thinking about the "digital bits" that we move around the Internet. Personally, and professionally I am dedicated to quantifying, and understanding those bits that are the most important to us as individuals, professionals, and business owners. Like many other folks who work in the tech sector I have always been good at paying attention to the digital bits, I am just not as good at others when monetizing these bits, adding to my own wealth. When you talk about this world in the world as much as I have, you see just a handful of responses. Most "normals" aren't very interested in things at this level--they just want to benefit from the Internet&nbsp;and aren't really interested in how it works. People who are associated with the tech sector and understand the value of these bits, often do not see them as "your" bits, they seem them as their bits--something they can extract value from, and generate revenue. Then there are always a handful of "normals" who are interested in understanding more, because of security and privacy concerns, as well as a handful of tech sector folks who actually care about the humans enough to balance the desire to just make profits. The Imbalance In All OF This Is What Fascinates Me&nbsp;The majority of the "normals" don't care about the complexity of the bits they generate, as well as who has access to them. Folks in the tech sector love to tell me regularly that people don't care about this stuff, they just want the convenience, and for it all to work. However, they are also overwhelmingly interested in the bits you generate each day&nbsp;because there is plenty of money to be made extracting insights from your bits, and selling those insights, as well as the raw bits to other companies so they can do the same. This is why EVERYTHING is being connected to the Internet--the convenience...[<a href="/2017/01/09/the-api-driven-marketplace-that-is-my-digital-self/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/09/service-level-agreements-for-researchers-who-depend-on-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-service-level-agreements.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/09/service-level-agreements-for-researchers-who-depend-on-apis/">Service Level Agreements for Researchers Who Depend On APIs</a></h3>
			<p><em>09 Jan 2017</em></p>
			<p>
I came across a pretty interesting post on using APIs for research, and the benefits, and challenges that researchers face when depending on APIs. It was another side of API stability and availability that I hadn't considered too much lately. Social media platforms like Twitter and Facebook are rich with findings to be studied across almost any discipline. I regularly find social media API studies at universities from areas like healthcare and Zika virus,&nbsp;algorithmic intellectual property protection, all the way up to US Navy surveillance programs that are studying Twitter.
APIs are being used for research, but there are rarely API platform plans crafted with research in mind. Flexible rate limits, custom terms of service, that give them access to the data they need. I'm assuming that some companies have behind the scenes deals with some universities, or larger enterprise research groups (IBM, etc), as well as government agencies, and police agencies. The problem with this, is that 1) there is no virtual public front door to walk through and understand research levels of access, and 2) the details of partnerships are not publicly, equitable, and auditable by journalists, and other groups.
The author of this essay provides a lot of details regarding what it is like to depend on APIs for your research. Some of them could put your career in jeopardy if the terms of service, and access levels change before you could finish your research, or dissertation. I'm not sure what the responsibility of API providers should be when it comes to making their resources available for research, but it is something I will be exploring further. I will be reaching out to researchers about their API usage, but will also be helping encourage API providers to share their side of things, and maybe eventually formalize how API providers make their valuable resources available for important research.
[<a href="/2017/01/09/service-level-agreements-for-researchers-who-depend-on-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/09/evaluating-a-new-channel-for-publishing-my-bits/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/shutterstock_developers_home.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/09/evaluating-a-new-channel-for-publishing-my-bits/">Evaluating A New Channel For Publishing My Bits</a></h3>
			<p><em>09 Jan 2017</em></p>
			<p>I have used Shutterstock for some time now when it comes stock images but I've only recently started playing around with their publishing program, hoping to make some money from some of my photos and videos. As with any other channel&nbsp;that I am considering for inclusion in my&nbsp;line-up of tools and services, I am spending time going through their platform and evaluate the tech, business, and political considerations of adding any new service to work into my world.&nbsp; First, a service should always have an API. This isn't just because of what I do for a living&nbsp;and my obsession with APIs. This is so that I can integrate seamlessly with my existing operations. Another side of this&nbsp;argument&nbsp;is that I will always be able to get my data and content out of a system, but I am working to be a little more proactive than that. I want my system, that operates within my domain to be the lead, and any new channel I adopt only play second fiddle. In this scenario, each photo or video that I publish to Shutterstock will live within my image and video systems&nbsp;and then&nbsp;with the Shutterstock API, I will publish to the Shutterstock domain as I deem worthy.&nbsp; The Shutterstock API (potentially) gives me more access and control over my digital bits&nbsp;and allows me to do more with fewer resources. I do not have to depend on APIs, or a platform's data portability to get my data and content out, I've always possessed this control and ownership from the beginning. Then this control and ownership is&nbsp;now exercised and strengthened in three areas: technology, business, and politics. I technical have control over my bits. I have business control over where they are sold, by whom, and how much of the action they get. I have political control, and when I want to change, evolve, or end the relationship I can do what I think is best for me, and my API...[<a href="/2017/01/09/evaluating-a-new-channel-for-publishing-my-bits/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/06/algorithmia039s-multiplatform-data-storage-solution-for-machine-learning-workflows/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_06_at_11.14.26_am.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/06/algorithmia039s-multiplatform-data-storage-solution-for-machine-learning-workflows/">Algorithmia&#039;s Multi-Platform Data Storage Solution For Machine Learning Workflows</a></h3>
			<p><em>06 Jan 2017</em></p>
			<p>I've been working with Algorithmia to manage a large number of images as part of&nbsp;my algorithmic rotoscope side project, and they have a really nice omni-platform approach to allowing me to manage my images and other files I am using in my machine learning workflows. Images, files, and the input and output of heavy object is an essential part of almost any machine learning task, and Algorithmia makes easy to do across the storage platforms we use the most (hopefully).&nbsp; Algorithmia provides you with local data storage--pretty standard stuff, but they also allow you to connect your Amazon S3 account, or your Dropbox account, and connect to specific folders, buckets, while helping you handle all of your permissions. Maybe I have my blinders on with this because I heavily use Amazon S3 as me default online storage, and Dropbox is my secondary store, but I think the concept still is worth sharing.. This allows me to seamlessly manage the objects, documents, files, and other images I store across my operation as part of my machine learning workflow. &nbsp;Algorithmia even provides you with an intuitive way of referencing files, by allowing each Data URI to uniquely identifies files and directories, with each composed of a protocol and a path, with each service having its own unique protocol: data:// Algorithmia hosted data dropbox:// Dropbox default connected accounts S3:// Amazon S3 default connected account This approach dramatically simplifies my operations when working with files, and allows me to leverage the API driven storage services I am already putting to work, while also taking advantage of the growing number of algorithms available to me in Algorithmia's catalog. In my algorithmic rotoscope project I am breaking videos into individual images, producing 60 images per second of video, and uploading to Amazon S3. Once images are uploaded, I can then run Algorithmia's Deep Filter algorithm against all images, sometimes thousands of images, using their text models, or any of the 25+...[<a href="/2017/01/06/algorithmia039s-multiplatform-data-storage-solution-for-machine-learning-workflows/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/05/what-i-learned-crafting-api-definitions-for-66-of-the-amazon-web-services/"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/company/logos/amazon-aws-logo.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/05/what-i-learned-crafting-api-definitions-for-66-of-the-amazon-web-services/">What I Learned Crafting API Definitions For 66 Of The Amazon Web Services</a></h3>
			<p><em>05 Jan 2017</em></p>
			<p>I just finished crafting API definitions for 66 of the Amazon Web Services. You can find it all on Github, indexed with an APIs.json. While I wish all API providers would do this hard work on their, I do enjoy the process because it forces me to learn a lot of each API, and the details of what providers are up to. I learned quite a bit about Amazon Web Services going through the over 2000 paths that are available across the 66 services.  The Importance Of Consistency Across TeamsWhen you bounce from service to service within the AWS ecosystem you can tell that consistency is a challenge for Amazon. Consistency is lacking in API design, documentation, and other critical areas. This is something that is actually getting worse with some of their newer projects. While the older AWS APIs aren't the best possible design because they are: "?Action= based", at least they are consistent, and the documentation is using the same template. Some of the newer APIs are better designed, but their documentation is all over the place, and they are deviating from the consistency that seemed to exist with some of the older API efforts.   Clear Picture Of Essential Building BlocksThere are a variety of building blocks employed in support of AWS APIs, but there is a pretty clear definition of what are considered to be the essential building blocks that exist across ALL AWs APIs: Documentation - Overall, developer, and API documentation to support the services. Getting Started - What you need to get up and going with any of the AWS solutions. Frequently Asked Questions - A list of the frequently asked questions asked of each service. Pricing - The pricing for using each service, with some providing a calculator to assist. Amazon also provides a centralized blog, code, support, and what I'd consider to be essential building blocks, and some of the individual services do a good job linking to these...[<a href="/2017/01/05/what-i-learned-crafting-api-definitions-for-66-of-the-amazon-web-services/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/04/explaining-to-normals-why-every-api-is-different/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-broken-gears.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/04/explaining-to-normals-why-every-api-is-different/">Explaining To Normals Why Every API Is Different</a></h3>
			<p><em>04 Jan 2017</em></p>
			<p>I enjoy having conversations with "normals" about APIs, especially when they approach me after doing a great deal of research, and are pretty knowledgeable about the landscape, even if they may lack deeper awareness around the technical details. These conversations are important to me&nbsp;because it is these folks that will make the biggest impact with APIs--it won't be the entrepreneurs, developers, architects, and us true believers. While having one of these conversations yesterday, the topic of API design came up, and we were talking about the differences between seemingly similar APIs like Flickr and Instagram, or maybe Twitter and Facebook. I was asked, "why are these APIs are so different? I thought the whole thing with APIs is that they are interoperable, and make integration easier?" &lt;&lt; I love getting asked this&nbsp;because it helps me see the API space for what it is, not the delusion that many of us API believers are peddling.&nbsp; So why are the designs of APIs so different, even between seemingly similar APIs? Integration - APIs make integration into web, mobile, and devices apps easier. It will also make integration with other systems easier. However, very few API providers truly want their APIs to work seamlessly with the competition! Silos - Many API providers operate in silos, and I have encountered teams who do almost no due diligence on existing API design patterns, standards, or even looking at the potential competition, and what already exists before crafting their API design strategy.&nbsp; Intellectual Property - Not many folks see the separate between their API design, the naming, ordering, and structure of the interface, and their backend API code, resulting in some distorted views of what is proprietary and what is not. Venture Capital - The investors in many of the companies behind APIs are not interested in being open and interoperable with others in their industry, resulting in a pretty narrow, and selfish focus when it comes to API design patterns....[<a href="/2017/01/04/explaining-to-normals-why-every-api-is-different/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/04/api-calls-as-opposed-to-api-traffic/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-traffic-light.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/04/api-calls-as-opposed-to-api-traffic/">API Calls as Opposed to API Traffic</a></h3>
			<p><em>04 Jan 2017</em></p>
			<p>
I was doing some planning around a potential business model for commercial implementations of OpenReferral, which provides Open211 open data and API services for cities, allowing citizens to find local services, and I had separated out two types of metrics: 1) API calls &nbsp;2) API traffic. My partner in crime on the project asked me what the difference was, looking for some clarification on how it might possibly contribute to the bottom line of municipalities looking to fund this important open data work.
So, what is the difference between API call and API traffic in this context?

API Call - This is the measurement of each call made to the API by web, mobile, and device applications.
API Traffic - This is the measurement of each click made via URLs / URIs served up as part of any API response.

In this context, we are looking to provide municipalities, non-profit organizations, and even commercial efforts that are delivering 211 services in cities around the world. I am not suggesting that every bit of revenue and profit be squeezed out of the operation of these important services, I am simply suggesting that there are ways to generate revenue that can become important in keeping services up and running, and impact the quality of that services--it takes money to do this stuff right.
Think of API traffic like an affiliate program or in service of lead generation. This approach requires the usage of some sort of URL shortener services&nbsp;so that you can broker, and measure each click made on a link served up by an API. This opens up other security and privacy concerns we should think about, but it does provides a potential layer for generating valuable traffic to internal, and partner web and mobile applications. This is just one of several approaches I am considering when we are thinking about monetization of open data using APIs.


[<a href="/2017/01/04/api-calls-as-opposed-to-api-traffic/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/04/a-glimpse-at-minimum-bar-for-business-api-operations-in-2017/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2017_01_04_at_12.13.05_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/04/a-glimpse-at-minimum-bar-for-business-api-operations-in-2017/">A Glimpse At Minimum Bar For Business API Operations in 2017</a></h3>
			<p><em>04 Jan 2017</em></p>
			<p>
I look at a lot of API portals and developer areas , and experience a number of innovative approaches from startups, as well as a handful of leading API providers, but the Lufthansa Airlines API portal (which recently came across on my radar) I feel represents the next wave of API providers, as the mainstream business world wakes up to the importance of doing business online in a machine readable way. Their developer program isn't anything amazing, &nbsp;it's pretty run of the mill, but I think it represents the minimum bar for SMB and SMEs out there in 2017.
The Lufthansa developer portal has all the basics including documentation, getting started, an application showcase, blog, and they are using Github, Stack Overflow, Twitter, and have a service status page. They provide APIs for their core business resources including cities, airports, aircraft, and the schedule of their flights. Honestly, it is a pretty boring, mundane representation of an API, something you are unlikely to find written up in Techcrunch, but this is why I like it. In 2017, we are getting down to the boring business of doing business on the web (maybe security will come soon?).
I'm hoping this is what 2017 is all about when it comes to APIs--where average small businesses and enterprises getting their API operations up and running. Its is like 2002 for websites, and 2012 for mobile--APIs are what you do if you are doing business online in 2017. They aren't the latest tech trend or fad, it is about acknowledging there are many applications and systems that will need to integrate with your resources, and having simple, low cost, machine-readable APIs is how you do this. Let's all get down to business in 2017, and leave behind the hype when it comes to the API life cycle.
[<a href="/2017/01/04/a-glimpse-at-minimum-bar-for-business-api-operations-in-2017/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/03/learning-about-machine-learning-apis-with-my-algorithmic-rotoscope-work/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/valleyrivertreeline/clean_view/file-00_01_06_83.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/03/learning-about-machine-learning-apis-with-my-algorithmic-rotoscope-work/">Learning About Machine Learning APIs With My Algorithmic Rotoscope Work</a></h3>
			<p><em>03 Jan 2017</em></p>
			<p>I was playing around with Algorithmia for a story about their business model back in December, when I got sucked into playing with their DeepFilter service, resulting in a 4-week long distraction which ultimately became what I am calling my&nbsp;algorithmic rotoscope&nbsp;work. After weeks of playing around, I have a good grasp of what it takes to separate videos into individual images, applying the&nbsp;Algorithmia&nbsp;machine learning filters, and reassembling them as videos. I also have several of my own texture filters created now using the AWS AMI and process provided&nbsp;Algorithmia--you can learn more about algorithmic rotoscope, and details of what I did via the Github project updatese. The project has been a great distraction from what I should be doing. After the election, I just did not feel like doing my regular writing, scheduling of Tweets, processing of press releases, and the other things I do on a regular basis. Algorithmic Rotoscope provided a creative, yet a&nbsp;still very API focused project to take my mind off things during the holidays. It was a concept I couldn't get out of my head, which is always a sign for me that I should be working on a project. The work was more involved than I anticipated, but after a couple weeks of tinkering, I have the core process for applying filters to videos working well, allowing me to easily apply the algorithmic textures. Other than just being a distraction, this project has been a great learning experience for me,&nbsp;with several aspects keeping me engaged: Algorithmia's Image Filters&nbsp; - Their very cool DeepFilter service, which allows you to apply artistically and stylish filters to your images using their API or CLI, providing over 30 filters you can use right away. Training Style Transfer Models - Firing up an Amazon GPU instance, look through art books and find interesting pieces that can be used to train the machine learning models, so you can define your own filters. Applying Filters To Images...[<a href="/2017/01/03/learning-about-machine-learning-apis-with-my-algorithmic-rotoscope-work/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/01/03/exploring-the-economics-of-wholesale-and-retail-algorithmic-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/algorithmia_pricing_how_it_works.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/01/03/exploring-the-economics-of-wholesale-and-retail-algorithmic-apis/">Exploring The Economics of Wholesale and Retail Algorithmic APIs</a></h3>
			<p><em>03 Jan 2017</em></p>
			<p>I got sucked into a month long project applying machine learning filters to video over the holidays. The project began with me doing the research on the economics behind Algorithmia's machine learning services, specifically the DeepFilter algorithm in their catalog. My algorithmic rotoscope work applying Algorithmia's Deep Filters to images and drone videos has given me a hands-on&nbsp;view of Algorithmia's approach to algorithms, and APIs, and the opportunity to think pretty deeply about the economics of all of this. I think Algorithmia's vision of all of this has a lot of potential for not just image filters, but any sort of algorithmic and machine learning API. Retail Algorithmic and Machine Learning APIsUsing Algorithmia is pretty straightforward. With their API or CLI you can make calls to a variety of algorithms in their catalog, in this case their DeepFilter solution. All I do is pass them the URL of an image, what I want the new filtered image to be called, and the name of the filter that I want to be applied. Algorithmia provides an API explorer you can copy &amp; paste the required JSON into, or they also provide a demo application for you to use--no JSON required.&nbsp; Training Your Own Style Transfer Models Using Their AWS AMIThe first "rabbit hole" concept I fell into when doing the research on Algorithmia's model was their story on creating your own style transfer models, providing you step by step details on how to train them, including a ready to go AWS AMI that you can run as a GPU instance. At first, I thought they were just cannibalizing their own service, but then I realized it was much more savvier than that. They were offloading much of the costly compute resources needed to create the models, but the end product still resulted in using their Deep Filter APIs.&nbsp; Developing My Own API Layer For Working With Images and VideosOnce I had experience using Algorithmia's deep filter...[<a href="/2017/01/03/exploring-the-economics-of-wholesale-and-retail-algorithmic-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/11/16/pandora-vs-target-when-considering-how-public-to-be-with-your-api-operations/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/target_v_pandora.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/11/16/pandora-vs-target-when-considering-how-public-to-be-with-your-api-operations/">Pandora vs Target When Considering How Public To Be With Your API Operations</a></h3>
			<p><em>16 Nov 2016</em></p>
			<p>I am reworking the API Evangelist developer area, and shifting most of my content to be available as YAML and JSON data on the Github repositories that drive my network of sites. I'm doing this partly because I am not in the business of managing and growing an API community, and because there are some really badly behaved people out there that I'm just not really interested in having keys to my internal network. I am happy to open up read only access to my work publicly and write capabilities to my trusted partners, but having self-service access to my server(s) just isn't fun in the current online climate.&nbsp; I get it when folks want to keep their valuable data, content, and algorithm under lock and key, and require developers to build a relationship before they get access&nbsp;or increased levels of consumption. However, this hasn't always been the tune I'm whistling. There are plenty of examples of me telling API providers to provide self-service access to their resources in the past--well, we've fucked that off, with our bad behavior as API consumers. It's not that the concept won't work, it is just that it won't work with the number of assholes on the web these days it just isn't a good idea. Even with keeping the lock and key on our API resources, we can still be public with our API operations--there are many positive reasons for doing so. SEO is probably the first, I mean how are people going to find your APIs if you can't Google for them. Providing information to the press, and making the resources that support your APIs self-service can reduce&nbsp;the workload when you do give people access. Some examples of this can be found when looking at the Target vs. Pandora developer programs, both required approval to get access, but Target is much more open than Pandora with their overall story. Target really doesn't have that much more than...[<a href="/2016/11/16/pandora-vs-target-when-considering-how-public-to-be-with-your-api-operations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/11/16/focusing-on-a-single-example-of-what-an-api-is-when-onboarding-folks/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-focused-targed.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/11/16/focusing-on-a-single-example-of-what-an-api-is-when-onboarding-folks/">Focusing On A Single Example Of What An API Is When On-Boarding Folks</a></h3>
			<p><em>16 Nov 2016</em></p>
			<p>
Talking to people, and telling stories on a regular basis always pushes me to evolve my understanding of how people see (or don't see) APIs, and pushes me to keep shifting the way I tell stories. I've always felt that that education about APIs should be relevant to the users, but usually this centers around making it familiar,&nbsp;and speak to whoever I am helping onboard. After talking to some folks recently at @APIStrat, I'm adding to my thinking on this, and focusing on making my stories more precise for folks I talk with.
One of the reasons I really like APIs is that they are so versatile. You can take and a piece of data, content, or an algorithm (code), and wrap with an API, and provide read and write access via the Internet. However, I think the average person does not thrive in this environment and need an explanation that is more precise. It doesn't help them to know that APIs can do anything, they need it to be relevant, but also providing a single solution that they can wrap their heads around, and apply in their world.
I am just sharing this thinking as I'm working to add it to my storytelling toolbox. I'm really committed to helping people understand what APIs are, so they can help push back on platforms to have APIs, as well as be more open with existing APIs. It doesn't do any good to confuse people with an unlimited number of API scenarios. We should be dialing in our storytelling so that we can help onboard them with the concept, and increase the number of folks who understand that they can take control over their own information on the platforms they depend on each day.
[<a href="/2016/11/16/focusing-on-a-single-example-of-what-an-api-is-when-onboarding-folks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/11/14/the-taiwanese-government-posts-an-apis-json-index/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_11_14_at_11.34.48_am.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/11/14/the-taiwanese-government-posts-an-apis-json-index/">The Taiwanese Government Posts An APIs.json Index</a></h3>
			<p><em>14 Nov 2016</em></p>
			<p>
My friend and partner in crime Nicolas Greni&eacute; (@picsoung), and operator of our open source API search engine APIs.io, just let me know that the Taiwanese government just added an APIs.json&nbsp;file for their government open data site. Adding to the other authoritative (added by owner) government API indexes like from Trade.gov in the United States.
We haven't' had a lot of time to move forward the APIs.json&nbsp;index lately, but honestly it doesn't need much pushing forward at this point in time. Our primary objective is to continue getting adoption like this, and not radically shift the specification until we have more feedback from the community, across a large number of API operators.
You can learn more about the specification we've been working on over at apisjson.org, and see other existing implements from Fibit, Plivo, and Actuity&nbsp;Scheduling. Once you've created an APIs.json&nbsp;definition for your API operations you can add to the APIs.io search engine using the submission form, or using the APIs.io API. We are gearing up for another push forward regarding tooling being developed on the specification over the holidays and will have more information in 2017.&nbsp;
[<a href="/2016/11/14/the-taiwanese-government-posts-an-apis-json-index/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/11/14/the-api-evangelist-mission-continues/"><img src="http://s3.amazonaws.com/kinlane-productions2/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/11/14/the-api-evangelist-mission-continues/">The API Evangelist Mission Continues</a></h3>
			<p><em>14 Nov 2016</em></p>
			<p>I tried to get back to normal last week on API Evangelist -- I failed. The previous week was @APIStrat in Boston, which was a success. It was the Presidential election that caused me to swerve and put things into the ditch. I was devastated and saddened by the results. Not because my party lost, but because we chose someone who ran on such a racially, and religiously&nbsp;charged platform, that was so threatening to women.&nbsp; It is easy to mistake what I do as the API Evangelist, as being a voice for the startup community -- cheerleading APIs in the service of the seemingly&nbsp;endless wave of tech companies coming out of Silicon Valley. While I do pay attention to the technology, and business of how these companies are using APIs, making sure the content, data, and algorithms they employ are transparent and observable by partners, 3rd party developers, end-users, and industry regulators is my primary objective. My mission is not about open APIs because open always makes things better, or simply open for business. I believe corporate, government and institutional data, content, and algorithmic resources should be as transparent and observable as possible, by those who are impacted by their existence. Meaning if you are collecting data about me, I should know what you are collecting, and have access to it. I should be able to move it around or delete it. Trusted regulators and auditors should also be able to peek behind the curtains of the algorithms that are increasingly impacting our world, and make sure they size up with the claims being made. I have spent six years pushing on startups and the enterprise to be more transparent and inclusive with their resources by employing APIs. During this time I did the same for the city, state, and the federal government. I've also extended this to higher educational institutions. With Donald Trump in office, this does not change, it just ups the stakes...[<a href="/2016/11/14/the-api-evangelist-mission-continues/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/11/14/daisy-chaining-multiple-api-paths-using-stoplight-scenarios/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/stoplight_scenarios.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/11/14/daisy-chaining-multiple-api-paths-using-stoplight-scenarios/">Daisy Chaining Multiple API Paths Using Stoplight Scenarios</a></h3>
			<p><em>14 Nov 2016</em></p>
			<p>There aren't too many startups doing interesting things in the API space right now. One of the exceptions is Stoplight.io. I am working really hard to find some of the good things in the API space to focus on, in hopes of not being too dark with my writing, after the election. Stoplight's new Scenarios is something new, something progressive, and I think could have a larger impact down the road--making it worth covering here on API Evangelist. Stoplight Scenarios is billed as "Test, automate, and debug web APIs + AWS Lambda". You can make API and AWS Lambda calls, and test, automate, and debug the responses -- pretty standard stuff we are seeing across several API service providers. Where it gets interesting for me is that you can daisy chain these requests together into a variety of scenarios. You can do this with a single API, or you can do it across a variety of disparate APIs--making for some pretty valuable "juju" in my opinion. As I am learning about a new API, I am often frustrated by having to connect the dots between many different paths, adding, then pulling, then updating, and other common "scenarios" I will want to accomplish. I would love for API providers to do this heavy lifting for me, and provide me with a variety of the common scenarios I need as an API consumer to get up and going. This is just the local possibilities around using scenarios, I'll explore the possibilities between many distributed APIs in future posts. If you haven't played with Stoplight before, I recommend heading over there. Their API design, definition, mocking, and documentation tooling is leading edge stuff in the world of APIs right now. The Stoplight Scenarios just ups the value their platform brings to the table, continuing to make them on of the few bright spots in the API world for me these days. Let me know what you think after...[<a href="/2016/11/14/daisy-chaining-multiple-api-paths-using-stoplight-scenarios/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/11/10/maintaining-the-api-community-at-scale-apistrat/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/steve_and_i_apistrat_2016.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/11/10/maintaining-the-api-community-at-scale-apistrat/">Maintaining The API Community At Scale #APIStrat</a></h3>
			<p><em>10 Nov 2016</em></p>
			<p>The 7th edition of API Strategy &amp; Practice wrapped up last week. It has been difficult to gather my thoughts with the election going on, but I wanted to shift my attention back to the API community for a bit. Steve Willmott and I started APIStrat back in 2012 to help establish an open, vendor-neutral community to discuss the technology, business, and politics of APIs -- we more than succeeded! It always warms my heart when people come up to me and share that they didn't see anything different about APIStrat, from other events before they attended&nbsp;and that they do now. Steve and I worked our asses off to make it a rich, inclusive, and meaningful conversation around APIs. It makes me very happy to hear people seeing the event as we intended, and going home with a headful of API knowledge. One thing I heard from several of the core members who have been there since the first one, and is something that has lingered in my mind, not just because of APIStrat, but also because of where are at in the wider API space -- is that the API world is expanding, and there are concerns regarding how we can maintain community. I consider our hard work in evangelizing APIs to be a success, as space is expanding like never before. Everybody is doing APIs, it isn't just a niche anymore. We are at the point we were with the web in 2001, and folks aren't asking if they should be doing APIs anymore--they are just doing them. Many of us in the original group are tired. We've been at this for a while. Many of the younger ones are still easily excited and distracted by new technology. There are many new entrants who just need to hear the same old stories we've been telling for years so that they can just get up to speed. Many people expressed their concerns around maintaining...[<a href="/2016/11/10/maintaining-the-api-community-at-scale-apistrat/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/11/10/how-do-you-work-towards-a-more-diverse-inclusive-tech-conference/"><img src="http://apistrat.com/wp-content/uploads/2014/12/apisrtrat-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/11/10/how-do-you-work-towards-a-more-diverse-inclusive-tech-conference/">How Do You Work Towards A More Diverse Inclusive Tech Conference?</a></h3>
			<p><em>10 Nov 2016</em></p>
			<p>The 7th edition of API Strategy &amp; Practice Conference happened last week. While I wasn't fully engaged throughout the planning process for this edition, due to my summer being disrupted, I wanted to take the time to share some of what happened to make it more of an inclusive technology event. There is a lot of people who are "interested" in making their events more diverse and inclusive, but APIStrat is "committed" to this (thanks, Charles Ashley III @CAsh_The3rd), and here are some of what we did. Strong Female Lead - Put a woman in charge. Period. She will set a good tone. Invite Women To Speak - Work to ONLY invite women when getting started. Invite People of Color To Speak - Work to ONLY invite people of color when getting started. Have Code Of Conduct - Make there is a code of conduct present, and communicated well. Enforce Code of Conduct - Sadly, we had to do&nbsp;this&nbsp;round, but it sets the right tone. No Manels - Do not have any panels where you only have men. Numbers&nbsp;- Know your numbers, and work every moment to increase them wherever you can. Repeat, Rinse - Repeat all of this at all levels, session chairs, keynotes, reg counter, etc. We are nowhere near where we want to be with making #APIStrat a truly inclusive event, but we are making improvements with each edition. We have our checklist and have been building on it with each event. It takes a leadership team that is committed to this. Steve, Lorinda, Amelia, and the team delivered this round -- I wish I could take credit. I saw more women, diverse faces, and topics at #APIStrat this round -- leaving me very, please. If you are running a tech conference, please put in the extra work. You can't just give this 5 or 10% effort. You literally have to invite NO WHITE MEN to your event for the first couple...[<a href="/2016/11/10/how-do-you-work-towards-a-more-diverse-inclusive-tech-conference/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/11/04/drone-recovery-in-the-attention-economy/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/kin_drinking.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/11/04/drone-recovery-in-the-attention-economy/">Drone Recovery In The Attention Economy</a></h3>
			<p><em>04 Nov 2016</em></p>
			<p>Difficult To Keep My AttentionWhen I was young I was always curious when it came to technology. I set up the entire computer lab for my 7th-grade math teacher back in 1983. I programmed computers all through high school, even having a job programming the software used by schools in the State of Oregon in COBOL. I was really good at&nbsp;school&nbsp;until I wasn't interested. If I got bored, which I did growing up in rural Oregon, I tended not to pay&nbsp;attention&nbsp;in school. Eventually, it got kicked out of high school in my senior year, and I ended up getting into drugs, and a lot of trouble. From 1990 through 1995 I spent my time traveling the country partying and dealing drugs until I finally hit a wall and needed a way out. To get my life out of the ditch I turned to what I already knew, the outdoors (I grew up out in the woods), and computers. I was good at paying&nbsp;attention&nbsp;to the bits and bytes in the emerging world of personal computing, and I would leverage technology to get me out of this mess I found myself in. Your browser does not support the video tag. Attention To Family &amp; CareerAfter spending a summer in the Oregon wilderness getting clean and healthy, I moved to the nearest city&nbsp;and got to work building a career. By the first&nbsp;dot com bubble, I had found success, married a young lady, and had a beautiful baby girl. I had left my troubled past behind. It was a period in my life where there the harder I worked, the better I felt. I had a good&nbsp;job and bought a house (two), but my attention always seemed to be on finding further business success, a sort&nbsp;of chronic entrepreneurial condition,&nbsp;resulting in having at least one, and often times multiple startup projects going on at any point in time. In total, I had almost 14 separate startups with only two I...[<a href="/2016/11/04/drone-recovery-in-the-attention-economy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/28/we-will-need-machine-readable-transparency-report-info-via-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_28_at_12.52.27_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/28/we-will-need-machine-readable-transparency-report-info-via-apis/">We Will Need Machine Readable Transparency Report Info Via APIs</a></h3>
			<p><em>28 Oct 2016</em></p>
			<p>
I was reading the latest Yahoo transparency report, as well as the Tumblr. When a company releases their latest version of this data, it tends to prompt me to take a look at some of the other providers who have them, like Google and Twitter. I am interested in what I can learn from these reports, about what the government is up to, as well as the incentives behind each platform publishing their reports. I fascinated by studying the process and approach of each company, tracking on what some of the common building blocks so that I can include in my API transparency research.
The addition of CSV downloads of information requests by Twitter is worth noting, and adding to my list of building blocks. Platform providers are going to have to consider adding machine-readable versions of their transparency reports to help address the growing number of requests they will bet getting in this area. Providing CSV, JSON, or even YAML access to transparency report information is going to become important for us to understand how this legal and very political layer of our worlds is evolving.

The number of government and court requests for information are up. The number of companies publishing transparency reports in response to public demand for transparency is increasing. Providing machine readable representations of this data, as well as evolving a common API and data schema definition is the only way we are going to be able to manage this growth, and make sense of the data across platform providers. I will keep adding company transparency reports to my research, and take the time to aggregate the common elements in play across them, and see if I can't contribute to this common API and schema definition for use by providers
[<a href="/2016/10/28/we-will-need-machine-readable-transparency-report-info-via-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/28/a-drone-law-api-for-use-in-planning-and-at-flight-time/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/copy_of_united_states_data_map.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/28/a-drone-law-api-for-use-in-planning-and-at-flight-time/">A Drone Law API For Use In Planning And At Flight Time</a></h3>
			<p><em>28 Oct 2016</em></p>
			<p>&nbsp; Photo: Drones and Society I went down to the police department in Hermosa Beach and filed my application for a drone permit. It's been two weeks and I haven't heard back. When I get done with @APIStrat I will go down there and talk to them again, and probably have to resubmit my application. Hermosa Beach is purported to have some of the strictest laws. I'm submitting mine so I can just play with my drone, and program it on the beach without getting into trouble.&nbsp; Before I fly my drone I use the B4UFLY mobile app from the Federal Aviation Administration to know whether I should fly my drone or not. It tells me whether I'm near an airport, military base, other location or event. This helps, but it doesn't help me with the legal side of things, what the laws are in my area, and any assistance in acquiring permits and approval. The industry is young, I'm sure eager startups out there are already working their asses off to aggregate the data, and serving it up via APIs for use in mobile and drone applications. As an operator, I am going to need the laws applying to the drone, bu also laws applying to video, and other data gathered. Can I be filming? What are private / public property data and privacy laws? I am gathering video, weather, temperature, and other crop data over commercial farms, am I allowed to keep it? What are the rules with other types of imaging&nbsp;like infrared? Are there any noise ordinances? There are numerous considerations when flying a drone for both personal and commercial scenarios. To be an informed pilot I need all this at run(flight)time. I guess I might also need some of it before I head out to my flight location, but also whatever I can also get at flight time. The drone sector seems to begin the process of tackling many of the...[<a href="/2016/10/28/a-drone-law-api-for-use-in-planning-and-at-flight-time/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/27/why-upgrade-to-a-paid-plan/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/pricing_and_faq_nanoscale_io.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/27/why-upgrade-to-a-paid-plan/">Why upgrade to a paid plan?</a></h3>
			<p><em>27 Oct 2016</em></p>
			<p>
I thought the microservices platform Nanoscale.io have an interesting argument for why you would upgrade to a paid plan. On their pricing page, after they break down each of the pricing plans they provide you with four reasons of why you would want to upgrade from their free tier.

More powerful APIs - Your nanoscale.io hosted microservices can run longer and perform more complex operations, or access slower source systems, without timing out.
Get premium support - Having a problem with nanoscale.io? Think you may have found a bug? Submit your inquiry and get a guaranteed response from our technical team.
Influence product roadmap - We are open to new feature consideration, and give preference to paid accounts to influence which ones get built sooner.
Deploy anywhere - Upgrade your downloadable nanoscale.io server with a production license to deploy your microservices on any infrastructure you choose.

More power and support seems like no-brainers. Being able to influence the roadmap is a compelling reason and something I would pay money for! ;-) The deploy anywhere I think is a sign of the future, not just for how you will buy services for your API, this is how you will deploy your APIs for your consumers. In cloud. On-premise. On-device. You can consume our API anywhere you want--if you pay for it!
I have been aggregating the plans and pricing of API providers, and service providers for a while now. People are getting better at providing a decent breakdown of their API plans, but they aren't always that good at articulating the reasons why you would go from free to a paid plan. I think this is an area of stress, and concern for many API providers, and an area they could use more examples to follow from out in the wild.
[<a href="/2016/10/27/why-upgrade-to-a-paid-plan/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/27/tooling-to-help-aggregate-dns-across-multiple-service-providers/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/68747470733a2f2f7261772e6769746875622e636f6d2f4e6574666c69782f64656e6f6d696e61746f722f6d61737465722f64656e6f6d696e61746f722e6a7067.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/27/tooling-to-help-aggregate-dns-across-multiple-service-providers/">Tooling To Help Aggregate DNS Across Multiple Service Providers</a></h3>
			<p><em>27 Oct 2016</em></p>
			<p>Adrian Cockroft (@adrianco) turned me on to a DNS aggregation solution the other day while I was working on updating the API definitions for the API providers that are included in my API DS research. It was a very appropriate day for thinking deeply about aggregate DNS, with the DDOS attack against Dyn going on.

DNS provider redundancy: the idea behind @denominatorOSS - one API/tool for many providers to allow switching. /cc @adrianfcole
&mdash; adrian cockcroft (@adrianco) October 21, 2016
Denominator&nbsp;is a portable Java library for manipulating DNS clouds. It has pluggable backends, including AWS Route53, Neustar Ultra, DynECT, Rackspace Cloud DNS, and OpenStack Designate. Here is a good post on it from back in 2013, describing it as a multi-vendor interface for DNS.&nbsp;

There doesn't look to be a lot of activity around the project in the last year, but it provides a good model for what I'd like to eventually see across all the major stops along the API lifecycle. I picture a wealth of aggregate tooling like Denominator that can act as a broker between API service providers&nbsp;and help switch, migrate, and sync between providers whether you are deploying, managing, testing, monitoring, or dialing in your DNS.
As I read the multiple investigations into what happened with the DDOS attack on Dyn last week, it seems relevant to learn more about aggregate DNS API solutions like Denominator. I will spend some time looking for other similar open tooling that is vendor-neutral, as well as vendor-switchable. We are going to need open source circuit breakers like this to help route, switch, migrate, and sync DNS across many service providers in this volatile landscape.
[<a href="/2016/10/27/tooling-to-help-aggregate-dns-across-multiple-service-providers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/27/allowing-for-relationships-between-api-developers-at-the-app-level/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_26_at_9.05.40_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/27/allowing-for-relationships-between-api-developers-at-the-app-level/">Allowing For Relationships Between API Developers At The App Level</a></h3>
			<p><em>27 Oct 2016</em></p>
			<p>
Managing developers access to an API is API management 101. Managing the relationships between developers, and allowing for multiple users associated with an API application isn't something I have seen before. Slack just added the ability to add what they call app collaborators--"adding any existing member of your Slack team as a Collaborator, including members and&nbsp;Guest Accounts."
The functionality felt a lot like the social aspects that made Github more attractive than just Git. When it comes to developing messaging apps and bots I can see a social layer doing pretty well. "Once a Collaborator is added, they&rsquo;ll receive a notification from Slackbot letting them know they now have access to your app." Smells like an opportunity for API management providers to bake into user management solution, and if you wanted to take it to the next level, add a Slack messaging layer as option.
Its a small feature, but I think it is one of the things that made Github work, and could have a similar impact at the API management level, allowing for more engagement between users who are working together on API integration. I am going to add it as a building block for my API management research, even if I haven't seen it elsewhere. It is something I'd like to see more of, and maybe if I plant the bug, more providers will implement.
[<a href="/2016/10/27/allowing-for-relationships-between-api-developers-at-the-app-level/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/26/transparent-data-transfer-control-apis-at-the-iot-device-level/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-device-data.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/26/transparent-data-transfer-control-apis-at-the-iot-device-level/">Transparent Data Transfer Control APIs At The IoT Device Level</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>I am diving deep into the DJi drone developer platform, and one of the elements of the DJi Drone Guidance API that caught my attention was the data transfer control methods. In this situation, the transfer control methods are designed for just the data being sent as part of the drone guidance systems, but I think it provides a blueprint that can be used across almost any IoT device connectivity. DJI provides four methods for managing the drone data transfer control: Start Transfer -&nbsp;Inform guidance to start data transfer. Stop Transfer -&nbsp;Inform guidance to stop data transfer. Release Transfer -&nbsp;Release the data transfer thread. Wait For Board Ready -&nbsp;Set callback function handler for hen data from guidance comes, it will be called by data transfer thread. The "wait for board ready" method acts as a sort of web hook, that can notify any application build on the API that data is now being transferred, opening up the possibilities for notifying a device owner, and operator that data is being transferred. To me, this can be a critical aspect of building trust that our devices have our best interests in mind, providing some essential transparency in the data layer of the IoT space. Data transfer control APIs for IoT devices like this will not ensure healthy data practices. This implementation is designed to provide transfer control capabilities to the developer, it is now up to the developer to include the end users in this process. Many current mobile application business models do not incentivize this type of transparency, as you do not want end users, and often times 3rd party developers involved in data gathering, and revenue generation this valuable "exhaust" of Internet-connected devices. I am hoping this evolves and changes as the Internet matures, and the number of connected devices increases. We need transparency at the device data transfer level, and we need all humans involved / impacted to be a literate, active, and will...[<a href="/2016/10/26/transparent-data-transfer-control-apis-at-the-iot-device-level/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/26/potential-for-apis-to-target-us-online-by-adding-more-context/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_25_at_1.08.16_pm.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/26/potential-for-apis-to-target-us-online-by-adding-more-context/">Potential For APIs To Target Us Online By Adding More Context</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>Many folks see me simply as a cheerleader for APIs&nbsp;when in reality I am more of an evangelist for the bad that can happen with APIs. I believe that sharing of data, content, and algorithms using web APIs has the potential for good, but in reality, they are often be used for doing some pretty shady shit.&nbsp; An example of this is found in my inbox this morning, and I'm sure is something everyone will encounter at some point in their daily lives. It is an email for an undelivered Fedex package, which I know better than to click on, but sadly I think it is one that many folks will fall for. Why do they fall for this? Because the email potentially has relevance, as I just ordered a handful of packages from Amazon, which were being shipped via Fedex (I do not order much online). Using the FedEx&nbsp;API, anyone can query the status of a&nbsp;package. I'm assuming that there are&nbsp;folks out there who are scanning for the presence of delivering notifications--I'm not up to speed on the details of how you can do this. I'm unsure if they can get my email alongside this information, but I don't think this matters. I think they can correlate data about where I live, and the fact I'm receiving packages--whether the email came from API, or through other forms intelligence, it doesn't matter.&nbsp; My point is more around the fact that APIs are increasingly opening up signals about our daily lives, providing a wealth of context for phishing campaigns, increasing the chance that people will fall for these attacks. My solution to this problem does not involve a knee-jerk response to providing APIs, I am just looking to just warn API providers that they should be&nbsp;monitoring for this type of behavior on top of an API, and we should help the average email users&nbsp;and Amazon&nbsp;package&nbsp;receiver that these dangers exist. Everyone should pause and think deeply about...[<a href="/2016/10/26/potential-for-apis-to-target-us-online-by-adding-more-context/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/26/learning-the-dimensions-of-the-dji-drone-sdks-and-apis/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/dji_phantom_3_pro.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/26/learning-the-dimensions-of-the-dji-drone-sdks-and-apis/">Learning The Dimensions Of The DJI Drone SDKs And APIs</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>I am going through the DJi DJI drone developer area which has three distinct SDKs, which allow us to leverage a variety of APIs that make the drone magic happen. I'm still wrapping my head around the intersection of drones and APIs, and this is my attempt to distil down what I'm finding in their developer area, and absorb some of what is going across the industry. This is not &nbsp;meant to be a complete list. It is meant for my learning, and hopefully yours along the way. There are a variety of devices being connected to the Internet, but other than the automobile I don't think there is another object that is as complex as the drone. I'm fascinated by what is possible with this device, and the variety of APIs it has, the interaction with the RC controller, mobile device, and with other resources the clouds. I personally fly a DJI drone, so I am going through the DJI developer area, learning about their three SDKs, as they seem to be the ecosystem furthest along in their understanding the API potential -- think Twitter for IoT. The DJI Onboard SDK&nbsp;This SDK allows for communication with the DJI flight controller over a direct serial connection, to monitor and control aircraft flight behavior with the Onboard API&nbsp;while utilizing the built-in Intelligent Navigation Modes to create autonomous flight paths and maneuvers. Some of the actions for the onboard SDK are: Activation -&nbsp;Before you start exploring DJI Onboard SDK functionality via our ROS examples, you will need to go through the "Activation" process. Obtain/Release Flight Control - Managing the process to get flight control. Take Off - Initiate a take-off&nbsp;for the drone. Landing - Tell the device to land. Go Home - Tell the device to go home. Gimbal Control - Manage gimbal for camera. Altitude Control - Manage the altitude for the drone. Photo Taking - Allow for taking photos Start/Stop Video Recording - Start and...[<a href="/2016/10/26/learning-the-dimensions-of-the-dji-drone-sdks-and-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/26/asynchonous-conversational-interfaces-for-us-anti-social-folks/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-anti-social.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/26/asynchonous-conversational-interfaces-for-us-anti-social-folks/">Asynchonous Conversational Interfaces For Us Anti Social Folks</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>I get why people are interested in voice-enabled&nbsp;solutions like Alexa and Siri. I'm personally not a fan of speaking to get what I want, but I get the attraction for others. Similarly,&nbsp;I get why people are interested in bot enabled solutions like Facebook and Slack are bringing to the table, but I'm personally not a fan of the human-led noise in both of these channels, let alone automating this mayhem with bots. In short, I'm not 100% on board that voice and bots will be as revolutionary as promised. I do think they will have a significant impact and are worthy of paying attention to, but when it comes to API driven conversational interfaces, I'm putting my money on push driven approaches to making API magic happen. Approaches like Push by Zapier, and Webtask.io, where you can initiate a single, or chain of API driven events from the click on a button in the browser, in a web page, on my mobile phone, or hell, using the Amazon Dash button approach. These web tasks operate in an asynchronous way, making them more conversational-esque. Allowing those of us who are anti-social, and have adequate air gapped our social and messaging channels, and haven't fully subscribed to the surveillance economy, alternate solutions. These mediums could even facilitate a back and forth, passing machine readable values, until the desired result has been achieved. Some conversations could be predefined or saved, allowing me to trigger using a button at any point (ie. reorder that product from Amazon, retweet that article from last week). I'm not saying I don't want to have an API-enabled conversation, I'm just not sure I want a speaker or bot always present to get what I need to get done in my day. I understand that I am not the norm. There are plenty of folks who have no problem with devices listening around their home or business, and are super excited when it comes...[<a href="/2016/10/26/asynchonous-conversational-interfaces-for-us-anti-social-folks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/26/apis-helping-drones-generate-alpha-used-in-high-frequency-trading/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/gamaya_agriculture_analytics.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/26/apis-helping-drones-generate-alpha-used-in-high-frequency-trading/">APIs Helping Drones Generate Alpha Used In High Frequency Trading</a></h3>
			<p><em>26 Oct 2016</em></p>
			<p>One of the things I love about my world as the API Evangelist is the time I get diving into rabbit holes and learning about different areas where technology is being applied. I do not always agree with the business motivations behind what is going on, which can result in some often pretty shady situations, but I enjoy stepping back and understanding the data, API and approaches behind what is going on. I was doing some research on drone APIs recently, and as I was falling down the rabbit hole,&nbsp;I found myself reading about drones being a source of alpha? WTF is alpha? I had no idea and wanted to learn more about what an alpha generation platform was, and how drones and APIs are playing a role--here is the definition I found: An alpha generation platform is a technology solution used in algorithmic trading to develop quantitative financial models, or trading strategies, that generate consistent alpha, or absolute returns. The process of alpha generation refers to generating excess returns. The article that triggered this is about drones generating alpha was focused on the data generation, and communication capabilities of drones, and how they can be used in trading algorithms. Companies are looking to fly over crops, and predict yields, aggregate data, and add to the resources that available in the "alpha generation platform". I'm not convinced of the reality of this approach, but it does provide for some interesting scenarios as I am learning about the data drones can gather, how this transfer occurs to the cloud, and be put to work using existing approaches to video, image, analysis, and visualization APIs. Also, it provides fuel for my alternate design fiction writing, where I explore the possibilities&nbsp;of technology, both good and bad. From what I understand, this type of data would be considered "dirty" in the alpha workflow, in the same category of other data, you might gather from Amazon sales, Twitter sentiment, and...[<a href="/2016/10/26/apis-helping-drones-generate-alpha-used-in-high-frequency-trading/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/25/with-mobile-we-are-the-product-with-iot-lets-get-a-piece-of-the-action/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-revenue-share.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/25/with-mobile-we-are-the-product-with-iot-lets-get-a-piece-of-the-action/">With Mobile We Are The Product -- With IoT Lets Get A Piece Of The Action</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>Internet-connected devices generate data. The most recent wave of mobile devices has opened up an unprecedented world of data generation and harvesting from the network, device, and application layers. The location data, photos, videos, and other valuable exhaust from these devices is why there has been so much investment in technology, and why we are seeing continued investment in the Internet of Things (IoT). When it came to mobile phones this opportunity was new, and it isn't always clear that we are the product when it comes to making money off connecting these devices to the Internet. People aren't always aware of how much data they are generating, and how much this data is generating revenue for the latest generation of entrepreneurs--because it's new. Things have moved along, and it's not a secret anymore that devices connected to the Internet generating data has the potential to be very valuable in the right situation. I have been historically frustrated with people's lack of awareness of this, but I'm hoping that with each wave of technology that comes in the future, we will get smarter about this, and stop being the product when we can, and begin demanding a piece of this action (if we do it at all). If our wearable fitness device is used in any healthcare study, if that weather, water, or pollution sensor in our yard is generating revenue, we should get a piece of the action. If a device in our homes or businesses is generating data, we should be a more aware of, and willing participant in this new supply chain. The average person may not always care about their privacy&nbsp;in the surveillance economy, but maybe they'll care about lost opportunities for making money. At the consumer level this isn't always a coherent&nbsp;argument, but as you approach the work at home world, and into the professional territory, it can begin making more sense. Not all weather and pollution monitors might make...[<a href="/2016/10/25/with-mobile-we-are-the-product-with-iot-lets-get-a-piece-of-the-action/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/25/thinking-about-an-api-observability-stack/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/datadog_observability_dashboard.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/25/thinking-about-an-api-observability-stack/">Thinking About An API Observability Stack</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>I am learning about observability from reading Stripes post on Veneur, a high performance and global aggregation for Datadog. While the math of it all is over my head, the definition makes a lot of sense and provides me with a nice Venn&nbsp;diagram overlap across several areas of my API research, including testing, monitoring, logging, analysis, and visualization. The Wikipedia definition for observability is: Formally, a system is said to be observable if, for any possible sequence of state and control vectors, the current state can be determined in finite time using only the outputs (this definition is slanted towards the state space representation). Less formally, this means that from the system's outputs it is possible to determine the behavior of the entire system. If a system is not observable, this means the current values of some of its states cannot be determined through output sensors. Stripe provides a great technical breakdown of the tools, services used to establish observability as part of their system operations, but I wanted to step back, and think about observability through a business and political lens. The business imperative for observability might seem clear, as you want as much visibility and awareness into your API operations as you possibly can, so you can provide a reliable level of service. I am thinking about the incentives for extending this observability beyond internal groups, to partners, developers, regulators, or the public--encouraging transparent observability. This moves into the area of API and algorithmic transparency you hear me ranting about regularly, and the benefits APIs bring to the table when you apply in areas like policing and surveillance, or other more regulated sectors of our society. When you take the assertions applied as part of API testing and monitoring practices, and you link them up to this observability stack definition, and open things up beyond just core technical groups, I think we are moving into new territory where we can begin to observe...[<a href="/2016/10/25/thinking-about-an-api-observability-stack/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/25/looking-at-the-latest-api-related-certification-stories/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-certification.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/25/looking-at-the-latest-api-related-certification-stories/">Looking At The Latest API Related Certification Stories</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>As I curate the interesting news from across the API space each week I tag things to put them into different buckets. At the end of each week, I look through each bucket, deciding which area(s) I will be writing about each week. I am always trying to identify patterns and evolve the different areas of my research. One of the areas I'm considering adding as a formal area of API research is when it comes to certification.&nbsp; I have been seeing the area come up quite a bit, making me think it is something I should be thinking more about, and researching&nbsp;as its own project. This week there were four items thought caught my attention: BigML Certifications are Here! - Professional certification on machine learning APIs, from a leading API provider. Better Business Bureau yanks Wells Fargo's accreditation - &nbsp;Ok, not quite certification, but a classic representation and story to consider. Why cybersecurity certifications suck - A look into why certifications can be better, and applied in a critical area of the online world. Eligible Announces SOC2 Certification - Certification than an insurance API provider on their availability, processing integrity, confidentiality, or privacy. These posts came after another four items from last week, providing, even more, to think about when it comes to APIs and certification: A new certification program for Open Source Hardware - I certify that I am truly open source, not that faux open source you see a lot of--we are the real deal! Online Dante Certification Program Now Available - Professional certification on audio APIs, from an older school software provider. Transform your business; become a Google Certified Professional Data Engineer - Professional certification that you know your stuff when it comes to data, specifically data in the Google ecosystem--data engineers are hot. IBM launches Watson application developer certification - Another professional certification for machine learning and artificial intelligence, this time from big blue. There is a lot going on...[<a href="/2016/10/25/looking-at-the-latest-api-related-certification-stories/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2016/10/25/api-technology-does-not-have-to-mindlessly-march-forward/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-marching-shouting.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2016/10/25/api-technology-does-not-have-to-mindlessly-march-forward/">API Technology Does Not Have To Mindlessly March Forward</a></h3>
			<p><em>25 Oct 2016</em></p>
			<p>I am seeing more people asking that we put on the brakes when it comes to technology, looking to slow the adoption of new technology, in favor of mastery of the existing, and getting our house in order with the technology we already in play. One of the core tenets of my message as the API Evangelist centers on the importance of doing what we are already doing&nbsp;and doing it better. You can see this message present in my 2014 recommendation on an API strategy for the US federal government--do more of what is already in motion, don't disrupt by just doing the new. I have seen a lot of API technology float by in the last six years of doing API Evangelist. I can still get excited by some of it, but far less than I did in 2010 when I first started. This is partly because I'm tired, but mostly it is because I've seen a lot of shit float by, and it has to be meaningful in some way to get me excited. I am fairly willing to keep an open mind when it comes to microservices, DevOps, GraphQL, voice, bots, drones, and the other technological frontlines, but I refuse to accept that technology has to mindlessly march forward. This is more about selling us new things than it is every about truly bringing us real solutions to everyday problems. Whether it's addressing technical debt and monoliths, the security concerns of the Internet of Things, or anything in between, we should always work to step back and ask if the new technology will actually provide a solution, or create three new problems for each solution it brings. I am not anti new tech, I love new shiny tech objects, but I think for our own sanity we should learn to be more thoughtful. New technology trends can be exciting, and fun to play with, but when it comes to production environments, and...[<a href="/2016/10/25/api-technology-does-not-have-to-mindlessly-march-forward/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page13" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page15" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home Page</a></li>
    <li><a href="/blog/">The Blog</a></li>
    <li><a href="https://101.apievangelist.com/">API 101</a></li>
    <li><a href="http://history.apievangelist.com">History of APIs</a></li>
    <li><a href="https://women-in-tech.apievangelist.com/">Women in Technology</a></li>    
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
