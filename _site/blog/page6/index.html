<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/">A Couple More Questions For The Equifax CEO About Their Breach</a></h3>
        <span class="post-date">06 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/digital-bits-capital-dc-flag-side-view.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="https://www.theverge.com/2017/10/3/16410806/equifax-ceo-blame-breach-patch-congress-testimony">Speaking to the House Energy and Commerce Committee, former Equifax CEO Richard Smith pointed the finger at a single developer who failed to patch the Apache Struts vulnerability</a>. Saying that protocol was followed, and a single developer was responsible, shifting the blame away from leadership. It sounds like a good answer, but when you operate in the space you understand that this was a systemic failure, and you shouldn’t be relying on a single individual, or even a single piece of scanning software to verify the patch was applied. You really should have many layers in place to help prevent breaches like we saw with Equifax.</p>

<p>If I was interviewing the CEO, I’d have a few other questions for him, getting at some of the other systemic and process failures based upon his lack of leadership, and awareness:</p>

<ul>
  <li><strong>API Monitoring &amp; Testing</strong> - You say the scanner for the Apache Struts vulnerability failed, but what about other monitoring and testing. The plugin in questions was a REST plugin, that allowed for API communication with your systems. Due to the vulnerability, extra junk information was allowed to get through. Where were your added API request and response integrity testing and monitoring process? Sure you were scanning for the vulnerability, but are you keeping an eye on the details of the data being passed back and forth? API monitoring &amp; testing has been around for many years, and service providers like <a href="http://apis.how/8nlsropidv">Runscope</a> do this for a living. What other layers of monitoring and testing were in place?</li>
  <li><strong>API Management</strong> - When you expose APIs like you did from Apache Struts, what does the standardized management approach look like? What sort of metering, logging, rate limiting, and analysis occurs on each endpoint, and verification occurs, ensuring that only required clients should have access? API management has been standard procedure for over a decade now for exposing APIs like this both internally and externally. Why didn’t your API management process stop this sort of breach after only a couple hundred record went out? API management is about awareness regarding access to all your resources. You should have a dashboard, or at least some reports that you view as a CEO on this aspect of operations.</li>
</ul>

<p>These are just two examples of systems and processes that should have been in place. You should not be depending on a single person, or a single tool to catch this type of security incident There should be many layers in place, with security triggers, and notifications in place. Your CTO should be in tune with all of these layers, and you as the CEO should be getting briefed on how they work, and have a hand in making sure they are in place. I’m guessing that your company is doing APIs, but is dramatically behind the times when it comes to commonplace API management practices. This is your fault as the CEO. This is not the fault of a single employee, or any group of employees.</p>

<p>I am guessing that as a CEO you are more concerned with the selling of this data, than you are of securing it in storage, or transit. I’m guessing you are intimately aware of the layers that enable you to generate revenue, but you are barely investing in the technology and processes to do this securely, while respecting the privacy of your users. They are just livestock to you. They are just products on a shelf. It shows your lack of leadership to point the finger at a single person, or single piece of technology. There should have been many layers in place to catch this type of breach beyond a single vulnerability. It demonstrates your lack of knowledge regarding modern trends in how we secure and provide access to data, and you should never have been put in charge of such a large data brokerage company.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/">Teaching My Client Three Approaches To Modular UI Design Using Their APIs</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/holmes-county/holmes-county-listing.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am working with a client to develop a simple user interface on top of a Human Services Data API (HSDA) I launched for them. They want a basic website for searching, browsing, and navigating the organizations, locations, and services available in their API. A part of this work is helping them understand how modular and configurable their web site is, with each page, or portion of a page being a simple API call. It is taking a while for them to fully understand what they have, and the potential of evolving a web application in this way, but I feel like they are beginning to understand, and are taking the reigns a little more when it comes to dictate what they want within this new world.</p>

<p>When I first published a basic listing of human services they were disappointed. They had envisioned a map of the listings, allowing users to navigate in a more visual way. I got to work helping them see the basic API call(s) behind the listing, and how we could use the JSON response in any way we wanted. I am looking to provide three main ways in which I can put the API data to work in a variety of web applications:</p>

<ul>
  <li><strong>Server-Side</strong> - A pretty standard PHP call to the API, taking the results and rendering to the page using HTML.</li>
  <li><strong>Client-Side</strong> - Leveraging JavaScript in the browser to call the API and then render to the page using Jquery.</li>
  <li><strong>Static Push</strong> - Calling the APPI using PHP, then publishing as YAML or JSON to a Jekyll site and rendering with Liquid and HTML.</li>
</ul>

<p>What the resulting HTML, CSS, and JavaScript looks like in all these scenarios is up to the individual who is in charge of dictating the user experience. In this case, it is my client. They just aren’t used to having this much control over dictating the overal user experience. Which path they choose depends on a few things like whether they want the content to be easily indexed by search engines, or if they want things to be more JavaScript enabled magic (ie. maps, transitions, flows). All the API does is gives them full access to the entire surface area of the human services schema they have stored in their database.</p>

<p>After moving past the public website, I’m beginning to show them the power of not just GETs via their API, I’m showing them the power of POST, PUT, and DELETE. I find it is easy to show them the potential in an administrative system using a basic form. I find people get forms. Once you show them that in order to POST they have to have a special token or key, otherwise the API will reject, they feel a whole lot better about the process. I find the form tends to put things into context for them, beyond displaying data and content, and allowing them to actually manage all this data and content. I find the modularity of an API really lends itself to giving business users more control over the user interface. They may not be able to do everything themselves, but they tend to be more invested in the process, and enjoy more ownership over the process–which is a good thing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/how-api-evangelist-works/">How API Evangelist Works</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/crypto-machine-bletchley_copper_circuit.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve covered this topic several times before, but I figured I’d share again for folks who might have just become readers int he last year. Providing an overview of how API Evangelist works, to help eliminate confusion as you are navigating around my site, as well as to help you find what you are looking for. First, API Evangelist was started in the summer of 2010 as a research site to help me better understand what is going on in the world of APIs. In 2017, it is still a research site, but it has grown and expanded pretty dramatically into a network of websites, driven by a data and a content core.</p>

<p>The most import thing to remember is that <a href="https://github.com/api-evangelist">all my sites run on Github</a>, which is my workbench in the the API Evangelist workshop. apievangelist.com is the front door of the workshop, with each area of my research existing as its own Github repository, at its own subdomain with the apievangelist domain. An example of this can be found in my API design research, where you will find at <a href="http://design.apievangelist.com/">design.apievangelist.com</a>. As I do my work each day, I publish my research to each of my domains, in the form of YAML data for one of these areas:</p>

<ul>
  <li><strong>Organizatons</strong> - Companies, organizations, institutions, programs, and government agencies doing anything interesting with APIs.</li>
  <li><strong>Individuals</strong> - The individual people at organizations, or independently doing anything interesting with APIs.</li>
  <li><strong>News</strong> - The interesting API related, or other news I curate and tag daily in my feed reader or as I browse the web.</li>
  <li><strong>Tools</strong> - The open source tooling I come across that I think is relevant to the API space in some way.</li>
  <li><strong>Building Blocks</strong> - The common building blocks I find across the organizations, and tooling I’m studying, showing the good and the bad of doing APIs.</li>
  <li><strong>Patents</strong> - The API related patents I harvest from the US Patent Office, showing how IP is impacting the world of APIs.</li>
</ul>

<p>You can find the data for each of my research areas in the _ data folder for each repository. Which is then rendered as HTML for each subdomain using Liquid via each Jekyll CMS driven website. All of this is research. It isn’t meant to be perfect, or a comprehensive directory for others to use. If you find value in it–great!! However, it is just my research laying on my workbench. It will change, evolve, and be remixed and reworked as I see fit, to support my view of the API sector. You are always welcome to learn from this research, or even fork and reuse it in your own work. You are also welcome to submit pull requests to add or update content that you come across about your organization or open source tool.</p>

<p>The thing to remember about API Evangelist is it exist primarily for me. It is about me learning. I take what I learn and publish as blog posts to API Evangelist. This is how I work through what I’m discovering as part of my research each day, and use as a vehicle to move my understanding of APIs forward. This is where it starts getting real. After seven years of doing this I am reaching 4K to 7K page views per day, and clearly other folks find value in reading, and sifting through my research. Because of this I have four partners, <a href="http://apis.how/ake3nxbapm">3Scale</a>, <a href="http://apis.how/5ytnitnakm">Restlet</a>, <a href="http://apis.how/8nlsropidv">Runscope</a>, and <a href="http://apis.how/zflfesymzk">Tyk</a> who pay me money to have their logo on the home page, in the navigation, and via a footer toolbar. Runscope also pays me to place a re-marketing tag on my site so they can show advertising to my users on other websites, and Facebook. This is how I pay my rent, an how I eat each month.</p>

<p>Beyond this base, I take my research and create API Evangelist industry guides. <a href="http://definitions.apievangelist.com/#Guide">API Definitions</a>, and <a href="http://design.apievangelist.com/#Guide">API Design</a> are the two most recent editions. I’m currently working on one for data, database, as well as deployment, and management. These guides are sometimes underwritten by my partners, but mostly they are just the end result of my API research. I also spend time and energy taking what I know and craft API strategy and white papers for clients, and occasionally I actually create APIs for people–mostly in the realm of human services, or other social good efforts. I’m not really interested in building APIs for startups, or in service of the Silicon Valley machine. Even tough I enjoy watching, studying, and learning from this world, because there are endless lessons regarding how we can use technology in this community, as well as how we should not be using technology.</p>

<p>That is a pretty basic walk through of API Evangelist works. It is important to remember I am doing this research for myself. To learn, and to make a living. API Evangelist is a production, a persona I created to help me wade through the technology, business, and politics of APIs. It reflects who I am, but honestly is increasingly more bullshit than it is reality, kind of like the API space. I hope you enjoy this work. I enjoy hearing from my readers, and hearing how my research impacts your world. It keeps me learning each day, and from ever having to go get a real job. It is always a work in progress and never done. Which I know frustrates some, but I find endlessly interesting, and is something that reflects the API journey, something you have to get used to if you are going to be successful doing APIs in this crazy world.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/how-api-evangelist-works/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/show-that-api-call-behind-that-dashboard-visualization/">Show The API Call Behind Each Dashboard Visualization</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><a href="https://www.kentik.com/kentik-apis-enable-multi-solution-integration/"><img src="https://s3.amazonaws.com/kinlane-productions/kentik/kentik_API_menu-300w.png" align="right" width="35%" style="padding: 15px;" /></a></p>
<p>I am a big fan of user interfaces that bring APIs out of the shadows. Historically, APIs are often a footnote in the software as a service (SaaS) world, available as a link way down at the bottom of the page, in the settings, or help areas. Rarely, are APIs made a first class citizen in the operations of a web application, which really just perpetuates the myth that APIs aren’t for everybody, and the “normals” shouldn’t worry their little heads about it. When in reality, EVERYBODY should know about APIs, and have the opportunity to put them to work, so we should stop burying the links to our APIs, and our developer areas. If your API is too technical for a business user to understand what is going on, then you should probably get to work simplifying it, not burying it and keeping it in developer and IT realm.</p>

<p>I have written before about <a href="https://apievangelist.com/2016/10/24/the-api-behind-every-feature-in-the-user-interface/">how DNS provider CloudFlare provides an API behind every feature in their user interface</a>, and I’ve found <a href="https://www.kentik.com/kentik-apis-enable-multi-solution-integration/">another great example of this over at the network API provider Kentik</a>. In their network dashboard visualization tooling they provide a robust set of tooling for accessing the data behind the visuals, allowing you to export, view SQL, show API call, and enter share view. In their post, they proceed to instruction about how you can get your API key as part of your account, as well as providing a pretty robust introduction into why APIs are important. This is how ALL dashboards should work in my opinion. Any user should be introduced to APIs, and have the ability to get at the data behind, and export it, or directly make an API call in their browser or at the command line.</p>

<p>Developers like to think this stuff should be out of reach of the average user, but that is more about our own insecurities, and power trips, than it is about the average users ability to grasp this stuff. There is no reason why ALL user interfaces can’t be developed on top of APIs, with native functionality for getting at the API call, as well as the data, content, or algorithms behind the user interface feature. It makes for more powerful user interfaces, as well as more educated, literate, and efficient power users of our applications. If all web applications operated this way, we’d see a much more API literate business world, where users would be asking more questions, curious about how things work, and experimenting with ways they can be more successful in what they do. While I do see positive examples like Kentik out there, I also find that many web application developers are further retreating from APIs being front and center, preferring to keep them in the shadows of web and mobile applications, out of the reach of the average user. Something we need to reverse.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/show-that-api-call-behind-that-dashboard-visualization/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/big-data-is-not-abut-access-using-web-apis/">Big Data Is Not About Access Using Web APIs</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/dragon_close-up_yellow_collage.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m neck deep in research around data and APIs right now, and after <a href="http://apievangelist.com/2017/10/03/looking-at-the-37-apached-data-projects/">looking at 37 of the Apache data projects</a> it is pretty clear that web APIs are not a priority in this world. There are some of the projects that have web APIs, and there a couple projects that look to bridge several of the projects with an aggregate or gateway API, but you can tell that the engineers behind the majority of these open source projects are not concerned with access at this level. Many engineers will counter this point by saying that web APIs can’t handle the volume, and it shows that the concept isn’t applicable in all scenarios. I’m not saying web APIs should be used for the core functionality at scale, I’m saying that web APIs should be present to provide access to the result state of the core features for each of these platform, whatever that is, which something that web APIs excel at.</p>

<p>From my vantage point the lack of web APIs isn’t a technical one, it is a business and political motivation. When it comes to big data the objectives are always about access, and it definitely isn’t about the wide audience access that comes when you use HTTP, and the web for API access. The objective is to aggregate, move around, and work with as much data as you possibly can amongst a core group of knowledgable developers. Then you distribute awareness, access, and usage to designated parties via distilled analysis, visualizations, or in some cases to other systems where the result can be accessed and put to use. Wide access to this data is not the primary objective, paying forward much of the power and control we currently see around database to API efforts. Big data isn’t about democratization. Big Data is about aggregating as much as you can and selling the distilled down wisdom from analysis, or derived as part of machine learning efforts.</p>

<p>I am not saying there is some grand conspiracy here. It just isn’t the objective of big data folks. They have their marching orders, and the technology they develop reflect these marching orders. It reflects the influence money and investment has on the technology. The ideology that drives how the tech is engineered, and the algorithms handle specific inputs, and provide intended outputs. Big data is often sold as data liberation, democratization, and access to your data, building on much of what APIs have done in recent years. However, in the last couple of years the investment model has shifted, the clients who are purchasing and implementing big data have evolved, and they aren’t your API access type of people. They don’t see wide access to data as a priority. You are either in the club, and know how to use the Apache X technology, or you are sanctioned one of the dashboard analysis visualization machine learning wisdom drips from the big data. Reaching a wide audience is not necessary.</p>

<p>For me, this isn’t some amazing revelation. It is just watching power do what power does in the technology space. Us engineers like to think we have control over where technology goes, yet we are just cogs in the larger business wheel. We program the technology to do exactly what we are paid to do. We don’t craft liberating technology, or the best performing technology. We assume engineer roles, with paychecks, and bosses who tell us what we should be building. This is how web APIs will fail. This is how web APIs will be rendered yesterdays technology. Not because they fail technically, it is because the ideology of the hedge funds, enterprise groups, and surveillance capitalism organizations that are selling to law enforcement and the government will stop funding data systems that require wide access. The engineers will go along with it because it will be real time, evented, complex, and satisfying to engineer in our <a href="http://kinlane.com/2017/10/04/isolated-development-environments/">isolated development environments (IDE</a>). I’ve been doing data since the 1980s, and in my experience this is how data works. Data is widely seen as power, and all the technical elements, and many of the human elements involved often magically align themselves in service of this power, whether they realize they are doing it or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/big-data-is-not-abut-access-using-web-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/apis-used-to-give-access-to-resources-that-were-out-of-our-reach/">APIs Used To Give Us Access To Resources That Were Out Of Our Reach</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/mountainlake/clean_view/file-00_00_58_86.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I remember when almost all the APIs out there gave us developers access to things we couldn’t ever possibly get on our own. Some of it was about the network effect with the early Amazon and eBay marketplaces, or Flickr and Delicious, and then Twitter and Facebook. Then what really brought it home was going beyond the network effect, and delivering resources that were completely out of our reach like maps of the world around us, (seemingly) infinitely scalable compute and storage, SMS, and credit card payments. In the early days it really seemed like APIs were all about giving us access to something that was out of our reach as startups, or individuals.</p>

<p>While this still does exist, it seems like many APIs have flipped the table and it is all about giving them access to our personal and business data in ways that used to be out of their reach. Machine learning APIs are using parlour tricks to get access to our internal systems and databases. Voice enablement, entertainment, and cameras are gaining access to our homes, what we watch and listen to, and are able to look into the dark corners of our personal lives. Tinder, Facebook, and other platforms know our deep dark secrets, our personal thoughts, and have access to our email and intimate conversations. The API promise seems to have changed along the way, and stopped being about giving us access, and is now about giving them access.</p>

<p>I know it has always been about money, but the early vision of APIs seemed more honest. It seemed more about selling a product or service that people needed, and was more straight up. Now it just seems like APIs are invasive. Being used to infiltrate our professional and business worlds through our mobile phones. It feels like people just want access to us, purely so they can mine us and make more money. You just don’t see many Flickrs, Google Maps, or Amazon EC2s anymore. The new features in mobile devices we carry around, and the ones we install in our home don’t really benefit us in new and amazing ways. They seem to offer just enough to get us to adopt them, and install in our life, so they can get access to yet another data point. Maybe it is just because everything has been done, or maybe it is because it has all been taken over by the money people, looking for the next big thing (for them).</p>

<p>Oh no! Kin is ranting again. No, I’m not. I’m actually feeling pretty grounded in my writing lately, I’m just finding it takes a lot more work to find interesting APIs. I have to sift through many more emails from folks telling me about their exploitative API, before I come across something interesting. I go through 30 vulnerabilities posts in my feeds, before I come across one creative story about something platform is doing. There are 55 posts about ICOs, before I find an interesting investment in a startup doing something that matters. I’m willing to admit that I’m a grumpy API Evangelist most of the time, but I feel really happy, content, and enjoying my research overall. I just feel like the space has lost its way with this big data thing, and are using APIs to become more about infiltrating and extraction, that it is about delivering something that actually gives developers access to something meaningful. I just think we can do better. Something has to give, or this won’t continue to be sustainable much longer.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/apis-used-to-give-access-to-resources-that-were-out-of-our-reach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/">API Providers Should Provide Observability Into Government Developer Accounts</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve talked about this before, but after reading several articles recently about various federal government agencies collecting, and using social media accounts for surveillance lately, it is a drum I will be beating a lot more regularly. <a href="http://transparency.apievangelist.com/2013/02/26/api-transparency-report-as-essential-building-block/">Along with the transparency reports we are beginning to see emerge from the largest platform providers</a>, I’d like to start seeing more observability regarding which accounts, both user and developer are out of government agencies. Some platforms are good at highlighting how government of all shapes and sizes are using their platform, and some government agencies are good at showcasing their social media usage, but I’d like to understand this from purely an API developer account perspective.</p>

<p>I’d like to see more observability into which government agencies are requesting API keys. Maybe not specific agencies ad groups, and account details, although that would be a good idea as well down the road. I am just looking for some breakdown of how many developer accounts on a platform are government and law enforcement. What does their API consumption look like? If there is Oauth via a platform, is there any bypassing of the usual authentication flows to get at data, any differently than regular developers would be accessing, or requiring user approval? From what I am hearing, I’m guessing that there are more government accounts out there than platforms either realize, or are willing to admit. It seems like now is a good time to start asking these questions.</p>

<p>I would add on another layer to this. If an application developer is developing applications on behalf of law enforcement, or as part of a project for a government agency, there should be some sort of disclosure at this level as well. I know I’m asking a lot, and a number of people will call me crazy, but with everything going on these days, I’m feeling like we need a little more disclosure regarding how government(s) are using our platforms, as well as their contractors. The transparency disclosure that platforms have been engaging is a good start to the legal side of this conversation, but I’m looking for the darker, more lower level surveillance that I know is going on behind the scenes. The data gathering on U.S. citizens that doesn’t necessarily violate any particular law, because this is such new territory, and the platform terms of service might sanction it in some loopholy kind of way.</p>

<p>This isn’t just a U.S. government type of thing. I want this to be standard practice for all forms of government on the platforms we use. A sort of UN level, General Data Protection Regulation (GDPR). Which reminds me. I am woefully behind on what GDPR outlines, and how the rolling out of it is going. Ok, I’ll quick ranting now, and get back to work. Overall, we are going to have to open up some serious observability into how the online platforms we are depending are being accessed and use by the government, both on the legal side of things, as well as just general usage. Seems like the default on the general usage should always be full disclosure, but I’m guessing it isn’t a conversation anyone is having yet, which is why I bring up. Now we are having it. Thanks.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/">Letting Go In An API World Is Hard To Do</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/flower-barbed-wire_clean_view.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I encounter a number of folks who really, really, really want to do APIs. You know, because they are cool and all, but they just can’t do what it takes to let go a little, so that their valuable API resources can actually be put to use by other folks. Sometimes this happens because they don’t actually own the data, content, or algorithms they are serving up, but in other cases it is because they view their thing as being so valuable, and so important that they can’t share it openly enough, to be accessible via an API. Even if your APIs are private, you still have to document, and share access with folks, so they can understand what is happening, and have enough freedom to put to use in their application as part of their business, without too much constraint and restrictions.</p>

<p>Some folks tell me they want to do API, but I can usually tell pretty quickly that they won’t be able to go the distance. I find a lot of this has to do with perceptions of intellectual property, combined with a general distrust of EVERYONE. My thing is super valuable, extremely unique and original, and EVERYONE is going to want it. Which is why they want to do APIs, because EVERYONE will want it. Also, once it is available to EVERYONE via an API, competitors, and people we don’t want getting at it, will now be able to reverse engineer, and copy this amazing idea. However, if we don’t make accessible, we can’t get rich. Dilemna. Dilemna. Dilemna. What do we do? My answer is you probably that you shouldn’t be doing APIs.</p>

<p>You see, doing APIs, whether public or privately requires letting go a bit. Sure, you can dial in how much control you are willing to give up using API management solutions, but you still have to let go enough so that people can do SOMETHING with your valuable resource. If you can’t, APIs probably aren’t your jam. They just won’t work if you don’t give your API consumers enough room to breathe while developing and operating their integrations and applications. I understand if you can’t let go. The API game isn’t for everyone, or maybe there is some other data, content, and resources you don’t feel so strongly about that you could start with, and get the hang of doing APIs before you jump in with your prized possessions?</p>

<p>Another thing I might suggest, is that maybe you should twice about why these digital things are so important to you. Is it because they really matter to you, or is because you think they’ll make you a lot of money? If it is just the latter, they are probably not very valuable if you just keep them locked up. The best ideas are the ones that get used. The things that make the biggest impact get shared, and are usually pretty accessible. I’m guessing that most of your anxiety does not come from APIs, and what will happen when you launch them. I’m pretty sure it comes from some unhealthy views about what you have, the stories you’ve been told about intellectual property, and your obsession with getting rich. Again, which leaves us at the part of the story where you probably shouldn’t do APIs–I don’t think they are your jam.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your-communications-strategy/">Sharing Top Sections From Your API Documentation As Part Of Your Communications Strategy</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/top-aws-iam-documentation-pages-so-far-in-2017.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m always learning from the API communication practices from out of the different AWS teams. From <a href="http://communications.apievangelist.com/2016/10/04/amazon-alexa-as-an-example-when-it-comes-to-api-communications/">the regular storytelling coming out of the Alexa team</a>, to the <a href="https://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/">mythical tales of leadership at AWS that have contributed to the platform’s success</a>, the platform provides a wealth of examples that other API providers can emulate.</p>

<p>As I talked about last week, <a href="http://apievangelist.com/2017/09/29/developing-an-ability-to-repeat-the-same-api-stories-over-and-over-you-are-going-to-need-it/">finding creative ways to keep publishing interesting content to your blog as part of your API evangelism and communications strategy is hard</a>. It is something you have to work at. One way I find inspiration is by watching the API leaders, and learning from what they do. An interesting example I recently found out of the AWS security team, <a href="https://aws.amazon.com/blogs/security/the-top-20-aws-iam-documentation-pages-so-far-in-2017/">was their approach to showcasing the top 20 AWS IAM documentation pages so far in 2017</a>. It is a pretty simple, yet valuable way to deliver some content for your readers, that can also help you expose the dark corners of your API documentation, and other resources on your blog.</p>

<p>The approach from the AWS security team is a great way to generate content without having to come up with good ideas, but also will help with your SEO, especially if you can cross publish, or promote through other channels. It’s pretty basic content stuff, that helps with your overall SEO, and if you play it right, you could also get some SMM juice by tweeting out the store, as well as maybe a handful of the top links from your list. It is pretty listicle type stuff, but honestly if you do right, it will also deliver value. These are the top answers, in a specific category, that your API consumers are looking for answers in. Helping these answers rise to the top of your blog, search engine, and social media does your consumers good, as well as your platform.</p>

<p>One more tool for the API communications and evangelism toolbox. Something you can pull out when you don’t have any storytelling mojo. Which is something you will need on a regular basis as an API provider, or service provider. It is one of the tricks of trade that will keep your blog flowing, you readers reading, and hopefully your valuable API, products, services, and stories floating to the top of the heap. And that is what all of this is about–staying on top of the pile, keeping things relevant, valuable, and useful. If we can’t do that, it is time to go find something else to do.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your-communications-strategy/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/03/clearly-designate-api-bot-automation-accounts/">Clearly Designate API Bot Automation Accounts</a></h3>
        <span class="post-date">03 Oct 2017</span>
        <p><a href="https://www.shieldsquare.com/good-bots-and-bad-bots/"><img src="https://s3.amazonaws.com/kinlane-productions/shieldsquare/good-v-bad-bots.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m continuing <a href="http://apievangelist.com/2017/07/17/bot-observability-for-every-platform/">my research into bot platform observability</a>, and <a href="http://apievangelist.com/2017/08/15/which-platforms-have-control-over-the-conversations-around-their-bots/">how API platforms are handling (or not handling) bot automation on their platforms</a>, as I try to make sense of each <a href="http://apievangelist.com/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/">wave of the bot invasion on the shores of the API sector</a>. It is pretty clear that Twitter and Facebook aren’t that interested in taming automation on their platforms, unless there is more pressure applied to them externally. I’m looking to make sure there is a wealth of ideas, materials, and examples of how any API driven platform can (are) control bot automation on their platform, as the pressure from lawmakers, and the public intensifies.</p>

<p>Requiring users clearly identify automation accounts is a good first place to start. <a href="https://api.slack.com/bot-users">Establishing a clear designation for bot users has its precedents</a>, and requiring developers to provide an image, description, and some clear check or flag that identifies an account as automated just makes sense. <a href="https://medium.com/slack-developer-blog/the-bot-rulebook-a442d9fb21cb">Providing a clear definition of what a bot is, with articulate rules for what bots should and shouldn’t be doing</a> is next up on the checklist for API platforms. Sure, not all users will abide by this, but it is pretty easy to identify automated traffic versus human behavior, and having a clear separation allows accounts to automatically turned off when they fit a particular fingerprint, until a user can pass a sort of platform Turing test, or provide some sort of human identification.</p>

<p>Automation on API platforms has its uses. However, unmanaged automation via APIs has proven to be a problem. Platforms need to step up and manage this problem, or the government eventually will. Then it will become yet another burdensome regulation on business, and there will be nobody to blame except for the bad actors in the space (cough Twitter &amp; Facebook, cough, cough). Platforms tend to not see it as a problem because they aren’t the targets of harassment, and it tends to boost their metrics and bottom line when it comes to advertising and eyeballs. Platforms like Slack have a different business model, which dictates more control, otherwise it will run their paying customers off. The technology, and practices already exist for how to manage bot automation on API platforms effectively, they just aren’t ubiquitous because of the variances in how platforms generate their revenue.</p>

<p>I am going to continue to put together a bot governance package based upon <a href="http://bots.apievangelist.com/">my bot API research</a>. Regardless of the business model in place, ALL API platforms should have a public bot automation strategy in place, with clear guidelines for what is acceptable, and what is not. I’m looking to provide API platforms with a one-stop source for guidance on this journey. It isn’t rocket science, and it isn’t something that will take a huge investment if approached early on. Once it gets out of control, and you have congress crawling up your platforms ass on this stuff then it probably is going to get more expensive, and also bring down regulatory hammer on everyone else. So, as API platform operators let’s be proactive and take the problem of bot automation on directly, and learn from Twitter and Facebook’s pain.</p>

<p><em><strong>Photo Credit:</strong> <a href="https://www.shieldsquare.com/good-bots-and-bad-bots/">ShieldSquare</a></em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/03/clearly-designate-api-bot-automation-accounts/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/03/looking-at-the-37-apached-data-projects/">Looking At The 37 Apache Data Projects</a></h3>
        <span class="post-date">03 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/apache/apache-logo.png" align="right" width="30%" style="padding: 15px;" /></p>
<p>I’m spending time investing in <a href="http://data.apievangelist.com">my data</a>, as well as <a href="http://database.apievangelist.com">my database API research</a>. I’ll have guides, with accompanying stories coming out over the next couple weeks, but I want to take a moment to publish some of the raw research that I think paints an interesting picture about where things are headed.</p>

<p>When studying what is going on with data and APIs you can’t do any search without stumbling across an Apache project doing something or other with data. <a href="https://projects.apache.org/projects.html?category">I found 37 separate projects at Apache that were data related</a>, and wanted to publish as a single list I could learn from.</p>

<ul>
  <li><a href="https://projects.apache.org/project.html?airavata"><strong>Airvata</strong>**</a> - Apache Airavata is a micro-service architecture based software framework for executing and managing computational jobs and workflows on distributed computing resources including local clusters, supercomputers, national grids, academic and commercial clouds. Airavata is dominantly used to build Web-based science gateways and assist to compose, manage, execute, and monitor large scale applications (wrapped as Web services) and workflows composed of these services.</li>
  <li><a href="https://projects.apache.org/project.html?ambari"><strong>Ambari</strong></a> - Apache Ambari makes Hadoop cluster provisioning, managing, and monitoring dead simple.</li>
  <li><a href="https://projects.apache.org/project.html?apex"><strong>Apex</strong></a> - Apache Apex is a unified platform for big data stream and batch processing. Use cases include ingestion, ETL, real-time analytics, alerts and real-time actions. Apex is a Hadoop-native YARN implementation and uses HDFS by default. It simplifies development and productization of Hadoop applications by reducing time to market. Key features include Enterprise Grade Operability with Fault Tolerance, State Management, Event Processing Guarantees, No Data Loss, In-memory Performance &amp; Scalability and Native Window Support.</li>
  <li><a href="https://projects.apache.org/project.html?avro"><strong>Avro</strong></a> - Apache Avro is a data serialization system.</li>
  <li><a href="https://projects.apache.org/project.html?beam"><strong>Beam</strong></a> - Apache Beam is a unified programming model for both batch and streaming data processing, enabling efficient execution across diverse distributed execution engines and providing extensibility points for connecting to different technologies and user communities.</li>
  <li><a href="https://projects.apache.org/project.html?bigtop"><strong>Bigtop</strong></a> - Bigtop is a project for the development of packaging and tests of the Apache Hadoop ecosystem. The primary goal of Bigtop is to build a community around the packaging and interoperability testing of Hadoop-related projects. This includes testing at various levels (packaging, platform, runtime, upgrade, etc…) developed by a community with a focus on the system as a whole, rather than individual projects. In short we strive to be for Hadoop what Debian is to Linux.</li>
  <li><a href="https://projects.apache.org/project.html?bookkeeper"><strong>BookKeeper</strong></a> - BookKeeper is a reliable replicated log service. It can be used to turn any standalone service into a highly available replicated service. BookKeeper is highly available (no single point of failure), and scales horizontally as more storage nodes are added.</li>
  <li><a href="https://projects.apache.org/project.html?calcite"><strong>Calcite</strong></a> - Calcite is a framework for writing data management systems. It converts queries, represented in relational algebra, into an efficient executable form using pluggable query transformation rules. There is an optional SQL parser and JDBC driver. Calcite does not store data or have a preferred execution engine. Data formats, execution algorithms, planning rules, operator types, metadata, and cost model are added at runtime as plugins.</li>
  <li><a href="https://projects.apache.org/project.html?couchdb"><strong>CouchDB</strong></a> - Apache CouchDB is a database that completely embraces the web. Store your data with JSON documents. Access your documents with your web browser, via HTTP. Query, combine, and transform your documents with JavaScript. Apache CouchDB works well with modern web and mobile apps. You can even serve web apps directly out of Apache CouchDB. And you can distribute your data, or your apps, efficiently using Apache CouchDB’s incremental replication. Apache CouchDB supports master-master setups with automatic conflict detection.</li>
  <li><a href="https://projects.apache.org/project.html?crunch"><strong>Crunch</strong></a> - The Apache Crunch Java library provides a framework for writing, testing, and running MapReduce pipelines. Its goal is to make pipelines that are composed of many user-defined functions simple to write, easy to test, and efficient to run.</li>
  <li><a href="https://projects.apache.org/project.html?datafu"><strong>DataFu</strong></a> - Apache DataFu consists of two libraries: Apache DataFu Pig is a collection of useful user-defined functions for data analysis in Apache Pig. Apache DataFu Hourglass is a library for incrementally processing data using Apache Hadoop MapReduce. This library was inspired by the prevalence of sliding window computations over daily tracking data. Computations such as these typically happen at regular intervals (e.g. daily, weekly), and therefore the sliding nature of the computations means that much of the work is unnecessarily repeated. DataFu’s Hourglass was created to make these computations more efficient, yielding sometimes 50-95% reductions in computational resources.</li>
  <li><a href="https://projects.apache.org/project.html?drill"><strong>Drill</strong></a> - Apache Drill is a distributed MPP query layer that supports SQL and alternative query languages against NoSQL and Hadoop data storage systems. It was inspired in part by Google’s Dremel.</li>
  <li><a href="https://projects.apache.org/project.html?edgent"><strong>Edgent</strong></a> - Apache Edgent is a programming model and micro-kernel style runtime that can be embedded in gateways and small footprint edge devices enabling local, real-time, analytics on the continuous streams of data coming from equipment, vehicles, systems, appliances, devices and sensors of all kinds (for example, Raspberry Pis or smart phones). Working in conjunction with centralized analytic systems, Apache Edgent provides efficient and timely analytics across the whole IoT ecosystem: from the center to the edge.</li>
  <li><a href="https://projects.apache.org/project.html?falcon"><strong>Falcon</strong></a> - Apache Falcon is a data processing and management solution for Hadoop designed for data motion, coordination of data pipelines, lifecycle management, and data discovery. Falcon enables end consumers to quickly onboard their data and its associated processing and management tasks on Hadoop clusters.</li>
  <li><a href="https://projects.apache.org/project.html?flink"><strong>Flink</strong></a> - Flink is an open source system for expressive, declarative, fast, and efficient data analysis. It combines the scalability and programming flexibility of distributed MapReduce-like platforms with the efficiency, out-of-core execution, and query optimization capabilities found in parallel databases.</li>
  <li><a href="https://projects.apache.org/project.html?flume"><strong>Flume</strong></a> - Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store</li>
  <li><a href="https://projects.apache.org/project.html?giraph"><strong>Giraph</strong></a> - Apache Giraph is an iterative graph processing system built for high scalability. For example, it is currently used at Facebook to analyze the social graph formed by users and their connections.</li>
  <li><a href="https://projects.apache.org/project.html?hama"><strong>Hama</strong></a> - The Apache Hama is an efficient and scalable general-purpose BSP computing engine which can be used to speed up a large variety of compute-intensive analytics applications.</li>
  <li><a href="https://projects.apache.org/project.html?helix"><strong>Helix</strong></a> - Apache Helix is a generic cluster management framework used for the automatic management of partitioned, replicated and distributed resources hosted on a cluster of nodes. Helix automates reassignment of resources in the face of node failure and recovery, cluster expansion, and reconfiguration.</li>
  <li><a href="https://projects.apache.org/project.html?ignite"><strong>Ignite</strong></a> - Apache Ignite In-Memory Data Fabric is designed to deliver uncompromised performance for a wide set of in-memory computing use cases from high performance computing, to the industry most advanced data grid, in-memory SQL, in-memory file system, streaming, and more.</li>
  <li><a href="https://projects.apache.org/project.html?kafka"><strong>Kafka</strong></a> - A single Kafka broker can handle hundreds of megabytes of reads and writes per second from thousands of clients. Kafka is designed to allow a single cluster to serve as the central data backbone for a large organization. It can be elastically and transparently expanded without downtime. Data streams are partitioned and spread over a cluster of machines to allow data streams larger than the capability of any single machine and to allow clusters of co-ordinated consumers. Kafka has a modern cluster-centric design that offers strong durability and fault-tolerance guarantees. Messages are persisted on disk and replicated within the cluster to prevent data loss. Each broker can handle terabytes of messages without performance impact.</li>
  <li><a href="https://projects.apache.org/project.html?knox"><strong>Knox</strong></a> - The Apache Knox Gateway is a REST API Gateway for interacting with Hadoop clusters. The Knox Gateway provides a single access point for all REST interactions with Hadoop clusters. In this capacity, the Knox Gateway is able to provide valuable functionality to aid in the control, integration, monitoring and automation of critical administrative and analytical needs of the enterprise.</li>
  <li><a href="https://projects.apache.org/project.html?lens"><strong>Lens</strong></a> - Lens provides an Unified Analytics interface. Lens aims to cut the Data Analytics silos by providing a single view of data across multiple tiered data stores and optimal execution environment for the analytical query. It seamlessly integrates Hadoop with traditional data warehouses to appear like one.</li>
  <li><a href="https://projects.apache.org/project.html?metamodel"><strong>MetaModel</strong></a> - With MetaModel you get a uniform connector and query API to many very different datastore types, including: Relational (JDBC) databases, CSV files, Excel spreadsheets, XML files, JSON files, Fixed width files, MongoDB, Apache CouchDB, Apache HBase, Apache Cassandra, ElasticSearch, OpenOffice.org databases, Salesforce.com, SugarCRM and even collections of plain old Java objects (POJOs). MetaModel isn’t a data mapping framework. Instead we emphasize abstraction of metadata and ability to add data sources at runtime, making MetaModel great for generic data processing applications, less so for applications modeled around a particular domain.</li>
  <li><a href="https://projects.apache.org/project.html?oozie"><strong>Oozie</strong></a> - Oozie is a workflow scheduler system to manage Apache Hadoop jobs. Oozie is integrated with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box (such as Java map-reduce, Streaming map-reduce, Pig, Hive, Sqoop and Distcp) as well as system specific jobs (such as Java programs and shell scripts).</li>
  <li><a href="https://projects.apache.org/project.html?orc"><strong>ORC</strong></a> - ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. Storing data in a columnar format lets the reader read, decompress, and process only the values that are required for the current query.</li>
  <li><a href="https://projects.apache.org/project.html?parquet"><strong>Parquet</strong></a> - Apache Parquet is a general-purpose columnar storage format, built for Hadoop, usable with any choice of data processing framework, data model, or programming language.</li>
  <li><a href="https://projects.apache.org/project.html?phoenix"><strong>Phoenix</strong></a> - Apache Phoenix enables OLTP and operational analytics for Apache Hadoop by providing a relational database layer leveraging Apache HBase as its backing store. It includes integration with Apache Spark, Pig, Flume, Map Reduce, and other products in the Hadoop ecosystem. It is accessed as a JDBC driver and enables querying, updating, and managing HBase tables through standard SQL.</li>
  <li><a href="https://projects.apache.org/project.html?reef"><strong>REEF</strong></a> - Apache REEF (Retainable Evaluator Execution Framework) is a development framework that provides a control-plane for scheduling and coordinating task-level (data-plane) work on cluster resources obtained from a Resource Manager. REEF provides mechanisms that facilitate resource reuse for data caching, and state management abstractions that greatly ease the development of elastic data processing workflows on cloud platforms that support a Resource Manager service.</li>
  <li><a href="https://projects.apache.org/project.html?samza"><strong>Samza</strong></a> - Apache Samza provides a system for processing stream data from publish-subscribe systems such as Apache Kafka. The developer writes a stream processing task, and executes it as a Samza job. Samza then routes messages between stream processing tasks and the publish-subscribe systems that the messages are addressed to.</li>
  <li><a href="https://projects.apache.org/project.html?spark"><strong>Spark</strong></a> - Apache Spark is a fast and general engine for large-scale data processing. It offers high-level APIs in Java, Scala and Python as well as a rich set of libraries including stream processing, machine learning, and graph analytics.</li>
  <li><a href="https://projects.apache.org/project.html?sqoop"><strong>Sqoop</strong></a> - Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.</li>
  <li><a href="https://projects.apache.org/project.html?storm"><strong>Storm</strong></a> - Apache Storm is a distributed real-time computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing real-time computation.</li>
  <li><a href="https://projects.apache.org/project.html?tajo"><strong>Tajo</strong></a> - The main goal of Apache Tajo project is to build an advanced open source data warehouse system in Hadoop for processing web-scale data sets. Basically, Tajo provides SQL standard as a query language. Tajo is designed for both interactive and batch queries on data sets stored on HDFS and other data sources. Without hurting query response times, Tajo provides fault-tolerance and dynamic load balancing which are necessary for long-running queries. Tajo employs a cost-based and progressive query optimization techniques for optimizing running queries in order to avoid the worst query plans.</li>
  <li><a href="https://projects.apache.org/project.html?tez"><strong>Tez</strong></a> - Apache Tez is an effort to develop a generic application framework which can be used to process arbitrarily complex directed-acyclic graphs (DAGs) of data-processing tasks and also a reusable set of data-processing primitives which can be used by other projects.</li>
  <li><a href="https://projects.apache.org/project.html?vxquery"><strong>VXQuery</strong></a> - Apache VXQuery will be a standards compliant XML Query processor implemented in Java. The focus is on the evaluation of queries on large amounts of XML data. Specifically the goal is to evaluate queries on large collections of relatively small XML documents. To achieve this queries will be evaluated on a cluster of shared nothing machines.</li>
  <li><a href="https://projects.apache.org/project.html?zeppelin"><strong>Zeppelin</strong></a> - Zeppelin is a modern web-based tool for the data scientists to collaborate over large-scale data exploration and visualization projects.</li>
</ul>

<p>There is a serious amount of overlap between these projects. Not all of these projects have web APIs, while some of them are all about delivering a gateway or aggregate API across projects. There is a lot to process here, but I think listing them out provides an easier way to understand the big data explosion of projects over at Apache.</p>

<p>It is tough to understand what each of these do without actually playing with them, but that is something I just don’t have the time to do, so next up I’ll be doing independent searches for these project names, and finding stories from across the space regarding what folks are doing with these data solutions. That should give me enough to go on when putting them into specific buckets, and finding their place in my data, and database API research.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/03/looking-at-the-37-apached-data-projects/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/03/temporal-logic-of-actions-for-apis/">Temporal Logic of Actions For APIs</a></h3>
        <span class="post-date">03 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/status-berlin_dark_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m evolving forward my thoughts on algorithmic <a href="http://apievangelist.com/2017/02/28/a-checklist-for-api-observability/">observability</a> and <a href="http://apievangelist.com/2016/08/04/pushing-for-more-algorithmic-transparency-using-apis/">transparency</a> using APIs, and <a href="https://en.wikipedia.org/wiki/TLA%2B">I was recently introduced to TLA+, or the Temporal Logic of Actions</a>. It is the closest I’ve come to what I’m seeing in my head when I think about how we can provide observability into algorithms through existing external outputs (APIs). As I do with all my work here on API I want to process TLA+ as part of my API research, and see how I can layer it in with what I already know.</p>

<p>TLA+ is a formal specification language developed by Leslie Lamport, which can be used to design, model, document, and verify concurrent systems. It has been described as exhaustively-testable pseudocode which can provide a blueprint for software systems. In the context of design and documentation, TLA+ can be viewed as informal technical specifications. However, since TLA+ specifications are written in a formal language of logic and mathematics it can be used to uncover design flaws before system implementation is underway, and are amenable to model checking for finding all possible system behaviours up to some number of execution steps, and examines them for violations. TLA+ specifications use basic set theory to define safety (bad things won’t happen) and temporal logic to define liveness (good things eventually happen).</p>

<p>TLA+ specifications are organized into modules.Although the TLA+ standard is specified in typeset mathematical symbols, existing TLA+ tools use symbol definitions in ASCII, using several terms which require further definition:</p>

<ul>
  <li><strong>State</strong> - an assignment of values to variables</li>
  <li><strong>Behaviour</strong> - a sequence of states</li>
  <li><strong>Step</strong> - a pair of successive states in a behavior</li>
  <li><strong>Stuttering Step</strong> - a step during which variables are unchanged</li>
  <li><strong>Next-State Rlation</strong> - a relation describing how variables can change in any step</li>
  <li><strong>State Function</strong> - an expression containing variables and constants that is not a next-state relation</li>
  <li><strong>State Predicate</strong> - a Boolean-valued state function</li>
  <li><strong>Invariant</strong> - a state predicate true in all reachable states</li>
  <li><strong>Temporal Formula</strong> - an expression containing statements in temporal logic</li>
</ul>

<p>TLA+ is concerned with defining the correct system behavior, providing with a set of operators for working through what is going on, as well as working with data structures. There is tooling that has been developed to support TLA+ including an IDE, model checker, and proof system. It is all still substantially over my head, but I get what is going on enough to warrant moving forward, and hopefully absorbing more on the subject. As with most languages and specifications I come across it will just take some playing with, and absorbing the concepts at play, before things will come into focus.</p>

<p>I’m going to pick up some of my previous work around <a href="http://apievangelist.com/2016/10/06/adding-behaviordriven-development-assertions-to-my-api-research/">behavior driven assertions</a>, and how <a href="http://apievangelist.com/2016/04/08/the-api-assertions-we-make-believe-in-and-require-for-our-business-contracts/">assertions can be though of in terms of the business contracts APIs put forward</a>, and see where TLA+ fits in. It’s all still fuzzy, but API assertions and TLA+ feels like where I want to go with this. I’m thinking about how we can wrap algorithms in APIs, write assertions for them, and validate across the entire surface area of an algorithm, or stack of API exposed algorithms using TLA+. Maybe I’m barking up the wrong tree, but if nothing else it will get me thinking more about this side of my API research, which will push forward my thoughts on algorithmic transparency, and audit-able observability.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/03/temporal-logic-of-actions-for-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/">Database To Database Then API, Instead Of Directly To API</a></h3>
        <span class="post-date">03 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/database-to-database-api.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am working with a team to expose a database as an API. With projects like this there can be a lot of anxiety in exposing a database directly as an API. Security is the first one, but in my experience, most of the time security is just cover for anxiety about a messy backend. The group I’m working with has been managing the same database for over a decade, adding on clients, and making the magic happen via a whole bunch of databases and table kung fu. Keeping this monster up and running has been priority number one, and evolving, decentralizing, or decoupling has never quite been a priority.</p>

<p>The database team has learned the hard way, and they have the resources to keep things up and running, but never seem to have them when it comes to refactoring it and thinking differently, let alone tackling the delivery of a web API on top of things. There will need to be a significant amount of education and training around REST, and doing APIs properly before we can move forward, something there really isn’t a lot of time or interest in doing. To help bridge the gap I am suggesting that we do an entirely new API, with it’s own database, and we focus on database to database communication, since that is what the team knows. We can launch an Amazon RDS instance, with an EC2 instance running the API, and the database team can work directly with RDS (MySQL) which they are already familiar with.</p>

<p>We can have a dedicated API team handle the new API and database, and the existing team can handle the syncing from database to database. This also keeps the messy, aggregate, overworked database out of reach of the new API. We get an API. The database team anxiety levels are lowered. It balances things out a little. Sure there will still be some work between databases, but the API can be a fresh start, and it won’t be burdened by the legacy. The database to database connection can carry this load. Maybe once this pilot is done, the database team will feel a little better about doing APIs, and be a little more involved with the next one.</p>

<p>I am going to pitch this approach in coming weeks. I’m not sure if it will be well received, but I’m hoping it will help bridge the new to the old a little bit. I know the database team likes to keep things centralized, which is one reason they have this legacy beast, so there might be some more selling to occur on that front. Doing APIs isn’t always about the technical. It is often about the politics of how things get done on the ground. Many organizations have messy databases, which they worry will make them look bad when any of it is exposed as an API. I get it, we are all self-conscious about the way our backends look. However, sometimes we still need to find ways to move things forward, and find compromise. I hope this database to database, then to API does the trick.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/">The Waves Of API Driven Bots Invading Our Shores</a></h3>
        <span class="post-date">02 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/beach-rocks-currents_blue_circuit_5.jpg" align="right" width="40%" style="padding: 15p;" /></p>
<p>As each wave of technology comes crashing on the shores of the API space you’ll mostly find me silent, listening and watching what is happening. Occasionally you’ll hear me grumble about the aggressiveness of a single wave, or how unaware each wave is of the rest of the beach, or of the waves that came before them. Mostly I am just yelling back to the waves that claim, “we are going to change the beach forever”, and “we are the wave that matters, better than all the waves that came before us”. Mostly, it is the hype, and the unrealistic claims being made by each wave that bothers me, not the waves themselves.</p>

<p>I do not think that technology won’t have an impact on the beach. I just think that us technologists tend to over-hype, and over-believe in the power each wave of technology, and that we do not consider the impact on the wider beach, and the amount of sand that ends up in everything. I don’t doubt that there will be some gems found in the sand, and that geologically speaking that the ocean plays a significant role in how the coastline is shaped. I’m just choosing to sit back on the bluff and enjoy my time on the beach, and not choosing to be a three year old playing in each of the waves, super excited by the sound each crash makes on the beach. I’m not saying that playing in the waves is wrong, I’m just choosing to look at the bigger picture from up here on the bluff.</p>

<p>You can see one such canvas being painted over the last couple of years with what has become to be known as “bots”. Little automated nuggets of tech goodness, or evil, depending on your location on the beach. People love saying that bots will change everything. They’ll be your assistant. They’ll do everything for you. They’ll automate your life. Take care of your parking tickets. Buy your groceries. Raise your children. Feed hungry people in Africa. When in reality, they tend to be annoying, harassing, and can be mess up an entire election kind of bad. They can DDoS. They can threaten to kill and rape you. But, hey, let’s keep investing in them, and building platforms that support them, without ever acknowledging the negative consequences they have on our beautiful beach.</p>

<p>Some days when I’m swimming in the bot waves I’ll be completely consumed. The undertow grabs me, spins me around, and I don’t know which way is up, and I end up with a mouth and ass-crack full of
sand before I can make my way to the beach. I felt this way over last Christmas as I tried to make sense of the fake news engine, and what was coming out of Russia. Other days I feel like I’m walking on the beach collecting agates, finding some polished glass, but occasionally also finding some really beautiful agates. Today is one of those days, and I seem to be finding more bots that are actually useful, and do one thing well, without all the bullshit, and hype. Showing me the potential of this technology, and the specs of usefulness it can bring to our silicon beach.</p>

<p>I’m finding useful bots that will convert a file for me. Transcribe a audio file. Notify me of a changes within my domain(s), which happens to be beach front real estate. Amidst all the sand and gravel I am seeing meaningful bot implementations. They aren’t anywhere near the automation we have been promised, but they are providing some value. Today I have a handful of these agates in the palm of my hand. We’ll see if I’m actually able to use these in my day to day world. Maybe hang one from the window by a string. Mount one in a piece of jewelry that will be worn infrequently. Who knows, maybe one will become something I hold in my regularly, rubbing, soothing me as I do what I do as the API Evangelist each day.</p>

<p>Even with all of this (potential) usefulness I am finding in today’s bot waves, I’m still reminded of the power of the ocean. The dangers of sneaker waves while I’m heads down looking for agates. The power of the undertow while swimming on a sunny day. The ability for thousands of waves to come in and take away the beach, the bluff, and destroy the house I’ve built, as well as my neighbors. I’m reminded that no matter how shiny each gem is that I find in the waves, or how much I love the sound each wave crashing on the beach, but I can never stop thinking about the power of the ocean at scale. I mean, as our president recently point out, we are all “Surrounded by water. Big water. Ocean water.” Let’s not be distracted by each wave, and make sure we are always paying attention to the bigger picture. I feel like we have to do this for the “kids” as well, so the waves don’t sneak up on them.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/02/the-ca-acquisition-of-runscope/">The CA Acquisition Of Runscope</a></h3>
        <span class="post-date">02 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/runscope/ca-runscope-acquisitions.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>You won’t find me talking about the acquisition of API startups very often. I’m just not a fan of the game. I am not anti-venture capital, but I find the majority of investment in the API startup ecosystem works against everything we are trying to do with APIs. In my opinion, VC investment shouldn’t be the default, it should be an exception. There are other ways to build a business, and I see too many useful API tools get ruined while playing this game. With that said, I tend to not cover the topic, unless I get really pissed off, or the occasional investment or acquisition that I feel will result in a positive result.</p>

<p><a href="https://blog.runscope.com/posts/301">Last week we saw the Runscope acquisition by CA</a>. This is an acquisition that doesn’t leave me concerned. Runscope is a partner of mine, run by people I know and care about, and they offer a tool that is useful in the API sector. If they’d had been acquired by many other bigcos I would have been more concerned, or even upset (if it had been certain ones). However, I have experience with CA, and while they are an enterprise beast, I’ve seen them make acquisitions before that weren’t damaging to the services and tooling they acquired. I trust that CA isn’t acquiring Runscope to just eliminate a strong player from the sector, and that they are actually interested in what Runscope does.</p>

<p>I have seen CA’s role in the API space through <a href="http://www.apiacademy.co/">the lens of the API Academy team,</a> as well as through public and private conversations with other CA employees, on a variety of other teams. I’ve gone on-site and participated in API training session, and I have seen evidence that CA is invested in helping evolve their enterprise to be an API aware organization. Something that you can see reflected in how they approach doing business with their customers. I’m currently working to help move forward some API curriculum with the API academy team, which wouldn’t be happening if I didn’t feel they were committed to helping invest in API literacy across the API space.</p>

<p>The CA acquisition of Runscope doesn’t leave me nervous. I feel like it is a good match. Also, despite the CEO of Runscope, John Sheehan and I often butting heads about startup and VC culture, I feel like he has played the game in an honest and respectful way. He’s made the best choices he could have as a CEO in this game. He cares about making a high quality, useful API product. He genuinely cares about the API space. Even though I think he loves the startup and investment game a little more than he should. All of this leaves me without the indigestion that API startup investment and acquisitions usually leaves in my stomach. I don’t feel like we are losing yet another valuable tool. I feel like CA will be a good steward of Runscope, and the team will actually get the opportunity to evolve, grow, and do better things.</p>

<p>Nice work y’all! Here is to everything being 200 OK!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/02/the-ca-acquisition-of-runscope/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/">Learning About API Design With Resources That Matter To You</a></h3>
        <span class="post-date">02 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/hack-education/hack-education-data.png" align="right" width="35%" style="padding: 15px;" /></p>
<p>I have been helping my partner in crime Audrey Watters (<a href="https://twitter.com/audreywatters">@audreywatters</a>) evolve her data work as part of her Columbia Spencer Education Journalism Fellowship, where she is publishing <a href="https://github.com/hack-education-data/">a wealth of ed-tech funding data to Github</a>. I worked with her to evolve the schema she is using across the Google Sheet, and YAML data stores she is using. Something that will autogenerate APIs (well dynamic JSON) based upon the filename, and the fields she chooses as part of her data stores. I just planted the seeds, and she has been cranking away creating repos, and building data stores since this last summer.</p>

<p>She mentioned to me recently that she thought she had been being consistent in her naming conventions across her work, but had recently noticed some inconsistencies–realizing the importance of a consistent design and schema across the projects, something that really could become problematic at scale if she hadn’t caught. Luckily she was able to fix with some work, and was back on track. She isn’t as automated in the replication of data across her projects, but that is a good thing. It is forcing her to think more deeply about the naming and overall design of her static data APIs, which she uses across many repos, and displayed in a variety of lists, outlines, and stories she is telling around her work.</p>

<p>Audrey has spent seven years listening to me talk about API design blah blah blah, but until she was working with her own data, that she cared about, she didn’t fully grasp some of the API design and implications of working with the access, reusability, and maintenance of data at scale. I’ve offered to automate more of the maintenance, replication, and standardization of data across her repos, but she’s declined. She said she finds it valuable to work with the design, and naming of her data stores, for us in different projects. She likes keeping here YAML data stores in separate repos, and then working with them individually in specific uses cases. As part of her work, she has a master ed-tech investor data store, and API of investors behind each ed-tech company, but then when she aggregates for her wider ed-tech funding, she replicates and names (or renames) it to fit that project.</p>

<p>The work that she is doing is what I consider static API design, where the data is YAML or JSON on Github, but then each project dynamically generates JSON, XML, CSV, or RSS using Liquid, and then also generates HTML UI elements using Liquid as well. It’s not full blown API design, and deployment, but the same API definition, schema, and design concerns come into plays, because if she isn’t thoughtful, and consistent, she will feel the pain at some point at the client level (Liquid/Jekyll). Also, since she is doing this across so many repos, at some point she will begin feeling the pain at a pretty significant scale. However, since she actually cares about the data she is managing, it is important for her to take the time to do it right, and not always opt for automation, so that she can make sure she gets the design and schema details right. Something I wish more API data stewards would realize.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/02/open-api-definitions-for-entire-schema-org-vocabulary-dont-reinvent-wheel/">OpenAPI Definitions For Entire Schema.org Vocabulary (Do Not Reinvent Wheel)</a></h3>
        <span class="post-date">02 Oct 2017</span>
        <p><a href="http://schema.org/"><img src="https://s3.amazonaws.com/kinlane-productions/schema-org/schema-org.png" align="right" width="30%" style="padding: 15px;" /></a></p>
<p>I am preparing my Schema.org Github repo with a variety of data sources for use across my API tooling and other projects. I’m trying to get better at using a common vocabulary, and not reinventing the wheel each time I start a new project. Schema.org has the most robust vocabulary of shared schema available today–so I am using this existing work as the core of mine.</p>

<p>I am slicing and dicing the schema.org vocabulary into several formats that I can use in my OpenAPI-driven editors, and other tooling. <a href="http://schema.org/docs/developers.html">I took the JSON-LD representation for Schema.org</a>, and published it as a simpler JSON schema definition format that can be applied quickly to an OpenAPI. It isn’t perfect, and you lose a lot of the semantics in the process, but I think it still provides an important base for API designers, architects, and developers to use across their OpenAPI.</p>

<p>It is pretty verbose, with over 150K lines, but it provides a fairly consolidated view of Schema.org classes, in a single set of definitions:</p>

<script src="https://gist.github.com/kinlane/a71967811f8ab47c15919d75107c5843.js"></script>

<p>You can download a copy via the Gist, or you can find as <a href="https://github.com/api-evangelist-tools/schema-org/blob/master/_data/definitions/index.json">JSON</a> and <a href="https://github.com/api-evangelist-tools/schema-org/blob/master/_data/definitions/index.yaml">YAML</a> in my Github repository for this work. I’m going to be creating complete OpenAPI for each Schema.org class, as well as individual JSON schema files for each class. I just haven’t to figure out how to decouple them into individual files, yet containing all the relevant schema. I have the code, I just need to dial it in, when I have more time.</p>

<p>I am going to use this Schema.org JSON schema as an autocomplete in my API design tooling, and using the OpenAPI as the source definition for my API deployment and testing tooling. I’ve been evolving <a href="http://org.open.referral.adopta.agency/">my Human Services Data API</a> work to easily generate server side code using OpenAPI, and I’m going to use the same code base to generate any Schema.org API, and deploy as AWS EC2 instance. I’m not looking to develop a SaaS solution, but a quick deploy solution for my own work, and projects I work on with my clients. As I work with more, I will validate that each of these definitions are 100% correct, and properly represent the Schema.org vocabulary.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/02/open-api-definitions-for-entire-schema-org-vocabulary-dont-reinvent-wheel/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/29/2017-the-api-stack-for-disrupting-the-world/">The API Stack For Disrupting The World</a></h3>
        <span class="post-date">29 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/fredericksburg-downtown-flag.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I know people don’t understand why I’m so obsessed with APIs. Sometimes I ask the same question. When I began in 2010, it was 75% about my belief in the good that APIs can do, and 25% about pushing back on the bad things being done with APIs. In 2017, it is 15% about the good, and 85% about pushing back on the bad things that APIs can do. API driven platforms are being used for some pretty shady things these days, and increasingly they are a force for disruption, and not about making the world a better place.</p>

<p>With this in mind, I wanted to take a moment to highlight the API stack right now that is being used to disrupt the world around us. These are the APIs that have shifted the political landscape in the U.S., and are being used to replicate, automate, and scale this disruption around the world.</p>

<ul>
  <li><strong>Facebook</strong> - The network effect is what brings the troublemakers to Facebook.They are on pace to have 2 billion active users. Something that has the potential to create quite a network effect when sharing stories and links, and when you seed that, target it, and grow it using the Facebook advertising engine–it makes for an excellent engine for disruption.</li>
  <li><strong>Twitter</strong> - Twitter is a different beast. Less of the mainstream population than Facebook enjoys, but still a sizable, and very public audience. You can use the Twitter engine to spin things up, get people sharing, do some of the same sharing of stories and links, seeding, targeting, and growing with advertising. Often times the viral nature will spread to Facebook on take on a life of its own.</li>
  <li><strong>Reddit</strong> - Now Reddit is entirely just an organic engine for disseminating information, which makes it great for propaganda, everything fake, and stoking the haters. The network effect that is Reddit, works very, very well will Twitter and Facebook, making for a perfect storm of virality that can spread like wildfire.</li>
  <li><strong>WordPress</strong> - WordPress is where the news, and other websites get published. Because WordPress is an open source solution, it can be installed anywhere. It can be installed as many times as you want, with no costs beyond your hosting. Each of those installations have an API, which allow you to easily publish across hundreds or thousands of installations. When you slap advertising on these beasts, and plant your seeds across Facebook, Twitter, and Reddit, you make for a pretty efficient propaganda machine.</li>
  <li><strong>Google</strong> - Google is the advertising engine for use on the WordPress sites. Google Adwords and Adsense are where disruptors buy the ads they need to plant seeds, but more importantly, it is how they generate revenue from the click throughs, and page views generated from the Facebook, Twitter, and Reddit network effects. Beyond advertising, the Google index provides another great way for the message to spread. All you have to do is play the SEO game, which is something that is greatly aided, and gamed, by the inbound traffic received from Facebook, Twitter, and Reddit.</li>
</ul>

<p>This is a pretty simplistic snapshot of the API surface area that is being used to mess with our realities right now, but if you break down the number API resources available for each of these platforms, you begin to see all the knobs and dials that can be turned at scale, to disrupt the world. I’ll write another post that walks through all the paths and endpoints, but I want to look at the problem from the 100K view, and point the finger at APIs. Without them, such a small group of people wouldn’t be able to do such a huge amount of damage.</p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/ellis-island-nazi-poster.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>The key driver of disruption here is the advertising engine brought to the table by Google, Facebook, and Twitter, which allows troublemakers to sew the seeds of destruction, as well as target, and grow revenue that supports their efforts. Secondarily, the network effect of Facebook, Twitter, and Reddit allow for the larger demographics to be stirred up, and mobilized, and potentially completing the loop for very disruptive memes. When advertising alone can’t drum up the attention needed, the API driven bot armies on Twitter, Facebook, and Reddit pick up the slack, get to work voting up, liking, sharing, until each platforms algorithms get triggered, and do the rest of the work. Not every message will enjoy the virality necessary to do damage, but every once in awhile the disruptors will hit the big time, and there message will take on a life of its own, with each platform doing the rest of the work for them.</p>

<p>All of this would be possible without APIs. However, it would take armies of people to do. APIs are the bullhorn, the amplification, the automation needed to scale this type of disruption. In my next post I will publish a complete list of the knobs and dials that can be turned by the disruptors, to crank up the volume on their campaigns, and to direct their bot armies in support of a specific message, or to attack a target. All of the advertising and targeting engines for the platforms above have APIs, allow campaigns to be automated, and scaled for both spending, and generating revenue. WordPress APIs make the open source platform into a kind of printing press for the propaganda machine, with Facebook, Twitter, Google, and Reddit APIs acting as the messenger boys. APIs are the key element, that makes all of this seem so deafening at the moment.</p>

<p>APIs are not evil, nor are they good, or even neutral. They are just a tool. They only do what the platform operators design them to do, and what the API consumer decides to put it to work doing. This is what has made APIs so interesting to me. When you set this stage, the possibilities for innovation have been great. However, recent history has shown, when advertising is the main revenue engine for both the platforms and consumers, the API providers are more than willing to ignore and look the other way at what is going on. This has made for a perfect storm of disruption at a scale we’ve never seen before, and because of the technical complexity, it is something that most people don’t even see happening. They can’t see the strings. They can’t see how their world is being disrupted. They don’t see their role in it. They don’t understand that things are being amplified and algorithmically distorted–they just think that is the way the world is.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/29/2017-the-api-stack-for-disrupting-the-world/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/29/developing-an-ability-to-repeat-the-same-api-stories-over-and-over-you-are-going-to-need-it/">Developing The Ability To Repeat The Same API Stories Over And Over</a></h3>
        <span class="post-date">29 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/kinlane-whiteboard-api-bw-artsy.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>After seven years of telling stories on API Evangelist I’ve had to repeat myself from time to time. Honestly, I repeat myself A LOT. Hopefully I do it in a way that some of you don’t notice, or at least you are good at filtering the stories you’ve already heard from your feed timeline. My primary target audience is the waves of new folks to the world of APIs I catch with the SEO net I’m casting and working on a daily basis. Secondarily, it is the API echo chamber, and folks who have been following me for a while. I try to write stories across the spectrum, speaking to the leading edge API conversations, as well as the 101 level, and everything in between.</p>

<p>Ask anyone doing API evangelism, advocacy, training, outreach, and leadership–and they’ll that you have to repeat yourself a lot. It is something you get pretty sick of, and if you don’t find ways to make things interesting, and change things up, you will burn out. To help tell the same story over and over I’m always looking for a slightly different angle. Let’s take API Meetups as an example. Writing a story about conducting an API Meetup has been done. Overdone. To write a new story about it I’ll evaluate what is happening at the Meetup that is different, or maybe the company, or the speaker. Diving into the background of what they are doing looking for interesting things they’ve done. You have to find an angle to wrap the boring in something of value.</p>

<p>API documentation is another topic I cover over, and over, and over. You can only talk about static or interactive API documentation so much. Then you move into the process behind. Maybe a list of other supporting elements like code samples, visualizations, or authentication. How was the onboarding process improved? How the open source solution behind it simplifies the process. You really have to work at this stuff. You have to explore, scratch, dig through your intended topic until you find an angle that you truly care about. Sure, it has to matter to your readers, but if you don’t care about it, the chances of writing an interesting story diminishes.</p>

<p>This process requires you to get to know a topic. Read other people’s writing on the topic. Study it. Spin it around. Dive into other angles like the company or people behind. Spend time learning the history of how we got here with the topic. If you do all this work, there is a greater chance you will be able to find some new angle that will be interesting. Also, when something new happens in any topical area, you have this wealth of knowledge about it, and you might find a new spark here as well. Even after all that, you still might not find what you are looking for. You still end up with many half finished stories in your notebook. It is just the way things go. It’s ok. Not everything you write has to see the light of day. Sometimes it will just be exercise for the next round of inspiration. That hard work you are experiencing to find a good story is what it takes to reach the point where you are able to discover the gems, those stories that people read, retweet, and talk about.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/29/developing-an-ability-to-repeat-the-same-api-stories-over-and-over-you-are-going-to-need-it/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/29/tyk-is-conducting-api-surgery-meetups/">Tyk Is Conducting API Surgery Meetups</a></h3>
        <span class="post-date">29 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/tyk/tyk-api-surgery-singapore.jpeg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was having one of my regular calls with <a href="https://tyk.io/">the Tyk team</a> as part of our partnership, discussing what they are up to these days. I’m always looking to understand their road map, and see where I can discover any stories to tell about what they are up to. A part of their strategy to build awarness around their API management solution that I found was interesting, was <a href="https://www.meetup.com/Tyk-Users-Singapore/photos/28111064/463973670/">the API Surgery event they held in Singapore last month</a>, where they brought together API providers, developers, and architects to learn more about how Tyk can help them out in their operations.</p>

<p>API surgery seems like an interesting evolution in the Meetup formula. They have a lot of the same elements as a regular Meetup like making sure there was pizza and drinks, but instead of presentations, they ask folks to bring their APIs along, and they  walk them through setting up Tyk, and deliver an API management layer for their API operations. If they don’t have their own API, no problem. Tyk makes sure there are test APIs for them to use while learning about how things work. Helping them understand how to deliver API developer onboarding, documentation, authentication, rate limiting, monitoring, analytics, and the other features that Tyk delivers.</p>

<p>They had about 12 people show up to the event, with a handful of business users, as well as some student developers. They even got a couple of new clients from the event. It seems like a good way to not beat around the bush about what an API service provider is wanting from event attendees, and getting down to the business at hand, learning how to secure and manage your API. I think the Meetup format still works for API providers, and service providers looking to reach an audience, but I like hearing about evolutions in the concept, and doing things that might bring out a different type of audience, and cut out some of the same tactics we’ve seen play out over the last decade.</p>

<p>I could see Meetups like this working well at this scale. You don’t need to attract large audiences with this approach. You just need a handful of interested people, looking to learn about your solution, and understand how it solves a problem they have. Tyk doesn’t have to play games about why they are putting on the event, and people get the focus time with a single API service provider. Programming language meetups still make sense, but I think as the API sector continues to expand that API service provider, or even API provider focused gatherings can also make sense. I’m going to keep an eye on what Tyk is doing, and look for other examples of Meetups like this. It might reflect some positive changes out there on the landscape.</p>

<p><em><strong>Disclosure:</strong> Tyk is an API Evangelist partner.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/29/tyk-is-conducting-api-surgery-meetups/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/">API Design Industry Guide: GraphQL, A Query Language For APIs</a></h3>
        <span class="post-date">29 Sep 2017</span>
        <p><a href="http://design.apievangelist.com/#Guide"><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-industry-guide-graphql.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><em>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure.</em></p>

<p>GraphQL is a query language designed by Facebook to build client applications using a flexible syntax and provide a system for describing the data requirements and interactions required by each application. GraphQL began as a Facebook project that soon began powering all their mobile applications. By 2015, became a formal specification. GraphQL provides a query language for your APIs that allows users to describe how they would like their API requests be fulfilled. The approach shifts the API design process to be more about request flexibility requiring API providers to design all API paths ahead of time. It opts for an augmented query language over investing in static schema that requires specific API paths.</p>

<p>REST APIs focus on paths to your resources, but GraphQL is all about fields and data types, with everything accessed through a single API path. GraphQL does a better job of providing a more comprehensive approach access to data stored in a database by offloading design to the query layer for interpretation at query render time. The ability to define what data is returned opens up some interesting approaches to delivering resources, especially when it comes to potentially constrained network environments.</p>

<p>When it comes to providing access to data used in responsive web and mobile applications, GraphQL can be successful in allowing application developers to get exactly what they need for an interface and nothing more. This can increase performance and give UI / UX designers more of a voice in what an API does. GraphQL has played a significant role in the evolution of React, Facebook’s open source framework for deploying user interfaces. React is well-known has achieved some significant traction in application development circles. This design approach to delivering data using APIs a natural fit for rapidly delivering web and mobile apps.</p>

<p>GraphQL has seen some significant adoption beyond Facebook, notably at Github and Pinterest. GraphQL strengths become clear when it is used to deliver complex data stores quickly and efficiently by developers that require a greater level of control what data they need. While GraphQL is not traditional API design, it is an important design constraint to consider when planning the future of your API design practices and toolbox.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/">API Design Industry Guide: gRPC, Open Source RPC Framework</a></h3>
        <span class="post-date">28 Sep 2017</span>
        <p><a href="http://design.apievangelist.com/#Guide"><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-industry-guide-grpc-open-source-rpc-framework.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><em>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure.</em></p>

<p>gRPC is a high-performance open source remote procedure call (RPC) framework that is often used to deploy APIs across data centers that also supporting load balancing, tracing, health checks and authentication. While gRPC excels in more controlled, tightly coupled environments, it is also applicable for delivering resources to web, mobile, and other Internet connected devices.</p>

<p>When crafting gRPC APIs, you begin by defining the service using Protocol Buffers, a language and toolset for binary serialization that has support across 10 leading programming languages. Protocol Buffers can be used to generate client and server stubs in these programming languages with tight API/client  coupling  — delivering a higher level of performance than your average REST API and SDK can.</p>

<p>gRPC API design patterns takes advantage of HTTP/2 advances and uses authenticated bi-directional streaming to deliver APIs that can be scaled to millions of RPC calls per second. Its an effective  approach for larger, more demanding API platforms that have begun to see the performance limitations of a more RESTful API design approach. gRPC is not ideal for every API implementation, but is definitely an approach providers should consider when high volumes anticipated, especially within the data center or other tightly controlled environment.</p>

<p>Google has been using gRPC internally for over a decade now, but has recently committed to delivering all their public APIs using gRPC in addition to RESTful APIs, demonstrating that the API design patterns can coexist. This approach makes it a welcome addition to any microservice style architecture. It has  the added benefit of API management features like authentication, tracing, load balancing, and health checking that are required to deliver high  performance.</p>

<p>gRPC is definitely more of an industrial grade API design pattern, shifting APIs into the next gear when it comes to performance. It also leverages the next generation of the HTTP protocol, HTTP/2. While not an API design pattern that every API provider will be working with, they should be aware it exists so that they understand what it is and the role it plays in the space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/">How Do We Help Folks Understand That APIs Are A Journey?</a></h3>
        <span class="post-date">28 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/path-in-the-woods-black-white.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was hanging out with my friend Mike Amundsen (@mamund) in Colorado last month and we ended up discussing folks uncertainty with APIs. You see, many folks that he has been talking to were extremely nervous about all the unknowns in the world of APIs, and were looking for more direction regarding what they should be doing (or not doing). Not all people thrive in a world of unknown unknowns, and not even in a world of known unknowns. Many just want a world of known knowns. This is something that makes the API landscape a very scary thing to some folk, and world where they will not thrive and be successful unless we can all begin to find a way to help them understand that this is all a journey.</p>

<p>I love figuring all of this API stuff out, and I know Mike does too. We like thinking about the lofty concepts, as well as figuring out how to piece all the technical elements together in ways that work in a variety of business sectors. Many folks we are pushing APIs on aren’t like us, and just want to be told what to do. They just want the technology solution to their problem. A template. A working blueprint. It freaks them out to have so many options, possibilities, patterns, and directions they take things. I feel like we are setting folks up for failure when we talk them into embarking on an API journey without the proper training, equipment, support, and guidance.</p>

<p>I think about the last seven years doing this, and how much I’ve learned. Realizing this makes me want to keep doing APIs, just so I can keep learning new things. I thought I understood REST when I started. I didn’t. I thought I understand the web when I started, I didn’t (still don’t). I was missing a lot of the basics, and no matter what folks told me, or how precise their language was, I still needed to bang my head on something over and over before I got it. I was missing a significant amount of why hypermedia can be a good approach without truly understanding content negotiation, and link relations. Realizing how much I still need to explore and learn has only emboldened me on my journey, but I’m not convinced this will be the case with everyone. We are wrong to assume everyone is like us.</p>

<p>As technologists and autodidacts we often overestimate our own ability, as well as what others are capable of. We realize APIs are not a destination, but a journey. However, we suck at explaining this to others. We are horrible at understanding all of the stepping stones that got us here, and recreating them for others. I put myself into this group. I think about this stuff full time, and I still regularly stumble when it comes to on-boarding folks with what API are, and properly helping them in their journey. I still do not have a proper set of on-boarding lessons for folks, <a href="http://101.apievangelist.com">beyond my API 101 stuff</a>. I talk a lot of talk about the API life cycle, the API economy, and all the business and politics of APIs, but I still can’t point folks to where the yellow brick road is. We have to get better at this if we expect folks to ever catch up.</p>

<p>This is one reason I feel <a href="http://zapier.com">Zapier</a>, and other <a href="http://ipaas.apievangelist.com">iPaaS</a> providers are so important. We should be helping people understand APIs and integration in context of the problems they are trying to solve, not in terms of REST, SDKs, or any of the other technical jargon we spew. With Zapier, folks can play with Zaps (recipes) that deliver meaningful API integration that actually solve a problem in their world. They can play with what is possible, without learning all the technical pieces first. They can evolve in their business world, while also progressing on their API journey. IDK. I’m just trying to find ways to help folks better understand what APIs are. I’ll never make everything known to them, but I’m hoping that I can help make folks a little less nervous about the known unknowns, and who knows maybe some day they’ll feel brave enough, and confident in their API awareness that they’ll be able to operate in a world of the unknown unknowns, and settle in on the perpetual journey that are APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/28/api-design-industry-guide-the-restlet-platform-story/">API Design Industry Guide: The Restlet Platform Story</a></h3>
        <span class="post-date">28 Sep 2017</span>
        <p><a href="http://design.apievangelist.com/#Guide"><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-industry-guide-the-restlet-platform-story.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><em>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure.</em></p>

<p>Restlet began as an open source Java API framework over a decade ago and has evolved into an API studio, client, and cloud platform with an API design core. At the center of the API lifecycle management platform is its API designer which gives you a visual view of an API and an OpenAPI or RAML view, providing a machine readable accounting of each API’s contract.</p>

<p>The Restlet Studio allows you to design and document your APIs, starting from scratch, or import existing API design patterns using OpenAPI for RAML. Using the Restlet design UI you can shape the paths, parameters, headers and complete requests and responses for any API. Then, take the definition and actually put it to work in development, staging, or production environments.</p>

<p>Restlet demonstrates how API design is more than just a momentary phase where you are developing APIs and is actively defining every stop along the API lifecycle from design to deprecation. While designing an API in the Restlet API Studio, you can also work to test and automate using the client, helping ensure a usable and complete API is designed. The Restlet Client provides a dashboard to verify the desired API contract in a way that can be shared across teams, with clients, and across stakeholders.</p>

<p>Once the API design process has matured and evolved and is ready for deployment, Restlet empowers production deployment by, generating server and client side code with documentation and a landing page for consumers to access and put an API to work. The Restlet Cloud provides all the components you need to quickly deploy, manage, and scale an API, while using the API design studio as the central place where truth around the API is defined — touching every other aspect of API operations.</p>

<p>Less of a plug for Restlet, I am hoping it is a demonstration of how API design is central to every aspect of API operations and can be central to API service providers. API design isn’t just about the technical design of the surface area of API requests and responses. It is about designing and defining all aspects of doing business using APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/28/api-design-industry-guide-the-restlet-platform-story/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/28/apistrat-and-the-openapi-initiative/">APIStrat And The OpenAPI Initiative</a></h3>
        <span class="post-date">28 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/openapi-membership-september-2017.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>We are getting closer to <a href="http://events.linuxfoundation.org/events/apistrat">APIStrat in Portland, Oregon, October 31st through November 2nd</a>. So I’m going to keep crafting stories that help convince you should be there. It is the first APIStrat conference as an OpenAPI event, operated by the Linux Foundation events team. Steve and I are still playing a big part, and will be MC’ing, but like OpenAPI, APIStrat has grown to the point where we need to let it become more than just something Steve, myself, and the 3Scale team can execute by ourselves.</p>

<p>APIStrat has always been a place where we gather and talk about OpenAPI, going back to when it was affectionately known as Swagger. Tony, and the team have spoken before, and there has been many other sessions, workshops, and keynotes involving the API specification format. This APIStrat is going to be no different, but there will be an even heavier presence for the specification. Since Tony Tam is stepping away, we are giving a full hour on mainstage for him and folks involved in the evolution of OpenAPI to share their story. Darrel Miller will be holding also be holding a workshop on the first day, where several folks involved in the OAI will be sharing knowledge.</p>

<p>There will also be an OAI booth presence, and I know that Jeff ErnstFriedman will be present for OAI membership discussions. If your company is investing in OpenAPI as part of your API operations, and developing tooling around the specification, you should be considering joining the OAI. <a href="https://www.openapis.org/membership/members">Take a look at the current membership list</a>. I’m a member, and so are other heavy hitters like Adobe, Google, Microsoft, IBM, and even my partner in crime 3Scale, and Tyk are present. As a member you get in on the Slack channel conversations, participate on marketing and governance calls, and you get invited to participate on the APIStrat crew (if you want).</p>

<p>Let me know if you are interested becoming a member, I can hook you up with Jeff. He’s the man. If you’d rather, head over to <a href="http://events.linuxfoundation.org/events/apistrat/attend/registration">APIStrat registration</a> and get signed up to join in on the conversation in Portland. I can make sure you get some dedicated time with Jeff there, and he can make sure you get hooked up as a member. If you are looking to be part of the conversation that is APIStrat, and help guide the direction OpenAPI is headed, this is where you need to be. So, I’ll see you in Portland next month, and look forward to hearing what you are up to with your API strategy.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/28/apistrat-and-the-openapi-initiative/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/27/moving-beyond-just-distributed-api-scale-towards-federated-api-scale/">Thinking Beyond Just Distributed API Scale Towards Federated API Scale</a></h3>
        <span class="post-date">27 Sep 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/losangelescloudy/dali_three/file-00_00_35_50.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>You hear a lot about doing APIs at scale in our space. Many folks dismiss web APIs because they feel they won’t scale, and aren’t performing at the scale they envision. The majority of these discussions focus on how do you scale large operations of Twitter, Facebook, or Google scope. A single organization operating API infrastructure at scale, distributed across many geographical regions, supporting millions of users. There are plenty of discussions going on regarding the technology, business, and politics of doing APIs at this scale. I find myself thinking in similar ways, but more federated version of this, where the latest technology might not always be the right answer.</p>

<p><a href="http://org.open.referral.adopta.agency/">My Human Services Data API (HDSA) work</a> is the best example I have of this. Where I’m having to keep the technology, and API definition bar as low as possible to onboard as many people as I possibly can, but then eventually, be able to aggregate large amounts of data across many federated instance. I have 3,144 counties, and 19,354 cities to consider. They should all be speaking a common schema when it comes to the sharing of human services data. Something that is easier said, than done. When you get on the ground you realize many of them are stuck in 1990s, or early 2000s edition of the web, and just do not have the resources needed to move things forward. They can’t afford the latest SaaS service, and they can’t drop the ball, or thousands, or millions of people will suffer–the stakes are high.</p>

<p>When I go into large companies, who have a large teams, and significant number of resources, the conversation around scale is much different. Sure, there is distributed scale. Sure, there is volume scale. However, most times the distribution and volume exists within a single company or organization. A single command and control structure. However, I’m talking about federated distribution and volume, with no single command and control structure. I’m facing not just how you deliver technology across all these nodes, but how do you train, consider extremely short budgets, and other aspects of ensuring things get done consistently, reliably, without disruption. Cities aren’t the only example of this in our world. I’m also seeing the same across state and federal agencies, as well as k-12, and higher educational institutions. Again, where the technological bar is low, but the stakes are high.</p>

<p>This real world is just a different game than tech culture likes to admit. I can’t always take what I learn in the tech sector and immediately apply on the ground in the mainstream worlds. Some of it applies, but honestly I’ve had to throw a lot of seemingly sensible decision out lately, opting for much simpler, web based solutions, that I’m confident staff can be trained on, and can realistically be implemented in existing IT environments. If we keep moving fast and breaking things, and some point we are going to have to stop, pick up some of the pieces, and help take care of the folks we’ve left behind.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/27/moving-beyond-just-distributed-api-scale-towards-federated-api-scale/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/27/api-design-industry-guide-application-level-profile-semantics-alps/">API Design Industry Guide: Application-Level Profile Semantics (ALPS)</a></h3>
        <span class="post-date">27 Sep 2017</span>
        <p><a href="http://design.apievangelist.com/#Guide"><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-industry-guide-alps.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><em>This post is from the latest copy of my <a href="http://design.apievangelist.com/#Guide">API Evangelist API Design Industry Guide</a>, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure.</em></p>

<p>Current API design focusses on using schema to help quantify the payload of the request and response structure of our APIs. JSON Schema, MSON, and other data specifications have emerged to help us quantify the bits we are passing back and forth with APIs. Alongside this evolution, another data format has emerged to help us define simple descriptions of our application-level semantics, similar to how we are using HTML microformats to share data on the web, Application-Level Profile Semantics (ALPS).</p>

<p>ALPS goes well beyond schema, which provides a representation of a plan or theory in the form of an outline or model. ALPS provides a way to define the meaning behind the data, content, and other resources you are making available via an API. ALPS seeks to establish a shared understanding by illuminating the meaning behind hypermedia interfaces (data and state transitions) such as HTML, Collection+JSON, HAL or Siren. It encourages reusability of common profile documents across the media types we are depending on.</p>

<p>Using ALPS you can easily define the common data elements we all use in our API like contacts, todo lists. It can even describe the structure of our APIS for verbose and more useful error responses. What really matters is that you can also define the transitions surrounding these data elements. You can get at the meaning and use behind them, like rolling dice, or playing with a deck of cards. It’s much more than just metadata describing the data elements at work.</p>

<p>If we want our APIs designs to reflect the meaning and interactions around the valuable resources we are serving up, we need to work hard to make sure we are all using common data formats and schema. Schema.org provides us with a good start, but we also need to invest in more in ALPS registries, directories, and dictionaries. These provide machine readable definitions of the common data elements exchanged between systems and applications, as well as the meaning, relationships, interactions, and transitions that make these data elements valuable in our digital worlds.</p>

<p>ALPS has been submitted as an Internet Engineering Task Force (IETF) draft, and provides one possible standard to consider when looking to define the semantics behind API operations.  Visit: http://apis.how/alps-io/ for more information.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/27/api-design-industry-guide-application-level-profile-semantics-alps/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/">A Reminder To Always Have A Plan B For Our API Related Github Infrastructure</a></h3>
        <span class="post-date">27 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-api-evangelist-flagged.jpeg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I had a scare this last weekend regarding my Github infrastructure. My Github organization for API Evangelist was flagged as SPAM and taken down. The Github organization contains almost 100 repositories that I use across my platform. These repositories drive the public side of my research, but also contain YAML files that are used in automation across my entire platform, and network of websites. At about 12:00 PM on Saturday, everything came to a screeching halt, with all the data I depend on to make things go around becoming unavailable.</p>

<p>I have backups of all the data, and the website templates that produce the public side of API Evangelist. I also have a plan B in place for setting up a Jekyll instance that runs on Amazon EC2, but I hadn’t ever actually ran any drills on plan B. After submitting a ticket to Github, I got to work firing up the AWS EC2 instance, and unloading and unpacking the almost 100 website backups for my API Evangelist research. After getting things setup, and as I was preparing to switch over the DNS, I got an email from Github saying:</p>

<blockquote>
  <p>Sorry for the hassle! It appears your organization had been caught up in a spam filter and was flagged incorrectly. I’ve cleared that flag now, so your account should be back to normal. You shouldn’t see that message again, but let me know if I can help with anything else!</p>
</blockquote>

<p>Crisis averted. Luckily this was just my own company Github organization. I operate numerous other API developer portals, code repositories, documentation sites, and other API related projects and tooling that lives entirely on Github. If my personal account was frozen, or any of these organizations taken offline, I would have been in a lot more hot water, and accountable to my clients. Overall I was down for a little over six hours. It showed me the fragile nature of depending on Github, not just for my data driven project public presence, but also it being the center of so many workflows that depend on the YAML, JSON, and code that I publish there.</p>

<p>The experience has forced me to look at my backup process some more, and given me the opportunity to actually run a live drill on failing over to a secondary provider. Even with this failover I still wouldn’t have the Github API available as part of these data project workflows, an API that I depend on, which I really cannot replace. I can replace Git, and Jekyll, but not the Github API portion of the orchestration I depend on so heavily. I have to do some more meditating on this dependency in my world. Don’t get me wrong. I love me some Githubz, but this worries me. Any API dependency worries me if I can’t easily replicate and replace. But, I guess this is what the API game is often about right? Establishing dependencies that are difficult or impossible to walk away from. ;-(</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/27/the-value-of-api-driven-events/">The Value of API Driven Events</a></h3>
        <span class="post-date">27 Sep 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/hermosabeach/dark_dali/file-00_00_11_64.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am spending a lot of time lately thinking about event sourcing, evented architecture, real time, and webhooks. I’m revisiting some of the existing aspects of how we move our bits around the Internet in real time and at scale as part of existing conversation I am having, as well as some projects I’m working on. <a href="http://apievangelist.com/2017/08/21/making-sense-of-api-activity-with-webhook-events/">I recently wrote about making sense of API activity with webhook events</a>, and as I’m crafting a list of meaningful events for <a href="http://org.open.referral.adopta.agency/">my Human Services Data API (HSDA) work</a>, I’m thinking about how these events reflect the value that occurs via API platforms.</p>

<p>As I’m going through the different APIs I’m exposing via a platform, I am working to identify and catalog events in which folks can subscribe to using webhooks. These are the events that occur, like adding a new organization, updating a service, or completing a batch import–all the things people will care about the most. These are the events and activities that occur because their is an API, which have the most value to API consumers, and platform operators. This is what actually matters, and why we are doing an API in the first place, to enable these events to occur. The more these events are triggered, and the more people we have subscribing and engaging with these events, the more value that is generated using an API.</p>

<p>In aggregate, using modern approaches to API management, we might provide analytics and reports that demonstrate all this value being created, to justify the existence of our API. In some implementations, this value created is how we might be charging our API consumers, partners, and other stakeholders. However, in some cases we might even considering paying API consumers when these events occur, incentivizing a certain event-driven behavior that benefits the platform. It is easy to think of API value generation simply as the number of API calls, but I think webhooks has helped establish a new way to look at how value is generated, based upon the number of subscribers to any particular event, or possible a type of of event.</p>

<p>I feel like this is one of the reason we are finally seeing more investment in event sourcing, and evented architecture, and the real time streaming of data and content. The events that matter are getting prioritized, and the technology is advancing to support these events that matter. IDK. As I push forward with my webhook research, and revisit my real time API research, and expand into new realms of messaging and focusing on events, I’m rethinking how we measure and quantify value generation via API platforms. For a long time the measure has been number of API consumers, and the number of API calls, but I feel like things are shifting to the types of events that occur, and how meaningful these events are to API providers, consumers, and their application end-users.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/27/the-value-of-api-driven-events/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/27/caching-for-your-api-is-easier-than-you-think-and-something-you-should-invest-in/">Caching For Your API Is Easier Than You Think And Something You Should Invest In</a></h3>
        <span class="post-date">27 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/internet-gauages-3.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m encountering more API providers who have performance and scalability concerns with their APIs, who are making technical procurement decisions (gateways, proxies, etc) based upon these challenges, but have not invested any time or energy into planning and optimization of caching for their existing web servers that are delivering their APIs. Caching is another aspect of HTTP that I keep finding folks have little or no awareness of, and do not consider more investment in it to assist them in alleviating their scalability and performance concerns.</p>

<p>There was a meeting I attended a couple weeks back where an API implementation was concerned about a new project for bulk loading and syncing of data between multiple external systems and their own, because of the strain it put on their database. Citing that they received millions of website, and API calls daily, they said they could not take the added load on their already strained systems during the day, limiting this type of activity to a narrow window at night. I began inquiring regarding caching practices in place on web, and API traffic, and they acknowledged that they new of no such activity or practices in place. This isn’t uncommon in my experiences, and I regularly encounter IT groups who just don’t have the time and HTTP awareness to implement any coherent strategy–this particular one just happened to admit it.</p>

<p><a href="http://www.apiacademy.co/how-to-http-caching-for-restful-hypermedia-apis/">My friends over at the API Academy have a great post on caching for RESTful and Hypermedia APIs</a>, so I won’t be addressing the details of HTTP, and how you can optimize your APIs in this way. API caching isn’t an unproven technology, and it is a well known aspect of operating on the web, but it does take some investment and awareness. Like API design in general, you have to get to know the resources you are serving up, understand how your consumers are putting these resources to work, and adjust, dial-in, and tweak your caching strategy. It is something that gets incrementally harder, the more time zones you operate in, but with some investment you can significantly increase the scalability of your APIs, the performance of properly cached paths, and do more with less resources. Scaling the size of your server isn’t always the first sensible thing you should be doing, a coherent caching strategy will be a much wiser and cost-effective approach in the long run.</p>

<p>A lack of API caching strategy amongst my clients and readers has a damaging effect on API operations. However, I’d say the most damage done isn’t by the lack of a strategy, it is the reverberating decisions made around the inability to properly scale, and deliver the performance API clients are needing. I see many technology procurement decisions being made where scalability and performance are a major part of the conversation and decision making process. Where conversations around API caching have never occurred. This is just lazy. This is just ignoring one of the key tenets of what makes the web work. This is just investing in technical debt, over making sensible architectural decisions, and spending the time to get to know the resources you are serving up, and how your customers are using them. Learning about HTTP, and caching does take some investment and planning, but it is nowhere the investment and planning that will be required to unwind the technical debt you’ve acquired made from the other bad technology purchasing decisions you’ve made along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/27/caching-for-your-api-is-easier-than-you-think-and-something-you-should-invest-in/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/26/api-design-industry-guide-api-stylebook/">API Design Industry Guide: API Stylebook</a></h3>
        <span class="post-date">26 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-industry-guide-api-stylebook.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>This post is from the latest copy of my <a href="http://design.apievangelist.com/#Guide">API Evangelist API Design Industry Guide</a>, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure.</em></p>

<p>Arnaud Lauret (@arno_di_loreto), the API Handyman (@apihandyman), has been developing an API Stylebook that provides a collection of resources for API designers. It is a brilliant aggregation of thirteen API design guides from Atlassian, Cisco, Cloud Foundry, Google, Haufe, Heroku, Microsoft, PayPal, Red Hat, The White House, and Zalando. It highlights best practices used by leading API providers.</p>

<p>“The API Stylebook aims to help API Designers to solve API design matters and build their API design guidelines by providing quick and easy access to selected and categorized resources”, says Lauret. A unique community resource, it provides deep linking to specific topics within publicly available API design guidelines. Instead of reinventing the wheel or searching Google for hours, API designers quickly can find solutions and inspiration from these existing best practices.</p>

<p>More than just a list of guidelines, it is a machine readable distillation of the thirteen API design guides into a master list of API design topics you can consider when crafting your own API design guide. It is slick. I like Arnaud’s approach to analyzing the existing API design patterns across the API platforms who have shared their guides. I also really like the YAML approach and it’s presented as a very good looking website using Github, and Github Pages.</p>

<p>This is how API literacy tools should be constructed and it provides a valuable lesson in API design. You can take that lesson and execute what you’ve learned along the way, with a very hands-on process. Using the API Stylebook, you can craft an API design guide for your team to follow and employ across API operations. Anyone can fork the API Stylebook, pick and choose the best practices across the thirteen API design guides, and then publish a version as a Github repository. The resulting repo can easily be included in your API docs, as a standalone website.</p>

<p>There are two things going on here. First, API providers are sharing their views on API design best practices — important stuff. Second,  an aggregator (API Handyman) makes these best practices machine-readable, forkable, and reusable. This knowledge layer of API operations becomes even more valuable when it is openly accessible and shareable in this way. We need our APIs to employ common patterns and speak in common formats so that they’ll work together and reduce friction in the industries where they being put to work. API Stylebook approaches API design through continuous integration, allowing API development groups to define API methodologies that can be deeply integrated into our lifecycle.</p>

<p>The more API design guides that are sourced in the API Stylebook, the better the available topics will become. Arnaud will be adding new API design guide to the stack. It is better for the community if you start with his existing list of topics and add your own YAML definitions that drive the API Stylebook on Github, but we’ll take what we can get. It is a process that every company, organization, institution and government agency should go through and what you learn along the way is essential for any API team.</p>

<p>API design isn’t just knowing the details of REST or any single architecture pattern. It’s the knowledge you gain from the definition process, and a huge part of this process is having an effective pool of community knowledge to pull from. Arnaud has done an amazing job at processing disparate and unstructured API design and yielding coherent list of topics that we can all consider in our own API design process. We should all be investing in (and building on) his work.It is something I’m baking into my API design research wherever I can — dovetailing the common building blocks I’ve aggregate across his very detailed work.</p>

<p>REST is a philosphy, not a standard. The current wave of web API success we see across the technology landscape has been about API providers building on top of the best practices of the web API providers that came before them, going back to the early pioneers of SalesForce, eBay, and Amazon. Every API architect I know has learned from reverse enginnering and studying the practices of what they’d consider to be well designed APIs, and understanding the bad design practices out there. If knowledge isn’t shared, then the next generation of API developers do not learn from what came before them. Making the practice of publishing your API design guide for your API not just good for your own API operations, but good for the entire API community.</p>

<p>The most significant API movements in the last five years came from Apiary in my opinion, but the most significant API movements in the next five years will be about sharing API design patterns on Github. Allowing API providers to borrow from each other, resuse, and communicate around common API patterns that are working, or not working. API definitions are allowing us to publish these common design patterns to Github, and the next generation of continous integration and deployment tooling like API Stylebook allow us to publish, share, and even test the quality of our APIs.</p>

<p>I will be continuing to invest in API Stylebook, writing stories about it, and helping API providers understand the value of the open source project and sharing of their own API design patterns, and since it’s machine readable, I will be integrating  the API Stylebook into all of my API design research.</p>

<p align="center"><a href="http://design.apievangelist.com/#Guide"><strong>Get A Copy Of The API Evangelist API Industry Guide</strong></a></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/26/api-design-industry-guide-api-stylebook/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/26/keeping-the-web-layer-in-kafka-with-the-rest-proxy/">Keeping The Web API Layer In Kafka With A REST Proxy</a></h3>
        <span class="post-date">26 Sep 2017</span>
        <p><a href="https://www.confluent.io/"><img src="https://s3.amazonaws.com/kinlane-productions/confluent/confluent-kafka-platform.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m slowly learning more about <a href="https://kafka.apache.org/">Kafka</a>, and the other messaging and data streaming solutions gaining traction in the API space. If you aren’t on the Kafka train yet, “Kafka is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.” I’m still learning about how Kafka works, and with no real production experience, it is something that is taking time.</p>

<p>As part of my conversations on the subject, I was introduced to <a href="https://www.confluent.io/">Confluent</a>, a platform version of Kafka, which is the quickest way I have seen to get started with real-time data streams. As part of the Confluent offering I noticed they have a REST proxy, which you can find <a href="https://docs.confluent.io/current/kafka-rest/docs/api.html">the API documentation here</a>, and <a href="https://github.com/confluentinc/kafka-rest">the code for the Kafka REST proxy on Github</a>. According to the Github repo, “the Kafka REST Proxy provides a RESTful interface to a Kafka cluster. It makes it easy to produce and consume messages, view the state of the cluster, and perform administrative actions without using the native Kafka protocol or clients.”</p>

<p>I’ve noticed that many of the other messaging and data streaming solutions out of Apache these days have diverted from using REST, which makes sense for speed, and scale, but when it comes to reaching a wider audience I can still see the need to have RESTful API. Delivering a kind of multi-speed solution that allows developers to pick their speed based upon their skills, awareness, and need. I’m feeling like the platform approach of Confluent, combined with a RESTful layer, will give them an advantage over other Kafa service providers, or just deploying the open source solution out of the box.</p>

<p>REST isn’t always the most efficient, or scalable solution, but when it comes to reaching a wide audience of developers, and allowing consumers to get up and running quickly, REST is still a sensible approach. Honestly, I don’t think it is just REST, it is also about leveraging the web. Not that everyone understand the web, but I think it is what a large number of developers have been exposed to, and have been building on in the last decade. I can see high volume API solutions in the future often having a native protocol and client, but also supporting REST, and gRPC to make their solutions more accessible, performant, scalable, and quickly adopted and integrated alongside existing infrastructure.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/26/keeping-the-web-layer-in-kafka-with-the-rest-proxy/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/26/why-does-aws-charge-by-usage-and-other-apis-still-use-plans/">Why Does AWS Charge By Usage And Other APIs Still Use Plans?</a></h3>
        <span class="post-date">26 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/adam-smith_feed_people.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="https://aws.amazon.com/blogs/aws/new-per-second-billing-for-ec2-instances-and-ebs-volumes/">Amazon Web Services recently updated their billing for EC2 instances to be by the second</a>, which I really like, because I’ll fire up an instance and run for minutes, then shut things down. I’m just looking to process patent downloads, or other intensive workload projects. Beyond just EC2, the rest of Amazon’s platform is still very usage based. Meaning, you get charged for whatever you use, with unit pricing for each resource designed to compliment how it gets put to use. You get charged for the hard costs of compute, storage, and bandwidth, but you also see per message, job, entry, and other types of billing depending on the type of resource being delivered via API.</p>

<p>With this model for doing APIs, I’m wondering why so many API providers still have access plans and tiers. I’ve vented several times that I think service tiers are a legacy of a SaaS way of thinking and does not scale for API consumers. Maybe back when we used a handful of APIs, but the number of APIs I’m using is pushing 50 these days, and I can’t alway justify a monthly subscription to get what I need. I’m looking to just get access to valuable API resources, and get billed for whatever I use. If I don’t use anything for 6 months, I don’t get billed for anything. Also, I want to be able to run large jobs which consume intense amounts of resources without hitting tier and other limits–just charge me for what I use. If I have a $1,000.00 to spend today, let me spend it. Don’t make me jump through hoops.</p>

<p>I know the answer to my question regarding why so many API startups do this. It is because the resources being provided via the API isn’t the product, us API consumers are. They are looking to ensure a certain level of headcount, monthly, and annual subscribers, so that they can sell us to their investors, and ultimately whoever purchases us like cattle in the end. I’m sure there are other reasons for having pricing tiers, but this is still the primary reason we see SaaS based pricing continue to be so pervasive in an API driven world. For me, if an API provider has tiered pricing, I’ll almost always go look elsewhere. I just can’t manage 40 or 50 subscriptions, and if the number of APIs keep growing, I can’t handle 100-200 subscriptions. I just need to pay for the resources my business needs, and nothing more. I sure don’t have time to be a product in your startup cattle auction.</p>

<p>Pay as you go, usage based pricing is one way the cloud giants will suffocate out the small startups in the future. While startups are trying to court investors, and their acquirers, the cloud giants will just offer a competing service, drop the price to run competitors off, then once the coast is clear, raise prices back up to an profitable state. To compete, more API providers will have to go with a utility based pricing strategy, while also offering wholesale versions of their APIs in the marketplaces for AWS, Azure, and Google. I can’t help think that things might have been different if the scales didn’t tip so hard towards all of us API consumers being the product, and API providers focused more on running businesses, and catering to our real world business needs. Oh well, we got the world we have, I’ll just keep plowing forward.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/26/why-does-aws-charge-by-usage-and-other-apis-still-use-plans/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/26/apis-arent-just-about-mobile-make-sure-you-are-considering-the-bigger-picture/">APIs Are Not Just About Mobile, Make Sure You Are Considering The Bigger Picture</a></h3>
        <span class="post-date">26 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/photos/space-suit.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>When I started API Evangelist in 2010, API usage in mobile phones was the biggest factor contributing to me quitting my job, and becoming a independent voice for all APIs. I was being asked to deliver APIs to drive mobile applications on the iPhone, and while helping run technology for Google I/O I saw an increased need for resources to be delivered to this emerging platform. I knew that APIs were going to play an essential role in ensuring data, content, and algorithms could be put to use in mobile applications.</p>

<p>Even with the importance of mobile, it wasn’t the only reason I knew that APIs were going to be important, which is something that still resonates today. In 2007, I saw the growing importance of social media APIs, and how messaging, images, and video were being made more distributed using APIs. Then in 2008, I saw that I could deliver global infrastructure using web APIs, demonstrating that web APIs weren’t a toy, and that you could operate a real business using web APIs. Then the whole mobile thing was just the tipping point, which demonstrated that the web was maturing beyond just websites, and it would be how we’d be doing business for some time to come.</p>

<p>Every day I see people with blinders on focusing in on one slice of the API pie, seeing APIs as purely about commerce, social, cloud, mobile, IoT, messaging, or other growing aspect of the API economy. People are good at seeing things through the lens of their products, services, and industry. It is easy for them to ignore those people over there, or the other aspects of why leveraging the web is so important to all of this working. They get excited about a new open source solution, protocol, or pattern, and focus in exclusively on a single aspect of how we deliver technology–sometimes at the cost of other areas of their operations, or the future. If mobile is your world, and you are in the business of building top notch mobile apps, then this is the world you see.</p>

<p>I am in the business of web APIs. Understanding the technology, business, and politics of delivering data, content, and algorithms by leveraging the web. I’m not in the business of commerce, social, mobile, or IoT. I’m also not betting on bots, voice enablement, serverless, or the blockchain. I’m in the business of understand how the web can make all of these things work, or not work. I’m always fascinated how passionate folks get about a specific approach, and even aggressive about how they communicate about why it is better than everything else. It is also interesting how they are so willing to ignore the negative consequences, as part of their passionate belief system. You see this playing out at Facebook right now on a pretty large scale–a whole lot of unintended consequences from many good folks believing delusionally in the power of technology, and it a significant amount of pushback before they’ll change their tune.</p>

<p>In my seven years as the API Evangelist I’ve been tempted to keep focused on just government, or maybe just open data, or possibly social good APIs, but I know the dangers of doing this. I’m happy to support folks who are down in their silos, and don’t give them grief for not seeing the big picture. However, when folks shut me down, question my agenda, and are 100% confident they have the answers, I have to step back, and let them continue on their journey alone. There is a certain privilege that comes along with living in technological silos, and I feel like some people I know who are doing APIs in the service of mobile have their blinders on right now and are ignoring their web roots, as well as the web future. Also, they are missing out on opportunities for learning around how APIs being used on devices, the network, as part of bots and automation, voice and conversational interfaces. Remember, that APIs aren’t just for mobile, and you should be ready and open to use APIs for anything that might come along, and not ignoring the bigger picture.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/26/apis-arent-just-about-mobile-make-sure-you-are-considering-the-bigger-picture/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/26/concerns-around-working-with-api-evangelist-at-bigcos/">Concerns Around Working With The API Evangelist At Large Organizations</a></h3>
        <span class="post-date">26 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/kin-chesapeake-sun_dark_dali.jpg" align="right" width="30%" style="padding: 15px;" /></p>
<p>I know that I make some tech companies nervous. They see me as being unpredictable, with no guarantees regarding what I will say, in a world where the message should be tightly controlled. I feel it in the silence from many of the folks that are paying attention to me at large companies, and I’ve heard it specifically from some of my friends who aren’t concerned with telling me personally. These concerns keep them from working with me on storytelling projects, and prevent them from telling me stories about what is happening internally behind their firewall. It often doesn’t stop employees from telling me things off the record, but it does hinder official relationships, and on the record stories from being shared.</p>

<p>I just want folks to know that I’m not in the scoop, or gotcha business. I only check-in on my page views monthly to help articulate where things are with my sponsors. I’m more than happy to keep conversations off the record, anonymize sources and topics. Even the folks in the space who have pissed me off do not get directly called out by me. Well, most of them. I’ve gone after Oracle a couple of times, but they are the worst of the worst. There are other startups and bigcos who I do not like, and you don’t ever hear me talking trash about them on my blog. Most of my rants are anonymized, generalized, and I take extra care to ensure no enterprise egos, careers, or brands are hurt in the making of API Evangelist.</p>

<p>If you study my work, you’ll see that I talk regularly with federal government agencies, and large enterprise organizations weekly, and I never disclose things I shouldn’t be. If you find me unpredictable, I’m guessing you really haven’t been tuning into what I’ve been doing for very long, or your insecurities run deeper than anything to do with me. I’m not in the business of making folks look bad. Most of the companies who are looking bad in the API space do not need my help, they excel at doing it on their own. I’m usually just chiming in to help amplify, and use as a case study for what API providers should consider NOT DOING in their own API operations. Sure, I may call you out for your dumb patents, and the harmful acquisitions you make, but anything I rant about is going to already be public material–I NEVER do this with private conversations.</p>

<p>So, if you are experiencing reservations about sharing stories with me, or possibly sponsoring some storytelling on API Evangelist because you are worried about what will happen, stop fretting. If you are upfront with me, clear about what is on the record, and what is off, and honest about what you are looking to get out of the relationship, things will be fine. Even if they end up being rocky, I’m not the kind of person to call you out on the blog. I may complain, rant, and vent, but you can look through seven years of the blog and you won’t find me doing that about anyone I’ve specifically worked with on storytelling projects. I don’t always agree with why corporations, institutions, and government agencies are so controlling of the message around their API operations, but I will be respectful of any line you draw for me.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/26/concerns-around-working-with-api-evangelist-at-bigcos/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/25/latest-copy-of-the-api-evangelist-api-design-industry-guide/">Latest Copy Of The API Evangelist API Design Industry Guide</a></h3>
        <span class="post-date">25 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-guide-2017-09-25.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve been struggling to get the latest edition of my industry guides out the door. I have a new Adobe Indesign format which I really like as a constraint, but is also pushing my desktop publishing skills. What is really kicking my ass though, is the editing. This latest copy was professionally edited, but I ran out of money to pay him on future guides, and I ended up making some slight changes to this one as well. I am very self-conscious of my grammar and spelling mistakes. I’m capable of editing my own stuff, and my grammar and spelling is high quality. The problem is that I’m too close to the content, and with each edit I make changes, which then introduce new mistakes. Also my brain moves too fast sometimes, and I just make silly mistakes, and overlook things by just reading it the way my brain intended.</p>

<p>Anyways, I’m over stressing on it all. I just want to get my guides out. I have too much of a back log, and since I can afford a professional editor to shadow my work, I’m just going to put them out there. If you find mistakes, feel free to <a href="https://github.com/api-evangelist/design/issues">submit a Github issue on the repo for my API design research</a>. I have too many guides to get out, and it is more important to me that my research moves forward, I spend the time distilling things down into a guide, and hitting publish. I can’t wait for perfect. If folks discount my work because I’m moving so fast, too bad. It is more important that the knowledge is in my head. If you want to help fund me so I can properly afford an editor, I welcome that as well–I have one who will work with me full time, I just need the cash! Anyways, I’m finally getting around to publishing this edition of the API design industry guide, which I hope provides a snapshot of the space.</p>

<p>My <a href="http://design.apievangelist.com/#Guide">API Evangelist API Design Industry Guide</a> is not meant for the API echo chamber. It is meant for executives, business folks, IT, and developers who are looking to do APIs outside of the mainstream tech community. My goal isn’t to cover in detail every aspect of API design. My goal is to cover the industry of API design, while focusing on the highlights of each working area. I track on the service providers who deliver solutions in the API design space, as well as some of the open source tooling that is available. Then I try to look at some of the common building blocks of APIs design, with an emphasis on REST and hypermedia. I also inject a handful of one and two page articles in there covering a variety of topics, as well as how the world of API design is shifting with the introduction of new approaches like gRPC and GraphQL. My definition of API design is not dogmatically REST. It is about pragmatically stepping back from API development and thinking about the best patterns available to us in the space.</p>

<p>Once I get all of my core research area published in this new format, I will work to update them more regularly, and try to keep them rolling forward with new versions. If you would like to sponsor one, or invest in <a href="http://apievangelist.com/api-lifecycle/">one of the other 85+ areas of my API industry research</a>, feel free to reach out. Thanks for your patience while I found the mojo to work on these again, and your help in identifying any errors or mistakes I’ve made. Also, take notice my new approach to making these available, <a href="http://apis.how/jxcvi8wng2">where you can always download them for free</a>, or you can <a href="https://gum.co/mujnIl">purchase the latest copy using Gumroad for a small fee</a>. This helps support my work, and you’ll be added to the mailing list, and automatically get a free copy when I update in the future. I’m moving forward to work on my API deployment, and management guides, as well as some sponsored guides in the areas of data, database, and the trend of fake news, accounts, bots, and more.</p>

<p>Thanks again for your support!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/25/latest-copy-of-the-api-evangelist-api-design-industry-guide/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/25/considering-the-future-of-the-openapi-initiative/">Considering The Future Of The OpenAPI Initiative</a></h3>
        <span class="post-date">25 Sep 2017</span>
        <p><a href="https://www.openapis.org/"><img src="https://s3.amazonaws.com/kinlane-productions/openapi/OpenAPI_Pantone.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="https://www.openapis.org/membership/members">I’m a member of the OpenAPI Iniative (OAI)</a>. I’m not very active on the governance or marketing, but I enjoy hanging out in the hallways of the Slack channel, and being part of the conversation. I’m pretty confident in the core group’s ability to steer the direction of the specification, and leave my influence to be more about storytelling externally, and planting seeds in the minds of folks who are putting the API specification to use. I have a much different style to influencing the API space than many of the companies I share membership within the OAI–it is just my way.</p>

<p>I am working with more groups to help them craft, maintain, and evangelize around a specific OpenAPI definition, for use in a specific industry. THe primary one on the table for me is the <a href="https://openreferral.github.io/api-specification/">Human Services Data API (HSDA)</a>. Which is an OpenAPI for helping cities, municipalities, and non-profit organizations that help deliver information around human services, speak a common language. This is just one example of industry specific API definitions emerging. I am seeing OpenAPI emerge for PSD2, FHIR, helping guide the conversation going on in the financial and healthcare sectors.</p>

<p>The OpenAPI as a top level API specification standard is maturing, and is something that reached version 3.0, and once the services and tooling catch up, we’ll see another boom in industry specific API definitions emerge. This is when we are going to see the need to start harmonizing, standardizing, and merging many disparate standards into a single specification, or at least interoperable specifications. You see this happening right now with OpenAPI, API Blueprint, and RAML–they are all part of the OpenAPI Initiative (OAI). In the next five years you will see this same thing begin occurring for other industry specific APIs, and we’ll eventually need governing bodies to help move forward these independent efforts, as well as feed needs back up the supply chain to OpenAPI.</p>

<p><a href="http://openreferral.org/"><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/OpenReferral_Logo_Green.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>Maybe not right away, but eventually the OpenAPI will need to start thinking about how it establishes separate industry working groups dedicated to specific implementations of OpenAPI. I can see a healthcare, banking, education, transportation, messaging, and other specific industries emerge, needing a more stabilized spec, and formal group to help drive forward incarnations of the spec. It’s taking me awhile to get the HSDA working group to be OpenAPI literate, but I’m already seeing the speed picking up, as different members learn to contribute to the HSDA OpenAPI, and understanding it is a central truth for code, documentation, testing, and for everything we are doing as part of the working group.</p>

<p>Just some food for thought. Like I said, we are a long way off from needing this, but I can already see it on the horizon. I’d just like to plant the seed with the OAI, as well as with folks out there who are pushing forward a specific OpenAPI within an industry. As you look to formalize what you are doing you might want to join the OAI, and participate in some of the conversation going on at the higher level. <a href="http://events.linuxfoundation.org/events/apistrat">Maybe come to APIStrat in Portland this November, and bring your OPenAPI discussions with you</a>. APIStrat has long been the place where we hammer out API definition conversations, so it make sense to keep going, especially now that it is an official OpenAPI (OAI) conference. If you have any questions about what I’m doing with HSDA, and the future of industry specific API definitions, feel free to reach out.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/25/considering-the-future-of-the-openapi-initiative/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/25/data-streaming-in-the-api-landscape/">Data Streaming In The API Landscape</a></h3>
        <span class="post-date">25 Sep 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/goldsilverfalls/creativity/file-00_00_40_84.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was taking a fresh look at <a href="http://realtime.apievangelist.com/">my real time API research</a> as part of some data streaming, and event sourcing conversations I was having last week. My research areas are never perfect, but I’d say that real time is still the best umbrella to think about some of the shifts we are seeing on the landscape recently. They are nothing new, but there has been renewed energy, new and interesting conversation going on, as well as some growing trends that I cannot ignore. To support my research, I took a day this week to dive in, have a conversation with my buddy Alex over at the <a href="https://thenewstack.io/">TheNewStack.io</a>, and the new CEO of WSO2 Tyler Jewell around what is happening.</p>

<p>The way I approach my research is to always step back and look at what is happening already in the space, and I wanted to take another look at some of the real time API service providers I was already keeping eye on in the space:</p>

<ul>
  <li><a href="https://www.pubnub.com"><strong>Pubnub</strong></a> - APIs for developers building secure realtime Mobile, Web, and IoT Apps.</li>
  <li><a href="https://streamdata.io/"><strong>StreamData</strong></a> - Transform any API into a real-time data stream without a single line of server code.</li>
  <li><a href="https://fanout.io/"><strong>Fanout.io</strong></a> - Fanout’s reverse proxy helps you push data to connected devices instantly.</li>
  <li><a href="https://firebase.google.com/"><strong>Firebase</strong></a> - Store and sync data with our NoSQL cloud database. Data is synced across all clients in real time, and remains available when your app goes offline.</li>
  <li><a href="https://pusher.com/"><strong>Pusher</strong></a> - Leaders in real time technologies. We empower all developers to create live features for web and mobile apps with our simple hosted API.</li>
</ul>

<p>I’ve been tracking on what these providers have been doing for a while. They’ve all been pushing to boundaries of what is streaming, and real time APIs for some time. Another open source solution that I think is worth noting, which I believe some of the above services have leverages is Netty.io.</p>

<ul>
  <li><a href="http://netty.io/"><strong>Netty</strong></a> - Netty is an asynchronous event-driven network application framework
for rapid development of maintainable high performance protocol servers &amp; clients.</li>
</ul>

<p>I also wanted to make sure and include Google’s approach to a technology that has been around a while:</p>

<ul>
  <li><a href="https://cloud.google.com/pubsub/docs/"><strong>Google Cloud Pub/Sub</strong></a> - Google Cloud Pub/Sub is a fully-managed real-time messaging service that allows you to send and receive messages between independent applications.</li>
</ul>

<p>Next, I wanted to refresh my understanding of all the Apache projects that speak to this realm. I’m always trying to keep a handle on what they each actually offer, and how they overlap. So, seeing them side by side like this helps me think about how they fit into the big picture.</p>

<ul>
  <li><a href="https://kafka.apache.org/"><strong>Apache Kafka</strong></a> - Kafka™ is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.</li>
  <li><a href="https://flink.apache.org/"><strong>Apache Flink</strong></a> -  Apache Flink® is an open-source stream processing framework for distributed, high-performing, always-available, and accurate data streaming applications.</li>
  <li><a href="https://spark.apache.org/streaming/"><strong>Apache Spark</strong></a> - Spark Streaming makes it easy to build scalable fault-tolerant streaming applications. Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.</li>
  <li><a href="http://storm.apache.org/"><strong>Apache Storm</strong></a> Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing.</li>
  <li><a href="https://activemq.apache.org/apollo/"><strong>Apache Apollo</strong></a> - ActiveMQ Apollo is a faster, more reliable, easier to maintain messaging broker built from the foundations of the original ActiveMQ.</li>
</ul>

<p>One thing I think is worth noting with all of these is the absence of the web when you read through their APIs. Apollo had some significant RESTful approaches, and you find gateways and plugins for some of the others, but when you consider how these technologies fit into the wider API picture, I’d say they aren’t about embracing the web.</p>

<p>On that note, I think it is worth mentioning what is going on over at Google, with their gRPC effort, which provides “bi-directional streaming and fully integrated pluggable authentication with http/2 based transport”:</p>

<ul>
  <li><a href="https://grpc.io/"><strong>gRPC</strong></a> - A high performance, open-source universal RPC framework</li>
</ul>

<p>Also, I think most notably, they are continuing the tradition of APIs embracing the web, and built on top of HTTP/2. For me, this is always important, and trumps just being open source in my book. The more web an open source technology, and a company’s service utilize, the more comfortable I’m going to feel telling my readers they should be baking this into their operations.</p>

<p>After these services and tooling, I don’t want to forget about the good ol fashioned protocols available out there, that help use doing things in real time. <a href="http://realtime.apievangelist.com/#BuildingBlocks">I’m tracking on 12 real time protocols</a> that I see in use across the companies, organizations, institutions, and government agencies I’m tracking on:</p>

<ul>
  <li><a href="https://stomp.github.io/"><strong>Simple (or Streaming) Text Orientated Messaging Protocol (STOMP)</strong></a> - STOMP is the Simple (or Streaming) Text Orientated Messaging Protocol. STOMP provides an interoperable wire format so that STOMP clients can communicate with any STOMP message broker to provide easy and widespread messaging interoperability among many languages, platforms and brokers.</li>
  <li><a href="https://www.amqp.org/"><strong>Advanced Message Queuing Protocol (AMQP)</strong></a> - The Advanced Message Queuing Protocol (AMQP) is an open standard for passing business messages between applications or organizations. It connects systems, feeds business processes with the information they need and reliably transmits onward the instructions that achieve their goals.</li>
  <li><a href="http://mqtt.org/"><strong>MQTT</strong></a> - MQTT is a machine-to-machine (M2M)/Internet of Things connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport. It is useful for connections with remote locations where a small code footprint is required and/or network bandwidth is at a premium.</li>
  <li><a href="http://activemq.apache.org/apollo/documentation/openwire-manual.html"><strong>OpenWire</strong></a> - OpenWire is our cross language Wire Protocol to allow native access to ActiveMQ from a number of different languages and platforms. The Java OpenWire transport is the default transport in ActiveMQ 4.x or later.</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API"><strong>Websockets</strong></a> - WebSocket is a protocol providing full-duplex communication channels over a single TCP connection. The WebSocket protocol was standardized by the IETF as RFC 6455 in 2011, and the WebSocket API in Web IDL is being standardized by the W3C.</li>
  <li><a href="https://xmpp.org/about/"><strong>Extensible Messaging and Presence Protocol (XMPP)</strong></a> - XMPP is the Extensible Messaging and Presence Protocol, a set of open technologies for instant messaging, presence, multi-party chat, voice and video calls, collaboration, lightweight middleware, content syndication, and generalized routing of XML data.</li>
  <li><a href="https://github.com/sockjs"><strong>SockJS</strong></a> - SockJS is a browser JavaScript library that provides a WebSocket-like object. SockJS gives you a coherent, cross-browser, Javascript API which creates a low latency, full duplex, cross-domain communication channel between the browser and the web server.</li>
  <li><a href="https://github.com/pubsubhubbub/"><strong>PubSubHubbub</strong></a> - PubSubHubbub is an open protocol for distributed publish/subscribe communication on the Internet. Initially designed to extend the Atom (and RSS) protocols for data feeds, the protocol can be applied to any data type (e.g. HTML, text, pictures, audio, video) as long as it is accessible via HTTP. Its main purpose is to provide real-time notifications of changes, which improves upon the typical situation where a client periodically polls the feed server at some arbitrary interval. In this way, PubSubHubbub provides pushed HTTP notifications without requiring clients to spend resources on polling for changes.</li>
  <li><a href="https://www.ietf.org/rfc/rfc2326.txt"><strong>Real Time Streaming Protocol (RTSP)</strong></a> - The Real Time Streaming Protocol (RTSP) is a network control protocol designed for use in entertainment and communications systems to control streaming media servers. The protocol is used for establishing and controlling media sessions between end points. Clients of media servers issue VCR-style commands, such as play and pause, to facilitate real-time control of playback of media files from the server.</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events"><strong>Server-Sent Events</strong></a> - Server-sent events (SSE) is a technology where a browser receives automatic updates from a server via HTTP connection. The Server-Sent Events EventSource API is standardized as part of HTML5 by the W3C.</li>
  <li><a href="https://developer.apple.com/streaming/"><strong>HTTP Live Streaming (HLS)</strong></a> - HTTP Live Streaming (also known as HLS) is an HTTP-based media streaming communications protocol implemented by Apple Inc. as part of its QuickTime, Safari, OS X, and iOS software.</li>
  <li><a href="https://www.pubnub.com/blog/2014-12-01-http-long-polling/"><strong>HTTP Long Polling</strong></a> - HTTP long polling, where the client polls the server requesting new information. The server holds the request open until new data is available. Once available, the server responds and sends the new information. When the client receives the new information, it immediately sends another request, and the operation is repeated. This effectively emulates a server push feature.</li>
</ul>

<p>These protocols are used by the majority of the service providers and tooling I list above, but in my research I’m always trying to focus on not just the services and tooling, but the actual open standards that they support.</p>

<p>I have to also mention the entry level aspect of real time in my opinion. Something, that many API providers support, but also is the 101 level approach that some companies, organizations, institutions, and agencies need to be exposed to before they get overwhelmed with other approaches.</p>

<ul>
  <li><a href="http://webhooks.apievangelist.com"><strong>Webhooks</strong></a> - A webhook in web development is a method of augmenting or altering the behavior of a web page, or web application, with custom callbacks. These callbacks may be maintained, modified, and managed by third-party users and developers who may not necessarily be affiliated with the originating website or application.</li>
</ul>

<p>That is <a href="http://realtime.apievangelist.com/">the real time API landscape</a>. Sure, there are other services, and tooling, but this is the cream on top. I’m also struggling with the overlap with event sourcing, evented architecture, messaging, and other layers of the API space that are being used to move bits and bytes around today. Technologists aren’t always the best at using precise words, or keeping things simple, and easy to understand, let alone articulate. This is one of the concerns I have with streaming API approaches, is that they are often over the heads, and beyond the needs of some API providers, and may API consumers. They have their place within certain use cases, and large organizations that have the resources, but I spend a lot of time worrying about the little guy.</p>

<p>I think a good example of web API vs streaming API can be found in the Twitter API community. Many folks just need simple, intuitive, RESTful endpoints to get access to data, and content. While a much smaller slice of the pie will have the technology, skills, and compute capacity to do things at scale. Regardless, I see technologies like Apache Kafka being turned into plug and play, infrastructure as a service approaches, allowing anyone to quickly deploy to Heroku, and just put to work via a SaaS model. So, of course, I will still be paying attention, and trying to make sense out of all of this. I don’t know where any one it will be going, but I will keep tuning in, and telling stories about how real time, and streaming API technology is being used, or not being used.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/25/data-streaming-in-the-api-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/25/talking-with-more-federal-agencies-about-api-micro-consulting/">Talking With More Federal Agencies About API Micro Consulting</a></h3>
        <span class="post-date">25 Sep 2017</span>
        <p><a href="https://skylight.digital/"><img src="https://s3.amazonaws.com/kinlane-productions/skylight/services-infographic.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I have been having more conversations with federal agencies as part of my work with <a href="https://skylight.digital/">my Skylight partners</a> about API related microconsulting. One recent conversation, which I won’t mention the agency, because I haven’t gotten approval, involved bug bounties on top of an API they are rolling out. The agency isn’t looking for the regular technology procurement lifecycle around this project, they are just looking for a little bit of research and consulting to help ensure they are on the right track when it comes to hardening their API approach.</p>

<p>Micro consulting like this will usually not exceed $5,000.00 USD, and will always be a short term commitment. From my vantage point micro consulting will always be API related, and in this particular case involves studying how other API providers in the private sector are leveraging bug bounties to help harden their APIs either before they go public, or afterwards in an ongoing fashion. After I do the research I will be taking this work back to <a href="https://skylight.digital/">my team of consultants at Skylight</a>, and we’ll put together formal report and presentation that we will bring back to the federal government agency to put into motion.</p>

<p>This approach to doing APIs in the federal government (or any government) is a win-win. It fits with my approach to doing research at API Evangelist, and it provides API expertise for federal agencies in small, affordable, and bite-size chunks. Government agencies do not have to wait months, or years, and spend massive amounts of money to gain access to API expertise. For Skylight, it gets our foot in the door within government, and helps demonstrate the expertise we bring to the table. Something that will almost always turn into additional micro procurement relationships, as well as potentially larger scale, ongoing project relationships.</p>

<p>Personally, I like my API consulting just like my APIs, small, and doing one thing well. I don’t like consulting contracts that try to do too much. I also like getting paid in chunks that can usually be put on the corporate credit card, and avoid too much purchase order, vendor system, 30, 60, and 90 day wrangling–or worse, not getting paid at all. You’ll hear me beating the micro consulting, and micro procurement drum a lot more in coming months. I’m going to be working to educate more government agencies, and even the enterprise of the potential when it comes to API related content creation, storytelling, training, and research. I’m predicting it will have the same effect as APIs are having on how companies, organizations, institutions, and government agencies are doing business in the digital economy.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/25/talking-with-more-federal-agencies-about-api-micro-consulting/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/25/embeddable-api-integrations-for-non-developers-with-zapier/">Providing Embeddable API Integrations For Non-Developers With Zapier</a></h3>
        <span class="post-date">25 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/zapier/zapier-embeddable-zaps.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m regularly working to make APIs more accessible to non-developers, and <a href="https://zapier.com/app/explore">Zapier</a> is the #1 way I do this. Zapier provides ready-to-go API integration recipes for over 750 APIs, providing IFTTT-like functionality, but in a way that actual pays the whole API thing forward (Zapier has APIs, IFTTT does not). One of the benefits of having APIs is you can build embeddable tooling on top of them, and <a href="https://zapier.com/developer/documentation/v2/shared-zaps/#embedding-your-zaps">Zapier has some basic embeddable tools available to anyone</a>, with some more advanced options for <a href="https://zapier.com/developer/documentation/v2/zap-templates-partners-api/">partners via their partner API</a>.</p>

<p>Using the Zapier basic embeddable widget you can list one or many Zaps, providing recipes for any user to integrate with one or many APIs, that can be embedded into a web page, or within an application:</p>

<p>&#x3C;script type="text/javascript" src="https://zapier.com/zapbook/embed/widget.js?guided_zaps=2618,1035,977"&#x3E;&#x3C;/script&#x3E;</p>

<p>All you do is add the id for each of the Zaps you wish to list, under the “guided_zaps” variable, and it will list the icon, title, and “use this zap” functionality, all wrapped in the appropriate powered by Zapier branding. I’m developing lists of useful Zaps ranging from working with Google Sheets, to managing social media presence on Twitter and Facebook. Everyday, useful things that the average user might find valuable when it comes to automating, and taking control over their online presence. Anytime I reference possible API integration use cases in a story, I’m going to start embedding a widget of actual Zaps you can use to accomplish whatever I’m talking about below.</p>

<p>I’m also trying to carve out time to develop some of my own Zaps, and sign up to become a Zapier partner, so I can begin to develop some more advanced editions of embeddable tooling. I want to make my own JavaScript library that will spider any text in a story, and turn references to API integration into popup tooltips, with API literacy, training, and action links. I have a handful of API 101 style solutions I’d like to see exist, but at this point it is a matter of carving out the time to make happen. I’m working to invest more in my guidance for my power user, non-developer audience, continuing to put APIs within reach for everyone–not just the developer, IT, and startup community.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/25/embeddable-api-integrations-for-non-developers-with-zapier/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/22/not-everyone-needs-api-scale-some-just-need-api/">Not Everyone Needs API Scale, Some Just Need API</a></h3>
        <span class="post-date">22 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/old-yellow-house.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I know that catering to the enterprise is where the money is at. I know that playing with all the cool new containerized, event sourcing, continuously integrated and deployed solutions are where you can prove you know your stuff. However, in my world I come across so many companies, organizations, and government agencies that just need things to work. They don’t have the skills, resources, or time to play with everything cool, and really could just use some better access to their data and content across their business, with trusted partners, and maybe solicit the help of 3rd party developers to help carry the load.</p>

<p>Many of the conversation I am having within startup and tech circles often focus on scale, and the latest tech. I get that this is the way things work in alpha tech circles, and this is applicable in your worlds, always moving forward, pushing the scope of what we are doing, and making sure we are playing with the latest tools is how it’s done. However, not everyone has this luxury, and many companies can’t afford to hire the talent needed, or pay the cost associated with doing things the most modern approach, or even the right way. Remember, when you are talking about Kafka, Kuburnetes, Docker, GraphQL, and other leading edge solutions, you are talking from a place of privilege. Meaning you probably have the time, resources, and space to implement the most modern approach, and have the team to do it right.</p>

<p>I’m not trying to stop you from having fun, and doing what you do. I am just trying to share what I’m seeing on the ground at companies, organizations, and government agencies I’m talking to. I’m spending a lot of time trying to help get folks up to speed on everything I’m seeing, and many of them are intimidated by the pace at which things move, the scope of implementations they are reading about across tech blogs, and insecure about what they don’t know. I’m finding that I’m helping folks think through their basic usage of the cloud, over containerization, and helping them understand basic APIs and webhooks, over evented architecture and modern approaches to messaging. They just aren’t ready for much of what I’m reading and tracking on in my monitoring of the API space.</p>

<p>I guess I’m just asking for some help. If you are writing about tech, maybe reach out to small businesses and organizations outside the tech bubble and ask them what their challenges are, over writing about yet another startup, or cool new tech. Maybe take a portion of that investment you just got and establish some grants for nonprofits, students, or others to spend some time learning about new technology (not yours), and pay a contractor to help them solve a single, small problem–enjoying a little micro procurement assist, on your dime. Maybe rather than disrupting an industry, you could reach out to some smaller companies in the space and mentor them? IDK. I’m just spitballin here. I just feel like the gap is widening when it comes to technology on the ground in your average city, over what I see in the Bay Area, and other major tech hubs. If we don’t work to close this gap, it is going to bite us all in the ass at some point, if it already isn’t.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/22/not-everyone-needs-api-scale-some-just-need-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/22/i-am-a-professional-in-my-industry-where-should-i-begin-with-apis/">I Am A Professional In My Industry, Where Should I Begin With APIs?</a></h3>
        <span class="post-date">22 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/methuselah-mountain.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>A regularly question I get from business folks out in the space, is regarding where they should start with APIs. My world is usually broke into two areas: 1) Providing APIs, and 2) Consuming APIs. I’d say that these business folk I keep coming across could easily span both of these areas, making it significantly more complicated to help them understand where they should be getting started with APIs. With the API landscape being so wide, and APIs becoming so ubiquitous across many industries, helping someone onboard to the concept can get pretty complex and confusing pretty quick.</p>

<p>I always try to prime the pump with <a href="http://101.apievangelist.com/">my API 101 material</a>, and encourage folks to learn about <a href="http://history.apievangelist.com/">the history of APIs</a>. I find that before you begin getting bombarded with the technical details of APIs it helps to get the lay of the land, and understand what is going on at the highest level, developing a better understanding of how we got here. Before you get working with any single API, you should try to understand why it has begun to be such a big part of everything we know of online today, and via our mobile phones. Most people don’t realize that they are using APIs everyday, as part of their regular business activity, and common things they do in their personal lives–things like buy products from Amazon, and sharing updates with friends on Facebook.</p>

<p>Next, I recommend looking at the software you use each day in your work. If you are an architect, look a the CAD software you are using. If you are in healthcare, look at the administrative systems you use, and the devices you put to work. If you are retail, look at the point of sale (POS), and payment systems you use. All of these companies have APIs in one form or another. They might not all be public APIs like Facebook, Twitter, and Google, but they have APIs that might make a lot more sense to you in your world. You should be learning about the companies behind this software, searching their websites, knowledge-bases, and other systems for APIs, and understanding what they do, and how you can possibly put them to use in your world, and help you do what you do better.</p>

<p>Take a look at your smart phone. All those icons for the applications you use have APIs behind them. Mobile phones are why APIs have become such a big thing. It is how information is sent and received by mobile applications. If you want to learn about APIs, you should start by learning in the context of the applications, services, and tooling you are already depending on and putting to use. A great example of this for business users can be found with the spreadsheet. Both Google and Microsoft have APIs, allowing you to get data into, and out of a spreadsheet. I often feel like the spreadsheet API is one of the most important, and underrated APIs out there, and spreadsheets are one of the most important API clients out there as well, but often gets overlooked by developers who don’t love the spreadsheet as much as business users.</p>

<p>As a professional in any industry, I recommend starting here. The chances you will discover and learn about APIs in a more meaningful and impactful way is greater, than if you just take some learn to code, or other technical approach. I’m going to begin building this into my API lessons and training work I’ve kicked off recently, and see if I can develop more material that is less API provider vs API consumer, and more about helping folks in specific industries, or who might be using specific software, platforms, services, and tools. Regardless, if you are looking to jumpstart your own API learning I recommend just looking around you, I think you’d be surprised how many APIs there are behind the solutions you are already putting to use.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/22/i-am-a-professional-in-my-industry-where-should-i-begin-with-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/21/that-point-where-api-session-management-becomes-api-surveillance/">That Point Where API Session Management Become API Surveillance</a></h3>
        <span class="post-date">21 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/border-traffic.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="https://www.youtube.com/watch?v=i7YUymQkZ_8&amp;feature=youtu.be">I was talking to my friends TC2027 Computer and Information Security class at Tec de Monterrey via a Google hangout today</a>, and one of the questions I got was around managing API sessions using JWT, which was spawned from a <a href="https://blog.websecurify.com/2017/02/hacking-json-web-tokens.html">story about security JWT</a>. A student was curious about managing session across API consumption, while addressing securing concerns, making sure tokens aren’t abused, and there isn’t API consumption from 3rd parties who shouldn’t have access going unnoticed.</p>

<p>I feel like there are two important, and often competing interests occurring here. We want to secure our API resources, making sure data isn’t leaked, and prevent breaches. We want to make sure we know who is accessing resources, and develop a heightened awareness regarding who is accessing what, and how they are putting them to use. However, the more we march down the road of managing session, logging, analyzing, tracking, and securing our APIs, we are also simultaneously ramping up the surveillance of our platforms, and the web, mobile, network, and device clients who are putting our resources to use. Sure, we want to secure things, but we also want to think about the opportunity for abuse, as we are working to manage abuse on our platforms.</p>

<p>To answer the question around how to track sessions across API operations I recommended thinking about that identification layer, which includes JWT and OAuth, depending on the situation. After that you should be looking other dimensions for identifying session like IP address, timestamps, user agent, and any other identifying characteristics. An app or user token is much more about identification, than it ever provides actual security, and to truly identify a valid session you should have more than one dimension beyond that key to acknowledge valid sessions, as well as just session in general. Identifying what healthy sessions look like, as well as unhealthy, or unique sessions that might be out of the realm of normal operations.</p>

<p>To accomplish all of this, I recommend implementing a modern API management solution, but also pulling in logging from all other layers including DNS, web server, database, and any other system in the stack. To be able to truly identify healthy and unhealthy sessions you need visibility, and synchronicity across all logging layers of the API stack. Does the API management logs reflect DNS, and web server, etc. This is where access tiers, rate limits, and overall consumption awareness  really comes in, and having the right tools to lock things down, freeze keys and tokens, as well as being able to identify what healthy API consumption looks like, providing a blueprint for what API sessions should, or shouldn’t be occurring.</p>

<p>At this point in the conversation I also like to point out that we should be stopping and considering at what point all of this API authentication, security, logging, analysis, and reporting and session management becomes surveillance. Are we seeking API security because it is what we need, or just because it is what we do. I know we are defensive about our resources, and we should be going the distance to keep data private and secure, but at some point by collecting more data, and establishing more logging streams, we actually begin to work against ourselves. I’m not saying it isn’t worth it in some cases, I am just saying that we should be questioning our own motivations, and the potential for introducing more abuse, as we police, surveil, and secure our APIs from abuse.</p>

<p>As technologists, we aren’t always the best at stepping back from our work, and making sure we aren’t introducing new problems alongside our solutions. This is why I have my <a href="http://surveillance.apievangelist.com/">API surveillance research</a>, alongside my API <a href="http://authentication.apievangelist.com/">authentication</a>, <a href="http://security.apievangelist.com/">security</a>, <a href="http://logging.apievangelist.com/">logging</a>, and other management research. We tend to get excited about, and hyper focused on the tech for tech’s sake. The irony of this situation is that we can also introduce exploitation and abuse around our practices for addressing exploitation and abuse around our APIs. Let’s definitely keep having conversations around how we authenticate, secure, and log to make sure things are locked down, but let’s also make sure we are having sensible discussions around how we are surveilling our API consumers, and end users along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/21/that-point-where-api-session-management-becomes-api-surveillance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/21/the-concept-of-api-management-has-expanded-so-much-the-concept-should-be-retired/">The Concept Of API Management Has Expanded So Much the Concept Should Be Retired</a></h3>
        <span class="post-date">21 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/old-barn.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>API management was the first area of my research I started tracking on in 2010, and has been the seed for <a href="http://apievangelist.com/api-lifecycle/">the 85+ areas of the API lifecycle I’m tracking on in 2017</a>. It was a necessary vehicle for the API sector to move more mainstream, but in 2017 I’m feeling the concept is just too large, and the business of APIs has evolved enough that we should be focusing in on each aspect of API management on its own, and retire the concept entirely. I feel like at this point it will continue to confuse, and be abused, and that we can get more precise in what we are trying to accomplish, and better serve our customers along the way.</p>

<p>The main concepts of API management at play have historically been about authentication, service composition, logging, analytics, and billing. There are plenty of other elements that have often been lumped in there like portal, documentation, support, and other aspects, but securing, tracking, and generating revenue from a variety of APIs, and consumers has been center stage. I’d say that some of the positive aspects of the maturing and evolution of API manage include more of a focus on authentication, as well as the awareness introduced by logging and analytics. I’d say some areas that worry me is that security discussions often stop with API management, and we don’t seem to be having evolved conversations around service conversation, billing, and monetization of our API resources. You rarely see these things discussed when we talk about GraphQL, gRPC, evented architecture, data streaming, and other hot topics in the API sector.</p>

<p>I feel like the technology of APIs conversations have outpaced the business of APIs conversations as API management matured and moved forward. Advancements in logging, analytics, and reporting have definitely advanced, but understanding the value generated by providing different services to different consumers, seeing the cost associated with operations, and the value generated, then charging or even paying consumers involved in that value generation in real-time, seems to be being lost. We are getting better and the tech of making our digital bits more accessible, and moving them around, but we seemed to be losing the thread about quantifying the value, and associating revenue with it in real-time. I see this aspect of API management still occurring, I’m just not seeing the conversations around it move forward as fast as the other areas of API management.</p>

<p>API <a href="http://monetization.apievangelist.com/">monetization</a> and <a href="http://plans.apievangelist.com/">plans</a> are two separate area of my research, and are something I’ll keep talking about. Alongside <a href="http://authentication.apievangelist.com/">authentication</a>, <a href="http://logging.apievangelist.com/">logging</a>, <a href="http://analysis.apievangelist.com/">analysis</a>, and <a href="http://security.apievangelist.com/">security</a>. I think the reason we don’t hear more stories about API service composition and monetization is that a) companies see this as their secret sauce, and b) there aren’t service providers delivering in these areas exclusively, adding to the conversation. How to rate limit, craft API plans, set pricing at the service and tier levels are some of the most common questions I get. Partly because there isn’t enough conversation and resources to help people navigate, but also because there is insecurity, and skewed views of intellectual property and secret sauce. People in the API sector suck at sharing anything they view is their secret sauce, and with no service providers dedicated to API monetization, nobody is pumping the story machine (beyond me).</p>

<p>I’m feeling like I might be winding down <a href="http://management.apievangelist.com/">my focus on API management</a>, and focus in on the specific aspects of API management. I’ve been working on my API management guide over the summer, but I’m thinking I’ll abandon it. I might just focus on the specific aspects of conducting API management. IDK. Maybe I’ll still provide a 100K view for people, while introducing separate, much deeper looks at the elements that make up API management. I still have to worry about onboarding the folks who haven’t been around in the sector for the last ten years, and help them learn everything we all have learned along the way. I’m just feeling like the concept is a little dated, and is something that can start working against us in some of the conversations we are having about our API operations, where some important elements like security, and monetization can fall through the cracks.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/21/the-concept-of-api-management-has-expanded-so-much-the-concept-should-be-retired/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/21/i-am-not-a-card-carrying-restafarian-i-just-believe-in-the-web/">I Am Not A Card Carrying Restafarian I Just Believe In The Web</a></h3>
        <span class="post-date">21 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/statue-face-open-mouth_blue_circuit.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am always surprised at the folks who I meet for the first time who automatically assume I’m all about the REST. It is always something that is more telling about the way they see the world (or don’t), than it ever is about me as THE API Evangelist. It is easy to think I’m going to get all RESTY, and start quoting Roy, but I’m no card carrying RESTafarian, like my buddy Darrel Miller (@darrel_miller) (not that is what Darrel does ;-). Really the only thing I get passionate about is making sure we are reusing the web, and I am pretty much be a sellout on almost everything else.</p>

<p>I am just looking to understand how folks are exposing interfaces for their digital resources using the web, making them available for use in other applications. I feel like RESTful approaches are always a good start for folks to begin considering, and learning from when beginning their journey, but I’m rarely going to get all dogmatic about REST. There are trade-offs with any approach you take to providing programmatic interfaces using the web, and you should understand what these are whether your are using REST, Hypermedia, (g)RPC, GraphQL, or any other number of protocols and technologies available out there. A RESTful approach using the web just tends to be the lowest common denominator, the cheapest, and widest reaching solution we have on the table. Rarely is it ever the perfect solution–there are no such things. #sorry</p>

<p>If you are entering into discussions with me thinking I’m 100% team REST, you are mistaken, and you have profiled yourself considerably for me. It shows me that you haven’t done a lot of (wide) reading on the subject of APIs, and while you may be an expert, you probably are a very siloed expert who doesn’t entertain a lot of outside opinions, and keep an eye on how the space is shifting and changing. When I encounter folks like you in the space you’ll often find me pretty quiet, submissive, and just nodding my head a lot. As you aren’t my target audience, and there isn’t much I can say that will shift your world view. Your opinions are pretty set, and I’m not going to be the one who moves them forward. My role is to reach folks are looking for answers, not those who already have them.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/21/i-am-not-a-card-carrying-restafarian-i-just-believe-in-the-web/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/21/getting-beyond-openapi-being-about-api-documentation/">Getting Beyond OpenAPI Being About API Documentation</a></h3>
        <span class="post-date">21 Sep 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/yellowtree/clean_view/file-00_00_50_85.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://www.bizcoder.com/openapi-is-not-what-i-thought">Darrel Miller has a thought provoking post on OpenAPI not being what he thought</a>, shining a light on a very important dimension of what OpenAPI does, and doesn’t do in the API space. In my experience, OpenAPI is rarely what people think, and I want to revisit once slice of Darrel’s story, in regards to folks generally thinking OpenAPI (Swagger) as being all about API documentation. In 2017, the majority of folks I talk to think OpenAPI is about documenting your APIs–something that always makes me sad, but I get it, and is something I regularly work to combat this notion.</p>

<p>First, and foremost, OpenAPI is a bridge to understanding and being able to communicate around using HTTP as a transport, and our greatest hope for helping developers learn their HTTPs and 123s. I meet developers on a regular basis who are building web APIs, yet do not have a firm grasp on what HTTP is. Hell, I’ve had a career dedicated to web APIs for the last seven years, and I’m still developing my grasp on what it is, learning new things from folks like Erik Wilde (<a href="https://twitter.com/dret">@dret</a>), Darrel Miller (<a href="https://twitter.com/darrel_miller">@darrel_miller</a>), and Mike Amundsen (<a href="https://twitter.com/mamund">@mamund</a>) on a regular basis. In the API game, you should always be learning, and the web is the center of your existence at the moment as a software engineer, and should be the focus of what you are learning about to push forward your knowledge.</p>

<p>Darrel has a great line in his post where he has “<em>a higher chance of convincing developers to stop drinking Mountain Dew than to pry a documentation generator from the hands of a dev with a deadline.”</em> Meaning, most developers don’t have the time or interest to learn about what OpenAPIs, or can do for them in their busy world, they just want the help delivering documentation–a very visual representation of the work they’ve done, and is something they can demonstrate to their boss, partners, and customers. Most developers aren’t spending the time trying to know and understand everything API, thinking deeply on the subject like Darrel and I are doing. Most don’t even have time to read our blog posts. A sad fact of doing business in the tech space, but is something us in charge of API standards and tooling, or even selling API services should be aware of.</p>

<p>You see an essence of this with API code generators, and API testing from OpenAPI. Although in much lesser quantities than API documentation enjoys. Developers just want the assist, they really don’t care whether it is the right way of doing things, or the wrong way, and how it fits into the bigger picture. API developers just want to get their work done, and move on. It is up to us analysts, standards shepherds, and API service providers to help educate, illuminate, and incentivize developers to get over their limiting views on what OpenAPI is and/or develop the next killer tooling that helps make their lives insanely easier like Swagger UI did for API documentation. We need to learn from the impact this tooling has made, and make sure the other lifecycle solutions we are delivering speak in similar tones.</p>

<p>If you are reading this piece, and are still in the camp of folks who still see OpenAPI as Swagger UI, don’t feel bad, it is a common misconception, and one that was exacerbated by the move from Swagger to OpenAPI. My recommendation is that you begin to look at OpenAPI independent of any tooling it enables. Think of it as a checklist for your HTTP learning, sharing, and communication across your API development team. It shouldn’t be just about delivering documentation, code, tests, or anything else. OpenAPI is about making sure you have the HTTP details of your API delivered in a consistent way, across not just a single APIs, but all the APIs you are delivering. OpenAPI is the bridge to where you are now with your API operations, to where you should be when it comes to the definition, design, deployment, management, and delivering sustainable contracts around the digital assets you are serving up internally, with partners, and 3rd party developers. It may see like extra work to think about it this way, but it is something that will save you time and money down the road.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/21/getting-beyond-openapi-being-about-api-documentation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/20/jekyll-as-a-hypermedia-client/">Using Jekyll As A Hypermedia Client</a></h3>
        <span class="post-date">20 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/subway/hypermedia-siren-subway-stop.png" align="right" width="35%" style="padding: 15px;" /></p>
<p>I am picking up some of my past work, so that I can move forward in a new way. A while ago, I began working on <a href="http://subway.map.apievangelist.com/">my subway map API</a> to help me articulate aspects of the API lifecycle, and provide a “vehicle” for helping folks explore some often complex API concepts, in a way that would incrementally introduce them to new ideas. I used the subway map as an analogy because it has been historically used to help folks understand complex systems, and help them navigate it, even if they don’t fully understand everything about it. <a href="https://apievangelist.com/2015/11/29/the-api-lifecycle-my-talk-from-defrag-and-apistrat/">I gave a talk at @APIStrat in Austin, TX on this subject</a>, but something I haven’t moved forward in over a year.</p>

<p>My new approach to using the subway map model is still using hypermedia (Siren), but I’m not wanting a single API to control the data for every client. I’m looking to develop a static, federated approach to delivering subway map experience. I want to be able to quickly publish a common map, but then be able evolve them independently, designed for specific implementations and use cases. Since I’m so Jekyll and Github centered in how I deliver projects, I’m looking for a way to do this in a static way, that can be forked. So, I got to work on publishing Siren YAML to Github, and seeing if it is possible to use Liquid and HTML as the client. Again, I want this to be static. All this could easily be building this in JavaScript, but I want things static and forkable.</p>

<p><a href="http://jekyll.hypermedia.client.apievangelist.com/design/requests/">For my proof of concept I published 15 “stops” along the request “line” for my API design “area”</a>. I don’t have the visual elements present for this functionality, as I just wanted to prove that I could use Liquid and HTML for a hypermedia client, using Siren YAML published to Github. I was forced to add a layout: property to my Siren schema, which is probably heresy to couple to the client in this way, but it is something I’m willing to take a hit for. Everything else is pure Siren. While there is still a lot more work to be done, I was able to expand the boundaries of how I use hypermedia and Jekyll, in a single proof of concept–telling me the idea is worth moving forward with.</p>

<p>To make things work I published a set of Siren hypermedia YAML documents (I know Kevin, I’m making you cringe, but bear with me) to <a href="https://github.com/api-evangelist-tools/jekyll-hypermedia-client/tree/master/_design">a Jekyll collection called _design</a>. Then I have <a href="https://github.com/api-evangelist-tools/jekyll-hypermedia-client/tree/master/_layouts">three Jekyll client templates in the _layouts folder, called area, lines, and stops</a>. My client isn’t that sophisticated for this proof of concept, but I am able to easily work with the entities, properties, and links in Liquid effectively. I’m just wanting to show that I can take my YAML to the next level, and expand my link relations beyond just next and previous that is often associated with Jekyll _posts, opening up <a href="https://www.iana.org/assignments/link-relations/link-relations.xhtml">the entire IANA link relations catalog of options</a>, plus anything custom I will need (ie. area, line, stop). It doesn’t look like much, but it provides a pretty compelling example of using Jekyll and Github to deliver complex content that will be changing regularly in a static way.</p>

<p><a href="http://jekyll.hypermedia.client.apievangelist.com/design/requests/">The viewable side of my hypermedia Jekyll subway map API client doesn’t look that attractive yet</a>, but it provides the basic next/previous functionality, as well links back to the area, or line of coverage. This is just the first two types of experience I’m looking to provide as people explore. I will be introducing transfers, help, and other supporting link relations between the content being made available. Eventually the home page of these projects will be a subway map with accompanying key, and then you choose the area, explore lines, and get off on any stop you desire. It’s just a start, but I feel my Jekyll hypermedia client proof of concept is a success, and I’ll get to work on publishing more content, and adding the visual elements that make it truly a subway map experience.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/20/jekyll-as-a-hypermedia-client/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/20/looking-to-2024-what-do-apis-look-like/">Looking To 2024, What Do APIs Look Like?</a></h3>
        <span class="post-date">20 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/redes-fast-flux-623x425_blue_electricity.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>I’m not a big fan of predictions. It is a game that analysts and investors play to try and shape the world they want to see. I’m usually focused on shaping the world I want to see by understanding where we are, how we got here, and making incremental shifts in our behavior today. I tend to think that technology futurists are more about ignoring the past, and being in denial about today, than they are ever about what is happening in the future. However, with that said, let me share some thoughts about what I think the future will hold when it comes to doing APIs.</p>

<p>Mostly, by 2024 things will look much like they do today. Nothing moves as fast we think they will in the tech sector. I’ve done a lot of studying the history of predictions by technology blogs, and analysts firms over the last decade about what they thought 2017 would be like, and it is 98% bullshit. It was more about getting what they wanted in the year they made the prediction, than it was ever about the future. Technology always feels like it is moving faster than ever before but honestly, what has happened in the last decade that is really seismic? I’d say iPhone is the biggest, with mostly everything else being pretty incremental, and slow moving.</p>

<p>By 2024, we will still be struggling with technical debt. However, the debt limit ceiling will have been raised significantly. We won’t have decoupled the monolith, all while adding to it. The web will still be preferred approach to delivering APIs, although there will be numerous add-ons, and proprietary bastardizations surgically attached to it. REST APIs will be relic of the past, but web APIs will still be the preferred approach because it is simple, however there will be multiple speed API approaches reflecting what we are seeing from gRPC. Think about how different the web APIs are today, from the web APIs in 2010–there really isn’t that many changes. They still suck as bad as they did back then, with a few well designed ones available–just like there was in 2010.</p>

<p>There will be significant movements in the world of API definitions and schema. OpenAPI will have opened up a flood of competing specifications, as well as industry specific implementations using those formats. With the mainstream adoption of web APis, the need for common schema, dictionaries, and design patterns will increase. A significant portion of these API definitions will follow the web, reusing existing patterns, and evolving them in meaningful ways. The rest will be proprietary, complex, and not actually do many API providers much good, yet they will be used by popular APIs, so they will end up being baked into systems and applications. There will never be a single definition to rule them all, although we will see some strong media types emerge that encourage doing APIs in a powerful way at scale.</p>

<p>Much of the API sector will be deployed on the backs of cloud giants. The web landscape will be so volatile and un-secure, small business will not be able to operate without the assistance of platforms with the security expertise to defend our APIs. Cyberwarfare will be normalized, and corporations and state actors will routinely disrupt, infiltrate, and make doing business online near impossible. If you want to stay in business as an API provider you will have to pay a security tax to a cloud enforcer to ensure your services stay up. This will continue to give a unhealthy amount of power to large companies to dictate which types of APIs should be available, which new ideas get created, and shaping the overall notion of what is APIs–kind of the gilded age, after the wild west.</p>

<p>In 2024, algorithmic APIs will outpace strictly data or content APIs, with artificial intelligence, and machine learning dominating the landscape. This shift will mostly cause noise, disturbances, and fuel cyber(in)security, but there will be incremental evolution in these disciplines that actually deliver real business value. There will be enough value created from AI, ML, and algorithmic APIs to keep investment going, but most of it will be hype, over promised, and just confusing to users, continuing to give APIs a bad time. Luckily APIs will also be used to help make these black box algorithms more observable, accountable, and regulated, although may will still remain out of sight due to intellectual property, complexity, and other forms of technological theater.</p>

<p>Another aspect the API space that will continue to move front and center is event-driven architecture. As API move into the mainstream, much of the design thinking that goes into them will be heavily centered around business activities, and the events that matter to humans (and companies). We will see event sourcing, webhooks, and real-time technologies continue to evolve and grow, and become baked into our API approaches, much like REST design patterns are today. While it will continue to grow and evolve, the event landscape will be littered with technologies that confuse, complex, and go against much of what we’ve built over the years with simple API design approaches, but this won’t stop folks from thinking it is a good idea.</p>

<p>APIs in 2024 won’t be the shiny, API saved world we envision. It will still be pretty messy, and many will question whether we should be doing APIs in the first place, but doing business on the web will require moving data, and content around at scale, leveraging open and commercial algorithms to get things done. So APIs will persist, but not necessary thrive. They will be ubiquitous, and rarely exciting. Kind of like a gas station or convenience store. You will use them daily, they will be everywhere, but rarely will they bring inspiration as they have in the early days. One side effect of this will be that all government will be doing APis, but sadly it won’t bring the agility, and nimbleness we’ve all promised. It will usually just make things harder, and open public resources up for exploitation by commercial vendors who are offering the true solution for a fee in the parking lot.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/20/looking-to-2024-what-do-apis-look-like/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/20/what-were-the-main-api-developments-in-2017/">What Were The Main API Developments In 2017</a></h3>
        <span class="post-date">20 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/amusement-park-2_light_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>The main API development in 2017 has been the continued shift towards mainstream API adoption. The concept has been moving outside of the tech sector for a couple years now, but in 2017 it is very clear that it’s not just something startups are doing. This is having a profound shift in how we talk about APIs, and how we approach the API lifecycle. APIs have historically been something new and smaller companies are doing, but often will deliver at scale (AWS, SalesForce, eBay, etc.). The mainstream shift in the API sector brings a whole new set of challenges, and opportunities as existing companies, with existing technology in place, work to shift towards an API way of doing things.</p>

<p>This shift impacts the technology of doing APIs, but really isn’t the main event–things will be business as usual when it comes to the technology of APIs for many years to come. I’d say the main event has been in the business of doing APIs. How these APIs get funded will be entirely different from how startup focused APIs get funded. This shift in financial incentives behind why APIs are developed, operated, and ultimately deprecated, will have profound effects on what is an API. They will have less of the startup shine, and become more robust, providing commercial, and industrial grade digital resources that are more mature than the newer, younger APIs we’ve seen in recent years.</p>

<p>Alongside this shift, another development in the business of APIs has occurred. The funding landscape for startups has shifted substantially. In the last couple years the majority of startup funding has dried up, making it a much more competitive environment for the API providers and service providers startups to get the money they need to grow and evolve. This vacuum has allowed for a new, more volatile, and API driven way of funding to emerge, based upon the blockchain, in the form of Initial Coin Offering, or simply ICO. This approach to raising money is quickly becoming a preferred, albeit a more volatile approach to funding startups. It is something that will work against the reliability and stability of depending on APIs, even more so than we saw with startup and investment culture over the last 7 years.</p>

<p>These two shifts in the business of APIs will be at odds with each other. There will still be startups doing APIs, but they will often be more volatile, ephemeral, and unreliable, but they will still continue to define what is next. While the mainstream adoption of APIs will become a more mature, stable, sustainable approach to doing business with APIs. Providing the services, tooling, and resources that are needed to make the economy work in the Internet age. This shift in the business of APIs landscape will have positive and negative effects on the API sector, but ultimately working together to move things forward in a more sensible way than we have in the wild west of the API sector. We will als begin to see more standards emerge, and more industry leaders who dominate their sector because they’ve found a sustainable way of doing APIs, that transcends much of the hype we’ve seen, and will continue to see in the startup community.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/20/what-were-the-main-api-developments-in-2017/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/20/what-are-the-unsolvd-problems-in-the-api-space/">What Are The Unsolved Problems In The API Space?</a></h3>
        <span class="post-date">20 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/sand-hand_dali_three.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>There are always endless numbers of fabricated unsolved problems in the API space. These are unsolved problems that are usually unsolved because they were just made up to get someone to buy a new service or product. They aren’t real problems. Technology is good at being applied to make believe problems, vendor fabricated problems, and solving real problems created by the last couple of waves of technology. I’d say that biggest unsolved problem is how APIs can actually solve the legacy technical debt challenges large companies, institutions, and government agencies face. There is a lot of rhetoric regarding how APis can unwind all of this, but honestly there are few examples of it in practice. With many API efforts in 2017, bogged down in cultural friction, and a web (pun intended) of technical complexity.</p>

<p>One aspect of this problem of legacy technical debt is the problem of delivering technology in an Internet age, without actually embracing the web. People doing APIs don’t always have the knowledge of the web, and what makes it work before they get to work doing APIs. Vendors are offering up tools and services that provide web API solutions, but don’t always embrace the web, and the existing standards and protocols that make the web work. APIs are the next evolution of the web, and rely on many of the same concepts for it to work. When you ignore the web when doing APIs, you will always face challenges in interoperability, and reuse, and often end up building siloed solutions that do not achieve deliver the solutions that doing APIs have promised.</p>

<p>I regularly see API providers, and service providers delivering their solutions to problems using APIs, with no awareness of the web they are relying on, and the building blocks that make it work. We often see technological solutions that can potentially deliver value, but are done so in a proprietary way, with all roads leading towards a commercial solution, extracting any value from open source implementations, and the web, while not giving anything back. All of this behavior is just leading to the next generation of technical debt, that is being sold as the solution for the last couple of generations of technical debt. When in reality, everyone wants to just operate efficiently, securely, and effectively using the web, but rarely do they ever fully invest in learning what that means, and working with vendors who support this vision.</p>

<p>It is easy to say the biggest unsolved problems are the new ones, but in my opinion, this is lazy. New technology will always become yesterday’s old technology. in the race to adopt each new wave of technological solution it is easy to pass the biggest existing problems off to other folks, and pay attention to the new and shiney technology. This is easy. Yes it just contributes to the problem, and rarely actually provides comprehensive, equitable solutions that will make a difference unwinding the technical debt we’ve already built up. So I tend not to get distracted with the endless waves of made up problems you read about daily. I think the biggest unsolved problem in the space is technical debt, and how we unwind it as we move forward. This isn’t just about adopting new acronyms, phrases, terms, and technology. It is often about deeply considering the why and how we are doing this, and having open conversations about what technology we should be doing, and shouldn’t be doing. Then doing it in small enough chunks that it can be eliminated as soon as it shifts from asset to a liability, and either eliminated or replaced with something that works.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/20/what-are-the-unsolvd-problems-in-the-api-space/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/19/what-apis-excite-you-and-fuels-my-research-and-writing/">What APIs Excite Me And Fuels My Research And Writing</a></h3>
        <span class="post-date">19 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/old-gas-pumps.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>The number API that gets me out of bed each day, with an opportunity to apply what I’ve learned in the API sector is with the Human Services Data API (HSDA). Which is an open API standard I am the technical lead for which helps municipalities, and human service organizations better share information that helps people find services in their communities. This begins with the basics like food, housing, and healthcare, but in recent months I’m seeing the standard get applied in disaster scenarios like Hurricane Irma to help organize shelter information. This is why I do APIs. The project is always struggling for funding, and is something I do mostly for free, with small paychecks whenever we receive grants, or find projects where we can deliver an actual API on the ground.</p>

<p>Next, I’d say it is government APIs at the municipal, state, but mostly at the federal levels. I was a Presidential Innovation Fellow in the Obama administration, helping federal agencies publish their open data assets, take inventory of their web services. I don’t work for the government anymore, but it doesn’t mean the work hasn’t stopped. I’m regularly working on projects to ensure RFPs, and RFIs have appropriate API language in them, and talking with agencies about their API strategy, helping educate them what is going on in the private sector, and often times even across other government agencies. APIs like the new FOIA API, Recreational Information Database API, Regulations.gov, IRS APis, and others will have the biggest impact on our economy and our lives in my opinion, so I make sure to invest a considerable amount of time here whenever I can.</p>

<p>After that, working with API education and awareness at higher educational institutions is one my passions and interest. My partner in crime Audrey Watters has a site called Hack Education, where she covers how technology is applied in education, so I find my work often overlapping with her efforts. A portion of these conversations involve APIs at the institutional level, and working with campus IT, but mostly it about how the Facebook, Twitter, WordPress, Dropbox, Google, and other public APIs can be used in the classroom. My partner and I are dedicated to understanding the privacy implications of technology, and how APIs can be leveraged to give students and faculty more control over their data and content. We work regularly to tell stories, give talks, and conduct workshops that help folks understand what is possible at the intersection of APIs and education.</p>

<p>After that, I’d say the main stream API sector keeps me interested. I’m not that interested in the whole startup game, but I do find a significant amount of inspiration from studying the API pioneers like SalesForce and Amazon, and social platforms like Twitter and Facebook. As well as the cool kids like Twilio, Stripe, and Slack. I enjoy learning from these API leaders, studying their approaches, but where I find the most value is sharing these stories with folks in SMB, SME, and the enterprise. These are the real-world stories I thrive on, and enjoy retelling as part of my work on API Evangelist. I’m a technologist, so the technology of doing APIs can be compelling, and the business of doing this has some interesting aspects, but it’s mostly the politics of doing APIs that intrigues me. This primarily involves the politics of the humans involved within a company, or industry, providing what I always find to be the biggest challenges of doing APIs.</p>

<p>In all of these areas, what actually gets me up each day, is being able to tell stories. I’ve written about 3,000 blog posts on API Evangelist in seven years. I work to publish 3-5 posts each weekday, with some gaps in there due to life getting in the way. I enjoy writing about what I’m learning each day, showcasing the healthy practices I find in my research, and calling out the unhealthy practices I regularly come across. This is one of the reasons I find it so hard to take a regular job in the space, as most companies are looking to impose restrictions, or editorial control over my storytelling. This is something that would lead to me not really wanting to get up each day, and is the number one reason I don’t work in government, resulting in me pushing to make change from the outside-in. Storytelling is the most important tool in my toolbox, and it should be in every API providers as well.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/19/what-apis-excite-you-and-fuels-my-research-and-writing/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/19/who-are-the-most-influential-people-and-companies-to-keep-an-eye-on-in-api-space/">Who Are The Most Influential People And Companies To Keep An Eye On In API Space</a></h3>
        <span class="post-date">19 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/art-museum_dark_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>When it comes to the most influential people and companies in the API space that I am keeping an eye on, it always starts with the API pioneers. This begins with SalesForce, eBay, and Amazon. Then it moves into the social realm with Twitter and Facebook. All of these providers are still moving and shaking the space when it comes to APIs, and operating viable API platforms that dominate in their sector. While I do not always agree with the direction these platforms are taking, they continue to provide a wealth of healthy, and bad practices we should all be considering as part of our own API operations, even if we aren’t doing it at a similar scale.</p>

<p>Secondarily, I always recommend studying the cloud giants. Amazon is definitely the leader in this space, with their pioneering, first mover status, but Google is in a close second, and enjoys some API pioneering credentials with Google Maps, and other services in their stack. Even though Microsoft waiting so long to jump into the game I wouldn’t discount them from being an API mover and shaker with their Azure platform making all the right moves in the last couple of years as they played catch up. These three API providers are dictating much of what we know as being APIs in 2017, and will continue to do so in coming years. They will be leading the conversation, as well as sucking the oxygen out of other conversations they do not think are worthy. If you aren’t paying attention to the cloud APIs, you won’t remain competitive, no matter how well you do APIs.</p>

<p>Next, I always recommend you study the cool kids of APIs. Learning about how Twilio, Stripe, SendGrid, Keen, and the other API-first movers and shakers are doing what they do. These platforms are the gold standard when it comes to how you handle the technical, business, and politics of API operations. You can spend weeks in their platforms learning from how they craft their APIs, and operate their communities. These companies are all offering viable resources using web APIs, that developers need. They are offering these resources up in a way that is useful, inviting, and supportive of their consumers. They are actively investing in their API community, keeping in sync with what they are needing to be successful. It doesn’t matter which industry you are operating in, you should be paying attention to these companies, and learning from them on a regular basis.</p>


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/19/who-are-the-most-influential-people-and-companies-to-keep-an-eye-on-in-api-space/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/19/what-is-the-biggest-challenge-for-big-companies-doing-apis/">What Is The Biggest Challenge For Big Companies Doing APIs?</a></h3>
        <span class="post-date">19 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/castle-wall-circuits.JPG" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>The biggest challenge for big companies doing APIs is always about people and culture. Change is hard. Decoupling things at large companies is difficult. While APIs can operate at scale, they excel when they do one thing well, and aren’t burdened with the scope, and complexity of much of the software systems we see already operating within large companies. These large systems take large teams of people to operate, and shifting this culture, and displacing these people isn’t going to happen easily. People are naturally skeptical of new approaches, and get very defensive when it comes to their data, content, and other digital assets, as it can be seen as a threat to their livelihood–opening up and sharing these resources outside their sphere of influence.</p>

<p>The culture that has been established at large companies won’t be easily undone. It is a culture that historically has had a pretty large gulf between business groups, and the IT groups who delivered the last generation of APIs (web services), that weren’t meant to be accessible, and understandable by business users. Web APIs have become simpler, more intuitive, and have the potential to be owned, consumed, and even in some cases deployed by business users. Even with this potential, many of the legacy rifts still exist, and business users feel this isn’t their domain, and IT and developer groups often feel APIs are something that should stay in their domain–perpetuating and confusing existing challenges already in place.</p>

<p>While there may be small API success within large companies, they often experience significant roadblocks when they try to scale, or spread to other groups. A huge investment is needed in API training amongst not just business users, but also developer and IT groups who may not have the experience with the web that is needed to make an API program successful. This can be the most costly and time-consuming aspect of doing APIs, and with many APIs being born out of technical groups, and are often under-funded, experimental efforts, investment in the basics of web literacy, and API training is often anemic. Setting the stage for what is happening when you begin unraveling legacy systems and processes is essential to minimize friction across API implementations. Without it, humans and culture will be your biggest obstacles to API success.</p>

<p>Web literacy, and API training really isn’t much different than other areas where corporate training is being applied, but for some reason many companies just expect the technology folks to know what they know already, or problem solve and learn on the job. This might have been find when things got done in purely technical circles, but web APIs aren’t purely about tech. They are about leveraging the web to solve problems people face within a company, getting access to resources, and working with external partners to help move business forward. IT and developer staff aren’t always ready for this type of external facing roles, and if business users aren’t up to speed on what is needed, API implementations will stumble, sputter, and ultimately fail. Think of the partnerships it’s taken to make the web work at your company, everyone is using the web, why should it be different with APIs? If APIs are done right, and people are properly educated, there is no reason an entire group can’t work in concert.</p>

<p>Every API effort I’ve seen fail had one common road block–people. There were IT groups that sabotaged, sales teams that felt threatened, executive leadership who didn’t understand what was happening, or partners who were in proper alignment with API efforts. Sure, sometimes the challenges are purely technical. Lack of proper API design. Insufficient security or capacity. These are simply API training and education issues as well. You can’t throw the need for integration of resources between internal groups, external partners, or 3rd party developers using the web at any technical group and expect them to understand what is needed. Similarly, you can’t mandate APIs across business groups, and just expect them to get on-board without any friction. Invest in the web literacy skills, API training and awareness, and communication skills that will be required to do APIs right, and the chances your API efforts will succeed will greatly increase.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/19/what-is-the-biggest-challenge-for-big-companies-doing-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/19/what-has-been-the-biggest-change-in-the-api-industry-since-i-started-api-evangelist/">What Has Been The Biggest Change In The Industry Since I Started API Evangelist</a></h3>
        <span class="post-date">19 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/beach-rocks-currents_internet_numbers.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>The biggest change in the industry since I started doing API Evangelist in 2010 is who is doing APIs. In 2010 it was 95% startups doing APIs, with a handful of enterprise, and small businesses doing them. I’d say over the last couple years the biggest change is that this had spread beyond the startup community and is something we see across companies, organizations, institutions, and government agencies of all shapes and sizes. Granted, there are a variety when it comes to the level they are doing them, and the quality, but APIs are something that has been moving mainstream over the last seven years, and becoming more commonplace in many different industries.</p>

<p>In 2010 it was all about Twitter, Facebook, Amazon, and many of the API pioneers. This has been rapidly shifting to each wave of startups like Twilio, Stripe, Slack, and others. However, now in 2017 I am seeing insurance companies, airlines, car companies, universities, cities, and federal agencies with API programs. I mean, c’mon, Capital One has an API program (wink, wink). While I still hold influence with each wave of API service providers looking to sell to the space, and many of the API startup providers, my main audience is folks on the frontline of the enterprise, and government agencies at all levels. I also have a growing number of people at higher educational institutions tuning into what I’m writing as they look to evolve their approach to technology. APIs were mainly a startup thing in 2010, and in 2017 it is about getting business done in a digital age thing.</p>

<p>The technology of APIs is still expanding and we are seeing things push beyond just REST, and web APIs, but by far the biggest change has been more about the business of doing APIs, and more importantly sometimes, the politics of doing APIs. These are areas of the industry that are rapidly expanding and evolving as new people onboard with the concept of an API, and the opportunity for doing APIs. As we add new companies, organizations, institutions, agencies, and industries to API conversation, the technology of APIs hasn’t shifted to much, but the business and political landscape is flexing, shifting, and evolving at a pretty rapid pace, and it is something that isn’t always a good thing. Along with it comes privacy, security, financial, and other challenges that will only get worse if there isn’t more discussion and collective investment.</p>

<p>The shift I’ve seen between 2010 and 2017, feels a lot like the change I witnessed from 1995 to 2002 with the web, but this time it’s more than just about websites, it is also about mobile applications, devices, conversational interfaces, automation, and much more. Honestly, it is simply just the next evolution of the web, where there is significantly more channels for operating on than just a browser, and there is a growing amount of digital assets being distributed via the web beyond just text, and images. Video has picked up speed, voice and audio are finally maturing, and algorithms, machine learning, and artificial intelligence are seeing a significant uptick. While all of these areas will have their impact, the biggest changes will come from leading industries like healthcare, education, banking, transportation, and others going beyond just dipping their toes in the API space, but baking it into everything they do.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/19/what-has-been-the-biggest-change-in-the-api-industry-since-i-started-api-evangelist/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/18/why-did-we-need-the-api-evangelist/">Why Did We Need The API Evangelist?</a></h3>
        <span class="post-date">18 Sep 2017</span>
        <p><img src="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>You needed the API Evangelist because there was nobody paying attention to the big picture of the API space. Sure, there are many vendors who pay attention to the big picture, and there are analysts who are paid to pay attention to the bigger picture to help validate the vendors, but there is nobody independent. At least there wasn’t in 2010 when I started. Now, there are a number of leading API experts and visionaries who work at different companies, and are able to maintain an independent view of the space, but in 2010 this did not exist. I’d like to think I helped make such a thing possible, but honestly it probably would have happened without me.</p>

<p>Developer advocates, and evangelists tend to pay attention to a specific API, set of APIs, or API services and tooling. I pay attention to everything. I keep an eye on as many APIs as I possibly can, and work to evaluate all the services, tools, and technology that emerges on the landscape. I try to remain objective about what is working, and what is not, and share stories about both. I still have my biases, and tend to hold grudges against a few companies for their bad behavior, but for the most part I’m just trying to share an honest view of what is going on at the 100K view. Something that differs from the analysts, because I don’t have a vendor driven agenda, I’m just looking to understand.</p>

<p>Another area that I benefit the space is educating the normals about what is going on. I’m priming your customers, and the decision makers who will be buying your products, services, and putting your tooling to work. Not every company is willing to invest heavily in the area of API education beyond their own products and services, and it is something that needs significant investment. I’ve had API service providers thank me for providing articles, white papers, research, and guides they can use to help validate what they are saying. I’m used by newspapers, tech blogs, and ocasionally analysts to validate their own findings and stories about what is going on in the API space. I help API providers and service providers do better, and this is why some companies support me financially by sponsoring my work.</p>

<p>Even with all my work over the last seven years, we need hundreds of more API Evangelists. We need API Evangelist in every industry, and in every country and region. It’s not a model that will scale, and don’t think about going to get some funding to make it happen. We just need other people who care about their sectors, have the capacity to make sense of the technology, while also still be able to explain what is going on to normals, and holding their own with developers and IT folks. You need the API Evangelist because most people are just looking to sell you something, even if they are really nice folks. You need the API Evangelist, because I’m going to look at things with an honest and critical eye that isn’t blinded by what I’m trying to sell you, what my boss’s or investor’s agenda is. Even if that agenda is mostly positive, they will always miss a significant portion of what is going on. I’m where you come to ask questions, and read stories about what is happening, without things being skewed by money. I’m not doing this to get rich, build a startup, exit, or doing anything beyond just making a decent living, and paying my bills.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/18/why-did-we-need-the-api-evangelist/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/18/what-is-the-role-of-an-influencer-in-the-api-industry/">What Is The Role Of An Influencer In The API Industry?</a></h3>
        <span class="post-date">18 Sep 2017</span>
        <p><img src="http://i1.wp.com/restlet.dreamhosters.com/wp-content/uploads/2013/12/kinlane.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>The idea of an influencer in the API space will mean many things to many different people. I have pretty strong opinions about what an influencer should do, and it is always something that should be as free of product pitches as it possibly can. Influencing someone in the API space should mean that you are not just influencing their decision to buy your product or service. That is sales, which has it’s place, but we are talking about influencing. I would also add that influencing SHOULD NOT be steeped in convincing folks regarding what they should invest in, at the technology purchasing level, all the way up to the venture capital level. The role of an influencer in the API industry should always be about education, awareness, and helping influence how average flks get everyday problems solved.</p>

<p>Being an influencer always begins with listening and learning. We are not broadcasting or pitching, we want to influence, so we need to have an idea about who we are influencing, and what will resonate and help them solve the problems they face. I do a significant portion of this by reading blogs, tuning into Twitter, and spending time on Github understanding what folks are building. Next, I engage in conversations with folks who are doing APIs, looking to understand APIs, and listening to what their challenges are, and what matters to them. At this stage I am not influencing anyone. I am being influenced. I’m absorbing what is going on, educating myself about what the problem set looks like, and better understanding my potential audience, when and if I get around to doing some of that influencing.</p>

<p>With a better understanding of an industry, a specific audience, and potentially the problems and challenges faced with doing APIs, I will usually step back from APIs entirely. I want to better understand the industry outside of just doing APIs. I want to understand the companies, organizations, instutions, and potentially government influence on what is happening. Everything that is already going on often weighs on doing APIs way more than the technology will ever by itself. I’m looking to understand the business and politics of operating in any sector before I will ever begin doing any sort of influencing within an industry, and to any specific audience. In technology circles, I find that many of us operate within silos, with our blinders on, and don’t always understand the scope of the problem we are looking to provide API solutions for. Stepping back is always healthy.</p>

<p>Once I’ve done my research, engaged in conversations with folks in an area I’m looking to influence, I’ll begin to write stories on the blog. This is all just exercising and training for the white papers, guides, workshops and talks I will be giving in any area I’m trying to influence. I will do this for months, repeating, reworking my ideas, and developing my understanding. The process usually brings more people out of the woodworks, opening up even more conversations, and influencing my industry, but also potentially adding to the number of folks I will be influencing. Slowly I will build the knowledge and awarness needed to truly be able to influence people in any industry, ensuring I have the platform of knowledge I will need, and grasp the scope of the challenges and problems we will be looking to deliver API solutions for.</p>

<p>The role of an API influencer is always a two-way street. You should be influenced just as much, or more than you are influencing. You should be working with influencers to understand your challenges. Tell us your stories, even if they are confidental. Help us understand your industries, and the unique problems and challenges that exist in there. Invest in us listening to your stories, and us telling your story on our blogs, and other longer form content. This is how we’ll help work through what is going on, and find the right path for your API journey. We can bring a lot of value to your API operations, and help you work through the challenges you face. This isn’t about content creation, or simply workshops, training, white papers, and public speaking. This is about influencing, and making an impact. You can’t do this without truly knowing what is going on, and being able to intelligently speak what is going on. This takes time, practice, investment, and actually giving a shit. It is something not everyone can pull off.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/18/what-is-the-role-of-an-influencer-in-the-api-industry/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/18/why-was-my-week-of-rants-so-well-received/">Why Was My Week of API Rants So Well Received?</a></h3>
        <span class="post-date">18 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/36349140070_d5ec39cb34_z.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>A couple weeks back I spent the entire week ranting on API Evangelist, instead of my usual lineup of API stories. Normally these types of stories end up on KinLane.com, or my rants edition, and usually don’t get tweeted out. I’m just venting. However on this particular week, I had enough people “piss in my cheerios”, that I felt the space needed to hear my rants, instead of the usual “nice guy” tone I tend to take on here. Granted, I can be pretty outspoken and blunt in my storytelling, but I usually work very hard to keep a professional tone, and be as nice as I possibly can. There are plenty of assholes in the API space, and I really don’t want to be one of them–even though it comes pretty naturally for me. ;-)</p>

<p>I actually got tired of the tone by mid-week, but I had so many people Tweet, email, and post on LinkedIn, Facebook, and Slack that they were enjoying the rants, I kept it going until that Friday. I was moving to New York, and really didn’t have much time to do the normal amount of research it takes to write stories, and the rants were an easy want to get content up that took me about 10 minutes to write. So why did people like the posts? First, I have to say that not everyone did. I heard from a number of other people that thought I was being a diva, and found the tone offensive. I also heard from a number of folks who were concerned for my mental state, and made sure they checked in on me. So, there were a number of emotions shared, but overwhelmingly people did find it worthwhile.</p>

<p>I feel that people enjoyed it because I was speaking truth in an environment where not many folks do. You see most people have jobs, bosses, and investors. I do not. Few people want to get in trouble with their boss and lose their job. If you run a startup you don’t want to piss off your investors, and lose your funding, or not be able to get funding when you are seeking it, because you said the wrong thing. I’d say these are the top two reasons. Sure, there are other reasons, like you might not get invited to te right events, or be able to hang out with the cool kids, but money is the number one reason people don’t speak the truth on a day to day basis. This is how the world keeps people in line, is with the purse strings. Honestly, most of it isn’t direct, it is self-filtering, where people just perceive there will be repercussions, and they just keep things quiet, and do not rock the boat.</p>

<p>People look to me to not bullshit them in API space. Sell them unneeded products and services, and tell them make believe marketing stories. They are used to me speaking my mind, and being honest about what I’m seeing. I’m not going to be pushing a product, service, or company I don’t agree with just because I was given money. I’m going to be honest about each wave of technology that comes along. I’m also going to call out the everyday bullshit and games we all encounter, but are forced to keep playing to keep our jobs, and get that next round of investment. I’m always amazed at how people change over time when they get to know me, begin to lower their barriers, and realize I’m not going to screw them over, take their ideas, or sell them something they don’t need, simply because they are in my sales funnel, or because I need to make my numbers. I don’t think people realize how puckered up they are on a daily basis from all the stuff we are bombarded with.</p>

<p>Even when you do find rants or seemingly truth speakers online, most people know they have an agenda. I was just ranting because my bullshit levels had overflowed, and enough people had pissed me off. I really don’t think I’m going to change much with my rants, I was just blowing off steam. I think people enjoy an agenda-less ranty story everyone once in awhile. In a world where everything is awesome, and each wave of technology is revolutionary, some folks just want the real deal, no bullshit coating. Much of what I’m saying is what all y’all are thinking anyways. You just don’t get to say it, so you enjoy hearing me say it. My ability to be able to write this way comes from many years of having a job, mortgage, boss, investors, and a wife, where I had to keep my mouth shut. Now I have none of those things, and I have a hot girlfriend who is rantier than I am, but in a way smarter way than I could ever hope to be. So, why not. Speak truth. I mean there is a lot of fake out there these days, it feels good to just tell it like it is.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/18/why-was-my-week-of-rants-so-well-received/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/18/what-it-was-about-web-apis-that-first-captured-my-attention/">What It Was About Web APIs That First Captured My Attention?</a></h3>
        <span class="post-date">18 Sep 2017</span>
        <p><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" align="right" width="40%" style="padding: 15px;" /></p>

<p><em>I am spending two days this week with the <a href="https://developer.capitalone.com/">Capital One DevExchange</a> team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts.</em></p>

<p>In the spring of 2010 I was ready for a career shift. I was running North American event for SAP, and had also taken up running events for Google, which included Google I/O and Developer Days. I was the VP of Technology, and made all the decisions around usage of tech, from email blasts, to registration, session scanning, and follow-up reporting. When I took over the role I was dealing with a literal hostage colocation facility for server infrastructure, and massive hardware expenditure on servers that I didn’t need most of the year. Then in 2007 I began using the Amazon Cloud, and got to work re-engineering systems to be more API-centric, leverage AWS APIs to orchestrate my operations.</p>

<p>By 2007 I had been playing around with web APIs for some time. I had incorporated payment and shipping APIs into commerce systems, and integrated Flickr, Delicious, Twitter, Facebook and other APIs into applications. I had plenty of SOAP web service experience when it came to enterprise infrastructure, but this was the first time I was deploying global infrastructure at scale using web APIs. I realized that web APIs weren’t just hobby toys, as my SAP IT director in Germany called them, they were an actual a tool I could use to operate a business at scale. My success resulted in more work, taking on more events, and scaling operations, which didn’t always pencil out to me actually being happier, even though the events scaled more efficiently, and out-performed what had come before.</p>

<p>The two Google I/O events where I managed the technology were the first ones where Google gave away their new Android mobile phones. I saw first hand what was happening in the mobile market, with the growth of the iPhone, and everyone scrambling to deploy APIs to support the new applications their were developing. Now, I was also beginning to develop new APIs to support what was possible via Android devices. It was clear that web APIs were going to be the preferred way to deliver the resources needed on mobile phones, and by 2010 there was no doubt that this mobile thing was going to be around for a while. Both SAP and Google were pushing on us to deliver resources that could be used on mobile platforms across all the events we were managing, and I saw that web APIs were how we would do this at scale.</p>

<p>I was using web APIs to deliver compute, storage, and other essential infrastructure to support global events. I was also using web APis to deliver resources to iPhone applications, and now Android applications. I wanted to better understand how this was being done, so in 2010 I began studying the world of APIs, looking at the common approaches to delivering APIs. I quickly saw there were plenty of pundits discussing the technical details of doing APIs, and I decided that I would focus on the business of doing APIs, and specifically how I can help convince business leaders to understand the potential. By summer of 2010 I had settled in on the name of my research blog, and by October I was beginning to publish my research on the blog. Seven years later, 3,000 blog posts later, I’m still doing it, and enjoy the focus on this important layer of not just the web, cloud, and mobile, but how APIs are being used in devices, on the network, and for bots, voices, and other conversational applications.</p>


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/18/what-it-was-about-web-apis-that-first-captured-my-attention/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/15/regulations-creeping-in-on-ai-ml-cognitive-and-other-fronts/">Regulations Creeping In On AI, ML, Cognitive, And Other Fronts</a></h3>
        <span class="post-date">15 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/supreme-court-judgement.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I wrote an piece earlier today about not fearing AI, but possessing a significant amount of concern when it comes to the people behind. I figured I’d continue with the trend on this Friday afternoon, and talk about the coming regulations when it comes to artificial intelligence (AI), machine learning (ML), and everything cognitive, intelligent, and algorithmic. I am not fully a believer in regulations being the only solution, but I know they are the solutions that bigcos tend to pay attention to. Which is why they spend so much money to distort, and bend them to what they want to see in their industries.</p>

<p>We are entering a phase of the Internet where there are going to an increased number of calls for regulations. Whether it’s privacy, security, breaches, or specifically on technology like drones, artificial intelligence, bots, and machine learning, expect more government involvement in the future. This isn’t because government is inheriently bad, and is looking to suffocate business, it is primarily because these areas of technology are being defined by the worst among us. When you bundle with the not so bad folks, and even many of the good folks refusing to reign in their industry partners, and fellow technologists, you end up with more regulations imposed to stablize things. If the tech space was more willing to step up and take the lead regarding acceptable practices, this wouldn’t be necessary.</p>

<p>Algorithms are making more decisions in our lives. After seeing what Facebook and Twitter have done during the last US election, and seeing AI and ML continue being applied to important aspects of our lives, there will be more inquirires by the government, and calls for the government to step in. I know that platforms don’t want to be regulated, and with very libertarian stances in much of Silicon Valley, there is a significant undertow of anti-regulatory, and anti-government sentiment. However, if you believe in the wisdom of the crowds, you have to acknowledge your role in determining how the crowds behave. You are developing the platforms that we can automate with bots. You are providing the platforms for deploying the next generation of AI and ML. In many cases, these existing tech players are also investing in the next wave of startups. If you don’t set the tone for what acceptable practices are, the federal government eventually will.</p>

<p>When this happens I’m going to be here to point at APIs as one possible regulatory solution. I’m going to have years of blog posts on the subject, with plenty of evidence about how APIs, and API management can be provide observability into how platforms, and algorithms operate. I’m not doing this because this is the future I want. I’m doing this because this is the future you all have set the stage for. I get that you don’t want to reign in your startup co-founders, trusted partners, or investors. The system is designed to punish you if you do. However, if you let the worst of the worst lead the conversation when it comes to AI, ML, algorithms, and other tech trends, regulations will be the only answer. So, put on your seatbelt, and get ready for the government stepping up to dictate the rules of the road, because it seems to be the only thing many of you will actually respond to, whether you like it or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/15/regulations-creeping-in-on-ai-ml-cognitive-and-other-fronts/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/15/sensibly-thinking-about-where-technology-ends-and-the-human-part-begins-with-apis/">Sensibly Thinking About Where Technology Ends And The Human Part Begins With APIs</a></h3>
        <span class="post-date">15 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/mosaic-face_blue_circuit_3.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>Our team at a hackathon I’m participating in this week is working on a data aggregation tool for helping merge multiple hurrican shelter data sources from Irma in Florida. While the need for the data is winding down, the use case for the tool could be something that lives on, and could help communities in the future. This projects aggregates multiple data sources for shelters from FEMA, municipal, and sources pulled together by volunteers. Our team is focused on aggregating, and doing as much heavy lifting to  automatically merge and cleanse the data as they can, but then at the right moment render it for humans to step in and finish the work.</p>

<p>I was impressed with the balance struck by the team. Knowing where to apply technology, and when to rely on humans. The problem of merging open data from multiple sources is a big and complex one. It is one that I’ve seen many technologists think they can step up and solve simply with their tech toolbox, no humans necessary. Our team quickly saw the scope of the program, discussed at length about what they could accomplish, and what they couldn’t accomplish, then got to work in the code to deliver the functionality, but then also developed a web interface to allow humans to step in at just the right point. Striking a balance between the human and technological aspects of doing this–which is what the Human Services Data Specification (HSDS) is all about.</p>

<p>I know there is a significant amount of information out there about User Experience (UX), and also increasingly Developer Experience (DX). However, I think skills to know where to apply technology, and when to step back from using it, and focusing on augmenting, empowering, and putting the humans in charge are seriously deficient in our sector. I regularly encounter developers who think that technology is the solution, and humans are the problem. This contempt always degrades the amount of investment in the user interface portion of the question, and will also shift the developer experience portion, ensuring the API speaks to the technological needs, and not the human needs. This isn’t how all of this should work. It isn’t about the tech. It is about what the technology does for humans, not the other way around</p>

<p>The balance of API backend to human front end on this week’s human services hackathon was refreshing to see. Early on I saw the team leaning towards trying to merge, clean up, and solve all the data problems in the code, and I was a little concerned. However, by the end of the 2nd night they showed me their API definition and design, as well as the web interface meant for humans. I felt they struck a perfect balance between the tech and human aspects of delivering human services. This balance is a topic you will here more about here on the blog as I talk about APIs, and how they are being wielded for artificial intelligence, machine learning, voice, bots, and every other digital layer of our world that often seems to be being consumed by technology. I’m always looking for the emphasis of human over the technology, and I am pleased with the outcome of this hackathon. I’ll showcase the work once we are done. It is something I’m thinking will be useful in supporting future natural disasters, something I’m feeling is going to become a more common occurrence in our world.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/15/sensibly-thinking-about-where-technology-ends-and-the-human-part-begins-with-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/15/i-do-not-fear-ai-i-fear-the-people-doing-ai/">I Do Not Fear AI, I Fear The People Doing AI</a></h3>
        <span class="post-date">15 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/servers-blue-circuit.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>There is a lot of FUD out there when it comes to artificial intelligence (AI) and machine learning (ML). The tech press enjoy yanking people’s chain when it comes to the dangers of artificial intelligence. AI is coming for your jobs. AI is racist, sexist, and biased. AI will be lead to World War III. AI will secure and protect us from the bad out there. AI will be the source of all of our worries, and the solution to all of our worries. I’m interested in the storytelling around all of this, and I’m fascinated by the distracting quality of technology when it comes to absolving the humans behind of doing bad things.</p>

<p>We have the technology to make this black boxes more observability and accountable. The algorithms feeding us news, judging us in courtrooms, and deciding if we are insurable or a risk, can all be wrapped with APIs, and made more accountable. However, there are many human reasons why we don’t do this. Every AI out there can be held accountable, it isn’t rocket science. The technology exists to keep AI from hurting us, judging us, and impacting our lives in negative ways. However, it is the people behind who do not want it, otherwise their stories won’t work. Their stories won’t have the desired effect and control over our lives.</p>

<p>APIs are the layer being wielded for good and for bad on the Internet. Facebook, Twitter, and Reddit, all leverage APIs to be available on our mobile phones. APIs are how people automate, advertise, and fund their activities on their platforms. APIs are how AI and ML are being exposed, wielded, and leveraged. The technology is already there to make them more accountable, we just don’t have the human will to use the technology we have. There is more money to be made in telling wild stories about what is possible. Telling stories that make folks afraid, and in awe of what is possible with technology. APIs are used to tell you the stories, while also making the fire shoot from the stage, and the smoke and the mirrors operate, instead of helping us see, understand, and verify what is going on behind the scenes.</p>

<p>We rarely discuss the fact that AI isn’t coming for our jobs. It is the people behind the AI, at the companies developing, deploying, and operating AI that are coming for our jobs. AI, like APIs, are neither good, nor bad, nor neutral–they are a tool. They are technology, and anything they do is because of us humans. I don’t fear AI. I only fear the people doing AI. The people who tell the stories. The people who are believers. I don’t fear technology because I know we have the tools to do what is right, and hold the people who are using technology in bad ways accountable. I’m afraid because we don’t seem to have the will to look behind the curtain. We hold up many of the people telling stories about AI as visionaries, leaders, and truth tellers. I don’t fear AI, I only fear its followers.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/15/i-do-not-fear-ai-i-fear-the-people-doing-ai/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/15/i-will-see-you-at-apistrat-in-portland-in-november/">I Will See You At APIStrat In Portland This November</a></h3>
        <span class="post-date">15 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/apistrat/apistrat-portland-screenshot.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://events.linuxfoundation.org/events/apistrat/program/schedule">We are putting the finishing touches on the schedule for APIStrat in Portland, OR, October 31st through November 2nd</a>. We have all the workshops, sessions, and keynotes dialed in (not all keynotes announced, wink, wink), and it is all just about making sure y’all show up and participate in the conversation. This is my 2nd favorite part of the event, the build-up for the big day(s). This is the 8th APIStrat we’ve done, and it is the first one we’ve done as part of the Linux Foundation, and with the OpenAPI Initiative. I’m excited.</p>

<p><a href="http://events.linuxfoundation.org/events/apistrat/program/schedule">Make sure and take a look at the session schedule</a>. We received over 165 submissions, and had a program committee of almost 30 people vote to decide with 60 would be accepted. I am the program chair and helped make some difficult decisions, but ultimately I”m pretty proud of the lineup we’ve pulled together. It’s much of the same popular topics as you’ve seen at previous events, with new faces, and brands, but there is also some of the leading edge conversations around serverless, gRPC, GraphQL. Of course, there is also going to be a lot of talk about OpenAPI, in workshops, sessions, and on the main stage. So check out the schedule if you haven’t, it’s pretty sweet lineup.</p>

<p>I want to personally thank <a href="http://www.microsoft.com/">Microsoft</a>, Stoplight, <a href="https://smartbear.com/">SmartBear</a>, <a href="https://www.getpostman.com/">Postman</a>, <a href="https://developer.capitalone.com/">CapitalOne DevExchange</a>, <a href="https://apimatic.io/">APIMATIC</a>, <a href="http://www.redhat.com/">Red Hat</a>, <a href="https://cloud.google.com/">Google</a>, and <a href="http://www.cloud-elements.com/">Cloud Elements</a> for <a href="http://events.linuxfoundation.org/events/apistrat/sponsors/our-sponsors">sponsoring and making sure APIStrat happens</a>. Of course, thank you to <a href="http://www.linuxfoundation.org/">The Linux Foundation</a>, and the <a href="https://www.openapis.org/">OpenAPI Initiative (OAI)</a> for taking the lead on APIStrat as it continues to grow and mature with the API community. I want to also thank my partner in crime, Steve Willmott, and the 3Scale / Red Hat team–without them APIStrat wouldn’t be a thing.</p>

<p>Next up for me, now that the schedule is dialed in. Is to just tell stories about what will be happening. I’m going to go through each of the speakers, and companies who are present and look to see what they are up to with APIs. It is something I always try to do in the final months of build up to the conference. APIStrat has been an important part of how I learn about what is going on with APIs, and who the interesting companies, and people are. Hopefully it is the same for you, and we can both be there in November, and learn what is going on together. It will be my first event where I’m not giving a talk. ;-) I’ll still be MC’ing, and harassing y’all in the hallways, but I’m looking forward to being able to tune into more of the talks as they occur.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/15/i-will-see-you-at-apistrat-in-portland-in-november/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/14/using-3rd-party-apis-to-break-you-out-of-your-enterprise-bubble/">Using 3rd Party APIs To Break You Out Of Your Enterprise Bubble</a></h3>
        <span class="post-date">14 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-bubbles.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m participating in a hackathon in Princeton, New Jersey as part of <a href="http://org.open.referral.adopta.agency/">my work on the Human Services Data API (HSDA)</a>. We are at a large enterprise financial group’s office, as part of a three day social good hackathon / code sprint. Everybody participating is taking time off from their normal day job as back-end or front-end programmer, and business analyst, to build something for the greater good. Since it is an enterprise developer group the concept of a hackathon is somewhat new to them, and is the first time they’ve worked on external projects, instead of an internally focused hackathon event.</p>

<p>I’m enjoying watching the two teams working on human services projects be forced out of their bubble. One of the projects has three separate 3rd party APIs to work with. 1) Simple spreadsheet deployed web API, 2) government agency published web API, and 3) HSDA API operated by a municipal organization. I am sitting here watching them get exposed to the variety of implementations, quality of data and interface, and wrestle with establishing their project requirements. After being pulled from their bubble trying to understand the APIs, they are also finding themselves pulled out of their local development world, having to potentially use 3rd party tools, services, and even reverse engineering a library or codebase in a language they are not familiar with.</p>

<p>This is all very, very healthy. No matter what gets built at this hackathon, the fact that they are being pulled out of their bubbles, will benefit their world. They are thinking outside their governance bubble. They are forced to learn about the API best or worst practices of other organizations. They are having to use services, tools, and programming languages they aren’t familiar with. All with the motivation of potentially building something for good. They are exercising their skills and knowledge in ways that they won’t encounter in the routine, and highly structured worlds they exist in. Another layer of all of this is that a portion of the team members are from an external group, and have never even met in person–I just watched two of them introduce themselves, and make the connection that they’ve worked together on many projects, but never met in person. #win</p>

<p>This isn’t just startup style thinking for a hackathon. The objective of this event is to build on top of existing tooling, improve existing processes, and add value to existing non-profit organizations. Even with these objectives, the most value is the exhaust from the conversations, planning, and what folks are learning along the way. Also, getting these folks out of their bubble tackling meaningful problems, pulling them away from their routine, and feeling like they are making a change. The hackathon format is part of this, but the API(s) are really a catalyst for change, and a vehicle for helping pull folks out of their carefully crafted environments. The APIs are helping these enterprise developers, project managers, and business analysts think differently, and consider other approaches to getting things done. Hopefully something that will stick with them in the future.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/14/using-3rd-party-apis-to-break-you-out-of-your-enterprise-bubble/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/14/lost-in-api-transit/">Lost In API Transit</a></h3>
        <span class="post-date">14 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/subway/mta-subway-map.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I got on the New York Subway today heading for Penn Station to catch a train (New Jersey Transit) out to Princeton for a hackathon. As I was navigating my way through Metropolitan Transit Authority (MTA) and the New Jersey Transit I was thinking about <a href="http://apievangelist.com/2017/08/17/testing-out-the-concept-of-api-transit-instead-of-api-lifecycle/">my usage of API transit instead of API lifecycle</a>. The number one response I had to this concept from readers was in regards the cognitive load experienced when you first look at a subway map that represents API infrastructure, and would anyone even know what I was talking about.</p>

<p>It’s true, when you first look at any of the API subway maps I’ve created so far, you scratch your head to figure out what they mean. I haven’t spent a lot of time making them coherent, but I am also just getting going with the work. Truthfully, they’ll get more complicated, over getting simpler. However, each time I first use the subway in NYC there is also a pretty significant cognitive load. I’ve ridden the subway many times, but each time I still have to study the map, learn the portion I need to get what I need done, and accept that much of it I won’t actually ever understand. I usually only learn what applies to me, and the more time I spend riding a transit system, the more time it comes into focus–something that applies to any transit system in the world I’ve used.</p>

<p>Think about when you start a new job, or adopt an existing legacy project as an API product manager. You do not immediately understand all the moving parts, absorb any diagrams, or documentation the first time you look at them. It takes time experiencing a system, before you will get acquainted, and become a local, like someone riding the MTA or NJT transit systems. Now that I live in NYC I’m going to spend time learning the transit system so I can get around, but I’m also going to invest energy learning it from an operators perspective, and understand the challenges they are facining maintaining, evolving, and keeping the system usable for users. I’m sure there will be a wealth of analogies in there for me when it comes to IT and API infrastructure.</p>

<p>Currently, I’m pretty lost on the MTA and NJT transit systems, but it’s slowly coming into focus. A significant piece of this is the maps that are available to me. Also the physical display systems available to me in the stations, as well as online, and on my mobile phone. I’m pushing forward the next generation of my API Subway MAP tooling at the same time. I’m creating a simple Siren-defined, Jekyll-driven, Github hosted map that helps me walk through a variety stops along the API lifecycle, along a handful of “lines” from design, to deployment, testing, and security. We’ll see how well I do bridging these concepts, but I’m hopeful that eventually it will come into focus, and I’ll stop being so lost, and develop a better understanding of what is going on.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/14/lost-in-api-transit/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/13/a-sample-openapi-30-file-to-get-started/">A Sample OpenAPI 3.0 File To Get Started</a></h3>
        <span class="post-date">13 Sep 2017</span>
        <p><a href="https://github.com/OAI/OpenAPI-Specification/edit/OpenAPI.next/examples/v3.0/petstore.yaml"><img src="https://s3.amazonaws.com/kinlane-productions/openapi/openapi-logo.png" align="right" width="15%" style="padding: 15px;" /></a></p>
<p>I am investing more time into <a href="http://schema.org.apis.apievangelist.com/">my Schema.org work</a>, alongside my learning about OpenAPI 3.0. <a href="http://apievangelist.com/2017/09/12/my-favorite-part-of-openapi-30-is-the-components-object/">I’m pretty excited about the components object</a>, and I want to push forward some of my Schema.org dictionary ideas, to help folks get better at reusing common schema throughout their work. <a href="http://Schema.org">Schema.org</a> is the most robust vocabulary out there, and we shouldn’t be reinventing the wheel in this area. I know the most important reason that folks aren’t using is that they either don’t know about it, or they are just lazy. I figure if I create some ready to go schema in an OpenAPI 3.0 components object, maybe people will be more inclined to put common schema to use.</p>

<p>To share my components I need basic OpenAPI 3.0 shell to hold all my reusable schema. I really don’t care about the paths, and other elements being their. <a href="https://github.com/OAI/OpenAPI-Specification/edit/OpenAPI.next/examples/v3.0/petstore.yaml">So I headed over to the OpenAPI 3.0 Github repo and borrowed the sample Petstore OpenAPI 3.0 my friend Darrel Miller created</a>:</p>

<script src="https://gist.github.com/kinlane/43934f44fd591a6ee59a45267d9e3066.js"></script>

<p>I will change all the information in this sample to reflect my work, but I figured before I did I would share this example document with my readers. At first glance it doesn’t look much different than version 2.0 of OpenAPI, but once you start studying you see the differences. You see the responses have JSON specific content types inserted in between their schema references. There is also a components object, with a couple of schema present–this is all I need. There are a bunch of other things you can store in your components object, but I think this provides a nice first look at what is going on.</p>

<p>If you are looking for some other working examples of OpenAPI 3.0 in action, <a href="https://github.com/Mermade/openapi3-examples/tree/master/pass/OAI">head over to Mike Ralphson’s repository</a>, he has some additional ones you can play with. I don’t know about you, but I learn from others. I need to reverse engineer API definitions from other people before I become fluent myself. I’m going to spend some time hand-crafting some OpenAPI 3.0 definitions, so that I become more fluent. It is tedious work when you are just getting going, but once you get it down, it becomes like any other language you use. I’m hoping to cut my teeth on this Schema.org work. I’m going to replicate the OpenAPI 2.0 work I did when I created over a 1,000 OpenAPIs for each of the Schema.org objects. I’m going to be using them to deploy APIs for clients, and in my API training and storytelling. I want all my examples to be reuable patterns that already exist, not anything custom that I pull out of my magic arse.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/13/a-sample-openapi-30-file-to-get-started/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/13/kubernetes-json-schema-extracted-from-openapi/">Kubernetes JSON Schema Extracted From OpenAPI</a></h3>
        <span class="post-date">13 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/kubernetes/kubernetes-json-schema.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve been doing my regular trolling of Github lately, looking for anything interesting. <a href="https://github.com/garethr/kubernetes-json-schema">I came across a repository this week that contained JSON Schema for Kubernetes</a>. Something that is interesting by itself, but I also thought the fact that they had autogenerated the individual JSON Schema files from the Kubernetes OpenAPI was worth a story. It demonstrates for me, the growing importance of schema in all of this, and shows that having them readily available on Github is becoming more important for API providers and consumers.</p>

<p>Creating schema is an important aspect of crafting an OpenAPI, but I find that many API providers, or the consumers who are creating OpenAPIs and publishing them to Github are not always investing the time into making sure the definitions, or schema portion of them are complete. Another aspect, as Gareth Rushgrove, the author of the Github repo where I found these Kubernetes schema points out, is the JSON Schema in OpenAPI often leaves much to be desired. Until version 3.0 it hasn’t supported everything you need, and many of the ways you are going to use these schema aren’t going to be able to use them in an OpenAPI, and you will need them as individual schema files like Gareth has done.</p>

<p>I just published the latest version of the OpenAPI for <a href="http://org.open.referral.adopta.agency/">my Human Services Data API (HSDA) work</a>, and one of the things I’ve done is extracted the JSON Schema into separate files so I can use them in schema validation, and other services and tooling I will be using throughout the API lifecycle. I’ve setup an API that automatically extracts and generates them from the OpenAPI, but I’m also creating a Github repo that does this automatically for any OpenAPI I publish into the data folder for the Github repository. This way all I have to do is publish an OpenAPI, and there is automatically a page that tells me how complete or incomplete my schema are, as well as generates individual representations that I can use independent of the OpenAPI.</p>

<p>I am hoping this is the beginning of folks investing more into getting their schema act together. I’m also hoping this is something that OpenAPI 3.0 will help us focus on more as well. Pushing API designers, architects, and developers to get their schema house in order, and publish them not just as OpenAPI, but individual JSON Schema, so they can be used independently. I’m investing more cycles into helping folks learn about <a href="http://json-schema.org/">JSON Schema</a> as I’m pushing my own awareness forward, and will be creating more tooling, training material, and stories that help out on this front. I’m a big fan of OpenAPI, and defining our APIs, but as an old database guy I’m hoping to help stimulate the schema side of the equation, which I think is often just as important.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/13/kubernetes-json-schema-extracted-from-openapi/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/13/versioneye-sdk-security-notifications/">VersionEye SDK Security Notifications</a></h3>
        <span class="post-date">13 Sep 2017</span>
        <p><a href="https://www.versioneye.com/"><img src="https://s3.amazonaws.com/kinlane-productions/versioneye/versioneye-logo.jpeg" align="right" width="30%" style="padding: 15px;" /></a></p>
<p>I’ve written about <a href="https://www.versioneye.com/">VersionEye</a> a couple of times. They help you monitor the 3rd party code you use, keeping an eye on dependencies, license violations, and security issues. <a href="http://apievangelist.com/2017/08/01/api-sdk-licensing-notifications-using-versioneye/">I’ve written about the license portion of this equation</a>, but they came up again while doing my API security research, and I wanted to make sure I revisited what they were up to in this aspect of the API lifecycle, floating them up on my radar.</p>

<p>VersionEye is keeping an eye on multiple security databases and helps you monitor the SDKs you are using in your application. Inversely, if you are an API provider generating SDKs for your API consumers to put to use, it seems like you should be proactively leverage VersionEye to help you be the eye on the security aspects of your SDK management. They even help developers within their existing CI/CD workflows, which is something that you should be considering as you plan, craft, and support your APIs. Making it as easy for you to leverage your APIs SDKs in your own workflow, and doing the same for your consumers, while also paying attention to security at each step, breaking your CI/CD process when security is breached.</p>

<p><a href="http://apievangelist.com/2017/08/09/open-sourcing-your-api-like-version-eye/">I also wrote about how VersionEye has open sourced their APIs</a> a while back, highlighting how you can also deploy into any environment you desire. I’m fascinated by the model VersionEye provides for the API space. They are offering valuable services that help us manage our crazy worlds, with a viable commercial and open source offering, that integrates with your existing CI/CD workflow. Next, I’m going to study the dependency portion of what VersionEye offer, then take some time to better understand their business model and pricing. VersionEye is pretty close to what I like to see in a service provider. They don’t have all the shine of a brand new startup, but they have all the important elements that really matter.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/13/versioneye-sdk-security-notifications/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/13/webhook-delivery-headers-from-github-api/">Webhook Delivery Headers From Github API</a></h3>
        <span class="post-date">13 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-circle-icon.png" align="right" width="30%" style="padding: 15px;" /></p>
<p>I am continuing my learning about Webhooks, and <a href="https://developer.github.com/webhooks/">Github keeps my notebook full with interesting building blocks we can use when crafting our own webhook strategies</a>. I’m not using everything I’m learning from Github in my current strategy, but I like adding each of these building blocks to my <a href="http://webhooks.apievangelist.com/">webhook research</a>, so that I can use in future guides that I publish. Today’s post overlaps two areas of my research into webhooks, and how headers are being used by a variety of API providers.</p>

<p>Github is using HTTP headers as part of the webhook response, providing the recipients of webhooks with more information about what is happening with each outgoing request. They are providing three custom headers along with each payload:</p>

<ul>
  <li><strong>X-GitHub-Event</strong> - Name of the event that triggered this delivery.</li>
  <li><strong>X-Hub-Signature</strong> - HMAC hex digest of the payload, using the hook’s secret as the key (if configured).</li>
  <li><strong>X-GitHub-Delivery</strong> - Unique ID for this delivery.</li>
</ul>

<p>In addition to these three custom headers, the User-Agent for the requests will have the prefix GitHub-Hookshot–so that your systems can identify these incoming requests more specifically. I like getting the name of the event, and definitely like the example of using the signature to make sure the payload hasn’t been tampered with, or from an untrustworthy source. Additionally you get a unique identifier for the delivery, allowing you to be able to record, and pull up unique webhook receipts.</p>

<p>I’m adding these all as building blocks to my webhook research. I still have a notebook full of other Github, Stripe, Twilio, and leading approaches to webhooks. Once I get through this round I’m going to apply what I’ve learned to the project I’m working on, and then see about pushing out the first draft of my webhooks guide–something I’ve never done before. If nothing else, I’m learning a lot. I’m learning from all the leaders in the space, who are several versions into their webhook designs. I’m finding the biggest challenge right now, is how I hold back and don’t do everything, keeping my webhook designs simple, intuitive, but as powerful as possible.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/13/webhook-delivery-headers-from-github-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/12/exporting-definitions-for-all-things-apis-including-bots/">Machine Readable Definitions For All Things API, Including Your Bots</a></h3>
        <span class="post-date">12 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-robot-lightning-bold.png" align="right" width="30%" style="padding: 15px;" /></p>
<p>Every aspect of my business runs as either YAML or JSON. This blog post is YAML stored on Github, viewed as HTML using Jekyll. All the companies, services, tooling, building blocks, patents, and other components of my research all live as YAML on Github. Any API I design is born, and lives as an OpenAPI YAML document on Github. Sure, much of this will be imported, exported, and exported with a variety of other tools, but the YAML and JSON definition is key to every stop along the life cycle of my business, and the work that I do.</p>

<p>It isn’t just me. I’m seeing a big shift in how many platforms, services, and tooling operate, with often times YAML, and still in many situations it has JSON, XML, and CSV at its core. Everything you do should have some sort of schema definition, providing you with a template that you can reuse, share, collaborate, and communicate around. Platforms should allow for the creation of these template schema, and enable the exporting, and importing of them, opening up interoperability, and cross-platform functionality–much like APIs do in real-time using HTTP. This is what OpenAPI has done for the API lifecycle, and there should be many complementary, or even competing formats that accomplish the same, but for specific industries, and use cases.</p>

<p>You can see this in action over at AWS, <a href="https://developer.amazon.com/blogs/alexa/post/d362a0ab-61f3-4b17-9fb7-1ad12f39496e/export-your-amazon-lex-bot-schema-to-use-in-your-alexa-skill">with the ability to export your Lex bot schema for use in your Alexa skill</a>. Sure, this is interoperability on the same platform, but it does provide one example of how YAML and JSON definitions can help use share, reuse, and develop common templates for not just APIs, but also the clients, tooling, and other platforms we are engaging with. You’ll see this expand to every aspect of tech as continuous integration and deployment takes root, and Github continues it’s expansion beyond startups, into the enterprise, government, and other institutions. Along the way there will be a lot of no name schema finding success, but we will also need a lot more standardization and maturing as we’ve seen with OpenAPI, for all of this to work.</p>

<p>I hear a lot of grumbling from folks when it comes to YAML. I get it, I had the same feeling. It also reminds me of how I felt about JSON when it first emerged. However, I find YAML to be very liberating of brackets, slashes, and other delimiters, but I also find it is just one format, and I should always be supporting JSON, XML, and CSV when it comes to one dimensional schema. I don’t find it a challenge to convert between the formats, or keep some things one-dimensional to bridge to my spreadsheet oriented users. I actually feel it helps me think outside of my bubble. I enjoy rifling through the YAML and JSON templates I find on Github from a variety of operations, defining their bots, conversational interfaces, visualizations, CI/CD, configuration, clients, and other aspects of operations. Even if I’m never using them, I find it interesting to learn how others define what they are up to.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/12/exporting-definitions-for-all-things-apis-including-bots/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/12/openapi-30-tooling-discovery-on-github-and-social-media/">OpenAPI 3.0 Tooling Discovery On Github And Social Media</a></h3>
        <span class="post-date">12 Sep 2017</span>
        <p><a href="https://github.com/Mermade/awesome-openapi3"><img src="https://s3.amazonaws.com/kinlane-productions/mike-ralphson/openapi_awesome1.png" align="right" width="35%" style="padding: 15px;" /></a></p>
<p>I’ve been setting aside time to browse through and explore tagged projects on Github each week, learning about what is new and trending out there on the Githubz. It is a great way to explore what is being built, and what is getting traction with users. You have to wade through a lot of useless stuff, but when I come across the gems it is always worth it. I’ve been providing guidance to all my customers that they should be publishing their projects to Github, as well as tagging them coherently, so that they come up as part of tagged searches via the Github website, and the API (I do a lot of discovery via the API).</p>

<p>When I am browsing API projects on Github I usually have a couple of orgs and users I tend to peek in on, and my friend Mike Ralphson (<a href="https://twitter.com/PermittedSoc">@PermittedSoc</a>) is always one. Except, I usually don’t have to remember to peek in on Mike’s work, because he is really good at tagging his work, and building interesting projects, so his stuff is usually coming up as I’m browsing tags. <a href="https://github.com/Mermade/awesome-openapi3">He is the first repository I’ve come across that is organizing OpenAPI 3.0 tooling</a>, and on his project he has some great advice for project owners: “Why not make your project discoverable by using the topic <a href="https://github.com/search?utf8=%E2%9C%93&amp;q=topic%3Aopenapi3&amp;type=Repositories&amp;ref=advsearch&amp;l=&amp;l=">openapi3 on GitHub</a> and using the hashtag <a href="https://twitter.com/search?q=%23openapi3">#openapi3</a> on social media?” « Great advice Mike!!</p>

<p>As I said, I regularly monitor Github tags, and I also monitor a variety of hashtags on Twitter for API chatter. If you aren’t tagging your projects, and Tweeting them out with appropriate hashtags, the likelihood they are going to be found decreases pretty significantly. This is how Mike will find your OpenAPI 3.0 tooling for inclusion in his catalog, and it is how I will find your project for inclusion in stories via API Evangelist. It’s a pretty basic thing, but it is one that I know many of you are overlooking because you are down in the weeds working on your project, and even when you come up for air, you probably aren’t always thinking about self-promotion (you’re not a narcissist like me, or are you?)</p>

<p>Twitter #hashtags has long been a discovery mechanism on social media, but the tagging on Github is quickly picking up steam when it comes to coding project discovery. Also, with the myriad of ways in which Github repos are being used beyond code, Github tagging makes it a discovery tool in general. When you consider how API providers are publishing their API portals, documentation, SDKs, definitions, schema, guides, and much more, it makes Github one of the most important API discovery tools out there, moving well beyond what ProgrammableWeb or Google brings to the table. I’ll continue to turn up the volume on what is possible with Github, as it is no secret that I’m a fan. Everything I do runs on Github, from my website, to my APIs, and supporting tooling–making it a pretty critical part of what I do in the API sector.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/12/openapi-30-tooling-discovery-on-github-and-social-media/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/12/my-favorite-part-of-openapi-30-is-the-components-object/">My Favorite Part Of OpenAPI 3.0 Is The Components Object</a></h3>
        <span class="post-date">12 Sep 2017</span>
        <p><a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#componentsObject"><img src="https://s3.amazonaws.com/kinlane-productions/openapi/openapi-30-components-object.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>There were a number of changes made to the structure of Open API in the move to version 3.0 that I am a fan of, but if I had to point at a single seismic shift that I think will move the conversation forward it is <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#componentsObject">the components object</a>. According to the specification the components object, “holds a set of reusable objects for different aspects of the OAS. All objects defined within the components object will have no effect on the API unless they are explicitly referenced from properties outside the components object.” It is the store for for all the common and reusable aspects of defining, and designing your APIs–which will have huge benefits on how we are doing all of this.</p>

<p>Here is the laundry list of what you can put into your OpenAPI 3.0 components object, and reference throughout your API definitions:</p>

<ul>
  <li><strong>schemas</strong> - An object to hold reusable data schema used across your definitions.</li>
  <li><strong>responses</strong> - An object to hold reusable responses, status codes, and their references.</li>
  <li><strong>parameters</strong> - An object to hold reusable parameters you are using throughout your API requests.</li>
  <li><strong>examples</strong> - An object to hold reusable the examples of requests and responses used in your design.</li>
  <li><strong>requestBodies</strong> - An object to hold reusable the bodies that will be sent with your API request.</li>
  <li><strong>headers</strong> - An object to hold reusable headers that define the HTTP structure of your requests.</li>
  <li><strong>securitySchemes</strong> - An object to hold reusable security definitions that protect your API resources.</li>
  <li><strong>links</strong> - An object to hold reusable links that get applied to API requests, moving it towards hypermedia.</li>
  <li><strong>callbacks</strong> - An object to hold reusable callbacks that can be applied.</li>
</ul>

<p><a href="http://apievangelist.com/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/">I’ve written about how many API developers see this stuff as duplicate work across our APIs</a>, where I see them as common, resusable patterns that we should be getting organized–the OpenAPI 3.0 components object is the beginning of us getting this house in order. The components object is how API architects and designers can ensure that API developers are being consistent in their work, and not just reusing common elements, but reusing well thought out, fully baked elements that adhere to standards and common definitions used throughout the industry.</p>

<p>The OpenAPI 3.0 components object is where we are going to start injecting API literacy training into the development process. It is where we will teach developers about headers, and common ways of securing our APIs. It is where we will start reusing common dictinaries like Schema.org so we STOP re-inventing the wheel when it comes to defining our schema definitions, fields, and other mundane aspects of crafting an API. The components object isn’t just where we are reusing components within a single OpenAPI, it is where we will start reusing across all the OpenAPIs we are crafting, and learning, sharing, collaborating, and reusing across OpenAPIs that are made publicly available.</p>

<p>The OpenAPI 3.0 components object is where we are going to start delivering the hypermedia literacy that was required to get the adopttion that hypermedia advocates envision, but were stonewalled because people just didn’t get it. I’m pretty excited about this aspect of OpenAPI 3.0, and I got myself so fired up about it last night I started building some of my API dictionary tooling I’ve had in my head for a while, but didn’t have just the right vehicle in mind for delivering at scale. I haven’t had much time for playing with OpenAPI 3.0, or the tooling that has emerged, but I got the bug now. I’m going to prioritize some work in this area, if nothing else for generating some relevant stories here on the blog, and keeping me in tune with folks are doing. Oh, that reminds me, have you seen what my friend Mike Ralphson (<a href="https://twitter.com/PermittedSoc">@PermittedSoc</a>) is up to? <a href="https://github.com/Mermade/">He is leading the charge when it comes to OpenAPI 3.0 tooling</a> « I recommend keeping an eye on what he is up to on Github.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/12/my-favorite-part-of-openapi-30-is-the-components-object/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/12/the-us-postal-services-wakes-up-to-the-api-management-opportunity-in-new-audit/">The US Postal Service Wakes Up To The API Management Opportunity In New Audit</a></h3>
        <span class="post-date">12 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/usps/office-of-inspector-general-united-states-postal-service-api-audit-report.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="https://www.uspsoig.gov/sites/default/files/document-library-files/2017/IT-AR-17-006.pdf">The Office Of Inspector General for US Postal Service published an audit report on the federal agencies API strategy</a>, which has opened their eyes to the potential of API management, and the direct value it can bring to their customers, and their business. The USPS has some extremely high value APIs that are baked into ecommerce solutions around the country, and have even launched an API management solution recently, but until now have not been actively analyzing and using API usage to guide them in any of their business planning decisions.</p>

<p>According to the report, “The Postal Service captures customer API usage data and distributes it to stakeholders outside of the Web Tools team via spreadsheets every month. However, management is not using that data to plan for future API needs. This occurred because management did not agree on which group was responsible for reviewing and making decisions about captured usage data.” I’m sure this is common in other agencies, as APIs are often evolved within IT groups, that can have significant canyons between them and any business units. Data isn’t shared, unless a project specifically designates it to be shared, or leadership directs it, leaving real-time API management data out of reach of those business groups making decisions.</p>

<p>It is good to see another federal agency wake up to the potential of API management, and the awareness it can bring to business groups. It’s not just some technical implementation with logfiles, it is actual business intelligence that can be used to guide the agency forward, and help an agency better serve constituents (customers). The awareness introduced by doing APIs, and then properly managing APIs, analyzing usage, and building and understanding what is happening, is a journey. It’s a journey that not all federal agencies have even begun (sadly). It is important that other agencies follow USPS lead, because it is likely you are already gathering valuable data, and just passing it on to external partners like USPS has been doing, not capturing any of the value for yourself. Compounding the budget, and other business challenges you are already facing, when you could be using this data to make better informed decisions, or even more important, establishing new revenue streams from your valuable public sector resources.</p>

<p>While it may seem far fetched at the moment, but <a href="https://apievangelist.com/2017/05/05/taxation-on-public-data-via-the-api-management-layer/">this API management layer reflects the future of government revenue and tax base</a>. This is how companies in the private sector are generating revenue, and if commercial partners are building solutions on top of public sector data and other digital resources, <a href="http://apievangelist.com/2015/08/24/setting-a-precedent-when-charging-for-high-volume-access-to-government-apis/">these government agencies should be able to generate new revenue streams from these partnerships</a>. This is how government works with physical public resources, there should be no difference when it comes to digital public resources. We just haven’t reached the realization that this is the future of how we make sure government is funded, and has the resources it needs to not just compete in the digital world, but actually innovate as many of us hope it will. It will take many years for federal agencies to get to this point. This is why they need to get started on their API journey, and begin managing their data assets in an organized way as the USPS is beginning to do.</p>

<p>API management has been around for a decade. It isn’t some new concept, and their are plenty of open source solutions available for federal agencies to put to use. All the major cloud platforms have it baked into their operations, making it a commodity, alongside compute, storage, DNS, and the other building blocks of our digital worlds. I’ll be looking for other ways to influence government leadership to light the API fire within federal agencies like the Office of the Inspector General has done at the U.S. Postal Service. It is important that agencies be developing awareness, and making business decisions from the APIs they offer, just like they are doing from their web properties. Something that will set the stage for future for how the government serves its constituents, customers, and generates the revenue it needs to keep operating, and even possibly leading in the digital evolution of the public sector.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/12/the-us-postal-services-wakes-up-to-the-api-management-opportunity-in-new-audit/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/11/always-being-prepared-for-an-api-future-that-may-never-come/">Always Being Prepared For An API Future That May Not Come</a></h3>
        <span class="post-date">11 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/copper-servers.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m just coming out of a sprint for <a href="http://org.open.referral.adopta.agency/">my Human Services Data API (HSDA) work</a>. Throughout the process of gathering feedback across emails, Slack Channels, and Github Issues, and trying to decide where I should be steering this ship, I’m regularly reminded that I’m often preparing for a future that may never come. I’m working real hard to make my API design as future proof as possible, but I find that in many cases I risk leaving folks behind with some of my API design decisions. When it comes to the audience for this API, municipalities and nonprofit organizations, this concern was present with every decision I have been making.</p>

<p>As part of this latest evolution, I took hypermedia and GraphQL off the table, as both areas seem to confuse and muddy the conversation, not help. I was hoping that GraphQL might help some of the requests around query-ability of APIs, and the tendency to load up individual paths with numerous parameters, and enums. I tried to facilitate discussions around unique identifiers, moving things beyond just incremental integers, taking a cue from Twitter and other large providers, but many just deemed these conversation overkill, unnecessary, and bothersome. While I have learned a lot over the last seven years as the API Evangelist, I am regularly reminded that not everyone has been along for the ride, and I need to always bring things back to ground level, even if it means making some cringe-worthy API design choices.</p>

<p>I find as a technologist, I suffer from hopeless futurism. Even though I know better, I still tend to prefer looking at what is next, preparing as much as I can for the future, even at the expense of where we’ve been, and possibly leaving people behind. I can argue until I’m blue in the face regarding the benefits of hypermedia when it comes to supporting clients, and the benefits of GraphQL when it comes to giving API consumers a stronger voice when it comes to querying and getting access to EXACTLY the data they need, but without the proper groundwork, and education, my audience is rarely going to care. APIs are a journey, and I feel like APIs have to take their course, and folks have to be along for the ride. There is just no hurrying this process, no matter how much knowledge about the future I may possess, or how passionate and aggressive I am about why a particular API design decision will matter down the road.</p>

<p>I am working very hard to tame my tech bro futurism fetish, and better understand what the humans here on the ground in the present will need. I’m also working a lot harder to try and figure out how I can incorporate API lessons into my API design and definition work. How can I teach folks about headers, with specific design decisions I’ve made? How can I teach folks about how the client will break with a certain API design approach? As a technologist it is very hard to allow myself the space to make sub-standard API design decisions for the sake of helping an audience learn along the way. I want everything to be right, dammit! However, I’d much rather that my APIs actually get used, and the folks I’m targeting with my designs aren’t turned off by what I’ve delivered because they don’t see the value, or relevance. I’m working hard to not always be a tech boy scout, and always being prepared for a future that may not come.</p>

<p><strong>P.S.</strong> I am guessing that all the folks who keep saying I’m so anti-GraphQL will not recognize how much I’m incorporating it into my work, and storytelling, and that this is a POSITIVE story about how GraphQL might have helped. It also still highlighting my argument(s) around investment in API education, and not being aggressive when you are pushing GraphQL, as I’ve learned first hand and keep trying to share in my (aggressive) GraphQL posts.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/11/always-being-prepared-for-an-api-future-that-may-never-come/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/11/api-education-is-needed-but-rarely-prioritized-in-current-environment/">API Education Is Needed But Rarely Prioritized In The Current Environment</a></h3>
        <span class="post-date">11 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/facebook/facebook-blueprint-screenshot.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2017/08/31/your-lack-of-investment-in-api-education-will-be-the-end-of-your-api-service/">I wrote about this in a mean way during my rant week</a>, but I wanted to bring up the topic of education and training when it comes to APIs in a more constructive way this week. Amidst the regular requests I get for API architects, developers, product managers, and evangelists I am reminding many companies that they will often need to hire for these roles internally, training and grooming existing employees, as finding seasoned veterans in any of these areas will prove to be difficult. I wish I had my own API school, where I was helping train waves of qualified employees, but sadly most of the folks with existing skills are employed.</p>

<p>The challenge of investing in API training and education doesn’t stop with your immediate team, this is something that needs to occur in most cases company-wide. I’ve talk with several groups about developing internal workshops, and training, but I find most of them aren’t truly interested in the investment needed, and are often looking for some free content, or someone they can get to come and speak for free or very low pay. It shows me that many companies aren’t quite ready to make the investment it will take to ensure their staff are ready for the work that lies ahead, and don’t value making sure their workers have the skills they’ll need to be successful in the API-driven world we are finding ourselves in.</p>

<p>This isn’t something I’ve just encountered at SMB, SME, and the enterprise. Government agencies are always cashed strapped, under-resourced, and lacking in the skills needed for the next wave. This is also a problem I’m seeing across startups. I’ve had discussions with startup groups selling tools and services to the API space, who are hitting significant challenges once they start selling their solutions outside the mainstream tech ecosystem. Many folks at large companies, small businesses, and government agencies just don’t have some of the basics when it comes the web, and how modern approaches to APIs work. To become an active customer, they are going to need some investment to get their customers up to speed, something I find the investors behind startups are rarely keen on spending money on.</p>

<p>I am working on a workshop series for a health care group in October, and I’m working real hard to develop some structure to help make sure I cover the fundamentals of why APIs are important, beginning with the web and HTTP. I’m trying to show the space from not just the API provider perspective, but also from the API consumer view, because everyone should be both. I’m also working on more stories to help educate why companies, investors, institutions, and government agencies should be investing more into the area of API education. Not just understanding how to provide and consume them, but how to secure them, understand that they are driving everything mobile, and can help securely open up their operations to allow for assistant by 3rd party providers. Humans will be the number one challenge you face when it comes to doing APIs in your organization, so make sure you are investing wisely. I’d love to hear more about the challenges you are facing, or how you are finding success when it comes to educating your staff or customers about everything API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/11/api-education-is-needed-but-rarely-prioritized-in-current-environment/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/11/making-sure-definitions-in-openapi-are-robust-for-use-in-schema-validation/">Making Sure Definitions In OpenAPI Are Robust For Use In Schema Validation</a></h3>
        <span class="post-date">11 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/hsda-schema.png" align="right" width="40%" style="padding: 15p;" /></p>
<p>I’m working on v1.2 of <a href="http://org.open.referral.adopta.agency/">my Human Sevices Data API (HSDA)</a>, and with this wave of work I’m making sue there is a functional API for validating all JSON that gets posted as the body in requests, as well as when it gets returned as part of API responses. To drive my validator I’m using JSON schema, which I already have defined as part of <a href="https://github.com/human-services/portal/tree/master/_data/api-commons">the OpenAPI definition for the project</a>. I want to reuse, and build on top of this work, but I found the definitions for my OpenAPI to be pretty deficient in much of the details I am needing to validate the request and response bodies of my HSDA APIs.</p>

<p>The process has showed me the importance of making sure the definitions portion of my OpenAPIs are as robust as I can. Possessing required, default, regex patterns, and other details I’m going to need to make sure my schema validator is as robust as possible. I’m entering the phase of this project where vendors, and implementors are looking for guidance on whether or not their schema are HSDS/A compliant, and they are supporting the fields necessary to get a stamp of approval. The schema validator is essential to this, but the new validation API I’ve created is only as good as the JSON schema that I’m using as part of its engine.</p>

<p>I come across a number of OpenAPIs in the wild which do not possess schema definitions, and references for each API. These API providers are only describing enough of the surface area of their API to be able to generate API documentation using Swagger UI. This is something I’ve also been guilty of in the past, where I would only define the surface area of the API, just to get what I needed for my API discovery needs. Over the last year, I’ve spent more time making sure the definitions portion of the OpenAPI is also present, but it isn’t until now that I’ve been making sure the fine details of the schema are present. I need this to be in place for validation, which will be used across monitoring, testing, and other stops along the API life cycle I will be delivering as part of this work.</p>

<p>Honestly, my JSON schema chops were not up to snuff for this work, something I’ve struggled with making the time for this summer. However, I feel like I’m finally getting there. I’m beyond the basics of JSON schema validation, and have my simple API validator API in place. I just need to make sure I’m always investing the time required to develop robust JSON schema for all my APIs, so that the validator provides rich responses with each API schema validation. I’m thinking I will be build a tool for helping identify what is lacking with the definitions in my OpenAPIs, pointing out the common things I’m lacking, and running this before I ever consider actually validating the schema that are used in the API request body, or the API response body for the projects I’m working on. Always making sure the schema, and API definitions are harmonized, and speaking the same language is essential to all of this human services API effort to work properly.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/11/making-sure-definitions-in-openapi-are-robust-for-use-in-schema-validation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/11/version-1-2-of-human-services-data-api/">Version 1.2 Draft Of The Human Services Data API</a></h3>
        <span class="post-date">11 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/hsda-v1-2.png" align="right" width="40%" style="padding: 15p;" /></p>
<p>I have been working on the next version of the Human Services Data API (HSDA) OpenAPI lately, taking all the comments from the Github repository, and pushing forward the specification as far as I can with the minor v1.2 release. <a href="https://github.com/openreferral/api-specification/issues?q=is%3Aissue+is%3Aopen+label%3Av1.2">I have the Github issues organized by v1.2</a>, and have invested time moving forward the OpenAPI for the project, as well as my demo site for the effort.</p>

<p>With this release I am focusing on six main areas, based upon feedback from the group, and what makes sense to move forward without any non-breaking changes:</p>

<ul>
  <li><a href="https://github.com/openreferral/api-specification/issues/45">/complete</a> - add an /everything to each core resource, allowing access to all sub resouces.</li>
  <li><a href="query">query</a> - Shifting query parameter to be array, allowing for multiple fields to be queried.</li>
  <li><a href="https://github.com/openreferral/api-specification/issues/39">content negotiation</a> - Allow for JSON, XML, and responses.</li>
  <li><a href="https://github.com/openreferral/api-specification/issues/12">sorting</a> - Adding sorting.</li>
  <li><a href="https://github.com/openreferral/api-specification/issues/10">pagination</a> - Adding pagination.</li>
  <li><a href="https://github.com/openreferral/api-specification/issues/48">status codes</a> - Add more status codes.</li>
</ul>

<p>These were main concerns regarding what was missing from the last release, and were the top items that made sense to push forward this round. I’ve made some other major shifts to the project, but before I go through those, I wanted to provide some more insight into these v1.2 changes to the core HSDA specification. Helping shed some light on why I did what I did, while I am looking to make the API interface as usable as possible for HSDA implementations, vendors servicing the space, as well as developers looking to build web, mobile, voice, and other applications on top of any APIs that support the implementation.</p>

<p><strong>Complete</strong>
Getting access to the entire surface area of the core resources (organizations, locations, and services), as well as all the subresources (phones, physical address, mailing address, etc.) was the most voiced request from v1.1. I had laid out several options to access the entire surface are of HSDA resources, but folks seem focused on a single set of API paths to accomplish what they needed from a vendor and implementation perspective. It is my job to keep the API serving all types of integrations and use cases, but definitely couldn’t ignore providing a single set of paths for GET, POST, and PUT of organizations, locations, and services.</p>

<p>It was important to me to keep the core resources accessible as a flat, one dimensional, and machine readable documentation, so API consumers could quickly import into spreadsheets as CSV, or make lists of addresses, or possible lookups and updates to a single phone number. I didn’t want to abandon these use cases, or introduce breaking changes, so I introduced a /complete path for all three of the core sources (organizations, locations, and services. These paths allow for GET, POST, and PUT requests of multi-dimensional JSON objects, access the core, as well as sub-resources for any data stored within the API. These paths should accommodate the heavy system to system vendor and implementation usage that was voiced as part of the feedback process, while still preserving other individual use cases.</p>

<p><strong>Query</strong>
With v1.1 the query parameter for making API requests on organizations, locations, and services was simply a string, which you could provide a set of strings for. At the request of the community we’ve made this an array, allowing you to specify multiple fields and values as part of your query. To ensure I didn’t introduce a breaking change, I did not alter the existing query parameter, instead I added a new parameter called queries, which allows you to get more detailed with your queries. Now there is a simple search, and more robust multi-field search, giving more control to API consumers. In the future we will explore more query power, but this might reside in the search API portion of this conversation which we’ll address later.</p>

<p><strong>Content Negotiation</strong>
The Human Services Data Specification (HSDS) is a CSV format. While HSDA has focused on using JSON as part of all API requests and responses, I made sure that v1.1 stayed true to the original HSDS format, making the entire surface area accessible via simple API paths, and returning one dimensional responses that can still be returned as CSV. So with v1.2 I allowed for the negotiation of either CSV, XML, or JSON content types for all the primary HSDA paths. The /complete paths do not support CSV, as they provide access to resources, and sub-resources that cannot be returned as CSV, but the rest of the surface allows API consumers to negotiate the format they desire.</p>

<p>Keeping the surface area of the entire HSDS format accessible via simple API paths, without authentication, and providing the option of returning data in CSV, opens up the API to be used to export to Excel and Google docs in a single step. This will open up the ability to extract core resources like organizations, locations, services, as well as sub-resources such as phone and address lists into CSV format, which can then easily be used by a much wider audience than just developers, and other common API consumers.</p>

<p><strong>Sorting</strong>
There was no ability to sort any data within an HSDA responses with previous version. With version 1.2 you can now provide a sort_by parameter which determines the field to sort by, and order, which determines whether to sort by ascending (asc), or descending (desc) order. I had in there to add the ability to sort by what has been changed recently, but I have run out of time, and will make sure it gets into future releases. It was important that we at least get basic sorting features in there for this release, and can add more aspects to this dimension in the near future.</p>

<p><strong>Pagination</strong>
In previous versions of HSDA you could pass in a parameter to specify which page to return, and per_page to determine how many results to return per page. However, there was no data return telling you which page you were on, the count per page, or anything else about what is next or previous as part of each results. Several solutions to this were presented as part of the feedback process for v1.0 and v1.1, but not much feedback was given on the subject. Again, I was looking to introduce this feature without any breaking changes. With the flat, one dimensional array structure of the HSDA response structure it would be difficult to add in any envelope, or collection for returning pagination data, so I set out to find examples of how it can be done without disrupting the current response structure.</p>

<p>After looking at Github an a handful of other approaches I opted to add a customer header called x-pagination which provides a JSON object containing total_pages, first_page, last_page, previous_page, and next_page to each GET response, allowing consumers to easily navigation the pagination for large API responses. This approach does not introduce any breaking changes, while still providing all the data needed by API consumers to navigate the surface area of any HSDA implementation, across organizations, locations, and services. I do have some concerns about developers being HTTP header aware, and know how to access headers, but it is something that with a little bit of education, can open a whole new world to them–something any API developer should have in their toolbox.</p>

<p><strong>Status Codes</strong>
One area that HSDA v1.0 and 1.1 were deficient in was when it came to HTTP status code guidance. I had this slated for v1.3, but I was needing to know when I hit an error in some of the validation, documentation, and other tooling  had been working on. So I took this opportunity to add 403, and 500 HTTP status codes to all API responses. All the GET paths are publicly available, but with this edition I’ve introduced an API management service, allowing me to secure all POST, PUT, and DELETE paths, opening them up to multiple users in a secure way. I didn’t want all other users to simply get a 404, so I added 403 guidance. I will be adding more specific HTTP status code guidance, and error response schema in future versions.</p>

<p><strong>Additional Services</strong>
That was the majority of features involved with the v1.2 release. However there were other aspects of HSDA that were left out of v1.1, like meta, search, and taxonomy. Also, as part of the v1.1 feedback process there were other features thrown out that were needed as part of future releases. All of this had the potential to add unnecessary complexity to the core set of resources, making the specification bloated, making things even more complex than it already is. To help alleviate these challenges I’ve started breaking future APIs into separate projects, or services. Here are the additional seven services I’ve added:</p>

<ul>
  <li><strong>HSDA Search</strong> - A service dedicated to search across HSDA implementations.</li>
  <li><strong>HSDA Bulk</strong> - A service dedicated to managing bulk operations across HSDA implementations.</li>
  <li><strong>HSDA Taxonomy</strong> - A service dedicated to working with taxonomy across HSDA implementations.</li>
  <li><strong>HSDA Orchestration</strong> - A service dedicated to handling orchestration, evented infrastructure, and webhooks across HSDA implementations.</li>
  <li><strong>HSDA Meta</strong> - A service dedicated to handling meta data and logging across HSDA implementations.</li>
  <li><strong>HSDA Management</strong> - A service dedicated to introducing an API management across HSDA implementations.</li>
  <li><strong>HSDA Utility</strong> - A service dedicated to housing all utility APIs across HSDA implementations.</li>
</ul>

<p>All of these services are meant to augment and complement the core set of HSDA resources, and sub-resources without adding unnecessary complexity to them. These projects are meant to act as a buffet of services that human service providers can choose from, or they can opt to just stick with the basic. I’ve reset the version for each of these projects to v1.0, and will be moving them forward at their own pace, independent of what the core HSDA specification is doing. I’ve introduced separate OpenAPI definitions for each project, and I am pushing forward independent code repositories for delivering PHP/MySQL implementations for each service area. Like the other features for HSDA v1.2 above, I wanted to take a moment and explain the logic behind each of these services.</p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/hsda-organizations-swagger-ui.png" align="right" width="40%" style="padding: 15p;" /></p>
<p><strong>HSDA Search</strong>
Early on in the release of v1.1 the question of search quickly began to muddy the conversation. I saw this was going to be a challenge. I saw the communities desire to deliver features via query parameters, and knew that search was going to introduce a number of parameters beyond what was needed to just manage core resources (organizations, locations, and services). We also started getting some great feedback from vendors in the Github issues for search features that went well beyond what was needed for individual resources, and often spanned all the core resources. Search had been a separate path in the core HSDA specification, and with this version I decided to break it off the core specification and put it into it’s own project, where it can become a first class citizen.</p>

<p>The current HSDA search v1.0 specification got all query, queries, sorting, and pagination that the core HSDA resource received in the v1.2 release, but once they were added it got broken off into it’s own service. Technically this is a breaking change, but I think it is one the community will support, as it will add some valuable search features. Immediately, I added a set of collections that spanned all core resources, including organizations, locations, and services, and I took the /complete features I had just added to HSDA and introduced them to HSDA search. This makes all the search results as rich, and complete as possible, providing access to the entire surface area. I feel like ultimately this is where we will be experimenting with allowing consumers to restrict, or expand the results they are looking to get back, while keeping core resources cachable, available at known paths.</p>

<p><strong>HSDA Bulk</strong>
One of the conversations that led to the introduction of /complete paths for all core resources in v1.2 was the needs of vendors, and the heavy lifting requirements of individual implementations. There was the need to load large volumes of data into systems, as well as between disparate systems. There was talk about the load this causes of systems that drive critical websites and other infrastructure, resulting it being something done during off-peak hours. Some of this work can be done via the primary HSDA /complete service, but it was clear that a separate bulk set of API would be needed to help handle the load, and meet the unique needs of system integrators, that were different than what web, mobile, voice, and other applications developers would be building.</p>

<p>HSDA Bulk reflects the HSDA /complete paths for organizations, locations, and services, but these paths accept the posting of arrays of objects, complete with all their sub-object. Instead of directly loading these into the main database upon POSTing, they are entered individually into a jobs database, where they can be queried, and run independently on a schedule, or based upon specific events. HSDA bulk will work in conjunction with HSDA, HSDA Meta, HSDA Management, and HSDA orchestration, or it can be run independently, based upon custom criteria. The goal is to provide a way to handle the bulk needs of HSDA implementations which can be deployed, scaled, and operated independently of any core HSDA implementation, limiting the impact on core websites, mobile applications, and other applications.</p>

<p><strong>HSDA Taxonomy</strong>
The taxonomy portion of HSDA got separated as part of the v1.1 release. I quickly saw it needed more thinking regarding the handling of multiple taxonomies, as well as allowing for the accessing of services beyond core HSDA resource management, or even search. HSDA Taxonomy is now it’s own project, and can be deployed independently of any HSDA implementation, but provide another doorway for querying, browsing and getting at services based upon any supported taxonomy. The v1.0 version of HSDA Taxonomy will support AIRS and Open Eligibility, but will be designed to support any other taxonomy, and allow for customization by individual implementations, while maintaining a common API for usage across multiple HSDA implementations.</p>

<p><strong>HSDA Orchestration</strong>
Throughout HSDA v1.1 discussion I kept hearing about the need for notifying users of changes to HSDA data, and the need to push and ping external systems with information. As part of the build up to v1.2 I conducted a significant amount of research into the event and webhook implementations of leading API providers like Box, Twilio, Stripe, and others. I’ve taken this research and created a v1.0 draft for an HSDA Orchestration solution. Working to alleviate a wide variety of needs for handling events that occur across HSDA implementations, engaging with external implementations, and making HSDA a two-way street.</p>

<p>HSDA Orchestration will potentially work with HSDA Meta, HSDA Bulk, HSDA Management, and of course, HSDA core to bring implementations alive. A number of events will be defined around common HSDA task that occur like POSTing of new organizations, or locations, or updating of individual records, or possibly the submission of bulk updates that need running. Every API call within an HSDA implementation can now be tracked using HSDA Meta, and HSDA Orchestration will monitor this, and allow API consumers to subscribe to these events via webhooks, and receive a ping when event occurs, or receive a fat ping, which pushes data associated with an event to an external URL. HSDA orchestration will handle all the monitoring, tracking, notification, and syncing needed between HSDA implementations via a separate, independent service that works with the HSDA stack.</p>

<p><strong>HSDA Meta</strong>
HSDA Meta is another feature that got set aside with the v1.1 release. With v1.2 I’ve set it up as it’s own project. Now each API call made to any core HSDA path will be added to the HSDA Meta system, recording the service, path, verb, parameters, and body of each request. HSDA Meta is designed to providing a logging solution, eventually a transactional layer that can be rolled forward or backwards, and is intended as stated before to work with HSDA Bulk, HSDA Orchestration, and leveraging HSDA Management for access, and auditability of all activity.</p>

<p><strong>HSDA Management</strong>
HSDA is in need of an API management layer. Many of the paths available allow for reading, writing, and deleting of data. The original HSDA v1.0 and v1.1 only allowed for a single administrative key for accessing all POST, PUT, and DELETE API paths. With the v1.2 release I’ve begun this separate project for allowing the adding, authenticating, and managing of API users who are looking to get at HSDA data. The current implementation allows for many users, and each users to have access to one, or many of the services, and supporting API paths. It is up to each implementation to decide which users get access to which. In future releases we will add the notion of access plans, allowing for trusted groups to be established, including partners, and internal consumers. The goal with this is to identify a common interface for HSDA implementations, which behind the scenes could be any number of existing API management implementations.</p>

<p><strong>HSDA Utility</strong>
Last, I needed a place to put any utility APIs I needed to help manage HSDA implementations. Right now there are two core set of APIs. One for managing which services are available across and HSDA implementation, and another for validating HSDA schema, and eventually the APIs themselves. I will be putting any other utility API within this service area. It will become the catch-all for any API that doesn’t fit into it’s service area.</p>

<p><strong>HSDA Specification</strong>
That is it. That is the bulk of the work I’ve done for the v1.2 release of HSDA. I’m pretty happy with how things have worked out. I feel there is a lot more coherency across the specification now, and the service mindset will allow for much more constructive conversations across the projects. <a href="https://openreferral.github.io/api-specification/hsda/">I have updated the HSDA specification site with all eight of the OpenAPIs, publishing a separate documentation page for each one.</a> Each page provides an HTML view for each service, as well as link to the YAML version of the OpenAPI, the demo website, as well as Github Issues for each project. Next step is to drive the feedback and comments via the Github issues, include anything that is missing, and push v1.2 out the door, and begin working on v1.3, as well as the v1.1 for the seven other projects that were added with this release.</p>

<p><strong>HSDA Implementation</strong>
I do not ever feel an OpenAPI is ready for prime time until I have a working version of it. <a href="http://developer.open.referral.adopta.agency/">I have created working versions of all eight HSDA implementations.</a> The core HSDA is the most complete and robust, with HSDA Search, HSDA Bulk, HSDA Meta, HSDA Management, HSDA Utility, HSDA Taxonomy, and HSDA Orchestration following up in that order. They are v1.0 draft implementations, and for the most part are working, but have not been hardened yet. I would feel comfortable putting HSDA, and HSDA Management, and HSDA Meta into a real world implementation in coming weeks, something I will actually be doing with two separate implementations–using real world projects to harden them.</p>

<p>I have updated the HSDA demo portal to contain all eight projects, and I have leveraged Github authentication as the HSDA Management layer, allowing anyone to signup and use their Github account to access with each API. Each API call is logged, and I can easily revoke access to any account, or push reset on the demo as needed. Now that I have a working copy, I will be publishing a development version of the portal, so that I do not break the demo in the future, and can move forward with releases a little more gracefully than I did with this one. I will be maturing all eight implementations, and offering them up as official Adopta.Agency products for deployment on AWS in the near future.</p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/schema-org-api-definitions.png" align="right" width="40%" style="padding: 15p;" /></p>
<p><strong>Looking To The Future</strong>
This release of HSDA and the supporting code is all about looking towards the future. I’ve separated things out into independent services to handle what is next, and I’ve re-engineered my PHP/MySQL implementations to prepare for the future. Each of the eight solutions are 100% OpenAPI driven. The database and server side code is all OpenAPI driven. The portal, documentation, and the schema validation is all OpenAPI driven. Next, I’m setting up monitoring, and testing, that will all be OpenAPI driven.</p>

<p>I also have two other services I did not include in this story because they are meant for the future. One is HSDA Custom, which allows for the addition of any field, or collection to the core HSDA implementation, accommodating the needs of individual providers. This is only possible because of OpenAPI, and each custom field will be added as x-[field], keeping things validating. The second one I’m calling HSDA Aggregation, which will be my first attempt to sync, aggregate, and migrate data across many HSDA implementations. Now that I have the base, I’m going to setup five separate demo implementations, and begin to work on robust sets of test data, which I can use to push forward an aggregate and federated version of HSDA.</p>

<p>The OpenAPI core for my HSDA work has allowed me to do some interesting things with how the APIs are delivered, as well as many of the supporting tooling. This approach to delivering HSDA implementations can be applied to any API. I will be taking <a href="http://schema.org.apis.apievangelist.com/">my list of several hundred Schema.org OpenAPIs</a>, and building a catalog of API definitions that can be easily deployed on AWS. I’m not going to do this as an automated software service, but I will be hand deploying solutions for clients using this approach. Providing streamlined, well-defined, yet hand-crafted API implementations for any possible situation. This was born out of hearing from HSDA providers about how they begin storing all types of data into the organizations, locations, and service data stores–things that really should be in a separate system. Eventually I’ll be suggesting other HSDA projects that assist providers with events, messaging, and other common solutions beyond just organizations, locations, and services.</p>

<p>Anyways, that concludes this sprint. I will be doing more work throughout the week, and we have a three day hackathon this week. So I’m looking forward to moving things forward more, but for right now I’m pretty glad with what I’ve achieved.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/11/version-1-2-of-human-services-data-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/08/when-i-see-the-landscape-of-api-tooling-i-see-future-of-technical-debt/">When I Look At The Landscape Of API Services & Tooling I See The Future Of Technical Debt</a></h3>
        <span class="post-date">08 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/servers-hallway-door.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>There are a number of API service and tooling providers that I still get excited about in the space. 3Scale, Restlet, Runscope, and Tyk - to begin with my sponsors! ;-) ;-) ;-) However, there are others like Postman, APIMATIC, Materia, OAuth.io, Stoplight, Apicurio, API Platform, API Umbrella, Github, API Science, and others that keep me thinking good thoughts about the things that API service providers are doing. However, I also see a lot of services and tooling that are simply playing the startup game, and have more to do with investment, then they do about APIs.</p>

<p>It is these services and tools I see as the next generation of technical debt. When you bundle the vendors who are usually chasing trends as part of their investment and exit strategy, and really don’t care about truly helping you solve your technical, and business challenges, with your existing problems, you are just multiplying your problems. These types of customers only want you as an active customer, preferably locked into a contract, with their services and tools baked into your operations. You know what all of this leads to? Technical debt. When you buy into the vendor stories, and jump on trends, without thinking through the consequences of your actions, and the long term effects on your road map, you end up with a significant amount of technical debt down the road.</p>

<p>I have taken a number of IT and developer leadership positions in my career, where I had to come in and clean up the mess from the previous guy (always guys). Nobody was questioning the decisions being made, and allowed someone to make purchasing, and technology decisions that ended up just taking things in a bad direction. That vendor we bought into was acquired, and now that tool we depend on is part of a larger enterprise suite we really don’t need, but because we can’t unwind it from our systems, we are forced to keep paying the subscription. We went for that trendy to way of doing things, decoupling, automating, assembling a framework, offshoring, outsourcing, and whatever came along with the current technological season, and investment cycle. We didn’t invest in internal capacity, or leveraging the web and standards, and now we are locked into this proprietary way of getting things done.</p>

<p>Ok, I get it. It is hard to see what is a trend, and which vendors are full of shit. I mean, they took us out to lunch, and were real nice guys. Right? They spoke all the buzz words, and seemed to really get the problems we faced keeping things up and running. I’ve made many bad decisions when it came to leading the IT or developer charge (I used to program in ColdFusion), but most of the time these were decisions that were handed to me, not ones that I made on my own (1/3 were my bad decisions ;-). These experiences have made me very skeptical about which technology I invest in, and the world of APIs has taken this to new levels for me–I trust nobody. This is the default stance I take now. I won’t adopt something new, or change the way I do things, if there is no way to easily recover, evolve, or mitigate form the decision. While the majority of these lessons have come from unreliable APIs, I still see many folks doubling down on API services and tooling that is going to burn them down the road. I just don’t think some of us are being honest with ourselves about how all this technical debt occurs in the first place, and somehow it is all just inevitable.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/08/when-i-see-the-landscape-of-api-tooling-i-see-future-of-technical-debt/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/08/i-wish-i-had-time-to-tell-that-story/">I Wish I Had Time To Tell That API Story</a></h3>
        <span class="post-date">08 Sep 2017</span>
        <p><a href="https://en.wikipedia.org/wiki/Storytelling"><img src="https://s3.amazonaws.com/kinlane-productions/photos/Millais_Boyhood_of_Raleigh.jpg" align="right" style="padding: 15px;" /></a></p>
<p>If you have followed my work in the API space you know that I consider myself an API storyteller before I ever would an API evangelist, architect, or the other skills I bring to the table. Telling stories about what folks are up to in the space is the most important thing to me, and I feel it is the most common thing people stumble across, and end up associating with my brand. You hear me talk regularly about how important stories are, and how all of this API thing is only a thing, because of stories. Really, telling stories is the most important you should be doing if you are an API provider or API service provider, and something you need to be prioritizing.</p>

<p>I was talking with a friend, and client the other day about their API operations, and after they told me a great story about the impact their APIs were making I said, “you should tell that story”! Which they responded, “I wish I had time to tell that story, but I don’t. My boss doesn’t prioritize me spending time on telling stories about what we are doing.” ;-( It just broke my heart. I get really, really busy during the week with phone calls, social media, and other project related activity. However, I always will stop what I’m doing and write 3-5 blog posts for API Evangelist about what I’m doing, and what I’m seeing. I know many of the stories are mundane and probably pretty boring, but they are exercise for me, of my ideas, my words, and how I communicate with other people.</p>

<p>The way that enterprise groups and startups operate is something I’m very familiar with. I’ve been scolded by many bosses, and told not read or write on my blog. This is one of the reasons I don’t work in government anymore, or in the enterprise, as it would KILL ME to not be able to tell stories. I need storytelling to do what I do. To work through ideas. It is how I learn from others. Why would I want to do something that I can’t tell others about? Why would I not prioritize the cool things my clients are doing with my APIs? Sure, there are some classified, and sensitive situations where you definitely would not, but most of the reasons I hear for not telling stories publicly about the cool things you are doing are complete bullshit. I’m sorry, but they are. Even if you have to package it as a white paper or case study, you should be putting this down for others to learn from.</p>

<p>When you find yourself telling your creative side (or me), that you wish you had time to tell that story, you should consider that a canary in the coal mine. A sign that there is other illnesses going on. Sure, once or twice is fine, but if this becomes a sustained thing, or worse–you stop wanting to tell stories at all, then you should be looking for a new gig. You just had your mojo killed. Nobody deserves that. No employer should kills their employees storytelling mojo. Even if you are all business, telling stories is essential to making things work. Press releases are stories. Ok, they are usually a very sad, pathetic story, but they are a story. Your company blog should be active. Your personal blog should be active. Go check out your personal blog, when was the last time you wrote something you were passionate about? If it was more than a year ago, your employer has put you in a box, and is looking to keep you there.</p>

<p><strong>Photo Credit:</strong> The Boyhood of Raleigh by Sir John Everett Millais, oil on canvas, 1870. A seafarer tells the young Sir Walter Raleigh and his brother the story of what happened out at sea, from <a href="https://en.wikipedia.org/wiki/Storytelling">the Wikipedia entry for storytelling</a>.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/08/i-wish-i-had-time-to-tell-that-story/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/08/responding-to-a-webhook/">Responding To A Webhook</a></h3>
        <span class="post-date">08 Sep 2017</span>
        <p><a href="https://stripe.com/docs/webhooks"><img src="https://s3.amazonaws.com/kinlane-productions/stripe/stripe-using-webhooks.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>There are many details of doing APIs you don’t think about until you either a) gain the experience from doing APIs, or b) learn from the API providers already in the space. When you are just getting going with your API efforts you pretty much have to rely on b), unless you have the resources to hire a team with existing API experience. Which many of my readers will not have the luxury to do, so they need as much helping learning from the pioneers who came first, wherever they can.</p>

<p>One of the API pioneers you should be learning from is the payment API provider Stripe. <a href="https://stripe.com/docs/webhooks">I’ve been studying their approach to webhooks lately</a>, and I’ve managed to extract a number of interesting nuggets I will be sharing in separate blog posts. Today’s topic is responding to a webhook, which Stripe provides the following guidance:</p>

<p><em>To acknowledge receipt of a webhook, your endpoint should return a 2xx HTTP status code. Any other information returned in the request headers or request body is ignored. All response codes outside this range, including 3xx codes, will indicate to Stripe that you did not receive the webhook. This does mean that a URL redirection or a “Not Modified” response will be treated as a failure.</em></p>

<p>To be honest, I had never thought I should be responding to the webhooks I’ve setup. I treated them like a UDP request and once they went out the door and I processed, I didn’t need to response at all. How rude! I hadn’t seen any of my existing API providers offer up guidance in this area, or more likely I never noticed it. This is one of the reasons I like going though API providers documentation when I’m not integrating with them, because I tend to have a different eye for what is going on.</p>

<p>Anyways, I’m adding webhook responses to my list of building blocks for <a href="http://webhooks.apievangelist.com/">my webhook research</a>, and will be including it in future guidance. It seems like a pretty significant thing to help API consumers deliver to complete the webhook loop, and it injects more HTTP status code awareness and literacy into the conversation, which I think is always a good thing for everyone. Thinking about how we are signaling back and forth in the API game is always an important part of the equation.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/08/responding-to-a-webhook/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/08/cloud-marketplace-becoming-the-new-wholesale-api-discovery-platform/">Cloud Marketplace Becoming The New Wholesale API Discovery Platform</a></h3>
        <span class="post-date">08 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/aws/aws-marketplace.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m keeping an eye on <a href="https://aws.amazon.com/marketplace/">the AWS Marketplace</a>, as well as what Azure and Google are up to, looking for growing signs of anything API. I’d have to say that, while Azure is in close second, that AWS is growing faster when it comes to the availability of APIs in their marketplace. What I find interesting about this growth is it isn’t just about the cloud, it is about wholesale APIs, and as it grows it quickly becomes about API discovery as well.</p>

<p>The API conversation on AWS Marketplace has for a while been dominated by API service providers, and specifically the API management providers who have pioneered the space:</p>

<ul>
  <li><a href="https://aws.amazon.com/marketplace/pp/B00QHIY9OW?qid=1504806402998&amp;sr=0-7&amp;ref_=srh_res_product_title">3Scale</a></li>
  <li><a href="https://aws.amazon.com/marketplace/pp/B00VGV1HYA?qid=1504806402998&amp;sr=0-9&amp;ref_=srh_res_product_title">CA</a></li>
  <li><a href="https://aws.amazon.com/marketplace/pp/B00PD1KTHU?qid=1504806443214&amp;sr=0-16&amp;ref_=srh_res_product_title">WSO2</a></li>
  <li><a href="https://aws.amazon.com/marketplace/pp/B01C4JFD4W?qid=1504806443214&amp;sr=0-13&amp;ref_=srh_res_product_title">Akana</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=12b4567a-9d6f-4aa9-8ac1-ec81dfc6f65a">Strong Loop</a></li>
</ul>

<p>After management, we see some of the familiar faces from the API space doing API aggregation, database to API deployment, security, integration platform as a service (iPaaS), real time, logging, authentication, and monitoring with Runscope.</p>

<ul>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=94ee7aa2-9b70-4257-a216-cd84b15360b6">Cloud Elements</a> (Aggregation)</li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=a80f81b0-f5a7-4986-8518-4c7983839df2">SlashDB</a> (Database)</li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=be64f318-a43d-4680-b616-70cdba28048f">Runscope</a> (Monitoring)</li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=eaae8e10-662d-4b01-8cf0-ce911795cefby">Zapier</a> (iPaaS)</li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=bb1ddfd2-7171-4f3f-aaf5-483a076a15df">Peach API Security</a> (Security)</li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=1bbca7b6-ff68-4779-abe0-45a796ff298d">Streamdata</a> (Real Time)</li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=40b06c9b-6c89-4150-8e76-00a57426f8fe">Auth0</a> (Authentication)</li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=4d82d3d5-c3bb-48f5-964c-f370adb08482">Okta</a> (Authentication)</li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=0c9f4b82-f870-4972-b027-9164cc100097">LogEntries</a> (Logging)</li>
</ul>

<p>All rounding off the API lifecycle, providing a growing number of tools that API provides can deploy into their existing AWS infrastructure to help manage API operations. This is how API providers should be operating, offering retail SaaS versions of their APIs, but also cloud deployable, wholesale versions of their offerings that run in any cloud, not just AWS.</p>

<p>The portion of this aspect of API operations that is capturing my attention is the individual API providers are moving to offer their API up via AWS marketplace, moving things beyond just API service providers selling their tools to the space. Most notably are the API rockstars from the space:</p>

<ul>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=cbdb3b66-b0ad-49f9-8e00-7cfed4f5dd6c">Stripe</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=818fd455-b47c-4a62-9e3d-1a346e0f5e0e">Twilio</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=a9e53967-0ebd-413a-b857-482097546806">Sendgrid</a></li>
</ul>

<p>After these well known API providers there are a handful of other companies offering up wholesale editions of their APIs, so that potential customers can bake into their existing infrastructure, alongside their own APIs, or possibly other 3rd party APIs.</p>

<ul>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=75a7e002-c13e-48fb-94b3-a05331a41ade">Pitney Bowes</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=3581ba71-392f-4668-8579-f3e5d1418635">Docomo</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=6e712093-4d32-4341-8119-9e9ddfaa4b7c">OpenShot</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=182a3530-4b66-470c-aaa0-8eea59631d17">Valtira</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=1e61d40e-23ef-40de-ba11-62ba339cba9b">Twinword</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=1e8de6c6-dff1-45d0-886f-3c24f27214c1">PokitDok</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=a6112a2b-f363-47d8-9d13-5cd90c3f082e">Segment.io</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=11eb064d-0d54-48ff-8348-787004609401">BigML</a></li>
  <li><a href="https://aws.amazon.com/marketplace/seller-profile?id=6b589746-c296-48fd-8509-63eb18c81f88">Diffbot</a></li>
</ul>

<p>These APIs are offering a variety of services but real quick I noticed location, machine learning, video editing, PDFs, health care, payments, sms, and other API driven solutions. It is a pretty impressive start to what I see as the future of API discovery and deployment, as well as any other stop along the lifecycle with all the API service providers offering their warez in the marketplace.</p>

<p>I’m going to setup a monitoring script to alert me of any new API focused additions to the AWS marketplace, using of course, the <a href="https://aws.amazon.com/documentation/marketplace/">AWS Marketplace API</a>. I’ve seen enough growth here to warrant the extra work, and added monitoring channel. I’m feeling like this will grow beyond my earlier thoughts about wholesale API deployment, and potentially pushing forward the API discovery conversation, and changing how we will be finding the APIs we use across our infrastructure. I will also keep an eye on Azure and Google in this area, as well as startup players like <a href="http://algorithmia.com">Algorithmia</a> who are specializing in areas like machine learning and artificial intelligence.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/08/cloud-marketplace-becoming-the-new-wholesale-api-discovery-platform/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/07/automatically-generating-openapi-from-a-yaml-dataset-using-jekyll/">Automatically Generating OpenAPI From A YAML Dataset Using Jekyll</a></h3>
        <span class="post-date">07 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/openapi-dynamic.png" align="right" width="35%" style="padding: 15px;" /></p>
<p>I was brainstorming with Shelby Switzer (<a href="https://twitter.com/switzerly">@switzerly</a>) yesterday around potential projects for upcoming events we are attending, looking for interesting ideas we can push forward, and one of the ideas we settled in on, was automatically generating OpenAPIs from any open data set. We aren’t just looking for some code to do this, we are looking for a forkable, reusable way of doing this that anyone could potentially put to work making open data more accessible. It’s an interesting idea that I think could have legs, and compliment some of the existing projects I’m tackling, and would help folks make their open data more usable.</p>

<p>To develop a proof of concept I took one of my existing projects for <a href="http://api.integration.tool.apievangelist.com/">publishing an API integration page within the developer portal of API providers</a>, and replaced the hand crafted OpenAPI with a dynamic one. <a href="https://github.com/api-evangelist-tools/api-integration/blob/master/_data/integrations.yaml">The project is driven from a single YAML data file</a>, which I manage and publish using Google Sheets, and already had a static API and OpenAPI documentation, making it a perfect proof of concept. As I said, the OpenAPI is currently static YAML, so I got to work making it dynamically driven from the YAML data store. The integrations.yaml data store has eight fields, which I hd published as four separate API paths, depending on which category each entry is in. I was able to assemble the OpenAPI using a handful of variables already in the config.yaml for the project, but the rest I was able to generate by mounting the integrations.yaml, dynamically identifying the fields and the field types, and then generating the API paths, and schema definitions needed in the OpenAPI.</p>

<p>It’s totally hacky at the moment, and just a proof of concept, but it works. I’m using <a href="https://github.com/api-evangelist-tools/api-integration/blob/master/apis/openapi.yaml">the dynamically generated OpenAPI</a> to drive <a href="http://api.integration.tool.apievangelist.com/documentation/">the Swagger UI documentation</a> on the project. I’m not sure why I hadn’t thought of this before, but this is why I spend time hanging with smart folks like Shelby, who ask good questions, and are curious about pushing forward concepts like this. Liquid, the language used by Jekyll to deliver HTML in Github driven project like this is very limiting, providing some serious constraints when it comes to delivering tools like this. As I get stronger in my knowledge of it, and push the boundaries of what it can do, I’m able to do some pretty interesting things on top of YAML and JSON data stored on Github, within Jekyll sites like this. It can be pretty hacky, and would make many programmers cringe, but I like it.</p>

<p>While the idea needs a lot more work, it provides an interesting seed for how OpenAPI can be generated from a single (or multiple) open data file in CSV, JSON, or YAML–which Jekyll speaks natively. The possibilities to commit open data files into a Github repo and have OpenAPI, schema, documentation, and even UI elements automatically generated is pretty huge. This approach to making open data accessible holds a significant amount of potential when it comes to making the open data more discoverable, accessible, forkable, and reusable–which all open data should be by default. I will keep pushing the idea forward, and see where Shelby takes it, and report back here when I have anything more to share.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/07/automatically-generating-openapi-from-a-yaml-dataset-using-jekyll/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/07/why-i-like-a-service-mindset-over-resource-focus-when-it-comes-to-apis/">Why I Like A Service Mindset Over A Resource Focus When It Comes To APIs</a></h3>
        <span class="post-date">07 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/human-services/human-services-docs-screenshot.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am currently crafting a set of services as part of <a href="http://org.open.referral.adopta.agency/">my Human Sevices Data API (API) work</a>. The core set of services for organizations, locations, and services are grouped together as a single service, as this is what I was handed, but all the additional APIs I introduce will be bundled as separate set of individual services. Over the last couple of weeks I’ve introduced seven new services, with a handful more coming in the near future. I’m enjoying this way of focusing on services, over the legacy way that is very resource focused, as I feel like it lets me step back and look at the big picture.</p>

<p>When I was defining the core API for this work I was very centered on the resources I was making available (organization, locations, and services), but once I took on a service mindset I began to see a number of things I was missing. With each service I find myself thinking about the full life cycle, not just the APIs that deliver the service. I’m thinking about the easy ones like design, deployment, and management, but I’m also thinking about monitoring, testing, and security. Then I’m delivering documentation, support, communications, and thinking about my monetization strategy, and access plans. I’m not just doing this once, I am thinking about it in the context of each individual service, as well as across all of them, taking care of the business of the services I’m delivering, not just the technical.</p>

<p>While some folks I talk to look at some of this as repeat work across my projects, I just see them as common patterns, that I should be reusing, refining, and delivering in consistent ways. I’m thinking about delivering the technology in a consistent way, and the operational, but I’m beginning to think about education, training, and how I can help folks on the provider and consumer side of things learn how things are working. I’m not just doing the technical heavy lifting to deliver APIs and then walking away, I’m bundling each search with what is needed to be valuable and successful as an actual service, that is API driven from start to finish. The service is accessible via an API, but it is also delivered, managed, and supported using APIs–everything has an API.</p>

<p>The Human Services Data APIs (HSDA) I am delivering aren’t just a single API, or set of service. They are an open source set of services that I’m putting out there for others to adopt and deliver as part of their own operations. I don’t want these to just be plug and play APIs, and want them to be plug and play services that deliver the information people need to find vital services in their community. Thinking of my APIs as services, and breaking them up into independent microservices helps me address the technical, business, and politics of delivering the technical components cities and organizations are needing. I’ve been pushing the business and politics of APIs since I’ve started, and trying to doing things in as small pieces as I can since the beginning, but the microservices conversations I’ve been tuning into have helped me think beyond the tech, the size, and actually consider how I’m doing this to deliver services to humans–it just an interesting twist that my primary project is all about delivering human service microservices. ;-)</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/07/why-i-like-a-service-mindset-over-resource-focus-when-it-comes-to-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/07/all-federal-government-public-apis-should-begin-with-a-github-repo/">All Federal Government Public API Projects Should Begin With A Github Repo</a></h3>
        <span class="post-date">07 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/68747470733a2f2f662e636c6f75642e6769746875622e636f6d2f6173736574732f3238323735392f313333353931312f32386233656336362d333563302d313165332d386565362d3636323732623966343138362e706e67.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m gearing up for a conversation about the next edition of the FOIA API, and <a href="http://apievangelist.com/2017/09/07/an-openapi-contract-for-the-freedom-of-information/">in preparation I’ve created an OpenAPI definition to help guide the conversation</a>, which I drafted based upon <a href="https://github.com/18F/beta.foia.gov/blob/master/agency-api-spec.md">the specifications published to Github</a> by the FOIA API team at 18F. This was after spending some time reading through <a href="https://github.com/18F/foia-recommendations/blob/master/recommendations.md">the FOIA recommendations for the project</a>, which is also published to Github. Having the project information available on Github, makes it easy for analysts like me to quickly get up to speed on what is going on, and provide valuable feedback to the team.</p>

<p>In my opinion, EVERY government API should start with a Github repo flushing out the needs and requirements for the project, exactly like 18F is doing as part of their FOIA work. All the details of the project are there for not just the project team, but for external participants like myself. When it comes to engaging with folks like me, the API project team doesn’t have to do anything, except send me a link to the Github repository, and maybe point out some specifics, but if the README is complete, only <a href="https://github.com/18F/foia-recommendations/">the repo link is necessary</a>. <a href="https://github.com/18F/foia-recommendations/issues">This opens up conversation around the project using Github Issues</a>, which leaves a history of the discussions that are occurring throughout the project’s life cycle. Any newcomers can invest the time into digesting the documentation, discussion, and then begin to constructively add value to what is already happening.</p>

<p>I know this type of transparent, observable project performance is hard for many folks in government. Hell, it is hard for 18F, and people like myself who do it regularly, by default. It takes a certain fortitude to do things out in the open like this, but this is precisely why you should be doing it. The process injects sunlight into ALL government projects by default. You know your work will be scrutinized from day one, all the way to delivery, so you tend to have your act together. It forces you to open up to other folks ideas and feedback, which isn’t always pleasant, but when done right, can make or break an API project. I mean, your API is going to be public, why not kick it off in the same way? Doing public APIs are all about learning, growing, and establishing a sort of R&amp;D lab around a specific set of resources and services. If this is baked into the DNA of your API project, the chances the API itself will find success is much greater.</p>

<p>I spend a lot of time interfacing with government agencies around APIs. I spend more unpaid time on the phone with folks, and with the right groups I am more than happy to do this. However, I regularly encounter groups who are looking to do APIs, don’t have any existing public APIs, and no Github presence. These are the individuals I encounter who have the worst skills at working well with others, coherently sharing documentation, and many of these projects never get off the ground due to politics. Doing public APIs helps us learn how to be more transparent, observable, and accountable for the projects we are delivering. It isn’t always easy work. It is something that is a journey, and we get better at over time. More government agencies should be working with, and learning from 18F when it comes to delivering projects using Github. Your agency will be better off for it, and the public will benefit from a more observable, and accountable government.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/07/all-federal-government-public-apis-should-begin-with-a-github-repo/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/07/an-openapi-contract-for-the-freedom-of-information/">An OpenAPI Contract For The Freedom Of Information</a></h3>
        <span class="post-date">07 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/foia/freedom-of-information-stamp.jpg" align="right" width="30%" style="padding: 15px;" /></p>
<p>Today’s stories are all based around my preparation for providing some feedback on the next edition of the FOIA.gov API. I have a call with the project team, and want to provide ongoing feedback, so I am loading the project up in my brain, and doing some writing on the topic. The first thing that I do when getting to know any API project, now matter where it is at in it’s lifecycle, is craft an <a href="https://github.com/OAI/OpenAPI-Specification">OpenAPI</a>, which will act as a central contract for discussions. Plus, there is no better way, short of integration, to get to know an API than crafting a complete (enough) OpenAPI definition.</p>

<p>After looking through <a href="https://github.com/18F/foia-recommendations/blob/master/recommendations.md">the FOIA recommendations for the project</a>, I took the draft FOIA API specification and crafted this OpenAPI definition:</p>

<script src="https://gist.github.com/kinlane/ca85e904c05e1ec28c6d277f62be2f80.js"></script>

<p>The specification is just for a single path, that allows you to POST a FOIA request. I made sure I thought through the supporting schema that gets posted, flushing out using the definitions (JSON schema) portion of the OpenAPI. This helps me see all the moving parts, and connect the dots between the API request and response, complete with definition for three HTTP status codes (200,404,500)–just the basics. Now I can see the technical details of a FOIA request in my head, preparing me for my discussion with the project owners.</p>

<p>After loading the technical details in my head, I always like to step back and think about the business, political, and ultimately human aspects of this. This is a Freedom of Information Act (FOIA) API, being used by U.S. citizens to request that information within the federal government be freed. That is pretty significant, and represents why I do API Evangelist. I enjoy helping ensure APIs like this exist, are usable, and become a reality. It is interesting to think of the importance of this OpenAPI contract, and the potential it will have to make information in government more accessible. Providing a potential blueprint that can be used by all federal agencies, establishing a common interface for how the public can engage with government when it comes to holding it more accountable.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/07/an-openapi-contract-for-the-freedom-of-information/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/06/when-to-build-or-depend-on-an-api-service-provider/">When To Build Or Depend On An API Service Provider</a></h3>
        <span class="post-date">06 Sep 2017</span>
        <p><a href="http://monitoring.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-heart-monitor.png" align="right" width="35%" style="padding: 15px;" /></a></p>
<p>I am at that all too familiar place with a project where I am having to decide whether I want to build what I need, or depend on an API service provider. As an engineer it is always easy to think you can just build what you need, but the more experience you have, you begin to realize this isn’t always the smartest move. I’m at that point with API monitoring. I have a growing number of endpoints that I need to make sure are alive and active, but I also see an endless road map of detailed requests when it comes to granularity of what “alive and active” actually means.</p>

<p>At first I was just going to use my default cron job service to hit the base url and API paths defined in my OpenAPI for each project, checking for the expected HTTP status code. Then I thought I better start checking for a valid schema. Then I thought I better start checking for valid data. My API project is an open source solution, and I thought about each of my clients and implementations as me for testing and monitoring for their needs. Then I thought, no way!! I’m just going to use <a href="http://apis.how/8nlsropidv">Runscope</a>, and build in documentation and processes that each of my clients and implementations can also use Runscope to dial in monitoring and testing of their API on their own terms.</p>

<p>Since all of my API projects is OpenAPI driven, and Runscope is an OpenAPI driven API service provider (as ALL should be), I can use this as the seed for setting up testing and monitoring. Not all of my API implementations will be using 100% of the microservices I’m defining, or 100% of the API paths available fo each of the microservices I’m defining. Each microservice has it’s core set of paths that deliver the service, but then I’m also bundling in database, server, DNS, logging and other microservice operational level APIs that not all my implementations will care about monitoring (sadly). So it is important for my clients and implementations to be easily select with APIs they care about monitoring, which OpenAPI will help do the heavy lifting. When it comes to exactly what API monitoring and testing means to them, I’ll rely on Runscope to do the heavy lifting.</p>

<p>If Runscope didn’t have the ability to import an OpenAPI to plant the seeds for API testing and monitoring I might have opted to just build out a basic solution myself. The manual process of setting up my API monitoring and testing for each client would quickly become more work than just building a solution–even if it was nowhere near as good as Runscope. However, we are increasingly living in an OpenAPI driven API lifecycle where service providers of all shapes and sizes allow for the importing and exporting of common API definition formats like OpenAPI. Helping API providers and architects like myself stick to what we do best, and not reinvent the wheel for each stop along the API lifecycle.</p>

<p><strong>Disclosure:</strong> <a href="http://apis.how/8nlsropidv">Runscope</a> is an API Evangelist partner.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/06/when-to-build-or-depend-on-an-api-service-provider/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/06/github-oauth-applications-as-a-blueprint/">Github OAuth Applications As A Blueprint</a></h3>
        <span class="post-date">06 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-oauth-application.png" align="right" width="20%" style="padding: 15px;" /></p>
<p>I was creating a very light-weight API management solution for one of my projects the other day, and I wanted to give my API consumers a quick and dirty way to begin making calls against the API. Most of the API paths are publicly available, but there were a handful of POST, PUT, and DELETE paths I didn’t want to just have open to the public. I didn’t feel like this situation warranted a full blown API management solution like <a href="http://apis.how/zflfesymzk">Tyk</a> or <a href="http://apis.how/ake3nxbapm">3Scale</a>, but if I could just let people authenticate with their existing Github account, it would suffice.</p>

<p>This project has it’s own Github organization, with each of the APIs living as open source API repositories, so I just leveraged Github, and the ability to create Github OAuth applications to do what I needed. You can find OAuth applications under your Github organizational settings, and when you are creating it, all you really need is to give the application a name, description, and a home page and callback URL, then you are given a client id and secret you can use to authenticate individual users with their Github accounts. I didn’t even have to do the complete OAuth dance to get access to resources, or refresh tokens (may will soon), I was just able to implement a single page PHP script to accomplish what I needed for this version:</p>

<script src="https://gist.github.com/kinlane/25e58174ed8b51a30274a3b371b4ca02.js"></script>

<p>I am wiring this script up to a Github login icon on my developer portal, and each API consumer will be routed to Github to authenticate, and then the page will handle the callback where I capture the valid Github OAuth token, and the login, name, email, and other basic Github information about the user. Right now the API is open to anyone who authenticates, but eventually I will be evaluating the maturity of the Github account, and limiting access based upon a variety of criteria (number of repos, account creation date, etc.). For now, I’m just looking for a quick and dirty way to allow my API consumers to get access to resources without creating yet another account. Normally I would be using OAuth.io for this, but I’m trying to minimize dependencies on 3rd party services for this project, and Github OAuth applications plus this script worked well.</p>

<p>Once a user is authenticated they can use their Github user name as the appid, and the valid Github OAuth token as the appkey, which are both passed through as headers, leveraging encryption in transport. I’m not overly worried about security of my APIs, this is more about a first line of defense and identifying consumers, however I will be validating the token with particular API calls. I’m also considering publishing API consumption data to Github repository created within each users accounts as part of API activity, publishing it as YAML, with a simple dashboard for view (authenticated with Github of course). I’ve had this model in my head for some time, and have written about it before, but I’m just now getting around to having a project to implement it in. I’m calling it my poor man’s API management, and something that can be done on a budget (FREE), but if my needs grow any further I will be using a more professional grade solution like <a href="http://apis.how/ake3nxbapm">3Scale</a> or <a href="http://apis.how/zflfesymzk">Tyk</a>.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/06/github-oauth-applications-as-a-blueprint/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/06/azure-matching-aws-with-service-storytelling/">Azure Matching AWS When It Comes To Serverless Storytelling</a></h3>
        <span class="post-date">06 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/azure/functions/azure-functions-thumbnail.png" align="right" style="padding: 25px;" /></p>
<p>I consume a huge amount of blog and Twitter feeds each week. I evaluate the stories published by major tech blogs, cloud providers, and individual API providers. In my work there is a significant amount of duplicity in stories, mostly because of press release regurgitation, but one area I watch closely is the volume of stories coming out of major cloud computing providers around specific topics that are relevant to APIs. One of these topics I’m watching closely is the new area of serverless, and what type of stories each providers are putting out there.</p>

<p>Amazon has long held the front runner position because AWS Lambda was the first major cloud provider to do serverless, coining the term, and dominating the conversation with their brand of API evangelism. However, in the last couple months I have to say that Microsoft is matching AWS when it comes to the storytelling coming out of Azure in the area of serverless and function as a service (FaaS). Amazon definitely has an organic lead in the conversation, but when it comes to the shear volume, and regular drumbeat of serverless stories Microsoft is keeping pace. After watching several months of sustained storytelling, it looks like they could even pass up Amazon in the near future.</p>

<p>When you are down in the weeds you tend to not see how narratives spread across the space, and the power of this type of storytelling, but from my vantage point, it is how all the stories we tell at the ground level get seeded, and become reality. It isn’t something you can do overnight, and very few organizations have the resources, and staying power to make this type of storytelling a sustainable thing. I know that many startups and enterprise groups simply see this as content creation and syndication, but that is the quickest way to make your operations unsustainable. Nobody enjoys operating a content farm, and if nobody cares about the content when it is being made, then nobody will care about the content when it is syndicated and consumed–this is why I tell stories, and you should to.</p>

<p>Stories are how all of this works. It is stories that developers tell within their circles that influence what tools they will adopt. It is stories at the VC level that determine which industries, trends, and startups they’ll invest in. <a href="https://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/">Think about the now infamous Jeff Bezos mandate</a>, which has been elevated to mythical status, and contributed to much of the cloud adoption we have seen to date. It is this kind of storytelling that will determine each winner of the current and future battles between cloud giants. Whether it is serverless, devops, microservices, machine learning, artificial intelligence, internet of things, and any other scifi, API-driven topic we can come up with in the coming years. I have to admit, it is interesting to see Microsoft do so well in the area of storytelling after many years of sucking at it.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/06/azure-matching-aws-with-service-storytelling/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/06/keeping-things-one-dimensional-to-form-api-to-spreadsheet-in-one-step/">Keeping Things One Dimensional To Go From API To Spreadsheet In One Step</a></h3>
        <span class="post-date">06 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-api-to-spreadsheet.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I have been working on the next version of <a href="http://org.open.referral.adopta.agency/">my human services work</a>, which provides a way for cities to make information about organizations, locations, and services available on the web. Part of the feedback from the community around what was missing from the last version, was the number of API calls you needed to make to get a complete representation of a resource, and its sub-resources, as each API response was one dimensional. An example would be that you could get a list of locations, but to get at the list of services you had to make a separate API call. This wasn’t a lapse in API design, it was a result of the schema being born out of a CSV format, and me working to stay true to the original design, and usage of the schema.</p>

<p>In the latest version, I did release a handful of paths that provide a complete representation of each resource and it’s sub-resources. However, I have maintained the original one dimension representation of each resource and sub-resources, allowing me to offer an XML, JSON, as well as CSV representation for each API call. This allows API consumers to pull CSV lists of organizations, locations, services, and their sub-resources like address and phone lists. While not something that would be useful in all API implementations, I feel like the audience for municipal level human services data will benefit significantly being able to go from API to spreadsheet in a single step. All the GET paths for organizations, locations, and services are publicly available by default, not requiring authentication, making CSV data available via a single URL–something anyone can make happen.</p>

<p>While weighing API design decisions as part of my Human Services Data API (HSDA) work I am having to consider not just the technical of how I should be doing this. I am also deeply considering how the API will be put to use, and who will be doing that. While I am thinking about the heavy system to system integration needs of human service providers, as well as the web, mobile, and other applications. I am also thinking about the individual user who might just need a list of the names of organizations, or the addresses of services in a simple CSV format, so that they can work with the data in their most familiar format–the spreadsheet. I am just focusing on the API side of things at the moment, but once I’m done with the latest version I am going to think about some simple linking, and embeddable tooling that allows users to put CSV data from the API to work in a single click using Google Sheets, and Microsoft Excel.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/06/keeping-things-one-dimensional-to-form-api-to-spreadsheet-in-one-step/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/05/just-waiting-the-graphql-assault-out/">Just Waiting The GraphQL Assault Out</a></h3>
        <span class="post-date">05 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/solidies-planning-attack-blue-matrix.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was reading a story on GraphQL this weekend which I won’t be linking to or citing because that is what they want, and they do not deserve the attention, that was just (yet) another hating on REST post. <a href="http://apievangelist.com/2017/06/12/revisiting-graphql-as-part-of-my-api-toolbox/">As I’ve mentioned before, the GraphQL’s primary strength seems to be they have endless waves of bros who love to write blog posts hating on REST, and web APIs</a>. This particular post shows it’s absurdity by stating that HTTP is just a bad idea, wait…uh what? Yeah, you know that thing we use for the entire web, apparently it’s just not a good idea when it comes to exchanging data. Ok, buddy.</p>

<p>When it comes to GraphQL, I’m still watching, learning, and will continue evaluating it as a tool in my API toolbox, but when it comes to the argument of GraphQL vs. Web APIs I will just be waiting out the current assault as I did with all the other haters. The link data haters ran out of steam. The hypermedia haters ran out of steam. The GraphQL haters will also run out steam. All of these technologies are viable tools in our API toolbox, but NONE of them are THE solution. These assaults on “what came before” is just a very tired tactic in the toolbox of startups–you hire young men, give them some cash (which doesn’t last for long), get them all wound up, and let them loose talking trash on the space, selling your warez.</p>

<p>GraphQL has many uses. It is not a replacement for web APIs. It is just one tool in our toolbox. If you are following the advice of any of these web API haters you will wake up in a couple of years with a significant amount of technical debt, and probably also be very busy chasing the next wave of technology be pushed by vendors. My advice is that all API providers learn about the web, gain several years of experience developing web APIs, learn about linked data, hypermedia, GraphQL, and even gRPC if you have some high performance, high volume needs. Don’t spend much time listening to the haters, as they really don’t deserve your attention. Eventually they will go away, find another job, and technological kool-aid to drink.</p>

<p>In my opinion, there is (almost) always a grain of usefulness with each wave of technology that comes along. The trick is cutting through the bullshit, tuning out the haters, and understanding what is real and what is not real when it comes to the vendor noise. You should not be adopting every trend that comes along, but you should be tuning into the conversation and learning. After you do this long enough you will begin to see the patterns and tricks used by folks trying to push their warez. Hating on whatever came before is just one of these tricks. This is why startups hire young, energetic, an usually male voices to lead this charge, as they have no sense of history, and truly believe what they are pushing. Your job as a technologist is to develop the experience necessary to know what is real, and what is not, and keep a cool head as the volume gets turned up on each technological assault.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/05/just-waiting-the-graphql-assault-out/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/05/a-new-minimumviable-documentation-jekyll-template-for-apis/">A New Minimum Viable Documentation(MVD) Jekyll Template For APIs</a></h3>
        <span class="post-date">05 Sep 2017</span>
        <p><a href="https://launchany.github.io/mvd-template/"><img src="https://s3.amazonaws.com/kinlane-productions/launchany/minimum-viable-documentation-template+for-apis.png" align="right" width="40%" style="padding:15px;" /></a></p>
<p>I am a big fan of <a href="https://jekyllrb.com/">Jekyll</a>, the static content management system (CMS). <a href="https://github.com/api-evangelist">All of API Evangelist runs as hundreds of little Jekyll driven Github repositories</a>, in a sort of microservices concert, allowing me to orchestrate my research, data, and the stories I tell across all of my projects. I recommend that API providers launch their API portals using Jekyll, whether you choose to run on Github, or anywhere else using the light-weight portable solution. I have <a href="http://portal.minimum.apievangelist.com/">several Jekyll templates I use to to fork and turn into new API portals</a>, providing me with a robust toolbox for making APIs more usable.</p>

<p>My friend and collaborator James Higginbotham(<a href="https://twitter.com/launchany">@launchany</a>) has launched <a href="https://launchany.github.io/mvd-template/">a new minimum viable documentation (MVD) template for APIs</a>, providing API provides with everything they need out of the gate when it comes to a presence for their API. The MVD solution provides you with a place for your getting started, workflows, code samples, reference material, with OpenAPI as the heartbeat–providing you with everything you need when it comes to API documentation. <a href="https://github.com/launchany/mvd-template">It all is an open source package available on Github</a>, allowing any API provider to fork and quickly change the content and look and feel to match your needs. Which in my opinion, is the way ALL API documentation solutions should be. None of us should be re-inventing the wheel when it comes to our API portals, there are too many good examples out their to follow.</p>

<p>I know that Jekyll is intimidating for many folks. I’m currently dealing with this on several fronts, but trust me when I say that Jekyll will become one of the most important tools in your API toolbox. It takes a bit to learn the structure of Jekyll, and get over some of the quirks of learning to program using Liquid, but once you do, it will open up a whole new world for you. It is much more than just a static content management system (CMS). For me, it’s most significant strength has become as a data management system (DMS)??, with OpenAPI as the heart. I use Jekyll (and Github) for managing all my OpenAPI definitions, JSON and YAML files, and increasingly publishing my data sets in this way instead of relying on server-side technology. If you are looking for an new solution when it comes to your API portal, <a href="https://launchany.github.io/mvd-template/">I recommend taking a look at what James is up to</a>.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/05/a-new-minimumviable-documentation-jekyll-template-for-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/05/api-evangelist-is-a-performance/">API Evangelist Is A Performance</a></h3>
        <span class="post-date">05 Sep 2017</span>
        <p><img src="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png" align="right" width="30%" style="padding: 15px;" /></p>
<p>I think I freaked a couple of folks out last week, so I wanted to take a moment and remind folks that API Evangelist is a performance. Sure, it is rooted in my personality, and I keep it as true to my view of the world of APIs as I can, but it is just a performance I do daily. When I sit down at the keyboard and research the world of APIs I am truly (mostly) interested in the technology, but when I craft the words you read here on the blog I am performing a dance that is meant to be interesting to the technology community in a way that draws them in, but then also gives them a swift kick in the pants when it comes to ethics of the technology, business, and politics of doing all of this.</p>

<p>Sure, my personality shines through all of this, and I’m being genuine when I talk about my own battles with mental illness, and other things, but please remember API Evangelist is a performance. It is a performance that is meant to counteract the regular stream of fake news that comes out of the Silicon Valley funded technology machine. <a href="http://contrafabulists.com/">API Evangelist is a Contrabulist production</a>, pushing back on the often exploitative nature of APIs. Not that APIs are exploitative, it is the people who are doing APIs are exploitative. Back in 2010, I saw that APIs were providing a peek behind the increasingly black box nature of web technology that was invading our lives through our mobile devices, and jumped at the opportunity to jam my foot in the door, even as the VC power brokers continue to look to look for ways to close this door.</p>

<p>In 2011, I found my voice as the API Evangelist explaining APIs to the normals, making these often abstract things more accessible. Along the way, I also developed the tone of this voice pushing back on the politics of doing APIs, calling out the illnesses I see in the space. These are the two areas I hear the most praise from my readers, something that has significantly shaped my performance over the last seven years. I have a pantheon of API characters in my head when I tell stories on API Evangelist, speaking to specific groups, while showcasing as many of the best practices from the space as I possibly can. I’m looking to shine a light on the good, first and foremost, but I’m also never going to shy away from showcasing the illnesses in the space as I have nothing to lose. I’m never looking to get VC funding, or do any technology startup, so throwing myself against the machine doesn’t ever worry me–I will keep doing it until I grow weary of this production.</p>

<p>I just wanted to take the time to help folks understand that all of this is a show. Sure, my rant last week was rooted in my own dark personal thoughts, but it was meant to be a reflection of the space (your darkness). I’m touched at the folks who reached out to me with concern, but I’m fine. If I am ranting on the Internet you can always be sure I’m fine. It is when I go silent for any sustained amount of time is when you should have concern. When I’m in my dark place I have NO interest in performing as API Evangelist, and increasingly I have little interest in Internet technology when I feel this way. If you are reading this, thank you for tuning into my little production. I enjoy doing it because it keeps me learning each day. It keeps me writing and telling stories each day. Hopefully along the way some of you also get some value from the stories I tell, whether their are positive, or a little dark like they were last week.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/05/api-evangelist-is-a-performance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/05/acknowledging-the-good-in-the-api-space/">Acknowledging The Good In The API Space</a></h3>
        <span class="post-date">05 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/kin-lane/kin-lane-api-evangelist-cartoon.png" align="right" width="25%" style="padding: 15px;" /></p>
<p>With such a dark week of blog posts last week I wanted to make sure and start this week off with a brighter post, talking about the good I see in the API space. It can be easy to find than some of the darker things I talked about, but after seven years doing this I see enough good things going on in the API community, that I keep doing this performance I call API Evangelist. It can be easy to rant and rave about the bad, but I find it takes a lot of work to identify the good things going on in the cracks, as they rarely get the attention of the mainstream tech community propaganda engine.</p>

<p>First, there are some really smart folks who truly care about human beings and are dedicated to the world of APIs. I do not know of any other layer of technology that sustains a community of people that is not just about startups and mindless moving forward of technology in every industry. I can use all of my fingers counting the folks who truly care about doing APIs, and making a meaningful impact with them. I have had the pleasure of working with these folks, and brining many of them together as part of my APIStrat conference, and regularly enjoy learning from them, reading their stories, and engaging with them on a regular basis as part of this API journey.</p>

<p>Second, not all APIs are startup focused. I work with many API providers who are doing very interesting, non-startup, non-VC investment, and most importantly, non-exploitative API things. I regularly work with passionate folks doing APIs at all levels of government, making an impact on the environment, pushing for transparency in our legal system, helping provide human services, and truly making change in a meaningful way using web APIs. APIs are neither good, nor bad, or are they neutral, they are simply a reflection of their creators and operators. It keeps me going, to learn about the many ways in which APIs are being used for good, and moving beyond their startup origins, doing meaningful things that makes the lives of human beings better.</p>

<p>Third, APIs are just the next evolution of the web. All the bad things I talked about last week can just as easily be applied to the web. APIs do not have to be the next vendor solution, or something you have to buy. APIs are just about moving our bits and bytes around online, using low cost web technology. They often become the scapegoat for exploitation, unreliability, security, and privacy concerns, but as I said before they are just a reflection of their creator and operators. This is one of the main reasons I’m an evangelist for APIs, not because they are always a good idea, but because when they are done right, they can bring some important observability into some existing technological situations, helping us understand exactly what is going on behind the digital and algorithmic curtain.</p>

<p>It was a learning experience to spend a week ranting openly about the space last week. The way people responded, or didn’t response was very telling about the API community. It feels like something I will be doing regularly (maybe not at that scale), because I felt like it pushed back much of the illness in the space that can become very suffocating to an independent operator like me. The venture backed technology machine doesn’t always realize (or maybe it does) what an invasive and assaulting force it is. They think they are just asking for some free time, or for a guest post opportunity, and don’t often see how damaging they are, because everyone is doing it. I feel like I was able to carve out a defensive zone around what I do, even if it was just for a bit. Thanks again for all your support folks.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/05/acknowledging-the-good-in-the-api-space/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/01/the-why-of-the-unhinged-decoupled-api-evangelist-rant-week/">The Why (And End) Of The Unhinged (Decoupled) API Evangelist Rant Week</a></h3>
        <span class="post-date">01 Sep 2017</span>
        <p><img src="http://i1.wp.com/restlet.dreamhosters.com/wp-content/uploads/2013/12/kinlane.png" align="right" width="40%" style="padding: 15px" /></p>
<p>I know many of you are thinking Kin Lane has lost his marbles (again). In reality, I lost them last week for a couple days because someone really pissed me off, then after a couple more folks pissing in my Cheerios, I checked out last week (this happens from time to time). This week I am actually feeling quite fine after moving to NYC from LA, but the posts for the last couple of days are from my notebook entries made while in a dark place last week. Normally, these posts would never see the light of day, but I’m feeling like they probably should this week. Its no secret, I’m fairly sure I’d be classified in the bi-polar realm (never been diagnosed), something I’ve thoroughly enjoyed since I was a teen, but for the last 20 years is something I’ve had 96% control of. I get angry, fly off the handle sometimes, and have bouts of depression, and life feels like a roller coaster, but for the most part I know the signals, know when to check out, and I am actually able leverage it to my favor–crafting the person that you all know as API Evangelist. It is the fuel for my research, and how I write these words.</p>

<p>Shocking? Run you off? Ok. I’ll accept that. I just wanted to show my readers the contrast between the night and the day, and showcase how hard I work to be really, really nice, and highlight the best of the API space on a daily regular basis. I’m hoping the honesty helps you see what is really going on, with the contrast showing you how much I work to sift through the world of APIs and find useful nuggets of information you might find valuable in your API journey. I really do enjoy what I do as the API Evangelist (most of the time), and I take pleasure in helping people understand the good and the bad of it, in as nice as possible way as I can. What grinds my gears is the folks who feel they need to jump on me, question my motivations, assume there is a hidden agenda, or just inflict their messed up version of the world on this magical world I’ve manage to carve out for myself (for all of us). I may seem pushy and intense this week, but I’m guessing y’all are in denial about how pushy and intense about the things you are passionate about in your world.</p>

<p>I also want to take a moment to highlight the mental illness that exists in the tech sector. It is everywhere if you know what to look for. How do you think I’m able to wrap my head around everything going on with APIs, and why I am an autodidact, and have an affinity for computers from an early age? Most of the white men programmers y’all are putting on a pedestal are mentally ill, they are either just really good at hiding it, or are so privileged that nobody has diagnosed or called them out for it. It is why they are so good at the computerz and Internetz. It is why many are taking pharmaceuticals and microdosing. Trust me, I’ve been there. Done that. Give them 10, 20 years, a divorce, more startup failures, and health problems, you’ll see more of them lose their shit. It is just a matter of time. The real danger there is that most of them don’t know they are ill, or are in denial. I got hints when I was 16, and saw the full spectrum from 20-25, then by the time my daughter was born at 28 I had already figured out most of the telltale signs I needed to keep myself grounded–most of the time. There are still exceptions, and moments when things sneak up on me.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/kin-lane/kin-lane-talks.jpg" align="right" width="25%" style="padding: 15px;" /></p>
<p>As you read my posts this week, I’m sure you were like damn. WTF is going on? He’s paranoid, wacky, or unhinged. Read them all again, I’m only speaking truth. Is the racism and sexism that is ubiquitous in the tech industry any more crazy than me? Is the endless quest for money at all costs in the startup world any whackier than what I’ve written? Following every trend. Telling wild tales of what computer and technology is capable of. Worshipping the tech gods like Elon Musk, Peter Thiel, Marc Andressen, Mark Zuckerberg, Bill Gates, Jeff Bezos and others any more sane that what I do as the API Evangelist? Is the exploitation of people’s privacy and security anymore more sane than what I’m putting forth about the API space? Is ignoring what advertising is doing to the web all about logical straightforward thinking? What makes tech CEOs, and entrepreneurs so much more valuable than teachers, nurses, and other folks? Y’all seem way crazier than I do. I’m sorry. It is just the crazy you know, and is being sensationalized–normalized.</p>

<p>I’m just living, doing what I love, studying the world of APIs, and trying to share my knowledge through my writing. I’m not exploiting and taking advantage of you to get rich. I’m just trying to make a living, and make sense of this Internet age we find ourselves in. Which I have to say that your crazy, seems to making the world a pretty crazy place lately, with Trump and all. Just saying. Anyways, I’m going to take things back down a notch. I’m going to stay off the phone with some of you crazy folks, and stay out of your chaotic companies and organization, and settle back in with my nice NPR like API Evangelist tone. So please don’t come pissing me off again, make sure and pay your invoices, and don’t pick fights with me, and hopefully we won’t have to go here again. I’ll keep things way more sane, less rantier, with just the occasional amping up of things to make some points get across properly along the way.</p>

<p>I also want to thank all of you who reached out privately to make sure I was ok. This means the world to me. You guys are my heroes, and I encourage more of you to do this with other folks in the space. Together, maybe we can all take the crazy down a notch or two and begin to get things back to normal. I have a pretty good handle on my crazy, but I know there are many other folks out there that need your help right now. We need more discussion, education, and support when it comes to mental illness in the space. I personally have talked two people down off the ledge privately in my time as the API Evangelist, and I’m sure there are plenty of others I haven’t have the chance to help. So please talk to each other, and be understanding. You never know when someone might be slipping into the dark.</p>

<p>As I told Tony Tam (my hero) earlier this week–thanks for putting up with me, I really appreciate it.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/01/the-why-of-the-unhinged-decoupled-api-evangelist-rant-week/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/01/the-fact-that-you-dont-know-who-i-am-shows-you-live-in-a-silo/">The Fact That You Do Not Know Who I Am Shows You Live In A Silo</a></h3>
        <span class="post-date">01 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/silo-road.jpg" align="right" width="30%" style="padding: 15px;" /></p>
<p>Don’t who know who I am? I am the API Evangelist. Ok, I know this post is dripping with ego. However, it is the last post in my week of API rants, and I’m just pumped from writing all of these. These types of posts are so easy to write because I don’t have to do any research, and real work, I just write, putting my mad skills at whitesplaining and mansplaining to work–tapping into my privilege. So I’m going to end the week with a bang, and fully channel the ego that has developed along with the persona that is API Evangelist.</p>

<p>However, there is a touch of truth to this. If you are operating an API today, and you do not know who I am, I’m just going to put it out there–you live in a silo. I have published around 3,000 blog posts since 2010 on APIs. I’m publishing 3-5 posts a day, and have consistently done so for seven years. There are definitely some major gaps in that, but my SEO placement is pretty damn good. You type API or APIs, and I’m in the top 30 usually, with the occasional popping up on home page. The number thing I get from folks who message me is that they can’t search for anything API without coming across one of my posts, so they want to talk to me. So why is it that you do not know who I am? I have some ideas on that.</p>

<p>It is because you do not read much outside your silo. When you do, you don’t give any credit to authorship. So when you have read any of my posts you didn’t associate them with a person named Kin Lane. You operate within your silo 98% of the time, and the 2% you get out, you really don’t read much, or learn from others. I on the other hand spend 98% of my time studying what others are doing, and 2% hiding away. My goal is to share this with you. I’d say 75% of my work is just referencing and building on the work of others, only 25% is of my own creation. I’m putting all of this out there for you, and you don’t even know it exists. What does that tell you about your information diet? It tells me that you probably aren’t getting enough exercise and nutrients as part of your regular daily intake, that will make your API operations be less healthy and strong–reading is good for healthy bones girls and boys!</p>

<p>Kin Lane doesn’t have this much ego, but API Evangelist does. The fact that you don’t know who I am shows you aren’t spending enough time studying the API space before launching an API. I’m hoping that you in your API journey you learn more about the importance of coming out of your bubble, and learning from your community, and the wider API community. It is why we do APIs, and why APIs work (when they do). I wrote this title to be provocative and part of my week of rants, but honestly it is true. If you haven’t come across one of my API posts, and stumbled on my blog at some point you should probably think about why this is. The most successful API providers and evangelists I know are tuned into their communities, industries, and wider API space, and are familiar with my work–even if they don’t all like me. ;-)</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/01/the-fact-that-you-dont-know-who-i-am-shows-you-live-in-a-silo/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/01/you-think-you-are-so-smart-you-didnt-do-any-due-diligence-before-launching-your-api/">You Think You Are So Smart You Did Not Conduct Any Due Diligence Before Launching Your API</a></h3>
        <span class="post-date">01 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/supreme-court-judgement.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>You know your API stuff. You know it so well, you don’t even need to look at other APIs. There is no reason to Google and look for other APIs because your stuff is that good. Your idea came to you in a flash, and you worked for an entire weekend to bring to life. Your a genius. Everyone has told you so. This stuff just comes to you, and as long as you are left alone, the magic just happens. If people just stay out of your way, do not burden you with outside influences, and unnecessary concerns, you will keep rolling out amazing APIs that everyone will love and need.</p>

<p>You consume books, and digest endless blog posts and white papers recommended by your trusted network of friends. You don’t ever notice authorship. They don’t matter. It is all about feeding your mind, and you will decide whether it is worthy or not. You don’t save bookmarks for citations or attributions, once inside your brain ALL ideas becomes yours. If someone’s idea is dumb, you make sure an let them know, making sure they are aware of how they are substandard and beneath you. If your friends let you know your ideas are amazing, you let them know they are great too, and will be rewarded by being in your presence, and part of your team.</p>

<p>That one chick that was hired last year made the mistake of blurting out in a meeting, “isn’t that the same thing as that startup that launched last month?”. She isn’t on the core team anymore. You did look at what she was talking about, and their API design is inferior, and the look of their site just turned you off–no need to continue. This is why you don’t conduct due diligence for your API projects. Why spend time looking at so many bad ideas? It takes away from your time to make the magic happen. Why do people keep wasting your time with this stuff? It is clear your ideas are superior, just look at your numbers. Clearly your APIs are well received, and all the feedback from partners have been great. I mean 60% of top startups in the sector are using your service, who cares what else is out there.</p>

<p>All of this is true in your bubble, but it won’t always pencil out outside. What you do not know or see will always eventually begin to diminish your work. The aspects of the markets you don’t see will never see the value of your work. You will never truly grow and evolve if you do not acknowledge the ideas that have influenced you. You weren’t born with all this knowledge, you are learning, borrowing, stealing, and building on the ideas of others. You should always learn from what is already out there, even if it is the worst of the worst. Through studying your competition your ideas will become hardened and truly competitive, and they will have strength when operating outside your bubble, beyond your control. You never know, you might actually come across a gem in the pile of bad ideas, something or someone you never imagined. Something that might shift the paradigm for you.</p>

<p>In your youth, this type of isolation, and bubble creation might work, but eventually you will miss certain signals and trends. There will be market forces that you and your network will miss, and once you begin to fall behind, it will be difficult to catch up. You will be building genius APIs that nobody wants or understands because your work as lost its relevance and context. You may have your finger on the pulse now, but you are not being honest with yourself about how you found that pulse, and are too confident that you will always have it. A significant part of your success has been your privilege, and that you are playing in this game while things are still new. With each wave of growth, and entry of new players, you will become irrelevant, and eventually old and in the way. The new geniuses will make the same mistakes you did, and shut you out, just like you have done to so many. It is how all of this works as time moves on.</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/01/you-think-you-are-so-smart-you-didnt-do-any-due-diligence-before-launching-your-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/01/you-have-no-api-imagation-creativity-or-sensibility/">You Have No API Imagination, Creativity, Or Sensibility</a></h3>
        <span class="post-date">01 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algorotoscope/cyber-api-description-wars/mosaic-face_internet_numbers.jpg" align="right" width="35%" style="padding: 15px;" /></p>
<p>I know you are used to people telling you that you are creative, and your ideas are great, but I’m here to tell you they aren’t. You lack any imagination, creativity, or sensibility when it comes to your APIs. Some of it is because you are personally lacking in these areas, part of it is because you have no diversity on your team, but it is mostly because you all are just doing this to make money. As creative as you think doing a startups is, they are really just about making money for your bosses, and investors–not a lot of imagination, creativity, or sensibility is required.</p>

<p>You could invest the time to come up with good ideas for applications and stories on your blog, but you really don’t want to do the work, or even stand out in the group. It is much easier to just phone it in, follow the group, and let your bosses and the existing industry trends dictate what you do each day. If the business sector you operate within is doing it, you are doing it. If you see something funny online or at a conference you will do it. You have a handful of blogs you read each weekend, that you will rewrite the best posts from and publish on your own blog. Your Twitter account is just retweeting what you find, and you don’t even push out your own stories, because you have already tweeted out the story you copied in the first place.</p>

<p>Don’t beat yourself up about this, you come by it honestly. Your privilege affords you never really getting out of your comfort zone, and the people around you make you feel good enough. Everyone on  your team is the same, and your bosses really don’t care, as long as you are just creating content, and sending out all the required signals. Just make it look like you are always busy, and keep all the channels active. You don’t actually have to support your API consumers, just make sure you are having conversations with them on forums, and the channels your boss can keep an eye on. Doing too much will make you a target. Keep an eye on your coworkers and never do anymore than they are, establishing a kind of solidarity of mediocrity. This isn’t rocket surgery, its API theater.</p>

<p>You know deep down that you have some creativity in there, but it is something that has never been encouraged. This is the damaging effects of your privileged world. Your parents, teachers, bosses, and friends never push you, and neither do you. You don’t have to. If this story pisses you off, you really have nobody to blame. You’ve never had to work hard. You have never pushed yourself to do any of the hard work required to fail, on the path to becoming creative, developing your sensibility, and honing your imagination. How are you ever going to know what you are capable of if you do not put yourself out there. Creativity isn’t created in a silo. People aren’t just born with sensibility. And API aren’t lacking in imagination by default. There are many APIs operating out there that possess all these characteristics, and are leading the conversation–why aren’t you one of them?</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/01/you-have-no-api-imagation-creativity-or-sensibility/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/31/your-lack-of-investment-in-api-education-will-be-the-end-of-your-api-service/">Your Lack Of Investment In API Education Will Be The End Of Your API Service</a></h3>
        <span class="post-date">31 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/36698086536_f214416faf_z.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>Your API service is the next big thing. It is something that every API provider will be needing, and you are confident it will be something that makes the API management space look like a momentary trend. You have a SaaS, as well as an open source edition, and have invested thousands of hours into your website, documentation, and you even have an API. Everything is CI/CD ready, and you speak fluent OpenAPI. You have all the bases covered, and adoption with the first wave of users has been great, and the sales numbers are meeting all the project. There is just one thing, you haven’t invested anything into the educational resources for your customers, and the companies they work for.</p>

<p>After a year of operation you are so confident in your team, and the services and tools you have, you feel like this is a sure thing. You’ve sold to all your startup friends, and have trended well on Product Hunt and Hacker News, and you see organic discussions about how your services are essential. Your API service provider startup is now ready for primetime, you are expand beyond the west coast, and begin targeting a handful of verticals in a couple of the biggest markets. Your research tells you that these industries are struggling with their APIs, and what you have is the solution to their pain–this is gonna be easy!</p>

<p>Six months later, your new user numbers are still growing, but active users have declined. Nobody is using your tool after the free trial period. You’ve hired armies of sales guys to go out and push, and come back with more information about what is happening. The number one thing you hear is that people just don’t get it. Not only do they not understand why your tools and services are valuable, they really don’t speak the same language when it comes to the API providers you are used to selling to. The groups you have worked with so far just get it, and they see the value to their mature API operations. The new businesses you’ve been targeting barely know what an HTTP status code is, and just aren’t that interested in CI/CD, microservices, GraphQL, or any of the other buzzwords you are used to pushing.</p>

<p>The companies you have in the funnel right now all have API programs, but they just aren’t that API literate and aware. They are just getting by, doing a mix of web services and web APIs, with traditional database connections, FTP dumps, and whatever else it takes to get thing done. They don’t understand the web, anything API, are lucky if they can go a week without sever outage or hiccup, let alone continuously integrate or deploy anything. You didn’t budget for this. Hell, you didn’t budget for educating users about anything, let alone playing catch up at this scale. These developers don’t get API at all, and without a base level of awareness, they can’t see where your tools and services fit into the big picture.</p>

<p>You’ve taken this intel back to your boss, but he doesn’t care. You obviously aren’t selling hard enough. Push harder to get them to sign a contract. Tell them whatever they want to hear to get them signed on. There is no budget for playing schoolteacher. This isn’t grade school. It isn’t your problem that they are so API illiterate. After another six months of this it is clear you are about to replaced with someone who will get the job done. Your lack of planning and investment in the area of education regarding not just your product, but also how a modern application and API lifecycle works will be the end of at least your job. Don’t worry though, in another 12 months it will be the end of the API service provider startup you work for as well. The world just wasn’t ready for your amazing API tools and services.</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/31/your-lack-of-investment-in-api-education-will-be-the-end-of-your-api-service/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/31/admit-you-do-not-respect-your-api-consumers-and-end-users/">Admit It You Do Not Respect Your API Consumers And End Users</a></h3>
        <span class="post-date">31 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/photos/fat-riches-russian.jpg" align="right" width="35%" style="padding: 15px;" /></p>
<p>Just admit it, you could care less about your API consumers. You are just playing this whole API game because you read somewhere that this is what everyone should be doing now. You figured you can get some good press out of doing an API, get some free work from developers, and look like you are one of the cool kids for a while. You do the song and dance well, you have developed and deployed an API. It will look like the other APIs out there, but when it comes to supporting developers, or actually investing in the community, you really aren’t that interested in rolling up your sleeves and making a difference. You just don’t really care that much, as long as it looks like you are playing the API game.</p>

<p>Honestly, you’d do any trend that comes along, but this one has so many perks you couldn’t ignore it. Not only do you get to be API cool, you did all the right things, launched on Product Hunt, and you have a presence at all the right tech events. Developers are lining up to build applications, and are willing to work for free. Most of the apps that get built are worthless, but the SDKs you provide act as a vacuum for data. You’ve managed to double your budget by selling the data you acquire to your partners, and other data brokers. You could give away your API for free, and still make a killing, but hell, you have to keep charging just so you look legit, and don’t raise any alarm bells.</p>

<p>It is hard to respect developers who line up and work for free like this. And the users, they are so damn clueless regarding what is going on, they’ll hand over their address book and location in real-time without ever thinking twice. This is just to easy. APIs are such a great racket. You really don’t have to do anything but blog everyone once in a while, show up at events and drink beer, and make sure the API doesn’t break. What a sweet gig huh? No, not really, you are just a pretty sad excuse of a person, and it will catch up with you somewhere. You really represent everything wrong with technology right now, and are contributing to the world being a worse place than it already is–nice job!</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/31/admit-you-do-not-respect-your-api-consumers-and-end-users/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/31/the-api-space-is-in-the-tractor-beam-of-the-cloud-giants-now/">The API Space Is In The Tractor Beam Of The Cloud Giants Now</a></h3>
        <span class="post-date">31 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/photos/death-star.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>A growing number of SMBs, SMEs, and other institutions, organizations, and government agencies are launching APIs, but the age APIs as the core product will thin, and those that do emerge and operate independently will be increasingly absorbed into the cloud platforms they operate on. The tractor beam of AWS, Google, and Azure are becoming to strong for us API providers to resit. We use their platforms to deploy and manage our APIs, we’ve ceded control over our operations to their clouds, it is just a matter of time before each of the APIs we depend on are assimilated into the cloud machine.</p>

<p>Sure, we’ll still get access to valuable resources that we couldn’t launch ourselves, things like Google Maps, and resources at scale like Amazon EC2. But our platforms will be closely watching our trajectories and they will make the calculating decision whether what we are doing is valuable enough to acquire, or just suffocate by launching a competitive service. Whoops, sorry! What you are doing is a great idea, but you will have three choices, don’t do it, sell to us, or we’ll drive you out of business. The open landscape of APIs will become thousands of cloud APIs that feel like we have an unlimited amount of resources, but in reality it will just limit and control available resources, allowing the cloud bigcos to steer the space wherever they want to–controlling the creation and distribution of ideas.</p>

<p>It was fun while it lasted. We were able to work on our own farms for a while, and use each others free range organic API resources, but pretty soon it will just be the factory model of API consumption. Sure, prices will be low when the cloud giants are doing battle, but when they aren’t we’ll be paying a premium for simple resources. When any newcomer emerges looking to disrupt any area of the API space they’ll be quickly consumed, suffocated, and silenced, removing any threat. Think of the broadband access in the United States currently, that is what API consumption will look like in 20 years. There will be no options, no diversity, and we’ll be slaves to the bigcos. We will look back at the cloud period of APIs and regret that we ever gave them so much power.</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/31/the-api-space-is-in-the-tractor-beam-of-the-cloud-giants-now/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/31/apis-will-just-get-more-unreliable-as-funding-becomes-more-volatile/">APIs Will Just Get More Unreliable As Funding Becomes More Volatile</a></h3>
        <span class="post-date">31 Aug 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/36349148770_1a9d2692b2_z.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>People love to point out that APIs are unreliable. You can’t depend on them. They go away at any point, and they just aren’t something you want to be building your business on top of. When in reality APIs aren’t reliable it is the business, people, and investment behind them. The reality of the startup game is that us API consumers aren’t actually the customer, we are just numbers in a larger game where startup founders and their investors are looking for enterprise customers to purchase their startup getting the desired exit. The API is just about attracting consumers, who will do the legwork to bring in users, adding to the value of a company.</p>

<p>As the startup funding landscape continues to dry up, shift, and evolve towards <a href="https://www.icoalert.com/">more riskier and volatile versions of investment like ICO</a>s, things are only going to get worse. Of course, few conversation will place the blame on the people and companies behind, but APIs will continue to be the scapegoat for the instability. It works just like the robots coming for your jobs. You never hear that rich people who own companies, that are making decisions to replace workers with robots are coming for your jobs. Its the robots. Technology in many forms makes for a great blame shield, absorbing the responsibility for the volatility, instability, and scams that are going on across the landscape.</p>

<p>In reality, nothing much changes for us API consumers. You need to get to know your API providers, well as the company and people behind them. Study their approach to operating their API. Do they communicate? Do they have proper support? Do they communicate their uptime status? What type of funding is propping them up, and the shape of their business model use. Make sure you always have a plan B if you can, and do not trust that ANY API will be around forever. If possible, come up with failover plans, and run drills with your team regarding what will happen when APIs fail, become unreliable, or go away entirely. Ultimately it is up to you to determine the impact unreliable APIs will have on your APIs. We can blame API providers, startups, and VCs as much as we want, but it is entirely up to us to help stabilize the API space for our users.</p>

<p>Investment, fundraising, and chasing exits will be the #1 area of instability for the API space. After that it will be the network, ranging from network neutrality going away to lack of investment in capacity and security. While the growth in the number of companies, organizations, institutions, and government agencies doing APIs will keep things on forward trajectory, the reputation of APIs will continue to take a hit when it comes to the stories people tell about who is to blame. APIs are the frontline for almost everything occurring online these days, and this will make it the first place everyone will be pointing the finger. It is up to us in the API community to keep telling stories putting the blame where it is deserved. If we are not telling stories, the other side will, and they’ll be firing up the pundits, continuing to let irresponsible API startups get away with their bullshit games.</p>

<p><em><strong>Note:</strong> If my writing is a little dark this week, <a href="http://apievangelist.com/2017/08/28/api-rant-vs-api-research/">here is a little explainer</a>–don’t worry, things will back to normal at API Evangelist soon.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/31/apis-will-just-get-more-unreliable-as-funding-becomes-more-volatile/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

<p align="center"><a href="http://apievangelist.com/archive/"><strong>View Previous Posts Via Archives</strong></a></p>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/post-con-2019/" target="_blank"><img src="https://apievangelist.com/images/300x250-postcon-2019.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/post-con-2019/" class="image"><img src="https://apievangelist.com/images/300x250-postcon-2019.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
