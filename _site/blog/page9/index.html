<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }
    
    .container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
    }
    .video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }    

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/24/some-new-api-evangelist-art/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-butterfly-vertical.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/24/some-new-api-evangelist-art/">Some New API Evangelist Art</a></h3>
			<p><em>24 Oct 2017</em></p>
			<p>When I first started API Evangelist I spent two days trying to create a logo. I then spent another couple days trying to find a service to create something. Unhappy with everything I produced, I resorted to what I considered a temporary logo, where I just typed a JSON representation of the logo, mimicking what a JSON response for calling a logo through an API might look like. Seven years later, the logo has stuck, resulting in me never actually invested any more energy into my logo. The API Evangelist imagery is long overdue for an overhaul. I stopped wearing the logo on my signature black t-shirts a couple years back, and I do not want to reach the 10 year mark before I actually do anything new. My logo, and the other artwork I’ve accumulated over the last several years played their role, but I’m stoked to finally begin the process of evolving some artwork for API Evangelist. To help me move the needle I’ve began working with my friend Bryan Mathers (@BryanMMathers), where I have experienced his Visual Thinkery induced journey, where he generates image ideas by engaging in a series of conversations. Producing some whimsical, colorful, and entertaining images for anything he can imagine out of our discussions. It is something I’ve experienced as part of our parent company Contrafabulists, and stoked to experience as part of my API Evangelist work. Bryan doodled the butterfly image while we were talking, and didn’t anticipate it would be the one I’d choose to be the logo. Honestly, at first I didn’t think it was logo quality, but after spending time looking and thinking about, I feel it suits what I’m doing very well. It still has the technical elements in the brackets in the wing outline, and the digital or pixel nature of the color in the wings, but is moving beyond the tech and represents the natural, more human side of things I...[<a href="/2017/10/24/some-new-api-evangelist-art/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/24/aws-api-gateway-export-in-openapi-and-postman-formats/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/aws-api-gateway-export.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/24/aws-api-gateway-export-in-openapi-and-postman-formats/">AWS API Gateway Export In OpenAPI and Postman Formats</a></h3>
			<p><em>24 Oct 2017</em></p>
			<p>I wrote about being able to import an OpenAPI into the AWS API Gateway to jumpstart your API the other day. OpenAPI definitions are increasingly used for every stop along the API life cycle, and being able to import an OpenAPI to start a new API, or update an existing in your API gateway is a pretty important feature for streamlining API operations. OpenAPI is great for defining the surface area of deploying and managing your API, as well as potentially generate client SDKs, and interactive API documentation for your API developer portal. Another important aspect of this API lifecycle is being able to get your definitions out in a machine readable format as well. All service providers should be making API definitions a two-way street, just like Amazon does with the AWS API Gateway. Using the AWS Console you can export an OpenAPI definition for any API. What makes things interesting is you can also export an OpenAPI complete with all the OpenAPI extensions they use to customize each API within the API Gateway. Also, they provide an export to OpenAPI, but with Postman specific extensions, allowing you to use use in the desktop client tooling when developing as well as integrating with any API. I’ve said it before, and I’ll say it again. Every API service provider should allow for the importing and exporting of common API definition formats like OpenAPI and Postman. If you are selling services and tooling to API designers, developers, architects, and providers, you should ALWAYS provide a way for them to generate a static definition of what is going on–bonus, if you allow them to publish API definitions to Github. I know some API service providers like to allow for API import, but worry about customers abandoning their service if there is an export. Too bad, offer better services, and affordable pricing models, and people will stick around. Beyond selling services to folks with day jobs, having the...[<a href="/2017/10/24/aws-api-gateway-export-in-openapi-and-postman-formats/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/23/budget-api-management-using-github/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-oauth-api-management.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/23/budget-api-management-using-github/">Budget API Management Using Github</a></h3>
			<p><em>23 Oct 2017</em></p>
			<p>I am always looking for the cheapest, easiest ways to get things done in the world of APIs. As a small business owner I’m always on the hunt for hacks to get done what I need, and hopefully make things easier for my users, while keeping things free, or at least minimally priced for my business. When it comes to my simplest of APIs, where I’m not looking to fully manage, but I do want anyone using them to authenticate, and pass in API keys, so that I can track on their use. In some cases I’m going to bill against this usage, but for the most part I just want to secure, and quantify their consumption. The quickest and dirtiest way I will enable authentication for any API is using Github. First thing you do is create a Github OAuth application, which is available under settings for your Github user or organization. Then I add a JavaScript icon and login link, and then paste a PHP script at another location, where it will be handling the login. All you have to do is update the URLs in both scripts, and when someone clicks on icon, they’ll be authenticated and then dropped back on the original page with username, and valid OAuth token–which then at this point you have a validated user, and valid token. In some situations I just require that the appid and appkey be passed in with each API call. I do not rate limit, or bill against usage. I am just looking to identify consumers. Other projects I’m actively billing against consumption, but I’m doing this by processing the web server logs for the API. Again, bare bones operations. For the next level up, in the login PHP script I’m actually adding a user to a specific plan I’ve setup for an API using the AWS API Gateway, and adding their key to the gateway. Now a user has access to...[<a href="/2017/10/23/budget-api-management-using-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/23/apis-reduce-everything-to-a-transaction/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/reduce+everything+to+a+transaction_tr.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/23/apis-reduce-everything-to-a-transaction/">APIs Reduce Everything To A Transaction</a></h3>
			<p><em>23 Oct 2017</em></p>
			<p>My partner in crime Audrey Watters crafted a phrase that I use regularly, that “APIs reduce everything to a transaction”. She first said it jokingly a few years back, but is something I regularly repeat, and think about regularly, as I feel it profoundly describes the world I study. I like the phrase because of its dual meaning to me. If I say it with a straight face, in different company, I will get different responses. Some will be positive, and others will be negative. Which I think represents the world of APIs in a way that show how APIs are neither good, bad, or neutral. If you are an API believer, when I describe how APIs reduce everything to a transaction, you probably see this as a positive. You are enabling something. You are distilling down aspects of our personal and business worlds down into a small enough representation, so that it can be transmitted via an API. Enabling payments, messages, likes, shares, and other aspects of our digital economy. Your work as an API believer is all about reducing things down to a transaction, so that you can make people’s lives better, and enable some kind of functionality that will deliver value online, and via our mobile phones. API transactions are enablers, and by using APIs, you are working to make the world a better place. If you aren’t an API believer, when I describe how APIs reduce everything to a transaction, you are probably troubled, and left asking why I would want to do this. Not everything can or should be distilled down into a single transaction. Shifting something to be a transaction opens it up for being bought and sold. This is the nature of transactions. Even if you are delivering value to end-users by reducing a piece of their world to a transaction, now that it is a transaction, it is vulnerable to other market forces. It is these unintended...[<a href="/2017/10/23/apis-reduce-everything-to-a-transaction/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/23/api-monetization-framework-as-introduced-by-aws-marketplace/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/aws-marketplace-aws-saas-seller-integration-guide.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/23/api-monetization-framework-as-introduced-by-aws-marketplace/">API Monetization Framework As Introduced By AWS Marketplace</a></h3>
			<p><em>23 Oct 2017</em></p>
			<p>I am learning about the AWS Marketplace through the lens of selling your API there, adding a new dimension to my API monetization and API plan research. I’ve invested a significant amount of energy to try and standardize what I learn from studying the pricing and plans for the API operations of the leading API providers. As I do this work I regularly hear from folks who like to tell me how I’ll never be able to standardize and normalize this, and that it is too big of a challenge to distill down. I agree that things seem too big to tame at the current moment, but with API pioneers like AWS, who have been doing this stuff for a while, you can begin to see the future of how this stuff will play itself out. Amazon set into motion a significant portion of how we think about monetizing our API resources. The pay for what you use model has been popularized by Amazon, and continue to dominate conversations around how we generate revenue around our valuable digital assets. AWS has some of the most sophisticated pricing structure around their API services, as well as very mature pricing calculators, and have created markets around their resources (ie. spot instances for compute). You can see these concepts playing out in the guidance they offer software developers in their AWS Marketplace Seller Guide, which helps sellers modify their SaaS products to sell them through AWS Marketplace using two models: 1) metering, or 2) contract. When you list or application in the AWS Marketplace you must choose between one of these models, but both involve thinking critically about your monetization strategy, which includes your hard costs, as well as where the value will lie with your customers–striking the balance necessary to operate a viable API business. According to the AWS Marketplace Seller Guide, each SaaS application owner listing through AWS Marketplace has two options for listing and billing...[<a href="/2017/10/23/api-monetization-framework-as-introduced-by-aws-marketplace/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/23/a-simple-api-with-aws-dynamodb-lambda-and-api-gateway/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/aws-dynamodb-lambda-api-gateway.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/23/a-simple-api-with-aws-dynamodb-lambda-and-api-gateway/">A Simple API With AWS DynamoDB, Lambda, and API Gateway</a></h3>
			<p><em>23 Oct 2017</em></p>
			<p>I’ve setup a few Lambda scripts from time to time, but haven’t had any dedicated project time to push forward API serverless concepts. Over the weekend I had a chance to deploy a couple of APIs using AWS DynamoDB, Lambda, and API Gateway, lighting up some of the serverless API possibilities in my brain. Like most areas of the tech sector, I think the term is dumb, and there is too much hype, but I think underneath there is some interesting possibilities, at least enough to keep me playing around with things. Right now my primary API setup is Amazon Aurora (MysQL) backend, with API deployed on EC2, using Slim API framework in PHP. It is clean, simple, and gets the job done. I use 3Scale, or Github for the API management layer. This new approach simplifies some things for me, but definitely goes further down the AWS rabbit hole with the adoption of API Gateway and Lamdba, but also introduces some interesting enough benefits, that has me considering for use on some specific projects. Identity and Access Management (IAM) Role The first thing you need to do to make the whole AWS thing work is setup a role using AWS IAM. I created a role just for this project, and added CloudWatchFullAccess, AmazonDynamoDBFullAccess, and AWSLambdaDynamoDBExecutionRole. I need this role to handle a bunch of management level things with the database, and logging. IAM is one of the missing aspects of hand crafting my APIs, and is why I am considering adopting on behalf of my customers, to help them get a handle on security. Simple API Database Backends Using AWS DynamoDB I am a big fan of relational databases, mostly out of habit and experience. A client of mine is fluent in AWS DynamoDB, which is a simple NoSQL solution, so I felt compelled to ensure the backend database for their APIs spoke DynamoDB. It’s a pretty simple database, so I got to work...[<a href="/2017/10/23/a-simple-api-with-aws-dynamodb-lambda-and-api-gateway/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/20/selling-your-aws-api-gateway-driven-api-through-the-aws-marketplace/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-on-sea_diego_rivera1.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/20/selling-your-aws-api-gateway-driven-api-through-the-aws-marketplace/">Selling Your AWS API Gateway Driven API Through The AWS Marketplace</a></h3>
			<p><em>20 Oct 2017</em></p>
			<p>I am getting intimate with AWS API Gateway. Learning about what it does, and what it doesn’t do. The gateway brings a number of essential API management elements to the table, like issuing keys, establishing plans, and enforcing rate limits. However, it also lacks many of the other active elements of API management like billing for usage, which is an important aspect of managing API consumption for API providers. With AWS, things tend to work like legos, meaning many of their services work together to deliver a larger picture, and I’ve been learning more about how AWS API Gateway works with the AWS Marketplace to deliver some of the business of API features I’m looking for. Here is the blurb from the AWS API Gateway documentation regarding how you can setup AWS API Gateway to work with AWS Marketplace, making your API available for sale as a SaaS service: After you build, test, and deploy your API, you can package it in an API Gateway usage plan and sell the plan as a Software as a Service (SaaS) product through AWS Marketplace. API buyers subscribing to your product offering are billed by AWS Marketplace based on the number of requests made to the usage plan. To sell your API on AWS Marketplace, you must set up the sales channel to integrate AWS Marketplace with API Gateway. Generally speaking, this involves listing your product on AWS Marketplace, setting up an IAM role with appropriate policies to allow API Gateway to send usage metrics to AWS Marketplace, associating an AWS Marketplace product with an API Gateway usage plan, and associating an AWS Marketplace buyer with an API Gateway API key. Details are discussed in the following sections. To enable your customers to buy your product on AWS Marketplace, you must register your developer portal (an external application) with AWS Marketplace. The developer portal must handle the subscription requests that are redirected from the AWS Marketplace console. While...[<a href="/2017/10/20/selling-your-aws-api-gateway-driven-api-through-the-aws-marketplace/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/20/api-management-dashboard-the-provider-view/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-on-sea_atari_missle.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/20/api-management-dashboard-the-provider-view/">API Management Dashboard: The Provider View</a></h3>
			<p><em>20 Oct 2017</em></p>
			<p>I’m helping some clients think through their approach to API management. These projects have different needs, as well as different resources available to them, so I’m looking to distill things down to the essential components needed to get the job done. I’ve taken a look at the API consumer account basics as well as their usage, and next I want to consider the view of all of this from the API provider vantage point. For both of my current projects, I’m needing to think about the UI elements that deliver on API management elements from the API provider perspective. To help me think though the UI elements needed for helping manage the essential elements of managing APIs I wanted to create a simple list of each screen that will be needed to get the job done. So far, I have the following X UI elements as part of my API management base: Creation - The landing page for account creation. Ideally, these are using OpenID / OAuth for major providers, eliminating the need for passwords. Login - The page for logging back in after a user has registered. Ideally, these are using OpenID / OAuth for major providers, eliminating the need for passwords. Dashboard - The landing page once an API provider is logged in – providing access to all aspects of API management. APIs - A list of APIs, with detail pages for managing each individual API definition. Plans - A list of API plans, with detail pages for managing each individual API plan definition. Accounts - A list of API consumer accounts, with detail pages for managing each individual API consumer. Usage - - A list of API calls from logs, with tools for breaking down by API, plan, or consumer. Invoices - A list of all invoices that have been generated for a specific time period, across specific consumers. With a detail page for seeing individual invoice details. There may be more...[<a href="/2017/10/20/api-management-dashboard-the-provider-view/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/20/api-management-dashboard-the-consumer-view/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-on-sea_constitution.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/20/api-management-dashboard-the-consumer-view/">API Management Dashboard: The Consumer View</a></h3>
			<p><em>20 Oct 2017</em></p>
			<p>I’m helping some clients think through their approach to API management. These projects have different needs, as well as different resources available to them, so I’m looking to distill things down to the essential components needed to get the job done. I’ve taken a look at the API consumer account basics as well as their usage, and next I want to consider the view of all of this from the API provider vantage point. For both of my current projects, I’m needing to think about the UI elements that deliver on API management elements from both the API provider and consumer levels. I’ve already tackled the API provider view, next up is the API consumer view. To help me think though the UI elements needed for helping manage the essential elements of managing API consumption for developers I wanted to create a simple list of each screen that will be needed to get the job done. So far, I have the following X UI elements as part of my API management base: Creation - The landing page for developer account creation. Ideally, these are using OpenID / OAuth for major providers, eliminating the need for passwords. Login - The page for logging back in after a developer has registered. Ideally, these are using OpenID / OAuth for major providers, eliminating the need for passwords. Dashboard - The landing page once an API consumer is logged in – providing access to all aspects of their API access. Account - The ability to update developer account information. Key(s) - The ability to get the master set, or multiple copies of API keys. Plans - Viewing all available plans, with the ability to see which plan a consumer is part of and switch plans if relevant. Usage - See history of all API consumption by API and time period. Credit Card - The addition and updating of account credit card. Billing - See history of all invoices for...[<a href="/2017/10/20/api-management-dashboard-the-consumer-view/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/20/api-developer-account-usage-basics/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-on-sea_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/20/api-developer-account-usage-basics/">API Developer Account Usage Basics</a></h3>
			<p><em>20 Oct 2017</em></p>
			<p>I’m helping some clients think through their approach to API management. These projects have different needs, as well as different resources available to them, so I’m looking to distill things down to the essential components needed to get the job done. I spent some time thinking through the developer account basics, and now I want to break down the aspects of API consumption and usage around these APIs and developer accounts. I want to to think about the moving parts of how we measure, quantify, communicate, and invoice as part of the API management process. Having A Plan We have developers signed up, with API keys that they’ll be required to pass in with each API call they make. The next portion of API management I want to map out for my clients is the understanding and management of how API consumers are using resources. One important concept that I find many API providers, and would be API providers don’t fully grasp, is service composition. Something that requires the presence of a variety of access tiers, or API plans, which define the business contract we are making with each API providers. API plans usually have these basic elements: plan id - A unique id for the plan. plan name - A name for the plan. plan description - A description for the plan. plan limits - Details about limits of the plan. plan timeframe - The timeframe for limits applied. There can be more than one plan, and each plan can provide different types of access to different APIs. There might be plans for new users versus verified ones, as well as possibly partners. The why and how of each plan will vary from API provider to provider, but their are all essential to managing API usage. Something that needs to be well defined, in place, with APIs and consumers organized into their respective tiers. Once this is done, we can begin thinking about the...[<a href="/2017/10/20/api-developer-account-usage-basics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/20/api-developer-account-basics/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-on-sea_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/20/api-developer-account-basics/">API Developer Account Basics</a></h3>
			<p><em>20 Oct 2017</em></p>
			<p>I’m helping some clients think through their approach to API management. These projects have different needs, as well as different resources available to them, so I’m looking to distill things down to the essential components needed to get the job done. The first element you need to manage API access is the ability for API consumers to be able to sign up for an account, that will be used to identify, measure usage, and engage with each API consumer. Starts With An Account While each company may have more details associated with each account, each account will have these basics: account id - A unique identifier for each API account. name - A first name and last name, or organization name. email - A valid email address to communicate with each user. Depending on how we enable account creation and login, there might also be a password. However, if we use existing OpenID or OAuth implementations, like Github, Twitter, Google, or Facebook, this won’t be needed. We are relying on these authentication formats as the security layer, eliminating the need for yet another password. However, we still may need to store some sort of token identifying the user, adding these two possible elements: password - A password or phrase that is unique to each user. token - An OAuth or other token issued by 3rd party provider. That provides us with the basics of each developer API developer account. It really isn’t anything different than a regular account for any online service. Where things start to shift a little specifically for APIs, is that we need some sort of keys for each account that is signing up for API access. The standard approach is to provide some sort of API key, and possibly a secondary secret to compliment it: api key - A token that can be passed with each API call. api secret - A second token, that can be passed with each API...[<a href="/2017/10/20/api-developer-account-basics/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/19/the-tractor-beam-of-the-database-in-an-api-world/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/dragon-shadows-black-white-outline.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/19/the-tractor-beam-of-the-database-in-an-api-world/">The Tractor Beam Of The Database In An API World</a></h3>
			<p><em>19 Oct 2017</em></p>
			<p>I’m an old database person. I’ve been working with databases since my first job in 1987. Cobol. FoxPro. SQL Server. MySQL. I have had a production database in my charge accessible via the web since 1998. I understand how databases are the center of gravity when it comes to data. Something that hasn’t changed in an API driven world. This is something that will make microservices in a containerized landscape much harder than some developers will want to admit. The tractor beam of the database will not give up control to data so easily, either because of technical limitations, business constraints, or political gravity. Databases are all about the storage and access to data. APIs are about access to data. Storage, and the control that surrounds it is what creates the tractor beam. Most of the reasons for control over the storage of data are not looking to do harm. Security. Privacy. Value. Quality. Availability. There are many reasons stewards of data want to control who can access data, and what they can do with it. However, once control over data is established, I find it often morphs and evolves in many ways, that can eventually become harmful to meaningful and beneficial access to data. Which is usually the goal behind doing APIs, but is often seen as a threat to the mission of data stewards, and results in a tractor beam that API related projects will find themselves caught up in, and difficult to ever break free of. The most obvious representation of this tractor beam is that all data retrieved via an API usually comes from a central database. Also, all data generated or posted via an API, also ends up within a database. The central database always has an appetite for more data, whether scaled horizontally or vertically. Next, it is always difficult to break off subsets of data into separate API-driven project, or prevent newly established ones from being pulled in,...[<a href="/2017/10/19/the-tractor-beam-of-the-database-in-an-api-world/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/19/importing-openapi-definition-to-create-an-api-with-aws-api-gateway/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/api-gateway/aws-api-gateway-create-new-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/19/importing-openapi-definition-to-create-an-api-with-aws-api-gateway/">Importing OpenAPI Definition To Create An API With AWS API Gateway</a></h3>
			<p><em>19 Oct 2017</em></p>
			<p>I’ve been learning more about AWS API Gateway, and wanted to share some of what I’m learning with my readers. The AWS API Gateway is a robust way to deploy and manage an API on the AWS platform. The concept of an API gateway has been around for years, but the AWS approach reflects the commoditization of API deployment and management, making it a worthwhile cloud API service to understand in more depth. With the acquisition or all the original API management providers in recent years, as well as Apigee’s IPO, API management is now a default element of major cloud providers. Since AWS is the leading cloud provider, AWS API Gateway will play a significant role into the deployment and management of a growing number of APIs we see. Using AWS API Gateway you can deploy a new API, or you can use it to manage an existing API–demonstrating the power of a gateway. What really makes AWS API Gateway reflect where things are going in the space, is the ability to import and define your API using OpenAPI. When you first begin with the new API wizard, you can upload or copy / paste your OpenAPI, defining the surface area of the API, no matter how you are wiring up the backend. OpenAPI is primarily associated with publishing API documentation because of the success of Swagger UI, and secondarily associated with generating SDKs and code samples. However, increasingly the OpenAPI specification is also being used to deploy and define aspects of API management, which is in alignment with the AWS API Gateway approach. I have server side code that will take an OpenAPI and generate the server side code needed to work with the database, and handle requests and responses using the Slim API framework. I’ll keep doing this for many of my APIs, but for some of them I’m going to be adopting an AWS API Gateway approach to help standardize API...[<a href="/2017/10/19/importing-openapi-definition-to-create-an-api-with-aws-api-gateway/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/19/adding-ping-events-to-my-webhooks-and-api-research/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-webhook-ping-events.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/19/adding-ping-events-to-my-webhooks-and-api-research/">Adding Ping Events To My Webhooks And API Research</a></h3>
			<p><em>19 Oct 2017</em></p>
			<p>I am adding another building block to my webhooks research out of Github. As I continue this work, it is clear that Gthub will continue to play a significant role in my webhook research and storytelling, because they seem to be the most advanced when it comes to orchestration via API and webhooks. I’m guessing this is a by-product of continuous integration (CI) and continuous deployment (CD), which Github is at the heart of. The API platforms that have embraced automation and orchestration as part of what they do, always have the most advanced webhook implementations, and provide the best examples of webhooks in action, which we can all consider as part of our operations. Today’s webhook building block is the ping event. “When you create a new webhook, we’ll send you a simple ping event to let you know you’ve set up the webhook correctly. This event isn’t stored so it isn’t retrievable via the Events API. You can trigger a ping again by calling the ping endpoint.” A pretty simple, but handy features when it comes to getting up and going with webhooks, making sure everything is working properly out of the gate–something that clearly comes from experience, and listening to the problems your consumers are encountering. These types of subtle webhook features are exactly the types of building blocks I’m looking to aggregate as part of my research. As I do with other areas of my research, is at some point I will publish all of these into a single, (hopefully) coherent guide to webhooks. After going through the webhook implementations across the leading providers like Github, I should have a wealth of common patterns in use. Since webhooks aren’t any formal standard, it is yet another aspect of doing business with APIs we have to learn from the health practices already in use across the space. It helps to emulate providers like Github, because developers are pretty familiar with how Github...[<a href="/2017/10/19/adding-ping-events-to-my-webhooks-and-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/18/most-api-developers-will-not-care-as-much-as-you-do/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/40_45_800_500_0_max_0_-5_-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/18/most-api-developers-will-not-care-as-much-as-you-do/">Most API Developers Will Not Care As Much As You Do</a></h3>
			<p><em>18 Oct 2017</em></p>
			<p>I believe in the potential of what APIs can do, and care about learning how we can do things right. Part of it is my job, but part of it is me wanting to do things well. Master my approach to delivering APIs, using my well-rounded API toolbox. Reading the approach of other leading API providers, and honing my understanding of healthy, and not so healthy practices. I thoroughly enjoy studying what is going on and then applying it across what I do. However I am reminded regularly that most people are not interested in knowing, and doing things right–they just want things done. As many of us discuss the finer details of API design, and the benefits of one approach over the other, other folks would rather us just point them to the solution that will work for them. They really don’t care about the details, or mastering the approach, they just want it to work in their situation. Whether it is the individual, the project or organization they are working in, the environment is just not conducive to learning, understanding, and growth. They are just interested in services and tools that can deliver the desired solution for as cheap as possible–free, and open source whenever available. You can see this reality playing out across the space. An example is OpenAPI (fka Swagger). It has largely been successful because of Swagger UI. Most people think OpenAPI is all about documentation, with their understanding reflecting the solution they delivered, not the full benefits brought to the table as part of the process of implementing the specification. This is just one example of how folks across the API space are interested in solutions, rather than the journey. This is why many API programs will stagnate and fail, because folks do them thinking they’ll achieve some easier way of doing things, easy integrations, effortless innovation, or some other myth around API Valhalla. I feel like much of...[<a href="/2017/10/18/most-api-developers-will-not-care-as-much-as-you-do/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/16/the-basics-of-api-management/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/42_16_600_400_0_max_1_1_1-5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/16/the-basics-of-api-management/">The Basics Of API Management</a></h3>
			<p><em>16 Oct 2017</em></p>
			<p>I am developing a basic API management strategy for one of my client’s API. With each area of their API strategy I am taking what I’ve learned monitoring the API sector, but pausing for a moment to think about again, and then applying to their operations. Over the years I have separated out many aspects of API management, distilling it down to a core set of elements that reflect the evolution of API management as its evolved into a digital commodity. It helps me to think through these aspects of API operations in general, but also applying to a specific API I am working on, helping me further refine my API strategy advice. API management is the oldest area of my research. It has spawned every other area of the lifecycle I track on, but also is the most mature aspect of the API economy. This project I am working on gives me an opportunity to think about what is API management, and what should be spun off into separate areas of concern. I am looking to distill API management down to: Portal - A single URL to find out everything about an API, and get up and running working the resources that are available. On-Boarding - Think just about how you get a new developer to from landing on the home page of the portal to making their first API call, and then an application in production. Accounts - Allowing API consumers to sign up for an account, either for individual, or business access to API resources. Applications - Enable each account holder to register one or many applications which will be putting API resources to use. Authentication - Providing one, or multiple ways for API consumers to authenticate and get access to API resources. Services - Defining which services are available across one or many API paths providing HTTP access to a variety of business services. Logging - Every call to the API...[<a href="/2017/10/16/the-basics-of-api-management/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/16/operating-your-api-portal-using-github/"><img src="https://s3.amazonaws.com/kinlane-productions2/jekyll/jekyllrb.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/16/operating-your-api-portal-using-github/">Operating Your API Portal Using Github</a></h3>
			<p><em>16 Oct 2017</em></p>
			<p>Operating on Github is natural for me, but I am regularly reminded what a foreign concept it is for some of the API providers I’m speaking with. Github is the cheapest, easiest way to launch a public or private developer portal for your API. With the introduction of Github Pages, each Github repository is turned into a place to host any API related project. In my opinion, every API should begin with Github, providing a place to put your API definition, portal, and other elements of your API operations. If you are just getting going with understand how Github can be used to support your API operations, I wanted to provide a simple checklist of the concepts at play, that will lead you being able to publish your API portal to Github. Github Account - You will need an account to be able to use Github. Anything you do on Github that is public will be free. You can do private portals on Github, but this story is about using it for a public API portal. Github Organization - I recommend starting an organization for your API operations, instead of under just a single users account. Then you can make the definition for the API the first repository, and possibly the portal your second repository you create. Github Repo - A Github repository is basically a folder on the platform which you can start the code, pages, and other content used as part of API operations. Github Pages - Each Github repository has the ability to turn on a public project site, which can be used as a hosting location for a developer portal. Jekyll - Github Pages allows any Github repository to become a website hosting location which you can access via your Github user account, or even provide an address using your own domain. I recommend every API provider think about hosting their API portal on Github. The learning curve isn’t that significant...[<a href="/2017/10/16/operating-your-api-portal-using-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/16/api-design-maturity-at-capital-one/"><img src="https://s3.amazonaws.com/kinlane-productions2/capital-one/capital-one-api-maturing-pyramid.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/16/api-design-maturity-at-capital-one/">API Design Maturity At Capital One</a></h3>
			<p><em>16 Oct 2017</em></p>
			<p>API design is something that many have tried to quantify and measure, but very few ever establish any meaningful way of doing so properly in my experience. I’ve been learning about the approach to API governance from the Capital One DevExchange team, and found their approach to defining API design maturity pretty interesting. I’m mostly interested in their approach because it speaks to actual business objectives, and aren’t about the common technical aspects we see API design being quantified across the community each day. Capital One breaks things down into five distinctive layers that offer value to any organization doing APIs. Starting at the bottom of their maturity period, here are the levels of maturity they are measuring things by: Functional - Doing the basics, providing some low-level functionality, and nothing more. Reliable - An API that is reliable, and scalable, beyond just basic functionality. Intuitive - Where an investment in developer experience is made, further standardizing and streamlining what an API does. Empowering - Where an API really begins to deliver value to an organization by being function, reliable, and intuitive, which all contributes significantly to operations. Transformative - APIs that are game changer. Few APIs ever rise to this level, but all should aspire to this level of maturity. It provides a whole other lens for looking at API design through. Moving beyond just, is it RESTful? Hypermedia, GraphQL, gRPC, or other emerging approach. It also understands that not all APIs will be equal, and that we should be standardizing the value the design of our APIs deliver to our business operations. Forcing us to ask some pretty simple questions about the API design patterns we are using, and the actual value they bring to the table. Moving beyond API design being about technical details, and considering the business, and other political aspects of doing APIs. The Capital One API design maturity definition also demonstrates another important aspect of all of this. That...[<a href="/2017/10/16/api-design-maturity-at-capital-one/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/16/adwords-api-release-and-sunset-schedule-for-2018/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/beach-rocks-currents_kand_two.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/16/adwords-api-release-and-sunset-schedule-for-2018/">AdWords API Release and Sunset Schedule For 2018</a></h3>
			<p><em>16 Oct 2017</em></p>
			<p>APIs are not forever, and eventually will go away. The trick with API deprecation is to communicate clearly, and regularly with API consumers, making sure they are prepared for the future. I’ve been tracking on the healthy, and not so healthy practices when it comes to API deprecation for some time now, but felt like Google had some more examples I wanted to add to our toolbox. Their approach to setting expectations around API deprecation is worthy of emulating, and making common practice across industries. The Google Adwords API team is changing their release schedule, which in turns impacts the number of APIs they’ll support, and how quickly they will be deprecating their APIs. They will be releasing new versions of the API three times a year, in February, June and September. They will also be only supporting two releases concurrently at all times, and three releases for a brief period of four weeks, pushing the pace of API deprecation alongside each release. I think that Google’s approach provides a nice blueprint that other API provides might consider adopting. Adopting an API release and sunset schedule helps communicate the changes on the horizon, but it also provides a regular rhythm that API consumers can learn to depend on. You just know that there will be three releases a year, and you have a quantified amount of time to invest in evolving integration before any API is deprecated. It’s not just the communication around the roadmap, it is about establishing the schedule, and establishing an API release and sunset cadence that API consumers can be in sync with. Something that can go a lot further than just publishing a road map, and tweeting things out. I’ll add this example to my API deprecation research. Unfortunately the topic is one that is widely communicated around in the API space, but Google has long a strong player when it comes to finding healthy API deprecation examples to follow....[<a href="/2017/10/16/adwords-api-release-and-sunset-schedule-for-2018/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/12/obfuscating-the-evolving-code-behind-my-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/christianity-under-construction_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/12/obfuscating-the-evolving-code-behind-my-api/">Obfuscating The Evolving Code Behind My API</a></h3>
			<p><em>12 Oct 2017</em></p>
			<p>I’m dialing in a set of machine learning APIs that I use to obfuscate and distort the images I use across my storytelling. The code is getting a little more hardened, but there is still so much work ahead when it comes to making sure it does exactly what I needed it to do, with only dials and controls I need–nothing more. I’m the only consumer of my APIs, which I use them daily, with updates made to the code along the way, evolving the request and response structures until they meet my needs. Eventually the APIs will be done (enough), and I’ll stop messing with them, but that will take a couple months more of pushing forward the code. While the code for these APIs are far from finished, I find the API helps obfuscate and diffuse the unfinished nature of things. The API maintains a single set of paths, and I might still evolve the number of parameters it accepts, and the fields it outputs, the overall will keep a pretty tight surface area despite the perpetually unfinished backend. I like this aspect of operating APIs, and how they can be used as a facade, allowing you to maintain one narrative on the front-end, even with another occurring behind behind the scenes. I feel like API facades really fit with my style of coding. I’m not really an A grade programmer, more a B- level one, but I know how to get things to work–if I have a polished facade, things look good. Honestly, I’m pretty embarrassed by my wrappers for TensorFlow. I’m still figuring everything out, and finding new ways of manipulating and applying Tensor Flow models, so my code is rapidly changing, maturing, and not always complete. When it comes to the API interface I am focused on not introducing breaking changes, and maintaining a coherent request and response structure for the image manipulation APIs. Even though the code will at...[<a href="/2017/10/12/obfuscating-the-evolving-code-behind-my-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/12/bots-voice-and-conversational-apis-are-your-next-generation-of-api-clients/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-conversational-interfaces.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/12/bots-voice-and-conversational-apis-are-your-next-generation-of-api-clients/">Bots, Voice, And Conversational APIs Are Your Next Generation Of API Clients</a></h3>
			<p><em>12 Oct 2017</em></p>
			<p>Around 2010, the world of APIs began picking up speed with the introduction of the iPhone, and then Android mobile platforms. Web APIs had been used for delivering data and content to websites for almost a decade at that point, but their potential for delivering resources to mobile phones is what pushed APIs into the spotlight. The API management providers pushed the notion of being multi-channel, and being able to deliver to web and mobile clients, using a common stack of APIs. Seven years later, web and mobile are still the dominant clients for API resources, but we are seeing a next generation of clients begin to get more traction, which includes voice, bot, and other conversational interfaces. If you deliver data and content to your customers via your website and mobile applications, the chance that you will also be delivering it to conversational interfaces, and the bots and assistants emerging via Alexa and Google Home, as well as on Slack, Facebook, Twitter, and other messaging platforms, is increasing. I’m not selling that everything will be done with virtual assistants, and voice commands in the near future, but as a client we will continue to see mainstream user adoption, and voice be used in automobiles, and other Internet connected devices emerging in our world. I am not a big fan of talking to devices, but I know many people who are. I don’t think Siri, Alexa, and Google Home will live up to the hype, but there is enough resources being invested into these platforms, and the devices that they are enabling, that some of it will stick. In the cracks, interesting things will happen, and some conversational interfaces will evolve and become useful. In other cases, as a consumer, you won’t be able to avoid the conversational interfaces, and be required to engage with bots, and use voice enabled devices. This will push the need to have conversationally literate APIs that can deliver data...[<a href="/2017/10/12/bots-voice-and-conversational-apis-are-your-next-generation-of-api-clients/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/12/air-an-asthma-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/propeller/air-asthma-api-text-phone-aa3.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/12/air-an-asthma-api/">Air, An Asthma API</a></h3>
			<p><em>12 Oct 2017</em></p>
			<p>You don’t find me showcasing specific APIs often. I’m usually talking about an API because of their approach to the technology, business, or politics of how they do APIs. It just isn’t my style to highlight APIs, unless I think they are interesting, and delivering value that is worth talking about, or possibly reflecting a meaningful trend that is going on. In this case it is a useful API that I think brings value, but also provides an example of an API I can showcase to non-developer folks as a meaningful example of an API. The API I’m talking about today, is the Air API, an asthma API from Propeller, which provides a set of free tools to help people understand current asthma conditions in their neighborhoods. The project is led by the Propeller data scientists and clinical researchers, looking to leverage Air API to help predict how asthma may be affected by local conditions, including a series of tools that share local asthma conditions, ranging from an email or text subscription, to an embeddable Air Widget for other websites. The Air API provides an easy to explain example of what is possible with APIs. Environmental APIs will continue to be an important aspect of doing APIs. Aggregating sensor and other data to help us understand the air, water, weather, and other critical environmental factors that impact our lives each day. I like the idea of these APIs being open and available to 3rd party developers to build tools on top of them, while the platforms using them as a marketing vehicle for their other products and services, while making sure to keep the valuable data accessible to everyone. I’ll put the Air API into my toolbox of APIs I use to help onboard folks with APIs. If they are impacted by asthma, or know someone who is, it helps make the personal connection, which can be important when on-boarding folks with the abstract concepts...[<a href="/2017/10/12/air-an-asthma-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/11/their-security-practices-are-questionable-but-their-communication-is/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/status-berlin_propaganda_leaflets.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/11/their-security-practices-are-questionable-but-their-communication-is/">Their Security Practices Are Questionable But Their Communication Is</a></h3>
			<p><em>11 Oct 2017</em></p>
			<p>I study the API universe every day of the week, looking for common patterns in the way people are using technology. I study almost 100 stops along the API lifecycle, looking for healthy practices that companies, organizations, institutions, and government agencies can follow when dialing in their API operations. Along the way I am also looking for patterns that aren’t so healthy, which are contributing to many of the problems we see across the API sector, but more importantly the applications and devices that they are delivering valuable data, content, media, and algorithms to. One layer of my research is centered around studying API security, which also includes keeping up with vulnerabilities and breaches. I also pay attention to cybersecurity, which is a more theatrical version of regular security, with more drama, hype, and storytelling. I’ve been reading everything I can on the Equifax, Accenture, and other scary breaches, and like the other areas of the industry I track on, I’m beginning to see some common patterns emerge. It is something that starts with the way we use (or don’t use) technology, but then is significantly amplified by the human side of things. There are a number of common patterns that contribute to these breaches on the technical side, such as not enough monitoring, logging, and redundancy in security practices. However, there are also many common patterns emerging from the business approach by leadership during security incidents, and breaches. These companies security practices are questionable, but I’d say the thing that is the most unacceptable about all of these is the communication around these security events. I feel like they demonstrate just how dysfunctional things are behind the scenes at these companies, but also demonstrate their complete lack of respect and concern for individuals who are impacted by these incidents. I am pretty shocked by seeing how little some companies are investing in API security. The lack of conversation from API providers about their security...[<a href="/2017/10/11/their-security-practices-are-questionable-but-their-communication-is/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/11/the-grpc-meetup-kit/"><img src="https://s3.amazonaws.com/kinlane-productions2/grpc/grpc-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/11/the-grpc-meetup-kit/">The gRPC Meetup Kit</a></h3>
			<p><em>11 Oct 2017</em></p>
			<p>I wrote about Tyk’s API surgery meetups last week, and adding a new approach to our API event and workshop toolbox, and next I wanted to highlight the gRPC Meetup Kit, a resource for creating your own gRPC event. gRPC is an approach out of Google for designing, delivering, and operating high performance APIs. If you look at the latest wave of APIs out of Google you’ll see they are all REST and/or gRPC. Most of them are dual speed, providing both REST and gRPC. gRPC is an open source initiative, but very much a Google led effort that we’ve seen picking up momentum in 2017. While I am keeping an eye on gRPC itself, this particular story is about the concept of providing a Meetup kit for your API related service or tooling, providing an “In a Box” solution that anyone can use to hold a Meetup. The gRPC team provides three groups of resources: gRPC 101 Presentation Talk - A 15 minute course introduction video. Slides - Slides that go along with the talk. Codelab - A 45m codelab that attendees can complete using Cloud Shell. Resources and community gRPC Website Codelab Building a gRPC service with Node.js Building a gRPC service with C# GitHub Source Extended gRPC Ecosystem Blog Youtube Channel Ask Questions Gitter Chat Google Group Stack Overflow Keep in Touch Twitter Request Support for Your Event gRPC Stickers Sign up for office hours with gRPC team It provides a nice blueprint for what is needed when crafting your own Meetup Kit as well as some material you could weave into any other type of Meetup, or workshop that might contain gRPC. Maybe an API design and protocol workshop, where you cover all of the existing approaches out there today like REST, Hypermedia, gRPC, GraphQL, and others. If nothing else the gRPC Meetup Kit provides a nice forkable project, that you could use as scaffolding for your own kit. As I...[<a href="/2017/10/11/the-grpc-meetup-kit/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/11/the-api-coaches-at-capital-one/"><img src="https://s3.amazonaws.com/kinlane-productions2/capital-one/21586757_10155715320589813_1876210064026688571_o.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/11/the-api-coaches-at-capital-one/">The API Coaches At Capital One</a></h3>
			<p><em>11 Oct 2017</em></p>
			<p>API evangelism and even advocacy at many organizations has always been a challenge to introduce, because many groups aren’t really well versed in the discipline, and often times it tends to take on a more marketing or even sales like approach, which can hurt its impact. I’ve worked with groups to rebrand, and change how they evangelize APIs internally, with partners, and the public, trying to ensure the efforts are more effective. While I still bundle all of this under my API evangelism research, I am always looking for new approaches that push the boundaries, and evolve what we know as API evangelism, advocacy, outreach, and other variations. I was introduced to a new variation of the internal API evangelism concept a few weeks back while at Capital One talking with my friend Matthew Reinbold(@libel_vox) about their approach to API governance. His team at the Capital One API Center of Excellence has the concept of the API coach, and I think Matt’s description from his recent API governance blueprint story sums it up well: At minimum, the standards must be a journey, not a destination. A key component to “selective standardization” is knowing what to select. It is one thing for us in our ivory tower to throw darts at market forces and team needs. It is entirely another to repeatedly engage with those doing the work. Our coaching effort identifies those passionate practitioners throughout our lines of business who have raised their hands and said, “getting this right is important to my teams and me”. Coaches not only receive additional training that they then apply to their teams. They also earn access to evolving our standards. In this way, standards aren’t something that are dictated to teams. Teams drive the standards. These aren’t alien requirements from another planet. They see their own needs and concerns reflected back at them. That is an incredibly powerful motivator toward acceptance and buy-in. A significant difference here between...[<a href="/2017/10/11/the-api-coaches-at-capital-one/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/11/provide-an-open-source-threat-information-database-and-api-then-sell-premium/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-door-lock_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/11/provide-an-open-source-threat-information-database-and-api-then-sell-premium/">Provide An Open Source Threat Information Database And API Then Sell Premium</a></h3>
			<p><em>11 Oct 2017</em></p>
			<p>I was doing some API security research and stumbled across vFeed, a “Correlated Vulnerability and Threat Intelligence Database Wrapper”, providing a JSON API of vulnerabilities from the vFeed database. The approach is a Python API, and not a web API, but I think provides an interesting blueprint for open source APIs. What I found interesting (somewhat) from the vFeed approach was the fact they provide an open source API, and database, but if you want a production version of the database with all the threat intelligence you have to pay for it. I would say their technical and business approach needs a significant amount of work, but I think there is a workable version of it in there. First, I would create a Python, PHP, Node.js, Java, Go, Ruby version of the API, making sure it is a web API. Next, remove the production restriction on the database, allowing anyone to deploy a working edition, just minus all the threat data. There is a lot of value in there being an open source set of threat intelligence sharing databases and API. Then after that, get smarter about having a variety different free and paid data subscriptions, not just a single database–leverage the API presence. You could also get smarter about how the database and API enables companies to share their threat data, plugging it into a larger network, making some of it free, and some of it paid–with revenue share all around. There should be a suite of open source threat information sharing databases and APIs, and a federated network of API implementations. Complete with a wealth of open data for folks to tap into and learn from, but also with some revenue generating opportunities throughout the long tail, helping companies fund aspects of their API security operations. Budget shortfalls are a big contributor to security incidents, and some revenue generating activity would be positive. So, not a perfect model, but enough food for thought...[<a href="/2017/10/11/provide-an-open-source-threat-information-database-and-api-then-sell-premium/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/10/explore-download-api-and-share-data/"><img src="https://s3.amazonaws.com/kinlane-productions2/nyc-open-data/nyc-opendata-explore-download-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/10/explore-download-api-and-share-data/">Explore, Download, API, And Share Data</a></h3>
			<p><em>10 Oct 2017</em></p>
			<p>I’m regularly looking through API providers, service providers, and open data platforms looking for interesting ways in which folks are exposing APIs. I have written about Kentik exposing the API call behind each dashboard visualization for their networking solution, as well as CloudFlare providing an API link for each DNS tool available via their platform. All demonstrating healthy way we can show how APIs are right behind everything we do, and today’s example of how to provide API access is out of New York Open Data, providing access to 311 service requests made available via the Socrata platform. The page I’m showcasing provides access 311 service requests from 2010 to present, with all the columns and meta data for the dataset, complete with a handy navigation toolbar that lets you view data in Carto or Plot.ly, download the full dataset, access via API, or simply share via Twitter, Facebook, or email. It is a pretty simple example of offering up multiple paths for data consumers to get what they want from a dataset. Not everyone is going to want the API. Depending on who you are you might go straight for the download, or opt to access via one of the visualization and charting tools. Depending on who you are targeting with your data, the list of tools might vary, but the NYC OpenData example via Socrata provides a nice example to build upon. With the most important message being do not provide only the options you would choose–get to know your consumers, and deliver solutions they will also need. It provides a different approach to making APIs behind available to users than the Kentik or CloudFlare approaches do, but it adds to the number of examples I have to show people how APIs and API enabled integration can be exposed through the UI, helping educate the massess about what is possible. I could see standardized buttons, drop downs, and other embeddable tooling emerge for...[<a href="/2017/10/10/explore-download-api-and-share-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/10/connecting-service-level-agreements-to-api-monitoring/"><img src="https://s3.amazonaws.com/kinlane-productions2/apimetrics/api-metrics-latency-sla.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/10/connecting-service-level-agreements-to-api-monitoring/">Connecting Service Level Agreements To API Monitoring</a></h3>
			<p><em>10 Oct 2017</em></p>
			<p>Monitoring your API availability should be standard practice for internal and external APIs. If you have the resources to custom build API monitoring, testing, and performance infrastructure, I am guessing you already have some pretty cool stuff in place. If you don’t, then you should not be reinventing the wheel out there, and you should be leveraging one of the existing API monitoring services out there on the market. When you are getting started with monitoring your APIs I recommend you begin with uptime and downtime, and once you deliver successfully on that front, I recommend you work on API performance, and the responsiveness of your APIs. You should begin with making sure you are delivering the service level agreement you have in place with your API consumers. What, you don’t have a service level agreement? No better time to start than now. If you don’t already have an explicitly stated SLA in place, I recommend creating one internally, and see what you can do to live up to your API SLA, then once you ensure things are operating at acceptable levels, you share with your API consumers. I am guessing they will be pretty pleased to hear that you are taking the initiative to offer an SLA, and are committed enough to your API to work towards such a high bar for API operations. To help you manage defining, and then ultimately monitoring and living up to your API SLA, I recommend taking a look at APIMetrics, who is obsessively focused on API quality, performance, and reliability. They spend a lot of time monitoring public APIs, and have developed a pretty sophisticated approach to ranking and scoring your API to ensure you meet your SLA. As you can see in the picture for this story, the APIMetrics administrative dashboard provides a pretty robust way for you to measure any API you want, and establish metrics and triggers that let you know if you’ve met...[<a href="/2017/10/10/connecting-service-level-agreements-to-api-monitoring/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/10/algorithmic-observability-should-work-like-machine-readable-food-labels/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorithms/new-food-labels.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/10/algorithmic-observability-should-work-like-machine-readable-food-labels/">Algorithmic Observability Should Work Like Machine Readable Food Labels</a></h3>
			<p><em>10 Oct 2017</em></p>
			<p>I’ve been doing a lot of thinking about algorithmic transparency, as well as a more evolved version of it I’ve labeled as algorithmic observability. Many algorithmic developers feel their algorithms should remain black boxes, usually due to intellectual property concerns, but in reality the reasons will vary. My stance is that algorithms should be open source, or at the very least have some mechanisms for auditing, assessing, and verifying that algorithms are doing what they promise, and that algorithms aren’t doing harm behind the scenes. This is a concept I know algorithm owners and creators will resist, but algorithms observability should work like food labels, but work in a more machine readable way, allowing them to be validated by other external (or internal) systems. Similar to food you buy in the store, you shouldn’t have to give away the whole recipe and secret sauce behind your algorithm, but there should be all the relevant data points, inputs, outputs, and other “ingredients” or “nutrients” that go into the resulting algorithm. I talked about algorithm attribution before, and I think there should be some sort of algorithmic observability manifest, which provides the “label” for an algorithm in a machine readable format. It should give all the relevant sources, attribution, as well as input and outputs for an algorithm–with different schema for different industries. In addition to there being an algorithmic observability “label” available for all algorithms, there should be live, or at least virtualized, sandboxed instances of the algorithm for verification, and auditing of what is provided on the label. As we saw with the Volkswagen emissions scandal, algorithm owners could cheat, but it would provide an important next step for helping us understand the performance, or lack of performance when it comes to the algorithms we are depending on. Why I call this algorithmic observability, instead of algorithmic transparency, is each algorithm should be observable using it’s existing inputs and outputs (API), and not just be...[<a href="/2017/10/10/algorithmic-observability-should-work-like-machine-readable-food-labels/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/10/a-guest-blogger-program-to-create-unique-content-for-your-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/runscope/runscope-featured-guest-series.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/10/a-guest-blogger-program-to-create-unique-content-for-your-api/">A Guest Blogger Program To Create Unique Content For Your API</a></h3>
			<p><em>10 Oct 2017</em></p>
			<p>Creating regular content for your blog is essential to maintaining a presence. If you don’t publish regularly, and refresh your content, you will find your SEO, and wider presence quickly becoming irrelevant. I understand that unlike me, many of you have jobs, and responsibilities when it comes to operating your APIs, and carving out the time to craft regular blog posts can be difficult. To help you in your storytelling journey I am always looking for other stories to help alleviate your pain, while helping keep your blog active, and ensure folks will continue stumbling across your API, or API service, while Google, or on social media. Another interesting example of how to keep your blog fresh came from my partners over at Runscope, who conducted a featured guest blog post series, where they were paying API community leaders to help “create an incredible resource of blog posts about APIs, microservices, DevOps, and QA.” Which has produced a handful of interesting posts: Monolith to Microservices: Transforming a web-scale, real-world e-commerce platform using the Strangler Pattern You Might Not Need GraphQL 3 Easy Steps to Cloud Operational Excellence Building a Steam Powered IoT API with Thingsboard One thing to note is that Runscope paid $500.00 per post to help raise the bar when it comes to the type of author that will step up for such an effort. I’ve seen companies try to do this before, offering gift cards, swag, and even nothing in return, with varying grades of success and failure. I’m not saying a guest author program for your blog will always yield the results you are looking for, but it is a good way to help build relationships with your community, and help augment your existing workload, with some regular storytelling on the blog. A guest blogger program is a tool I will be adding to my API communications research, expanding on the tools API operators have in their toolbox to keep their...[<a href="/2017/10/10/a-guest-blogger-program-to-create-unique-content-for-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/09/treating-your-apis-like-they-are-infrastructure/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripe-api-versioning.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/09/treating-your-apis-like-they-are-infrastructure/">Treating Your APIs Like They Are Infrastructure</a></h3>
			<p><em>09 Oct 2017</em></p>
			<p>We all (well most of us) strive to deliver as stable of an API presence as we possibly can. It is something that is easier said than done. It is something that takes caring, as well as the right resources, experience, team, management, and budget to do APIs just right. It is something the API idols our there make look easy, when they really have invested a lot of time and energy into developing a agile, yet scalable approach to ensuring APIs stay up and running. Something that you might able to achieve with a single API, but can easily be lost between each API version, as we steer the ship forward. I spend a lot of time at the developer portals of these leading API providers looking for interesting insight into how they are operating, and I though Stripe’s vision around versioning their API is worth highlighting. Specifically their quote about treating your API like they are real life physical infrastructure. “Like a connected power grid or water supply, after hooking it up, an API should run without interruption for as long as possible.Our mission at Stripe is to provide the economic infrastructure for the internet. Just like a power company shouldn’t change its voltage every two years, we believe that our users should be able to trust that a web API will be as stable as possible.” This is possible. This is how I view Amazon S3, and Pinboard. These are two APIs I depend on to make my business work. Storage and bookmarking are two essential resources in my world, and both these APIs have consistently delivered stable API infrastructure, that I know I can depend on. I think it is also interesting to note that one is a tech giant, while the other is a viable small business (not startup). Demonstrating for me that there isn’t a single path to being a reliable, stable, API provider, despite what some folks might...[<a href="/2017/10/09/treating-your-apis-like-they-are-infrastructure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/09/publishing-your-api-road-map-using-trello/"><img src="https://s3.amazonaws.com/kinlane-productions2/tyk/tyk-road-map.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/09/publishing-your-api-road-map-using-trello/">Publishing Your API Road Map Using Trello</a></h3>
			<p><em>09 Oct 2017</em></p>
			<p>I consider a road map for any API to be an essential building block, whether it is a public API or not. You should be in the business of planning the next steps for your API in an organized way, and you should be sharing that with your API consumers so that they can stay up to speed on what is right around the corner. If you want to really go the extra mile I recommend following what Tyk is up to, with their public road map using Trello. With the API management platform Tyk, you don’t just see a listing of their API road map, you see all the work and conversation behind the road ma using the visual collaboration platform Trello. Using their road map you can see proposed features, which is great to see if something you want has already been suggested, and you can get at a list of what the next minor releases will contain. Plus using the menu bar you can get at a history of the changes the Tyk team has made to the platform, going back for the entire history of the Trello board. Using Trello you can subscribe to, or vote up any of the message boards. If you want to submit something you need to sign-up and post something to the Tyk community. Then they’ll consider adding it to the proposed road map features. It is a pretty low cost, easy to make public, approach to delivering a road map. Sometimes this stuff doesn’t need a complex solution, just one that provides some transparency, and help your customers understand what is next. Tyk provides a nice way to provide a road map that any other API provider, or service provider can follow. Another interesting approach to delivering an API road map that I can add to my research. I’m a big fan of having many different ways of delivering the essential building blocks of API...[<a href="/2017/10/09/publishing-your-api-road-map-using-trello/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/09/learning-about-api-governance-from-capital-one-devexchange/"><img src="https://cdn-images-1.medium.com/max/800/0*DOAJXcR7VcCN3iOh.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/09/learning-about-api-governance-from-capital-one-devexchange/">Learning About API Governance From Capital One DevExchange</a></h3>
			<p><em>09 Oct 2017</em></p>
			<p>I am still working through my notes from a recent visit to Capital One, where I spent time talking with Matthew Reinbold (@libel_vox) about their API governance strategy. I was given a walk through their approach to defining API standards across groups, as well as how they incentivize, encourage, and even measure what is happening. I’m still processing my notes from our talk, and waiting to see Matt publish more on his work, before I publish too many details, but I think it is worth looking at from a high level view, setting the bar for other API governance conversations I am engaging in. First, what is API governance. I personally know that many of my readers have a lot of misconceptions about what it is, and what it isn’t. I’m not interesting in defining a single definition of API governance. I am hoping to help define it so that you can find it a version of it that you can apply across your API operations. API governance is at its simplest form, about ensuring consistency in how you do API across your development groups, and a more robust definition might be about having an individual or team dedicated to establishing organization-wide API standards, helping train, educate, enforce, and in the case of capital one, measure their success. Before you can begin thinking about API governance, you need to start establishing what your API standards are. In my experience this usually begins with API design, but should also quickly also be about consistent, API deployment, management, monitoring, testing, SDKs, clients, and every other stop along the API lifecycle. Without well-defined, and properly socialized API standards, you won’t be able to establish any sort of API governance that has any sort of impact. I know this sounds simple, but I know more API providers who do not have any kind of API design, or other guide for their operations, than I know API providers who have...[<a href="/2017/10/09/learning-about-api-governance-from-capital-one-devexchange/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/09/communication-strategy-filler-using-sections-of-your-api-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions2/aws/aws-tweet-documentation-links.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/09/communication-strategy-filler-using-sections-of-your-api-documentation/">Communication Strategy Filler Using Sections Of Your API Documentation</a></h3>
			<p><em>09 Oct 2017</em></p>
			<p>&lt;/a&gt; Coming up with things creative things to write about regularly on the blog, and on Twitter when you are operating an API is hard. It has taken a lot of discipline to keep posts going up on API Evangelist regularly for the last seven years–totaling almost 3K total stories told so far. I don’t expect every API provider to have the same obsessive compulsive disorder that I do, so I’m always looking for innovative things that they can do to communicate with their API communities–something that Amazon Web Services is always good at providing healthy examples that I feel I can showcase. One thing the AWS team does on a regular basis is tweeting out links to specific areas of their documentation, that helps users accomplish specific things with AWS APIs. The AWS security team is great at doing this, with recent examples focusing on securing things with the AWS Directory Service, and API Organizations. Each contains a useful description, attractive looking image, and a link to a specific page in the documentation that helps you learn more about what is possible. I have been pushing myself to make sure all headers, and sub headers in my API documentation have anchors, so that I can not just link to a specific page, but I can link to a specific section, path, or other relevant item within my API documentation. This helps me in my storytelling when I’m looking to reference specific topics, and would help when it comes to tweeting out regular elements across my documentation in tweets. I’m slowly going to push out some of the lower grade tweets of curated news that I push out, and replace with relevant work I do in specific areas of my research–using my own work to fill the cracks over less than exciting things I may come across in the API space. Tweeting out what is possible with your API, with links to specific sections of...[<a href="/2017/10/09/communication-strategy-filler-using-sections-of-your-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/06/thinking-about-why-we-rate-limit-our-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/power-lines-empty-space_sunday.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/06/thinking-about-why-we-rate-limit-our-apis/">Thinking About Why We Rate Limit Our APIs</a></h3>
			<p><em>06 Oct 2017</em></p>
			<p>I am helping a client think through their API management solution at the moment, so I’m working through all the moving parts of how, and why of API management solutions. The API management landscape has shifted since the last time I helped a small company navigate the process of getting up and running, so I wanted to work through each aspect and think critically before I make any recommendations. My client has a content API, which isn’t very complex, but possesses some pretty valuable data they’ve aggregated, curated, and are looking to make available via a simple web API. It is pretty clear that all developers will need a key to be access the API, but I wanted to pause for a moment and think more about API rate limiting. Why do we rate limit? The primary reason is to help manage the compute resources available for all API consumers. You don’t want any single user hitting the server too hard, and taking things down for everyone else. I’d say after that, the next major reason is to enforce API access tiers, and ensure API consumers are only consuming what they should be. Which both seem like pretty dated concepts, that might need re-evaluation in general, but also in the context of this particular project. There is no free access to this API. I believe there will be a public account for test driving (making very limited # of calls), and some that drive their embeddable strategy, but for access to the majority of content, developers will have to register for a key, and provide a credit card to pay for their consumption. Which leaves me with the question, should we be rate limiting at all? If users are paying for whatever they consume, and there is a credit card on file, do we want to rate limit? Why are we so worried about server capacity in a cloud world? It seems like rate limiting...[<a href="/2017/10/06/thinking-about-why-we-rate-limit-our-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/06/the-api-management-landscape-has-shifted-more-than-i-anticipated/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/yellow-tree-in-the-rocks.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/06/the-api-management-landscape-has-shifted-more-than-i-anticipated/">The API Management Landscape Has Shifted More Than I Anticipated</a></h3>
			<p><em>06 Oct 2017</em></p>
			<p>It is interesting to take a fresh look at the API management landscape these days. It has been a while since I’ve looked through all the providers to see where their pricing is at, and what they offer. I’d say the space has definitely shifted from what things looked like 2012 through 2015. There are still a number of open source offerings, which there weren’t in 2012, but the old guard has solidly turned their attention to the enterprise. There are the cloud solutions like Restlet, ad SlashDB which really help you get up and running from existing data sources in the cloud, but for this particular project I am looking for a simple proxy and connector approach to deploying on any infrastructure, and they don’t quite fit the requirements. Apigee, and the other more enterprise offerings have always been out of my league, and 3Scale’s entry level package is up to $750, which is a little out of my reach, but I do know they are open sourcing their offering, now that they are part of Red Hat. There is API Umbrella, APIMan, Fusio, Monarch, and handful of other solutions that will work, but they take certain platform, or specific language commitment that doesn’t work for this project. Everything else is of the enterprise caliber, nothing really that I would recommend to my customers who are just getting started on their API journey. I’m really left with the cloud giants, which I guess is one of the main reasons we are at this junction in the evolution of API management. API management becoming a commodity has really shifted the landscape, making it more difficult to be a strong player like Tyk and Kong are managing to pull off. If my customer was looking to launch a data API from an existing database I’d point them to SlashDb or Restlet. If they are an enterprise customer I’d point them to 3Scale. Tyk is pretty much...[<a href="/2017/10/06/the-api-management-landscape-has-shifted-more-than-i-anticipated/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/06/publish-share-monetize-machine-learning-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/azure/azure-publish-share-monetize-ml.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/06/publish-share-monetize-machine-learning-apis/">Publish, Share, Monetize Machine Learning APIs</a></h3>
			<p><em>06 Oct 2017</em></p>
			<p>I’ve been playing with Tensor Flow for over a year now, specifically when it comes to working with images and video, but it has been something that has helped me understand what things looks like behind the algorithmic curtain that seems to be part of a growing number of tech marketing strategies right now. Part of this learning is exploring beyond Google’s approach, who is behind Tensor Flow, and understand what is going on at AWS, as well as Azure. I’m stil getting my feet wet learning about what Microsoft is up to with their platform, but I did notice one aspect of the Azure Machine Learning Studio emphasized developers to, “publish, share, monetize” their ML models. While I’m sure there will be a lot of useless vapor ware being sold within this realm, I’m simply seeing it as the next step in API monetization, and specifically the algorithmic evolution of being an API provider. As the label says in the three ML models for sale in the picture, this is all experimental. Nobody knows what will actually work, or even what the market will bear. However, this is something APIs, and the business of APIs excel at. Making a digital resource available to consumers in a retail, or even wholesale way via marketplaces like Azure and AWS, then playing around with features, pricing, and other elements, until you find the sweet spot. This is how Amazon figured out the whole cloud computing game, and became the leader. It is how Twilio, Stipe and other API as a product companies figured out what developers needed, and what these markets would bear. This will play out in marketplaces like Azure and Google, as well as startup players like Algorithmia–which is where I’ve been cutting my teeth, and learning about ML. The challenge for ML API entrepreneurs will be helping consumers understand what their models do, or do not do. I see it as an opportunity, because...[<a href="/2017/10/06/publish-share-monetize-machine-learning-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/digital-bits-capital-dc-flag-side-view.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/">A Couple More Questions For The Equifax CEO About Their Breach</a></h3>
			<p><em>06 Oct 2017</em></p>
			<p>Speaking to the House Energy and Commerce Committee, former Equifax CEO Richard Smith pointed the finger at a single developer who failed to patch the Apache Struts vulnerability. Saying that protocol was followed, and a single developer was responsible, shifting the blame away from leadership. It sounds like a good answer, but when you operate in the space you understand that this was a systemic failure, and you shouldn’t be relying on a single individual, or even a single piece of scanning software to verify the patch was applied. You really should have many layers in place to help prevent breaches like we saw with Equifax. If I was interviewing the CEO, I’d have a few other questions for him, getting at some of the other systemic and process failures based upon his lack of leadership, and awareness: API Monitoring &amp; Testing - You say the scanner for the Apache Struts vulnerability failed, but what about other monitoring and testing. The plugin in questions was a REST plugin, that allowed for API communication with your systems. Due to the vulnerability, extra junk information was allowed to get through. Where were your added API request and response integrity testing and monitoring process? Sure you were scanning for the vulnerability, but are you keeping an eye on the details of the data being passed back and forth? API monitoring &amp; testing has been around for many years, and service providers like Runscope do this for a living. What other layers of monitoring and testing were in place? API Management - When you expose APIs like you did from Apache Struts, what does the standardized management approach look like? What sort of metering, logging, rate limiting, and analysis occurs on each endpoint, and verification occurs, ensuring that only required clients should have access? API management has been standard procedure for over a decade now for exposing APIs like this both internally and externally. Why didn’t your API management...[<a href="/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/"><img src="ttps://s3.amazonaws.com/kinlane-productions2/holmes-county/holmes-county-listing.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/">Teaching My Client Three Approaches To Modular UI Design Using Their APIs</a></h3>
			<p><em>05 Oct 2017</em></p>
			<p>I am working with a client to develop a simple user interface on top of a Human Services Data API (HSDA) I launched for them. They want a basic website for searching, browsing, and navigating the organizations, locations, and services available in their API. A part of this work is helping them understand how modular and configurable their web site is, with each page, or portion of a page being a simple API call. It is taking a while for them to fully understand what they have, and the potential of evolving a web application in this way, but I feel like they are beginning to understand, and are taking the reigns a little more when it comes to dictate what they want within this new world. When I first published a basic listing of human services they were disappointed. They had envisioned a map of the listings, allowing users to navigate in a more visual way. I got to work helping them see the basic API call(s) behind the listing, and how we could use the JSON response in any way we wanted. I am looking to provide three main ways in which I can put the API data to work in a variety of web applications: Server-Side - A pretty standard PHP call to the API, taking the results and rendering to the page using HTML. Client-Side - Leveraging JavaScript in the browser to call the API and then render to the page using Jquery. Static Push - Calling the APPI using PHP, then publishing as YAML or JSON to a Jekyll site and rendering with Liquid and HTML. What the resulting HTML, CSS, and JavaScript looks like in all these scenarios is up to the individual who is in charge of dictating the user experience. In this case, it is my client. They just aren’t used to having this much control over dictating the overal user experience. Which path they choose depends...[<a href="/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/05/show-the-api-call-behind-each-dashboard-visualization/"><img src="https://s3.amazonaws.com/kinlane-productions2/kentik/kentik_API_menu-300w.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/05/show-the-api-call-behind-each-dashboard-visualization/">Show The API Call Behind Each Dashboard Visualization</a></h3>
			<p><em>05 Oct 2017</em></p>
			<p>I am a big fan of user interfaces that bring APIs out of the shadows. Historically, APIs are often a footnote in the software as a service (SaaS) world, available as a link way down at the bottom of the page, in the settings, or help areas. Rarely, are APIs made a first class citizen in the operations of a web application, which really just perpetuates the myth that APIs aren’t for everybody, and the “normals” shouldn’t worry their little heads about it. When in reality, EVERYBODY should know about APIs, and have the opportunity to put them to work, so we should stop burying the links to our APIs, and our developer areas. If your API is too technical for a business user to understand what is going on, then you should probably get to work simplifying it, not burying it and keeping it in developer and IT realm. I have written before about how DNS provider CloudFlare provides an API behind every feature in their user interface, and I’ve found another great example of this over at the network API provider Kentik. In their network dashboard visualization tooling they provide a robust set of tooling for accessing the data behind the visuals, allowing you to export, view SQL, show API call, and enter share view. In their post, they proceed to instruction about how you can get your API key as part of your account, as well as providing a pretty robust introduction into why APIs are important. This is how ALL dashboards should work in my opinion. Any user should be introduced to APIs, and have the ability to get at the data behind, and export it, or directly make an API call in their browser or at the command line. Developers like to think this stuff should be out of reach of the average user, but that is more about our own insecurities, and power trips, than it is about the...[<a href="/2017/10/05/show-the-api-call-behind-each-dashboard-visualization/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/05/how-api-evangelist-works/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/crypto-machine-bletchley_copper_circuit.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/05/how-api-evangelist-works/">How API Evangelist Works</a></h3>
			<p><em>05 Oct 2017</em></p>
			<p>I’ve covered this topic several times before, but I figured I’d share again for folks who might have just become readers int he last year. Providing an overview of how API Evangelist works, to help eliminate confusion as you are navigating around my site, as well as to help you find what you are looking for. First, API Evangelist was started in the summer of 2010 as a research site to help me better understand what is going on in the world of APIs. In 2017, it is still a research site, but it has grown and expanded pretty dramatically into a network of websites, driven by a data and a content core. The most import thing to remember is that all my sites run on Github, which is my workbench in the the API Evangelist workshop. apievangelist.com is the front door of the workshop, with each area of my research existing as its own Github repository, at its own subdomain with the apievangelist domain. An example of this can be found in my API design research, where you will find at design.apievangelist.com. As I do my work each day, I publish my research to each of my domains, in the form of YAML data for one of these areas: Organizatons - Companies, organizations, institutions, programs, and government agencies doing anything interesting with APIs. Individuals - The individual people at organizations, or independently doing anything interesting with APIs. News - The interesting API related, or other news I curate and tag daily in my feed reader or as I browse the web. Tools - The open source tooling I come across that I think is relevant to the API space in some way. Building Blocks - The common building blocks I find across the organizations, and tooling I’m studying, showing the good and the bad of doing APIs. Patents - The API related patents I harvest from the US Patent Office, showing how IP is...[<a href="/2017/10/05/how-api-evangelist-works/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/05/big-data-is-not-about-access-using-web-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/dragon_close-up_yellow_collage.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/05/big-data-is-not-about-access-using-web-apis/">Big Data Is Not About Access Using Web APIs</a></h3>
			<p><em>05 Oct 2017</em></p>
			<p>I’m neck deep in research around data and APIs right now, and after looking at 37 of the Apache data projects it is pretty clear that web APIs are not a priority in this world. There are some of the projects that have web APIs, and there a couple projects that look to bridge several of the projects with an aggregate or gateway API, but you can tell that the engineers behind the majority of these open source projects are not concerned with access at this level. Many engineers will counter this point by saying that web APIs can’t handle the volume, and it shows that the concept isn’t applicable in all scenarios. I’m not saying web APIs should be used for the core functionality at scale, I’m saying that web APIs should be present to provide access to the result state of the core features for each of these platform, whatever that is, which something that web APIs excel at. From my vantage point the lack of web APIs isn’t a technical one, it is a business and political motivation. When it comes to big data the objectives are always about access, and it definitely isn’t about the wide audience access that comes when you use HTTP, and the web for API access. The objective is to aggregate, move around, and work with as much data as you possibly can amongst a core group of knowledgable developers. Then you distribute awareness, access, and usage to designated parties via distilled analysis, visualizations, or in some cases to other systems where the result can be accessed and put to use. Wide access to this data is not the primary objective, paying forward much of the power and control we currently see around database to API efforts. Big data isn’t about democratization. Big Data is about aggregating as much as you can and selling the distilled down wisdom from analysis, or derived as part of machine learning...[<a href="/2017/10/05/big-data-is-not-about-access-using-web-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/top-aws-iam-documentation-pages-so-far-in-2017.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your/">Sharing Top Sections From Your API Documentation As Part Of Your</a></h3>
			<p><em>04 Oct 2017</em></p>
			<p>I’m always learning from the API communication practices from out of the different AWS teams. From the regular storytelling coming out of the Alexa team, to the mythical tales of leadership at AWS that have contributed to the platform’s success, the platform provides a wealth of examples that other API providers can emulate. As I talked about last week, finding creative ways to keep publishing interesting content to your blog as part of your API evangelism and communications strategy is hard. It is something you have to work at. One way I find inspiration is by watching the API leaders, and learning from what they do. An interesting example I recently found out of the AWS security team, was their approach to showcasing the top 20 AWS IAM documentation pages so far in 2017. It is a pretty simple, yet valuable way to deliver some content for your readers, that can also help you expose the dark corners of your API documentation, and other resources on your blog. The approach from the AWS security team is a great way to generate content without having to come up with good ideas, but also will help with your SEO, especially if you can cross publish, or promote through other channels. It’s pretty basic content stuff, that helps with your overall SEO, and if you play it right, you could also get some SMM juice by tweeting out the store, as well as maybe a handful of the top links from your list. It is pretty listicle type stuff, but honestly if you do right, it will also deliver value. These are the top answers, in a specific category, that your API consumers are looking for answers in. Helping these answers rise to the top of your blog, search engine, and social media does your consumers good, as well as your platform. One more tool for the API communications and evangelism toolbox. Something you can pull out when...[<a href="/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/flower-barbed-wire_clean_view.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/">Letting Go In An API World Is Hard To Do</a></h3>
			<p><em>04 Oct 2017</em></p>
			<p>I encounter a number of folks who really, really, really want to do APIs. You know, because they are cool and all, but they just can’t do what it takes to let go a little, so that their valuable API resources can actually be put to use by other folks. Sometimes this happens because they don’t actually own the data, content, or algorithms they are serving up, but in other cases it is because they view their thing as being so valuable, and so important that they can’t share it openly enough, to be accessible via an API. Even if your APIs are private, you still have to document, and share access with folks, so they can understand what is happening, and have enough freedom to put to use in their application as part of their business, without too much constraint and restrictions. Some folks tell me they want to do API, but I can usually tell pretty quickly that they won’t be able to go the distance. I find a lot of this has to do with perceptions of intellectual property, combined with a general distrust of EVERYONE. My thing is super valuable, extremely unique and original, and EVERYONE is going to want it. Which is why they want to do APIs, because EVERYONE will want it. Also, once it is available to EVERYONE via an API, competitors, and people we don’t want getting at it, will now be able to reverse engineer, and copy this amazing idea. However, if we don’t make accessible, we can’t get rich. Dilemna. Dilemna. Dilemna. What do we do? My answer is you probably that you shouldn’t be doing APIs. You see, doing APIs, whether public or privately requires letting go a bit. Sure, you can dial in how much control you are willing to give up using API management solutions, but you still have to let go enough so that people can do SOMETHING with your valuable...[<a href="/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/04/apis-used-to-give-us-access-to-resources-that-were-out-of-our-reach/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/mountainlake/clean_view/file-00_00_58_86.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/04/apis-used-to-give-us-access-to-resources-that-were-out-of-our-reach/">APIs Used To Give Us Access To Resources That Were Out Of Our Reach</a></h3>
			<p><em>04 Oct 2017</em></p>
			<p>I remember when almost all the APIs out there gave us developers access to things we couldn’t ever possibly get on our own. Some of it was about the network effect with the early Amazon and eBay marketplaces, or Flickr and Delicious, and then Twitter and Facebook. Then what really brought it home was going beyond the network effect, and delivering resources that were completely out of our reach like maps of the world around us, (seemingly) infinitely scalable compute and storage, SMS, and credit card payments. In the early days it really seemed like APIs were all about giving us access to something that was out of our reach as startups, or individuals. While this still does exist, it seems like many APIs have flipped the table and it is all about giving them access to our personal and business data in ways that used to be out of their reach. Machine learning APIs are using parlour tricks to get access to our internal systems and databases. Voice enablement, entertainment, and cameras are gaining access to our homes, what we watch and listen to, and are able to look into the dark corners of our personal lives. Tinder, Facebook, and other platforms know our deep dark secrets, our personal thoughts, and have access to our email and intimate conversations. The API promise seems to have changed along the way, and stopped being about giving us access, and is now about giving them access. I know it has always been about money, but the early vision of APIs seemed more honest. It seemed more about selling a product or service that people needed, and was more straight up. Now it just seems like APIs are invasive. Being used to infiltrate our professional and business worlds through our mobile phones. It feels like people just want access to us, purely so they can mine us and make more money. You just don’t see many Flickrs, Google...[<a href="/2017/10/04/apis-used-to-give-us-access-to-resources-that-were-out-of-our-reach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/">API Providers Should Provide Observability Into Government Developer Accounts</a></h3>
			<p><em>04 Oct 2017</em></p>
			<p>I’ve talked about this before, but after reading several articles recently about various federal government agencies collecting, and using social media accounts for surveillance lately, it is a drum I will be beating a lot more regularly. Along with the transparency reports we are beginning to see emerge from the largest platform providers, I’d like to start seeing more observability regarding which accounts, both user and developer are out of government agencies. Some platforms are good at highlighting how government of all shapes and sizes are using their platform, and some government agencies are good at showcasing their social media usage, but I’d like to understand this from purely an API developer account perspective. I’d like to see more observability into which government agencies are requesting API keys. Maybe not specific agencies ad groups, and account details, although that would be a good idea as well down the road. I am just looking for some breakdown of how many developer accounts on a platform are government and law enforcement. What does their API consumption look like? If there is Oauth via a platform, is there any bypassing of the usual authentication flows to get at data, any differently than regular developers would be accessing, or requiring user approval? From what I am hearing, I’m guessing that there are more government accounts out there than platforms either realize, or are willing to admit. It seems like now is a good time to start asking these questions. I would add on another layer to this. If an application developer is developing applications on behalf of law enforcement, or as part of a project for a government agency, there should be some sort of disclosure at this level as well. I know I’m asking a lot, and a number of people will call me crazy, but with everything going on these days, I’m feeling like we need a little more disclosure regarding how government(s) are using our platforms,...[<a href="/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/03/temporal-logic-of-actions-for-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/status-berlin_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/03/temporal-logic-of-actions-for-apis/">Temporal Logic of Actions For APIs</a></h3>
			<p><em>03 Oct 2017</em></p>
			<p>I’m evolving forward my thoughts on algorithmic observability and transparency using APIs, and I was recently introduced to TLA+, or the Temporal Logic of Actions. It is the closest I’ve come to what I’m seeing in my head when I think about how we can provide observability into algorithms through existing external outputs (APIs). As I do with all my work here on API I want to process TLA+ as part of my API research, and see how I can layer it in with what I already know. TLA+ is a formal specification language developed by Leslie Lamport, which can be used to design, model, document, and verify concurrent systems. It has been described as exhaustively-testable pseudocode which can provide a blueprint for software systems. In the context of design and documentation, TLA+ can be viewed as informal technical specifications. However, since TLA+ specifications are written in a formal language of logic and mathematics it can be used to uncover design flaws before system implementation is underway, and are amenable to model checking for finding all possible system behaviours up to some number of execution steps, and examines them for violations. TLA+ specifications use basic set theory to define safety (bad things won’t happen) and temporal logic to define liveness (good things eventually happen). TLA+ specifications are organized into modules.Although the TLA+ standard is specified in typeset mathematical symbols, existing TLA+ tools use symbol definitions in ASCII, using several terms which require further definition: State - an assignment of values to variables Behaviour - a sequence of states Step - a pair of successive states in a behavior Stuttering Step - a step during which variables are unchanged Next-State Rlation - a relation describing how variables can change in any step State Function - an expression containing variables and constants that is not a next-state relation State Predicate - a Boolean-valued state function Invariant - a state predicate true in all reachable states Temporal Formula...[<a href="/2017/10/03/temporal-logic-of-actions-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/03/looking-at-the-37-apache-data-projects/"><img src="https://s3.amazonaws.com/kinlane-productions2/apache/apache-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/03/looking-at-the-37-apache-data-projects/">Looking At The 37 Apache Data Projects</a></h3>
			<p><em>03 Oct 2017</em></p>
			<p>I’m spending time investing in my data, as well as my database API research. I’ll have guides, with accompanying stories coming out over the next couple weeks, but I want to take a moment to publish some of the raw research that I think paints an interesting picture about where things are headed. When studying what is going on with data and APIs you can’t do any search without stumbling across an Apache project doing something or other with data. I found 37 separate projects at Apache that were data related, and wanted to publish as a single list I could learn from. Airvata** - Apache Airavata is a micro-service architecture based software framework for executing and managing computational jobs and workflows on distributed computing resources including local clusters, supercomputers, national grids, academic and commercial clouds. Airavata is dominantly used to build Web-based science gateways and assist to compose, manage, execute, and monitor large scale applications (wrapped as Web services) and workflows composed of these services. Ambari - Apache Ambari makes Hadoop cluster provisioning, managing, and monitoring dead simple. Apex - Apache Apex is a unified platform for big data stream and batch processing. Use cases include ingestion, ETL, real-time analytics, alerts and real-time actions. Apex is a Hadoop-native YARN implementation and uses HDFS by default. It simplifies development and productization of Hadoop applications by reducing time to market. Key features include Enterprise Grade Operability with Fault Tolerance, State Management, Event Processing Guarantees, No Data Loss, In-memory Performance &amp; Scalability and Native Window Support. Avro - Apache Avro is a data serialization system. Beam - Apache Beam is a unified programming model for both batch and streaming data processing, enabling efficient execution across diverse distributed execution engines and providing extensibility points for connecting to different technologies and user communities. Bigtop - Bigtop is a project for the development of packaging and tests of the Apache Hadoop ecosystem. The primary goal of Bigtop is to build...[<a href="/2017/10/03/looking-at-the-37-apache-data-projects/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/database-to-database-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/">Database To Database Then API, Instead Of Directly To API</a></h3>
			<p><em>03 Oct 2017</em></p>
			<p>I am working with a team to expose a database as an API. With projects like this there can be a lot of anxiety in exposing a database directly as an API. Security is the first one, but in my experience, most of the time security is just cover for anxiety about a messy backend. The group I’m working with has been managing the same database for over a decade, adding on clients, and making the magic happen via a whole bunch of databases and table kung fu. Keeping this monster up and running has been priority number one, and evolving, decentralizing, or decoupling has never quite been a priority. The database team has learned the hard way, and they have the resources to keep things up and running, but never seem to have them when it comes to refactoring it and thinking differently, let alone tackling the delivery of a web API on top of things. There will need to be a significant amount of education and training around REST, and doing APIs properly before we can move forward, something there really isn’t a lot of time or interest in doing. To help bridge the gap I am suggesting that we do an entirely new API, with it’s own database, and we focus on database to database communication, since that is what the team knows. We can launch an Amazon RDS instance, with an EC2 instance running the API, and the database team can work directly with RDS (MySQL) which they are already familiar with. We can have a dedicated API team handle the new API and database, and the existing team can handle the syncing from database to database. This also keeps the messy, aggregate, overworked database out of reach of the new API. We get an API. The database team anxiety levels are lowered. It balances things out a little. Sure there will still be some work between databases, but the API...[<a href="/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/03/clearly-designate-api-bot-automation-accounts/"><img src="https://s3.amazonaws.com/kinlane-productions2/shieldsquare/good-v-bad-bots.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/03/clearly-designate-api-bot-automation-accounts/">Clearly Designate API Bot Automation Accounts</a></h3>
			<p><em>03 Oct 2017</em></p>
			<p>I’m continuing my research into bot platform observability, and how API platforms are handling (or not handling) bot automation on their platforms, as I try to make sense of each wave of the bot invasion on the shores of the API sector. It is pretty clear that Twitter and Facebook aren’t that interested in taming automation on their platforms, unless there is more pressure applied to them externally. I’m looking to make sure there is a wealth of ideas, materials, and examples of how any API driven platform can (are) control bot automation on their platform, as the pressure from lawmakers, and the public intensifies. Requiring users clearly identify automation accounts is a good first place to start. Establishing a clear designation for bot users has its precedents, and requiring developers to provide an image, description, and some clear check or flag that identifies an account as automated just makes sense. Providing a clear definition of what a bot is, with articulate rules for what bots should and shouldn’t be doing is next up on the checklist for API platforms. Sure, not all users will abide by this, but it is pretty easy to identify automated traffic versus human behavior, and having a clear separation allows accounts to automatically turned off when they fit a particular fingerprint, until a user can pass a sort of platform Turing test, or provide some sort of human identification. Automation on API platforms has its uses. However, unmanaged automation via APIs has proven to be a problem. Platforms need to step up and manage this problem, or the government eventually will. Then it will become yet another burdensome regulation on business, and there will be nobody to blame except for the bad actors in the space (cough Twitter &amp; Facebook, cough, cough). Platforms tend to not see it as a problem because they aren’t the targets of harassment, and it tends to boost their metrics and bottom line when...[<a href="/2017/10/03/clearly-designate-api-bot-automation-accounts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/beach-rocks-currents_blue_circuit_5.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/">The Waves Of API Driven Bots Invading Our Shores</a></h3>
			<p><em>02 Oct 2017</em></p>
			<p>As each wave of technology comes crashing on the shores of the API space you’ll mostly find me silent, listening and watching what is happening. Occasionally you’ll hear me grumble about the aggressiveness of a single wave, or how unaware each wave is of the rest of the beach, or of the waves that came before them. Mostly I am just yelling back to the waves that claim, “we are going to change the beach forever”, and “we are the wave that matters, better than all the waves that came before us”. Mostly, it is the hype, and the unrealistic claims being made by each wave that bothers me, not the waves themselves. I do not think that technology won’t have an impact on the beach. I just think that us technologists tend to over-hype, and over-believe in the power each wave of technology, and that we do not consider the impact on the wider beach, and the amount of sand that ends up in everything. I don’t doubt that there will be some gems found in the sand, and that geologically speaking that the ocean plays a significant role in how the coastline is shaped. I’m just choosing to sit back on the bluff and enjoy my time on the beach, and not choosing to be a three year old playing in each of the waves, super excited by the sound each crash makes on the beach. I’m not saying that playing in the waves is wrong, I’m just choosing to look at the bigger picture from up here on the bluff. You can see one such canvas being painted over the last couple of years with what has become to be known as “bots”. Little automated nuggets of tech goodness, or evil, depending on your location on the beach. People love saying that bots will change everything. They’ll be your assistant. They’ll do everything for you. They’ll automate your life. Take care of...[<a href="/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/02/the-ca-acquisition-of-runscope/"><img src="https://s3.amazonaws.com/kinlane-productions2/runscope/ca-runscope-acquisitions.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/02/the-ca-acquisition-of-runscope/">The CA Acquisition Of Runscope</a></h3>
			<p><em>02 Oct 2017</em></p>
			<p>You won’t find me talking about the acquisition of API startups very often. I’m just not a fan of the game. I am not anti-venture capital, but I find the majority of investment in the API startup ecosystem works against everything we are trying to do with APIs. In my opinion, VC investment shouldn’t be the default, it should be an exception. There are other ways to build a business, and I see too many useful API tools get ruined while playing this game. With that said, I tend to not cover the topic, unless I get really pissed off, or the occasional investment or acquisition that I feel will result in a positive result. Last week we saw the Runscope acquisition by CA. This is an acquisition that doesn’t leave me concerned. Runscope is a partner of mine, run by people I know and care about, and they offer a tool that is useful in the API sector. If they’d had been acquired by many other bigcos I would have been more concerned, or even upset (if it had been certain ones). However, I have experience with CA, and while they are an enterprise beast, I’ve seen them make acquisitions before that weren’t damaging to the services and tooling they acquired. I trust that CA isn’t acquiring Runscope to just eliminate a strong player from the sector, and that they are actually interested in what Runscope does. I have seen CA’s role in the API space through the lens of the API Academy team, as well as through public and private conversations with other CA employees, on a variety of other teams. I’ve gone on-site and participated in API training session, and I have seen evidence that CA is invested in helping evolve their enterprise to be an API aware organization. Something that you can see reflected in how they approach doing business with their customers. I’m currently working to help move forward some...[<a href="/2017/10/02/the-ca-acquisition-of-runscope/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/02/openapi-definitions-for-entire-schema-org-vocabulary-do-not-reinvent-wheel/"><img src="https://s3.amazonaws.com/kinlane-productions2/schema-org/schema-org.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/02/openapi-definitions-for-entire-schema-org-vocabulary-do-not-reinvent-wheel/">OpenAPI Definitions For Entire Schema.org Vocabulary (Do Not Reinvent Wheel)</a></h3>
			<p><em>02 Oct 2017</em></p>
			<p>I am preparing my Schema.org Github repo with a variety of data sources for use across my API tooling and other projects. I’m trying to get better at using a common vocabulary, and not reinventing the wheel each time I start a new project. Schema.org has the most robust vocabulary of shared schema available today–so I am using this existing work as the core of mine. I am slicing and dicing the schema.org vocabulary into several formats that I can use in my OpenAPI-driven editors, and other tooling. I took the JSON-LD representation for Schema.org, and published it as a simpler JSON schema definition format that can be applied quickly to an OpenAPI. It isn’t perfect, and you lose a lot of the semantics in the process, but I think it still provides an important base for API designers, architects, and developers to use across their OpenAPI. It is pretty verbose, with over 150K lines, but it provides a fairly consolidated view of Schema.org classes, in a single set of definitions: You can download a copy via the Gist, or you can find as JSON and YAML in my Github repository for this work. I’m going to be creating complete OpenAPI for each Schema.org class, as well as individual JSON schema files for each class. I just haven’t to figure out how to decouple them into individual files, yet containing all the relevant schema. I have the code, I just need to dial it in, when I have more time. I am going to use this Schema.org JSON schema as an autocomplete in my API design tooling, and using the OpenAPI as the source definition for my API deployment and testing tooling. I’ve been evolving my Human Services Data API work to easily generate server side code using OpenAPI, and I’m going to use the same code base to generate any Schema.org API, and deploy as AWS EC2 instance. I’m not looking to develop a...[<a href="/2017/10/02/openapi-definitions-for-entire-schema-org-vocabulary-do-not-reinvent-wheel/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/"><img src="https://s3.amazonaws.com/kinlane-productions2/hack-education/hack-education-data.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/">Learning About API Design With Resources That Matter To You</a></h3>
			<p><em>02 Oct 2017</em></p>
			<p>I have been helping my partner in crime Audrey Watters (@audreywatters) evolve her data work as part of her Columbia Spencer Education Journalism Fellowship, where she is publishing a wealth of ed-tech funding data to Github. I worked with her to evolve the schema she is using across the Google Sheet, and YAML data stores she is using. Something that will autogenerate APIs (well dynamic JSON) based upon the filename, and the fields she chooses as part of her data stores. I just planted the seeds, and she has been cranking away creating repos, and building data stores since this last summer. She mentioned to me recently that she thought she had been being consistent in her naming conventions across her work, but had recently noticed some inconsistencies–realizing the importance of a consistent design and schema across the projects, something that really could become problematic at scale if she hadn’t caught. Luckily she was able to fix with some work, and was back on track. She isn’t as automated in the replication of data across her projects, but that is a good thing. It is forcing her to think more deeply about the naming and overall design of her static data APIs, which she uses across many repos, and displayed in a variety of lists, outlines, and stories she is telling around her work. Audrey has spent seven years listening to me talk about API design blah blah blah, but until she was working with her own data, that she cared about, she didn’t fully grasp some of the API design and implications of working with the access, reusability, and maintenance of data at scale. I’ve offered to automate more of the maintenance, replication, and standardization of data across her repos, but she’s declined. She said she finds it valuable to work with the design, and naming of her data stores, for us in different projects. She likes keeping here YAML data stores in separate...[<a href="/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/29/tyk-is-conducting-api-surgery-meetups/"><img src="https://s3.amazonaws.com/kinlane-productions2/tyk/tyk-api-surgery-singapore.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/29/tyk-is-conducting-api-surgery-meetups/">Tyk Is Conducting API Surgery Meetups</a></h3>
			<p><em>29 Sep 2017</em></p>
			<p>I was having one of my regular calls with the Tyk team as part of our partnership, discussing what they are up to these days. I’m always looking to understand their road map, and see where I can discover any stories to tell about what they are up to. A part of their strategy to build awarness around their API management solution that I found was interesting, was the API Surgery event they held in Singapore last month, where they brought together API providers, developers, and architects to learn more about how Tyk can help them out in their operations. API surgery seems like an interesting evolution in the Meetup formula. They have a lot of the same elements as a regular Meetup like making sure there was pizza and drinks, but instead of presentations, they ask folks to bring their APIs along, and they walk them through setting up Tyk, and deliver an API management layer for their API operations. If they don’t have their own API, no problem. Tyk makes sure there are test APIs for them to use while learning about how things work. Helping them understand how to deliver API developer onboarding, documentation, authentication, rate limiting, monitoring, analytics, and the other features that Tyk delivers. They had about 12 people show up to the event, with a handful of business users, as well as some student developers. They even got a couple of new clients from the event. It seems like a good way to not beat around the bush about what an API service provider is wanting from event attendees, and getting down to the business at hand, learning how to secure and manage your API. I think the Meetup format still works for API providers, and service providers looking to reach an audience, but I like hearing about evolutions in the concept, and doing things that might bring out a different type of audience, and cut out some of...[<a href="/2017/09/29/tyk-is-conducting-api-surgery-meetups/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/29/the-api-stack-for-disrupting-the-world/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/fredericksburg-downtown-flag.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/29/the-api-stack-for-disrupting-the-world/">The API Stack For Disrupting The World</a></h3>
			<p><em>29 Sep 2017</em></p>
			<p>I know people don’t understand why I’m so obsessed with APIs. Sometimes I ask the same question. When I began in 2010, it was 75% about my belief in the good that APIs can do, and 25% about pushing back on the bad things being done with APIs. In 2017, it is 15% about the good, and 85% about pushing back on the bad things that APIs can do. API driven platforms are being used for some pretty shady things these days, and increasingly they are a force for disruption, and not about making the world a better place. With this in mind, I wanted to take a moment to highlight the API stack right now that is being used to disrupt the world around us. These are the APIs that have shifted the political landscape in the U.S., and are being used to replicate, automate, and scale this disruption around the world. Facebook - The network effect is what brings the troublemakers to Facebook.They are on pace to have 2 billion active users. Something that has the potential to create quite a network effect when sharing stories and links, and when you seed that, target it, and grow it using the Facebook advertising engine–it makes for an excellent engine for disruption. Twitter - Twitter is a different beast. Less of the mainstream population than Facebook enjoys, but still a sizable, and very public audience. You can use the Twitter engine to spin things up, get people sharing, do some of the same sharing of stories and links, seeding, targeting, and growing with advertising. Often times the viral nature will spread to Facebook on take on a life of its own. Reddit - Now Reddit is entirely just an organic engine for disseminating information, which makes it great for propaganda, everything fake, and stoking the haters. The network effect that is Reddit, works very, very well will Twitter and Facebook, making for a perfect storm...[<a href="/2017/09/29/the-api-stack-for-disrupting-the-world/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/29/developing-the-ability-to-repeat-the-same-api-stories-over-and-over/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/kinlane-whiteboard-api-bw-artsy.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/29/developing-the-ability-to-repeat-the-same-api-stories-over-and-over/">Developing The Ability To Repeat The Same API Stories Over And Over</a></h3>
			<p><em>29 Sep 2017</em></p>
			<p>After seven years of telling stories on API Evangelist I’ve had to repeat myself from time to time. Honestly, I repeat myself A LOT. Hopefully I do it in a way that some of you don’t notice, or at least you are good at filtering the stories you’ve already heard from your feed timeline. My primary target audience is the waves of new folks to the world of APIs I catch with the SEO net I’m casting and working on a daily basis. Secondarily, it is the API echo chamber, and folks who have been following me for a while. I try to write stories across the spectrum, speaking to the leading edge API conversations, as well as the 101 level, and everything in between. Ask anyone doing API evangelism, advocacy, training, outreach, and leadership–and they’ll that you have to repeat yourself a lot. It is something you get pretty sick of, and if you don’t find ways to make things interesting, and change things up, you will burn out. To help tell the same story over and over I’m always looking for a slightly different angle. Let’s take API Meetups as an example. Writing a story about conducting an API Meetup has been done. Overdone. To write a new story about it I’ll evaluate what is happening at the Meetup that is different, or maybe the company, or the speaker. Diving into the background of what they are doing looking for interesting things they’ve done. You have to find an angle to wrap the boring in something of value. API documentation is another topic I cover over, and over, and over. You can only talk about static or interactive API documentation so much. Then you move into the process behind. Maybe a list of other supporting elements like code samples, visualizations, or authentication. How was the onboarding process improved? How the open source solution behind it simplifies the process. You really have to work...[<a href="/2017/09/29/developing-the-ability-to-repeat-the-same-api-stories-over-and-over/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-graphql.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/">API Design Industry Guide: GraphQL, A Query Language For APIs</a></h3>
			<p><em>29 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. GraphQL is a query language designed by Facebook to build client applications using a flexible syntax and provide a system for describing the data requirements and interactions required by each application. GraphQL began as a Facebook project that soon began powering all their mobile applications. By 2015, became a formal specification. GraphQL provides a query language for your APIs that allows users to describe how they would like their API requests be fulfilled. The approach shifts the API design process to be more about request flexibility requiring API providers to design all API paths ahead of time. It opts for an augmented query language over investing in static schema that requires specific API paths. REST APIs focus on paths to your resources, but GraphQL is all about fields and data types, with everything accessed through a single API path. GraphQL does a better job of providing a more comprehensive approach access to data stored in a database by offloading design to the query layer for interpretation at query render time. The ability to define what data is returned opens up some interesting approaches to delivering resources, especially when it comes to potentially constrained network environments. When it comes to providing access to data used in responsive web and mobile applications, GraphQL can be successful in allowing application developers to get exactly what they need for an interface and nothing more....[<a href="/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/path-in-the-woods-black-white.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/">How Do We Help Folks Understand That APIs Are A Journey?</a></h3>
			<p><em>28 Sep 2017</em></p>
			<p>I was hanging out with my friend Mike Amundsen (@mamund) in Colorado last month and we ended up discussing folks uncertainty with APIs. You see, many folks that he has been talking to were extremely nervous about all the unknowns in the world of APIs, and were looking for more direction regarding what they should be doing (or not doing). Not all people thrive in a world of unknown unknowns, and not even in a world of known unknowns. Many just want a world of known knowns. This is something that makes the API landscape a very scary thing to some folk, and world where they will not thrive and be successful unless we can all begin to find a way to help them understand that this is all a journey. I love figuring all of this API stuff out, and I know Mike does too. We like thinking about the lofty concepts, as well as figuring out how to piece all the technical elements together in ways that work in a variety of business sectors. Many folks we are pushing APIs on aren’t like us, and just want to be told what to do. They just want the technology solution to their problem. A template. A working blueprint. It freaks them out to have so many options, possibilities, patterns, and directions they take things. I feel like we are setting folks up for failure when we talk them into embarking on an API journey without the proper training, equipment, support, and guidance. I think about the last seven years doing this, and how much I’ve learned. Realizing this makes me want to keep doing APIs, just so I can keep learning new things. I thought I understood REST when I started. I didn’t. I thought I understand the web when I started, I didn’t (still don’t). I was missing a lot of the basics, and no matter what folks told me, or how precise...[<a href="/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/28/apistrat-and-the-openapi-initiative/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-membership-september-2017.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/28/apistrat-and-the-openapi-initiative/">APIStrat And The OpenAPI Initiative</a></h3>
			<p><em>28 Sep 2017</em></p>
			<p>We are getting closer to APIStrat in Portland, Oregon, October 31st through November 2nd. So I’m going to keep crafting stories that help convince you should be there. It is the first APIStrat conference as an OpenAPI event, operated by the Linux Foundation events team. Steve and I are still playing a big part, and will be MC’ing, but like OpenAPI, APIStrat has grown to the point where we need to let it become more than just something Steve, myself, and the 3Scale team can execute by ourselves. APIStrat has always been a place where we gather and talk about OpenAPI, going back to when it was affectionately known as Swagger. Tony, and the team have spoken before, and there has been many other sessions, workshops, and keynotes involving the API specification format. This APIStrat is going to be no different, but there will be an even heavier presence for the specification. Since Tony Tam is stepping away, we are giving a full hour on mainstage for him and folks involved in the evolution of OpenAPI to share their story. Darrel Miller will be holding also be holding a workshop on the first day, where several folks involved in the OAI will be sharing knowledge. There will also be an OAI booth presence, and I know that Jeff ErnstFriedman will be present for OAI membership discussions. If your company is investing in OpenAPI as part of your API operations, and developing tooling around the specification, you should be considering joining the OAI. Take a look at the current membership list. I’m a member, and so are other heavy hitters like Adobe, Google, Microsoft, IBM, and even my partner in crime 3Scale, and Tyk are present. As a member you get in on the Slack channel conversations, participate on marketing and governance calls, and you get invited to participate on the APIStrat crew (if you want). Let me know if you are interested becoming a member,...[<a href="/2017/09/28/apistrat-and-the-openapi-initiative/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/28/api-design-industry-guide-the-restlet-platform-story/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-the-restlet-platform-story.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/28/api-design-industry-guide-the-restlet-platform-story/">API Design Industry Guide: The Restlet Platform Story</a></h3>
			<p><em>28 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. Restlet began as an open source Java API framework over a decade ago and has evolved into an API studio, client, and cloud platform with an API design core. At the center of the API lifecycle management platform is its API designer which gives you a visual view of an API and an OpenAPI or RAML view, providing a machine readable accounting of each API’s contract. The Restlet Studio allows you to design and document your APIs, starting from scratch, or import existing API design patterns using OpenAPI for RAML. Using the Restlet design UI you can shape the paths, parameters, headers and complete requests and responses for any API. Then, take the definition and actually put it to work in development, staging, or production environments. Restlet demonstrates how API design is more than just a momentary phase where you are developing APIs and is actively defining every stop along the API lifecycle from design to deprecation. While designing an API in the Restlet API Studio, you can also work to test and automate using the client, helping ensure a usable and complete API is designed. The Restlet Client provides a dashboard to verify the desired API contract in a way that can be shared across teams, with clients, and across stakeholders. Once the API design process has matured and evolved and is ready for deployment, Restlet empowers production deployment...[<a href="/2017/09/28/api-design-industry-guide-the-restlet-platform-story/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-grpc-open-source-rpc-framework.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/">API Design Industry Guide: gRPC, Open Source RPC Framework</a></h3>
			<p><em>28 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. gRPC is a high-performance open source remote procedure call (RPC) framework that is often used to deploy APIs across data centers that also supporting load balancing, tracing, health checks and authentication. While gRPC excels in more controlled, tightly coupled environments, it is also applicable for delivering resources to web, mobile, and other Internet connected devices. When crafting gRPC APIs, you begin by defining the service using Protocol Buffers, a language and toolset for binary serialization that has support across 10 leading programming languages. Protocol Buffers can be used to generate client and server stubs in these programming languages with tight API/client coupling — delivering a higher level of performance than your average REST API and SDK can. gRPC API design patterns takes advantage of HTTP/2 advances and uses authenticated bi-directional streaming to deliver APIs that can be scaled to millions of RPC calls per second. Its an effective approach for larger, more demanding API platforms that have begun to see the performance limitations of a more RESTful API design approach. gRPC is not ideal for every API implementation, but is definitely an approach providers should consider when high volumes anticipated, especially within the data center or other tightly controlled environment. Google has been using gRPC internally for over a decade now, but has recently committed to delivering all their public APIs using gRPC in addition to RESTful APIs, demonstrating that...[<a href="/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/thinking-beyond-just-distributed-api-scale-towards-federated-api-scale/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/losangelescloudy/dali_three/file-00_00_35_50.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/thinking-beyond-just-distributed-api-scale-towards-federated-api-scale/">Thinking Beyond Just Distributed API Scale Towards Federated API Scale</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>You hear a lot about doing APIs at scale in our space. Many folks dismiss web APIs because they feel they won’t scale, and aren’t performing at the scale they envision. The majority of these discussions focus on how do you scale large operations of Twitter, Facebook, or Google scope. A single organization operating API infrastructure at scale, distributed across many geographical regions, supporting millions of users. There are plenty of discussions going on regarding the technology, business, and politics of doing APIs at this scale. I find myself thinking in similar ways, but more federated version of this, where the latest technology might not always be the right answer. My Human Services Data API (HDSA) work is the best example I have of this. Where I’m having to keep the technology, and API definition bar as low as possible to onboard as many people as I possibly can, but then eventually, be able to aggregate large amounts of data across many federated instance. I have 3,144 counties, and 19,354 cities to consider. They should all be speaking a common schema when it comes to the sharing of human services data. Something that is easier said, than done. When you get on the ground you realize many of them are stuck in 1990s, or early 2000s edition of the web, and just do not have the resources needed to move things forward. They can’t afford the latest SaaS service, and they can’t drop the ball, or thousands, or millions of people will suffer–the stakes are high. When I go into large companies, who have a large teams, and significant number of resources, the conversation around scale is much different. Sure, there is distributed scale. Sure, there is volume scale. However, most times the distribution and volume exists within a single company or organization. A single command and control structure. However, I’m talking about federated distribution and volume, with no single command and control structure. I’m...[<a href="/2017/09/27/thinking-beyond-just-distributed-api-scale-towards-federated-api-scale/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/the-value-of-api-driven-events/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/hermosabeach/dark_dali/file-00_00_11_64.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/the-value-of-api-driven-events/">The Value of API Driven Events</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>I am spending a lot of time lately thinking about event sourcing, evented architecture, real time, and webhooks. I’m revisiting some of the existing aspects of how we move our bits around the Internet in real time and at scale as part of existing conversation I am having, as well as some projects I’m working on. I recently wrote about making sense of API activity with webhook events, and as I’m crafting a list of meaningful events for my Human Services Data API (HSDA) work, I’m thinking about how these events reflect the value that occurs via API platforms. As I’m going through the different APIs I’m exposing via a platform, I am working to identify and catalog events in which folks can subscribe to using webhooks. These are the events that occur, like adding a new organization, updating a service, or completing a batch import–all the things people will care about the most. These are the events and activities that occur because their is an API, which have the most value to API consumers, and platform operators. This is what actually matters, and why we are doing an API in the first place, to enable these events to occur. The more these events are triggered, and the more people we have subscribing and engaging with these events, the more value that is generated using an API. In aggregate, using modern approaches to API management, we might provide analytics and reports that demonstrate all this value being created, to justify the existence of our API. In some implementations, this value created is how we might be charging our API consumers, partners, and other stakeholders. However, in some cases we might even considering paying API consumers when these events occur, incentivizing a certain event-driven behavior that benefits the platform. It is easy to think of API value generation simply as the number of API calls, but I think webhooks has helped establish a new way to...[<a href="/2017/09/27/the-value-of-api-driven-events/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/caching-for-your-api-is-easier-than-you-think-and-something-you-should-invest/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/internet-gauages-3.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/caching-for-your-api-is-easier-than-you-think-and-something-you-should-invest/">Caching For Your API Is Easier Than You Think And Something You Should Invest</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>I’m encountering more API providers who have performance and scalability concerns with their APIs, who are making technical procurement decisions (gateways, proxies, etc) based upon these challenges, but have not invested any time or energy into planning and optimization of caching for their existing web servers that are delivering their APIs. Caching is another aspect of HTTP that I keep finding folks have little or no awareness of, and do not consider more investment in it to assist them in alleviating their scalability and performance concerns. There was a meeting I attended a couple weeks back where an API implementation was concerned about a new project for bulk loading and syncing of data between multiple external systems and their own, because of the strain it put on their database. Citing that they received millions of website, and API calls daily, they said they could not take the added load on their already strained systems during the day, limiting this type of activity to a narrow window at night. I began inquiring regarding caching practices in place on web, and API traffic, and they acknowledged that they new of no such activity or practices in place. This isn’t uncommon in my experiences, and I regularly encounter IT groups who just don’t have the time and HTTP awareness to implement any coherent strategy–this particular one just happened to admit it. My friends over at the API Academy have a great post on caching for RESTful and Hypermedia APIs, so I won’t be addressing the details of HTTP, and how you can optimize your APIs in this way. API caching isn’t an unproven technology, and it is a well known aspect of operating on the web, but it does take some investment and awareness. Like API design in general, you have to get to know the resources you are serving up, understand how your consumers are putting these resources to work, and adjust, dial-in, and tweak your caching...[<a href="/2017/09/27/caching-for-your-api-is-easier-than-you-think-and-something-you-should-invest/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/api-design-industry-guide-applicationlevel-profile-semantics-alps/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-api-stylebook.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/api-design-industry-guide-applicationlevel-profile-semantics-alps/">API Design Industry Guide: Application-Level Profile Semantics (ALPS)</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. Current API design focusses on using schema to help quantify the payload of the request and response structure of our APIs. JSON Schema, MSON, and other data specifications have emerged to help us quantify the bits we are passing back and forth with APIs. Alongside this evolution, another data format has emerged to help us define simple descriptions of our application-level semantics, similar to how we are using HTML microformats to share data on the web, Application-Level Profile Semantics (ALPS). ALPS goes well beyond schema, which provides a representation of a plan or theory in the form of an outline or model. ALPS provides a way to define the meaning behind the data, content, and other resources you are making available via an API. ALPS seeks to establish a shared understanding by illuminating the meaning behind hypermedia interfaces (data and state transitions) such as HTML, Collection+JSON, HAL or Siren. It encourages reusability of common profile documents across the media types we are depending on. Using ALPS you can easily define the common data elements we all use in our API like contacts, todo lists. It can even describe the structure of our APIS for verbose and more useful error responses. What really matters is that you can also define the transitions surrounding these data elements. You can get at the meaning and use behind them, like rolling dice, or playing...[<a href="/2017/09/27/api-design-industry-guide-applicationlevel-profile-semantics-alps/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-api-evangelist-flagged.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/">A Reminder To Always Have A Plan B For Our API Related Github Infrastructure</a></h3>
			<p><em>27 Sep 2017</em></p>
			<p>I had a scare this last weekend regarding my Github infrastructure. My Github organization for API Evangelist was flagged as SPAM and taken down. The Github organization contains almost 100 repositories that I use across my platform. These repositories drive the public side of my research, but also contain YAML files that are used in automation across my entire platform, and network of websites. At about 12:00 PM on Saturday, everything came to a screeching halt, with all the data I depend on to make things go around becoming unavailable. I have backups of all the data, and the website templates that produce the public side of API Evangelist. I also have a plan B in place for setting up a Jekyll instance that runs on Amazon EC2, but I hadn’t ever actually ran any drills on plan B. After submitting a ticket to Github, I got to work firing up the AWS EC2 instance, and unloading and unpacking the almost 100 website backups for my API Evangelist research. After getting things setup, and as I was preparing to switch over the DNS, I got an email from Github saying: Sorry for the hassle! It appears your organization had been caught up in a spam filter and was flagged incorrectly. I’ve cleared that flag now, so your account should be back to normal. You shouldn’t see that message again, but let me know if I can help with anything else! Crisis averted. Luckily this was just my own company Github organization. I operate numerous other API developer portals, code repositories, documentation sites, and other API related projects and tooling that lives entirely on Github. If my personal account was frozen, or any of these organizations taken offline, I would have been in a lot more hot water, and accountable to my clients. Overall I was down for a little over six hours. It showed me the fragile nature of depending on Github, not just for...[<a href="/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/why-does-aws-charge-by-usage-and-other-apis-still-use-plans/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_feed_people.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/why-does-aws-charge-by-usage-and-other-apis-still-use-plans/">Why Does AWS Charge By Usage And Other APIs Still Use Plans?</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>Amazon Web Services recently updated their billing for EC2 instances to be by the second, which I really like, because I’ll fire up an instance and run for minutes, then shut things down. I’m just looking to process patent downloads, or other intensive workload projects. Beyond just EC2, the rest of Amazon’s platform is still very usage based. Meaning, you get charged for whatever you use, with unit pricing for each resource designed to compliment how it gets put to use. You get charged for the hard costs of compute, storage, and bandwidth, but you also see per message, job, entry, and other types of billing depending on the type of resource being delivered via API. With this model for doing APIs, I’m wondering why so many API providers still have access plans and tiers. I’ve vented several times that I think service tiers are a legacy of a SaaS way of thinking and does not scale for API consumers. Maybe back when we used a handful of APIs, but the number of APIs I’m using is pushing 50 these days, and I can’t alway justify a monthly subscription to get what I need. I’m looking to just get access to valuable API resources, and get billed for whatever I use. If I don’t use anything for 6 months, I don’t get billed for anything. Also, I want to be able to run large jobs which consume intense amounts of resources without hitting tier and other limits–just charge me for what I use. If I have a $1,000.00 to spend today, let me spend it. Don’t make me jump through hoops. I know the answer to my question regarding why so many API startups do this. It is because the resources being provided via the API isn’t the product, us API consumers are. They are looking to ensure a certain level of headcount, monthly, and annual subscribers, so that they can sell us to their...[<a href="/2017/09/26/why-does-aws-charge-by-usage-and-other-apis-still-use-plans/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/keeping-the-web-api-layer-in-kafka-with-a-rest-proxy/"><img src="https://s3.amazonaws.com/kinlane-productions2/confluent/confluent-kafka-platform.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/keeping-the-web-api-layer-in-kafka-with-a-rest-proxy/">Keeping The Web API Layer In Kafka With A REST Proxy</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>I’m slowly learning more about Kafka, and the other messaging and data streaming solutions gaining traction in the API space. If you aren’t on the Kafka train yet, “Kafka is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.” I’m still learning about how Kafka works, and with no real production experience, it is something that is taking time. As part of my conversations on the subject, I was introduced to Confluent, a platform version of Kafka, which is the quickest way I have seen to get started with real-time data streams. As part of the Confluent offering I noticed they have a REST proxy, which you can find the API documentation here, and the code for the Kafka REST proxy on Github. According to the Github repo, “the Kafka REST Proxy provides a RESTful interface to a Kafka cluster. It makes it easy to produce and consume messages, view the state of the cluster, and perform administrative actions without using the native Kafka protocol or clients.” I’ve noticed that many of the other messaging and data streaming solutions out of Apache these days have diverted from using REST, which makes sense for speed, and scale, but when it comes to reaching a wider audience I can still see the need to have RESTful API. Delivering a kind of multi-speed solution that allows developers to pick their speed based upon their skills, awareness, and need. I’m feeling like the platform approach of Confluent, combined with a RESTful layer, will give them an advantage over other Kafa service providers, or just deploying the open source solution out of the box. REST isn’t always the most efficient, or scalable solution, but when it comes to reaching a wide audience of developers, and allowing consumers to get up and running quickly, REST is still a sensible approach. Honestly, I don’t think it is...[<a href="/2017/09/26/keeping-the-web-api-layer-in-kafka-with-a-rest-proxy/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/concerns-around-working-with-the-api-evangelist-at-large-organizations/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/kin-chesapeake-sun_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/concerns-around-working-with-the-api-evangelist-at-large-organizations/">Concerns Around Working With The API Evangelist At Large Organizations</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>I know that I make some tech companies nervous. They see me as being unpredictable, with no guarantees regarding what I will say, in a world where the message should be tightly controlled. I feel it in the silence from many of the folks that are paying attention to me at large companies, and I’ve heard it specifically from some of my friends who aren’t concerned with telling me personally. These concerns keep them from working with me on storytelling projects, and prevent them from telling me stories about what is happening internally behind their firewall. It often doesn’t stop employees from telling me things off the record, but it does hinder official relationships, and on the record stories from being shared. I just want folks to know that I’m not in the scoop, or gotcha business. I only check-in on my page views monthly to help articulate where things are with my sponsors. I’m more than happy to keep conversations off the record, anonymize sources and topics. Even the folks in the space who have pissed me off do not get directly called out by me. Well, most of them. I’ve gone after Oracle a couple of times, but they are the worst of the worst. There are other startups and bigcos who I do not like, and you don’t ever hear me talking trash about them on my blog. Most of my rants are anonymized, generalized, and I take extra care to ensure no enterprise egos, careers, or brands are hurt in the making of API Evangelist. If you study my work, you’ll see that I talk regularly with federal government agencies, and large enterprise organizations weekly, and I never disclose things I shouldn’t be. If you find me unpredictable, I’m guessing you really haven’t been tuning into what I’ve been doing for very long, or your insecurities run deeper than anything to do with me. I’m not in the business of making...[<a href="/2017/09/26/concerns-around-working-with-the-api-evangelist-at-large-organizations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/apis-are-not-just-about-mobile-make-sure-you-are-considering-the-bigger-picture/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/space-suit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/apis-are-not-just-about-mobile-make-sure-you-are-considering-the-bigger-picture/">APIs Are Not Just About Mobile, Make Sure You Are Considering The Bigger Picture</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>When I started API Evangelist in 2010, API usage in mobile phones was the biggest factor contributing to me quitting my job, and becoming a independent voice for all APIs. I was being asked to deliver APIs to drive mobile applications on the iPhone, and while helping run technology for Google I/O I saw an increased need for resources to be delivered to this emerging platform. I knew that APIs were going to play an essential role in ensuring data, content, and algorithms could be put to use in mobile applications. Even with the importance of mobile, it wasn’t the only reason I knew that APIs were going to be important, which is something that still resonates today. In 2007, I saw the growing importance of social media APIs, and how messaging, images, and video were being made more distributed using APIs. Then in 2008, I saw that I could deliver global infrastructure using web APIs, demonstrating that web APIs weren’t a toy, and that you could operate a real business using web APIs. Then the whole mobile thing was just the tipping point, which demonstrated that the web was maturing beyond just websites, and it would be how we’d be doing business for some time to come. Every day I see people with blinders on focusing in on one slice of the API pie, seeing APIs as purely about commerce, social, cloud, mobile, IoT, messaging, or other growing aspect of the API economy. People are good at seeing things through the lens of their products, services, and industry. It is easy for them to ignore those people over there, or the other aspects of why leveraging the web is so important to all of this working. They get excited about a new open source solution, protocol, or pattern, and focus in exclusively on a single aspect of how we deliver technology–sometimes at the cost of other areas of their operations, or the future. If...[<a href="/2017/09/26/apis-are-not-just-about-mobile-make-sure-you-are-considering-the-bigger-picture/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/26/api-design-industry-guide-api-stylebook/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-industry-guide-api-stylebook.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/26/api-design-industry-guide-api-stylebook/">API Design Industry Guide: API Stylebook</a></h3>
			<p><em>26 Sep 2017</em></p>
			<p>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure. Arnaud Lauret (@arno_di_loreto), the API Handyman (@apihandyman), has been developing an API Stylebook that provides a collection of resources for API designers. It is a brilliant aggregation of thirteen API design guides from Atlassian, Cisco, Cloud Foundry, Google, Haufe, Heroku, Microsoft, PayPal, Red Hat, The White House, and Zalando. It highlights best practices used by leading API providers. “The API Stylebook aims to help API Designers to solve API design matters and build their API design guidelines by providing quick and easy access to selected and categorized resources”, says Lauret. A unique community resource, it provides deep linking to specific topics within publicly available API design guidelines. Instead of reinventing the wheel or searching Google for hours, API designers quickly can find solutions and inspiration from these existing best practices. More than just a list of guidelines, it is a machine readable distillation of the thirteen API design guides into a master list of API design topics you can consider when crafting your own API design guide. It is slick. I like Arnaud’s approach to analyzing the existing API design patterns across the API platforms who have shared their guides. I also really like the YAML approach and it’s presented as a very good looking website using Github, and Github Pages. This is how API literacy tools should be constructed and it provides a valuable lesson in API design. You...[<a href="/2017/09/26/api-design-industry-guide-api-stylebook/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/talking-with-more-federal-agencies-about-api-micro-consulting/"><img src="https://s3.amazonaws.com/kinlane-productions2/skylight/services-infographic.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/talking-with-more-federal-agencies-about-api-micro-consulting/">Talking With More Federal Agencies About API Micro Consulting</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I have been having more conversations with federal agencies as part of my work with my Skylight partners about API related microconsulting. One recent conversation, which I won’t mention the agency, because I haven’t gotten approval, involved bug bounties on top of an API they are rolling out. The agency isn’t looking for the regular technology procurement lifecycle around this project, they are just looking for a little bit of research and consulting to help ensure they are on the right track when it comes to hardening their API approach. Micro consulting like this will usually not exceed $5,000.00 USD, and will always be a short term commitment. From my vantage point micro consulting will always be API related, and in this particular case involves studying how other API providers in the private sector are leveraging bug bounties to help harden their APIs either before they go public, or afterwards in an ongoing fashion. After I do the research I will be taking this work back to my team of consultants at Skylight, and we’ll put together formal report and presentation that we will bring back to the federal government agency to put into motion. This approach to doing APIs in the federal government (or any government) is a win-win. It fits with my approach to doing research at API Evangelist, and it provides API expertise for federal agencies in small, affordable, and bite-size chunks. Government agencies do not have to wait months, or years, and spend massive amounts of money to gain access to API expertise. For Skylight, it gets our foot in the door within government, and helps demonstrate the expertise we bring to the table. Something that will almost always turn into additional micro procurement relationships, as well as potentially larger scale, ongoing project relationships. Personally, I like my API consulting just like my APIs, small, and doing one thing well. I don’t like consulting contracts that try to do too much....[<a href="/2017/09/25/talking-with-more-federal-agencies-about-api-micro-consulting/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/providing-embeddable-api-integrations-for-nondevelopers-with-zapier/"><img src="https://s3.amazonaws.com/kinlane-productions2/zapier/zapier-embeddable-zaps.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/providing-embeddable-api-integrations-for-nondevelopers-with-zapier/">Providing Embeddable API Integrations For Non-Developers With Zapier</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I’m regularly working to make APIs more accessible to non-developers, and Zapier is the #1 way I do this. Zapier provides ready-to-go API integration recipes for over 750 APIs, providing IFTTT-like functionality, but in a way that actual pays the whole API thing forward (Zapier has APIs, IFTTT does not). One of the benefits of having APIs is you can build embeddable tooling on top of them, and Zapier has some basic embeddable tools available to anyone, with some more advanced options for partners via their partner API. Using the Zapier basic embeddable widget you can list one or many Zaps, providing recipes for any user to integrate with one or many APIs, that can be embedded into a web page, or within an application: &#x3C;script type="text/javascript" src="https://zapier.com/zapbook/embed/widget.js?guided_zaps=2618,1035,977"&#x3E;&#x3C;/script&#x3E; All you do is add the id for each of the Zaps you wish to list, under the “guided_zaps” variable, and it will list the icon, title, and “use this zap” functionality, all wrapped in the appropriate powered by Zapier branding. I’m developing lists of useful Zaps ranging from working with Google Sheets, to managing social media presence on Twitter and Facebook. Everyday, useful things that the average user might find valuable when it comes to automating, and taking control over their online presence. Anytime I reference possible API integration use cases in a story, I’m going to start embedding a widget of actual Zaps you can use to accomplish whatever I’m talking about below. I’m also trying to carve out time to develop some of my own Zaps, and sign up to become a Zapier partner, so I can begin to develop some more advanced editions of embeddable tooling. I want to make my own JavaScript library that will spider any text in a story, and turn references to API integration into popup tooltips, with API literacy, training, and action links. I have a handful of API 101 style solutions I’d like to see exist, but...[<a href="/2017/09/25/providing-embeddable-api-integrations-for-nondevelopers-with-zapier/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/latest-copy-of-the-api-evangelist-api-design-industry-guide/"><img src="https://s3.amazonaws.com/kinlane-productions2/guides/definition/design/api-design-guide-2017-09-25.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/latest-copy-of-the-api-evangelist-api-design-industry-guide/">Latest Copy Of The API Evangelist API Design Industry Guide</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I’ve been struggling to get the latest edition of my industry guides out the door. I have a new Adobe Indesign format which I really like as a constraint, but is also pushing my desktop publishing skills. What is really kicking my ass though, is the editing. This latest copy was professionally edited, but I ran out of money to pay him on future guides, and I ended up making some slight changes to this one as well. I am very self-conscious of my grammar and spelling mistakes. I’m capable of editing my own stuff, and my grammar and spelling is high quality. The problem is that I’m too close to the content, and with each edit I make changes, which then introduce new mistakes. Also my brain moves too fast sometimes, and I just make silly mistakes, and overlook things by just reading it the way my brain intended. Anyways, I’m over stressing on it all. I just want to get my guides out. I have too much of a back log, and since I can afford a professional editor to shadow my work, I’m just going to put them out there. If you find mistakes, feel free to submit a Github issue on the repo for my API design research. I have too many guides to get out, and it is more important to me that my research moves forward, I spend the time distilling things down into a guide, and hitting publish. I can’t wait for perfect. If folks discount my work because I’m moving so fast, too bad. It is more important that the knowledge is in my head. If you want to help fund me so I can properly afford an editor, I welcome that as well–I have one who will work with me full time, I just need the cash! Anyways, I’m finally getting around to publishing this edition of the API design industry guide, which I hope provides...[<a href="/2017/09/25/latest-copy-of-the-api-evangelist-api-design-industry-guide/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/data-streaming-in-the-api-landscape/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/goldsilverfalls/creativity/file-00_00_40_84.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/data-streaming-in-the-api-landscape/">Data Streaming In The API Landscape</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I was taking a fresh look at my real time API research as part of some data streaming, and event sourcing conversations I was having last week. My research areas are never perfect, but I’d say that real time is still the best umbrella to think about some of the shifts we are seeing on the landscape recently. They are nothing new, but there has been renewed energy, new and interesting conversation going on, as well as some growing trends that I cannot ignore. To support my research, I took a day this week to dive in, have a conversation with my buddy Alex over at the TheNewStack.io, and the new CEO of WSO2 Tyler Jewell around what is happening. The way I approach my research is to always step back and look at what is happening already in the space, and I wanted to take another look at some of the real time API service providers I was already keeping eye on in the space: Pubnub - APIs for developers building secure realtime Mobile, Web, and IoT Apps. StreamData - Transform any API into a real-time data stream without a single line of server code. Fanout.io - Fanout’s reverse proxy helps you push data to connected devices instantly. Firebase - Store and sync data with our NoSQL cloud database. Data is synced across all clients in real time, and remains available when your app goes offline. Pusher - Leaders in real time technologies. We empower all developers to create live features for web and mobile apps with our simple hosted API. I’ve been tracking on what these providers have been doing for a while. They’ve all been pushing to boundaries of what is streaming, and real time APIs for some time. Another open source solution that I think is worth noting, which I believe some of the above services have leverages is Netty.io. Netty - Netty is an asynchronous event-driven network application framework for...[<a href="/2017/09/25/data-streaming-in-the-api-landscape/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/25/considering-the-future-of-the-openapi-initiative/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/OpenAPI_Pantone.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/25/considering-the-future-of-the-openapi-initiative/">Considering The Future Of The OpenAPI Initiative</a></h3>
			<p><em>25 Sep 2017</em></p>
			<p>I’m a member of the OpenAPI Iniative (OAI). I’m not very active on the governance or marketing, but I enjoy hanging out in the hallways of the Slack channel, and being part of the conversation. I’m pretty confident in the core group’s ability to steer the direction of the specification, and leave my influence to be more about storytelling externally, and planting seeds in the minds of folks who are putting the API specification to use. I have a much different style to influencing the API space than many of the companies I share membership within the OAI–it is just my way. I am working with more groups to help them craft, maintain, and evangelize around a specific OpenAPI definition, for use in a specific industry. THe primary one on the table for me is the Human Services Data API (HSDA). Which is an OpenAPI for helping cities, municipalities, and non-profit organizations that help deliver information around human services, speak a common language. This is just one example of industry specific API definitions emerging. I am seeing OpenAPI emerge for PSD2, FHIR, helping guide the conversation going on in the financial and healthcare sectors. The OpenAPI as a top level API specification standard is maturing, and is something that reached version 3.0, and once the services and tooling catch up, we’ll see another boom in industry specific API definitions emerge. This is when we are going to see the need to start harmonizing, standardizing, and merging many disparate standards into a single specification, or at least interoperable specifications. You see this happening right now with OpenAPI, API Blueprint, and RAML–they are all part of the OpenAPI Initiative (OAI). In the next five years you will see this same thing begin occurring for other industry specific APIs, and we’ll eventually need governing bodies to help move forward these independent efforts, as well as feed needs back up the supply chain to OpenAPI. Maybe not right away,...[<a href="/2017/09/25/considering-the-future-of-the-openapi-initiative/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/22/not-everyone-needs-api-scale-some-just-need-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-yellow-house.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/22/not-everyone-needs-api-scale-some-just-need-api/">Not Everyone Needs API Scale, Some Just Need API</a></h3>
			<p><em>22 Sep 2017</em></p>
			<p>I know that catering to the enterprise is where the money is at. I know that playing with all the cool new containerized, event sourcing, continuously integrated and deployed solutions are where you can prove you know your stuff. However, in my world I come across so many companies, organizations, and government agencies that just need things to work. They don’t have the skills, resources, or time to play with everything cool, and really could just use some better access to their data and content across their business, with trusted partners, and maybe solicit the help of 3rd party developers to help carry the load. Many of the conversation I am having within startup and tech circles often focus on scale, and the latest tech. I get that this is the way things work in alpha tech circles, and this is applicable in your worlds, always moving forward, pushing the scope of what we are doing, and making sure we are playing with the latest tools is how it’s done. However, not everyone has this luxury, and many companies can’t afford to hire the talent needed, or pay the cost associated with doing things the most modern approach, or even the right way. Remember, when you are talking about Kafka, Kuburnetes, Docker, GraphQL, and other leading edge solutions, you are talking from a place of privilege. Meaning you probably have the time, resources, and space to implement the most modern approach, and have the team to do it right. I’m not trying to stop you from having fun, and doing what you do. I am just trying to share what I’m seeing on the ground at companies, organizations, and government agencies I’m talking to. I’m spending a lot of time trying to help get folks up to speed on everything I’m seeing, and many of them are intimidated by the pace at which things move, the scope of implementations they are reading about across tech...[<a href="/2017/09/22/not-everyone-needs-api-scale-some-just-need-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/22/i-am-a-professional-in-my-industry-where-should-i-begin-with-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/methuselah-mountain.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/22/i-am-a-professional-in-my-industry-where-should-i-begin-with-apis/">I Am A Professional In My Industry, Where Should I Begin With APIs?</a></h3>
			<p><em>22 Sep 2017</em></p>
			<p>A regularly question I get from business folks out in the space, is regarding where they should start with APIs. My world is usually broke into two areas: 1) Providing APIs, and 2) Consuming APIs. I’d say that these business folk I keep coming across could easily span both of these areas, making it significantly more complicated to help them understand where they should be getting started with APIs. With the API landscape being so wide, and APIs becoming so ubiquitous across many industries, helping someone onboard to the concept can get pretty complex and confusing pretty quick. I always try to prime the pump with my API 101 material, and encourage folks to learn about the history of APIs. I find that before you begin getting bombarded with the technical details of APIs it helps to get the lay of the land, and understand what is going on at the highest level, developing a better understanding of how we got here. Before you get working with any single API, you should try to understand why it has begun to be such a big part of everything we know of online today, and via our mobile phones. Most people don’t realize that they are using APIs everyday, as part of their regular business activity, and common things they do in their personal lives–things like buy products from Amazon, and sharing updates with friends on Facebook. Next, I recommend looking at the software you use each day in your work. If you are an architect, look a the CAD software you are using. If you are in healthcare, look at the administrative systems you use, and the devices you put to work. If you are retail, look at the point of sale (POS), and payment systems you use. All of these companies have APIs in one form or another. They might not all be public APIs like Facebook, Twitter, and Google, but they have APIs that...[<a href="/2017/09/22/i-am-a-professional-in-my-industry-where-should-i-begin-with-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/21/the-concept-of-api-management-has-expanded-so-much-the-concept-should-be/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-barn.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/21/the-concept-of-api-management-has-expanded-so-much-the-concept-should-be/">The Concept Of API Management Has Expanded So Much the Concept Should Be</a></h3>
			<p><em>21 Sep 2017</em></p>
			<p>API management was the first area of my research I started tracking on in 2010, and has been the seed for the 85+ areas of the API lifecycle I’m tracking on in 2017. It was a necessary vehicle for the API sector to move more mainstream, but in 2017 I’m feeling the concept is just too large, and the business of APIs has evolved enough that we should be focusing in on each aspect of API management on its own, and retire the concept entirely. I feel like at this point it will continue to confuse, and be abused, and that we can get more precise in what we are trying to accomplish, and better serve our customers along the way. The main concepts of API management at play have historically been about authentication, service composition, logging, analytics, and billing. There are plenty of other elements that have often been lumped in there like portal, documentation, support, and other aspects, but securing, tracking, and generating revenue from a variety of APIs, and consumers has been center stage. I’d say that some of the positive aspects of the maturing and evolution of API manage include more of a focus on authentication, as well as the awareness introduced by logging and analytics. I’d say some areas that worry me is that security discussions often stop with API management, and we don’t seem to be having evolved conversations around service conversation, billing, and monetization of our API resources. You rarely see these things discussed when we talk about GraphQL, gRPC, evented architecture, data streaming, and other hot topics in the API sector. I feel like the technology of APIs conversations have outpaced the business of APIs conversations as API management matured and moved forward. Advancements in logging, analytics, and reporting have definitely advanced, but understanding the value generated by providing different services to different consumers, seeing the cost associated with operations, and the value generated, then charging or...[<a href="/2017/09/21/the-concept-of-api-management-has-expanded-so-much-the-concept-should-be/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/21/that-point-where-api-session-management-become-api-surveillance/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/border-traffic.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/21/that-point-where-api-session-management-become-api-surveillance/">That Point Where API Session Management Become API Surveillance</a></h3>
			<p><em>21 Sep 2017</em></p>
			<p>I was talking to my friends TC2027 Computer and Information Security class at Tec de Monterrey via a Google hangout today, and one of the questions I got was around managing API sessions using JWT, which was spawned from a story about security JWT. A student was curious about managing session across API consumption, while addressing securing concerns, making sure tokens aren’t abused, and there isn’t API consumption from 3rd parties who shouldn’t have access going unnoticed. I feel like there are two important, and often competing interests occurring here. We want to secure our API resources, making sure data isn’t leaked, and prevent breaches. We want to make sure we know who is accessing resources, and develop a heightened awareness regarding who is accessing what, and how they are putting them to use. However, the more we march down the road of managing session, logging, analyzing, tracking, and securing our APIs, we are also simultaneously ramping up the surveillance of our platforms, and the web, mobile, network, and device clients who are putting our resources to use. Sure, we want to secure things, but we also want to think about the opportunity for abuse, as we are working to manage abuse on our platforms. To answer the question around how to track sessions across API operations I recommended thinking about that identification layer, which includes JWT and OAuth, depending on the situation. After that you should be looking other dimensions for identifying session like IP address, timestamps, user agent, and any other identifying characteristics. An app or user token is much more about identification, than it ever provides actual security, and to truly identify a valid session you should have more than one dimension beyond that key to acknowledge valid sessions, as well as just session in general. Identifying what healthy sessions look like, as well as unhealthy, or unique sessions that might be out of the realm of normal operations. To accomplish all...[<a href="/2017/09/21/that-point-where-api-session-management-become-api-surveillance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/21/i-am-not-a-card-carrying-restafarian-i-just-believe-in-the-web/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/statue-face-open-mouth_blue_circuit.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/21/i-am-not-a-card-carrying-restafarian-i-just-believe-in-the-web/">I Am Not A Card Carrying Restafarian I Just Believe In The Web</a></h3>
			<p><em>21 Sep 2017</em></p>
			<p>I am always surprised at the folks who I meet for the first time who automatically assume I’m all about the REST. It is always something that is more telling about the way they see the world (or don’t), than it ever is about me as THE API Evangelist. It is easy to think I’m going to get all RESTY, and start quoting Roy, but I’m no card carrying RESTafarian, like my buddy Darrel Miller (@darrel_miller) (not that is what Darrel does ;-). Really the only thing I get passionate about is making sure we are reusing the web, and I am pretty much be a sellout on almost everything else. I am just looking to understand how folks are exposing interfaces for their digital resources using the web, making them available for use in other applications. I feel like RESTful approaches are always a good start for folks to begin considering, and learning from when beginning their journey, but I’m rarely going to get all dogmatic about REST. There are trade-offs with any approach you take to providing programmatic interfaces using the web, and you should understand what these are whether your are using REST, Hypermedia, (g)RPC, GraphQL, or any other number of protocols and technologies available out there. A RESTful approach using the web just tends to be the lowest common denominator, the cheapest, and widest reaching solution we have on the table. Rarely is it ever the perfect solution–there are no such things. #sorry If you are entering into discussions with me thinking I’m 100% team REST, you are mistaken, and you have profiled yourself considerably for me. It shows me that you haven’t done a lot of (wide) reading on the subject of APIs, and while you may be an expert, you probably are a very siloed expert who doesn’t entertain a lot of outside opinions, and keep an eye on how the space is shifting and changing. When I encounter...[<a href="/2017/09/21/i-am-not-a-card-carrying-restafarian-i-just-believe-in-the-web/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/21/getting-beyond-openapi-being-about-api-documentation/"><img src="http://kinlane-productions2.s3.amazonaws.com/algorotoscope/yellowtree/clean_view/file-00_00_50_85.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/21/getting-beyond-openapi-being-about-api-documentation/">Getting Beyond OpenAPI Being About API Documentation</a></h3>
			<p><em>21 Sep 2017</em></p>
			<p>Darrel Miller has a thought provoking post on OpenAPI not being what he thought, shining a light on a very important dimension of what OpenAPI does, and doesn’t do in the API space. In my experience, OpenAPI is rarely what people think, and I want to revisit once slice of Darrel’s story, in regards to folks generally thinking OpenAPI (Swagger) as being all about API documentation. In 2017, the majority of folks I talk to think OpenAPI is about documenting your APIs–something that always makes me sad, but I get it, and is something I regularly work to combat this notion. First, and foremost, OpenAPI is a bridge to understanding and being able to communicate around using HTTP as a transport, and our greatest hope for helping developers learn their HTTPs and 123s. I meet developers on a regular basis who are building web APIs, yet do not have a firm grasp on what HTTP is. Hell, I’ve had a career dedicated to web APIs for the last seven years, and I’m still developing my grasp on what it is, learning new things from folks like Erik Wilde (@dret), Darrel Miller (@darrel_miller), and Mike Amundsen (@mamund) on a regular basis. In the API game, you should always be learning, and the web is the center of your existence at the moment as a software engineer, and should be the focus of what you are learning about to push forward your knowledge. Darrel has a great line in his post where he has “a higher chance of convincing developers to stop drinking Mountain Dew than to pry a documentation generator from the hands of a dev with a deadline.” Meaning, most developers don’t have the time or interest to learn about what OpenAPIs, or can do for them in their busy world, they just want the help delivering documentation–a very visual representation of the work they’ve done, and is something they can demonstrate to their boss,...[<a href="/2017/09/21/getting-beyond-openapi-being-about-api-documentation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/20/what-were-the-main-api-developments-in-2017/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/amusement-park-2_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/20/what-were-the-main-api-developments-in-2017/">What Were The Main API Developments In 2017</a></h3>
			<p><em>20 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The main API development in 2017 has been the continued shift towards mainstream API adoption. The concept has been moving outside of the tech sector for a couple years now, but in 2017 it is very clear that it’s not just something startups are doing. This is having a profound shift in how we talk about APIs, and how we approach the API lifecycle. APIs have historically been something new and smaller companies are doing, but often will deliver at scale (AWS, SalesForce, eBay, etc.). The mainstream shift in the API sector brings a whole new set of challenges, and opportunities as existing companies, with existing technology in place, work to shift towards an API way of doing things. This shift impacts the technology of doing APIs, but really isn’t the main event–things will be business as usual when it comes to the technology of APIs for many years to come. I’d say the main event has been in the business of doing APIs. How these APIs get funded will be entirely different from how startup focused APIs get funded. This shift in financial incentives behind why APIs are developed, operated, and ultimately deprecated, will have profound effects on what is an API. They will have less of the startup shine, and become more robust, providing commercial, and industrial grade digital resources that are more mature than the newer, younger APIs we’ve seen in recent years. Alongside this shift, another development in the business of APIs has occurred. The funding landscape for startups has...[<a href="/2017/09/20/what-were-the-main-api-developments-in-2017/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/20/what-are-the-unsolved-problems-in-the-api-space/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/sand-hand_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/20/what-are-the-unsolved-problems-in-the-api-space/">What Are The Unsolved Problems In The API Space?</a></h3>
			<p><em>20 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. There are always endless numbers of fabricated unsolved problems in the API space. These are unsolved problems that are usually unsolved because they were just made up to get someone to buy a new service or product. They aren’t real problems. Technology is good at being applied to make believe problems, vendor fabricated problems, and solving real problems created by the last couple of waves of technology. I’d say that biggest unsolved problem is how APIs can actually solve the legacy technical debt challenges large companies, institutions, and government agencies face. There is a lot of rhetoric regarding how APis can unwind all of this, but honestly there are few examples of it in practice. With many API efforts in 2017, bogged down in cultural friction, and a web (pun intended) of technical complexity. One aspect of this problem of legacy technical debt is the problem of delivering technology in an Internet age, without actually embracing the web. People doing APIs don’t always have the knowledge of the web, and what makes it work before they get to work doing APIs. Vendors are offering up tools and services that provide web API solutions, but don’t always embrace the web, and the existing standards and protocols that make the web work. APIs are the next evolution of the web, and rely on many of the same concepts for it to work. When you ignore the web when doing APIs, you will always face challenges in interoperability, and reuse, and often end up building siloed solutions...[<a href="/2017/09/20/what-are-the-unsolved-problems-in-the-api-space/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/20/using-jekyll-as-a-hypermedia-client/"><img src="https://s3.amazonaws.com/kinlane-productions2/subway/hypermedia-siren-subway-stop.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/20/using-jekyll-as-a-hypermedia-client/">Using Jekyll As A Hypermedia Client</a></h3>
			<p><em>20 Sep 2017</em></p>
			<p>I am picking up some of my past work, so that I can move forward in a new way. A while ago, I began working on my subway map API to help me articulate aspects of the API lifecycle, and provide a “vehicle” for helping folks explore some often complex API concepts, in a way that would incrementally introduce them to new ideas. I used the subway map as an analogy because it has been historically used to help folks understand complex systems, and help them navigate it, even if they don’t fully understand everything about it. I gave a talk at @APIStrat in Austin, TX on this subject, but something I haven’t moved forward in over a year. My new approach to using the subway map model is still using hypermedia (Siren), but I’m not wanting a single API to control the data for every client. I’m looking to develop a static, federated approach to delivering subway map experience. I want to be able to quickly publish a common map, but then be able evolve them independently, designed for specific implementations and use cases. Since I’m so Jekyll and Github centered in how I deliver projects, I’m looking for a way to do this in a static way, that can be forked. So, I got to work on publishing Siren YAML to Github, and seeing if it is possible to use Liquid and HTML as the client. Again, I want this to be static. All this could easily be building this in JavaScript, but I want things static and forkable. For my proof of concept I published 15 “stops” along the request “line” for my API design “area”. I don’t have the visual elements present for this functionality, as I just wanted to prove that I could use Liquid and HTML for a hypermedia client, using Siren YAML published to Github. I was forced to add a layout: property to my Siren schema, which...[<a href="/2017/09/20/using-jekyll-as-a-hypermedia-client/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/20/looking-to-2024-what-do-apis-look-like/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/redes-fast-flux-623x425_blue_electricity.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/20/looking-to-2024-what-do-apis-look-like/">Looking To 2024, What Do APIs Look Like?</a></h3>
			<p><em>20 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. I’m not a big fan of predictions. It is a game that analysts and investors play to try and shape the world they want to see. I’m usually focused on shaping the world I want to see by understanding where we are, how we got here, and making incremental shifts in our behavior today. I tend to think that technology futurists are more about ignoring the past, and being in denial about today, than they are ever about what is happening in the future. However, with that said, let me share some thoughts about what I think the future will hold when it comes to doing APIs. Mostly, by 2024 things will look much like they do today. Nothing moves as fast we think they will in the tech sector. I’ve done a lot of studying the history of predictions by technology blogs, and analysts firms over the last decade about what they thought 2017 would be like, and it is 98% bullshit. It was more about getting what they wanted in the year they made the prediction, than it was ever about the future. Technology always feels like it is moving faster than ever before but honestly, what has happened in the last decade that is really seismic? I’d say iPhone is the biggest, with mostly everything else being pretty incremental, and slow moving. By 2024, we will still be struggling with technical debt. However, the debt limit ceiling will have been raised significantly. We won’t have decoupled the monolith, all while adding...[<a href="/2017/09/20/looking-to-2024-what-do-apis-look-like/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/19/who-are-the-most-influential-people-and-companies-to-keep-an-eye-on-in-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/19/who-are-the-most-influential-people-and-companies-to-keep-an-eye-on-in-api/">Who Are The Most Influential People And Companies To Keep An Eye On In API</a></h3>
			<p><em>19 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. When it comes to the most influential people and companies in the API space that I am keeping an eye on, it always starts with the API pioneers. This begins with SalesForce, eBay, and Amazon. Then it moves into the social realm with Twitter and Facebook. All of these providers are still moving and shaking the space when it comes to APIs, and operating viable API platforms that dominate in their sector. While I do not always agree with the direction these platforms are taking, they continue to provide a wealth of healthy, and bad practices we should all be considering as part of our own API operations, even if we aren’t doing it at a similar scale. Secondarily, I always recommend studying the cloud giants. Amazon is definitely the leader in this space, with their pioneering, first mover status, but Google is in a close second, and enjoys some API pioneering credentials with Google Maps, and other services in their stack. Even though Microsoft waiting so long to jump into the game I wouldn’t discount them from being an API mover and shaker with their Azure platform making all the right moves in the last couple of years as they played catch up. These three API providers are dictating much of what we know as being APIs in 2017, and will continue to do so in coming years. They will be leading the conversation, as well as sucking the oxygen out of other conversations they do not think are worthy. If you aren’t...[<a href="/2017/09/19/who-are-the-most-influential-people-and-companies-to-keep-an-eye-on-in-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/19/what-is-the-biggest-challenge-for-big-companies-doing-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/castle-wall-circuits.JPG" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/19/what-is-the-biggest-challenge-for-big-companies-doing-apis/">What Is The Biggest Challenge For Big Companies Doing APIs?</a></h3>
			<p><em>19 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The biggest challenge for big companies doing APIs is always about people and culture. Change is hard. Decoupling things at large companies is difficult. While APIs can operate at scale, they excel when they do one thing well, and aren’t burdened with the scope, and complexity of much of the software systems we see already operating within large companies. These large systems take large teams of people to operate, and shifting this culture, and displacing these people isn’t going to happen easily. People are naturally skeptical of new approaches, and get very defensive when it comes to their data, content, and other digital assets, as it can be seen as a threat to their livelihood–opening up and sharing these resources outside their sphere of influence. The culture that has been established at large companies won’t be easily undone. It is a culture that historically has had a pretty large gulf between business groups, and the IT groups who delivered the last generation of APIs (web services), that weren’t meant to be accessible, and understandable by business users. Web APIs have become simpler, more intuitive, and have the potential to be owned, consumed, and even in some cases deployed by business users. Even with this potential, many of the legacy rifts still exist, and business users feel this isn’t their domain, and IT and developer groups often feel APIs are something that should stay in their domain–perpetuating and confusing existing challenges already in place. While there may be small API success within large companies, they...[<a href="/2017/09/19/what-is-the-biggest-challenge-for-big-companies-doing-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/19/what-has-been-the-biggest-change-in-the-industry-since-i-started-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/beach-rocks-currents_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/19/what-has-been-the-biggest-change-in-the-industry-since-i-started-api/">What Has Been The Biggest Change In The Industry Since I Started API</a></h3>
			<p><em>19 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The biggest change in the industry since I started doing API Evangelist in 2010 is who is doing APIs. In 2010 it was 95% startups doing APIs, with a handful of enterprise, and small businesses doing them. I’d say over the last couple years the biggest change is that this had spread beyond the startup community and is something we see across companies, organizations, institutions, and government agencies of all shapes and sizes. Granted, there are a variety when it comes to the level they are doing them, and the quality, but APIs are something that has been moving mainstream over the last seven years, and becoming more commonplace in many different industries. In 2010 it was all about Twitter, Facebook, Amazon, and many of the API pioneers. This has been rapidly shifting to each wave of startups like Twilio, Stripe, Slack, and others. However, now in 2017 I am seeing insurance companies, airlines, car companies, universities, cities, and federal agencies with API programs. I mean, c’mon, Capital One has an API program (wink, wink). While I still hold influence with each wave of API service providers looking to sell to the space, and many of the API startup providers, my main audience is folks on the frontline of the enterprise, and government agencies at all levels. I also have a growing number of people at higher educational institutions tuning into what I’m writing as they look to evolve their approach to technology. APIs were mainly a startup thing in 2010, and in 2017...[<a href="/2017/09/19/what-has-been-the-biggest-change-in-the-industry-since-i-started-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/19/what-apis-excite-me-and-fuels-my-research-and-writing/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-gas-pumps.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/19/what-apis-excite-me-and-fuels-my-research-and-writing/">What APIs Excite Me And Fuels My Research And Writing</a></h3>
			<p><em>19 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The number API that gets me out of bed each day, with an opportunity to apply what I’ve learned in the API sector is with the Human Services Data API (HSDA). Which is an open API standard I am the technical lead for which helps municipalities, and human service organizations better share information that helps people find services in their communities. This begins with the basics like food, housing, and healthcare, but in recent months I’m seeing the standard get applied in disaster scenarios like Hurricane Irma to help organize shelter information. This is why I do APIs. The project is always struggling for funding, and is something I do mostly for free, with small paychecks whenever we receive grants, or find projects where we can deliver an actual API on the ground. Next, I’d say it is government APIs at the municipal, state, but mostly at the federal levels. I was a Presidential Innovation Fellow in the Obama administration, helping federal agencies publish their open data assets, take inventory of their web services. I don’t work for the government anymore, but it doesn’t mean the work hasn’t stopped. I’m regularly working on projects to ensure RFPs, and RFIs have appropriate API language in them, and talking with agencies about their API strategy, helping educate them what is going on in the private sector, and often times even across other government agencies. APIs like the new FOIA API, Recreational Information Database API, Regulations.gov, IRS APis, and others will have the biggest impact on our...[<a href="/2017/09/19/what-apis-excite-me-and-fuels-my-research-and-writing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/18/why-was-my-week-of-api-rants-so-well-received/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/36349140070_d5ec39cb34_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/18/why-was-my-week-of-api-rants-so-well-received/">Why Was My Week of API Rants So Well Received?</a></h3>
			<p><em>18 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. A couple weeks back I spent the entire week ranting on API Evangelist, instead of my usual lineup of API stories. Normally these types of stories end up on KinLane.com, or my rants edition, and usually don’t get tweeted out. I’m just venting. However on this particular week, I had enough people “piss in my cheerios”, that I felt the space needed to hear my rants, instead of the usual “nice guy” tone I tend to take on here. Granted, I can be pretty outspoken and blunt in my storytelling, but I usually work very hard to keep a professional tone, and be as nice as I possibly can. There are plenty of assholes in the API space, and I really don’t want to be one of them–even though it comes pretty naturally for me. ;-) I actually got tired of the tone by mid-week, but I had so many people Tweet, email, and post on LinkedIn, Facebook, and Slack that they were enjoying the rants, I kept it going until that Friday. I was moving to New York, and really didn’t have much time to do the normal amount of research it takes to write stories, and the rants were an easy want to get content up that took me about 10 minutes to write. So why did people like the posts? First, I have to say that not everyone did. I heard from a number of other people that thought I was being a diva, and found the tone offensive. I also...[<a href="/2017/09/18/why-was-my-week-of-api-rants-so-well-received/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/18/why-did-we-need-the-api-evangelist/"><img src="http://s3.amazonaws.com/kinlane-productions2/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/18/why-did-we-need-the-api-evangelist/">Why Did We Need The API Evangelist?</a></h3>
			<p><em>18 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. You needed the API Evangelist because there was nobody paying attention to the big picture of the API space. Sure, there are many vendors who pay attention to the big picture, and there are analysts who are paid to pay attention to the bigger picture to help validate the vendors, but there is nobody independent. At least there wasn’t in 2010 when I started. Now, there are a number of leading API experts and visionaries who work at different companies, and are able to maintain an independent view of the space, but in 2010 this did not exist. I’d like to think I helped make such a thing possible, but honestly it probably would have happened without me. Developer advocates, and evangelists tend to pay attention to a specific API, set of APIs, or API services and tooling. I pay attention to everything. I keep an eye on as many APIs as I possibly can, and work to evaluate all the services, tools, and technology that emerges on the landscape. I try to remain objective about what is working, and what is not, and share stories about both. I still have my biases, and tend to hold grudges against a few companies for their bad behavior, but for the most part I’m just trying to share an honest view of what is going on at the 100K view. Something that differs from the analysts, because I don’t have a vendor driven agenda, I’m just looking to understand. Another area that I benefit the space...[<a href="/2017/09/18/why-did-we-need-the-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/18/what-it-was-about-web-apis-that-first-captured-my-attention/"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/18/what-it-was-about-web-apis-that-first-captured-my-attention/">What It Was About Web APIs That First Captured My Attention?</a></h3>
			<p><em>18 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. In the spring of 2010 I was ready for a career shift. I was running North American event for SAP, and had also taken up running events for Google, which included Google I/O and Developer Days. I was the VP of Technology, and made all the decisions around usage of tech, from email blasts, to registration, session scanning, and follow-up reporting. When I took over the role I was dealing with a literal hostage colocation facility for server infrastructure, and massive hardware expenditure on servers that I didn’t need most of the year. Then in 2007 I began using the Amazon Cloud, and got to work re-engineering systems to be more API-centric, leverage AWS APIs to orchestrate my operations. By 2007 I had been playing around with web APIs for some time. I had incorporated payment and shipping APIs into commerce systems, and integrated Flickr, Delicious, Twitter, Facebook and other APIs into applications. I had plenty of SOAP web service experience when it came to enterprise infrastructure, but this was the first time I was deploying global infrastructure at scale using web APIs. I realized that web APIs weren’t just hobby toys, as my SAP IT director in Germany called them, they were an actual a tool I could use to operate a business at scale. My success resulted in more work, taking on more events, and scaling operations, which didn’t always pencil out to me actually being happier, even though the events scaled more efficiently, and out-performed what had come before. The two...[<a href="/2017/09/18/what-it-was-about-web-apis-that-first-captured-my-attention/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/18/what-is-the-role-of-an-influencer-in-the-api-industry/"><img src="http://i1.wp.com/restlet.dreamhosters.com/wp-content/uploads/2013/12/kinlane.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/18/what-is-the-role-of-an-influencer-in-the-api-industry/">What Is The Role Of An Influencer In The API Industry?</a></h3>
			<p><em>18 Sep 2017</em></p>
			<p>I am spending two days this week with the Capital One DevExchange team outside of Washington DC, and they’ve provided me with a list of questions for one of our sessions, which they will be recording for internal use. To prepare, I wanted to work through my thoughts, and make sure each of these answers were on the tip of my tongue–here is one of those questions, along with my thoughts. The idea of an influencer in the API space will mean many things to many different people. I have pretty strong opinions about what an influencer should do, and it is always something that should be as free of product pitches as it possibly can. Influencing someone in the API space should mean that you are not just influencing their decision to buy your product or service. That is sales, which has it’s place, but we are talking about influencing. I would also add that influencing SHOULD NOT be steeped in convincing folks regarding what they should invest in, at the technology purchasing level, all the way up to the venture capital level. The role of an influencer in the API industry should always be about education, awareness, and helping influence how average flks get everyday problems solved. Being an influencer always begins with listening and learning. We are not broadcasting or pitching, we want to influence, so we need to have an idea about who we are influencing, and what will resonate and help them solve the problems they face. I do a significant portion of this by reading blogs, tuning into Twitter, and spending time on Github understanding what folks are building. Next, I engage in conversations with folks who are doing APIs, looking to understand APIs, and listening to what their challenges are, and what matters to them. At this stage I am not influencing anyone. I am being influenced. I’m absorbing what is going on, educating myself about what the...[<a href="/2017/09/18/what-is-the-role-of-an-influencer-in-the-api-industry/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/15/sensibly-thinking-about-where-technology-ends-and-the-human-part-begins-with/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/mosaic-face_blue_circuit_3.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/15/sensibly-thinking-about-where-technology-ends-and-the-human-part-begins-with/">Sensibly Thinking About Where Technology Ends And The Human Part Begins With</a></h3>
			<p><em>15 Sep 2017</em></p>
			<p>Our team at a hackathon I’m participating in this week is working on a data aggregation tool for helping merge multiple hurrican shelter data sources from Irma in Florida. While the need for the data is winding down, the use case for the tool could be something that lives on, and could help communities in the future. This projects aggregates multiple data sources for shelters from FEMA, municipal, and sources pulled together by volunteers. Our team is focused on aggregating, and doing as much heavy lifting to automatically merge and cleanse the data as they can, but then at the right moment render it for humans to step in and finish the work. I was impressed with the balance struck by the team. Knowing where to apply technology, and when to rely on humans. The problem of merging open data from multiple sources is a big and complex one. It is one that I’ve seen many technologists think they can step up and solve simply with their tech toolbox, no humans necessary. Our team quickly saw the scope of the program, discussed at length about what they could accomplish, and what they couldn’t accomplish, then got to work in the code to deliver the functionality, but then also developed a web interface to allow humans to step in at just the right point. Striking a balance between the human and technological aspects of doing this–which is what the Human Services Data Specification (HSDS) is all about. I know there is a significant amount of information out there about User Experience (UX), and also increasingly Developer Experience (DX). However, I think skills to know where to apply technology, and when to step back from using it, and focusing on augmenting, empowering, and putting the humans in charge are seriously deficient in our sector. I regularly encounter developers who think that technology is the solution, and humans are the problem. This contempt always degrades the amount of...[<a href="/2017/09/15/sensibly-thinking-about-where-technology-ends-and-the-human-part-begins-with/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/15/regulations-creeping-in-on-ai-ml-cognitive-and-other-fronts/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/supreme-court-judgement.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/15/regulations-creeping-in-on-ai-ml-cognitive-and-other-fronts/">Regulations Creeping In On AI, ML, Cognitive, And Other Fronts</a></h3>
			<p><em>15 Sep 2017</em></p>
			<p>I wrote an piece earlier today about not fearing AI, but possessing a significant amount of concern when it comes to the people behind. I figured I’d continue with the trend on this Friday afternoon, and talk about the coming regulations when it comes to artificial intelligence (AI), machine learning (ML), and everything cognitive, intelligent, and algorithmic. I am not fully a believer in regulations being the only solution, but I know they are the solutions that bigcos tend to pay attention to. Which is why they spend so much money to distort, and bend them to what they want to see in their industries. We are entering a phase of the Internet where there are going to an increased number of calls for regulations. Whether it’s privacy, security, breaches, or specifically on technology like drones, artificial intelligence, bots, and machine learning, expect more government involvement in the future. This isn’t because government is inheriently bad, and is looking to suffocate business, it is primarily because these areas of technology are being defined by the worst among us. When you bundle with the not so bad folks, and even many of the good folks refusing to reign in their industry partners, and fellow technologists, you end up with more regulations imposed to stablize things. If the tech space was more willing to step up and take the lead regarding acceptable practices, this wouldn’t be necessary. Algorithms are making more decisions in our lives. After seeing what Facebook and Twitter have done during the last US election, and seeing AI and ML continue being applied to important aspects of our lives, there will be more inquirires by the government, and calls for the government to step in. I know that platforms don’t want to be regulated, and with very libertarian stances in much of Silicon Valley, there is a significant undertow of anti-regulatory, and anti-government sentiment. However, if you believe in the wisdom of the crowds,...[<a href="/2017/09/15/regulations-creeping-in-on-ai-ml-cognitive-and-other-fronts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/15/i-will-see-you-at-apistrat-in-portland-this-november/"><img src="https://s3.amazonaws.com/kinlane-productions2/apistrat/apistrat-portland-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/15/i-will-see-you-at-apistrat-in-portland-this-november/">I Will See You At APIStrat In Portland This November</a></h3>
			<p><em>15 Sep 2017</em></p>
			<p>We are putting the finishing touches on the schedule for APIStrat in Portland, OR, October 31st through November 2nd. We have all the workshops, sessions, and keynotes dialed in (not all keynotes announced, wink, wink), and it is all just about making sure y’all show up and participate in the conversation. This is my 2nd favorite part of the event, the build-up for the big day(s). This is the 8th APIStrat we’ve done, and it is the first one we’ve done as part of the Linux Foundation, and with the OpenAPI Initiative. I’m excited. Make sure and take a look at the session schedule. We received over 165 submissions, and had a program committee of almost 30 people vote to decide with 60 would be accepted. I am the program chair and helped make some difficult decisions, but ultimately I”m pretty proud of the lineup we’ve pulled together. It’s much of the same popular topics as you’ve seen at previous events, with new faces, and brands, but there is also some of the leading edge conversations around serverless, gRPC, GraphQL. Of course, there is also going to be a lot of talk about OpenAPI, in workshops, sessions, and on the main stage. So check out the schedule if you haven’t, it’s pretty sweet lineup. I want to personally thank Microsoft, Stoplight, SmartBear, Postman, CapitalOne DevExchange, APIMATIC, Red Hat, Google, and Cloud Elements for sponsoring and making sure APIStrat happens. Of course, thank you to The Linux Foundation, and the OpenAPI Initiative (OAI) for taking the lead on APIStrat as it continues to grow and mature with the API community. I want to also thank my partner in crime, Steve Willmott, and the 3Scale / Red Hat team–without them APIStrat wouldn’t be a thing. Next up for me, now that the schedule is dialed in. Is to just tell stories about what will be happening. I’m going to go through each of the speakers, and companies...[<a href="/2017/09/15/i-will-see-you-at-apistrat-in-portland-this-november/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/15/i-do-not-fear-ai-i-fear-the-people-doing-ai/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/servers-blue-circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/15/i-do-not-fear-ai-i-fear-the-people-doing-ai/">I Do Not Fear AI, I Fear The People Doing AI</a></h3>
			<p><em>15 Sep 2017</em></p>
			<p>There is a lot of FUD out there when it comes to artificial intelligence (AI) and machine learning (ML). The tech press enjoy yanking people’s chain when it comes to the dangers of artificial intelligence. AI is coming for your jobs. AI is racist, sexist, and biased. AI will be lead to World War III. AI will secure and protect us from the bad out there. AI will be the source of all of our worries, and the solution to all of our worries. I’m interested in the storytelling around all of this, and I’m fascinated by the distracting quality of technology when it comes to absolving the humans behind of doing bad things. We have the technology to make this black boxes more observability and accountable. The algorithms feeding us news, judging us in courtrooms, and deciding if we are insurable or a risk, can all be wrapped with APIs, and made more accountable. However, there are many human reasons why we don’t do this. Every AI out there can be held accountable, it isn’t rocket science. The technology exists to keep AI from hurting us, judging us, and impacting our lives in negative ways. However, it is the people behind who do not want it, otherwise their stories won’t work. Their stories won’t have the desired effect and control over our lives. APIs are the layer being wielded for good and for bad on the Internet. Facebook, Twitter, and Reddit, all leverage APIs to be available on our mobile phones. APIs are how people automate, advertise, and fund their activities on their platforms. APIs are how AI and ML are being exposed, wielded, and leveraged. The technology is already there to make them more accountable, we just don’t have the human will to use the technology we have. There is more money to be made in telling wild stories about what is possible. Telling stories that make folks afraid, and in awe of...[<a href="/2017/09/15/i-do-not-fear-ai-i-fear-the-people-doing-ai/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/14/using-3rd-party-apis-to-break-you-out-of-your-enterprise-bubble/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-bubbles.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/14/using-3rd-party-apis-to-break-you-out-of-your-enterprise-bubble/">Using 3rd Party APIs To Break You Out Of Your Enterprise Bubble</a></h3>
			<p><em>14 Sep 2017</em></p>
			<p>I’m participating in a hackathon in Princeton, New Jersey as part of my work on the Human Services Data API (HSDA). We are at a large enterprise financial group’s office, as part of a three day social good hackathon / code sprint. Everybody participating is taking time off from their normal day job as back-end or front-end programmer, and business analyst, to build something for the greater good. Since it is an enterprise developer group the concept of a hackathon is somewhat new to them, and is the first time they’ve worked on external projects, instead of an internally focused hackathon event. I’m enjoying watching the two teams working on human services projects be forced out of their bubble. One of the projects has three separate 3rd party APIs to work with. 1) Simple spreadsheet deployed web API, 2) government agency published web API, and 3) HSDA API operated by a municipal organization. I am sitting here watching them get exposed to the variety of implementations, quality of data and interface, and wrestle with establishing their project requirements. After being pulled from their bubble trying to understand the APIs, they are also finding themselves pulled out of their local development world, having to potentially use 3rd party tools, services, and even reverse engineering a library or codebase in a language they are not familiar with. This is all very, very healthy. No matter what gets built at this hackathon, the fact that they are being pulled out of their bubbles, will benefit their world. They are thinking outside their governance bubble. They are forced to learn about the API best or worst practices of other organizations. They are having to use services, tools, and programming languages they aren’t familiar with. All with the motivation of potentially building something for good. They are exercising their skills and knowledge in ways that they won’t encounter in the routine, and highly structured worlds they exist in. Another layer...[<a href="/2017/09/14/using-3rd-party-apis-to-break-you-out-of-your-enterprise-bubble/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page8" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page10" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home Page</a></li>
    <li><a href="/blog/">The Blog</a></li>
    <li><a href="https://101.apievangelist.com/">API 101</a></li>
    <li><a href="http://history.apievangelist.com">History of APIs</a></li>
    <li><a href="https://women-in-tech.apievangelist.com/">Women in Technology</a></li>    
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
