<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>API Evangelist</title>
  <updated>2021-07-30T00:00:00Z</updated>
  <link rel="self" href="http://localhost:4000/atom.xml"/>
  <author><name>Kin Lane</name></author>
  <id>http://localhost:4000/atom.xml</id>
	<entry>
    <title>API Specifications Update for July 30th, 2021</title>
    <link href="http://apievangelist.com/2021/07/30/api-specifications-update-for-july-30th-2021/"/>
    <updated>2021-07-30T00:00:00Z</updated>
    <content><![CDATA[I try to stay up to speed with what is happening across the leading API specifications each week. The evolution of OpenAPI, AsyncAPI, and JSON Schema is important to what I do at Postman, but also to the wider API space. I find it helpful to understand what is happening across each of the API specification communities, and I wanted to also find a way to share what I am seeing with everyone else. Providing a single digest of what is happening, without having to do all the work I am doing each week. While I am sure there is much more going on that I am not tuned into, here is a snapshot of what I see happening. Past Meetings The OpenAPI initiative held its weekly technical developer community (TDC) meeting on Thursday, and you can find the agenda for the session here. In addition to the weekly TDC meeting, there is growing cadence of special interest group (SIG) meetings emerging with the security and travel SIGs both meeting this week. The groups are still getting going and establishing their meeting cadence and agenda. As I gather more links and resources I will make sure and share them. The most I have right now is a link to a pull request to add a SIG page to the OAI specification repository. Mike Ralphson (@permittedsoc), Frank Kilcommins, and I are also holding open office hours across all three specs every Thursday at 8:00 AM PDT, and you are welcome to join in--this update is the result of the work occurring within that meeting. Upcoming Meetings The OpenAPI TDC, Travel SIG, as well as the API specification public office hours will be repeating next week. There will also be an AsyncAPI SIG meeting on August 3rd— you can join in via the calendar or sign up to the mailing list on the home page of the AsyncAPI website. I am pushing to get the OpenAPI SIGs more visible in coming weeks to help with onboarding of new users. Additionally you can get in on the ground floor of upcoming potential meetings within the JSON Schema community for their JSON Schema interface description language vocabulary, and chime in on whether or not they should we have a regular call like OpenAPI TSC? #23. Providing some significant opportunities to help define the future of the fast growing JSON Schema specification. Interesting Issues I like to dive into the issues fore each of the specs to understand what folks are asking for and encountering when using the specs. I found the Include JSend in OpenAPI Spec #2664, Ability to provide message when specifying pattern for a query parameter in OpenAPI YAML files #2662, and Usage of field vs property #2660 to be compelling in the OpenAPI community. Over in the AsyncAPI universe I found Address perspective and channel reuse issues through introducing optional 'endpoint' concept #599, Support JSON Schema Draft 2020-12 #596, should binding specs be tied to specific versions of the AsyncAPI spec?...]]></content>
    <id>http://apievangelist.com/2021/07/30/api-specifications-update-for-july-30th-2021/</id>
  </entry><entry>
    <title>Seeing Api Change</title>
    <link href="http://apievangelist.com/2021/07/13/seeing-api-change/"/>
    <updated>2021-07-13T00:00:00Z</updated>
    <content><![CDATA[--- published: true layout: post title: 'Seeing API Change' image: https://kinlane-productions2.s3.amazonaws.com/algorotoscope-master/copper-circuit-old-barn-falling-over.jpg --- The API landscape within any enterprise organization is ever changing and evolving, and with APIs being such an abstract and often undocumented aspects of organization work, it can be very difficult to “see” APIs, let alone the forward motion and change of our API infrastructure. As much as we’d love for our teams to properly document all APIs used to power web, mobile, device, and network APIs we depend upon and iterate upon each day, the reality is that APIs regularly possess paths, parameters, schema, and other properties that aren’t always well documented and made “visible” as part of the API development process. This ever expanding shadow landscape of our enterprise API operations is the where the latest waves of startups like Akita Software&gt; are making their mark on the API lifecycle. Akita Software describes itself as an API observability service that helps organizations build API behavior models that expose the reality that exists across our existing API factory floor, then allow teams to produce diffs, merge, and extend definitions of our APIs. I love the type of innovation that Akita Software represents in the API space because they don’t directly see API specifications like OpenAPI in the same light as the rest of us do, they see what exists in the cracks and shadows of the OpenAPIs we produce as part of our regular API operations. Akita Software allows you to watch API traffic from a server, a browser, and via a proxy, and produce OpenAPI maps of the API landscape behind our applications and integrations, producing a more honest view of the ever changing, evolving, and potentially breaking waves of API change under the hood of our companies, organizations, institutions, and government agencies. While the end goal in using a service like Akita Software might be about delivering more up to date documentation, or ensuring the coverage of your API testing is more complete, my belief in why their approach matters is because it allows us to see API change. Just pause for a moment and think about how you “see” an API. Is it documentation? Is it the end application or integration? Then think about how you see the change that occurs across the evolution of existing APIs and the introduction of new APIs. It isn’t easy. This is why innovation at this front line of our API operations is so important. As many enterprise organizations invest in API observability in the sense that they should be testing and monitoring API instances, those who are further along in their API journey will soon realize that this effort is limited by what is known of your the API landscape. And that API observability involves perpetually discovering the reality of what APIs are in use, diff’ing that against what is known, and increasing the coverage of API observability efforts. I have conversations with Jean Yang of Akita Software in my archives that I haven’t been able to...]]></content>
    <id>http://apievangelist.com/2021/07/13/seeing-api-change/</id>
  </entry><entry>
    <title>Different Types of API Infrastructure All Working in Concert</title>
    <link href="http://apievangelist.com/2021/06/21/different-types-of-api-infrastructure-all-working-in-concert/"/>
    <updated>2021-06-21T00:00:00Z</updated>
    <content><![CDATA[API is a pretty catch all acronym for what is happening across the enterprise these days. I haven’t been writing enough here on the blog to help me separate the layers of the API onion as I work each day, so I wanted to take a crack at doing some storytelling around the different types of APIs I am seeing enterprise organizations work with. An API can be any type of programmatic interface, including language, hardware, or the more web-based, but the biggest defining characteristic of APIs in my experience are how they power business operations, and where in the supply chain they exist. Ultimately, the impact that APIs have on business is very much defined where they live within business operations, and how they work together overall to move a business forward—-here are the dimensions I am thinking about at the moment. Operations ~ This is one area that I find folks have trouble seeing APIs, and realizing that our APIs have APIs. Realizing that Github, Amazon, Jenkins, Kubernetes, and our other pieces of critical infrastructure have APIs isn’t a stretch for many, but applying them to API operations always seems like just out of reach for most teams cognitive load. In my mind, this is where true API governance will come from, and if we are looking to automate more of the API factory floor this is where we should be looking. Microservices ~ Developing simple, do one thing well services are here to stay, even with the complexity they introduce. Enterprise organizations are working to decompose their monolith into many individual services that utilize a variety of different patterns as part of overall operations. Making discovery, traceability, and observability a critical part of operations, as well as having an honest approach to mapping out, understanding, and minimizing dependencies across operations. Partner ~ Even before many enterprise organizations publish any API publicly, they are exposing digital services to their partners, laying the foundation for public API operations, but centered on making valuable resources available to trusted partners. Partner APIs are where enterprise organizations get their feet wet when it comes to exposing internal resources to the outside world, but are doing so in a way that has a more direct line to revenue and value creation. Public ~ While much of the growth in APIs over the last five years has been in the microservices and partner realm, public APIs are still increasing as a way of doing business on the web. From my position there really isn’t much different between partner and public APIs, but there is an important distinction here when it comes to helping organizations make the proper investment in their API Operations, and while not all may feel they aren’t ready for public APIs, they can take what they’ve learned with partners and keep growing their investment. 3rd Party ~ I feel like this one is the most obvious, but has become a surprising area of growth and adoption for many organizations I am...]]></content>
    <id>http://apievangelist.com/2021/06/21/different-types-of-api-infrastructure-all-working-in-concert/</id>
  </entry><entry>
    <title>The Different Ways to Engage with the OpenAPI Specification</title>
    <link href="http://apievangelist.com/2021/06/21/different-types-of-api-infrastructure-all-working-in-concert-copy/"/>
    <updated>2021-06-21T00:00:00Z</updated>
    <content><![CDATA[

I am spending time thinking about the big picture of how each of the leading API specifications are operating, and trying to find the best way to communicate the different ways people in the community can get more involved. To help put each of the building blocks used by each of the API specifications front and center I wanted to profile each of the building blocks used by OpenAPI, AsyncAPI, and JSON Schema. Helping me better understand the building blocks in use, but then also better understand how I can amplify, cross-pollinate, and drive more participation.

When it comes to OpenAPI, these are the three links I would give folks to get introduced to the specification:

    Website
    About
    Blog

Next I would recommend you get connected with OpenAPI via their primary social channels to stay in tune with what is going on:

    Twitter
    LinkedIn Group
    Github Org

Then when it comes to actually begin learning about the specification and what it does I recommend these channels:

    Documentation
    Introduction
    Getting Started
    Best Practices    
    Testimonials
    FAQ    
    Email List
    Calendar       

Once you learn about the spec and want to actually get involved in moving the specification forward:

    Specification
    Specification Repo
    Proposals
    Technical Steering-committee
    Contribute
    Registry

Then, if you want to go deeper like I have, I recommend diving into the business behind the OpenAPI specification:

    Charter
    Members
    Membership Inquiry Form
    Membership Join

There are also a handful of additional content resources that are worth looking at to understand OpenAPI:

    Style Guide
    Presentations    
    Wikipedia     

Then we can get into the legal details of how it all works if you want to understand this part of specification:

    Privacy Policy
    Terms Of-service
    Trademark Usage

Then there are two event related links that are worth noting, providing other ways to get involved in the community&quot;

    Meetups
    Conference


This provides a nice snapshot of how you can engage with the OpenAPI Initiative (OAI) at almost every level. I would say that all of this has come together organically over the years, but it provides a nice look at the overall landscape eof the OAI. I have all of this stored as an API so that I can keep updating, grouping, and using to understand how we can better engage with the OAI, but also help me compare how the OpenAPI specification is managed against other specs. Next I will take a similar look at AsyncAPI, and help me better understand the &quot;diff&quot; between how they are operated.]]></content>
    <id>http://apievangelist.com/2021/06/21/different-types-of-api-infrastructure-all-working-in-concert-copy/</id>
  </entry><entry>
    <title>Business-First API Design and Development</title>
    <link href="http://apievangelist.com/2021/06/21/business-first-api-design-and-development/"/>
    <updated>2021-06-21T00:00:00Z</updated>
    <content><![CDATA[I spend a lot of time thinking about API-first design and development, where you craft an OpenAPI definition, mock, document, and iterate upon the design of an API before you develop it. An API-first approach to the design and development of an API comes with a number of benefits for a team and the resulting API, but it is something that there can still be a lot of resistance to when it comes to gaining adoption across an organization. Some teams will be open to learning how to design their APIs using OpenAPI before they ever write any code, but others will resist for a variety of reasons, some founded, and others un-founded. I am always looking for ways that I can improve upon how I talk about API-first, but also how I make it better meet the needs of folks on the ground of API operations. My latest attempt in this area I am calling business-first API-first design and development—-it is a boring name, but it is what I'll start with and hopefully polish over time. Beginning with an API-First Design Approach My approach to API-first has gotten pretty polished through repeated demos with Postman customers and analysts. I have been able to distill down a pretty straightforward approach to designing an API using OpenAPI in a collaborative way down to a handful of steps. Providing with a variety of hands-on API-first examples that exist within public API workspaces, which leverage these API-first design building blocks. Workspace - Designate a specific workspace for each bounded context which an API is being developed, ensuring that all stakeholders are able to find not just the artifacts for each API, but also all of the operations around the API. Ensuring that there is always a dedicated workspace to find everything you need to produce or consume an API. Team - Invite individuals to be part of the team who will have access to the workspace, artifacts, and other elements of the design, development, and operations of any API. Ensuring all stakeholders have access to the work going on around each API, and is part of the conversation occurring around the design and development that is going on. OpenAPI - The machine readable API specification is the center of not just the API design process, but ultimately the entire lifecycle for the API. Every API begins with an OpenAPI definition that provides the details of the request and response surface area for each API path. Providing a machine and human readable contract for what an API does. Comments - The ability to have a conversation about each OpenAPI, as well as each individual parts of an OpenAPI. Allowing team members to be able to discuss each path, parameter, header, body, response, schema, example, and any other moving part of the API being defined. Keeping the conversation thread associated with each API contract. Mock - Using the OpenAPI for each API to generate a mock representation of an API from the examples provided for...]]></content>
    <id>http://apievangelist.com/2021/06/21/business-first-api-design-and-development/</id>
  </entry><entry>
    <title>My Oracle vs Google API Copyright Journey</title>
    <link href="http://apievangelist.com/2021/04/13/My-oracle-vs-google-api-copyright-journey/"/>
    <updated>2021-04-13T00:00:00Z</updated>
    <content><![CDATA[Soon after beginning my journey as the API Evangelist in 2010 I began quickly realizing that there was more to this game than just technology. The mission of the blog was to get beyond the technology of APIs, and focus on the business of APIs. Which in 2010 was emerging as the biggest driver of why and how you do APIs, over the actual technical styles, practices, and approaches to delivering API infrastructure. I also quickly realized that it wasn’t just about business, and that there direct and indirect political layers that determine which direction APIs are pushing the tech sector. One critical conversation involving APIs that spans both the business and politics of APIs was the Oracle vs. Google copyright case. I had read about the case back in 2010 and 2011, but I really didn’t understand what it meant until the case got up and running in 2012. Kicking off an almost decade long journey for me as I studied and thought about API copyright, followed the case, then actually begin contributing to the case as part of the EFF’s push to help keep APIs free from copyright, ultimately coming to a conclusion this last week as the Supreme Court returned their decision that APIs were indeed copyrightable, but Google’s use of the Java APIs was indeed fair use. The Oracle vs. Google API copyright case began as two parts, 1) for patent violations, and 2) for copyright infringement involving the Java API. The jury found Google in violation of copyright law for the 11,500 lines of code they reused from the Java API, but acknowledged that it fell under fair use, and then found no fault in the claim of patent violations. However, in the end Judge Alsup declared that fair use didn’t apply because you can’t actually copyright APIs—-which gets at the truth of all of this in my belief. The judge went the extra mile by actually learning Java so that he could understand the layers of an API, and how they actually work, something that if the Supreme Court had evaluated API copyright, they would have come to same conclusion. Anyways, of course Oracle filed an appeal, which ultimately overturned the the previous decision ad stated not only were APIs copyrightable, but Google’s usage of the Java API did not fall under fair use. Leaving APIs as something that is copyrightable, and some enterprise organizations feeling pretty emboldened about suing you for reuse or reimplementation. Thankfully Google elevated the conversation to the Supreme Court, bringing us to the decision last week that stepped us back from the edge a little bit with a pretty broad interpretation of what fair use means when it comes to APIs—something that I feel will give us the room we need to properly grow and innovate, but ultimately we need to help folks see that it is better for everyone if copyright doesn’t apply to APIs all. A Decade of Learning and Thinking About API Copyright I have spent...]]></content>
    <id>http://apievangelist.com/2021/04/13/My-oracle-vs-google-api-copyright-journey/</id>
  </entry><entry>
    <title>Aligning the API Specification Contribution Process Across OpenAPI, AsyncAPI, and GraphQL</title>
    <link href="http://apievangelist.com/2021/04/02/aligning-the-api-specification-contribution-process-across-openapi-asyncapi-and-graphql/"/>
    <updated>2021-04-02T00:00:00Z</updated>
    <content><![CDATA[Mike Ralphson (@permittedsoc) suggested that the OAI follow AsyncAPIs lead when it comes to adopting a GraphQL like approach to managing contributions to the specification in the OpenAPI specification technical steering committee (TSC) yesterday. AsyncAPI has followed GraphQL lead as it enters the Linux foundation, and it is something that the OAS TSC is strongly considering. Here are links to the existing contribution guidelines for these three specifications: GraphQL Contribution Guidelines AsyncAPI OpenAPI Admittedly, the OAS “participation” framework is a bit of a special snowflake, which has been mentioned might be one aspect of operations that might be preventing the specification from shifting gears into its next phase of development. To help me learn more about the approach I wanted to break down the GraphQL and AsyncAPI approach. GraphQL - Guiding Principles Backwards compatibility - Once a query is written, it should always mean the same thing and return the same shaped result. Future changes should not change the meaning of existing schema or queries or in any other way cause an existing compliant GraphQL service to become non-compliant for prior versions of the spec. Performance is a feature - GraphQL typically avoids syntax or behaviors that could jeopardize runtime efficiency, or that make demands of GraphQL services which cannot efficiently be fulfilled. Favor no change - As GraphQL is implemented in over a dozen languages under the collaboration of hundreds of individuals, incorporating any change has a high cost. Accordingly, proposed changes must meet a very high bar of added value. The burden of proof is on the contributor to illustrate this value. Enable new capabilities motivated by real use cases - Every change should intend on unlocking a real and reasonable use case. Real examples are always more compelling than theoretical ones, and common scenarios are more compelling than rare ones. RFCs should do more than offer a different way to reach an already achievable outcome. Simplicity and consistency over expressiveness and terseness - Plenty of behaviors and patterns found in other languages are intentionally absent from GraphQL. &quot;Possible but awkward&quot; is often favored over more complex alternatives. Simplicity (e.g. fewer concepts) is more important than expressing more sophisticated ideas or writing less. Preserve option value - It's hard to know what the future brings; whenever possible, decisions should be made that allow for more options in the future. Sometimes this is unintuitive: spec rules often begin more strict than necessary with a future option to loosen when motivated by a real use case.Understandability is just as important as correctness - The GraphQL spec, despite describing technical behavior, is intended to be read by people. Use natural tone and include motivation and examples. AsyncAPI - Guiding Principles Favor no change - As AsyncAPI is implemented in many languages under the collaboration of a lot of individuals, incorporating any change has a high cost. Accordingly, proposed changes must meet a very high bar of added value. The burden of proof is on the contributor to illustrate this value.Enable new...]]></content>
    <id>http://apievangelist.com/2021/04/02/aligning-the-api-specification-contribution-process-across-openapi-asyncapi-and-graphql/</id>
  </entry><entry>
    <title>When API Examples Become the Real Thing</title>
    <link href="http://apievangelist.com/2021/03/31/when-api-examples-become-the-real-thing/"/>
    <updated>2021-03-31T00:00:00Z</updated>
    <content><![CDATA[I am using Postman public workspaces to manage all of my projects right now, and as part of my Postman collection workspace I have a variety of collections where I am bending the concept of how Postman was intended to be used. I was needing many little APIs for my API specification projects to organize a mix of data I will be using to automate and orchestrate information gathering, publishing, and any other task I can automate in my world. After considering using AWS API Gateway + DynamoDB for this, I thought that it would be easier, and more cost effective for me to just use Postman. The platform doesn’t have an API deployment capability, and we do have integrations like with AWS API Gateway, but I really feel like these simple little APIs were even too small for justifying the increase on my AWS bill—I was determined to find a way to do on Postman. I knew I could accomplish what I wanted on Postman, I just needed to think out of the box a little. After some brainstorming I decided to just define each individual data store as a single collection, use the examples as the actual data store, then publish a mock server—-treating the result as more of a “static” API than a mock for testing or other purposes. I ended up with six separate static APIs that are hosted 100% on Postman. Articles (Docs) (Data) - Interesting articles that have been written about Postman Collections. Competition (Docs) (Data) - A list of example of how Postman competitors are using Postman Collections. Government (Docs) (Data) - Showing the different government agencies who are using Postman Collections. Pages (Docs) (Data) - The interesting pages that API Providers have published showcasing their collections. Partners (Docs) (Data) - Demonstrating how Postman partners are importing and exporting Postman Collections. Tooling (Docs) (Data) (Data) - All of the tooling I track on that is built around the Postman API schema specification. Each of these data store have their own URL, and can be iterated upon independently. They are all published in a public workspace so anyone can discover and use the static API collections, fork the collection, and mock using your own account. Sort of Bring Your Own Mock (BYOM), that way the rate limits on the API calls do not affect my accounts. I am using my simple APIs to manage data for each of these resources so that I can use as part of different types of orchestrations, maybe tweeting out something about a tool or article, to maybe parsing the domain behind each of the pages and pulling a screenshot. I have a whole mix of different API storytelling outcomes I am looking to set in motion, and these data stores will play a central role in helping me automate things. One thing I really like about this approach is I get documentation for each data store automatically as part of each collection—-I just have to provide good descriptions....]]></content>
    <id>http://apievangelist.com/2021/03/31/when-api-examples-become-the-real-thing/</id>
  </entry><entry>
    <title>A Workspace for Defining the API Lifecycle</title>
    <link href="http://apievangelist.com/2021/03/30/a-workspace-for-defining-the-api-lifecycle/"/>
    <updated>2021-03-30T00:00:00Z</updated>
    <content><![CDATA[I am moving my API lifecycle definition into a public workspace so that I can be a little more disciplined in how I version and move forward. It is a pretty lightweight draft at this point because I still have to coordinate internally more about Postman around it, as well as with some partners, but this first version helps me frame the discussions I am having. To help guide the process of defining the API lifecycle I created an OpenAPI to help provide the scaffolding for an API to define the API lifecycle, using Postman mocks and documentation to make things a little more tangible and real.



The creation of the OpenAPI, then generation of the Postman collection and example to help act as the data store for my static API lifecycle API provides me with a nice separation of the structure of how I’d like to talk about the API lifecycle, and the actual data and content that defines the API lifecycle. Using Postman public workspaces as an incubator for defining the API lifecycle in a machine readable and collaborative way. Allowing anyone to discover and play with via the public API workspace, but also allow them to fork and contribute to the work via pull requests, and engage in the feedback loop using Postman comments on the collection or the API—-further separating the conversation around the structure and details of the API lifecycle.]]></content>
    <id>http://apievangelist.com/2021/03/30/a-workspace-for-defining-the-api-lifecycle/</id>
  </entry><entry>
    <title>A Workspace for Defining API-First</title>
    <link href="http://apievangelist.com/2021/03/30/a-workspace-for-defining-apifirst/"/>
    <updated>2021-03-30T00:00:00Z</updated>
    <content><![CDATA[Alongside my API lifecycle public workspace I have established an API-first public workspace to help me guide conversations around what is API-first. Like the API lifecycle it is another area we use a lot, but don’t always have a coherent and relatable meaning behind exactly what it means. To get really meta, I am using an API-first process to define the API-first process (mind blown). I am using Postman to define an OpenAPI, which provides me with a structure and schema for how I will be defining API-first, but then I use collection examples to store the API-first definition I am evolving, then I mock and document it to make it a little more tangible, making it all available in a single API workspace.



I am looking to get more precise when I talk about the API lifecycle and API-first. There is no better way to be precise than using an API defined using an API-first process. I learned a lot by setting up the OpenAPI, collections, environment, and mock server for this definition. It pushed me to think a little more deeply about what it is I am trying to define, and pushed me to separate the scaffolding and framework for the definition, from the actual details of the implementation. The API-first workspace provides me with a framework and definition for pushing me forward in this journey, but it also makes it accessible to others to join in via the public workspace, and contribute to the framework via the OpenAPI, or the actual API-first definition using the collection, allowing anyone to fork and submit pull requests, or just leave feedback via the API or collection level comments.]]></content>
    <id>http://apievangelist.com/2021/03/30/a-workspace-for-defining-apifirst/</id>
  </entry><entry>
    <title>A CSV to JSON File Conversion Postman Collection</title>
    <link href="http://apievangelist.com/2021/03/29/a-csv-to-json-file-conversion-postman-collection/"/>
    <updated>2021-03-29T00:00:00Z</updated>
    <content><![CDATA[I am regularly needing to convert CSV files into JSON, and to help me manually get the job done, as well as automate on a schedule or via the Postman API, I created a simple request for pulling a CSV file and converting into a JSON response, and then rendering using the Postman Visualizer.



You can view the documentation for the collection in my file format conversion public workspace. I will be adding other file format conversion requests for this collection as I need them. If you have one you’d like to add feel free to fork the collection and add it yourself! ;-) If you have any feedback, feel free to comment on the collection, or any part of requests.]]></content>
    <id>http://apievangelist.com/2021/03/29/a-csv-to-json-file-conversion-postman-collection/</id>
  </entry><entry>
    <title>A Collection To Generate Static APIs Using Postman Mock Servers</title>
    <link href="http://apievangelist.com/2021/03/29/a-collection-to-generate-static-apis-using-postman-mock-servers/"/>
    <updated>2021-03-29T00:00:00Z</updated>
    <content><![CDATA[I have a lot of little datasets I am organizing for use across a variety of projects. Since Postman has replace Github as my place where I begin all of my API projects, I figured that it could also be used host all of my data projects as APIs. Postman collections often times resemble an OpenAPI definition in that it provides a reference of an API, but in practice collections go much further, allowing you to store example data so that it can be better used in documentation, mocks, testing, and other types of purposes. You can store examples in OpenAPI, but Postman really allows you to go the distance when it comes to actually bringing those examples to life as a documented static API using Postman collections and mock server. An example of this in action can be found with [my Fortune 500 public workspace](https://api-evangelist.postman.co/workspace/Fortune-500~08e01ed9-6906-47a0-a81e-369948912ef4/overview). I needed a list of 2020 Fortune 500 companies so that I can use it a variety of data mining and automation projects. I can easily download the data and publish a proper API, but it is so easy for me to use Postman collection in conjunction with the mock feature to publish simple mock APIs for each of my datasets. Using Postman as a sort of poor mans API hosting, which I won’t be opening up for public use, but will be using across a number of my API Projects. However, just because I won’t be sharing the URLs of my static (mock) APIs, I will be sharing all the collections I have developed around this work, beginning with my collection for generating my static (mock) data API from a CSV. This is just a first version of my collection for pulling any CSV, converting the CSV to JSON, creating a collection, adding the JSON as an example, and then publishing my collection to my public workspace. Next I will be adding an option for adding JSON converted from CSV to an existing collection, and also auto-generation of the mock server and environment with the URL published. It provides me with a quick way to pull any CSV file and quickly generate a collection that I can then share, export, mock, and make available for us in other workflows and orchestrations. The results provides me with a simple static API for all the Fortune 500 companies for both 2019 and 2020. This isn’t a normal usage of Postman collections and mock servers, but it does provide a quick and dirty way to generate static APIs from common datasets, then reuse that data across a variety of workflows and orchestrations. It reflects how I have used Github for the last 8 years, as a static host for all of my data. Now that I have Postman workspaces for all my data and API storage, I am converting all my data stores to be static APIs in this way. With Postman monitors and pre-request and post-request scripts in Postman, I am able to go a...]]></content>
    <id>http://apievangelist.com/2021/03/29/a-collection-to-generate-static-apis-using-postman-mock-servers/</id>
  </entry><entry>
    <title>Targeting the Enterprise and Ignoring Developer’s Needs Around a Specific Stop Along the API LIfecycle</title>
    <link href="http://apievangelist.com/2021/03/28/targeting-the-enterprise-and-ignoring-developer-s-needs-around-a-specific-stop-along-the-api-lifecycle/"/>
    <updated>2021-03-28T00:00:00Z</updated>
    <content><![CDATA[From time to time I see folks tweet how they are frustrated with Postman not doing exactly what they want in a particular area of the API lifecycle, letting us know they are moving on to greener pastures. The most common framing for these emotional tweet bursts usually centers around Postman catering to the enterprise and is forgetting our roots when it comes to being their specific reason hey adopted Postman. From my vantage point there are a number of ACTUAL reasons behind why Postman users invoke in these Tweet storms, with “selling to the enterprise” being actually pretty low on the list regarding why they find themselves in the position they are in—let me see if I can break down how I see this recurring theme. Selling to the Enterprise In response to Postman selling to the enterprise—yes we are. We are selling to the enterprise because we are, well….a business, and a venture backed business. The majority of our “developers”, “customer”, and “users” operate within the enterprise, so we are listening to what they need and providing a platform for them to be more successful. Sure, we are motivated by revenue, but this doesn’t mean we aren’t motivated by feedback from our users as well. Selling to the enterprise is a reality for any startup who is building software, and it really shouldn’t be a surprise to anyone, or really be much of a talking point when it comes to critique of any API service provider. I would ask a few things of anyone taking this position. Who do you sell to? - Do you plan on not selling to the enterprise as part of the business you are building, and tif you do, are you going to weight responses from developers who are not affiliated with an enterprise organization, over those who are? Always have a Plan B, C, and D - You should plan that EVERY platform you depend on will eventually head in a different direction, shut down, or generally stop meeting your needs, and have alternate suppliers available, this is business 101. Enterprise Complexity - Saying a platform has grown too complex for you means it may have, but it also might mean that maybe you aren’t aware of how a platform works, as well as maybe be out of alignment with the larger community a platform services. There is a little bit of truth to the statement about Postman selling to the enterprise. But there is whole lot omitted when someone tweets about this on Twitter. I would invite anyone using the “Postman is selling to the enterprise” to come argue why we shouldn’t to our board and investors. It really isn’t a defensible position in any form. I get it. I am a developer, and I am not always in agreement with enterprise way of doing things, but I am also not in denial about how the startup world works. I have had lots of frustrations with Postman capabilities over the years,...]]></content>
    <id>http://apievangelist.com/2021/03/28/targeting-the-enterprise-and-ignoring-developer-s-needs-around-a-specific-stop-along-the-api-lifecycle/</id>
  </entry><entry>
    <title>Setting a Baseline API Lifecycle Definition</title>
    <link href="http://apievangelist.com/2021/03/25/setting-a-baseline-api-lifecycle-definition/"/>
    <updated>2021-03-25T00:00:00Z</updated>
    <content><![CDATA[I find myself on this quest at regular intervals throughout the last decade-—seeking to define what the API lifecycle means. However, this time I am determined to ground myself in a vocabulary and set of visualizations that ground me an my storytelling around the API lifecycle, helping me be more coherent and precise when I talk about delivering APIs. Over the last decade I have worked to define what I would consider over a hundred stops along a lifecycle, but in 2021 I am looking to distill that down into the most meaningful, impactful, and wide reaching approach to describing what the API lifecycle is. To help me stabilizing my storytelling I’ve established this baseline API lifecycle definition. Publisher Lifecycle The first part of the API lifecycle definition is all about the publisher, something I have historically called deployment, but I think publisher provides a cleaner representation of what is happening-defined in a way that makes it inclusive to non-developers, helping make the API lifecycle more accessible to business groups. Design - A diverse API toolbox and full lifecycle approach to API design. Mock - Utilizing a mock representation of a service to drive an API-first approacht to brginning the lifecycle. Document - Ensuring there is always up to date, meaningful, and living documentation present for APIs. Test - Providing 100% test coverage for APIs when it comes to contract, integration, performance, and security. Publish - The act of bringing an API to life via using a gateway or artisinally hand turned on a lathe. Manage - Managing onboarding, authentication, authorization, rate limiting, logging, reporting, and quantifying value exchange. Monitor - Monitoring, automating, and orchestrating acorss API operations on a schedule from many different cloud regions. Discover - Ensuring that APIs are discoverable by default, and every API detail is gathered in transit and centralized in real-time. Collaborate - Ensuring all stakeholders are part of the process, and a feedback loop is present across all APIs. This vocabulary is intentionally using words that are common across the API lifecycle. I went for softer, wider reaching, and inclusive terms, but still stuck with the vocabulary you see on the “streets”. Similar to why I chose the word publish, all of these words will resonate with non-developers, making conversations and stories more inclusive. You notice I did not use the word development anywhere in here, and my list also has the important human aspect of collaboration. Consumer Lifecycle Next, I wanted to make sure and show the consumer side of the API lifecycle, pushing anyone I am talking to or who is reading my stories to think about the other side of the coin. I am looking for this side of the lifecycle to speak to both publisher and consumer, helping force publishers out of their siloed, and pull back the curtain a little bit when it comes to API consumers relationship with API publishers. Here is what I have outline when it comes to describing the consumer side of the API...]]></content>
    <id>http://apievangelist.com/2021/03/25/setting-a-baseline-api-lifecycle-definition/</id>
  </entry><entry>
    <title>An Event-Driven View of the API Lifecycle</title>
    <link href="http://apievangelist.com/2021/03/24/an-eventdriven-view-of-the-api-lifecycle/"/>
    <updated>2021-03-24T00:00:00Z</updated>
    <content><![CDATA[I am continuing my quest to define and visualize the API lifecycle across a diverse API toolbox. I am talking to anyone and everyone I possibly can when it comes getting their take on what the API lifecycle is, and what are the ways in which we can make more visible and tangible. I am meeting regularly with the Solace team to define the API lifecycle across a request and response, as well as an event-driven world. As part of our discussions Jonathan Schabowsky (@jschabowsky) shared his earlier vision of how he sees the event-driven API lifecycle which I though was worth documenting and including the visualization as part of my wider research. Jonathan breaks things down into four main areas or top level groups of the stops along his event-driven API lifecycle, but I think his outline provides a pretty interesting look at the API lifecycle from the view of an event-driven API service providers. Discover Event Streams: Search and locate events which are of interest Relationships: Understand the relationship between events, their sources and who is consuming them Define Use Case: Determine desired business outcome Schema: Create business objects that represent the event context Event: Construct event metadata including addressing (topic) and headers Specify Application: link events as inputs and outputs to the business logic Develop Generate code: Use code generators to create application “scaffolding” based on the application specification Add business logic: Create application which performs the required function Operate Secure: Enforce authentication and authorization policies Deploy: Start and connect to the runtime event broker Audit: Determine if there are any runtime vs. design time violations Monitor: Understand the utilization of events, schemas and applications based on historical and real-time metrics Improve: Enhance applications, events and schemas; deprecate events that aren’t being utilized Anytime I look at someone else’s definition of the API lifecycle I immediately see things that are missing, but then I always check myself knowing that my bias is always towards my perspective and view of the API lifecycle at the 500K view. I am not down in the trenches of API operations, although I have visibility into many different types of API operations. I strongly identify with his four areas, and think his individual stops along the lifecycle are simple and purposeful, representing both reality on the ground, but also lean towards having a strategy—-not just responding to what is happening. I am adding Solace’s view of the API lifecycle to my notebook. The outline and the visual. I am coming closer to establishing a baseline definition for what I’d consider to be a modern API lifecycle, but my biggest challenge is ensuring my definition is simple and relatable to the widest possible audience. I am notorious for providing TOO MUCH INFORMATION, and will lay out over 100 stops along the API lifecycle, which is not what folks need at any point in their journey. My mission is to get more precise in how I talk about the API lifecycle and distill things down to a simple vocabulary that I can then also work to visualize....]]></content>
    <id>http://apievangelist.com/2021/03/24/an-eventdriven-view-of-the-api-lifecycle/</id>
  </entry><entry>
    <title>Working to Visualize the API Lifecycle</title>
    <link href="http://apievangelist.com/2021/03/23/being-able-to-see-the-api-lifecycle/"/>
    <updated>2021-03-23T00:00:00Z</updated>
    <content><![CDATA[It is hard to have a discussion about things we can’t see. Some of us excel at dealing in the abstract, but most people prefer discussion involving more visual and tangible things. One of the critical aspects of the world of APIs I am hell bent on trying to visualize lately is the API lifecycle. This is one of this ubiquitous phrases you hear perpetually in the space (me included), but is something that means something different depending on who you talk to, and there just aren’t many very meaningful or quality visual representations of what the API lifecycle is.  As I gather up examples of how different API service and tooling providers work to visualize the API lifecycle I am going to share then here on the blog, with this one from digitalml being pretty interesting.



There are several things I really like about digitalml's approach to visualizing the API lifecycle. First it is people centered! Next I really like the pulley system showing the conplexity and multiple flows of the API lifecycle--I am always frustrated when the API lifecycle is a single linear direction. There are some things I don't like, mainly that it misses much of the sustainment portion of this conversation, and is just one slice of the API lifecycle from a single vendors view. But this is the game right? I can't fault them for sharing their slice of the pie--I do it all the time. Overall, it is a pretty slick looking visualization of the lifecycle that I think with some refinements could provide a pretty meaningful way to help paint a dynamic picture of what the API lifecycle actually is and does.

I am working really hard to lock down a single vocabulary for describing the API lifecycle. I am also very, very, very interested in coming up with a set of visual representations to go along with this vocabulary. I feel like the API lifecycle should be dynamic in nature to meet the different needs of different types of API Operations, but I can’t get past thinking that we are collectively missing out on significant forward motion by not coming to an agreement on what the hell we mean when we talk about the API lifecycle. As I keep noodling and researching on the topic I’ll publish any more thoughts or healthy examples here on the blog. If you have any examples or ideas feel free to tweet or email at me to let me know how you see things.


]]></content>
    <id>http://apievangelist.com/2021/03/23/being-able-to-see-the-api-lifecycle/</id>
  </entry><entry>
    <title>Managing API Change, Discovery, and Documentation Using Bump</title>
    <link href="http://apievangelist.com/2021/03/22/managing-api-change-discovery-and-documentation-using-bump/"/>
    <updated>2021-03-22T00:00:00Z</updated>
    <content><![CDATA[I have seen a lot of API service providers come and go over the last decade, and I always appreciate when simple, useful, and purpose built API solutions emerge. I can use up most of my fingers counting the number of innovative API startups to emerge in the last couple of years, which is something that really makes me happy. One of the API startups that caught my attention lately, and I recently had the pleasure of connecting with, was Bump. I usually like to formulate my own words for describing what an API startup does, but I think that Bump’s description does it more justice than I could. Bump is the first API contract management platform that helps document and track APIs: we intelligently identify changes in the APIs structure, and keep developers up to date. This isn’t just an API documentation solution. This is an API change management solution that realizes that documentation is the face of change within most organizations. Bump is an elegant, beautiful, and purposeful API change management solution built on the foundation of OpenAPI and AsyncAPI, helping ensure that your API documentation is future proof. It isn’t future proof just because of the of the built-in change management. It is future because it realizes that a diverse API toolbox is required to get us into the future, and that OpenAPI and AsyncAPI are how you will manage the evolution of API operations into this future. Ensuring that the surface area of your HTTP, TCP, MQTT, AMQP, Kafkam and other APIs are all working in concert, and are machine readable by default, but also human readable in a way that allows developers to always keep up with what is going on. Bump represents the latest wave of API service providers who are refining upon the existing building blocks of doing APIs, but also work to more seamlessly weave these building blocks into our existing software development lifecycle using Git and the CLI, while also abstracting away some of the entropy introduced through the forward motion of our API operations. Then smartly, Bump adds a discovery layer to all of this with their API hubs offering, touching on many of the top pain points of any API provider in one simple yet elegant SaaS offering, while also innovating in new and useful way. Which demonstrate a pretty solid understanding of how to deliver APIs effectively, but also having put some serious thought into how you can actually move things forward in any sort of meaningful way. Resulting in a startup that I am pretty eager to see evolve and mature as a mainstream API solution all by itself, or augmenting another solution like Postman. API service providers like Bump reminds me ow far we have come when it comes to API documentation over the last decade. Documentation is the most meaningful and tangible stop along the API lifecycle, and I enjoy seeing innovation at the edges of documentation, which historically has been a pretty commoditized portion...]]></content>
    <id>http://apievangelist.com/2021/03/22/managing-api-change-discovery-and-documentation-using-bump/</id>
  </entry><entry>
    <title>Postman Will Do for APIs What Github Did for Open Source</title>
    <link href="http://apievangelist.com/2021/03/20/postman-will-do-for-apis-what-github-did-for-open-source/"/>
    <updated>2021-03-20T00:00:00Z</updated>
    <content><![CDATA[I woke up thinking about something Abhinav said recently, “What GitHub did to code, I imagine Postman will do that for APIs, shortening the cycle from using an API to building things with it.&quot; Yes! That is how I see Postman. GitHub changed my relationship with code in many ways. Github plays a significant role in how I produce, manage, and evolve the code that I produce for my own side projects and businesses, as well as the code I engage with as part of my job and the projects I work on. This blog post is my mental exercise of thinking through what GitHub has given me when it comes to code, which in my world translates pretty nicely to what I do with APIs. First, I published my thoughts about what GitHub did for code, and now I want to explore what Postman is doing for APIs. Users Home - Give me a summary home page for my world. Profile - Let me manage my public and private profile. Followers - Let me follow other users Postman profiles. Following - Let people follow my Postman profile. Comments - I want to be able to se all of my comments. Discussions - I want to see my discussions with team members. Settings - Easy configuration of my account. Notifications - Allow me to receive notifications. Reminders - Provide me with reminders of common tasks. Session - Allow me to see all of my sessions. Teams - Help me manage the teams I am on. Organizations - Help me manage the orgs I belong to. Workspaces - Help me manage my workspaces. Identity - Help me manage my identity. Keys - Give me robust API key management. Tokens - Take the friction out of token management. Export - Allow me to export my account. Organizations Details - Mange the details of my organization(s). People - Manage all the people in my organization(s). Teams - Help me manage many different teams across orgs. Workspaces - Do the hard work of organizing workspaces. APIs - Allow me to manage hundreds or thousands of A?PIs. Collections - Help me manage many derivative API collections. Environments - Generate and manage my environments for me. Mock Servers - Ensure there are mock servers available. Scripts - Assist me in managing the scripts used across operations. Runners - Execute runners manually or as part of pipelines via CLI. Monitors - Helps me effectively run monitors across all APIs. Governance - Provide me with tools for governing API operations. Setting - A robust set of settings for configuring my organizations. RBAC - Give me role based access control for my organizations. Teams Details - Manage the details of my team(s). Invitations - Easily invite users to be part of teams. Members - Help me manage the members of my teams. Discussions - I want to be able to chat with team members. Reviews - I can assign reviews for my teams. Reminders - I can establish team...]]></content>
    <id>http://apievangelist.com/2021/03/20/postman-will-do-for-apis-what-github-did-for-open-source/</id>
  </entry><entry>
    <title>Defining API Security with Eric Sheridan (@eric_sheridan) of WhiteHat Security</title>
    <link href="http://apievangelist.com/2021/03/12/defining-api-security-with-eric-sheridan-of-whitehat-security/"/>
    <updated>2021-03-12T00:00:00Z</updated>
    <content><![CDATA[I sat down with Eric Sheridan (@eric_sheridan) of WhiteHat Security this week to talk about API security. I have been working with Eric as a partner of Postman for a number of months now, and I find their approach to security, plus the open source software and Postman collections they are building very thought provoking, so I wanted to begin recording some of our discussions. We were coming together to talk about their API security testing collectiona&gt;, but then ended up talking about the big picture of API security and how it fits into API governance. WhiteHat Security has API security solution you can run using Docker combined with a Postman collection to scan any API you have defined using a Postman collection, adding the much needed API security scanning to the API lifecycle.

Eric and I started our session intending to walk through their API security collection, and while we did that, something I had said in a previous conversation around API governance had stuck in Eric’s brain, so we worked through the concept of API governance blueprint that would include security scanning, something that their Postman collection would help deliver—-resulting in almost an hour long conversation about the role of API security in the API lifecycle and resulting governance.



The WhiteHat Security API testing collection already provides extremely rich feedback on the state of security with any API you have defined as a Postman collection, but we began brainstorming how this could also include machine readable output that could be used as part of a wider API governance observability strategy. Every run of a Postman collection produces an output which can be aggregated, indexed, and made available alongside other testing, performance, and operational characteristics. The goal, as discussed in the video would be a machine readable blueprint for API governance which includes security, which can then be implemented and reported upon in an automated way. Helping standardize how we operate APIs, making them more tangible and accountable in all the ways that matter the most.

Eric and I will be sitting down on a regular basis working through a variety of API security topics, ranging from the very technical down in the weeds, all the way up to industry level API security certification, reporting, regulation, and other high level concerns. You can checkout the public workspace for WhiteHat Security on the Postman API Network, and stay tuned here for future conversation between us about API security. I will be pulling more of these behind the scenes conversations I am having with smart people out into the open like this, helping me be more observable in all of the API lifecycle and governance conversation I am having in my work. Otherwise all of these great discussions tend to get chewed up by the calendar moving forward each, and I’d much prefer sharing everything I am learning with the audience here so that it can reach further than just my brain.]]></content>
    <id>http://apievangelist.com/2021/03/12/defining-api-security-with-eric-sheridan-of-whitehat-security/</id>
  </entry><entry>
    <title>What is Your API Lifecycle?</title>
    <link href="http://apievangelist.com/2021/03/11/what-is-your-api-lifecycle/"/>
    <updated>2021-03-11T00:00:00Z</updated>
    <content><![CDATA[I like asking questions on Twitter then leaving and coming back to see the great answers people leave. Sometimes I get crickets, but depending on how I phrase the question, and how people interpret my question, I might get a stream of interesting views of the world of APIs. I recently asked a simple question. What is your API lifecycle? This is one of the ubiquitous phrases that we use in the API space that has no concrete definition and means many different things to many different people. I was sitting on a call listening to a conversation, and figured I'd tweet and ask folks what their perspective was. What is your API lifecycle? Mine is, design, mock, document, test, deploy, manage, SDKs, secure, performance, monitor, evangelize.&amp;mdash; API Evangelist (@apievangelist) February 24, 2021 I really like Mike's view of the API lifecycle, but more importantly how he &quot;codifies&quot; his lifecycle. It isn't just about a linear set of stops along a lifecycle, and more about a healthy loop you can use to bring APIs to life. /* w/ repeating interations *//* exit-any b4 deploy *//* YMMV */do { explore, select, model, design, describe, sketch, prototype, build, document, test, secure, deploy} until { freeze, redirect, deprecate, donate}#API360&amp;mdash; Mike Amundsen (@mamund) February 25, 2021 Then Mike v2.0 chimes in with his honest and precise view of the landscape and how all of this truly works, introducing one of the most important stops along a modern API lifecycle--regret. One I see often is develop, deploy, document, regret, forget, sunset.&amp;mdash; Mike Ralphson (@PermittedSoc) February 24, 2021 Then Mike v3.0 jumps in with a design heavy view of the lifecycle that occurs between human beings. Mike makes it more like having a conversation about what is needed, then quickly deprecating and deleting. Discover and Define vocab: is the thing/capability defined? If not define.Discover and enhance vocab: relationships/synonyms with other things / affordances.API Design by my 3 questions.Implement primary and aliases.Iterate on feedback and usage data.Deprecate.Delete.&amp;mdash; Michael Hibay (@hibaymj) February 24, 2021 Then Frank demonstrates that emojis are an essential tool in all of our API toolbox. He also brings the wine/dine to the API lifecycle which I vote we make a required part of every API lifecycle. understand - [design - mock - review - (🗑️)] - [develop - secure - detailed docs - collections/flows] - [deploy - publish to dev portals] - [maintain - support devs - evolve] - retire (🌇)last stage has several cycles of (set-deprecation - wine/dine - unset-deprecation) 😋&amp;mdash; Frank Kilcommins (@fkilcommins) February 26, 2021 Next Marc contributes very sobering test-driven view of the API lifecycle, acknowledging that there will be bugs and tears. I really like his contribution of the challenge stop along the API lifecycle, which should also be required for all APIs. For the moment we only Design-&amp;gt;Challenge internally-&amp;gt;Document-&amp;gt;Develop-&amp;gt;Human Tests-&amp;gt;Deploy-&amp;gt;Bugfix. I would be glad to have the time to setup testing automation (more over for regression tests). Mocking+Challenging externally (public APIs) should be a must too !&amp;mdash; Marc...]]></content>
    <id>http://apievangelist.com/2021/03/11/what-is-your-api-lifecycle/</id>
  </entry><entry>
    <title>Using Postman Testing for API Governance</title>
    <link href="http://apievangelist.com/2021/03/10/using-postman-testing-for-api-governance/"/>
    <updated>2021-03-10T00:00:00Z</updated>
    <content><![CDATA[I have been evolving my approach to API governance with Postman since I started working there 1.5 years ago, but recently I am finding more time to invest in how I am “testing” the surface area of an API using it’s OpenAPI, and rolling I tall up in a public workspace I am simply calling governance. It is a slightly different approach then what you see with other JSON Schema centered approaches, which is something I am evolving side by side, but for this workspace of API governance collections I was going for a more free form, scripted based approach, where I may or may not use a JSON Schema to validate. To help me prepare for my upcoming conversations I wanted to record a walkthrough of my API governance workspace as it stands today.



I am just getting started with this work. The scripts I have are still pretty hacky, and need a lot of work, but it provides me with a set of API governance collections that anyone can implement, but then also evolve to meet their own needs. I am purposefully publishing these API governance “tests” or “questions” as a suite of different collections, with each question as an individual request. I am counting on them being a very modular buffet of what is possible with API governance. Ideally these just help jumpstart the imagination of developers who have better JavaScript skills than I do, so I can just start forking and working with others API governance collection, but until that happens I’ll make do with my tacky versions. ;-) They get the job done and demonstrate what is possible.

I am just looking to showcase how you can use existing Postman testing capabilities for API governance with this public workspace. I am hoping that the use of existing Postman capabilities for testing, and the ability to script API governance tests in JavaScript will make the approach more familiar to developers. Ultimately I am looking to demonstrate how Postman can be used to test APIs, as well as govern the design of an API, which I hope will open the flood gates when it comes to realizing that the API lifecycle has APIs. Which is something that opens up API governance to be more than just API design governance, and something that can be applied to documentation, monitoring, security, testing, or even governance itself—-mind blown! I have had this work in a public workspace for a couple months now, but just now getting back to moving forward, so stay tuned for more on API governance in coming months.]]></content>
    <id>http://apievangelist.com/2021/03/10/using-postman-testing-for-api-governance/</id>
  </entry><entry>
    <title>Evolving API Deployment to Be More Defined and Observable Using APIs</title>
    <link href="http://apievangelist.com/2021/03/10/evolving-api-deployment-to-be-more-defined-and-observable-using-apis/"/>
    <updated>2021-03-10T00:00:00Z</updated>
    <content><![CDATA[I love it when someone realizes that APIs have APIs during one of my talks or reading of of my stories. I find that people are so heads down in their daily jobs they don’t have much time to sit back and contemplate the bigger picture, which is something that might lead them to see APIs behind their API infrastructure, but also consider how it fits into the overall API lifecycle. Since 2014 I have been on a journey to produce a machine readable artifact from every stop along the API lifecycle, and the “API deployment” is one of the stops I’ve been very keen to lock down with a set of API-driven deployment blueprints that would help define, standardize, automate, and allow us to measure the deployment of our APIs. I am looking to turn APIs.json / APIs.yaml into a machine readable blueprint for API operations, and to be able to do that I need the API deployment process to be more defined and observable—something I am going to do with APIs. API deployment is one of the toughest stops along the lifecycle to nail down because there are so many different ways in which an API an be deployed. Pick your programming language, framework, platform, gateway, service, or open source tooling, and get the job done. It is something that doesn’t always lend itself to defining and standardizing because the landscape is so define by vendor pressure and developer dogma, but I am determined to bring some order to how I define and deploy APIs using a variety of common approaches. The first API deployment blueprint I’d like to share is my collection for deploying a simple API to AWS API Gateway backed by DynamoDB—-here is a walkthrough of how the API deployment blueprint collection works. This is a very simple approach to API deployment. The blueprint won’t handle very complex schema, and there is no layer for business logic. But for a simple one dimensional schema, it provides a pretty quick and dirty way to deploy an API in 10 steps. The blueprint can use more work making it a little more robust on search, filtering, performance, and other areas, but he gives me a series of known API requests for turning an OpenAPI specification into a real world read and write API that requires an API key to access. My simple API deployment process is defined by a 10 step collection that is documented and can be run manually, scheduled via a monitor, or as part of a pipeline using Newman. Providing a set of known inputs and outputs for the API deployment stop along a known API lifecycle. Every time I run this collection it stores the state of the API deployment in a Postman environment&gt;a. This concept pushes the boundaries of what you use Postman environments for, but it works well for providing me a machine readable snapshot of the deployment stop along the API lifecycle. Providing me with timestamps for deployment, evidence of...]]></content>
    <id>http://apievangelist.com/2021/03/10/evolving-api-deployment-to-be-more-defined-and-observable-using-apis/</id>
  </entry><entry>
    <title>Turning Environmental Data Into a More Usable API</title>
    <link href="http://apievangelist.com/2021/03/08/turning-environmental-data-into-a-more-usable-api/"/>
    <updated>2021-03-08T00:00:00Z</updated>
    <content><![CDATA[The work with our open data partners over at Metadata Technology North America (MTNA) continues, moving from our last post on CMS healthcare data to working with a variety of environmental datasets. I have a couple of open data projects going on right now, and as we find ourselves with a new administration in the United States, as well as getting closer to Earth Day 2021, we figured it would be a good time to help make data that is available as downloadable files more searchable and usable via a more modern web API.  I had given the MTNA team ten links to a mix of environmental data sets last week, and this week they came back to me with the next steps for deploying each one as a more accessible API, something we cover in the video of our regular open data discussions.



I had given MTNA a mixed bag of data and APIs, but I was really interested to see how they could help with three of the larger datasets, which were proving a little bit unwieldy when it came to publishing as a simple static API using Postman — here are the three I was most interested in:

CO2 Time Series 1970-2018
Global Wildfires from 2013-2016
Citizens considering climate change as serious problem per country

Next, they’ll be publishing each of these as an API using their Rich Data Services (RDS), and I’ll get to work publishing an OpenAPI and collection in a public workspace, or set of public workspaces. I like to keep datasets separated and then just share them to the workspaces I am needing to work with them. Once I get my hands on the APIs for these we will do another session on documenting them using Postman, publishing them to a public workspace so that others can fork, but then also put together an access plan for helping MTNA manage access to the data.

I am really enjoying these sessions with MTNA. They see data in an elevated way. I find their rigorous process for interrogating data as they inject it and make available via RDS very enlightening. Even with 30 years of database experience I find that I am learning a lot just watching them work. I am learning so much that I want to make sure we record all of our sessions, and publishing stories about what we are working on. Helping others learn from what we are doing, while also being able to access the data via a series of Postman public workspaces.


]]></content>
    <id>http://apievangelist.com/2021/03/08/turning-environmental-data-into-a-more-usable-api/</id>
  </entry><entry>
    <title>Turning Centers for Medicare & Medicaid Services Providers Data Files Into an API</title>
    <link href="http://apievangelist.com/2021/03/08/turning-centers-for-medicare-medicaid-services-providers-data-files-into-an-api/"/>
    <updated>2021-03-08T00:00:00Z</updated>
    <content><![CDATA[I am working with our open data partners over at Metadata Technology North America (MTNA) to publish a mix of open data APIs across a variety of areas. First up on the list are the providers. We are working to better understand how APIs are being used in healthcare, and to help us in our journey we are taking the data files on characteristics of hospitals and other types of healthcare facilities, including the name and address of the facility and the type of Medicare services the facility providers, and looking to make available via a first class API. There are data files outlining all of the CMS service providers for just shy of the last 20 years, making for a pretty rich data set that would be very useful as a modern API, so Pascal over at MTNA got to work breaking things down, something he covers in great detail via this video from our session.



We are unsure exactly where all of this work will end up. First we are looking to better understand the state of CMS data, but also use it to help us understand how their recent CMS Interoperability and Prior Authorization Proposed Rule is playing out in the market. We aren’t healthcare experts, but MTNA are data experts, and I know my way around the API space, so we are just looking to map things out from a data and API specification perspective. Next, once Pascal and team publish a production version of the CMS service provider directory, I will take it and produce an OpenAPI and Postman collection within a public workspace, helping make the CMS service provider directory API more accessible, and something we can iterate upon via the public workspace as we do some more storytelling.

]]></content>
    <id>http://apievangelist.com/2021/03/08/turning-centers-for-medicare-medicaid-services-providers-data-files-into-an-api/</id>
  </entry><entry>
    <title>Learning About Commonly Used Standards and Classifications API with Ariā From Stats New Zealand</title>
    <link href="http://apievangelist.com/2021/03/08/learning-about-commonly-used-standards-and-classifications-api-with-aria-from-stats-new-zealand/"/>
    <updated>2021-03-08T00:00:00Z</updated>
    <content><![CDATA[I did a marathon series of open data and specification sessions with our partners over at Metadata Technology North America (MTNA) last week, which during one of them I learned about Ariā, where you can find and download concepts and definitions, classifications, concordances, and standards used for data and statistical activities across New Zealand government. MTNA has put years of work into the platform, which is something they are now also rolling out for Stats Canada. I have had several discussions with MTNA about Ariā, but this working session was my first deep dive into the platform, and the API that power it. Here is our over 60 minute session where the MTNA team provides a comprehensive look at the power behind not just Ariā, but the entire standards and classification platform.



I am still getting up to speed on everything Ariā does. Next I will spend some time playing with the API. This is where I will be able to better understand how it all works. I am keen on trying to help document it, but then also think about the overall developer experience that could be stood up around not just individual implementations of the Ariā API, but for an overall global instance of it. As I discuss in the working sessions, it is really a fundamental part of the overall open data conversation, by providing all the essential building blocks all of us API developers are needing to round off our API operations. As developers, we really spend way too much time reinventing the wheel when it comes to the most commonly used standards and classifications which should be readily available via a simple but robust API so that it can be used in all types of applications.

If you’d like to learn more about Ariā I recommend watching the video, but you can also stay tuned for more working sessions, and storytelling around Ariā. If you have more questions I recommend reaching out to the MTNA folks as they are more than happy to answer questions about what they’ve been building, and what Ariā can provide. It really is something every company’s should be using to help them manage all of the essential building blocks of our society.]]></content>
    <id>http://apievangelist.com/2021/03/08/learning-about-commonly-used-standards-and-classifications-api-with-aria-from-stats-new-zealand/</id>
  </entry>
</feed>
