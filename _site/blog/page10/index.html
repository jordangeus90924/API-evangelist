<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }
    
    .container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
    }
    .video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }    

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/14/lost-in-api-transit/"><img src="https://s3.amazonaws.com/kinlane-productions2/subway/mta-subway-map.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/14/lost-in-api-transit/">Lost In API Transit</a></h3>
			<p><em>14 Sep 2017</em></p>
			<p>I got on the New York Subway today heading for Penn Station to catch a train (New Jersey Transit) out to Princeton for a hackathon. As I was navigating my way through Metropolitan Transit Authority (MTA) and the New Jersey Transit I was thinking about my usage of API transit instead of API lifecycle. The number one response I had to this concept from readers was in regards the cognitive load experienced when you first look at a subway map that represents API infrastructure, and would anyone even know what I was talking about. It’s true, when you first look at any of the API subway maps I’ve created so far, you scratch your head to figure out what they mean. I haven’t spent a lot of time making them coherent, but I am also just getting going with the work. Truthfully, they’ll get more complicated, over getting simpler. However, each time I first use the subway in NYC there is also a pretty significant cognitive load. I’ve ridden the subway many times, but each time I still have to study the map, learn the portion I need to get what I need done, and accept that much of it I won’t actually ever understand. I usually only learn what applies to me, and the more time I spend riding a transit system, the more time it comes into focus–something that applies to any transit system in the world I’ve used. Think about when you start a new job, or adopt an existing legacy project as an API product manager. You do not immediately understand all the moving parts, absorb any diagrams, or documentation the first time you look at them. It takes time experiencing a system, before you will get acquainted, and become a local, like someone riding the MTA or NJT transit systems. Now that I live in NYC I’m going to spend time learning the transit system so I can get around,...[<a href="/2017/09/14/lost-in-api-transit/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/13/webhook-delivery-headers-from-github-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-circle-icon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/13/webhook-delivery-headers-from-github-api/">Webhook Delivery Headers From Github API</a></h3>
			<p><em>13 Sep 2017</em></p>
			<p>I am continuing my learning about Webhooks, and Github keeps my notebook full with interesting building blocks we can use when crafting our own webhook strategies. I’m not using everything I’m learning from Github in my current strategy, but I like adding each of these building blocks to my webhook research, so that I can use in future guides that I publish. Today’s post overlaps two areas of my research into webhooks, and how headers are being used by a variety of API providers. Github is using HTTP headers as part of the webhook response, providing the recipients of webhooks with more information about what is happening with each outgoing request. They are providing three custom headers along with each payload: X-GitHub-Event - Name of the event that triggered this delivery. X-Hub-Signature - HMAC hex digest of the payload, using the hook’s secret as the key (if configured). X-GitHub-Delivery - Unique ID for this delivery. In addition to these three custom headers, the User-Agent for the requests will have the prefix GitHub-Hookshot–so that your systems can identify these incoming requests more specifically. I like getting the name of the event, and definitely like the example of using the signature to make sure the payload hasn’t been tampered with, or from an untrustworthy source. Additionally you get a unique identifier for the delivery, allowing you to be able to record, and pull up unique webhook receipts. I’m adding these all as building blocks to my webhook research. I still have a notebook full of other Github, Stripe, Twilio, and leading approaches to webhooks. Once I get through this round I’m going to apply what I’ve learned to the project I’m working on, and then see about pushing out the first draft of my webhooks guide–something I’ve never done before. If nothing else, I’m learning a lot. I’m learning from all the leaders in the space, who are several versions into their webhook designs. I’m finding the...[<a href="/2017/09/13/webhook-delivery-headers-from-github-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/13/versioneye-sdk-security-notifications/"><img src="https://s3.amazonaws.com/kinlane-productions2/versioneye/versioneye-logo.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/13/versioneye-sdk-security-notifications/">VersionEye SDK Security Notifications</a></h3>
			<p><em>13 Sep 2017</em></p>
			<p>
I’ve written about VersionEye a couple of times. They help you monitor the 3rd party code you use, keeping an eye on dependencies, license violations, and security issues. I’ve written about the license portion of this equation, but they came up again while doing my API security research, and I wanted to make sure I revisited what they were up to in this aspect of the API lifecycle, floating them up on my radar.

VersionEye is keeping an eye on multiple security databases and helps you monitor the SDKs you are using in your application. Inversely, if you are an API provider generating SDKs for your API consumers to put to use, it seems like you should be proactively leverage VersionEye to help you be the eye on the security aspects of your SDK management. They even help developers within their existing CI/CD workflows, which is something that you should be considering as you plan, craft, and support your APIs. Making it as easy for you to leverage your APIs SDKs in your own workflow, and doing the same for your consumers, while also paying attention to security at each step, breaking your CI/CD process when security is breached.

I also wrote about how VersionEye has open sourced their APIs a while back, highlighting how you can also deploy into any environment you desire. I’m fascinated by the model VersionEye provides for the API space. They are offering valuable services that help us manage our crazy worlds, with a viable commercial and open source offering, that integrates with your existing CI/CD workflow. Next, I’m going to study the dependency portion of what VersionEye offer, then take some time to better understand their business model and pricing. VersionEye is pretty close to what I like to see in a service provider. They don’t have all the shine of a brand new startup, but they have all the important elements that really matter.

[<a href="/2017/09/13/versioneye-sdk-security-notifications/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/13/kubernetes-json-schema-extracted-from-openapi/"><img src="https://s3.amazonaws.com/kinlane-productions2/kubernetes/kubernetes-json-schema.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/13/kubernetes-json-schema-extracted-from-openapi/">Kubernetes JSON Schema Extracted From OpenAPI</a></h3>
			<p><em>13 Sep 2017</em></p>
			<p>I’ve been doing my regular trolling of Github lately, looking for anything interesting. I came across a repository this week that contained JSON Schema for Kubernetes. Something that is interesting by itself, but I also thought the fact that they had autogenerated the individual JSON Schema files from the Kubernetes OpenAPI was worth a story. It demonstrates for me, the growing importance of schema in all of this, and shows that having them readily available on Github is becoming more important for API providers and consumers. Creating schema is an important aspect of crafting an OpenAPI, but I find that many API providers, or the consumers who are creating OpenAPIs and publishing them to Github are not always investing the time into making sure the definitions, or schema portion of them are complete. Another aspect, as Gareth Rushgrove, the author of the Github repo where I found these Kubernetes schema points out, is the JSON Schema in OpenAPI often leaves much to be desired. Until version 3.0 it hasn’t supported everything you need, and many of the ways you are going to use these schema aren’t going to be able to use them in an OpenAPI, and you will need them as individual schema files like Gareth has done. I just published the latest version of the OpenAPI for my Human Services Data API (HSDA) work, and one of the things I’ve done is extracted the JSON Schema into separate files so I can use them in schema validation, and other services and tooling I will be using throughout the API lifecycle. I’ve setup an API that automatically extracts and generates them from the OpenAPI, but I’m also creating a Github repo that does this automatically for any OpenAPI I publish into the data folder for the Github repository. This way all I have to do is publish an OpenAPI, and there is automatically a page that tells me how complete or incomplete my schema...[<a href="/2017/09/13/kubernetes-json-schema-extracted-from-openapi/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/13/a-sample-openapi-3-0-file-to-get-started/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-logo.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/13/a-sample-openapi-3-0-file-to-get-started/">A Sample OpenAPI 3.0 File To Get Started</a></h3>
			<p><em>13 Sep 2017</em></p>
			<p>I am investing more time into my Schema.org work, alongside my learning about OpenAPI 3.0. I’m pretty excited about the components object, and I want to push forward some of my Schema.org dictionary ideas, to help folks get better at reusing common schema throughout their work. Schema.org is the most robust vocabulary out there, and we shouldn’t be reinventing the wheel in this area. I know the most important reason that folks aren’t using is that they either don’t know about it, or they are just lazy. I figure if I create some ready to go schema in an OpenAPI 3.0 components object, maybe people will be more inclined to put common schema to use. To share my components I need basic OpenAPI 3.0 shell to hold all my reusable schema. I really don’t care about the paths, and other elements being their. So I headed over to the OpenAPI 3.0 Github repo and borrowed the sample Petstore OpenAPI 3.0 my friend Darrel Miller created: I will change all the information in this sample to reflect my work, but I figured before I did I would share this example document with my readers. At first glance it doesn’t look much different than version 2.0 of OpenAPI, but once you start studying you see the differences. You see the responses have JSON specific content types inserted in between their schema references. There is also a components object, with a couple of schema present–this is all I need. There are a bunch of other things you can store in your components object, but I think this provides a nice first look at what is going on. If you are looking for some other working examples of OpenAPI 3.0 in action, head over to Mike Ralphson’s repository, he has some additional ones you can play with. I don’t know about you, but I learn from others. I need to reverse engineer API definitions from other people before I...[<a href="/2017/09/13/a-sample-openapi-3-0-file-to-get-started/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/12/the-us-postal-service-wakes-up-to-the-api-management-opportunity-in-new-audit/"><img src="https://s3.amazonaws.com/kinlane-productions2/usps/office-of-inspector-general-united-states-postal-service-api-audit-report.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/12/the-us-postal-service-wakes-up-to-the-api-management-opportunity-in-new-audit/">The US Postal Service Wakes Up To The API Management Opportunity In New Audit</a></h3>
			<p><em>12 Sep 2017</em></p>
			<p>The Office Of Inspector General for US Postal Service published an audit report on the federal agencies API strategy, which has opened their eyes to the potential of API management, and the direct value it can bring to their customers, and their business. The USPS has some extremely high value APIs that are baked into ecommerce solutions around the country, and have even launched an API management solution recently, but until now have not been actively analyzing and using API usage to guide them in any of their business planning decisions. According to the report, “The Postal Service captures customer API usage data and distributes it to stakeholders outside of the Web Tools team via spreadsheets every month. However, management is not using that data to plan for future API needs. This occurred because management did not agree on which group was responsible for reviewing and making decisions about captured usage data.” I’m sure this is common in other agencies, as APIs are often evolved within IT groups, that can have significant canyons between them and any business units. Data isn’t shared, unless a project specifically designates it to be shared, or leadership directs it, leaving real-time API management data out of reach of those business groups making decisions. It is good to see another federal agency wake up to the potential of API management, and the awareness it can bring to business groups. It’s not just some technical implementation with logfiles, it is actual business intelligence that can be used to guide the agency forward, and help an agency better serve constituents (customers). The awareness introduced by doing APIs, and then properly managing APIs, analyzing usage, and building and understanding what is happening, is a journey. It’s a journey that not all federal agencies have even begun (sadly). It is important that other agencies follow USPS lead, because it is likely you are already gathering valuable data, and just passing it on to external...[<a href="/2017/09/12/the-us-postal-service-wakes-up-to-the-api-management-opportunity-in-new-audit/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/12/openapi-3-0-tooling-discovery-on-github-and-social-media/"><img src="https://s3.amazonaws.com/kinlane-productions2/mike-ralphson/openapi_awesome1.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/12/openapi-3-0-tooling-discovery-on-github-and-social-media/">OpenAPI 3.0 Tooling Discovery On Github And Social Media</a></h3>
			<p><em>12 Sep 2017</em></p>
			<p>I’ve been setting aside time to browse through and explore tagged projects on Github each week, learning about what is new and trending out there on the Githubz. It is a great way to explore what is being built, and what is getting traction with users. You have to wade through a lot of useless stuff, but when I come across the gems it is always worth it. I’ve been providing guidance to all my customers that they should be publishing their projects to Github, as well as tagging them coherently, so that they come up as part of tagged searches via the Github website, and the API (I do a lot of discovery via the API). When I am browsing API projects on Github I usually have a couple of orgs and users I tend to peek in on, and my friend Mike Ralphson (@PermittedSoc) is always one. Except, I usually don’t have to remember to peek in on Mike’s work, because he is really good at tagging his work, and building interesting projects, so his stuff is usually coming up as I’m browsing tags. He is the first repository I’ve come across that is organizing OpenAPI 3.0 tooling, and on his project he has some great advice for project owners: “Why not make your project discoverable by using the topic openapi3 on GitHub and using the hashtag #openapi3 on social media?” « Great advice Mike!! As I said, I regularly monitor Github tags, and I also monitor a variety of hashtags on Twitter for API chatter. If you aren’t tagging your projects, and Tweeting them out with appropriate hashtags, the likelihood they are going to be found decreases pretty significantly. This is how Mike will find your OpenAPI 3.0 tooling for inclusion in his catalog, and it is how I will find your project for inclusion in stories via API Evangelist. It’s a pretty basic thing, but it is one that I know many...[<a href="/2017/09/12/openapi-3-0-tooling-discovery-on-github-and-social-media/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/12/my-favorite-part-of-openapi-3-0-is-the-components-object/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-30-components-object.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/12/my-favorite-part-of-openapi-3-0-is-the-components-object/">My Favorite Part Of OpenAPI 3.0 Is The Components Object</a></h3>
			<p><em>12 Sep 2017</em></p>
			<p>There were a number of changes made to the structure of Open API in the move to version 3.0 that I am a fan of, but if I had to point at a single seismic shift that I think will move the conversation forward it is the components object. According to the specification the components object, “holds a set of reusable objects for different aspects of the OAS. All objects defined within the components object will have no effect on the API unless they are explicitly referenced from properties outside the components object.” It is the store for for all the common and reusable aspects of defining, and designing your APIs–which will have huge benefits on how we are doing all of this. Here is the laundry list of what you can put into your OpenAPI 3.0 components object, and reference throughout your API definitions: schemas - An object to hold reusable data schema used across your definitions. responses - An object to hold reusable responses, status codes, and their references. parameters - An object to hold reusable parameters you are using throughout your API requests. examples - An object to hold reusable the examples of requests and responses used in your design. requestBodies - An object to hold reusable the bodies that will be sent with your API request. headers - An object to hold reusable headers that define the HTTP structure of your requests. securitySchemes - An object to hold reusable security definitions that protect your API resources. links - An object to hold reusable links that get applied to API requests, moving it towards hypermedia. callbacks - An object to hold reusable callbacks that can be applied. I’ve written about how many API developers see this stuff as duplicate work across our APIs, where I see them as common, resusable patterns that we should be getting organized–the OpenAPI 3.0 components object is the beginning of us getting this house in order. The...[<a href="/2017/09/12/my-favorite-part-of-openapi-3-0-is-the-components-object/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/12/machine-readable-definitions-for-all-things-api-including-your-bots/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-robot-lightning-bold.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/12/machine-readable-definitions-for-all-things-api-including-your-bots/">Machine Readable Definitions For All Things API, Including Your Bots</a></h3>
			<p><em>12 Sep 2017</em></p>
			<p>Every aspect of my business runs as either YAML or JSON. This blog post is YAML stored on Github, viewed as HTML using Jekyll. All the companies, services, tooling, building blocks, patents, and other components of my research all live as YAML on Github. Any API I design is born, and lives as an OpenAPI YAML document on Github. Sure, much of this will be imported, exported, and exported with a variety of other tools, but the YAML and JSON definition is key to every stop along the life cycle of my business, and the work that I do. It isn’t just me. I’m seeing a big shift in how many platforms, services, and tooling operate, with often times YAML, and still in many situations it has JSON, XML, and CSV at its core. Everything you do should have some sort of schema definition, providing you with a template that you can reuse, share, collaborate, and communicate around. Platforms should allow for the creation of these template schema, and enable the exporting, and importing of them, opening up interoperability, and cross-platform functionality–much like APIs do in real-time using HTTP. This is what OpenAPI has done for the API lifecycle, and there should be many complementary, or even competing formats that accomplish the same, but for specific industries, and use cases. You can see this in action over at AWS, with the ability to export your Lex bot schema for use in your Alexa skill. Sure, this is interoperability on the same platform, but it does provide one example of how YAML and JSON definitions can help use share, reuse, and develop common templates for not just APIs, but also the clients, tooling, and other platforms we are engaging with. You’ll see this expand to every aspect of tech as continuous integration and deployment takes root, and Github continues it’s expansion beyond startups, into the enterprise, government, and other institutions. Along the way there will be...[<a href="/2017/09/12/machine-readable-definitions-for-all-things-api-including-your-bots/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/11/version-1-2-draft-of-the-human-services-data-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-referral/hsda-v1-2.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/11/version-1-2-draft-of-the-human-services-data-api/">Version 1.2 Draft Of The Human Services Data API</a></h3>
			<p><em>11 Sep 2017</em></p>
			<p>I have been working on the next version of the Human Services Data API (HSDA) OpenAPI lately, taking all the comments from the Github repository, and pushing forward the specification as far as I can with the minor v1.2 release. I have the Github issues organized by v1.2, and have invested time moving forward the OpenAPI for the project, as well as my demo site for the effort. With this release I am focusing on six main areas, based upon feedback from the group, and what makes sense to move forward without any non-breaking changes: /complete - add an /everything to each core resource, allowing access to all sub resouces. query - Shifting query parameter to be array, allowing for multiple fields to be queried. content negotiation - Allow for JSON, XML, and responses. sorting - Adding sorting. pagination - Adding pagination. status codes - Add more status codes. These were main concerns regarding what was missing from the last release, and were the top items that made sense to push forward this round. I’ve made some other major shifts to the project, but before I go through those, I wanted to provide some more insight into these v1.2 changes to the core HSDA specification. Helping shed some light on why I did what I did, while I am looking to make the API interface as usable as possible for HSDA implementations, vendors servicing the space, as well as developers looking to build web, mobile, voice, and other applications on top of any APIs that support the implementation. Complete Getting access to the entire surface area of the core resources (organizations, locations, and services), as well as all the subresources (phones, physical address, mailing address, etc.) was the most voiced request from v1.1. I had laid out several options to access the entire surface are of HSDA resources, but folks seem focused on a single set of API paths to accomplish what they needed from...[<a href="/2017/09/11/version-1-2-draft-of-the-human-services-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/11/making-sure-definitions-in-openapi-are-robust-for-use-in-schema-validation/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-referral/hsda-schema.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/11/making-sure-definitions-in-openapi-are-robust-for-use-in-schema-validation/">Making Sure Definitions In OpenAPI Are Robust For Use In Schema Validation</a></h3>
			<p><em>11 Sep 2017</em></p>
			<p>I’m working on v1.2 of my Human Sevices Data API (HSDA), and with this wave of work I’m making sue there is a functional API for validating all JSON that gets posted as the body in requests, as well as when it gets returned as part of API responses. To drive my validator I’m using JSON schema, which I already have defined as part of the OpenAPI definition for the project. I want to reuse, and build on top of this work, but I found the definitions for my OpenAPI to be pretty deficient in much of the details I am needing to validate the request and response bodies of my HSDA APIs. The process has showed me the importance of making sure the definitions portion of my OpenAPIs are as robust as I can. Possessing required, default, regex patterns, and other details I’m going to need to make sure my schema validator is as robust as possible. I’m entering the phase of this project where vendors, and implementors are looking for guidance on whether or not their schema are HSDS/A compliant, and they are supporting the fields necessary to get a stamp of approval. The schema validator is essential to this, but the new validation API I’ve created is only as good as the JSON schema that I’m using as part of its engine. I come across a number of OpenAPIs in the wild which do not possess schema definitions, and references for each API. These API providers are only describing enough of the surface area of their API to be able to generate API documentation using Swagger UI. This is something I’ve also been guilty of in the past, where I would only define the surface area of the API, just to get what I needed for my API discovery needs. Over the last year, I’ve spent more time making sure the definitions portion of the OpenAPI is also present, but it isn’t...[<a href="/2017/09/11/making-sure-definitions-in-openapi-are-robust-for-use-in-schema-validation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/11/api-education-is-needed-but-rarely-prioritized-in-the-current-environment/"><img src="https://s3.amazonaws.com/kinlane-productions2/facebook/facebook-blueprint-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/11/api-education-is-needed-but-rarely-prioritized-in-the-current-environment/">API Education Is Needed But Rarely Prioritized In The Current Environment</a></h3>
			<p><em>11 Sep 2017</em></p>
			<p>I wrote about this in a mean way during my rant week, but I wanted to bring up the topic of education and training when it comes to APIs in a more constructive way this week. Amidst the regular requests I get for API architects, developers, product managers, and evangelists I am reminding many companies that they will often need to hire for these roles internally, training and grooming existing employees, as finding seasoned veterans in any of these areas will prove to be difficult. I wish I had my own API school, where I was helping train waves of qualified employees, but sadly most of the folks with existing skills are employed. The challenge of investing in API training and education doesn’t stop with your immediate team, this is something that needs to occur in most cases company-wide. I’ve talk with several groups about developing internal workshops, and training, but I find most of them aren’t truly interested in the investment needed, and are often looking for some free content, or someone they can get to come and speak for free or very low pay. It shows me that many companies aren’t quite ready to make the investment it will take to ensure their staff are ready for the work that lies ahead, and don’t value making sure their workers have the skills they’ll need to be successful in the API-driven world we are finding ourselves in. This isn’t something I’ve just encountered at SMB, SME, and the enterprise. Government agencies are always cashed strapped, under-resourced, and lacking in the skills needed for the next wave. This is also a problem I’m seeing across startups. I’ve had discussions with startup groups selling tools and services to the API space, who are hitting significant challenges once they start selling their solutions outside the mainstream tech ecosystem. Many folks at large companies, small businesses, and government agencies just don’t have some of the basics when it...[<a href="/2017/09/11/api-education-is-needed-but-rarely-prioritized-in-the-current-environment/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/08/when-i-look-at-the-landscape-of-api-services-tooling-i-see-the-future-of/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/servers-hallway-door.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/08/when-i-look-at-the-landscape-of-api-services-tooling-i-see-the-future-of/">When I Look At The Landscape Of API Services & Tooling I See The Future Of</a></h3>
			<p><em>08 Sep 2017</em></p>
			<p>There are a number of API service and tooling providers that I still get excited about in the space. 3Scale, Restlet, Runscope, and Tyk - to begin with my sponsors! ;-) ;-) ;-) However, there are others like Postman, APIMATIC, Materia, OAuth.io, Stoplight, Apicurio, API Platform, API Umbrella, Github, API Science, and others that keep me thinking good thoughts about the things that API service providers are doing. However, I also see a lot of services and tooling that are simply playing the startup game, and have more to do with investment, then they do about APIs. It is these services and tools I see as the next generation of technical debt. When you bundle the vendors who are usually chasing trends as part of their investment and exit strategy, and really don’t care about truly helping you solve your technical, and business challenges, with your existing problems, you are just multiplying your problems. These types of customers only want you as an active customer, preferably locked into a contract, with their services and tools baked into your operations. You know what all of this leads to? Technical debt. When you buy into the vendor stories, and jump on trends, without thinking through the consequences of your actions, and the long term effects on your road map, you end up with a significant amount of technical debt down the road. I have taken a number of IT and developer leadership positions in my career, where I had to come in and clean up the mess from the previous guy (always guys). Nobody was questioning the decisions being made, and allowed someone to make purchasing, and technology decisions that ended up just taking things in a bad direction. That vendor we bought into was acquired, and now that tool we depend on is part of a larger enterprise suite we really don’t need, but because we can’t unwind it from our systems, we are forced to...[<a href="/2017/09/08/when-i-look-at-the-landscape-of-api-services-tooling-i-see-the-future-of/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/08/responding-to-a-webhook/"><img src="https://s3.amazonaws.com/kinlane-productions2/stripe/stripe-using-webhooks.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/08/responding-to-a-webhook/">Responding To A Webhook</a></h3>
			<p><em>08 Sep 2017</em></p>
			<p>There are many details of doing APIs you don’t think about until you either a) gain the experience from doing APIs, or b) learn from the API providers already in the space. When you are just getting going with your API efforts you pretty much have to rely on b), unless you have the resources to hire a team with existing API experience. Which many of my readers will not have the luxury to do, so they need as much helping learning from the pioneers who came first, wherever they can. One of the API pioneers you should be learning from is the payment API provider Stripe. I’ve been studying their approach to webhooks lately, and I’ve managed to extract a number of interesting nuggets I will be sharing in separate blog posts. Today’s topic is responding to a webhook, which Stripe provides the following guidance: To acknowledge receipt of a webhook, your endpoint should return a 2xx HTTP status code. Any other information returned in the request headers or request body is ignored. All response codes outside this range, including 3xx codes, will indicate to Stripe that you did not receive the webhook. This does mean that a URL redirection or a “Not Modified” response will be treated as a failure. To be honest, I had never thought I should be responding to the webhooks I’ve setup. I treated them like a UDP request and once they went out the door and I processed, I didn’t need to response at all. How rude! I hadn’t seen any of my existing API providers offer up guidance in this area, or more likely I never noticed it. This is one of the reasons I like going though API providers documentation when I’m not integrating with them, because I tend to have a different eye for what is going on. Anyways, I’m adding webhook responses to my list of building blocks for my webhook research, and will...[<a href="/2017/09/08/responding-to-a-webhook/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/08/i-wish-i-had-time-to-tell-that-api-story/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/Millais_Boyhood_of_Raleigh.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/08/i-wish-i-had-time-to-tell-that-api-story/">I Wish I Had Time To Tell That API Story</a></h3>
			<p><em>08 Sep 2017</em></p>
			<p>If you have followed my work in the API space you know that I consider myself an API storyteller before I ever would an API evangelist, architect, or the other skills I bring to the table. Telling stories about what folks are up to in the space is the most important thing to me, and I feel it is the most common thing people stumble across, and end up associating with my brand. You hear me talk regularly about how important stories are, and how all of this API thing is only a thing, because of stories. Really, telling stories is the most important you should be doing if you are an API provider or API service provider, and something you need to be prioritizing. I was talking with a friend, and client the other day about their API operations, and after they told me a great story about the impact their APIs were making I said, “you should tell that story”! Which they responded, “I wish I had time to tell that story, but I don’t. My boss doesn’t prioritize me spending time on telling stories about what we are doing.” ;-( It just broke my heart. I get really, really busy during the week with phone calls, social media, and other project related activity. However, I always will stop what I’m doing and write 3-5 blog posts for API Evangelist about what I’m doing, and what I’m seeing. I know many of the stories are mundane and probably pretty boring, but they are exercise for me, of my ideas, my words, and how I communicate with other people. The way that enterprise groups and startups operate is something I’m very familiar with. I’ve been scolded by many bosses, and told not read or write on my blog. This is one of the reasons I don’t work in government anymore, or in the enterprise, as it would KILL ME to not be able to...[<a href="/2017/09/08/i-wish-i-had-time-to-tell-that-api-story/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/08/cloud-marketplace-becoming-the-new-wholesale-api-discovery-platform/"><img src="https://s3.amazonaws.com/kinlane-productions2/aws/aws-marketplace.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/08/cloud-marketplace-becoming-the-new-wholesale-api-discovery-platform/">Cloud Marketplace Becoming The New Wholesale API Discovery Platform</a></h3>
			<p><em>08 Sep 2017</em></p>
			<p>I’m keeping an eye on the AWS Marketplace, as well as what Azure and Google are up to, looking for growing signs of anything API. I’d have to say that, while Azure is in close second, that AWS is growing faster when it comes to the availability of APIs in their marketplace. What I find interesting about this growth is it isn’t just about the cloud, it is about wholesale APIs, and as it grows it quickly becomes about API discovery as well. The API conversation on AWS Marketplace has for a while been dominated by API service providers, and specifically the API management providers who have pioneered the space: 3Scale CA WSO2 Akana Strong Loop After management, we see some of the familiar faces from the API space doing API aggregation, database to API deployment, security, integration platform as a service (iPaaS), real time, logging, authentication, and monitoring with Runscope. Cloud Elements (Aggregation) SlashDB (Database) Runscope (Monitoring) Zapier (iPaaS) Peach API Security (Security) Streamdata (Real Time) Auth0 (Authentication) Okta (Authentication) LogEntries (Logging) All rounding off the API lifecycle, providing a growing number of tools that API provides can deploy into their existing AWS infrastructure to help manage API operations. This is how API providers should be operating, offering retail SaaS versions of their APIs, but also cloud deployable, wholesale versions of their offerings that run in any cloud, not just AWS. The portion of this aspect of API operations that is capturing my attention is the individual API providers are moving to offer their API up via AWS marketplace, moving things beyond just API service providers selling their tools to the space. Most notably are the API rockstars from the space: Stripe Twilio Sendgrid After these well known API providers there are a handful of other companies offering up wholesale editions of their APIs, so that potential customers can bake into their existing infrastructure, alongside their own APIs, or possibly other 3rd party APIs....[<a href="/2017/09/08/cloud-marketplace-becoming-the-new-wholesale-api-discovery-platform/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/07/why-i-like-a-service-mindset-over-a-resource-focus-when-it-comes-to-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/human-services/human-services-docs-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/07/why-i-like-a-service-mindset-over-a-resource-focus-when-it-comes-to-apis/">Why I Like A Service Mindset Over A Resource Focus When It Comes To APIs</a></h3>
			<p><em>07 Sep 2017</em></p>
			<p>I am currently crafting a set of services as part of my Human Sevices Data API (API) work. The core set of services for organizations, locations, and services are grouped together as a single service, as this is what I was handed, but all the additional APIs I introduce will be bundled as separate set of individual services. Over the last couple of weeks I’ve introduced seven new services, with a handful more coming in the near future. I’m enjoying this way of focusing on services, over the legacy way that is very resource focused, as I feel like it lets me step back and look at the big picture. When I was defining the core API for this work I was very centered on the resources I was making available (organization, locations, and services), but once I took on a service mindset I began to see a number of things I was missing. With each service I find myself thinking about the full life cycle, not just the APIs that deliver the service. I’m thinking about the easy ones like design, deployment, and management, but I’m also thinking about monitoring, testing, and security. Then I’m delivering documentation, support, communications, and thinking about my monetization strategy, and access plans. I’m not just doing this once, I am thinking about it in the context of each individual service, as well as across all of them, taking care of the business of the services I’m delivering, not just the technical. While some folks I talk to look at some of this as repeat work across my projects, I just see them as common patterns, that I should be reusing, refining, and delivering in consistent ways. I’m thinking about delivering the technology in a consistent way, and the operational, but I’m beginning to think about education, training, and how I can help folks on the provider and consumer side of things learn how things are working. I’m not...[<a href="/2017/09/07/why-i-like-a-service-mindset-over-a-resource-focus-when-it-comes-to-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/07/automatically-generating-openapi-from-a-yaml-dataset-using-jekyll/"><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-dynamic.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/07/automatically-generating-openapi-from-a-yaml-dataset-using-jekyll/">Automatically Generating OpenAPI From A YAML Dataset Using Jekyll</a></h3>
			<p><em>07 Sep 2017</em></p>
			<p>I was brainstorming with Shelby Switzer (@switzerly) yesterday around potential projects for upcoming events we are attending, looking for interesting ideas we can push forward, and one of the ideas we settled in on, was automatically generating OpenAPIs from any open data set. We aren’t just looking for some code to do this, we are looking for a forkable, reusable way of doing this that anyone could potentially put to work making open data more accessible. It’s an interesting idea that I think could have legs, and compliment some of the existing projects I’m tackling, and would help folks make their open data more usable. To develop a proof of concept I took one of my existing projects for publishing an API integration page within the developer portal of API providers, and replaced the hand crafted OpenAPI with a dynamic one. The project is driven from a single YAML data file, which I manage and publish using Google Sheets, and already had a static API and OpenAPI documentation, making it a perfect proof of concept. As I said, the OpenAPI is currently static YAML, so I got to work making it dynamically driven from the YAML data store. The integrations.yaml data store has eight fields, which I hd published as four separate API paths, depending on which category each entry is in. I was able to assemble the OpenAPI using a handful of variables already in the config.yaml for the project, but the rest I was able to generate by mounting the integrations.yaml, dynamically identifying the fields and the field types, and then generating the API paths, and schema definitions needed in the OpenAPI. It’s totally hacky at the moment, and just a proof of concept, but it works. I’m using the dynamically generated OpenAPI to drive the Swagger UI documentation on the project. I’m not sure why I hadn’t thought of this before, but this is why I spend time hanging with smart folks...[<a href="/2017/09/07/automatically-generating-openapi-from-a-yaml-dataset-using-jekyll/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/07/an-openapi-contract-for-the-freedom-of-information/"><img src="https://s3.amazonaws.com/kinlane-productions2/foia/freedom-of-information-stamp.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/07/an-openapi-contract-for-the-freedom-of-information/">An OpenAPI Contract For The Freedom Of Information</a></h3>
			<p><em>07 Sep 2017</em></p>
			<p>Today’s stories are all based around my preparation for providing some feedback on the next edition of the FOIA.gov API. I have a call with the project team, and want to provide ongoing feedback, so I am loading the project up in my brain, and doing some writing on the topic. The first thing that I do when getting to know any API project, now matter where it is at in it’s lifecycle, is craft an OpenAPI, which will act as a central contract for discussions. Plus, there is no better way, short of integration, to get to know an API than crafting a complete (enough) OpenAPI definition. After looking through the FOIA recommendations for the project, I took the draft FOIA API specification and crafted this OpenAPI definition: The specification is just for a single path, that allows you to POST a FOIA request. I made sure I thought through the supporting schema that gets posted, flushing out using the definitions (JSON schema) portion of the OpenAPI. This helps me see all the moving parts, and connect the dots between the API request and response, complete with definition for three HTTP status codes (200,404,500)–just the basics. Now I can see the technical details of a FOIA request in my head, preparing me for my discussion with the project owners. After loading the technical details in my head, I always like to step back and think about the business, political, and ultimately human aspects of this. This is a Freedom of Information Act (FOIA) API, being used by U.S. citizens to request that information within the federal government be freed. That is pretty significant, and represents why I do API Evangelist. I enjoy helping ensure APIs like this exist, are usable, and become a reality. It is interesting to think of the importance of this OpenAPI contract, and the potential it will have to make information in government more accessible. Providing a potential blueprint that...[<a href="/2017/09/07/an-openapi-contract-for-the-freedom-of-information/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/07/all-federal-government-public-api-projects-should-begin-with-a-github-repo/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/state-2017/68747470733a2f2f662e636c6f75642e6769746875622e636f6d2f6173736574732f3238323735392f313333353931312f32386233656336362d333563302d313165332d386565362d3636323732623966343138362e706e67.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/07/all-federal-government-public-api-projects-should-begin-with-a-github-repo/">All Federal Government Public API Projects Should Begin With A Github Repo</a></h3>
			<p><em>07 Sep 2017</em></p>
			<p>I’m gearing up for a conversation about the next edition of the FOIA API, and in preparation I’ve created an OpenAPI definition to help guide the conversation, which I drafted based upon the specifications published to Github by the FOIA API team at 18F. This was after spending some time reading through the FOIA recommendations for the project, which is also published to Github. Having the project information available on Github, makes it easy for analysts like me to quickly get up to speed on what is going on, and provide valuable feedback to the team. In my opinion, EVERY government API should start with a Github repo flushing out the needs and requirements for the project, exactly like 18F is doing as part of their FOIA work. All the details of the project are there for not just the project team, but for external participants like myself. When it comes to engaging with folks like me, the API project team doesn’t have to do anything, except send me a link to the Github repository, and maybe point out some specifics, but if the README is complete, only the repo link is necessary. This opens up conversation around the project using Github Issues, which leaves a history of the discussions that are occurring throughout the project’s life cycle. Any newcomers can invest the time into digesting the documentation, discussion, and then begin to constructively add value to what is already happening. I know this type of transparent, observable project performance is hard for many folks in government. Hell, it is hard for 18F, and people like myself who do it regularly, by default. It takes a certain fortitude to do things out in the open like this, but this is precisely why you should be doing it. The process injects sunlight into ALL government projects by default. You know your work will be scrutinized from day one, all the way to delivery, so you tend...[<a href="/2017/09/07/all-federal-government-public-api-projects-should-begin-with-a-github-repo/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/06/when-to-build-or-depend-on-an-api-service-provider/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-heart-monitor.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/06/when-to-build-or-depend-on-an-api-service-provider/">When To Build Or Depend On An API Service Provider</a></h3>
			<p><em>06 Sep 2017</em></p>
			<p>I am at that all too familiar place with a project where I am having to decide whether I want to build what I need, or depend on an API service provider. As an engineer it is always easy to think you can just build what you need, but the more experience you have, you begin to realize this isn’t always the smartest move. I’m at that point with API monitoring. I have a growing number of endpoints that I need to make sure are alive and active, but I also see an endless road map of detailed requests when it comes to granularity of what “alive and active” actually means. At first I was just going to use my default cron job service to hit the base url and API paths defined in my OpenAPI for each project, checking for the expected HTTP status code. Then I thought I better start checking for a valid schema. Then I thought I better start checking for valid data. My API project is an open source solution, and I thought about each of my clients and implementations as me for testing and monitoring for their needs. Then I thought, no way!! I’m just going to use Runscope, and build in documentation and processes that each of my clients and implementations can also use Runscope to dial in monitoring and testing of their API on their own terms. Since all of my API projects is OpenAPI driven, and Runscope is an OpenAPI driven API service provider (as ALL should be), I can use this as the seed for setting up testing and monitoring. Not all of my API implementations will be using 100% of the microservices I’m defining, or 100% of the API paths available fo each of the microservices I’m defining. Each microservice has it’s core set of paths that deliver the service, but then I’m also bundling in database, server, DNS, logging and other microservice operational...[<a href="/2017/09/06/when-to-build-or-depend-on-an-api-service-provider/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/06/keeping-things-one-dimensional-to-go-from-api-to-spreadsheet-in-one-step/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-api-to-spreadsheet.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/06/keeping-things-one-dimensional-to-go-from-api-to-spreadsheet-in-one-step/">"Keeping Things One Dimensional To Go From API To Spreadsheet In One Step"</a></h3>
			<p><em>06 Sep 2017</em></p>
			<p>I have been working on the next version of my human services work, which provides a way for cities to make information about organizations, locations, and services available on the web. Part of the feedback from the community around what was missing from the last version, was the number of API calls you needed to make to get a complete representation of a resource, and its sub-resources, as each API response was one dimensional. An example would be that you could get a list of locations, but to get at the list of services you had to make a separate API call. This wasn’t a lapse in API design, it was a result of the schema being born out of a CSV format, and me working to stay true to the original design, and usage of the schema. In the latest version, I did release a handful of paths that provide a complete representation of each resource and it’s sub-resources. However, I have maintained the original one dimension representation of each resource and sub-resources, allowing me to offer an XML, JSON, as well as CSV representation for each API call. This allows API consumers to pull CSV lists of organizations, locations, services, and their sub-resources like address and phone lists. While not something that would be useful in all API implementations, I feel like the audience for municipal level human services data will benefit significantly being able to go from API to spreadsheet in a single step. All the GET paths for organizations, locations, and services are publicly available by default, not requiring authentication, making CSV data available via a single URL–something anyone can make happen. While weighing API design decisions as part of my Human Services Data API (HSDA) work I am having to consider not just the technical of how I should be doing this. I am also deeply considering how the API will be put to use, and who will be doing...[<a href="/2017/09/06/keeping-things-one-dimensional-to-go-from-api-to-spreadsheet-in-one-step/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/06/github-oauth-applications-as-a-blueprint/"><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-oauth-application.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/06/github-oauth-applications-as-a-blueprint/">Github OAuth Applications As A Blueprint</a></h3>
			<p><em>06 Sep 2017</em></p>
			<p>I was creating a very light-weight API management solution for one of my projects the other day, and I wanted to give my API consumers a quick and dirty way to begin making calls against the API. Most of the API paths are publicly available, but there were a handful of POST, PUT, and DELETE paths I didn’t want to just have open to the public. I didn’t feel like this situation warranted a full blown API management solution like Tyk or 3Scale, but if I could just let people authenticate with their existing Github account, it would suffice. This project has it’s own Github organization, with each of the APIs living as open source API repositories, so I just leveraged Github, and the ability to create Github OAuth applications to do what I needed. You can find OAuth applications under your Github organizational settings, and when you are creating it, all you really need is to give the application a name, description, and a home page and callback URL, then you are given a client id and secret you can use to authenticate individual users with their Github accounts. I didn’t even have to do the complete OAuth dance to get access to resources, or refresh tokens (may will soon), I was just able to implement a single page PHP script to accomplish what I needed for this version: I am wiring this script up to a Github login icon on my developer portal, and each API consumer will be routed to Github to authenticate, and then the page will handle the callback where I capture the valid Github OAuth token, and the login, name, email, and other basic Github information about the user. Right now the API is open to anyone who authenticates, but eventually I will be evaluating the maturity of the Github account, and limiting access based upon a variety of criteria (number of repos, account creation date, etc.). For now,...[<a href="/2017/09/06/github-oauth-applications-as-a-blueprint/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/06/azure-matching-aws-when-it-comes-to-serverless-storytelling/"><img src="https://s3.amazonaws.com/kinlane-productions2/azure/functions/azure-functions-thumbnail.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/06/azure-matching-aws-when-it-comes-to-serverless-storytelling/">Azure Matching AWS When It Comes To Serverless Storytelling</a></h3>
			<p><em>06 Sep 2017</em></p>
			<p>I consume a huge amount of blog and Twitter feeds each week. I evaluate the stories published by major tech blogs, cloud providers, and individual API providers. In my work there is a significant amount of duplicity in stories, mostly because of press release regurgitation, but one area I watch closely is the volume of stories coming out of major cloud computing providers around specific topics that are relevant to APIs. One of these topics I’m watching closely is the new area of serverless, and what type of stories each providers are putting out there. Amazon has long held the front runner position because AWS Lambda was the first major cloud provider to do serverless, coining the term, and dominating the conversation with their brand of API evangelism. However, in the last couple months I have to say that Microsoft is matching AWS when it comes to the storytelling coming out of Azure in the area of serverless and function as a service (FaaS). Amazon definitely has an organic lead in the conversation, but when it comes to the shear volume, and regular drumbeat of serverless stories Microsoft is keeping pace. After watching several months of sustained storytelling, it looks like they could even pass up Amazon in the near future. When you are down in the weeds you tend to not see how narratives spread across the space, and the power of this type of storytelling, but from my vantage point, it is how all the stories we tell at the ground level get seeded, and become reality. It isn’t something you can do overnight, and very few organizations have the resources, and staying power to make this type of storytelling a sustainable thing. I know that many startups and enterprise groups simply see this as content creation and syndication, but that is the quickest way to make your operations unsustainable. Nobody enjoys operating a content farm, and if nobody cares about the content...[<a href="/2017/09/06/azure-matching-aws-when-it-comes-to-serverless-storytelling/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/05/just-waiting-the-graphql-assault-out/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/solidies-planning-attack-blue-matrix.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/05/just-waiting-the-graphql-assault-out/">Just Waiting The GraphQL Assault Out</a></h3>
			<p><em>05 Sep 2017</em></p>
			<p>I was reading a story on GraphQL this weekend which I won’t be linking to or citing because that is what they want, and they do not deserve the attention, that was just (yet) another hating on REST post. As I’ve mentioned before, the GraphQL’s primary strength seems to be they have endless waves of bros who love to write blog posts hating on REST, and web APIs. This particular post shows it’s absurdity by stating that HTTP is just a bad idea, wait…uh what? Yeah, you know that thing we use for the entire web, apparently it’s just not a good idea when it comes to exchanging data. Ok, buddy. When it comes to GraphQL, I’m still watching, learning, and will continue evaluating it as a tool in my API toolbox, but when it comes to the argument of GraphQL vs. Web APIs I will just be waiting out the current assault as I did with all the other haters. The link data haters ran out of steam. The hypermedia haters ran out of steam. The GraphQL haters will also run out steam. All of these technologies are viable tools in our API toolbox, but NONE of them are THE solution. These assaults on “what came before” is just a very tired tactic in the toolbox of startups–you hire young men, give them some cash (which doesn’t last for long), get them all wound up, and let them loose talking trash on the space, selling your warez. GraphQL has many uses. It is not a replacement for web APIs. It is just one tool in our toolbox. If you are following the advice of any of these web API haters you will wake up in a couple of years with a significant amount of technical debt, and probably also be very busy chasing the next wave of technology be pushed by vendors. My advice is that all API providers learn about the web, gain...[<a href="/2017/09/05/just-waiting-the-graphql-assault-out/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/05/api-evangelist-is-a-performance/"><img src="http://s3.amazonaws.com/kinlane-productions2/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/05/api-evangelist-is-a-performance/">API Evangelist Is A Performance</a></h3>
			<p><em>05 Sep 2017</em></p>
			<p>I think I freaked a couple of folks out last week, so I wanted to take a moment and remind folks that API Evangelist is a performance. Sure, it is rooted in my personality, and I keep it as true to my view of the world of APIs as I can, but it is just a performance I do daily. When I sit down at the keyboard and research the world of APIs I am truly (mostly) interested in the technology, but when I craft the words you read here on the blog I am performing a dance that is meant to be interesting to the technology community in a way that draws them in, but then also gives them a swift kick in the pants when it comes to ethics of the technology, business, and politics of doing all of this. Sure, my personality shines through all of this, and I’m being genuine when I talk about my own battles with mental illness, and other things, but please remember API Evangelist is a performance. It is a performance that is meant to counteract the regular stream of fake news that comes out of the Silicon Valley funded technology machine. API Evangelist is a Contrabulist production, pushing back on the often exploitative nature of APIs. Not that APIs are exploitative, it is the people who are doing APIs are exploitative. Back in 2010, I saw that APIs were providing a peek behind the increasingly black box nature of web technology that was invading our lives through our mobile devices, and jumped at the opportunity to jam my foot in the door, even as the VC power brokers continue to look to look for ways to close this door. In 2011, I found my voice as the API Evangelist explaining APIs to the normals, making these often abstract things more accessible. Along the way, I also developed the tone of this voice pushing back on the...[<a href="/2017/09/05/api-evangelist-is-a-performance/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/05/acknowledging-the-good-in-the-api-space/"><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/kin-lane-api-evangelist-cartoon.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/05/acknowledging-the-good-in-the-api-space/">Acknowledging The Good In The API Space</a></h3>
			<p><em>05 Sep 2017</em></p>
			<p>With such a dark week of blog posts last week I wanted to make sure and start this week off with a brighter post, talking about the good I see in the API space. It can be easy to find than some of the darker things I talked about, but after seven years doing this I see enough good things going on in the API community, that I keep doing this performance I call API Evangelist. It can be easy to rant and rave about the bad, but I find it takes a lot of work to identify the good things going on in the cracks, as they rarely get the attention of the mainstream tech community propaganda engine. First, there are some really smart folks who truly care about human beings and are dedicated to the world of APIs. I do not know of any other layer of technology that sustains a community of people that is not just about startups and mindless moving forward of technology in every industry. I can use all of my fingers counting the folks who truly care about doing APIs, and making a meaningful impact with them. I have had the pleasure of working with these folks, and brining many of them together as part of my APIStrat conference, and regularly enjoy learning from them, reading their stories, and engaging with them on a regular basis as part of this API journey. Second, not all APIs are startup focused. I work with many API providers who are doing very interesting, non-startup, non-VC investment, and most importantly, non-exploitative API things. I regularly work with passionate folks doing APIs at all levels of government, making an impact on the environment, pushing for transparency in our legal system, helping provide human services, and truly making change in a meaningful way using web APIs. APIs are neither good, nor bad, or are they neutral, they are simply a reflection of their creators...[<a href="/2017/09/05/acknowledging-the-good-in-the-api-space/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/05/a-new-minimum-viable-documentationmvd-jekyll-template-for-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/launchany/minimum-viable-documentation-template+for-apis.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/05/a-new-minimum-viable-documentationmvd-jekyll-template-for-apis/">A New Minimum Viable Documentation(MVD) Jekyll Template For APIs</a></h3>
			<p><em>05 Sep 2017</em></p>
			<p>I am a big fan of Jekyll, the static content management system (CMS). All of API Evangelist runs as hundreds of little Jekyll driven Github repositories, in a sort of microservices concert, allowing me to orchestrate my research, data, and the stories I tell across all of my projects. I recommend that API providers launch their API portals using Jekyll, whether you choose to run on Github, or anywhere else using the light-weight portable solution. I have several Jekyll templates I use to to fork and turn into new API portals, providing me with a robust toolbox for making APIs more usable. My friend and collaborator James Higginbotham(@launchany) has launched a new minimum viable documentation (MVD) template for APIs, providing API provides with everything they need out of the gate when it comes to a presence for their API. The MVD solution provides you with a place for your getting started, workflows, code samples, reference material, with OpenAPI as the heartbeat–providing you with everything you need when it comes to API documentation. It all is an open source package available on Github, allowing any API provider to fork and quickly change the content and look and feel to match your needs. Which in my opinion, is the way ALL API documentation solutions should be. None of us should be re-inventing the wheel when it comes to our API portals, there are too many good examples out their to follow. I know that Jekyll is intimidating for many folks. I’m currently dealing with this on several fronts, but trust me when I say that Jekyll will become one of the most important tools in your API toolbox. It takes a bit to learn the structure of Jekyll, and get over some of the quirks of learning to program using Liquid, but once you do, it will open up a whole new world for you. It is much more than just a static content management system (CMS)....[<a href="/2017/09/05/a-new-minimum-viable-documentationmvd-jekyll-template-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/01/you-think-you-are-so-smart-you-did-not-conduct-any-due-diligence-before-launching/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/supreme-court-judgement.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/01/you-think-you-are-so-smart-you-did-not-conduct-any-due-diligence-before-launching/">You Think You Are So Smart You Did Not Conduct Any Due Diligence Before Launching</a></h3>
			<p><em>01 Sep 2017</em></p>
			<p>You know your API stuff. You know it so well, you don’t even need to look at other APIs. There is no reason to Google and look for other APIs because your stuff is that good. Your idea came to you in a flash, and you worked for an entire weekend to bring to life. Your a genius. Everyone has told you so. This stuff just comes to you, and as long as you are left alone, the magic just happens. If people just stay out of your way, do not burden you with outside influences, and unnecessary concerns, you will keep rolling out amazing APIs that everyone will love and need. You consume books, and digest endless blog posts and white papers recommended by your trusted network of friends. You don’t ever notice authorship. They don’t matter. It is all about feeding your mind, and you will decide whether it is worthy or not. You don’t save bookmarks for citations or attributions, once inside your brain ALL ideas becomes yours. If someone’s idea is dumb, you make sure an let them know, making sure they are aware of how they are substandard and beneath you. If your friends let you know your ideas are amazing, you let them know they are great too, and will be rewarded by being in your presence, and part of your team. That one chick that was hired last year made the mistake of blurting out in a meeting, “isn’t that the same thing as that startup that launched last month?”. She isn’t on the core team anymore. You did look at what she was talking about, and their API design is inferior, and the look of their site just turned you off–no need to continue. This is why you don’t conduct due diligence for your API projects. Why spend time looking at so many bad ideas? It takes away from your time to make the magic happen. Why...[<a href="/2017/09/01/you-think-you-are-so-smart-you-did-not-conduct-any-due-diligence-before-launching/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/01/you-have-no-api-imagination-creativity-or-sensibility/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/cyber-api-description-wars/mosaic-face_internet_numbers.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/01/you-have-no-api-imagination-creativity-or-sensibility/">You Have No API Imagination, Creativity, Or Sensibility</a></h3>
			<p><em>01 Sep 2017</em></p>
			<p>I know you are used to people telling you that you are creative, and your ideas are great, but I’m here to tell you they aren’t. You lack any imagination, creativity, or sensibility when it comes to your APIs. Some of it is because you are personally lacking in these areas, part of it is because you have no diversity on your team, but it is mostly because you all are just doing this to make money. As creative as you think doing a startups is, they are really just about making money for your bosses, and investors–not a lot of imagination, creativity, or sensibility is required. You could invest the time to come up with good ideas for applications and stories on your blog, but you really don’t want to do the work, or even stand out in the group. It is much easier to just phone it in, follow the group, and let your bosses and the existing industry trends dictate what you do each day. If the business sector you operate within is doing it, you are doing it. If you see something funny online or at a conference you will do it. You have a handful of blogs you read each weekend, that you will rewrite the best posts from and publish on your own blog. Your Twitter account is just retweeting what you find, and you don’t even push out your own stories, because you have already tweeted out the story you copied in the first place. Don’t beat yourself up about this, you come by it honestly. Your privilege affords you never really getting out of your comfort zone, and the people around you make you feel good enough. Everyone on your team is the same, and your bosses really don’t care, as long as you are just creating content, and sending out all the required signals. Just make it look like you are always busy, and keep all...[<a href="/2017/09/01/you-have-no-api-imagination-creativity-or-sensibility/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/01/the-why-and-end-of-the-unhinged-decoupled-api-evangelist-rant-week/"><img src="http://i1.wp.com/restlet.dreamhosters.com/wp-content/uploads/2013/12/kinlane.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/01/the-why-and-end-of-the-unhinged-decoupled-api-evangelist-rant-week/">The Why (And End) Of The Unhinged (Decoupled) API Evangelist Rant Week</a></h3>
			<p><em>01 Sep 2017</em></p>
			<p>I know many of you are thinking Kin Lane has lost his marbles (again). In reality, I lost them last week for a couple days because someone really pissed me off, then after a couple more folks pissing in my Cheerios, I checked out last week (this happens from time to time). This week I am actually feeling quite fine after moving to NYC from LA, but the posts for the last couple of days are from my notebook entries made while in a dark place last week. Normally, these posts would never see the light of day, but I’m feeling like they probably should this week. Its no secret, I’m fairly sure I’d be classified in the bi-polar realm (never been diagnosed), something I’ve thoroughly enjoyed since I was a teen, but for the last 20 years is something I’ve had 96% control of. I get angry, fly off the handle sometimes, and have bouts of depression, and life feels like a roller coaster, but for the most part I know the signals, know when to check out, and I am actually able leverage it to my favor–crafting the person that you all know as API Evangelist. It is the fuel for my research, and how I write these words. Shocking? Run you off? Ok. I’ll accept that. I just wanted to show my readers the contrast between the night and the day, and showcase how hard I work to be really, really nice, and highlight the best of the API space on a daily regular basis. I’m hoping the honesty helps you see what is really going on, with the contrast showing you how much I work to sift through the world of APIs and find useful nuggets of information you might find valuable in your API journey. I really do enjoy what I do as the API Evangelist (most of the time), and I take pleasure in helping people understand the good...[<a href="/2017/09/01/the-why-and-end-of-the-unhinged-decoupled-api-evangelist-rant-week/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/09/01/the-fact-that-you-do-not-know-who-i-am-shows-you-live-in-a-silo/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/silo-road.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/09/01/the-fact-that-you-do-not-know-who-i-am-shows-you-live-in-a-silo/">The Fact That You Do Not Know Who I Am Shows You Live In A Silo</a></h3>
			<p><em>01 Sep 2017</em></p>
			<p>Don’t who know who I am? I am the API Evangelist. Ok, I know this post is dripping with ego. However, it is the last post in my week of API rants, and I’m just pumped from writing all of these. These types of posts are so easy to write because I don’t have to do any research, and real work, I just write, putting my mad skills at whitesplaining and mansplaining to work–tapping into my privilege. So I’m going to end the week with a bang, and fully channel the ego that has developed along with the persona that is API Evangelist. However, there is a touch of truth to this. If you are operating an API today, and you do not know who I am, I’m just going to put it out there–you live in a silo. I have published around 3,000 blog posts since 2010 on APIs. I’m publishing 3-5 posts a day, and have consistently done so for seven years. There are definitely some major gaps in that, but my SEO placement is pretty damn good. You type API or APIs, and I’m in the top 30 usually, with the occasional popping up on home page. The number thing I get from folks who message me is that they can’t search for anything API without coming across one of my posts, so they want to talk to me. So why is it that you do not know who I am? I have some ideas on that. It is because you do not read much outside your silo. When you do, you don’t give any credit to authorship. So when you have read any of my posts you didn’t associate them with a person named Kin Lane. You operate within your silo 98% of the time, and the 2% you get out, you really don’t read much, or learn from others. I on the other hand spend 98% of my time studying...[<a href="/2017/09/01/the-fact-that-you-do-not-know-who-i-am-shows-you-live-in-a-silo/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/31/your-lack-of-investment-in-api-education-will-be-the-end-of-your-api-service/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/36698086536_f214416faf_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/31/your-lack-of-investment-in-api-education-will-be-the-end-of-your-api-service/">Your Lack Of Investment In API Education Will Be The End Of Your API Service</a></h3>
			<p><em>31 Aug 2017</em></p>
			<p>Your API service is the next big thing. It is something that every API provider will be needing, and you are confident it will be something that makes the API management space look like a momentary trend. You have a SaaS, as well as an open source edition, and have invested thousands of hours into your website, documentation, and you even have an API. Everything is CI/CD ready, and you speak fluent OpenAPI. You have all the bases covered, and adoption with the first wave of users has been great, and the sales numbers are meeting all the project. There is just one thing, you haven’t invested anything into the educational resources for your customers, and the companies they work for. After a year of operation you are so confident in your team, and the services and tools you have, you feel like this is a sure thing. You’ve sold to all your startup friends, and have trended well on Product Hunt and Hacker News, and you see organic discussions about how your services are essential. Your API service provider startup is now ready for primetime, you are expand beyond the west coast, and begin targeting a handful of verticals in a couple of the biggest markets. Your research tells you that these industries are struggling with their APIs, and what you have is the solution to their pain–this is gonna be easy! Six months later, your new user numbers are still growing, but active users have declined. Nobody is using your tool after the free trial period. You’ve hired armies of sales guys to go out and push, and come back with more information about what is happening. The number one thing you hear is that people just don’t get it. Not only do they not understand why your tools and services are valuable, they really don’t speak the same language when it comes to the API providers you are used to selling to....[<a href="/2017/08/31/your-lack-of-investment-in-api-education-will-be-the-end-of-your-api-service/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/31/the-api-space-is-in-the-tractor-beam-of-the-cloud-giants-now/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/death-star.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/31/the-api-space-is-in-the-tractor-beam-of-the-cloud-giants-now/">The API Space Is In The Tractor Beam Of The Cloud Giants Now</a></h3>
			<p><em>31 Aug 2017</em></p>
			<p>A growing number of SMBs, SMEs, and other institutions, organizations, and government agencies are launching APIs, but the age APIs as the core product will thin, and those that do emerge and operate independently will be increasingly absorbed into the cloud platforms they operate on. The tractor beam of AWS, Google, and Azure are becoming to strong for us API providers to resit. We use their platforms to deploy and manage our APIs, we’ve ceded control over our operations to their clouds, it is just a matter of time before each of the APIs we depend on are assimilated into the cloud machine. Sure, we’ll still get access to valuable resources that we couldn’t launch ourselves, things like Google Maps, and resources at scale like Amazon EC2. But our platforms will be closely watching our trajectories and they will make the calculating decision whether what we are doing is valuable enough to acquire, or just suffocate by launching a competitive service. Whoops, sorry! What you are doing is a great idea, but you will have three choices, don’t do it, sell to us, or we’ll drive you out of business. The open landscape of APIs will become thousands of cloud APIs that feel like we have an unlimited amount of resources, but in reality it will just limit and control available resources, allowing the cloud bigcos to steer the space wherever they want to–controlling the creation and distribution of ideas. It was fun while it lasted. We were able to work on our own farms for a while, and use each others free range organic API resources, but pretty soon it will just be the factory model of API consumption. Sure, prices will be low when the cloud giants are doing battle, but when they aren’t we’ll be paying a premium for simple resources. When any newcomer emerges looking to disrupt any area of the API space they’ll be quickly consumed, suffocated, and silenced, removing...[<a href="/2017/08/31/the-api-space-is-in-the-tractor-beam-of-the-cloud-giants-now/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/31/apis-will-just-get-more-unreliable-as-funding-becomes-more-volatile/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/36349148770_1a9d2692b2_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/31/apis-will-just-get-more-unreliable-as-funding-becomes-more-volatile/">APIs Will Just Get More Unreliable As Funding Becomes More Volatile</a></h3>
			<p><em>31 Aug 2017</em></p>
			<p>People love to point out that APIs are unreliable. You can’t depend on them. They go away at any point, and they just aren’t something you want to be building your business on top of. When in reality APIs aren’t reliable it is the business, people, and investment behind them. The reality of the startup game is that us API consumers aren’t actually the customer, we are just numbers in a larger game where startup founders and their investors are looking for enterprise customers to purchase their startup getting the desired exit. The API is just about attracting consumers, who will do the legwork to bring in users, adding to the value of a company. As the startup funding landscape continues to dry up, shift, and evolve towards more riskier and volatile versions of investment like ICOs, things are only going to get worse. Of course, few conversation will place the blame on the people and companies behind, but APIs will continue to be the scapegoat for the instability. It works just like the robots coming for your jobs. You never hear that rich people who own companies, that are making decisions to replace workers with robots are coming for your jobs. Its the robots. Technology in many forms makes for a great blame shield, absorbing the responsibility for the volatility, instability, and scams that are going on across the landscape. In reality, nothing much changes for us API consumers. You need to get to know your API providers, well as the company and people behind them. Study their approach to operating their API. Do they communicate? Do they have proper support? Do they communicate their uptime status? What type of funding is propping them up, and the shape of their business model use. Make sure you always have a plan B if you can, and do not trust that ANY API will be around forever. If possible, come up with failover plans, and run...[<a href="/2017/08/31/apis-will-just-get-more-unreliable-as-funding-becomes-more-volatile/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/31/admit-it-you-do-not-respect-your-api-consumers-and-end-users/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/fat-riches-russian.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/31/admit-it-you-do-not-respect-your-api-consumers-and-end-users/">Admit It You Do Not Respect Your API Consumers And End Users</a></h3>
			<p><em>31 Aug 2017</em></p>
			<p>Just admit it, you could care less about your API consumers. You are just playing this whole API game because you read somewhere that this is what everyone should be doing now. You figured you can get some good press out of doing an API, get some free work from developers, and look like you are one of the cool kids for a while. You do the song and dance well, you have developed and deployed an API. It will look like the other APIs out there, but when it comes to supporting developers, or actually investing in the community, you really aren’t that interested in rolling up your sleeves and making a difference. You just don’t really care that much, as long as it looks like you are playing the API game. Honestly, you’d do any trend that comes along, but this one has so many perks you couldn’t ignore it. Not only do you get to be API cool, you did all the right things, launched on Product Hunt, and you have a presence at all the right tech events. Developers are lining up to build applications, and are willing to work for free. Most of the apps that get built are worthless, but the SDKs you provide act as a vacuum for data. You’ve managed to double your budget by selling the data you acquire to your partners, and other data brokers. You could give away your API for free, and still make a killing, but hell, you have to keep charging just so you look legit, and don’t raise any alarm bells. It is hard to respect developers who line up and work for free like this. And the users, they are so damn clueless regarding what is going on, they’ll hand over their address book and location in real-time without ever thinking twice. This is just to easy. APIs are such a great racket. You really don’t have to do...[<a href="/2017/08/31/admit-it-you-do-not-respect-your-api-consumers-and-end-users/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/30/your-apis-are-an-invasive-species/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/36701179836_a69b280280_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/30/your-apis-are-an-invasive-species/">Your APIs Are An Invasive Species</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>We tend to look at APIs as something we opt into. As an API consumer we choose to integrate with these external APIs. We made a conscious decision to put an API to work. We navigate our way to their API portals, learn about an API from its documentation, and take back what we know along with a handful of URIs, and bake the APIs into our internal systems and applications. When in reality we are just worker bees sent to find pollen, and bring the pollen back to the hive, unaware that the pollen contains an invasive species, and rarely do we ever think too deeply about why we are doing all of this–just following orders. We are told by our coworkers, the tech blogosphere, and by API providers that we need these things. Often times we are enticed with dreams of striking it rich, and that we are mining a platform for its riches, but really we are being mined by the platform, making it richer. Facebook, Twitter, Uber, Google, and other platforms want us to think APIs are democratizing, and that we are being empowered. There is always light at the end of the tunnel for the free labor we do as API consumers, and application developers. Along this journey, rarely do we realize the data we giving away, the free labor we’ve offered up without ever being asked, and how we’ve done the heavy lifting for this invasive, disruptive species we’ve invited into our lives. Once embedded, this species is surveilling, extracting, and taking what it needs, only giving back what it needs to keep its host alive. We’ve been tricked by the API mirror. We look at it and see ourselves, and listen to the stories and myths that we tell ourselves, as we read they hype in the tech blogosphere, and on the Redditz and Githubz. We seek out the machine, do the work to integrate it into our...[<a href="/2017/08/30/your-apis-are-an-invasive-species/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/30/you-like-what-i-do-as-the-api-evangelist-sure-i-will-do-more-work-for-free/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/30/you-like-what-i-do-as-the-api-evangelist-sure-i-will-do-more-work-for-free/">You Like What I Do As The API Evangelist? Sure, I Will Do More Work For Free!</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>I get regular emails from folks telling me how much they love what I do, then asking me to work for free. I’m totally happy for folks to inform me about their company, products, case studies, and other API goings on with their company. This is the bread and butter for my storytelling on API Evangelist–please keep it coming. However, the folks who ask me to work for free, what are you thinking? Where does this ethic come from in the tech sector, where folks expect you to work for their startup for free? It is something that has really gotten to ridiculous levels. Join our webinar. Write a story for our blog. Contribute to a white paper. Join our podcast. Speak at our event. Teach a workshop. Come visit our company. Talk about our products and services. Help us craft our strategy. Do some research on this subject so we can use it. Tell us what we are doing wrong. Download, install, and play with our tool, and provide us with feedback. Take this 148 question survey that will only take you 45 minutes. Jump on the phone with us so we can pick your brain. Post our infographic to your site. Share this out via your Twitter account. Vote us up on Hacker News. Commit to our open source project. Help us create an API definition. Can you review this project specification. On, and on, and on. Oh, and hey I just wanted to email you again, because you didn’t respond to the last two emails! Don’t get me wrong. If you know me, and we have history, feel free to ask me for favors, but remember, I do not have a job. I’m happy to help my friends, and love participating in podcasts with smart people, and reviewing my friends writing, because they scratch my back. They throw me bones, and have a history of given me exposure, and paying me for...[<a href="/2017/08/30/you-like-what-i-do-as-the-api-evangelist-sure-i-will-do-more-work-for-free/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/30/sorry-the-stock-options-for-your-api-startup-do-me-more-harm-than-good/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/cactus-flower.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/30/sorry-the-stock-options-for-your-api-startup-do-me-more-harm-than-good/">Sorry The Stock Options For Your API Startup Do Me More Harm Than Good</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>I’m really honored that some of my partners are kind enough to offer me a piece of the action in their companies, in exchange for what I do. I really am. However, going forward I’m going to have to decline any stock options in exchange for work, or advising, because it really doesn’t pencil out for me. I know it is the currency you are working with, getting investment in exchange for options, and trying to get knowledge, talent, and other forms of investment in exchange as well. I trust it will work out in your favor, but from my vantage point, there really is no upside in the game. I regularly receive accusations of having and agenda because of real or perceived interest in companies, so with this hit on my brand, and the lack of return from the historic stock options I’ve had historically, it just doesn’t work out. My last advisor options netted me $319, for about 60 hours of work on, and travel costs out of my own pocket. I’m sure if I had more capital investment, and a bigger piece of the action, I’d fare better, but for the stake I get thrown–it just doesn’t do it for me. Plus, I have a manila folder in the filing cabinet of more significant shares that aren’t worth the paper they are printed on, so stock options really doesn’t float my boat in the first place. Please don’t let this hurt your feelings. I know you are heavily invested in the value of your options, but I’m just not there. I’m happy to find other arrangements to make things happen, but with the way that stock options get wielded against me, and used to potentially discredit what I’m doing, it just isn’t worth anymore. As it stands today I only have stake in a single company–Skylight Digital (5%). I have NO stake in any API related company, and I am going to...[<a href="/2017/08/30/sorry-the-stock-options-for-your-api-startup-do-me-more-harm-than-good/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/30/i-realize-that-this-is-a-hit-on-your-api-budget-but-it-is-my-rent-for-this/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/36609070321_c7122be05f_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/30/i-realize-that-this-is-a-hit-on-your-api-budget-but-it-is-my-rent-for-this/">I Realize That This Is A Hit On Your API Budget, But It Is My Rent For This</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>I am the first to admit that I suck at the money game. I just don’t care. Don’t get me wrong, I’ve made a significant amount of money in my career, and command a phat six figure salary when I’ve done the job thingy, and I don’t have a problem asking for a decent rate when I’m consulting. It is just that I don’t care about climbing the money ladder, because I realized in my early 20s that it was never enough. I’ve had business partners run off with hundreds of thousands of dollars in cash, screw me out of the equity in multiple companies, and I’ve experienced what people will do to get at that next rung of the ladder. I quickly saw that you are never happier with each rung you climb, and often times you end up much unhappier the higher up you go–which is why I stay where I am at. If you have been one of my partners you know I suck at invoicing regularly. Honestly, I don’t think about it until I have to. Which regularly I forget an entire month. Unless I’m looking looking at eviction, or have something I need to purchase, I’m more obsessed with working. I love what I do (mostly), and if someone would just drop money in my account each month, I just keep beating the API Evangelist drum, and never look to make beyond what it takes to keep me going each month. Being independent is a hustle, and it takes the support of my partners (thanks 3Scale, Runscope, Restlet, and Tyk) to make this work, along with some side consulting hustle to fill in the gaps. I mostly prefer the low monthly sponsorship money because the consulting hustle is a chore, and chasing down the money can be a full time job. I’d say 1/3 of the time its about getting plugged into corporate system(s), 1/3 of the time smooth sailing,...[<a href="/2017/08/30/i-realize-that-this-is-a-hit-on-your-api-budget-but-it-is-my-rent-for-this/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/30/holding-little-guys-more-accountable-than-we-do-vcs-and-bigcos/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/36700218276_b6782330e1_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/30/holding-little-guys-more-accountable-than-we-do-vcs-and-bigcos/">Holding Little Guys More Accountable Than We Do VCs And Bigcos</a></h3>
			<p><em>30 Aug 2017</em></p>
			<p>I spend a lot of time defending my space as the API Evangelist. I’ve had lengthy battles with folks in the comments of my blog for defending women, charging for my services, being pay for play, having secret agendas, and much more. I’ve had my site taken down a handful of times (before I made static on Github Pages), because I stood up for my girlfriend, or just joined in on the wrong (right) cause. When you have been doing this as long as you have, you see the size of the armies of tech bros that are out there, waiting to pounce. It is why I don’t share links to Reddit or Hacker News anymore. I stopped sharing to DZone for same reason, but they’ve since cleaned up their community, brought in some great talent (including women), and I’ve started syndicating there again. Most recently I had someone accuse me of pay for play, even though there was no disclosure statement present, and I have had two other folks accuse me of having an agenda set forth by my partners. If you know me, you understand how ridiculous this is, but this never stops the waves of young men finding their way to my site, and pointing the finger. What I find fascinating about this is these men never point the finger at Techcrunch, or call out API startups (and bigcos) for colluding, sponsoring, pay for play, and the other bullshittery that goes on. They go after folks who are outspoken against the machine, and never after the machine itself. If people don’t like something I said, or what someone I’m writing about is up to, they tend to go after me, without spending any time getting to know me, looking at my background, or looking at the seven years of storytelling on my site–there is a single page to do it! The majority of these folks rarely ever have their own blog, name...[<a href="/2017/08/30/holding-little-guys-more-accountable-than-we-do-vcs-and-bigcos/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/29/your-microservices-effort-will-fail-just-like-your-api-and-soa-initiatives/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-zoomed-in-on-sea_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/29/your-microservices-effort-will-fail-just-like-your-api-and-soa-initiatives/">Your Microservices Effort Will Fail Just Like Your API And SOA Initiatives</a></h3>
			<p><em>29 Aug 2017</em></p>
			<p>You are full steam ahead with your microservices campaign. You’ve read Martin Fowlers blog post, and talked about the topic with your team for the last six months. After a couple pilot projects, you are diving in, and have started decoupling the monolith of systems that you depend on to operate your business each day. You have mapped out all the technical details of all code, and backend systems in play, and have targeted about 30% of existing systems for reworking using a microservices strategy. Yet, despite all your planning, your microservices effort will still fail just like your API efforts, and its predecessor the SOA initiative did. Despite all your research, planning, and eye for the technical detail in about 7 months everything will begin to slow, and by month 10 you will begin to get very, very frustrated. You see, you haven’t included any of the human element in your planning. You have thought about the business, cultural, legal, financial, and other non-technical aspects of operating your systems. You’ve done a fine job of developing a strategy for decoupling your monolith database, but you haven’t invested any time in what it will take to educate and bring up to speed all the business users, support, QA, and other folks you will need to have up to speed to actually make all this work. You are so blinded by technological trends, you forget to spend time talking to the people. You are feeling good right now because you have surrounded yourself with yes men. People who are in perfect alignment with what you are doing. They think like you, and have drank the same kool-aid. However, once you start implementing your grand strategy to make every smaller, more micro, you will begin to see friction. Sales folks will begin to see you as a threat as you fragment their landscape, and be seen as taking a piece of their action. Business users will begin...[<a href="/2017/08/29/your-microservices-effort-will-fail-just-like-your-api-and-soa-initiatives/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/29/the-reason-your-api-sucks-is-there-are-no-women-and-people-of-color-on-your/"><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/kin-lane.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/29/the-reason-your-api-sucks-is-there-are-no-women-and-people-of-color-on-your/">The Reason Your API Sucks Is There Are No Women And People Of Color On Your</a></h3>
			<p><em>29 Aug 2017</em></p>
			<p>I know that many of you are insecure about your APIs. You aren’t transparent with your numbers, and many aspects of your API operations. You are stressed out because you built it, and nobody came. You were able to artificially inflate your new user numbers, and API calls through paid campaigns, and bot activity, but nobody is using it, and you just can’t figure out why. You are asking yourself why don’t anyone see the value your API brings to the table? Why aren’t you getting the traction you thought you would get when you first came up with the idea? You aren’t getting any traction with your API because it sucks. It was a bad idea. Nobody wants it. It sucks because it doesn’t provide any value in a highly competitive space, and you naively thought that if you built it everyone would come. You probably have a number of people around you telling you that your idea is great, and the API will be a hit. You’ve probably had this most of your life, and are used to people telling you that your ideas are great. It is why you feel so uncomfortable around anyone that is critical, because you just aren’t used to being told you that your ideas are dumb. It hurts your feelings. This is why you surround yourself with people who look, act and think like you do. It is why you don’t think women and people of color have the skills needed to work on your dumb, useless ideas. You don’t have the balls to surround yourself with anyone who doesn’t think like you. If you did, you might have been told early on that your idea wasn’t worthwhile, or you might have gotten additional feedback or criticism that would have helped shape it into something useful. I know this is hard for you to hear, and you think you are really smart, and you probably read one...[<a href="/2017/08/29/the-reason-your-api-sucks-is-there-are-no-women-and-people-of-color-on-your/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/29/the-reason-for-your-api-security-breach-you-did-nothing/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/36575484422_087495fca9_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/29/the-reason-for-your-api-security-breach-you-did-nothing/">The Reason For Your API Security Breach: You Did Nothing</a></h3>
			<p><em>29 Aug 2017</em></p>
			<p>You just got three separate calls, and countless emails alerting to the fact that you just had a major security breach. You don’t know the extent of the damage yet, but it looks like they got into your primary customer database via the APIs you depend on for all your mobile applications. You are sitting in your office chair, sweating, and trying to figure out how this happened. I will tell you, it is because you have done nothing. You have de-prioritized security at every turn, resulting in an open door for any hacker to walk through. Not only have you done nothing, you actually worked against anyone who brought up the topic of API security. You would respond: We don’t have the time. We don’t have the budget. We don’t have the skills. You never listened to anyone of your staff, even that security lady (what was her name?) you had hired last year, and then resigned, with a letter containing over 25 security holes she had been trying to take care of, but because of the toxic environment you’ve created, she was unable to do anything and moved on. You have created an environment where anyone who brings up security concerns feels persecuted, and even that their job is in jeopardy, making “doing nothing” the standard mode across all operations. You have eight separate mobile applications which all use APIs, and all of them using the customer database in question, which also stores credit cards, which is in violation of your PCI compliance–you know, those forms you sign off on each year? You felt these mobile APIs were secure because they were hidden behind your mobile applications, and your developers had given you a application security scan report last year. In this situation you would love to blame these developers, but all roads lead to you when it comes to responsibility for this situation. You begin to feel sick to your stomach thinking...[<a href="/2017/08/29/the-reason-for-your-api-security-breach-you-did-nothing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/29/extract-as-much-value-as-you-can-from-your-api-community-and-give-nothing-back/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/36745180055_45289923eb_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/29/extract-as-much-value-as-you-can-from-your-api-community-and-give-nothing-back/">Extract As Much Value As You Can From Your API Community And Give Nothing Back</a></h3>
			<p><em>29 Aug 2017</em></p>
			<p>You are in a sweet spot. You got a fat six figure job in the coolest department of your company, building out your API platform. You have a decent budget (never as much as you want) to throw hackathons, run Google and Twitter ads, and you can buy schwag to give away at your events. Sure there is a lot of pressure to deliver, but you are doing pretty well. All you gotta do is convince 3rd party developers to do thing with your companies APIs, develop web, mobile, voice, and other applications that generate buzz and deliver the return on investment your bosses are looking for. It is all about you and your team. Let’s get to work growth hacking! Attract as may new users as we can, and convince them to build as much as we possibly can. Let’s get them to develop SDKs, write articles for us on their blog, speak at our events, favorite things on hacker news, and whatever activities that we can. Your objective is to extract as much value from your API operations as you possibly can, and give nothing back. Expect developers to work for free. Expect your hackathons attendees to come up with the next great idea, build it, and hand it over to you for very little in return. This isn’t a partnership, this is an API ecosystem, and your team is determined to win at all costs. Your API isn’t a two-way street. All roads lead to your success, and your bosses getting what they want. You don’t care that 3rd party developers should be compensated, or that they have any rights to their intellectual property. The 5% of them that successfully build applications, we will offer them a job in exchange for it, or we’ll just replicate it internally, decrease their rate limits, and increase their error rates so that they can’t compete. Sure you want people to still feel inspired, but not...[<a href="/2017/08/29/extract-as-much-value-as-you-can-from-your-api-community-and-give-nothing-back/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/28/your-internal-dysfunction-is-not-my-api-problem/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/35910978054_906047b6cb_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/28/your-internal-dysfunction-is-not-my-api-problem/">Your Internal Dysfunction Is Not My API Problem</a></h3>
			<p><em>28 Aug 2017</em></p>
			<p>You hear a lot of discussion regarding public API vs private API. From my vantage point there is only web APIs that use public DNS, but I find that folks hung up on the separation usually have many other hangups about things they like to keep behind the firewall, and under the umbrella of private. These are usually the same folks who like to tell me that my public API stories don’t apply to them, and when you engage these folks in any ongoing fashion you tend to find that they are looking to keep a whole lot of dysfunction out of view from the public, and all the talk really has very little to do with APIs. I spend my days studying the best practices across the leading API providers, and understanding what is working and what is not working when it comes to operating APIs. I have seven years of research I’m happy to share with folks, and I entertain a number requests to jump on calls, participate in webinars, do hangouts, and go onsite to do workshops and talks. I’m usually happy to do these things, and when it is a government agency, non-profit organization, and sometimes higher educational institutions, I am happy to these things at no charge. I like sharing what I know, and letting folks decide what they can use from the knowledge I’ve aggregated. When I engage with folks I expect folks to not always be trusted–they don’t know me. However, I’m always surprised when folks think I have an agenda, looking to change them too fast, that I’m trying to shove something down their throat, and disrupt their position. First, I am always being invited in. I’m not a sales guy. I do not have anything to sell you except for my knowledge (which revenue goes to just goes back into doing what I do). There is regularly the quiet IT person who has carefully defended their...[<a href="/2017/08/28/your-internal-dysfunction-is-not-my-api-problem/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/28/this-weeks-troubling-api-patent/"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/Daimler_Reitwagen_color_drawing_1885%20%20DE%20patent%2036423%20-%20Basic%20original%20patent%20first%20motorcycle%20in%20the%20world.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/28/this-weeks-troubling-api-patent/">This Weeks Troubling API Patent</a></h3>
			<p><em>28 Aug 2017</em></p>
			<p>I found myself looped into another API patent situation. I’m going to write this up as I would any other patent story, then I will go deeper because of my deeper personal connection to this one, but I wanted to make sure I called this patent what it is, and what ALL API patents are–a bad idea. Today’s patent is for an automatch process and system for software development kit for application programming interface: Title: Automatch process and system for software development kit for application programming interface Patent# : US 20170102925 A1 Abstract: A computer system and process is provided to generate computer programming code, such as in a Software Development Kit (SDK). The SDK generated allows an application to use a given API. An API description interface of the system is operable to receive API-description code describing one or more endpoints of the API. A template interface is operable to receive one or more templates of code defining classes and/or functions in a programming language which can be selected by the selection of a set of templates. A data store is operable to use a defined data structure to store records of API description code to provide a structured stored description of the API. A code generation module is operable to combine records of API with templates of code which are arranged in sets by the language of the code they contain. The combining of records and code from templates may use pointers to a data structure which is common to corresponding templates in different sets to allow templates of selected languages to be combined with any API description stored. Original Assignee: Syed Adeel Ali, Zeeshan Bhatti, Parthasarathi Roop, APIMatic Limited If you have been in the API space for as long as I have you know that the generation of API SDKs using an API definition is not original or new, it is something that has been going on for quite some time,...[<a href="/2017/08/28/this-weeks-troubling-api-patent/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/28/disaster-api-rate-limit-considerations/"><img src="https://s3.amazonaws.com/kinlane-productions2/google-maps/google-maps-hurricane-harvey.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/28/disaster-api-rate-limit-considerations/">Disaster API Rate Limit Considerations</a></h3>
			<p><em>28 Aug 2017</em></p>
			<p>This API operations consideration won’t apply to every API, but for APIs that provide essential resources in a time of need, I wanted to highlight an API rate limit cry for help that came across my desk this weekend. Our friend over at Pinboard alerted me to someone in Texas asking for some help in getting Google to increase the Google Maps API rate limits for an app they were depending on as Hurricane Harvey: Hey @google @googlemaps @googlemapsapi can you please remove the limit on api access for @atxfloods? This is an emergency and we rely on it.&mdash; Jen Savage (@savagejen) August 27, 2017 The app they depended on had ceased working and was showing a Google Maps API rate limit error, and they were trying to get the attention of Google to help increase usage limits. As Pinboard points out, it would be nice if Google had more direct support channels to make requests like this, but it would be also great if API providers were monitoring API usage, aware of applications serving geographic locations being impacted, and would relax API rate limiting on their own. There are many reasons API providers leverage their API management infrastructure to make rate limit exceptions and natural disasters seems like it should be top of the list. I don’t think API providers are being malicious with rate limits in this area. I just think it is yet another area where technologists are blind to the way technology is making an impact (positive or negative) on the world around us. Staying in tune to the needs of applications that help people in their time of need seems like it will have to components, 1) knowing your applications (you should be doing this anyways) and identifying the ones that have a public service, and 2) staying in tune with natural and other disasters that are happening around the world. We see larger platforms like Facebook and Twitter rolling...[<a href="/2017/08/28/disaster-api-rate-limit-considerations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/28/api-rants-vs-api-research/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/36349140070_d5ec39cb34_z.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/28/api-rants-vs-api-research/">API Rants vs. API Research</a></h3>
			<p><em>28 Aug 2017</em></p>
			<p>I know many of you read my blog for the valuable nuggets of information extracted from my regular research into the world of APIs. I spend a great deal of time sifting through very boring, mundane, and sometimes valuable API related goings on. I have managed to muster the energy each week for the last seven years to sift through thousands of feeds, Tweets, and Github repositories looking for nuggets of API wisdom, best practices, and sometimes bad practices, to share here on the blog. Some weeks I find this an easy task, something I really enjoy the process, but most weeks it is a chore–some weeks I don’t give a shit at all. This is one of those weeks. Well, last week was too, but instead of NO blog posts, this week I’m going to shift things up so that I can get on track. I have had series of folks piss in my Cheerios lately, regarding the free and unpaid work that I do, and as a result, I find myself without any writing mojo for the second week in a row, and not caring about sifting through all your API startup blah blah blah. 1/3 is about being rude and bro assholes, 1/3 of it is that APIs are boring and y’all have no imagination, and 1/3 of it is I’m a mentally ill asshole. The result of all of this is that you get a week full of API rants, instead of API research. Sooooooo, if this side of my personality turns you off, I recommend you tuning out API Evangelist for at least a week, until I feel better, and find the energy to do what it is that I do. There will still be plenty of substance in my posts, and things will still be VERY API related, and something that you can apply in your regular work, I will just be taking off all filters, and it will...[<a href="/2017/08/28/api-rants-vs-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/22/thank-you-tony/"><img src="https://s3.amazonaws.com/kinlane-productions2/tony-tam.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/22/thank-you-tony/">Thank You Tony</a></h3>
			<p><em>22 Aug 2017</em></p>
			<p>Tony Tam, the creator of the OpenAPI specification, formerly known as Swagger, has announced he will be exiting his role at OAI and SmartBear. Tony says the specification is in good hands with Ron Ratovsky (@webron), Darrel Miller (@darrel_miller), and others in the OAI. Tony doesn’t give any hints about what he’ll be up to, but will be walking away from his baby entirely. I have given Tony a hard time during the transition from Wordnik to SmartBear, and the creation of the OpenAPI, but I am a huge fan of what he has done, and super bummed to see him go–hoping he won’t leave the API community completely. There are many building blocks that go into doing APIs and OpenAPI, or Swagger, is the most significant single building block that has emerged in the seven years I’ve been doing API Evangelist. Swagger has had a profound impact on the world of APIs, and OpenAPI will continue doing this in the future, if the right conditions are still present across the API landscape. Swagger has helped us talk about our APIs. Swagger has helped us collaborate around our APIs. Swagger has opened up a whole lifecycle of API tooling to help us along our journey. I always felt like Swagger reflected Tony’s personality, and with it’s evolution to OpenAPI, and the OpenAPI Initiative means it’s grown beyond it’s creator. OpenAPI is in good hands. I think it is a good time for Tony to step away, and feel like his baby has begun to grow up, becoming much bigger than what he can do on his own (even with Ron’s amazing help). Thank you for all your work Tony. You made your mark on the API space. You managed to develop something that was useful for API documentation and code generation, but quickly became about design, testing, monitoring, and every other stops along the API lifecycle. I am stoked to have had the chance to...[<a href="/2017/08/22/thank-you-tony/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/22/looking-at-facebook-blueprint-as-i-study-api-training-programs/"><img src="https://s3.amazonaws.com/kinlane-productions2/facebook/facebook-blueprint-screenshot.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/22/looking-at-facebook-blueprint-as-i-study-api-training-programs/">Looking At Facebook Blueprint As I Study API Training Programs</a></h3>
			<p><em>22 Aug 2017</em></p>
			<p>I am preparing a training section of my API Evangelist research, and part of the process involves learning about what other API providers and API service providers are up to in this area. On my list to look through is Facebook Blueprint, their training area for the platform. The courses present there aren’t specifically for the Facebook API, and is targeting primarily business uses, but the approach translates to API focused training materials, and showcases what is a priority for Facebook when it comes to educating their platform consumers. As part of my API training research I want to understand the building blocks employed by Facebook so that I can apply as part of my API Evangelist training efforts, and help other API providers and service providers apply as part of their operations as well. Here are a few of the API training building blocks I found present: Courses - A variety of online courses that teach you about everything Facebook. Webinars - The webinars they provide around the content they are publishing. Live - The live, in person workshops and courses they provide around the world. Case Studies - Case studies of companies who have used Facebook courses. Press - Press about the Facebook Blueprint, and how they are spreading the word. Certifications - Facebook specific certifications that you can archive. Exams - The tests that are available around the facebook courses. How it Works - Some details about how Facebook training works. Policies - The legal side of things, covering all of our bases. FAQ - Some of the frequently asked questions around the training platform. Support &amp; Help - Where you can get support and more help when it comes to training. Facebook breaks down their training into course categories and learning paths, providing two main ways for potential students to find what they are looking for. Facebook Blueprint provides…well, a blueprint that other API providers and service providers can consider when...[<a href="/2017/08/22/looking-at-facebook-blueprint-as-i-study-api-training-programs/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/22/county-level-marijuana-regulation-in-california-using-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/weed/mendocino-county-split-from-state-on-cannabis+track-and-trace.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/22/county-level-marijuana-regulation-in-california-using-apis/">County Level Marijuana Regulation In California Using APIs</a></h3>
			<p><em>22 Aug 2017</em></p>
			<p>Counties across the State of California are scrambling to get everything in order now that marijuana is legal, and the 3rd party vendors working with the state are using an API to try and bridge the regulatory needs of each county, as they look to regulate the brand new industry. It sounds like the marijuana regulatory API isn’t 100% ready for prime time, but it is interesting to hear that state is looking to “mitigate the burden of counties” when it comes to production of marijuana using APIs. I have been curating news about APIs in use across the growing marijuana industry, but this is the fist story I’ve written on the subject. Now that I’m seeing APIs use as part of the regulatory engine for the industry, things are getting a little more real, and not just be about finding seeds, stores, and other industry data. I’ll keep scratching around to see what I can find out about the software vendors mentioned in the article, and see if I can get my hands on any documentation, or a link to any active portal. I’m curious to see where this marijuana regulatory API train is headed. Since the marijuana industry is a completely new one for cities, counties, and states to manage, there is an opportunity to leverage new technology like APIs as part of the interactions between government entities, with the help of 3rd party providers. Maybe there is even some opportunity for revenue generation on top of these APIs, allowing for government to fund the software development in a way that it could also be used across other government systems. Push things forward with the rollout and expansion of the marijuana industry, and use that to fund other government systems that lack the funds, and are often years behind. The problems states face in working with counties doesn’t stop with this new marijuana industry, and there are so many other aspects of government...[<a href="/2017/08/22/county-level-marijuana-regulation-in-california-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/22/considering-how-machine-learning-apis-might-violate-privacy-and-security/"><img src="https://s3.amazonaws.com/kinlane-productions2/290x195cloudsecurity2014.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/22/considering-how-machine-learning-apis-might-violate-privacy-and-security/">Considering How Machine Learning APIs Might Violate Privacy and Security</a></h3>
			<p><em>22 Aug 2017</em></p>
			<p>I was reading about how Carbon Black, an endpoint detection and response (EDR) service, was exposing customer data via a 3r party API service they were using. The endpoint detection and response provider allows customers to optionally scan system and program files using the VirusTotal service. Carbon Black did not realize that premium subscribers of the VirusTotal service get access to the submitted files, allowing an company or government agency with premium access to VirusTotal’s application programming interface (API) can mine those files for sensitive data. It provides a pretty scary glimpse at the future of privacy and security in a world of 3rd party APIs if we don’t think deeply about the solutions we bake into our applications and services. Each API we bake into our applications should always be scrutinized for privacy and security concerns, making sure end-users aren’t being subjected to unnecessary situations. This situation sounds like it was both API provider and consumer contributing to the privacy violation, and adjusting platform access levels, and communicating with API consumers would be the best path forward. Beyond just this situation, I wanted to write about this topic as a cautionary tale for the unfolding machine learning API landscape. Make sure we are thinking deeply about what data and content we are making available to platforms via artificial intelligence and machine learning APIs. Make sure we are asking the hard questions about the security and privacy of data and content we are running through machine learning APIs. Make sure we are thinking deeply about what data and content sets we are running through the machine learning APIs, and reducing any unnecessary exposure of personal data, content, and media. It is easy to be captivated by the magic of artificial intelligence and machine learning APIs. It is easy to view APIs as something external, and not much of a privacy or security threat. However, with each API call we are inviting a 3rd party API...[<a href="/2017/08/22/considering-how-machine-learning-apis-might-violate-privacy-and-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/21/the-first-question-when-starting-an-api-is-always-should-we-be-doing-this/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/supreme-court-statues.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/21/the-first-question-when-starting-an-api-is-always-should-we-be-doing-this/">The First Question When Starting An API Is Always: Should We Be Doing This?</a></h3>
			<p><em>21 Aug 2017</em></p>
			<p>I was doing some more work on my list of potential female speakers from the API space. I have some slots to fill for @APIStrat, and I saw another API event was looking for suggestions when it came to speakers. A perfect time to invest some more cycles into finding female API talent. Twitter and Github is always where I go for discovery. I picked up where I left off working on this last time, turned on my search tools that use the Twitter and Github API, and got to work enriching the algorithm that drives my API talent search. Next up on my task list was to deploy a name microservice, that would help me filter Twitter and Github users by gender. I’m interested in API folks of all type, but for this round I need to be able to weight by female. I found a list of the top names from the United States which had them broken down by gender. I copied and pasted into a Google Sheet, fired up a Github repository, and published the spreadsheet of data to Github as YAML–giving me a male.yaml, and female.yaml listing of names. I will be be use these names in a variety of web and API applications, but I wanted to be able to help filter any search results by a female name for this project. I understand the limitations of this approach, but it is good enough for what I am looking to accomplish today. Next, I use my new name microservice as a filter for any Twitter or Github account I’m paying attention to as part of my API monitoring. Quickly giving me a list of accounts to look through as I am developing my list of women doing interesting things with APIs. Once I’m done I have a list of Twitter accounts, and Github accounts, I prepare them as a Google Sheet, then get ready to publish the YAML within...[<a href="/2017/08/21/the-first-question-when-starting-an-api-is-always-should-we-be-doing-this/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/21/making-sense-of-api-activity-with-webhook-events/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/border-traffic.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/21/making-sense-of-api-activity-with-webhook-events/">Making Sense Of API Activity With Webhook Events</a></h3>
			<p><em>21 Aug 2017</em></p>
			<p>I was doing some webhooks research as part of my human services work and I found myself studying the types of events used as part of webhook orchestration for Github, Box, Stripe, and Slack. Each of the event type lists for each of these platforms tell a lot about what is possible with each API, and the webhooks that get triggered as part of these events show what is important to developers who are integrating with each of these APIs. These event type lists really help make sense of the API activity for each of these APIs, providing a nice list to follow when developing your integration strategy. What I really like as I look through each of these webhook event lists is that they are usually in pretty plain language, describing events that matter, not just row updates with a timestamp. These events can be very broad, triggering a webhook when anything happens to a resource, or it could be granular and be all about a specific type of change, or anything in between. Each event type represents something API consumers want from an API, and would be normally polling the API for, but since there are webhook events, developers can get a push of data or a ping whenever an event occurs. Another thing that the presence of webhooks, and a robust list of events represent for me is the maturity of a platform. Github, Box, Stripe, and Slack are all very mature and robust platforms. There are meaningful events defined, and the platform behaves as a two way street, accepting requests, but also making requests whenever a meaningful event occurs. I’m getting to a place where I feel like basic webhook infrastructure should be default for all API providers. The problem is I don’t think there are well enough defined models for API providers to follow when they are planning this part of their API operations. Something we will need to tackle...[<a href="/2017/08/21/making-sense-of-api-activity-with-webhook-events/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/21/api-foraging-and-wildcraft/"><img src="https://s3.amazonaws.com/drone-recovery/diamond04.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/21/api-foraging-and-wildcraft/">API Foraging And Wildcraft</a></h3>
			<p><em>21 Aug 2017</em></p>
			<p>I was in Colorado this last week at a CA internal gathering listening to my friend Erik Wilde talking about APIs. One concept he touched on was what he called API gardening, where different types of API providers approached the planting, cultivating, and maintenance of their gardens in a variety of different ways. I really like this concept, and will be working my way through the slide deck from his talk, and see what else I can learn from his work. As he was talking I was thinking about a project I had just published to Github, which leverages the Google Sheets API, and the Github API to publish data as YAML, so I could publish as a simple set of static APIs. I’d consider this approach to be more about foraging and wildcrafting, then it is about tending to my API garden. My API garden just happens spread across a variety of services, often free and public space, in many different regions. I will pay for a service when it is needed, but I’d rather tend smaller, wilder, APIs, that grow in existing spaces–in the cracks. I use free services, and build upon the APIs for the services I am already using. I publish calendars to Google Calendar, then pull data and public APIs using the Google APIs. I use the Flickr and Instagram APIs for storing, publishing, sharing, and integrating with my applications using their APIs. I pull data from the Twitter API, Reddit API, and store in Google Sheets. All of these calendar, image, messaging, and link data will ultimately get published to Github as YAML, which then is shared as XML, JSON, Atom, CSV via static APIs. I am always foraging for data using public services, then planting the seeds, and wildcrafting other APIs–in hopes something will eventually grow. Right now Github is my primary jam, but since I’m just publishing static JSON, XML, YAML, and other media types, it can...[<a href="/2017/08/21/api-foraging-and-wildcraft/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/21/api-deployment-comes-in-many-shapes-and-sizes/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-deploy.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/21/api-deployment-comes-in-many-shapes-and-sizes/">API Deployment Comes In Many Shapes And Sizes</a></h3>
			<p><em>21 Aug 2017</em></p>
			<p>Deploying an API is an interesting concept that I’ve noticed folks struggle with a little bit when I bring it up. My research into API deployment was born back in 2011 and 2012 when my readers would ask me which API management provider would help them deploy an API. How you actually deploy an API varies pretty widely from company to company. Some rely on gateways to deploy and API from an existing backend system. Some hand-craft their own API using open source API frameworks like Tyk and deploy alongside your existing web real estate. Others rely on software as a services solutions like Restlet and Dreamfactory to connect to a data or content source and deploy an API in the clouds. Many folks I talk with simply see this as developing their APIs. I prefer to break out development into define, design, deploy, and then ultimately manage. In my experience, a properly defined and designed API can be deployed into a variety of environments. The resulting OpenAPI or other definition can be used to generate server side code necessary to deploy an API, or maybe used in a gateway solution like AWS API Gateway. For me, API deployment isn’t just about the deployment of code behind a single API, it includes the question about where we are deploying the code, acknowledging that there are many places available when it comes to deploying our API resources. API deployment can be permanent, ephemeral, or maybe just a sandbox. API deployment can be in a Docker container, which by default deploys APIs for controlling the underlying compute for the API you deploying. Most importantly, API deployment should be a planned, well-honed event. APIs should be able to be redeployed, load-balanced, and taken down without friction. It can be easy to find one way of deploying APIs, maybe using similar practices surrounding web deployments, or dependent on one gateway or cloud service. Ideally, API deployment comes in many...[<a href="/2017/08/21/api-deployment-comes-in-many-shapes-and-sizes/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/18/http-status-codes-and-the-politics-of-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/runscope/runscope-200-ok.jpeg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/18/http-status-codes-and-the-politics-of-apis/">HTTP Status Codes And The Politics Of APIs</a></h3>
			<p><em>18 Aug 2017</em></p>
			<p>The more I learn about the world of APIs, the more I understand how technology, business, and politics are all woven together into one often immovable monolith. Many things in the world of APIs seem purely like a technical thing, but in reality they are wrapped in, and wielded intentionally and unintentionally as part of larger business, and sometimes a personal agenda. An example of this can be found with the presence, or lack of presence with HTTP status codes, which the default status is usually 200 OK, 404 not found, or 500 internal error. While these seem like very granular technical details of whether or not an HTML, XML, CSV, or JSON document is returned or not as part of a single web request, there usage often dictates what is happening behind the firewall, and often times more importantly, what is not happening. I find people’s awareness that HTTP status codes exist (or not) a significant sign of their view of the wider web world. If they are aware they exist they most likely have some experience engaging with other experienced partners using the web. If they don’t, they most likely live a pretty isolated existence–even if they do have a web presence. Beyond just knowing that HTTP status codes exist, understanding the importance of, and the nuance surrounding each individual one demonstrates you are used to engaging with external actors at scale, leveraging web technology. I have to put out there that I DO NOT have an intimate knowledge of all HTTP status codes, because I have not exercised them as part of large scale projects, but it is something I do grasp the scope and importance of from the projects I have worked on. This is not a boolean thing, you knowing HTTP status codes or not. This is the result of many journeys, with many partners, across many different types of digital resources. You can tell how many journeys someone has...[<a href="/2017/08/18/http-status-codes-and-the-politics-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/17/where-are-all-the-api-focused-agencies/"><img src="https://s3.amazonaws.com/kinlane-productions2/good-api/good-api-agency.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/17/where-are-all-the-api-focused-agencies/">Where Are All The API Focused Agencies?</a></h3>
			<p><em>17 Aug 2017</em></p>
			<p>Earlier this week at the CA API Academy virtual gathering I spoke at in Boulder CO, the question around why there aren’t more API focused agencies came up. We were talking about the need for consulting services around common areas of API operations like design, deployment, management, testing, as well as training around API lifecycle related topics. We are seeing some movement in the area of API focused agencies, but not enough to cover the current demand. We are seeing full service shops like APIvista, and Good API emerge. There is also movement on the agency level when it comes to integration platform as a service (iPaaS), over at Left Hook Digital, helping companies leverage Zapier, and integrate with API platforms. There is definitely significant movement in the number of API focused agencies, but we are going to need more to meet the demand for API design, deployment, management, testing, and other stops along the API lifecycle. I’m guessing it will take a couple years for this side of the API business to mature. Then I’m figuring we will begin see to see more specialized API agencies emerge, helping with evangelism, support, API design, and maybe even industry focused iPaaS, similar to what we are seeing with LeftHook. As mainstream companies are waking up to the potential of APIs, they are going to need a lot of professional help to ensure they are successful in their API journey. We are going to need a number of general service, specialized, regional, and industry specific API agencies to help get us through the next couple of years. I am focusing on trying to scale the API training portion of this need. I am ramping up development of API training courses that span my API lifecycle and API stack research. Helping API providers and consumers navigate the world of APIs. I’m talking with partners about working together to develop and distribute API training, and will be working to...[<a href="/2017/08/17/where-are-all-the-api-focused-agencies/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/17/the-patent-application-information-retrieval-bulk-data-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/uspto/uspto-pair-bulk-data-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/17/the-patent-application-information-retrieval-bulk-data-api/">The Patent Application Information Retrieval Bulk Data API</a></h3>
			<p><em>17 Aug 2017</em></p>
			<p>
I stumbled across the Patent Application Information Retrieval Bulk Data API from the US Patent Office the other day. It provides a much more usable approach to getting at patent information than what I am using at the moment. Right now I am downloading XML files and searching for the occurrence of a handful of keywords. If I want to make a change I have to fire up a new AWS instance, change the code, and reprocess the downloaded files. The Patent Application Information Retrieval Bulk Data API gives me a much more efficient interface to work with.

The Patent Application Information Retrieval Bulk Data API contains the bibliographic, published document and patent term extension data tabs in Public PAIR from 1981 to present, with some additional data dating back to 1931. It has leveraged COTS semantics, maintains an open architecture, and the query syntax follows the standard Apache Solr search syntax, with API responses following the Solr formats. Providing for a much more powerful interface for querying patent data, which goes back further back in time then what I’ve been doing currently. I’m really interested in doing an API patent search for the 1990s, or maybe even earlier.

The Patent Application Information Retrieval Bulk Data API is a well designed API, with an attractive API portal and documentation, driven with an OpenAPI. The USPTO provide access to the patent data in the way that I think all government agencies should be doing it. You can use the API, or get at a JSON or XML download of the data. The API is “part of the US Patent and Trademark Office’s (USPTO) commitment to fostering a culture of open government as described by the 2013 Executive Order to make open and machine-readable data the new default for government information.” Which is pretty cool in my book, something we desperately are needing to become a reality across all federal agencies in 2017.

[<a href="/2017/08/17/the-patent-application-information-retrieval-bulk-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/16/wildcard-webhook-events/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-asterisk.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/16/wildcard-webhook-events/">Wildcard Webhook Events</a></h3>
			<p><em>16 Aug 2017</em></p>
			<p>
I have been studying the approach of a variety of webhook implementations in preparation for an API consulting project I’m working on. Even though I’m very familiar with how webhooks works, and confident in my ability to design and develop a solution, I’m ALWAYS looking to understand what leading API providers are up to, and how I can improve my knowledge and awareness.

With his round of research, Github has provided me with several webhook nuggets for my API storytelling notebook. One of their web features I though was the notion of a wildcard webhook event:


  Wildcard Event - We also support a wildcard (*) that will match all supported events. When you add the wildcard event, we’ll replace any existing events you have configured with the wildcard event and send you payloads for all supported events. You’ll also automatically get any new events we might add in the future.


I have been working to identify a set of objects and associated webhook events, and the notion of a wildcard event is interesting. It seems like you could apply this globally, or to specific objects / resources, allowing you to get pushes for any events that occur. I’m not sure I’ll have time to apply this feature in my current project, but it is worth adding to my webhook toolbox for future projects.

There are three other features I’ve extracted from Github’s approach to webhooks that I’ve added to my API storytelling notebook, to hopefully craft into future blog posts. I’ll be adding these all as potential webhook building blocks that API providers can consider. I’m hoping to find some more time and money to invest into my webhook research this fall, and be able to publish a formal guide for the world of webhooks. I’m always surprised by the lack of formal guidance when it comes to webhooks, and is something I’d like to see change in the near future.

[<a href="/2017/08/16/wildcard-webhook-events/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/16/the-importance-of-api-stories/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/ca-panel.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/16/the-importance-of-api-stories/">The Importance Of API Stories</a></h3>
			<p><em>16 Aug 2017</em></p>
			<p>I am an API storyteller before am an API architect, designer, or evangelist. My number one job is to tell stories about the API space. I make sure there is always (almost) 3-5 stories a day published to API Evangelist about what I’m seeing as I conduct my research on the sector, and thoughts I’m having while consulting and working on API projects. I’ve been telling stories like this for seven years, which has proven to me how much stories matter in the world of technology, and the worlds that it is impacting–which is pretty much everything right now. Occasionally I get folks who like to criticize what I do, making sure I know that stories don’t matter. That nobody in the enterprise or startups care about stories. Results are what matter. Ohhhhh reeeaaaly. ;-) I hate to tell you, it is all stories. VC investment in startups is all about the story. The markets all operate on stories. Twitter. Facebook. LinkedIn. Medium. TechCrunch. It is all stories. The stories we tell ourselves. The stories we tell each other. The stories we believe. The stories we refuse to believe. It is all stories. Stories are important to everything. The mythical story about Jeff Bezos’s mandate that all employees needed to use APIs internally is still 2-3% of my monthly traffic, down from 5-8% for the last couple of years, and it was written in 2012 (five years ago). I’ve seen this story on the home page of the GSA internal portal, and framed hanging on the wall in a bank in Amsterdam. Stories are important. Stories are still important when they aren’t true, or partially true, like the Amazon mythical tale is(n’t). Stories are how we make sense of all this abstract stuff, and turn it into relatable concepts that we can use within the constructs of our own worlds. Stories are how the cloud became a thing. Stories are why microservices and Devops is...[<a href="/2017/08/16/the-importance-of-api-stories/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/16/the-85-stops-along-the-api-lifecycle-that-i-track-on/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-lifecycle.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/16/the-85-stops-along-the-api-lifecycle-that-i-track-on/">The 85 Stops Along The API Lifecycle That I Track On</a></h3>
			<p><em>16 Aug 2017</em></p>
			<p>I am preparing a talk for tomorrow, and I needed a new list of each stop along the API lifecycle, and since each of my project exist as Github repositories, and are defined as a YAML and JSON data store, I can simply define a new liquid template for generating a new HTML listing of all the stops along the API lifecycle–after generating this list I figured I’d share here as a story. Here are the 85 stops along the API lifecycle landscape from my vantage point as the API Evangelist: Definitions Design Versioning Hypermedia DNS Low Hanging Fruit Scraping Database Deployment Rogue Microservices Algorithms Search Machine Learning Proxy Virtualization Containers Management Serverless Portal Getting Started Documentation Frequently Asked Questions Support Communications Road Map Issues Change Log Monitoring Testing Performance Caching Reliability Authentication Encryption Vulnerabilities Breaches Security Terms of Service (TOS) Surveillance Privacy Cybersecurity Reclaim Transparency Observability Licensing Copyright Accessibility Branding Regulation Patents Discovery Client Command Line Interface Bots Internet of Things Industrial Network IDE SDK Plugin Browsers Embeddable Visualization Analysis Logging Aggregation iPaaS Webhooks Integrations Migration Backups Real Time Orchestration Voice Spreadsheets Investment Monetization Plans Partners Certification Acquisitions Evangelism Showcase Deprecation I’m always presenting my API lifecycle research as a listing, or in a linear fashion. I always feel like I should be creating an actual lifecycle visualization, but then I always end up feeling like I should just invest in my subway API map work, and create more robust way to represent how the API lifecycle truly looks. Anyways, these 85 areas represent the scope of my API industry research, and provide a framework for thinking about not just the individual API lifecycle, but also the bigger picture of our API operations and partnerships. Not all of these areas apply to every API provider, but they do provide one perspective of the API landscape that all API providers can learn from. If there are any other stops along the lifecycle you think should...[<a href="/2017/08/16/the-85-stops-along-the-api-lifecycle-that-i-track-on/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/16/api-kindergarten-for-business-and-it-leaders/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-alphabet-apple.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/16/api-kindergarten-for-business-and-it-leaders/">API Kindergarten For Business And IT Leaders</a></h3>
			<p><em>16 Aug 2017</em></p>
			<p>
I’m working on a number of API courses and lessons lately. Some of these are API 101 courses, while others are more advanced courses for the seasoned API provider, and consumer. As I think about what is needed when it comes to classes and workshops across the API sector, I’m considering doing an API Kindergarten series, where business and IT leaders can learn the basics of doing business with APIs.

The curriculum for the API kindergarten program include hands on lessons on how to play nicely, get along with others, the importance of sharing, and helping them learn the important soft skills like not shitting your pants. I’m always surprised at the lack of basic skills by company, organizational, institutional, and government leadership when it comes to the essentials of why APIs work, and think a little primer on things might help some realize they shouldn’t be doing APIs in the first place, or maybe prevent some major crisis down the road.

The number of folks who tell me directly that they are all in on this API thing, yet when it comes to practice clearly are not has grown significantly in 2017. I feel like we need a whole series or curriculum to help make sure business and IT leadership is up for the challenge is desperately needed. Just wait, until I begin working on my sex edit course for the middle schoolers, where we teach them about safely integrating, and what is appropriate API behavior, and what is not. Honestly, I could spend days finding equivalences between the real world and doing APIs (not real world) and creating classes around them–fun stuff!

[<a href="/2017/08/16/api-kindergarten-for-business-and-it-leaders/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/15/which-platforms-have-control-over-the-conversation-around-their-bots/"><img src="https://s3.amazonaws.com/kinlane-productions2/bots/bots-slack-search.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/15/which-platforms-have-control-over-the-conversation-around-their-bots/">Which Platforms Have Control Over The Conversation Around Their Bots</a></h3>
			<p><em>15 Aug 2017</em></p>
			<p>I spend a lot of time monitoring API platforms, thinking about different ways of identifying which ones are taking control of the conversation around how their platforms operate. One example of this out in the wild can be found when it comes to bots, by doing a quick look at which of the major bot platforms own the conversation around this automation going on via their platforms. First you take a look at Twitter, by doing a quick Google search for Twitter Bots: Then you take a look at Facebook, by doing a quick Google search for Facebook Bots: Finally take a look at Slack, by doing a quick Google search for Slack Bots: It is pretty clear who owns the conversation when it comes to bots on their platform. While Twitter and Facebook both have information and guidance about doing bots they do not own the conversation like Slack does. Something that is reflected in the search engine placement. It is also something that sets the tone of the conversation that is going on within the community, and defines the types of bots that will emerge on the platform. As I’ve said before, if you have a web or mobile property online today, you need to be owning the conversation around your API or someone eventually will own it for you. The same comes to automation around your platform, and the introduction of bots, and automated users, traffic, advertising, and other aspects of doing business online today. Honestly, I wouldn’t want to be in the business of running a platform these days. It is why I work so hard to dominate and own my own presence, just so that I can beat back what is said about me, and own the conversation on at least Google, Twitter, LinkedIn, Facebook, and Github. Seems like to me, if you are going to enable automation on your platform via APIs, it should be something that you own...[<a href="/2017/08/15/which-platforms-have-control-over-the-conversation-around-their-bots/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/15/where-to-begin-with-webhooks-for-the-human-services-data-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/webhooks/RESTHooks.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/15/where-to-begin-with-webhooks-for-the-human-services-data-api/">Where To Begin With Webhooks For The Human Services Data API</a></h3>
			<p><em>15 Aug 2017</em></p>
			<p>I am getting to work on a base set of webhook specification for my human services data API work, and I wanted to take a fresh drive through a handful of the leading APIs I’m tracking on. I’m needing to make some recommendations regarding how human services data APIs should be pushing information via APIs, as we as providing APIs. Webhooks are fascinating to me because they really are just APIs in reverse. Webhooks are just an API request, where the target URL is a variable, allowing an API call to be made from a platform, to any target URL, on an triggering events, or on a schedule as a job. Here are six of the API providers I took a look at while doing this webhook research: Box Gumroad Venmo Github Stripe Slack All of these API providers offer webhooks, allowing developers to create an API call that will be fired off when a specific event occurs. These events are usually tied to a specific object. Box is documents. Github is a repository. Stripe is a payment. With human services it will be an organization, location, or service. There are a handful of key concepts at play when it comes to webhooks, making them an important part of the equation: Object - The object in which an event is occurring. For this project it is organizations, locations, services, contacts, and potentially other elements of API operations. Events - This is a list of events that can occur against all the objects that will trigger the execution of a webhook. Target - The URL of the webhook. This is the variable of the outgoing API call, allowing them to be defined by API consumers, nd executed by the API provider. Fat - The webhook will carry a payload, submitting a predefined schema, usually of the associated object to the target. Ping - The webhook does not carry a payload, simply pinging the target of a...[<a href="/2017/08/15/where-to-begin-with-webhooks-for-the-human-services-data-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/15/the-elasticsearch-security-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/elastic-search/elasticsearch-security-apis.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/15/the-elasticsearch-security-apis/">The ElasticSearch Security APIs</a></h3>
			<p><em>15 Aug 2017</em></p>
			<p>I was looking at the set of security APIs over at Elasticsearch as I was diving into my API security research recently. I thought the areas they provide security APIs for the search platform was worth noting and including in not just my API security research, but also search, deployment, and probably overlap with my authentication research. Authenticate API - The Authenticate API enables you to submit a request with a basic auth header to authenticate a user and retrieve information about the authenticated user. Clear Cache API - The Clear Cache API evicts users from the user cache. You can completely clear the cache or evict specific users. User Management APIs - The user API enables you to create, read, update, and delete users from the native realm. These users are commonly referred to as native users. Role Management APIs - The Roles API enables you to add, remove, and retrieve roles in the native realm. To use this API, you must have at least the manage_security cluster privilege. Role Mapping APIs - The Role Mapping API enables you to add, remove, and retrieve role-mappings. To use this API, you must have at least the manage_security cluster privilege. Privilege APIs - The has_privileges API allows you to determine whether the logged in user has a specified list of privileges. Token Management APIs - The token API enables you to create and invalidate bearer tokens for access without requiring basic authentication. The get token API takes the same parameters as a typical OAuth 2.0 token API except for the use of a JSON request body. Come to think of it, I’ll add this to my API management research as well. Much of this overlaps with what should be a common set of API management services as well. Like much of my research, there are many different dimensions to my API security research. I’m looking to see how API providers are securing their APIs, as well...[<a href="/2017/08/15/the-elasticsearch-security-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/15/addressing-bulk-api-operations-as-separate-set-of-services/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/cargo-ship-zoomed-in-on-sea_light_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/15/addressing-bulk-api-operations-as-separate-set-of-services/">Addressing Bulk API Operations As Separate Set Of Services</a></h3>
			<p><em>15 Aug 2017</em></p>
			<p>Part of the feedback I’ve received from the Human Services Data API (HSDA) evolution from v1.0 to v1.1 was that the API didn’t allow for volume or bulk GET, POST, PUT, or DELETE. This was intentionally in the incremental release which focused on just making sure the API reflected 100% of the surface are for the Human Services Data Specification (HSDS). I wanted to separate out the needs of bulk API consumers, so that I could think about it separately from the more simple, micro-use integrations the default Human Services Data API would accommodate. I don’t want the industrial grade needs of database and system administrators overriding the simple access needs of other individual API consumers. To kick off my human services bulk API definition I wanted to spend some time looking at other bulk implementations from a handful of leading providers: Intercom ElasticSearch SalesForce Diffbot As I went through these implementations, and searched through Stack Overflow about bulk HTTP POSTs, I really didn’t see much difference on the technical front. Bulk APIs are primarily about acknowledging heavy duty data consumption, and primarily used HTTP POST for allowing the submission of either a) large individual POST, or b) large number of POST. Technically there isn’t much to them, where you start finding the nuance of bulk APIs over regular APIs is in the process surrounding the API implementation, things like having tasks, jobs, history, notifications, webhooks, email, and logging. Meaning there is just a lot more process and expectation around these APIs, which also most likely translates into more robust background architecture, and rules regarding the process involved with access. Bulk APIs seem more about a separation of concerns. Bulk GET and POSTs require more infrastructure, and to do it properly you need process, and checks and balances to make sure a bulk operation is successfully executed, and there is sufficient history, notifications, and other events around bulk transactions. I’d say that bulk API operations...[<a href="/2017/08/15/addressing-bulk-api-operations-as-separate-set-of-services/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/14/some-microservice-thoughts-around-my-human-services-api-work/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/algo-microservices.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/14/some-microservice-thoughts-around-my-human-services-api-work/">Some Microservice Thoughts Around My Human Services API Work</a></h3>
			<p><em>14 Aug 2017</em></p>
			<p>The Human Services Data API I have been working on is about defining a set of API paths for working with organizations, locations, and services that are delivering human services in cities around the world. As I’m working to evolve the OpenAPI for the Human Services Data API (HSDA), I’m constantly mindful of bloat, unnecessary expansion of features, and always working to separate things by concern. My thoughts have evolved this due to a hackathon I attended this week in San Francisco where a team at Optmizely worked to decouple an existing human services application from its backend and help teach it to speak Human Services Data Specification (HSDS)–allowing it to speak a common language around the services that us humans depend on daily. As the hackathon team was decoupling the single page application (React) from the API backend (Firebase) I took the API calls behind and published to Github as two JSON files. One of the files was locations, which contained 217 human service locations in San Francisco, and metadata, which contained a handful of categories being used to organize and display the locations. In this situation, there is no notion of an organization, just 217 locations, offering human services across five categories. This legacy application, and forward engineering hackathon project was quickly becoming a microservices project, ironically it is a microservice project that was about delivering human services. ;-) Looking at this unfolding project through a microservices lens, I needed to provide a single service. In the context of Link-SF, the original project, I needed to offer a service that would deliver 217 locations where people can find human services in the areas of food, housing, hygiene, medical, and technology via an web, or mobile application. To help me achieve my goals I began to step through each of the steps of the lifecycle of any self-contained microservices: Github - Each of my services begins with a Github repository, so I created a...[<a href="/2017/08/14/some-microservice-thoughts-around-my-human-services-api-work/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/14/investing-the-time-to-learn-api-best-practices-so-you-do-not-reinvent-the/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/14/investing-the-time-to-learn-api-best-practices-so-you-do-not-reinvent-the/">Investing The Time To Learn API Best Practices So You Do Not Reinvent The</a></h3>
			<p><em>14 Aug 2017</em></p>
			<p>I was on a call the other day with a group of people who are in the trenches of organizations and companies working hard to deliver human services in cities around the country. We were meeting to kick of the design phase of a new type of API, and after they shared all their thoughts via project documentation, they were asking me to help identify examples of best practices from the space. The group felt they didn’t have the time, or the awareness of what is going on to be able to identify the best practices that already exist across the space. This is one of the reasons I stay out of the weeds of individual projects. I may help define, design, and even shadow the deployment and management, but I work hard to avoid the tractor beam of ongoing projects so that I can pay attention to the bigger picture and help share stories about what I’m seeing. I feel like there should be people like me in each industry helping shine a light on, and aggregating of best practices when it comes to the API life cycle. There is just too much work to be done, and it helps to have folks who have domain expertise, not just lightly understanding what is going on across many sectors like I do. I think it is fine for the sector to depend on API analysts like me, but I think that groups should also be investing in the time to pick up their heads up and pay attention to what else is going on when it comes to APIs in their industry. I understand that many groups are busy keeping systems operational, and dealing with real world problems, but dedicated reading of blogs, white papers, and tuning into social channels for other API providers is important as well. Each decision made on API design, deployment, and management should be established from time spent reading, and...[<a href="/2017/08/14/investing-the-time-to-learn-api-best-practices-so-you-do-not-reinvent-the/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/14/decoupling-the-business-of-my-human-services-microservice/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_dali_three.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/14/decoupling-the-business-of-my-human-services-microservice/">Decoupling The Business Of My Human Services Microservice</a></h3>
			<p><em>14 Aug 2017</em></p>
			<p>I’ve been looking at my human services API work through a microservices lens, triggered by the deployment of a reduced functionality version of the human services implementation I was working on this week. I’m thinking a lot about the technical side of decoupling services using APIs, but I want to also take a moment and think about the business side of decomposing services, while also making sure they are deployed in a way that meets both the provider and consumer side of the business equation. My human services microservice implementation is in the public service, which is a space where the business conversation often seems to disappear behind closed doors, but in reality needs to be front and center with each investment (commit) made into any service. Let’s take a moment to look at the monetization strategy and operational plan for my human service microservice. Yes, a public data microservice should have a monetization strategy and plan for operating and remaining sustainable. The goals for this type of microservice will be radically different than it would be for a commercial microservice, but it should have one all the same. Monetization - How am I evaluating the investment into this project alongside any value that is generated, which I can potentially capture or exchange some value for some money to keep going. Acquisition - What did it take for me to acquire the data and skills necessary to make the deployment possible. Development - What time was invested in setting up the platform, developing the schema, data, definitions, code, and visual elements. Operations - What does it take to operate the service? Maintain it, answer questions, provide support, and other realities of providing an online service today. Direct Value - What are the direct benefits of having this service available to people looking for human services, or organizations looking to provide human services. Indirect Value - What are the indirect benefits of having this service available,...[<a href="/2017/08/14/decoupling-the-business-of-my-human-services-microservice/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/14/api-platform-faq-and-qa-responsibility/"><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/aws-answers-icons.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/14/api-platform-faq-and-qa-responsibility/">API Platform FAQ And QA Responsibility</a></h3>
			<p><em>14 Aug 2017</em></p>
			<p>The discussion around whether or not you should be hosting your own questions and answers (QA) and frequently asked questions (FAQ) for your API has continued, with many of the leading API pioneers asserting responsibility over the operations of these important API resources. Amazon noticed that answers about their platform on Quora and Stack Exchange were usually out of date and often just plain wrong, prompting them to launch their own QA solution. I have written about using API providers using Stack Overflow for may years now. It the last few years I’ve had my readers push back on this for a variety of reasons, from the Stack Overflow community being primarily a white male bro-fest, to finding things being unreliable, out of date, and often a pretty hostile and unfriendly place for people to try and learn about APIs. I’d say that I still use Stack Overflow for about 40% of my querying of API and programming related subjects, but since I’m a white male who has been doing software for 30 years, I’m a little more resistant to the bro-fest. But, I get it, and hear what folks are saying, and get it is not always a suitable environment. Going back and forth on this subject, I’m back in the camp where API providers should be investing in operating their own QA, FAQ, and support forums. It’s definitely requires a significant amount of investment, policing, and sometimes taking stances that are unpopular, but if you are in this for the long game, it will be worth it. After watching AWS for a decade, you can see how incorrect information about your API operations can really begin to become a liability, and you might want to keep a tighter grip on where your API consumers go look for their answers. An added bonus is that you also get to set the tone for the types of questions that get answered, and the inclusiveness that...[<a href="/2017/08/14/api-platform-faq-and-qa-responsibility/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/11/link-relation-types-for-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/erik-wilde/link-relation-types-for-web-services.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/11/link-relation-types-for-apis/">Link Relation Types for APIs</a></h3>
			<p><em>11 Aug 2017</em></p>
			<p>
I have been reading through a number of specifications lately, trying to get more up to speed on what standards are available for me to choose from when designing APIs. Next up on my list is Link Relation Types for Web Services, by Erik Wilde. I wanted to take this informational specification and repost here on my site, partially because I find it easier to read, and the process of breaking things down and publishing as a posts helps me digest the specification and absorb more of what it contains.

I’m particularly interested in this one, because Erik captures what I’ve had in my head for APIs.json property types, but haven’t been able to always articulate as well as Erik does, let alone published as an official specification. I think his argument captures the challenge we face with mapping out the structure we have, and how we can balance the web with the API, making sure as much of it becomes machine readable as possible. I’ve grabbed the meat of Link Relation Types for Web Services and pasted here, so I can break down, and reference across my storytelling.

[<a href="/2017/08/11/link-relation-types-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/11/embeddable-api-tooling-discovery-with-json-home/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-widgets.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/11/embeddable-api-tooling-discovery-with-json-home/">Embeddable API Tooling Discovery With JSON Home</a></h3>
			<p><em>11 Aug 2017</em></p>
			<p>I have been studying JSON Home, trying to understand how it sizes up to APIs.json, and other formats I’m tracking on like Pivio. JSON Home has a number of interesting features, and I thought one of their examples was also interesting, and was relevant to my API embeddable research. In this example, JSON Home was describing a widget that was putting an API to use as part of its operation.

Here is the snippet from the JSON Home example, providing all details of how it works:



JSON Home seems very action oriented. Everything about the format leads you towards taking some sort of API driven action, something that makes a lot of sense when it comes to widgets and other embeddables. I could see JSON Home being used as some sort of definition for button or widget generation and building tooling, providing a machine readable definition for the embeddable tool, and what is possible with the API(s) behind.

I’ve been working towards embeddable directories and API stacks using APIs.json, providing distributed and embeddable tooling that API providers and consumers can publish anywhere. I will be spending more time thinking about how this world of API discovery can overlap with the world of API embeddables, providing not just a directory of buttons, badges, and widgets, but one that describes what is possible when you engage with any embeddable tool. I’m beginning to see JSON Home similar to how I see Postman Collections, something that is closer to runtime, or at least deploy time. Where APIs.json is much more about indexing, search, and discovery–maybe some detail about where the widgets are, or maybe more detail about what embeddable resources are available.

[<a href="/2017/08/11/embeddable-api-tooling-discovery-with-json-home/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/11/about-api-data-gov/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-data-gov/9299911959_bdc195fb56_o.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/11/about-api-data-gov/">About api.data.gov</a></h3>
			<p><em>11 Aug 2017</em></p>
			<p>I’m going to borrow, modify, and improve on the content from api.data.gov, because it is an important effort I want my readers to be aware of, because I want more of them to help apply educate other federal agencies regarding why it is a good idea to bake api.data.gov into their API operations, and help apply pressure until EVERY federal agency is up and running using a common API management layer. Ok, so what is api.data.gov? api.data.gov is a free API management service for federal agencies. Our aim is to make it easier for you to release and manage your APIs. api.data.gov acts as a layer above your existing APIs. It transparently adds extra functionality to your APIs and helps deal with some of the repetitive parts of managing APIs. Here are the features of api.data.gov: You’re in control: You still have complete control of building and hosting your APIs however you like. No changes required: No changes are required to your API, but when it’s accessed through api.data.gov, we’ll transparently add features and handle the boring stuff. Focus on the APIs: You’re freed from worrying about things like API keys, rate limiting, and gathering usage stats, so you can focus on building the next great API. Make it easy for your users: By providing a standard entry point to participating APIs, it’s easier for developers to explore and use APIs across the federal government. api.data.gov handles the API keys for you: API key signup: It’s quick and easy for users to signup for an API key and start using it immediately. Shared across services: Users can reuse their API key across all participating api.data.gov APIs. No coding required: No code changes are required to your API. If your API is being hit through api.data.gov, you can simply assume it’s from a valid user. api.data.gov tracks all the traffic to your API and give you tools to easily analyze it: Demonstrate value: Understand how your...[<a href="/2017/08/11/about-api-data-gov/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/11/a-hack-day-event-to-help-the-linksf-app-speak-human-services-data/"><img src="https://s3.amazonaws.com/kinlane-productions2/open-referral/optimizely/optimizely-hackathon.JPG" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/11/a-hack-day-event-to-help-the-linksf-app-speak-human-services-data/">A Hack Day Event To Help The Link-SF App Speak Human Services Data</a></h3>
			<p><em>11 Aug 2017</em></p>
			<p>I went up to San Francisco on Wednesday to participate in a social good hack day at Optimizely. They held their event at their downtown offices, where 20+ employees showed up to hack on some social good projects. Open Referral and our partner Benetech had suggested Human Services Data Specification (HSDS) as a possible project, which resulted in us being one of the hack projects for the event. The Open Referral Human Services Data Specification (HSDS) team consisted of five Optimizely developers. Derek Hammond - Software Engineer Michael Fields - Software Engineer Zachary Power - Software Engineering Intern Quinton Dang - Software Engineer Asa Schachar - Engineering Manager The overall strength of the team leaned toward being front-end web and mobile developers, so we decided to “forward engineer” the Link-SF application, which provides a simple web or mobile application to help folks find a variety of human services in a handful of categories like food, housing, hygiene, medical, and technology. Link-SF is an ongoing collaboration between the Tenderloin Technology Lab and Zendesk, Inc., and we wanted to help contribute to their work, while also making the application potentially deployable by other cities and regions. Once the team got to work on the project they identified that we could get at the data behind the SF application. The team decide they would forward engineer the dataset, the API, and the UI for the web and mobile application, making it all speak Human Services Data Specification (HSDS)–here is what they did: Took the Link-SF datasets and saved as a single JSON file. Converted the JSON schema to use HSDS – the changes weren’t significant. Made it so that the app reads location data from a Github repository (you can change this to your url) Updated the taxonomy fetch to use the new data Ensured the UI worked with the new data You can find the project in an Optimizely Github repository, which they are going to invest...[<a href="/2017/08/11/a-hack-day-event-to-help-the-linksf-app-speak-human-services-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/10/patent-number-9325732-computer-security-threat-sharing/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/soldier_computer_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/10/patent-number-9325732-computer-security-threat-sharing/">Patent Number 9325732: Computer Security Threat Sharing</a></h3>
			<p><em>10 Aug 2017</em></p>
			<p>The main reason that I tend to rail against API specific patents is that much of what I see being locks up reflects the parts and pieces that are making the web work. I see things like hypermedia, and other concepts that are inherently about sharing, collaboration, and reuse–something that should never be patented. This concept applies to other patents I’m seeing, but rather than being about the web, it is about trust, and sharing of information. Things that shouldn’t be locked up, and exist within realms where the concept of patents actually hurt the web and APIs. Today’s patent is out of Amazon, who are prolific patenters of web and API concepts. This one though is about the sharing of security threat sharing. Outlining something that should be commonplace on the web. Title - Computer security threat sharing Number - 09325732 Owner - Amazon Technologies, Inc. Abstract - A computer security threat sharing technology is described. A computer security threat is recognized at an organization. A partner network graph is queried for security nodes connected to a first security node representing the organization. The first security node is connected to at least a second security node representing a trusted security partner of the organization. The second security node is associated with identification information. The computer security threat recognized by the organization is communicated to the trusted security partner using the identification information associated with the second security node. I’m sorry. I just do not see this as unique, original, or remotely a concept that should be patentable. Similar to a previous patent I wrote about on trust, I just don’t think that sharing of security information needs to be locked up. The USPTO should recognize this. I feel like this type of patent shows how broken the patent process is, and how distorted company’s views on what is a patentable idea. Honestly, these types of patents feel lazy to me, and lack any creativity,...[<a href="/2017/08/10/patent-number-9325732-computer-security-threat-sharing/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/10/observability-in-botnet-takedown-by-government-on-private-infrastructure/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-bot-api.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/10/observability-in-botnet-takedown-by-government-on-private-infrastructure/">Observability In Botnet Takedown By Government On Private Infrastructure</a></h3>
			<p><em>10 Aug 2017</em></p>
			<p>I’m looking into how to make API security more transparent and observable lately, and looking for examples of companies, institutions, organizations, politicians, and the government are calling for observability into wherever APIs are impacting our world. Today’s example comes out of POLITICO’s Morning Cybersecurity email newsletter, which has become an amazing source of daily information for me, regarding transparency around the take down of bot networks. “If private companies cooperate with government agencies - for example, in the takedown of botnets using the companies’ infrastructure - they should do so as publicly as possible, argued the Center for Democracy &amp; Technology . “One upside to compulsory powers is that they presumptively become public eventually, and are usually overseen by judges or the legislative branch,” CDT argued in its filing. “Voluntary efforts run the risk of operating in the dark and obscuring a level of coordination that would be offensive to the general public. It is imperative that private actors do not evolve into state actors without all the attendant oversight and accountability that comes with the latter.” I’ve been tracking on the transparency statements and initiatives of all the API platforms. At some point I’m going to assemble the common building blocks of what is needed for executing platform transparency, and I will be including these asks of the federal government. As the Center for Democracy &amp; Technology states this relationship between the public and private sector when it comes to platform surveillance needs to be more transparent and observable in all forms. Bots, IoT, and the negative impacts of API automation needs to be included in the transparency disclosure stack. If the government is working with platform to discover, surveil, or shutdown bot networks there should be some point in which operations should be shared, including the details of what was done. We need platform transparency and observability at the public and private sector layer of engagement. Sure, this sharing of information would be...[<a href="/2017/08/10/observability-in-botnet-takedown-by-government-on-private-infrastructure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/10/my-focus-on-public-apis-also-applies-internally/"><img src="https://s3.amazonaws.com/kinlane-productions2/photos/public-market.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/10/my-focus-on-public-apis-also-applies-internally/">My Focus On Public APIs Also Applies Internally</a></h3>
			<p><em>10 Aug 2017</em></p>
			<p>A regular thing I hear from folks when we are having conversations about the API lifecycle, is that I focus on public APIs, and they are more interested in private APIs. Each time I hear this I try to take time and assess which parts of my public API research wouldn’t apply to internal APIs. You wouldn’t publish your APIs to pubic API search engines like APIs.io or ProgrammableWeb, and maybe not evangelizing your APIs at hackathons, but I’d say 90% of what I study is applicable to internal APIs, as well as publicly available APIs. With internal APIs, or private network partner APIs you still need a portal, documentation, SDKs, support mechanisms, and communication and feedback loops. Sure, how you use the common building blocks of API operations that I track on will vary between private and public APIs, but this shifts from industry to industry, and API to API as well–it isn’t just a public vs. private thing. I would say that 75% of my API industry research is derived from public API operations–it is just easier to access, and honestly more interesting to cover than private ones. The other 25% of internal API conversation I’m having, always benefit from thinking through the common API building blocks of public APIs, looking for ways they can be more successful with internal and partner APIs. I’d say that a significant part of the mindshare for the microservices philosophy is internally focused. I think this is something that will come back to hurt some implementations, cutting out many of the mechanisms and common elements required in a successful API formula. Things like portals, documentations, SDKs, testing, monitoring, discovery, support, communications all contribute to an API working, or not working. I’ve said it before, and I’ll say it again. I’m not convinced that there is any true separation in public vs private APIs, and there remains to be a great deal we can learn from public API...[<a href="/2017/08/10/my-focus-on-public-apis-also-applies-internally/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/10/image-logging-with-amazon-s3-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/freeway_atari_missle.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/10/image-logging-with-amazon-s3-api/">Image Logging With Amazon S3 API</a></h3>
			<p><em>10 Aug 2017</em></p>
			<p>I have been slowly evolving my network of websites in 2017, overhauling the look of them, as well as how they function. I am investing cycles into pushing as much of my infrastructure towards being as static as possible, minimizing my usage of JavaScript wherever I can. I am still using a significant amount of JavaScript libraries across my sites for a variety of use cases, but whenever I can, I am looking to kill my JavaScript or backend dependencies, and reduce the opportunity for any tracking and surveillance. While I still keep Google Analytics on my primary API Evangelist sites, as my revenue depends on it, whenever possible I keep personal projects without any JavaScript tracking mechanisms. Instead of JavaScript I am defaulting to image logging using Amazon S3. Most of my sites tend to have some sort of header image, which I store in a common public bucket on Amazon S3, all I have to do is turn on logging, and then get at logging details via the Amazon S3 API. Of course, images get cached within a user’s browser, but the GET for my images still gives me a pretty good set of numbers to work with. I’m not concerned with too much detail, I just generally want to understand the scope of traffic a project is getting, and whether it is 5, 50, 500, 5,000, or 50,000 visitors. My two primary CDNs are Amazon S3 and Github. I’m trying to pull together a base strategy for monitoring activity across my digital footprint. My business presence is very different than my personal presence, but with some of my personal writing, photography, and other creations I still like to keep a finger on the pulse of what is happening. I am just looking to minimize the data gathering and surveillance I am participating in these days. Keeping my personal and business websites static, and with a minimum footprint is increasingly important to me....[<a href="/2017/08/10/image-logging-with-amazon-s3-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/09/open-sourcing-your-api-like-versioneye/"><img src="https://s3.amazonaws.com/kinlane-productions2/versioneye/version-eye-containers.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/09/open-sourcing-your-api-like-versioneye/">Open Sourcing Your API Like VersionEye</a></h3>
			<p><em>09 Aug 2017</em></p>
			<p>I’m always on the hunt for healthy patterns that I would like to see API providers, and API service providers consider when crafting their own strategies. It’s what I do as the API Evangelist. Find common patterns. Understand the good ones, and the bad ones. Tell stories about both, helping folks understand the possibilities, and what they should be thinking about as they plan their operations. One very useful API that notifies you about security vulnerabilities, license violations and out-dated dependencies in your Git repositories, has a nice approach to delivering their API, as well as the other components of their stack. You can either use VersionEye in the cloud, or you can deploy on-premise: versioneye-core - Models, Services &amp; Mails for VersionEye crawl_r - VersionEye crawlers implemented in Ruby. versioneye-security - Security Crawler for VersionEye versioneye-api - JSON REST API for VersionEye versioneye-tasks - Thin wrapper around the versioneye-core. versioneye - VersionEye.com VersionEye also has their entire stack available as Docker images, ready for deployment anywhere you need them. I wanted have a single post that I can reference when talking about possible open source, on-premise, continuous integration approaches to delivering API solutions, that actually have a sensible business model. VersionEye spans the areas that I think API providers should consider investing in, delivering SaaS or on-premise, while also delivering open source solutions, and generating sensible amounts of revenue. Many APIs I come across do not have an open source version of their API. They may have open source SDKs, and other tooling on Github, but rarely does an API provider offer up an open source copy of their API, as well as Docker images. VersionEye’s approach to operating in the cloud, and on-premise, while leveraging open source and APIs, as well as dovetailing with existing continuous integration flows is worth bookmarking. I am feeling like this is the future of API deployment and consumption, but don’t get nervous, there is still plenty of...[<a href="/2017/08/09/open-sourcing-your-api-like-versioneye/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/09/continuous-integration-and-deployment-for-government-procurement/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/09/continuous-integration-and-deployment-for-government-procurement/">Continuous Integration And Deployment For Government Procurement</a></h3>
			<p><em>09 Aug 2017</em></p>
			<p>I was reading the Open by Default Portal Procurement Pilot for the Treasury Board of Canada, where section 6, Licensing states: “To support the objectives of the open government initiative, the Solution must be open source and licensed in accordance with the Massachusetts Institute of Technology License (“MIT License”). Under the resulting contract, the Contractor will be required to deposit the Solution’s source code on the GitHub platform (https://github.com) – under the MIT License.” This just seems like the way it should be for all government technology solutions. I’ve heard the naysayers in federal government say that proprietary software is the best route, but if it drives public infrastructure, in my opinion the code should be publicly available in this way. Honestly, code should be deployed at regular intervals throughout the development and deployment process, opening up the code to QA and security audits by the public, and 3rd parties. I hope this approach evolves into more of a continuous deployment and integration workflow when it comes to delivering software in government, where vendors have to plugin, open up their delivery cycles to more scrutiny, and leverage Github as the center of each procurement step from start to finish. Heck, let’s connect payments to each stop along the way. I’m a proponent of this not just to make the delivery of government software more observable and accountable. I want this process out in the open to help other agencies learn from the journey. Tune into the process, and maybe reuse, build upon, and evolve existing solutions as part of their operations. I will keep an eye on what is going on up in Canada, when it comes to requiring vendors publish code to Github. I also know there are similar efforts in the U.S. and other countries which I’ll also start scratching at and learning more about. It is definitely a healthy pattern I’d like to see more of, and I will continue to invest...[<a href="/2017/08/09/continuous-integration-and-deployment-for-government-procurement/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/09/an-open-source-api-security-intelligence-gathering-processing-and/"><img src="https://s3.amazonaws.com/kinlane-productions2/GOSINT/gosint.gif" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/09/an-open-source-api-security-intelligence-gathering-processing-and/">An Open Source API Security Intelligence Gathering, Processing, And</a></h3>
			<p><em>09 Aug 2017</em></p>
			<p>I was reading about GOSINT, the open source intelligence gathering and processing framework over at Cisco. “GOSINT allows a security analyst to collect and standardize structured and unstructured threat intelligence. Applying threat intelligence to security operations enriches alert data with additional confidence, context, and co-occurrence. This means that you are applying research from third parties to your event data to identify similar, or identical, indicators of malicious behavior.” The framework is written in Go, with a front-end in JavaScript frontend, and usage of APIs as threat intelligence sources. When you look at configuration section on the README for GOSINT, you’ll see information for setting up threat intelligence feeds, including Twitter API, Alien Vault the Open Threat Community API, VirusTotal API, and the Collaborative Research Into Threats (CRITS). GOSINT acts as an API aggregator for a variety of threat information, which then allows you to scour the information for threat indicators, which you can evolve over time, providing a pretty interesting model for not just threat information sharing, but also API driven aggregation, curation and sharing. GOSINT also has the notion of behaving as a “transfer station”, where you can export refined data as CSV or CRITS format. Right here seems like an opportunity for some Github integration, adding continuous integration and deployment to open source intelligence and processing workflows. Making sure refined, relevant threat information is available where it is needed, via existing API deployment and integration workflows. Wouldn’t take much to publish CSV, YAML, and JSON files to Github which can then be used to drive distributed dashboards, visualizations, and other awareness building tools. Plus, the refined threat information is now published as CSV/JSON/YAML on Github where it can be ingested by any system of application with access to the Github repository. GOSINT is just one of the interesting tooling I’m coming across as I turn up the volume on my API security research, thanks to the investment of ElasticBeam my API security partner....[<a href="/2017/08/09/an-open-source-api-security-intelligence-gathering-processing-and/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/09/a-fresh-look-at-the-embeddable-tools-built-on-the-twitter-api/"><img src="https://s3.amazonaws.com/kinlane-productions2/twitter/twitter-websites-embeddable.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/09/a-fresh-look-at-the-embeddable-tools-built-on-the-twitter-api/">A Fresh Look At The Embeddable Tools Built On The Twitter API</a></h3>
			<p><em>09 Aug 2017</em></p>
			<p>
Over the years I have regularly showcased Twitter as an example API driven embeddable tools like buttons, badges, and widgets. In 2017, after spending some time in the Twitter developer portal, it is good to see Twitter still investing in their embeddable tools. The landing page for the Twitter embeddables still provides the best example out there of the value of using APIs to drive data and content across a large number of remote web sites.

Twitter has distinct elements of their web embeddables:


  Tweet Button - That classic tweet button, allowing users to quickly Tweet from any website.
  Embedded Tweets - Taking any Tweet and embedding on a web page showing its full content.
  Embedded Timeline - Showing curated timelines on any website using a Twitter embeddable widget.
  Follow Button - Helping users quickly follow your Twitter account, or your companies Twitter account.
  Twitter Cards - Present link summaries, engaging images, product information, or inline video as embeddable cards in timeline.


Account interactions, messaging, posting, and other API enabled function made portable using JavaScript allowing it to be embedded and executed on any website. JavaScript widgets, buttons, and other embeddables are still a very tangible, useful example of APIs in action. Something I can talk about to anyone about, helping them understand why you might want to do APIs, or at least know about APIs.

We bash on Twitter a lot in the API community. However, after a decade of operation, you have to give it to them. They are still doing it. They are still keeping it simple with embeddable tools like this. I can confidently say that APIs are automating some serious illness on the Twitter API platform at the moment, and there are many things I’d like to be different with the Twitter API, but I am still pleased that I can keep finding examples from the Twitter platform to showcase on API Evangelist seven years of writing about them.

[<a href="/2017/08/09/a-fresh-look-at-the-embeddable-tools-built-on-the-twitter-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/08/reducing-developers-to-a-transaction-with-apis-microservices-serverless/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_dark_dali.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/08/reducing-developers-to-a-transaction-with-apis-microservices-serverless/">Reducing Developers To A Transaction With APIs, Microservices, Serverless,</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>A topic that keeps coming up in discussions with my partner in crime Audrey Watters (@audreywatters) about our podcast is around the future of labor in an API world. I have not written anything about this, which means I’m still in early stages of any research into this area, but it has come up in conversation, and reflected regularly in my monitoring of the API space, I need to begin working through my ideas in this area. A process that helps me better see what is coming down the API pipes, and fill the gaps in what I do not know. Audrey has long joked about my API world using a simple phrase: “reducing everything to a transaction”. She says it mostly in jest, but other times I feel like she wields it as the Cassandra she channels. I actually bring up the phrase more than she does, because it is something I regularly find myself working in the service of as the API Evangelist. By taking a pro API stance I am actively working to reduce legacy business, institutional, and government processes down and breaking them down into a variety of individual tasks, or if you see things through a commercial lens, transactions. Microservices A microservices philosophy is all about breaking down monoliths into small bite size chunks, so they can be transacted independently, scaled, evolved, and deprecated in isolation. Microservices should do one thing, and do it well (no backtalk). Microservices should do what it does as efficiently as possible, with as few dependencies as possible. Microservices are self-contained, self-sufficient, and have everything they need to get the job done under a single definition of a service (a real John Wayne of compute). And of course, everything has an API. Microservices aren’t just about decoupling the technology, they are are about decoupling the business, and the politics of doing business within SMB, SME, enterprises, institutions, and government agencies–the philosophy for reducing everything to...[<a href="/2017/08/08/reducing-developers-to-a-transaction-with-apis-microservices-serverless/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/08/patent-9397835-web-of-trust-management-in-a-distributed-system/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/gypsy-eyes_blue_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/08/patent-9397835-web-of-trust-management-in-a-distributed-system/">Patent #9397835, Web of trust management in a distributed system</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>I found a couple more API patents in my notebook that I wanted to get published. I try to take time regularly to publish the strangest API related patents I can find. Today’s patent is out of Amazon, which I find to be a fascinating outlet for patent storytelling. It isn’t squarely in the realm of APIs like some of my others, but I think tells a fascinating story by itself, showing how the web and the concept of a patent are colliding. Title - Web of trust management in a distributed system Number - 9397835 Owner - Amazon Technologies, Inc. Publication Date - 2016-07-19 Application Date - 2014-05-21 Abstract - A web of trust is used to validate states of a distributed system. The distributed system operates based at least in part on a domain trust. A root of trust issues the domain trust issues a domain trust. Domain trusts are updatable in accordance with rules of previous domain trusts so that a version of a domain trust is verifiable by verifying a chain of previous domain trust versions._ I like that trust is being patented. Digital trust as a patentable concept that Amazon can now delegate if they choose. I’m just fascinated by what concepts are now fair game for patenting, as they enter into the digital realm. Now I’m curious how many physical trust patents might exist. Is the management of trust patented in the physical world in any way? I guess I could see some of the components for determining trust could be patented, but I find the fact that trust, or more specifically trust management is patentable, as a troubling thing. It’s no secret that I’m anti API patents. I’m rarely convinced of the uniqueness of anything digital, warranting the issuing of a patent by the USPTO. I have to say that in a world where trust is patentable, the environment for suspect behavior will flourish. Pretty much what we...[<a href="/2017/08/08/patent-9397835-web-of-trust-management-in-a-distributed-system/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/08/my-url-shortener-is-just-an-api-with-postman-as-my-client/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-url-square.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/08/my-url-shortener-is-just-an-api-with-postman-as-my-client/">My URL Shortener Is Just An API With Postman As My Client</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>
I have my own URL shortener for API Evangelist called apis.how. I use it to track the click through rates for some of my research projects, and partner sponsorships. I’ve had the URL shortener in operation for about two years now, and I still do not have any type of UI for it, relying 100% on Postman for adding, searching, and managing the URLs I am shortening, and tracking on.

My URL shortener just hasn’t raised to a level of priority where I’ll invest any time into an administrative interface, or dashboard for my URL shortener. I used Bitly and Google for a while, but I really just needed a simple shortening with basic counts, nothing more. When I bought the domain I launched a handful of API endpoints to support, allowing me to add, update, search, and remove URLs, as well as track the click throughs, and query how many clicks a link received for each mont. I can easily accomplish all of this through the Postman interface, making basic calls to my simple API–no over-engineering necessary.

I have been considering running a daily job that pulls view counts for URLs and publishing to Github as YAML, where I can drive a simple visualization, but honestly I’m not that numbers oriented. I like what API clients like Postman and Restlet provide. Even though I’m equipped to make calls using JavaScript, PHP, or CURL, I prefer just accessing via my web client–no coding necessary. I wouldn’t manage all my systems in this way, but for really basic ones like my URL shortener, I’m not sure I will ever actually evolve it beyond just being a simple API.

[<a href="/2017/08/08/my-url-shortener-is-just-an-api-with-postman-as-my-client/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/08/http-as-a-substrate/"><img src="https://s3.amazonaws.com/drone-recovery/babyfoot04.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/08/http-as-a-substrate/">HTTP as a Substrate</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>I am spending a significant amount of time reading RFCs lately. I find the documents to be very cumbersome to read, but the more you read, the more tolerant you become. When I browse through RFCs I’m always reminded of how little I actually know about the web. In an effort to push forward my education, and maybe yours along the way, I’m going to be cherry picking specific sections of the interesting RFCs I’m digesting here on the blog. Today’s RFC is 3205, filed under Best Current Practice”, and is on the use of HTTP as a Substrate. _Recently there has been widespread interest in using Hypertext Transfer Protocol (HTTP) [1] as a substrate for other applications- level protocols. Various reasons cited for this interest have included: familiarity and mindshare, compatibility with widely deployed browsers, ability to reuse existing servers and client libraries, ease of prototyping servers using CGI scripts and similar extension mechanisms, authentication and SSL or TLS, the ability of HTTP to traverse firewalls, and cases where a server often needs to support HTTP anyway. The Internet community has a long tradition of protocol reuse, dating back to the use of Telnet as a substrate for FTP and SMTP. However, the recent interest in layering new protocols over HTTP has raised a number of questions when such use is appropriate, and the proper way to use HTTP in contexts where it is appropriate. Specifically, for a given application that is layered on top of HTTP: Should the application use a different port than the HTTP default of 80? Should the application use traditional HTTP methods (GET, POST, etc.) or should it define new methods? Should the application use http: URLs or define its own prefix? Should the application define its own MIME-types, or use something that already exists (like registering a new type of MIME-directory structure)? This memo recommends certain design decisions in answer to these questions. This memo is intended as...[<a href="/2017/08/08/http-as-a-substrate/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/08/api-message-integrity-with-json-web-token-jwt/"><img src="https://s3.amazonaws.com/kinlane-productions2/json-web-token/json-web-token.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/08/api-message-integrity-with-json-web-token-jwt/">API Message Integrity with JSON Web Token (JWT)</a></h3>
			<p><em>08 Aug 2017</em></p>
			<p>
I don’t have any production experience deploying JSON Web Tokens (JWT), but it has been something I’ve been reading up on, and staying in tune with for some time. I often reference JWT as the leading edge for API authentication, but there is one aspect of JWT I think is worth me referencing more often–message integrity. JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.

JWT can not only be used for authentication of both message sender/receiver, it can ensure the message integrity as well, leveraging a digital signature hash value of the message body to ensure the message integrity during transmission. It adds another interesting dimension to the API security conversation, and while not be applicable in all APIs, I know many that it would make a lot of sense. Many of the networks we use today and applications we depend on today are proxied, creating an environment where message integrity should always come into question, and JWT gives us another tool in our toolbox to help us keep things secure.

I’m working my way through each layer of API operations, looking for aspects of API security that are often obscured, hidden, or just not discussed as they should be. I feel like JWT is definitely one track of API security that has evolved the conversation significantly over the last couple years, and is something that can make a significant impact on the space with just a little more storytelling and education. I’m going to make sure API request and response message integrity is a regular part of my API security storytelling, curriculum, and live talks that I develop.

[<a href="/2017/08/08/api-message-integrity-with-json-web-token-jwt/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/07/the-subtle-ways-in-which-power-asserts-itself-in-face-of-api-engagements/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/power-lines-empty-space_copper_circuit.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/07/the-subtle-ways-in-which-power-asserts-itself-in-face-of-api-engagements/">The Subtle Ways In Which Power Asserts Itself In Face Of API Engagements</a></h3>
			<p><em>07 Aug 2017</em></p>
			<p>I’m rarely surprised by, but still often caught off guard by the subtle ways in which power asserts itself when faced with change the introduced by API projects. In my 30 years as a database professional I’ve seen numerous overt, covert, and subversive ways in which existing holders of power (data), but I often still get blindsided by the creative, and subtle ways in which folks defend what they already have, and work to keep things from changing. While doing unfunded work to define industry level API specifications, and help move forward the API conversation in multiple industries, I’ve been encountering two pockets of friction I want to understand better, so I can develop some sort of grease, that might make things smoother. There are two main pockets of practitioners in this setting, implementors (those you publish an API), and vendors (those who currently sell solutions to implementors). My role in any industry as the API Evangelist is to help ultimately define and showcase healthy, common API definitions, that can be reused across the API lifecycle–from design to deprecation. The Vendor Question Trying to understand the culture, and motivations of any SMB, SME, or enterprise is often a full time job I do not have the time or resources for. I work pretty hard to understand any of the key players in any industry being touched by APIs, but will still have much I need to learn. One thing I do know, is that I should never take a company for face value, because behind the mask there is always much more going on. Honestly, APIs are actually a pretty good barometer of what is going on behind the scenes, something I don’t think some API providers fully grasp before they decide to jump into the API game. The most obvious thing a vendor in an industry will do is ignore you. When approached about participating in discussions around a common API definition, they’ll just...[<a href="/2017/08/07/the-subtle-ways-in-which-power-asserts-itself-in-face-of-api-engagements/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/07/slow-moving-ransomware-as-the-new-business-model/"><img src="https://bioscopic.files.wordpress.com/2010/11/barneyoldfield.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/07/slow-moving-ransomware-as-the-new-business-model/">Slow Moving Ransomware As The New Business Model</a></h3>
			<p><em>07 Aug 2017</em></p>
			<p>I was reading about the difficulties the City of New York was having when it comes to migrating off of the Palantir platform, while also reading about the latest cybersecurity drama involving ransomware. I’m spending a lot of time studying cybersecurity lately, partly because they involve APIs, but mostly because it is something that is impacting every aspect of our lives, including our democracy, education, and healthcare. One thing I notice on the cybersecurity stage, is that everything is a much more extreme, intense, representation of what is going on in the mainstream tech industry. Ransomware is software that gets installed on your desktop or servers and locks up all your data until you pay the software developer (implementor) a ransom. Ransomware is just a much faster moving version of what many of us in the software industry call vendor lock-in. This is what you are seeing with Palantir, and the City of New York. What tech companies do is get you to install their software on your desktop or servers, or convince you to upload all your data into the cloud, and use their software. This is business 101 in the tech industry. You either develop cloud-based software, something that runs on-premise, or you are a mix of both. Ideally, your customers become dependent on you, and they keep paying your monthly, quarterly, or annual subscriptions (cough cough ransom). Here is where the crafty API part of the scheme comes in. Software providers can also make APIs that allow your desktop and server to integrate with their cloud solutions, allowing for much deeper integration of data, content, and algorithms. The presence of APIs SHOULD also mean that you can more easily get your data, content, and algorithms back, or have kept in sync the whole time, so that when you are ready to move on, you don’t have a problem getting your data and content back. The problem is, that APIs “CAN” enable this,...[<a href="/2017/08/07/slow-moving-ransomware-as-the-new-business-model/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/07/providing-code-citations-in-machine-learning-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/algorithmia/machine-learning-citation.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/07/providing-code-citations-in-machine-learning-apis/">Providing Code Citations In Machine Learning APIs</a></h3>
			<p><em>07 Aug 2017</em></p>
			<p>
I was playing around with the Style Thief, an image transfer API from Algorithmia, and I noticed the citation for the algorithm behind. The API is an adaptation of Anish Athalye’s Neural Style Transfer, and I thought the algorithmic citation of where the work was derived from was an interesting thing to take note of for my machine learning API research.

I noticed on Algorithmia’s page there was a Bibtex citation, which referenced the author, and project Github repository:

@misc{athalye2015neuralstyle,
   author = {Anish Athalye},
   title = {Neural Style},
   year = {2015},
   howpublished = {\url{https://github.com/anishathalye/neural-style}},
   note = {commit xxxxxxx}
}

This provides an interesting way to address citation in not just machine learning, but with open source driving algorithmic APIs in general. It gives me food for thought when it comes to what licensing I should be considering when wrapping open source software with an API. I’ve been thinking about dependencies a lot lately when it comes to APIs and their definitions, and I’d consider citation or attribution to be in a similar category. I guess rather then technical dependency, it is more in the business and legal dependency category.

Similar to how Pivio allows you to reference dependencies for your microservices, I’m thinking that API Commons, or some other format like Bibtext could provide a machine readable citation, that could be indexed as part of an APIs.json index. Allowing us API designers, architect, and providers to provide proper citation for where our work is derived. These aren’t just technical dependencies, but also business and political dependencies, that we should ensuring are present with each API we deploy, providing an observable provenance of where ideas come from, and a honest look at how we build on the work of each other.

[<a href="/2017/08/07/providing-code-citations-in-machine-learning-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/07/api-industry-standards-negotiation-by-media-type/"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-services.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/07/api-industry-standards-negotiation-by-media-type/">API Industry Standards Negotiation By Media Type</a></h3>
			<p><em>07 Aug 2017</em></p>
			<p>I am trying to help push forward the conversation around the API definition for the Human Services Data Specification (HSDS) in a constructive way amidst a number of competing interests. I was handed a schema for sharing data about about organizations, locations, and services in a CSV format. I took this schema and exposed it with a set of API paths, keeping the flat file structure in tact, making no assumptions around how someone would need to access the data. I simply added the ability to get HSDS over the web as JSON–I would like to extend to be HTML, CSV, JSON, and XML, reaching as wide as possible audience with the basic implementation. As we move forward discussions around HSDS and HSDA I’m looking to use media types to help separate the different types of access people are looking for using media types. I don’t want to leave folks who only have basic CSV export or import capabilities behind, but still wanted to provide guidance for exchanging HSDA over the web. To help organize higher levels of demand on the HSDS schema I’m going to break out into some specialized media types as well as the default set: Human Services Data Specification (HSDS) - text/csv - Keeping data package basic, spreadsheet friendly, yet portable and exchangeable. Human Services Data API (HSDA) - application/json and text/xml, text/csv, and text/html - Governing access at the most basic level, keeping true to schema, but allowing for content negotiation over the web. Human Services Data API (HSDA) Hypermedia - (application/hal+json and application/hal+xml) - Allowing for more comprehensive responses to HSDA requests, addressing searching, filtering, pagination, and relationship linking between any HSDS returend. Human Services Data API (HSDA) Bulk - (application/vnd.hsda.bulk) - Focusing on heavy system to system bulk transfers, and eventually syncing, backups, archives, and migrations. Dealing with the industrial levels of HSDA operations. Human Services Data API (HSDA) Federated - (application/vnd.hsda.federated) - Allowing for a federated HSDA...[<a href="/2017/08/07/api-industry-standards-negotiation-by-media-type/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/04/when-you-see-api-rate-limiting-as-security/"><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_copper_circuit_2.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/04/when-you-see-api-rate-limiting-as-security/">When You See API Rate Limiting As Security</a></h3>
			<p><em>04 Aug 2017</em></p>
			<p>I’m neck deep into my assessment of the world of API security this week, a process which always yields plenty of random thoughts, which end up becoming stories here on the blog. One aspect of API security I keep coming across in this research is the concept of API rate limiting as being security. This is something I’ve long attributed with API management service providers making their mark on the API landscape, but as I dig deeper I think there is more to this notion of what API security is (or isn’t). I think it has more to do with API providers, than companies selling their warez to these API providers. The API management service providers have definitely set the tone for API security conversation(good), by standing up a gateway, and providing tools for limiting what access is available–I think many data, content, and algorithmic stewards are very narrowly focus on security being ONLY about limiting access to their valuable resources. Many folks I come across see their resources as valuable, when they begin doing APIs they have a significant amount of concern around putting their resources on the Internet, and once you secure and begin rate limiting things, all security concerns appear to have been dealt with. Competitors, and others just can’t get at your valuable resources, they have to come through the gate–API security done. Many API providers I encounter have unrealistic views of the value of their data, content, and algorithms, and when you match this with their unrealistic views about how much others want access to this valuable content you end up with a vacuum which allows for some very narrow views of what API security is. To help support this type of thinking, I feel like the awareness generated from API management is often focused on generating revenue, and not always about understanding API abuse, and is also something can create blindspots when it comes to database, server, and DNS...[<a href="/2017/08/04/when-you-see-api-rate-limiting-as-security/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/04/understanding-the-words-we-use-to-describe-machine-learning-apis/"><img src="https://s3.amazonaws.com/kinlane-productions2/mining-machine-learning.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/04/understanding-the-words-we-use-to-describe-machine-learning-apis/">Understanding The Words We Use To Describe Machine Learning APIs</a></h3>
			<p><em>04 Aug 2017</em></p>
			<p>I spend a lot of time trying out new APIs, working to understand what it is they do, or do not do. I have a pretty robust way of looking at APIs, profiling the company, and the APIs they offer, but when I’m wading through the marketing content, API documentation, and other resources, I am regularly stumped by the language that is used to describe what an API does. Honestly, this problem isn’t exclusive to machine learning APIs, but with the recent explosion in artificial intelligence, machine learning, deep learning, cognitive and other types of algorithmic voodoo, the words being used seem to have gone to entirely new levels. I am interested in understanding what it is an API does. I want to go from zero to understanding in 2.34 seconds. I don’t want to wade through marketing, and documentation to understand what an API does. I want to find simple, concise language that properly describes an API. In the world of artificial intelligence, this can be difficult to do, and is something that varies from provider to provider. Some machine learning API providers are better at describing what they do, while others seem to prefer hype, and fluff when it comes to explaining what is actually possible. As I continue my work profiling Amazon, Microsoft, and Google APIs I want to develop an approach to helping me separate what an API does, and what a provider says it does. I am going to continue profiling each API using OpenAPI, and labeling them with a common set of tags I’m using to quantify what machine learning APIs actually do. As I’m doing this I also decided to add an extra tag field called x-hype-tags, which gives me a way to track each of the additional words I found in the marketing and documentation, that I may not be actually using to describe what the API does–maintaining much of the API providers intent. One thing that...[<a href="/2017/08/04/understanding-the-words-we-use-to-describe-machine-learning-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/04/including-api-dependencies-within-your-api-definition/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-red-seal.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/04/including-api-dependencies-within-your-api-definition/">Including API Dependencies Within Your API Definition</a></h3>
			<p><em>04 Aug 2017</em></p>
			<p>I was learning about Pivio, a discovery specification for microservices the other day, and found their focus on microservice dependency to be pretty interesting. API dependencies has been an area I have found myself increasingly thinking about, as well as tracking on in my API research. I’m pretty close to publishing a project dedicated to understanding API, and microservices dependencies, which would overlap with containers, serverless, and other aspects of the API lifecycle that are all about breaking down the monolith. Each service definition using Pivio has a depends_on object, which allows for defining both internal and external service dependencies. Here is a snippet from a sample Pivio document to help articulate this interesting feature: This is where you can start connecting the cords between all of your services, something that is applicable to any APIs, whether you’ve drank the microservices kool-aid or not. I find it interesting that Pivio has internal, and external. I’d love to see an OpenAPI linked off each of the services it depends on. I also am fascinated with the first question for external, why? What a great first question for any dependency–it should also be available for internal services as well. Every API provider should be inquiring why a dependency exist whenever possible, and having it quantified in this way just opens up more chances for this question to get asked. Seeing dependencies quantified in Pivio makes me happy. It has been something I’ve wanted to reflect in APIs.json for some time now. Currently, I do not know of any way to quantify the relationship between APIs, and Pivio provides a glimpse at one possible way we might be able to map this world out. I have been learning more about Cytoscape, a network data integration, analysis, and visualization framework. Having a machine readable API dependencies definition would allow me to create a network visualization of any API or microservices discovery catalog. It wouldn’t take much work at all...[<a href="/2017/08/04/including-api-dependencies-within-your-api-definition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/03/when-describing-your-machine-learning-apis-work-extra-hard-to-keep-things/"><img src="https://s3.amazonaws.com/kinlane-productions2/contrafabulists/machine+learning.jpg" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/03/when-describing-your-machine-learning-apis-work-extra-hard-to-keep-things/">When Describing Your Machine Learning APIs Work Extra Hard To Keep Things</a></h3>
			<p><em>03 Aug 2017</em></p>
			<p>I’m spending a significant amount of time learning about machine learning APIs lately. Some of what I’m reading is easy to follow, while most of it is not. A good deal of what I’m reading is technically complex, and more on the documentation side of the conversation. Other stuff I come across is difficult to read, not because it is technical, but because it is more algorithmic marketing magic, and doesn’t really get at what is really going on (or not) under the hood. If you are in the business of writing marketing copy, documentation, or even the API design itself, please work extra hard to keep things simple and in plain language. I read so much hype, jargon, fluff, and meaningless content about artificial intelligence and machine learning each day, I take pleasure anytime I find simple, concise, and information descriptions of what ML APIs do. In an exploding world of machine learning hype your products will stand out if they are straight up, and avoid the BS, which will pretty quickly turn off the savvy folks to whatever you are peddling. Really, this advice applies to any API, not just machine learning. It’s just the quantity of hype we are seeing around AI and ML in 2017 is reaching some pretty extreme levels. Following the hype is easy. Writing fluffy content doesn’t take any skills. Writing simple, concise, plain language names, descriptions, and other meta data for artificial intelligence and machine learning APIs takes time, and a significant amount of contemplation regarding the message you want to be sending. The ML APIs I come across that get right to the point, are always the ones that stick around in my mind, and find a place within my research and storytelling. We are going to continue to see an explosion in the number of algorithmic APIs, delivering across the artificial intelligence, machine learning, deep learning, cognitive, and other magical realms. The APIs that deliver...[<a href="/2017/08/03/when-describing-your-machine-learning-apis-work-extra-hard-to-keep-things/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/03/different-search-engines-for-api-discovery/"><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/services/api-discovery.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/03/different-search-engines-for-api-discovery/">Different Search Engines For API Discovery</a></h3>
			<p><em>03 Aug 2017</em></p>
			<p>
I was learning about the microservices discovery specification Pivio, which is a schema for framing the conversation, but also an uploader, search, and web interface for managing a collection of microservices. I found their use of ElasticSearch as the search engine for their tooling worth thinking about more. When we first launched APIs.json, we created APIs.io as the search engine–providing a custom developed public API search engine. I hadn’t thought of using ElasticSearch as an engine for searching APIs.json treated as a JSON document.

Honestly, I have been relying on the Github API as the search engine for my API discovery. Using it to uncover not just APIs.json, but OpenAPI, API Blueprint, and other API specification formats. This works well for public discovery, but I could see ElasticSearch being a quick and dirty way to launch a private or public engine for an API discovery, catalog, directory, or type of collection. I will add ElasticSearch, and other platforms I track on as part of my API deployment research as a API discovery building block, evolving the approaches I’m tracking on.

It is easy to think of API discovery as directories like ProgrammableWeb, or marketplaces like Mashape, and public API search engines like APIs.io–someone else’s discovery vehicle, which you are allowed to drive when you need. However, when you begin to consider other types of API discovery search engines, you realize that a collection of API discovery documents like JSON Home, Pivio, and APIs.json can quickly become your own personal API discovery vehicle. I’m going to write a separate piece on how I use Github as my API discovery engine, then I think I’ll step back and look at other approaches to searching JSON or YAML documents to see if I can find any search engines that might be able to be fine tuned specifically for API discovery.

[<a href="/2017/08/03/different-search-engines-for-api-discovery/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/03/api-discovery-using-json-home/"><img src="https://s3.amazonaws.com/kinlane-productions2/json-home/json-home-widget-example.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/03/api-discovery-using-json-home/">API Discovery Using JSON Home</a></h3>
			<p><em>03 Aug 2017</em></p>
			<p>I’m have finally dedicated some time to learning more about Home Documents for HTTP APIs, or simply JSON Home. I see JSON Home as a nice way to bring together the technical components for an API, very similar to what I’ve been trying to accomplish with APIs.json. One of the biggest differences I see is that I’d say APIs.json was born out of the world of open data and APIs, where JSON Home is born of the web (which actually makes better sense). I think the JSON Home description captures the specifications origins very well: The Web itself offers one way to address these issues, using links [RFC3986] to navigate between states. A link-driven application discovers relevant resources at run time, using a shared vocabulary of link relations [RFC5988] and internet media types [RFC6838] to support a “follow your nose” style of interaction - just as a Web browser does to navigate the Web. JSON Home provides any potential client with a machine readable set of instructions it can follow, involving one, or many APIs–providing a starting page for APIs which also enables: Extensibility - Because new server capabilities can be expressed as link relations, new features can be layered in without introducing a new API version; clients will discover them in the home document. Evolvability - Likewise, interfaces can change gradually by introducing a new link relation and/or format while still supporting the old ones. Customisation - Home documents can be tailored for the client, allowing different classes of service or different client permissions to be exposed naturally. Flexible deployment - Since URLs aren’t baked into documentation, the server can choose what URLs to use for a given service. JSON Home, is a home page specification which uses JSON to provide APIs with a a launching point for the interactions they offer, by providing a coherent set links, all wrapped in a single machine readable index. Each JSON begins with a handful of values:...[<a href="/2017/08/03/api-discovery-using-json-home/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 12px;"><a href="/2017/08/02/when-cities-use-a-common-api-definition-to-report-nonemergency-issues/"><img src="https://s3.amazonaws.com/kinlane-productions2/open311/open311-api-list.png" alt="API Evangelist" width="325" align="left" /></a></div>
			<h3><a href="/2017/08/02/when-cities-use-a-common-api-definition-to-report-nonemergency-issues/">When Cities Use A Common API Definition To Report Non-Emergency Issues</a></h3>
			<p><em>02 Aug 2017</em></p>
			<p>I am taking a deeper look at Open311, as part of some wider municipal level API research and development I am doing. I am going to be helping evolve an OpenAPI for the project, as well as JSON schema for the API and underlying data model. As I’m working my way through the Open311 portal reacquainting myself with the open format for reporting of non-emergency issues within cities, I came across the list of cities who have implemented Open311, and get a glimpse at what the future of APIs at the city level can be. When you land on the Open311 GeoReport v2 Servers listing page you get a table of the twenty-one cities who have published an Open311 API, with the name, country, API discovery document, API key request location, documentation, production and sandbox environment URLs. Twenty-one separate cities, twenty-one separate APIs for reporting non-emergency issues, all using the same API definition. This is the way that all APIs at the city, county, state, and federal levels should work. They should leverage common schema and API definition, providing a federated list of resources that can be integrated into any application. Imagine when all cities have a common 311 API for reporting of non-emergency issues, as well as a 211 API for finding human services within a city. Imagine when the entire stack of city services all use common API definition(s), and schema across all API requests and responses. In this environment any city will be able take existing SDKs, or open source application, tools, or plugin, and put to work for a new city, with little or no changes. This is the way city, county, state, and federal agency software and application development should work. There should be a buffet of open source solutions for each layer of government operations. When it comes to this level of API operations within cities, Open311 is the furthest along down the road in the journey. This is...[<a href="/2017/08/02/when-cities-use-a-common-api-definition-to-report-nonemergency-issues/">Read More</a>]</p>
			<p><hr /></p>
	  

		<hr />
		<ul class="pagination" style="text-align: center;">
			
				<li style="text-align:left;"><a href="/blog/page9" class="button"><< Prev</a></li>
			
				<li style="width: 75%"><span></span></li>
			
				<li style="text-align:right;"><a href="/blog/page11" class="button">Next >></a></li>
			
		</ul>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="60%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home Page</a></li>
    <li><a href="/blog/">The Blog</a></li>
    <li><a href="https://101.apievangelist.com/">API 101</a></li>
    <li><a href="http://history.apievangelist.com">History of APIs</a></li>
    <li><a href="https://women-in-tech.apievangelist.com/">Women in Technology</a></li>    
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
